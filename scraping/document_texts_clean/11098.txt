detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/11098statistical analysis of massive data streams: proceedings ofa workshop395 pages | 8.5 x 11 | pdfisbn 9780309593021 | doi 10.17226/11098committee on applied and theoretical statistics; division on engineering andphysical sciences; national research councilstatistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.statistical analysis ofmassive data streamsproceedings of a workshopcommittee on applied and theoretical statisticsdivision on engineering and physical sciencesnational research councilof the national academiesthe national academies presswashington, d.c.www.nap.eduistatistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.the national academies press 500 fifth street, n.w. washington, dc 20001notice: the project that is the subject of this report was approved by the governing board of the national research council, whose members are drawn from the councils of the national academy of sciences, the national academy of engineering, and the institute of medicine.the members of the committee responsible for the report were chosen for their special competences and with regard for appropriate balance.this study was supported by the national security agency (grant #mda904œ02œ1œ0114), the office of naval research (grant #n00014œ02œ1œ0860), and microsoft (grant #2327100). any opinions, findings, conclusions, or recommendations expressed in this publication arethose of the author(s) and do not necessarily reflect the views of the organizations or agencies that provided support for the project.international standard book number 0309093082 (pod)international standard book number 0309545560 (pdf)additional copies of this report are available from the national academies press,500 fifth street, n.w., lockbox 285, washington, dc20055; (800) 624œ6242 or (202) 334œ3313 (in the washington metropolitan area); internet, http://www.nap.educopyright 2004 by the national academy of sciences. all rights reserved.printed in the united states of americacover illustrations: the terms ﬁdata streamsﬂ and ﬁdata riversﬂ are used to describe sequences of digitally encoded signals used torepresent information in transmission. the left image is of the oksrukuyik river in alaska and the right image is an example of a crashingwave, similar to the largest recorded tsunami on siberia's kamchatka peninsula. both images illustrate the scientific challenge of handlingmassive amounts of continuously arriving data, where often there is so much data that only a short time window's worth is economicallystorable. the oksrukuyik river photo is courtesy of karie slavik of the university of michigan biological station; the tsunami photo is courtesy of the u.s. naval meteorology and oceanography command and was obtained from its web site. both images are reprinted with permission.iistatistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.iiistatistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.committtee on applied and theoretical statisticsedward j.wegman, chair, george mason universitydavid banks, duke universityalicia carriquiry, iowa state universitythomas cover, stanford universitykaren kafadar, university of colorado at denverthomas kepler, duke universitydouglas nychka, national center for atmospheric researchrichard olson, stanford universitydavid scott, rice universityedward c.waymire, oregon state universityleland wilkinson, spss, inc.yehuda vardi, rutgers universityscott zeger, johns hopkins university school of hygiene and public healthstaffbmsa workshop organizersscott weidman, bmsa directorrichard campbell, program officerbarbara wright, administrative assistantelectronic report designsarah brown, research associatemeeko oishi, internivstatistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.acknowledgement of reviewersthis report has been reviewed in draft form by individuals chosen for their diverse perspectives and technicalexpertise, in accordance with procedures approved by the report review committee of the national research council(nrc). the purpose of this independent review is to provide candid and critical comments that will assist theinstitution in making its published report as sound as possible and to ensure that the report meets institutional standardsfor objectivity, evidence, and responsiveness to the study charge. the review comments and draft manuscript remainconfidential to protect the integrity of the deliberative process. we wish to thank the following individuals for theirreview of this report:amy braverman, jet propulsion laboratoryron fedkiw, stanford universitydavid madigan, rutgers universityjennifer rexford, at&t laboratoriesalthough the reviewers listed above have provided many constructive comments and suggestions, they were notasked to endorse the conclusions or recommendations, nor did they see the final draft of the report before its release.responsibility for the final content of this cd report rests entirely with the authoring committee and the institution.acknowledgement of reviewersvstatistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.preface and workshop rationaleon december 13 and 14, 2002, the committee on applied and theoretical statistics of the national researchcouncil conducted a twoday workshop that explored methods for the analysis of streams of data so as to stimulatefurther progress in this field. to encourage crossfertilization of ideas, the workshop brought together a wide range ofresearchers who are dealing with massive data streams in different contexts. the presentations focused on five majorareas of research: atmospheric and meteorological data, highenergy physics, integrated data systems, network traffic,and mining commercial streams of data.the workshop was organized to allow researchers from different disciplines to share their perspectives on how touse statistical methods to analyze massive streams of data, so as to stimulate crossfertilization of ideas and furtherprogress in this field. the meeting focused on situations in which researchers are faced with massive amounts of dataarriving continually, making it necessary to perform very frequent analyses or reanalyses on the constantly arrivingdata. often there is so much data that only a short time window's worth may be economically stored, necessitatingsummarization strategies.the overall goals of this cd report are to improve communication among various communities working onproblems associated with massive data streams and to increase relevant activity within the statistical sciencescommunity. included in this report are the agenda of the workshop, the full and unedited text of the workshoppresentations, and biographical sketches of the speakers. the presentations represent independent research efforts onthe part of academia, the private sector, federally funded laboratories, and government agencies, and as such theyprovide a sampling rather than a comprehensive examination of the range of research and research challenges posed bymassive data streams. in addition to these proceedings, a set of more rigorous, technical papers corresponding to theworkshop presentations has also been published separately as a 2003 special issue of the journal of computational andgraphical statistics.this proceedings represents the viewpoints of its authors only and should not be taken as a consensus report ofthe board on mathematical sciences and their applications or the national research council.preface and workshop rationalevistatistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.committee on applied and theoretical statisticsstatistical analysis of massive data streamsnational research councilwashington, d.c.december 13 and 14, 2002 december 13 welcome and overview of sessions sallie kellermcnulty, los alamos national laboratory chair, committee on applied and theoreticalstatistics james schatz, national security agency session 1. atmospheric and meteorological data douglas nychka, session chair, national center for atmospheric research introduction john bates, national climatic data center exploratory climate analysis tools for environmental satelliteand weather radar data amy braverman, jet propulsion laboratory statistical challenges in the production and analysis of remotesensing earth science data at the jet propulsion laboratory ralph milliff, colorado research associates global and regional surface wind field inferences fromspaceborne scatterometer data report from breakout group 1statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved. session 2. highenergy physics david scott, session chair, rice university introduction robert jacobsen, lawrence berkeley national laboratory statistical analysis of high energy physics data paul padley, rice university some challenges in experimental particle physics data streams miron livny, university of wisconsinmadison data grids (or a distributed computing view of highenergy physics) report from breakout group luncheon keynote address daryl pregibon, at&t shannon research laboratories keynote address: graph miningšdiscovery inlarge networks session 3. integrated data systems sallie kellermcnulty, session chair, los alamos national laboratory introduction j.douglas reason, los alamos national laboratory global situational awareness kevin vixie, los alamos national laboratory incorporating invariants in mahalanobis distancebasedclassifiers: applications to face recognition john elder, elder research ensembles of models: simplicity (of function) through complexity (of form) report from breakout group afterdinner address mark hansen, bell laboratories announcement from the whitney museum of american art untitledpresentation 2statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved. december 14 session 4. network traffic wendy martinez, session chair, office of naval research introduction william cleveland, bell laboratories fsd models for openloop generation of internet packet traffic johannes gehrke, cornell university processing aggregate queries over continuous data streams edward wegman, george mason university visualization of internet packet headers paul whitney, pacific northwest national laboratory toward the routine analysis of moderate to largesize data report from breakout group session 5. mining commercial streams of data leland wilkinson, session chair, spss, inc. introduction lee rhodes, hewlettpackard laboratories a stream processor for extracting usage intelligence from highmomentum internet data pedro domingos, university of washington a general framework for mining massive data streams andrew moore, carnegie mellon university kdrballand adtrees: scalable massive science data analysis report from breakout group 3statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.sallie kellermcnultywelcome and overview of sessionstranscript of presentationbiosketch: sallie kellermcnulty is group leader for the statistical sciences group at los alamos nationallaboratory. before she moved to los alamos, dr. kellermcnulty was professor and director of graduate studies atthe department of statistics, kansas state university, where she has been on the faculty since 1985. she spent 2 yearsbetween 1994 and 1996 as program director, statistics and probability, division of mathematical sciences, nationalscience foundation. her ongoing areas of research focus on computational and graphical statistics applied tostatistical databases, including complex data/model integration and related software and modeling techniques, and sheis an expert in the area of data access and confidentiality. dr. kellermcnulty currently serves on two nationalresearch council committees, the cstb committee on computing and communications research to enable betteruse of information technology in government and the committee on national statistics' panel on the research onfuture census methods (for census 2010), and chairs the national academy of sciences' committee on applied andtheoretical statistics. she received her phd in statistics from iowa state university of science and technology. she isa fellow of the american statistical association (asa) and has held several positions within the asa, includingcurrently serving on its board of directors. she is an associate editor of statistical science and has served as associateeditor of the journal of computational and graphical statistics and the journal of the american statisticalassociation. she serves on the executive committee of the national institute of statistical sciences, on the executivecommittee of the american association for the advancement of science's section u, and chairs the committee ofpresidents of statistical societies. her web page can be found at http://www.stat.lanl.gov/people/skeller.shtmlwelcome and overview of sessions4statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.transcript of presentationms. kellermcnulty: okay, i would like to welcome everybody today. i am sallie kellermcnulty. i amthe current chair of the committee on applied and theoretical statistics. this workshop is actually sponsored bycats. that is the acronym for our committee. it is kind of a bit of a déja vu looking out into this room, back to 1995,the nucleus of people who held the first workshop, or at least attended the first workshop that cats had, on theanalysis of massive data sets. it has taken us a while to put a second workshop together. in fact, as cats tried to thinkabout what makes sense for a workshop today, that really deals with massive amounts of data, is where we decided wewould really try to actually jump ahead a bit and try to look at problems of streaming data, massive data streams.now, the workshop committee, which consisted of david scott, lee wilkinson, bill dumouchel and jenniferwidom, when they started planning this, they were pretty comfortable with the concept of massive data streams.i think that, by the time that this actually got together, they debated whether, instead of data streams, it should bedata rivers. several of you have asked me what constitutes a stream, how fast does the data have to flow. i am notqualified to answer that question, but i think our speakers throughout the day should be able to try to address what thatmeans to them.we need to give a really good thank you to our sponsors for this workshop, which is the office of naval researchand the national security agency. now i will turn it over to jim schatz from nsa. he will give us an enlightening,boosting talk for the workshop.welcome and overview of sessions5statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.james schatzwelcome and overview of sessionstranscript of presentationjames schatz is the chief of the mathematics research group at the national security agency.welcome and overview of sessions6statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.transcript of presentationmr. schatz: thanks, sallie. i am the chief of the math research office at nsa. the sponsorship of theconference here comes from an initiative that we have at the agency called advanced research and developmentactivity, and dr. dean collins is the head of that, whom i think some of you probably met at one of these conferenceslast year. we are very happy and proud to be part of the sponsorship and so forth.i am only going to talk for a few minutes, but interrupt with questions as needed, in the spirit of what i think youare here for, which is not lecturing to each other, but having a dialogue on things.of course, i don't think it is a big secret why the national security agency is interested in massive data sets. idon't know what the stream rate for massive data sets is either, but i think we are going to make it at our place.let me dwell on the obvious here just for a few minutes. as we look back over this past year, of course, a bigquestion for us, not only for us as individuals, but for the government in the form of official commissions and so forthis, could we have prevented 9/11.we look back on that at a kind of obvious point. there is a question, was there a message in our collectionsomewhere that said, attack at dawn, with all the details and so forth? while we certainly have done a due diligencesearch of everything we can lay our hands on that was available to us prior to 9/11 and we haven't found such atransmission, another type of question, though, that probably bothers us a lot more is if we got one last night that said,attack at dawn, would we have noticed it?we have so much data to look at, and we don't look at the data. our computers do first. so, if the computers don'tfind that message, and that means if the algorithms don't find that message, the analysts aren't going to read thatmessage. that is just sort of the beginning part, of course, the most obvious. already, it gets us into huge questionsabout what is the nature of our databases, how do we store data, how do we retrieve data.of course, in the past year of really being thrown into a whole new paradigm of intelligence analysis and so forth,we are more in the situation of asking the question, okay, we are probably not going to be fortunate enough to have amessage that says, attack at dawn. what we are going to have to do is find clues in a massive data set and put thattogether and, to do that, that there is something happening tomorrow morning.it has really been a huge change for us. it is not that we weren't thinking about massive data sets before; of coursewe were. when you are traditionally, after decades and decades, looking at welldefined nationstate targets, like iraq,and you wouldšyour way of approaching the analysis is sort of dictated by the fact that there is a country withnational boundaries and a military and diplomats and various things like that to worry about.we were certainly aware of terrorist targets and studying them and worried about them and taking action longbefore 9/11, but of course, an event like that pumps up the level of urgency just beyond anything else that you coulddo. the postmortem stuff of analyzing what we could have done or would have done will, of course, go on fordecades, just like today you still hear people talking about, could we have prevented pearl harbor. i think, 50 yearsfrom now, those same questions will be being asked. of course, we are here now and we have to worry about thefuture, for sure.welcome and overview of sessions7statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.i looked over the topics here and the group. it is a wonderful group of people. i am even starting to recognize lotsof names and people who are good friends like david scott, of course, who has been working with us for so long.i am not one of the experts in the group, but i know that we have got a good pile of the experts here. even if youare interested in environmental data or physics data, of course, there is one thing that you don't have to worry aboutwith that data, i hope, which is that a good portion of it is encrypted. even if we get past the encryption problem andsay, supposed that all of our data is unencrypted, you probably do have to deal with some amount of garbling and thatsort of stuff in your data, too.i imagine that we are all looking at the same kinds of questions, which are, there are certain events in our data thatwe would like to be able to spot and just pull out, because we know what they are going to be, and we just want tohave rapid ways to do that. i think the real sense of where the science is going, at least for us, and i think for you is,how do we take a massive data set and see pieces of what we need to see in many different places and pull it togetherand make conclusions based on that, and how do we find patterns?for us, a key aspect of this problem, of course, is we don't have a homogeneous type of a data set. we have gotany kind of communications medium that you can imagine, we will have that data. a key problem for us is kind ofcombining information from all these different things, and database structures for how you would actually set things upto make the algorithms run in a situation like that.certainly, kay anderson and dave harris, who are from our group are here today, were working on these typesof problems long ago. it didn't start a year ago, but post 9/11, we have ramped up dramatically our efforts in theseareas. s&t in the research area alone, there are dozens of people working on this who weren't working on it a year ago.we have certainly got tons to learn about this stuff. it just seems, with the data explosion that everybody is goingthrough, we are all kind of new at it, in a sense.i hope, in the spirit of these conferences, our guys will find ways to share technical things with you as best theycan, and that even with all your proprietary information that you have to worry about, you can have a good technicalexchange back with us. it is certainly an area where there are a lot of economic issues, and companies have ways ofdoing things and so forth, but hopefully the incrowd here can get down to the mathematics and the statistics and shareideas.we need a lot of help in this area. what we are doing is dramatically good. we have had some amazing successstories just in the past year that were an absolute direct result of what i would call data mining on massive data sets.i can assure you, for us, it is not just an academic exercise. we are right in the thick of it. we utilize it every day.it has done wonderful stuff for us in the past year, and we are getting a lot out of these conferences. i popped into onelast year, and i am glad to see a lot of the same faces.audience: [question off microphone.]mr. schatz: probably not, but i do want you to know that i am not just saying that to make you feel good. wereally have had some dramatic successes in terms of techniques we didn't have a year ago for looking for patterns inmassive data, drawing conclusions and taking some known attributes of a situation and mining through the datawelcome and overview of sessions8statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.to find new ones, and very algorithmic based, and really providing tools for our analysts.of course, however many analysts we havešand i wouldn't know what that number is, it is finite, and any givenhuman being can only look at so much text and pictures in one day.for us, it is all about teaching the machines how to work for us, and teaching the machines is teaching thealgorithms. i can't think of an example that we could share with you, but real examples, real intelligence, real impact,plenty of it, just in this past year, based on the kinds of techniques we are learning with you.anyway, i don't want to overstay my welcome, because there is some real work to do, but if there are a couplemore questions, i would be happy to talk.audience: [question off microphone.]mr. schatz: certainly, gigabytes on a daily basis and so forth. maybe our experts will find a way they cangive you a better sense of that. i don't really know.the thing is, we have lots of channels coming in at lots of rates, and if you put it all together, it would besomething astronomical. we probably span every range of problems you could think of. it is not as though we have themother lode coming in through one pipe every minute. we have lots of ways of collecting lots of sources.i am sure some of our most important stuff is very slow speed compared to the things you are talking about, andsome of it is very high speed.there isn't any kind of one technique that we are looking for, and any range of techniques herešyou know,something that takes longer and has to work at slower speeds is probably just as interesting to us as something that hasto work at the speed of light, we are going to have all kinds of problems to apply this stuff to.anything else i could give a vague kind of government answer to? okay, sallie, you are back, or maybe john isup, and thanks for being here, and we are happy to be part of this, and thanks for the research.welcome and overview of sessions9statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.douglas nychka, chair of session on atmospheric andmeteorological dataintroduction by session chairtranscript of presentationbiosketch: douglas nychka is a senior scientist at the national center for atmospheric research (ncar)and is also the project leader for the geophysical statistics project (gsp). he works closely with many of thepostdoctorate fellows at ncar and his primary goal is to emphasize interdisciplinary research: migrating statisticaltechniques to important scientific problems and using these problems to motivate statistical research. dr. nychka'spersonal research interests include nonparametric regression (mostly splines), and statistical computing, spatialstatistics, and spatial designs. he received his undergraduate degree from duke university in mathematics and physicsand his phd from the university of wisconsin under the direction of grace wahba. he came to gsp/ncar in 1997after spending 14 years as a faculty member in the statistics department at north carolina state university.introduction by session chair10statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.transcript of presentationmr. nychka: so, without further ado, our first speaker is going to be john bates at the national climatic datacenter. he will be talking about exploratory climate analysis and environmental satellites and weather radar data.introduction by session chair11statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.john batesexploratory climate analysis tools for environmentalsatellite and weather radar dataabstract of presentationtranscript of presentation and powerpoint slidesbiosketch: john j.bates is the chief of the remote sensing applications division of the u.s. nationaloceanic and atmospheric administration's (noaa's) national climatic data center. dr. bates received a phd inmeteorology from the university of wisconsinmadison in 1986 under william l.smith on the topic of satelliteremote sensing of airsea heat fluxes. dr. bates then received a postdoctoral fellowship at scripps institution ofoceanography (1986œ1988) to work with the california space institute and the climate research division. he joinedthe noaa environmental research laboratories in boulder, colorado, in 1988 and there continued his work inapplying remotely sensed data to climate applications. in 2002, dr. bates moved to the noaa national climatic datacenter in asheville, north carolina.dr. bates' research interests are in the areas of using operational and research satellite data and weather radar datato study the global water cycle and studying interactions of the ocean and atmosphere. he has authored over 25 peerreviewed journal articles on these subjects. he served on the ams committee on interaction of the sea andatmosphere (1987œ1990) and the ams committee on applied radiation (1991œ1994).as a member of the u.s. national research council's global energy and water cycle experiment (gewex)panel (1993œ1997), dr. bates reviewed u.s. agency participation and plans for observing the global water cycle. hewas awarded a 1998 editors' citation for excellence in refereeing geophysical research letters for ﬁthorough andefficient reviews of manuscripts on topics related to the measurement and climate implications of atmospheric watervapor.ﬂ he has also been a contributing author and u.s. government reviewer of the intergovernmental panel onclimate change assessment reports. he currently serves on the international gexex radiation panel, whose goal isto bring together theoretical and experimental insights into the radiative interactions and climateexploratory climate analysis tools for environmental satellite and weather radar data12statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.feedbacks associated with cloud processes, including the effects of water vapor within the atmosphere and at earth'ssurface.exploratory climate analysis tools for environmental satellite and weather radar data13statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.abstract of presentationexploratory climate analysis tools for environmental satellite and weather radar data john bates, nationalclimatic data center1. introductionoperational data from environmental satellites form the basis for a truly global climate observing system.similarly, weather radar provides the very high spatial and rapid time sampling of precipitation required to resolvephysical processes involved in extreme rainfall events. in the past, these data were primarily used to assess the currentstate of the atmosphere to help initialize weather forecast models and to monitor the shortterm evolution of systems(called nowcasting).the use of these data for climate analysis and monitoring is increasing rapidly. so, also, are the planning andimplementation for the next generation of environmental satellite and weather radar programs. these observingsystems challenge our ability to extract meaningful information on climate variability and trends. in this presentation, iwill attempt only to provide a brief glimpse of applications and analysis techniques used to extract information onclimate variability. first, i will describe the philosophical basis for the use of remote sensing data for climatemonitoring, which involves the application of the forward and inverse forms of the radiative transfer equation. then iwill present three examples of the application of statistical analysis techniques to climate monitoring: (1) the detectionof longterm climate trends, (2) the timespace analysis of very large environmental satellite and weather radar datasets, and (3) extreme event detection. finally, a few conclusions will be given.2. philosophy of the use of remote sensing data for climate monitoringremote sensing involves the use of active or passive techniques to measure different physical properties of theelectromagnetic spectrum and to relate those observations to more traditional geophysical variables such as surfacetemperature and precipitation. passive techniques use upwelling radiation from the earthatmosphere system indiscrete portions of the spectrum (e.g., visible, infrared, and microwave) to retrieve physical properties of the system.active techniques use a series of transmitted and returned signals to retrieve such information.this is done by using the radiative transfer equation in the socalled forward and inverse model solutions. in theforward problem, sample geophysical variables, such as surface temperature and vertical temperature and moistureprofiles, are input to the forward radiative transfer model. in the model, this information is combined with specifiedinstrument error characteristics and responsivity to produce simulated radiances. the inverse radiative transfer problemstarts with satelliteobserved radiances. because the inverse radiative transfer equation involves taking the inverse ofan illconditioned matrix, a priori information, in the form of a first guess of the solution, is required to stabilize thematrix prior to inversion. the output of this process is geophysicalexploratory climate analysis tools for environmental satellite and weather radar data14statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.retrievals. the ultimate understanding of the satellite or radar data requires full application of the forward and inverseproblems and the impact of uncertainties associated with each step in the process.exploratory climate analysis tools for environmental satellite and weather radar data15statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.transcript of presentationmr. bates: thank you. i didn't think we would be in the big room. it is nice to be in this building. i am going tomainly talk about some of our larger socalled massive data sets that we acquire now over the wire from bothenvironmental satellitesšthe ones you see on the television news every night, the geostationary satellites.noaa, as well as department of defense, also operate polarorbitingšthat is, satellites that go pole to pole andscan across a swath of data on a daily basis. also, right now, our biggest data stream coming in is actually the weatherradar data, the precipitation animations that you see now nightly on your local news. in talking in terms of what wejust heard, in terms of the different data sets that come in, they come in from all different sources.the national climatic data center is the official repository in the united states of all atmospheric weatherrelated data. as such, we get things like simple data streams, the automatic observing systems that give youtemperature, moisture, cloud height at the weather service field offices. those are mostly colocated now at majorairports for terminal forecasting, in particular.we have, in the united states, a set of what are called cooperative observers, about 3,000 people who have theirown little backyard weather station, but it is actually an officially calibrated station. they take reports. some of themphone them in, and deposit the data, and that is a rather old style way of doing things.we have data that comes in throughout the globe, reports like that, upperair reports from radiosondes, and thenthe higher data now are the satellite and weather radar data. the united states operates nominally two geostationarysatellites, one at 75 watts, one at 135 watts. the japanese satellite, which is at 155 degrees east, is failing. so, we arein the process of actually moving one of our united states satellites out there. then, of course, these polarorbitingsatellites.i am mostly going to talk about the polarorbiting satellites and some techniques we have used to analyze thosedata for climate signals. those data sets started in about 1979, late 1978, and then go through the present.exploratory climate analysis tools for environmental satellite and weather radar data16statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.this is what i want to talk about today, just give you a brief introduction of what we are thinking about asmassive data is coming in, and we are responsible for the past data as well as the future planning for data coming infrom the next generation of satellites. a couple, three examples of how we use some techniques, sometimes rathersimplistic, but powerful, to look at the longterm climate trends, some timespace analysisšthat is, when you havethese very high spatial and temporal data sets, you would like to reduce the dimensionality, but yet still get somethingmeaningful out about the system that we are trying to study. then, just briefly talk about amplification of the radardata. i just inherited the radar data science there, and so, that is new stuff, and it has just really begun in terms of datamining. so, when you have rare events in the radar such as tornadic thunderstorms, how can we detect those. then,just a couple of quick conclusions.that is what we are talking about in terms of massive here. so, the scale is time from about 2002 projecting outabout the next 15 years or so. this is probably conservative because we are reexamining this and looking at moredata, probably, more products being generated than we had considered before. on the axis here is terabytes, becausepeople aren't really thinking of pedabytes. those numbers are really 10, 20, 30 pedabytes. right now, we have got alittle over one pedabyte and daily we are probably ingesting something like a terabyte.the biggest data set coming in now is that we are getting the next rad datašthis is the weather radar datašfromabout 120 sites throughout the united states. we are getting about a third of that in real time. they used to come in onthe little xabite 8 millimeter cassettes. for years, we used to just have boxes of those because there wasn'texploratory climate analysis tools for environmental satellite and weather radar data17statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.a funded project to put this on mass store. in the last two years, we have had eight pc work stations with each havingeight readers, tape readers, on them, to read back through all the data and get it into mass store. so, now we can get ourhands on it.so, the first lesson is accessibility of the data, and we are really just in a position now to be going through thedata, because it is accessible. we are looking at data rates by 2010, on the order of the entire archivešthis iscumulative archives. so, that is not data read per year, so it is cumulative archive building up, of something over 30pedabytes by the year 2010 or so. so, that is getting fairly massive.in terms of remote sensing, there is a philosophical approach, and i am not sure how many of you have workedwith remote sensing data. there are two ways of looking at the data, sort of the data in the satellite observationcoordinates or the geophysical space of a problem you want to deal with.these are referred to variously as the forward problem. just very briefly, the forward problem, you havegeophysical variablesštemperature and moisture profiles of the atmosphere, the surface temperature, and yoursatellite is looking down into that system. so, using a forward modelša forward model being a radiative transfermodel, physical model for radio transfer in the atmospherešyou can simulate socalled radiances.the radiances are what the satellite will actually observe. in the middle are those ovals that we want to actuallywork on, understanding the satellite data and then understanding the processes of climate, and then, in fact, improvingmodeling. as an operational service and product agency, noaa is responsible for not just analyzing what is going onbut, foolishly, we are attempting to predict things. analogous to other businesses, we are in the warning business. thenational weather service, of course, is bold enough to issue warnings.however, when you issue warnings, you also want to look at things like false alarm rate. that is, you don't wantto issue warnings when, in fact, you don't have severe weather, tornadoes, etc.the other aspect of the problem, the socalled inverse problemšso, starting from the bottom therešyou take thesatellite radiances and we have an inverse model that is usually a mathematical expression for the radio transferequation which is nonlinear. we linearize the problem. then we have a linear set of equations. the inverse model,then, is an inverse set of equations. the matrix is usually ill conditioned. so, we go to those yellow boxes andcondition the matrix by adding a priori information, a forecastexploratory climate analysis tools for environmental satellite and weather radar data18statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.first guess, other a priori data, and then biases to somehow normalize the data set. we invert that to get geophysicalretrieval. then we can retrieve temperature and moisture profiles. we can retrieve surface properties, surfacetemperature, ocean wind speed, other geophysical quantities of interest.so, the first application, detection of longterm climate trends using environmental satellite data, the issue ofglobal warming has really surfaced in the last 10 years. we would like to know, is the earth warming, how much. aresystems changing? how much? is there more severe weather? that would just be an issue with the extremes in adistribution. you know, certain weather events are normal distributions. certain aren't. precipitation is not normallydistributed by any sense of the imagination. we get far fewer events of extreme rainfallšprecipitationšthan we do oflight precipitation. so, it is more of a log normal distribution. we would like to know, are the extremes changing. so,that is a small portion of those distributions.with satellite data, we face a couple of unique problems. first, we are sensing the atmosphere with satellites thathave a lifetime of three to five years. so, we need to create a socalled seamless time series so that, when you applytimespace analysis techniques, you are not just picking up artifacts of the data that have to do with a different satellitecoming on line. we use a threestep approach to that, something we call the nominal calibration. that is where youtake an individual satellite, do the best you can with that satellite in terms of calibrating it, normalizing the satellites,and i will show you what that means. we have different satellites with different biases. often, different empiricaltechniques are used to stitch those together in a socalled seamless manner. we would like an absolute calibration.that, of course, is very difficult, because what is absolute, what is the truth?then, we would like to apply some consistent algorithm. in the infrared, when you are remote sensing in theinfrared, and you are looking down at the atmosphere from space, in the infrared, clouds are opaque. so, in order tosend the temperature and moisture profile down to the surface, you have to choose or detect the cloudfree samples.so, you have to have a threshold that tells you, this is cloudy, this is clear. you can base that on a number of differentcharacteristics about the data, usually time and spatial characteristics, time and space variability. clouds move, thesurface tends to be more constant in temperature. not always, but the oceans certainly do. so, you use those statisticalproperties about changes in time and space of the data set, to allow you to identify clouds. you have to look atnavigation. you have to do all kinds of errorexploratory climate analysis tools for environmental satellite and weather radar data19statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.checks, and then build retrievals to go from your radiant space to your geophysical space.then, we get, finally, into the fun part, exploratory data analysis. i tend to view this as sort of my tool kit out therein the shop working on a data set where, you know, you throw things at it and see what sticks. once you get somethingthat looks interesting, you start to formulate hypotheses about the physical system, how it works, and how your data setcompares to what physics of the problems say are possible solutions. then you might go on to look at data analysis andconfirm your hypothesis.anyway, let's go through the first step here. i am going to take more time with this first example and a little lesswith the second and just briefly go into the third one.so, creation of seamless time series, you have here three different channels of data from a satellite, channel 8, thatis an infrared window, channel 10 is actually a moisture channel in the upper atmosphere, a socalled water vaporchannel, and these channelsš10, 11, 12šare all water vapor channels. we look at emission lines of water vapor in theatmosphere. channel 12 in particular we are going to look at because it is involved with a socalled water vaporfeedback mechanism in global warming. in global warming, we hear these numbers quotedšatmospheric, oh, thetemperature is going to go up two degrees in 100 years. actually, anthropogenic co2 manmade gasses only contributeone of the two degrees there. the extra warming, the other one degree of warming, comes from a socalled water vaporfeedback. so, there has been a lot of controversy in the community about, does this water vapor feedback, in fact, workthis way or not.so, the different colors in these three charts, then, i am showing three things. one is the average globaltemperature over time, and this is a 20year data set. so, the top line in each one of these is just your monthly meandata point for each of these satellites over time, about a 20year time series on each one. these are four differentchannels. the meanšyou see the march of the annual cycle up and downšthe standard deviation of the data set, andjust simply the number of observations, these are something like millions of observations a monthšyou can't read thatscale here, this is times 106. so, on the order of, you know, 5 or 6 million or so observations a month coming down.this is from the polarorbiting satellite. so, these have sampled the entire planet. the geostationary only samplethat region that they are over. you can see a bit of the problem here, especially in this channel 12 where, number one,there are offsets between the different colors. the different colors are the different satellites. over this time period,there are eight different satellites. they are designated by this noaa7, 8, 9, 10,exploratory climate analysis tools for environmental satellite and weather radar data20statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.etc. these are a series of the noaa polarorbiting satellites.so, there are a couple of things you can pick out right away. there are biases betweenšthis is supposed to be thesame channel, but physically we know there are some differences. we can account for some of those, physically, it isjust a matter of the system. we would like to seam these time series together. there are offsets and there is anotherproblem here that you can probably see. this yellow one drifts in time, while the satellite crossing time is actuallydrifting in time later in the day. this can be problematic, depending on what you are trying to study. so, we would liketo stitch those time series together to get a seamless time series, and then do time series analyses.so, it takes a lot of checking. over here, these are individual swaths of data, so, swath one, two, three. this is themiddle east. this is saudi arabia, the red sea. this is africa, south africa here. the different colors denote thedifferent temperatures that the system is radiating at. then, we have several other things going on here.we have already applied the cloud detection algorithm. so, the spotty pixilation of these swaths, the dark spotsare where we have detected clouds and then not put a color in. so, you only see color where we have detected alreadythat it is clear. then you have another process you are banding here. the instrument is calibrated every 40 lines. so,instead of looking at the earth, it looks inside the housing at black bodies with constant temperatures, so it can get anabsolute calibration every 40 lines.so, this is a couple of swaths. there are 14 swaths a day for each satellite. you start to composite them together ina global view now. so, this is the global area, this is the americas here, north and south america, the outlines are inwhite of the continental land boundaries, the pacific ocean. here, again, we have colorcoded the radiant temperatures.dark areas are missing. that is persistent cloudy areas. then, we have gridded these together for a fiveday periodwhere we can start to evaluate that visually also for quality.then, this is a longterm sort of diagram of the health of the satellite data set again. these are just simplestatistical quantities, but very helpful for scanning out bad data. what we have here plotted are simply the mean in red,of each swath, for an entire year. so, we just compute the mean of each channel, pretty simple, the maximum that wedetect and the minimum that we detect.you can start to see, when you do this, right away you get outliers, and those outliers occur preferentially indifferent seasons. as this satellite goes around, seasonally, you will see different things, and you tend to haveproblems. this alreadyexploratory climate analysis tools for environmental satellite and weather radar data21statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.allows us to throw a lot of data out that we have found is out of bounds. on the other hand, if you are looking forabnormal things, this may be the data you are interested in.for us, we know this is the bad data. we don't want that. so, we composite them, we obtain metadata, and wesave that for further analysis. now that we have seamed everything together, what do we want to do? well, we want totry to get a handle on what the system is doing.what we have done here is composite a bunch of different analyses together about the system. the top two panelsare the spatial patterns of empirical orthogonal analysis of precipitation, and then this is water vapor. what we havedone here is that we have subtracted the annual harmonics from the time series of the data sets, so that we can look atinterregnal variability. so, it is just a simple filtering technique. what we have done is, we have fit the first threeharmonics in the annual cycle to all the data sets, to every point in the tropics. so, these are 30 degrees north/southnow. we subtracted those out. then, we have done empirical orthogonal analysis on monthly mean data. this isprecipitation.this is the socalled el niño swing in the pacific. so, during warm events with el niño, you have less convectionand precipitation in the west pacific and more in the central and eastern pacific. that signal shows up much more as aglobal signal in the upper tropospheric humidity. you have teleconnections, socalled teleconnections, where thespecific pattern here, again, is much more moist in the central equatorial pacific. at the same time, you have muchdrier areas north and south of that. so, you have a speeding up of the whole atmospheric cycle.these are typical indices of time series. so, this is a 22year time series. these are el niño events. these are1982œ1983, a large event. then you tend to get a small cold event. this is a modest event in 1986œ1987, a big coldevent in 1989. cold events for the united states, in particular, are noted for droughts. it bounces around in the early1990s, and this is 1997œ1998, when it got a lot of publicity. you see it is sort of a fouryear periodicity. the other timeseries that i didn't show youšit is supposed to be over here and magically isn't, i don't thinkšthis is the other timeseries i wanted to show you. these are just global average time series, 30 degrees south, where we have stitched thesethings all together, we have gone through about 1.5 terabytes of data. now, we are just doing some really bigtimesummarizing of the data.these are just simple tropical time series, and here you see the speed of el niño again. it is about every three tofour years on these time series. the time series i amexploratory climate analysis tools for environmental satellite and weather radar data22statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.interested in are these guys for the tropics. this is this upper trop humidity. i have subtracted, still, these harmonics ofthe annual cycle. so, i was very surprised to see these time series sort of just look like white noise. i said, what in theworld is this? i said, i know what these are. this is an el niño event, this is an el niño event, this is a cold event, hereis the big 1997œ1998 el niño. so, this is sort of easy, when i see these beats of this time series. i know what thoseguys are. when i saw this i saidšof course, the first thing you always say to yourself, did i screw up. did i reallysubtract out the annual cycle from here to get interregnal variability?so, i went back. yes, i did. what in the world is going on with this? then i started noticing, while some of thesepeaks here are synchronous with these peaks here and some of these other ones are synchronous, there is a lot moregoing on. these time series, this one here, and this one which is a global radiation time series, there is a lot more goingon. there is much more, if you will, of white noise. this is more red, or this is a period of three or four years. this hasa lot of stuff going on. there are some synchronous events going on. based on that, and some talking with colleagues,we formulated a hypothesis that involves a seasonality and an interregnal time scale.what we came up with is also knowledge of the system. that is, the dynamics of the atmosphere works such that,when you get strong westerlies across the equator in these el niño cold events, you get westerlies, and this can lead tostrong eddies. big eddies are just big winter storms and they flex moisture up into the upper atmosphere. on the otherhand, this leads to the possibility of a dynamical wave duct. in the atmosphere, you can get storms in this configurationof the atmosphere in northern winter and spring, or you can get storms that come down deep into the tropics, andactually cross the equator. in summer, you can't get that. that is why you don't see those extremes in summer.this one, you have the opposite. this is an el niño warm event condition. you have deep convection extendingout deep across the pacific, along the equator to the central, and even the eastern pacific. what happens there is, youhave strong westerlies, dynamically, no winds and then westerlies. that means that these storms actually can't goagainst the gradient of the planet here and against this sheer. so, no storms get into the subtropics. it gets very dry, andthis helps balance the system in the warm and cold events. so, it is sort of a neat thing.on the longer time scale, we are interested in longerterm trends of the system. so, we want to take those longtime series and, again, do some rather simple things.exploratory climate analysis tools for environmental satellite and weather radar data23statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.longterm climatology, that is just the longterm monthly mean for 20 years. this is your global pattern. in thetropics, your blues are your monsoon regions, the reds are your desert regions, basically, and this is just a zonalaverage of that, that shows you that the tropics are moist, the subtropics are dry, in midlatitudes, you have more of aconstant temperaturemoisture relationship.this is your linear trend, pretty simple, just a linear fit to the system. subtropics, or the deep tropics, because ofthose el niño events of the last 20 years, are trending to tend slightly more moist in the subtropics to lowermidlatitudes drying out a little bit. of course, you would like to assess the statistical significance of any of this. thisconfidence interval is just computed at each grid point time series, and it is both the fit to the linear trend, plus a rednoise persistence term. that is just a simple lag one autocorrelation, and then fit to the significance in the length of thetime series.example number two, i will try and speed up a little bit. we would like to try to reduce the dimensionality ofmassive timespace data sets. one of the easiest ways to do this is also to take advantage of one of our dynamicalsystems. many of our systems propagate west to east basically along latitudes. so, what we can do is, we can takeadvantage of this by taking a cross section at any longitude, and then averaging data for latitudinal bands. by doingthis, we have reduced the dimensionality.so, here is an example of radar echoes from these weather radar precipitation data sets. we take a particularlongitude here. you are about 90 degrees west. then, what you can do is average fivedegree latitude swaths. fromthat, you end up with a diagram that you see on the bottom. we call these hovmöller diagrams. the weather pioneersexploratory climate analysis tools for environmental satellite and weather radar data24statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.did this. this is how they predictedšthe first predictions were not numerical models, but were statistical techniqueswhere we reduced the dimensionality of the data set, and then looked at the propagation speed. since this is a timespace diagram, this is the degrees of longitude per time steps. here are days on your ordinate here. we can actually,from these diagrams, just come off with a propagation speed of various phenomena here. these guys tend to propagateslower, and these guys are propagating a little faster. this, although simple, is a very powerful technique.i am going to skip that example and i am going to go right to the end here.what you would really like to do is analyze this in timefrequency space. you apply fft to both time and spacedimensions and you come out with a frequency wave number diagram that allows you to detect various atmosphericphenomena, so called maddenjulian oscillations, kelvin waves and other waves, their propagation direction, theirwave length and their periodicity.exploratory climate analysis tools for environmental satellite and weather radar data25statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.these are very powerful. we have used those now to look at onsets of changes in the monsoons and regimes thatfavor or suppress hurricane activity in both the atlantic and pacific oceans.just real quick, this is just data mining techniques. this is radar. this was a confirmed tornado. this is dopplervelocity sheer, smallscale signatures only.this is the largescale outflow boundary and techniques are being developed to classify those schemes. as withany classification techniquesši have just inherited this onešthe classification depends on your trainer and then yourcriteria for evaluating whether or not you have success, including probability of detection, falsealarm rate and soforth. so, some of those techniques are applicable to many different situations. again, with the public, if you areissuing warnings, like the national weather serviceexploratory climate analysis tools for environmental satellite and weather radar data26statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.does with severe weather, you want people to take those warnings seriously. so, you want to have not only a goodsuccess rate, but a low falsealarm rate. so, you need to balance all those different factors in evaluating any techniquefor detection.so, concluding, we have got these massive data streams that are going to continue increasing geometrically in thenext 10 to 15 years. the statistical tools range from simple to complex but, because we are dealing with such a difficultphenomenon, i really like a lot of the simple tools to first get a handle on our system. the outlook for hardware is thatthe hardware will probably keep up with these massive data rates, but our investment, i think, and i think many peopleare finding this rather obvious, so maybe i am just stating the obvious, that additional investment in the peopleresource is required, to ensure that the future generations have the technical skills required to fully exploit thesemassive data sets. thank you.exploratory climate analysis tools for environmental satellite and weather radar data27statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.amy bravermanstatistical challenges in the production and analysis ofremote sensing earth science data at the jet propulsionlaboratorytranscript of presentation and powerpoint slidesbiosketch: amy braverman is a statistician at the jet propulsion laboratory. she received a phd in statisticsfrom the university of california, los angeles, in 1999 and an ma in mathematics in 1992, also from ucla. from1999 to 2001 she was a caltech postdoctoral scholar at the jet propulsion laboratory (jpl) and was hired aspermanent staff in late 2001.dr. braverman's research focuses on data reduction techniques for massive data sets. these methods are based onstatistical clustering and signal processing algorithms modified for use in data analytic settings. at jpl dr. bravermanserves on project teams for the atmospheric infrared sounder (airs) and the multiangle imaging spectroradiometer(misr). she is responsible for the design of data reduction algorithms. she is also involved in active researchcollaborations with jpl's machine learning group to develop data mining techniques and tools for data from nasa'searth observing system. dr. braverman has published in both statistics and geoscience journals, is active in theamerican statistical association and the american geophysical union, and is an officer of the interface foundation ofnorth america.statistical challenges in the production and analysis of remote sensing earth science data atthe jet propulsion laboratory28statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.transcript of presentationms. kellermc nulty: our next speaker is amy braverman from the jet propulsion lab. amy has a mac,so this is going to be a new experience for me.ms. braverman: this is the first time i have used this computer for a presentation, and i also took the plungeand did my presentation in powerpoint for the first time. so, beware, if i have problems.ms. kellermc nulty: while they are setting this up, i want to remind everybody to look at your programs.we have scheduled some time in the afternoon for some breakout sessions with focused discussion in each of thepresentation areas.so, write your questions down and think about that, so if it isn't covered here and doesn't get covered in the break,you will have some time to talk to each other about some of the problem areas and ideas.ms. braverman: i would like to thank a couple of people. i would like to thank doug for inviting me tocome and talk here. i have been chomping at the bit to get some help, and this seems like the perfect opportunity to, idon't know, to whine for it, let's say. i would also like to thank the organizers for holding this workshop. it was basedon the proceedings of the 1995 conference that i got into this problem in the first place, and was able to find ralphkahn at the jet propulsion laboratoryšthat is how i met him, was reading the proceedings and saying to myself, gee,he is right across town there and i need a good application for my dissertation work, that looks like a good one. so, therest is history, and i finally got a job out of it, just last year. i actually got a job as a graduate student and then as a postdoc, and now as a regular bona fide person.i would like to include some interesting images in the presentation, and i don't necessarily plan to explain all ofthem in detail. they are just kind of there to liven things up. this is an aster visible nearir image of washingtontaken on june 1, 2001. aster is one of the instruments on nasa's terra satellite which was launched about three yearsago now, and i just thought, we are here in washington, i might as well show you washington.statistical challenges in the production and analysis of remote sensing earth science data atthe jet propulsion laboratory29statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.here is the outline for my talk. it is pretty simple. i just want to kind of go through some observations i havemade in my five years of various experiences working at jpl. i should tell you that i work in the earth and spacesciences division, which is a little bit unusual. most of the people who do my kind of work are up in the machinelearning group. i have the benefit, actually, of working directly with the scientists who face these problems, and i thinkthat is a real advantage, because i get to hear them talk about their problems, kind of on a daily basis, what it is theyneed to do.the image on the left there is fake. it was put together before any of the data that šwell, except for the el niñored blob therešbefore any of the data that it is trying to depict was actually collected. the el niño stuff, i don't knowwhere that comes from, exactly which satellite, but the other things are sort of designed to show what sorts of thingsthe earth observing system would eventually provide. this is kind of a dream, which is a global picture, easilyvisualized, of what the world is doing right now.anyway, what i was going to do is just run through what i see as the major statistical challenges in what we do atjpl, and then make some recommendations for how i think the statistics community could become more involved inwhat we do. i asked doug what he thought would make a good talk and he said, well, some recommendations for howwe could become more involved would be good. statisticians are curiously absent from the scene at nasa right now.i think part of the reason for that is that we are pretty much perceived as not being practical enough to contribute, andthat is something i want to come back to later.so, the earth observing system program is a longterm program to study thestatistical challenges in the production and analysis of remote sensing earth science data atthe jet propulsion laboratory30statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.earth's climate system. what that means is looking at the atmosphere, the oceans, the biosphere, and looking at it all asone integrated system, and studying the feedbacks that are involved.the upper graphic there is kind of a little bit of an illustration of why clouds are important in this system, and it isbecause all the energy the earth gets is from the sun. the question is, how is that radiation budget working out for us?some of that radiation is reflected back out into space. some of it gets all the way through to the ground, getsabsorbed, goes to power everything we do here, create fossil fueos and what not. that is really one of the majorthings we are trying to study. so, a very, very important question that we are trying to answer is, what are the radiativeeffects of clouds, and that necessitates knowing where the clouds are and how they are spatially distributed, how theyare changing over time. there are also what we call aerosols in the atmosphere, some of which are manmadešpollution, for examplešothers of which are natural, like forest fire smoke, and these things, too, have an impact on theradiative balance of the earth.the bottom image there is from an instrument called ceres, which i will mention again a little bit later on,which is a global map of thešwhat does it say, outgoing long wave flux of the earth, i guess september 30, 2001.anyway, we have lots of data at nasa. i am kind of amused by the question of, what is a massive data set. iknow that some people don't agree with this but, in my view, a massive data set depends on who you are and what yourcomputational resources are. you will understand why i have that perspective when i get on with what i am about tosay about what our job is at nasa. these data that we collect, we are actually in the data production business atnasa, above and beyond everything eose. we do participate in the analysis of the data that we collect, as you canimagine, but our primary responsibility is to build and fly instruments to collect data to study climate change.so, we are in the business of providing these data to the community. the community is a very diverse group ofindividuals with lots of different interests, lots of different opinions about what assumptions are valid, and lots ofdifferent resources. our users range from university or college researchers with desk top computers to people likenoaa, for example, as we just heard, who use some of our data. so, it is a real challenge to try to design a onesizefitsall sort of way of producing and distributing data that can satisfy everybody's needs.one of the things i wanted to mention specifically was that the eos program is a longterm program that involvesa number of different satellites and instruments. one of the prime intentions of the eos program is that these datawere supposed to be used synergistically. we were supposed to be able to combine data across instruments and acrossplatforms, and as yet, we haven't really done that, and we don't know of anybody who has, largely because of the hugedata volumes we are getting and the very complicated way in which the data are collected.statistical challenges in the production and analysis of remote sensing earth science data atthe jet propulsion laboratory31statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.so, these are nasa's 23 strategic questions. i don't know how easy that is to read from where you sit. nasa hasformulated these questions as a way of concretizing what sorts of problems we want to address with the data. i cancertainly refer you to the web page where this came from and let you know that i am not going to dwell on it or readthem all, but you get kind of an idea for what kind of the big picture questions are.so, the earth observing system, right now it has a collection of satellites involved. two of them are actually inorbit now. the terra satellite was launched on december 18, 1999, from vandenberg air force base in california. itcarries five instruments: misr, the multiangle imaging spectroradiometer; modis, the moderate resolutionimaging spectrometer; aster, which i think is the advanced spaceborne thermal emission radiometeršyou get tobe really good as acronyms when you work at nasa; ceres, clouds and the earth's radiant energy system; andmopitt, measurements of pollution in the troposphere. the reason misr is in red there is because that is one of theprojects that i work on, so i have drawn many of my examples from what i know about misr.nasa is also very good at making pictures and doing animations and doing pr. this is a depiction of the terrasatellite in orbit, and the instruments that it carries, doing what they do. if you have heard me talk before, which someof you have, you will recognize misr as the multicolored beams stretching out forward and aft along the direction offlight. this depicts the fact that misr collects data. so, misr looks down at the earth at nine angles and fourwavelengths and has a swath width of about 300 kilometers or so. the terra satellite is in a polar orbit. so, what we dois, westatistical challenges in the production and analysis of remote sensing earth science data atthe jet propulsion laboratory32statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.successively see the same spot on the ground at nine angles and four wavelengths simultaneously. so, we have 36channels' worth of data.on the other instruments onboard are modis, which has a much wider swath width and, therefore, gets a lotmore coverage. misr, with its skinny little swath width, gets global coverage about every nine days. modis, with itsmuch wider swath width, gets global coverage every day. the other instrument, ceres, was the red and yellow onesgoing back and forth like this in the back. i am not sure what its coverage is, nor am i sure about mopitt. aster iswhat they call the zoom lens of the terra platform, because it only looks at specific spots when it is told to do so, and ithas very high resolution, about a meter, i think. so, i wanted to put that up. the otheršnow i know i had better waitwith this until i finish describing what i want to say.the second eos satellite was launched on may 4, 2002. that is called the eosaqua. it carries four instruments.you will notice some of the names appear again. modis and ceres, it has a modis and ceres as well. it also hasthe airs instrument, which is the other project that i work on at jpl, which is the atmospheric infrared sounder,which looks down at earth at single view angle, but 2,378 spectral bands, and has a spatial resolution of about 15kilometers. misr was about 1 kilometer resolution. so, that is how i knew what ceres looked like, from looking atthat.then we have airs, amsu and hsb, which are actually bundled together in a single package, and are processedtogether at jpl. the spinning thing on the top is amsr/e, which is going to come up. there is modis. you get somesort of relative idea of what the swath widths are, the relative widths, from the animation. i don't know what amsr/edoes, to tell you the truth.there is way too much to know at nasa. it took me about four years before i could actually read a documentwithout having to look up every acronym that i came across and therefore kind of understand what was going on. i likethis, because you are getting bathed in the glow of the data here. i think that is kind of nice. choking on it is more likeit, as i said.[question off microphone from audience.]it depends on which instrument you are talking about. for misr, i can tell you it is about 75 gigabytes a day, andfor airs it is about 28.2 gigabytes a day. it is a little hard to say how big the data are, because the data are processedon successive levels. so, if you quote a big huge number, you are really talking about the same data in different forms.statistical challenges in the production and analysis of remote sensing earth science data atthe jet propulsion laboratory33statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.pretty much, any way you slice it, it is a lot. i wasn't going to concentrate on the ﬁgee, whiz, how big is it?ﬂstatistics, but one of the things people like to talk about is how, in the first six months of operation, the terra satellitedoubled all of nasa's holding from the time that it began. so, there is a lot of data. at the misr science teammeeting this week, we had the people from the data distribution center come and talk. they told us that we now have33 terabytesš33 terabytes is 11 percent of what we have in storage now for misr, and that is just one instrument onone platform.let me say a few things about what happens to the data when we get it. it comes zipping down from the satellitesand gets beamed around, and finally ends up at something called a data processing center called a daac, which is adistributive active archive center which, i think ed pointed out, is a big oxymoron, all by itself, and it gets processed.the data, as it arrives at the daac, is called level 0 data. it is just raw and uncalibrated from the spacecraft. itgoes through a series of processing steps called level 1 processing, which geolocates and calibrates the data and yieldsfor you a data product called level 1b2 in the terminology of things, which you can think of conceptually as a greatbig data set that has a row for every spatial scene on the ground, which would be 1 kilometer for misr and 15kilometers for airs, and a column for each observed channel, 36 columns for misr and 2,378 columns for airs.then comes the good part. so, that level 1 data is then pumped through what we call level 2 processingalgorithms. john alluded to the word retrieval, which was a mysterious term to me for quite some time. that is wherewe retrieve the geophysical quantities that, in theory, produce those radiances. so, level 2 data products are derivedgeophysical quantities that will form the basis for answering those 23 questions.because the data are so large, we have an obligation to provide them in a form that is a little bit easier to use, andthat is the socalled level 3 stage of processing, where we produce global gridded summaries of the data on a monthlyor a daily or a weekly or whatever basis. this is supposed to satisfy the needs of people who can't handle 75 gigabytesof data a day, and make the analysis a little bit easier.level 4 is sometimes talked about, and it is the analysis stage, where you put the input into a climate model anduse it to actually generate something.in my mind, i make a distinction betweenšlevels 1, 2 and 3 are what i call data production, and level 4 is thedata analysis stuff, and i think it is important to make that distinction.statistical challenges in the production and analysis of remote sensing earth science data atthe jet propulsion laboratory34statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.this is a granule map for airs. i said the data come down in chunks. every day we get 240 chunks of airs data.each granule is an array of 90 prints across and 135 footprints along for airs, and there are 2,378 radianceobservations for that.i just wanted to put that up there so you get some idea, if you wanted to order data, and you wanted to order awhole world's worth of data for one day, you would have to order 240 files that look like this. if you wanted a specificspot, you would have to figure out which granule it is in. this changes every day. for airs, a granule is defined as sixminutes' worth of data. one of the problems that we have is that each instrument defines its granule its own way.modis, for example, describes its granule as five minutes' worth of data. the naming conventions for the files are noteasy to figure out and they are not standardized. the sampling grids, the grids at which the data are collected, aredifferent for every instrument. so, you are looking at a big, big problem, if you want to compare data acrossinstruments.now, i wanted to mention just a few problems that i personally have encountered in working with the folks at jplon looking at where statistics and data analysis fit in. there is a tremendous amount of statistics that goes on at jpl,and data analysis. everybody does data analysis at jpl, and nobody is a statistician, except for me.one problem we had was with airs calibration data. the 2,378 channels are broken down into what they call 17different modules. channels go into the same modules because they share a certain amount of electronics. thechannels are supposed to be independent of one another.statistical challenges in the production and analysis of remote sensing earth science data atthe jet propulsion laboratory35statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.these are covariance matrices for eight of the modules. they asked me to try to determine whether or not themeasurements from these channels were, in fact, independent. to a lot of people at jpl, correlation meansindependence. zero correlation would mean independence. never mind the fact that the data don't really look verygaussian when you try to look at them, but these are just a couple of the covariance matrices that we generated sort ofa cohort so you could look at them quickly. it looks like we are doing okay on the last two, and then things get a littledicier as you go back, as you go down to numberšthis one here.the previous speaker alluded to doing retrievals and doing forward models. the calibration issue i think of as alevel 1 issue. it is something that you do at level 1 processing.this is an example of a level 2 problem. we collect from misr, for example, these radiances from nine anglesand four wavelengths. you see here six of the channels, rgb nadir and rgb 70 degree forward, and the socalledoptical depth retrieval that comes from it. as john said, the way this is done almost across the board is by matching theobserved radiances to radiances that are predicted, under the assumption that certain conditions are true. then,wherever you find the best matches, then, aha, that must be it; that is what we are going to call the truth.this is okay, i guess. it seems like a very rich area for statisticians to help improve how that is done. thethresholds for when they say something fits or doesn't fit are pretty much ad hoc, and could benefit from some goodprincipled statistical thinking.the other thing is in how they characterize the uncertainties associated with these retrievals. it has nothing to dowith variance at all. it has more to do with how many of the candidate models fit, how close they fit. for example, iflots of the models fit, we are pretty uncertain. if none of the models fit, we are even more uncertain, and if just one ofthe models fits well, then they say we are pretty certain. the uncertainty characterization tends to be certain, not verycertain, not certain at all, that sort of thing. it is qualitative, rather than quantitative.statistical challenges in the production and analysis of remote sensing earth science data atthe jet propulsion laboratory36statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.finally, the third area that i wanted to mention was the creation of level 3 products, which is how i got involvedand what my responsibilities on misr and airs are, to design level 3 products. level 3 products tend to be maps ofaverages and standard deviations. you take all the data for the period of time over which you are summarizing, andyou chop it up into little 1 degree by 1 degree bins. if you are creating a monthly summary, then you produce maps ofeach of the quantities you are interested in, with a mean value in each grid cell, and then maybe another map with thestandard deviation in each grid cell. if you are really clever, maybe you make some maps of the correlations orcovariances to go along with it.i think the natural reaction of a statistician is, oh, that is awful. you know, you are throwing away most of thedata there. it begins to make more sense when you stop to think about the operational problems that go into doing thesethings. doing things like density estimates, fancy stuff is just completely out of the question because of the processing.the processing has to keep up, basically, so it has to be fast.i will just plug my own thing here and something i have worked with ed wegman a little bit about. what ipropose to do for both misr and airs is to create a level 3 product that puts what i call a quantized data product.instead of simply providing a mean of standard deviation in each grid cell, we provide basically the results of aclustering algorithm.you have a number of representative vectors and associated weights, which might be the numbers of original datapoints represented by each of those representatives, and an error measure, which might be the withincluster meansquared error, and provide this product as a quantitative level 3 product that retains more of the distributionalinformation about the data than just a simple mean standard deviation would. in particular, it would retain some of theinformation about outliers which, for science analyses, tend to be among the most important things and the things youdon't want to smooth out and throw away.what this image here is, is just a map of the relative error in one of the products that i created to show at theamerican geophysical union last week. the original data set from which this image was created was about 550megabytes, and the compressed or quantized product was about 60 kilobytes. so, it is about a 10fold reduction in datasize, and you suffer pretty much, at worst, about a 7 percent error in the data, as measured by mean squared errorrelative to the average magnitude of the data within each grid cell.so, that is pretty good and it is quite good for a lot of applications. sort of the problem here is how do you tellpeople ahead of time whether it is good enough for theirstatistical challenges in the production and analysis of remote sensing earth science data atthe jet propulsion laboratory37statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.application, because you don't know what their application is and, of course, if that is good enough or not depends onwhat their application is.i wanted to show one other thing here, which is this little zippy animation. so, this is an animation of the aquaorbit. you can see how it goes. we are in a polar orbit. that little skinny red strip there is not too far off of what amisr swath would look like. an airs swath would be considerably wider than that. you see that what is going onthere, is that as you go around the earth is, of course, turning underneath you. so, how do you make a global summaryof data like that.someone who has thought about that, to some degree, is noel cressey, who has developed some techniques thatwe have experimented with a little bit, for creating kind of a level 3 product that would sort of take account of spatialand temporal dependencies, in order to produce kind of a monthly summary of these data, that takes are of sort of theinterpolation between swaths and over time. that is a big problem for us, too. i personally put that in the realm of thedata analysis rather than the data production.so, data analysis, i will go through this quick because i want to get to the end here. understanding what is in thedata is a necessary precursor for doing anything with it, for inference. the vast majority of what is done with eos datathese days is exploratory and descriptive, because we are still just trying to understand what is in it. inference is justsomething that is going to have to wait a little while. so, there are lots of opportunities for descriptive type techniquesthat need to be brought to bear on these data.statistical challenges in the production and analysis of remote sensing earth science data atthe jet propulsion laboratory38statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.i am going to just zip along. these are three areas where i think we could really benefit from the help of thestatistics community, which is multivariate visualization, particularly for data sets where you want to preserve thespatial and temporal context of the data.in my view, the real problem here is how do you visualize features of joint distributions of very highlymultivariate data as they evolve as you move around in space and time.data mining and analysis of data sets, it is pretty obvious we need help with that. no one can look at all this data.so, we are doing some things. i like to think of the level 3 products as kind of a first stab at that.finally, data fusion, which i was unable to find too much literature by statisticians on that, and that is reallyimportant. that is a big problem for us. we want to be able to combine data from different instruments on the samesatellite, from different satellites. we need to be able to combine information from ground sources with the satellitedata that we get in order to validate it. so, if anybody has any good ideas about that, or would like to work on that, wewould be very happy to have you help us.as far as analysis is concerned, i wanted to just mention a couple of examples.ed wegman has been real good to us and put together a new incarnation of his image tours ideas. i don't knowhow well you can see that. it is a little difficult to see here, but i will just run that. this was ed's answer to our problemabout how to visualize 36 channels' worth of information while retaining a spatial context.what is going on here is, we have 36 grayscale images, each representing a different channel's worth of data. it issuccessively producing the same linear combination as shown by the little thing down in the corner here, of the 36channels in each pixel of the data, and i will refer you to ed to explain that a little more carefully. the animationdoesn't run very long, and it is hard to see what is going on there, but what you hopefully noticed there was that certainfeatures pop in and out of view as you run that thing. it actually turned out to be pretty useful for finding features thatyou didn't otherwise know about.now, if you already knew those features were there, you could go straight to the image where it was most easy tosee, or to the combination of images where it was most easy to see. if you don't know they are there, which we don'tknow anything ahead of time about this, then you need something that is going to help you look at a lot of data quicklyand find these things, and this was very useful for that. we are still working withstatistical challenges in the production and analysis of remote sensing earth science data atthe jet propulsion laboratory39statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.this. we need to do more work with it in order to be able to help the scientists interpret what it is showing us.a second thing that we are heavily engaged in is collaborating with the machine learning systems people at jpl todo data mining, particularly active learning methods and tools. we got some internal money from nasa to pursuethis, and we are looking forward to eventually being able to show it at the jsm interface meeting.finally, we were able to suck bin yu in. i met bin at a workshop at msri, and she showed some interest in thedata. so, we put her up to a particularly vexing problem in the analyses of these data, which is how to detect thinclouds over snow and ice with misr data. you know, you are looking at a white object over a white background. thatis difficult to see. she was down in pasadena yesterday, gave a talk to our science team, and has some very nice resultsfor us, and we are looking forward to hearing more from her. she has a student working on the problem, actually, andthat has been great. now we have kind of got him learning how to order data, and that looks good for us because itmeans somebody is using our data, and it is good for him because it is a neat problem to work on.my general things i want to sayšthis (by the way) is the rgb nadir image of the previous movie here. i mighthave called this, like i said, a shameless cry for help, because nasa has so much wonderful data, whether you areinterested in massive data set or whatever particular area of analysis you are interested in, there is a wonderful nasadata set that would be really interesting for you, i am sure.the great prohibition, i think, has been that a lot of statisticians think of practical constraints as kind of a detailthat isn't really something they are interested in. what we really need people to do is to devote the time tounderstanding the problem context and the practical restrictions on the analyses, and to accept those as importantresearch problems in their own right. you know, it is not enough to think of a great way of summarizing data oranalyzing data. it has got to be something that can be done, or they won't pay any attention to us.thanks to ben and ed and a couple other people, we are moving in that direction. so, doug said to bring somespecific suggestions for how we could remedy the situation, and these are the ones that i came up with.statistical challenges in the production and analysis of remote sensing earth science data atthe jet propulsion laboratory40statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.the first one is that i would really love to see asa broker relationships between statisticians at colleges anduniversities and local nasa centers. like i said, the nasa centers are just a tremendous source of really interestingdata sets for teaching and research, and it would be great if we could get those into the hands of people who couldactually do something with them.we would love to see asa organize a workshop specifically devoted to statistical applications and problems ingeoscience, earth science, and remote sensing. doug and i were just at the american geophysical union meeting earlythis week, where we had a session on model testing and validation in the geosciences that went very well. it was very,very well received, very well attended. ed, doug and di cook spoke last year at the agu, and they are very, veryhappy to have us, and we would like to push forward some more formal relationship with the agu. in fact, theysuggested us having a committee and, they don't have any money, but if we could somehow find a way to come upwith some money to send young researchers and students to the agu to show their work, that would really be great,because that is really how we are going to inject ourselves into that community.also, i would like to see geoscience get a little bit higher profile at jsm. sometimes we have an occasionalsession on it, but it is nothing likešyou know, bioinformatics you see all the time. it would be nice to have a sessiondevoted to just bringing the problems to the statistics community, maybe not the solutions, but just tell us what theproblems are.this was ralph's suggestion, this last one, speaking like a strapped nasa researcher like he is. we would love tosee a funding program to fund work that is directly relevant to nasa's problems. nasa has not traditionally fundedstatistics, partly because of the problem with being practical and doing things very useful for missions. we think that,if we could get some seed money to prove how useful we could be, that that funding would then come later.so, those are the suggestions. i hope that we can make some of them ring true, and thank you.statistical challenges in the production and analysis of remote sensing earth science data atthe jet propulsion laboratory41statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.ralph milliffglobal and regional surface wind field inferences fromspaceborne scatterometer datatranscript of presentationtechnical papertitle pagepresentation summaryfigure 1figure 2figure 3table 1biosketch: ralph milliff is a research scientist at the colorado research associates division of northwestresearch associates. his expertise is in numerical modeling of the ocean and atmosphere and in the relation of airseadynamics to climate. he was an ocean modeling postdoctoral fellow at ncar (1989œ1991), and a staff scientist thereuntil 2001. dr. milliff has served as a member of the nasa ocean vector winds science team for the nscat andqscat missions (1991present). his current research involves the application of global surface vector wind datasetsof studies of upper ocean mixing and the ocean's general circulation, the maddenjulian oscillation, and thequasistationary waves of the southern hemisphere. in addition, dr. milliff is adapting methods of bayesianhierarchical models from probability and statistics to problems of airsea interaction.global and regional surface wind field inferences from spaceborne scatterometer data42statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.transcript of presentationmr. nychka: our next speaker is ralph milliff from colorado research associates in boulder, colorado.mr. milliff: i, too, would like to thank doug for inviting me. the work that i am going to talk about beganwith collaborations in the geophysical statistics project at the national center for atmospheric research, of whichdoug is the director. my earliest collaborations, and ongoing, are with mark berliner who is now at ohio statešhewas a prior director of gsdšand chris wikle, who was then a postdoc at gsd. also, in addition to doug and chrisand mark, i am happy to acknowledge tim hoar, who is the staff scientist for the statistics project, and jan morzel.sustaining research support has come from the nasa earth science enterprise ocean vector wind science team,and i acknowledge and appreciate that very much. since there is a jpl person in the audience, i am obliged to show apicture of their instrument. this is what the people in the trade call eye candy. it is for you to look at. it isn't really thereal system, but the data set that underlies the massive data stream i am going to talk about today is the surface windsover the global ocean. these all have begun to explode in volume and precision since about 1991 when, within theearthobserving era of satellite data sets, the first global wind data set began with the european space agency mission,ers1 and 2.before i tell you what a scatterometer is and how it works, i should convince you a little bit that the global oceansurface wind field is a worthwhile field to measure. the surface winds transfer momentum between the atmosphereand ocean. they modulate the transfer of heat and material properties, like carbon dioxide and liquid water. theseobviously have important implications for inferences on climate processes and the rates of climate change, when takenin a global perspective. on shorter time scales, the surface winds and these same exchanges are very important inpredicting weather. so, a scatterometer is a system that continually emits an active microwave pulse at the ocean'ssurface, where it is backscattered by capillary waves, and the backscatter signal is detected by the same platform inspace, and related to a surface wind speed. so, this is a retrieval algorithm, of the kind that john bates so cleanlydescribed in his presentation.we are retrieving the surface wind from what the surface wind has done to ripple the surface, and backscatter apulse of radiation that we know very well its properties. the little waves that backscatter radiation from ascatterometer are called cat's paws by sailors. they are the little ripples that form on the surface when a puff of windsheers the surface of the ocean.because we know the polarization frequency, the angles of incidence of the emitted pulse very well, and thebackscattered pulse as well, we use several returns to fit model functions for separately the wind speed and winddirection, and retrieve the vector wind. the returns are aggregated over what we call wind vector cells. so, that isgoing to be my pixel. instead of the radiance coming out of a particular patch or footprint on the surface of the earth, iam looking at backscattered returns over an area. so, they are aggregated over an area.in terms of data volume, the european system that i mentioned first was launched in 1991. it is the longestlivedscatterometer system on our record. it continued until 2000. this is a 12hour coverage of the globe. within eachswath there in black, thereglobal and regional surface wind field inferences from spaceborne scatterometer data43statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.are 70kilometer resolution cells that overlap each other. so, the wind vector cell in the ers system was 70 kilometers,and we got about 41% of global coverage in 24 hours.in 1996, nasa launched the nasa scatterometer, or nscat. it about doubled the precision and the volume ofthe surface vector wind data set. the data organization in this case is a 25kilometer resolution wind vector cells in twoswaths that orient along each side of the satellite ground track. so, these are the polar orbits for a 12hour coverage.you can see the gap at nadir in each swath, and then two swaths on either side of the satellite.the nscat system came to an abrupt end when the solar collector on the satellite bus failed dramatically in 1997.so, we had about nine months worth of data. in response to its failure, nasa launched what is called quickscat. itwas a quickscatterometer. it has an 1,800 kilometer swath, so about 18 degrees longitude, with 25kilometer windvector cell resolution. this is the 12hour coverage.now, we are seeing 92 percent of the globe every 24 hours. as of tonight, at 8:30, a second sea winds systemšsea winds is the scatterometer aboard the quickscat satellite, a second sea winds system will launch aboard ajapanese satellite, hopefully, and we will have, for the first time, synoptic coverage of the surface wind field of theglobal ocean every day. this will be the 12hour coverage from two sea wind systems. you can see that, in 12 hours,there are only very few gaps in the coverage of the surface wind fields.when we started to think about this problem, there were very large gaps, and this is one of the problems that amybrought up in her talk just a minute ago. so, our first statistical model was how to fill those gaps with physicallysensible surface winds. well, what do i mean by physically sensible surface winds? one property of the surface windfield that has emerged from the scatterometer data set, an average property that we can hang our hat on as physicistsand use statistical techniques to drive and time our interpolations is the spectral properties in wave number space. ifyou can permit me, i'll put the whole slide on this way. so, along what should be the ordinate, we have power spectraldensity or kinetic energy alongšthe abscissa is the spatial wave number. the spatial scales that correspond to thosewave numbers are listed on the top here.what we observe in the surface wind field for our planet is that they obey an approximate power law. there isalmost a constant slope in wave number space for the kinetic energy. that isn't the case, if you look at the other curveson this picture, for surface winds that come from weather center models. this is the climate model and these areforecast models.they depart from an approximate power law behavior on spatial scales much coarser than the grid resolution andnumerical models that generate the weather to begin with. so, we have a spatial constraint now, to use in ourinterpolation. these spectra are for a box in the north pacific ocean, averaged over the calendar year 2000. there areinteresting relations to twodimensional and threedimensional theoretical turbulence theories that also make this anappealing result, but they are not really relevant to this talk today.what we do notice now is that this spectral slope, this approximate power law, the slope of that spectrum has aspatial and temporal distribution. it is shallowest in the tropics, where convective events supply energy at small scales,and they propagate upscale in an inverse cascade to the larger scale. they are steeper as you go to higherglobal and regional surface wind field inferences from spaceborne scatterometer data44statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.latitudes. the western pacific and the indian oceans are the shallowest. typical slopesš this is for the zonal windšthere is a similar relationship with the mariana winds, and there is an annual march in the slope of these spectra. itbeginsšit is shallowest in the earliest part of the year and steepens slightly in the tropics and a less evident annualmarch in the midlatitude storm track regions.this pattern is repeatable. we can say that now because we have about three years. that was the picture for 2000.this is the picture for 2001. again, we see shallow slopes in the western pacific, shallowest slopes early in the year.we use these regional and seasonal properties of the surface wind field to perform that interpolation problem that imentioned. we need to account for this wave number deficit in the weather center winds. what the weather centerwinds have going for them is that they are available four times a day everywhere.the satellite isn't true. it drops out in rain, and the satellite has a nonparametric sampling scheme that has to dowith its polar orbit configuration. what we did was use a multirevolution wave length procedure to blend wavenumber deficient surface analysis of the surface wind four times a day, with the available scatterometer data. theconstraint and the reason why we used the wavelets was because wavelets have a multiresolution property that allowsyou to specify the slope of a fractal process. the slopes we were going to use, obviously, are those spectral slopes thatdistribute with space and time over the globe, as observed by the scatterometer.so, with an eight degree square on the globe every day, we collect the spectral slope over a 30degreelongspectrum, and store it, and use that, and sample from the log spike distribution based on that collection, to augment thewave number deficient weather center winds, whenever we don't have a satellite observation. this is an example ofthat blending procedure.the field i am looking at now is a scalar. it is the wind stress curls. this is the derivative of the two componentsof the winds, the eastwest component and the northsouth component. it is going to be noisy, but it is scalar. so, i canshow the effect of our blending technique, and it also points out some important meteorological features, that are abenefit of this particular procedure. wind stress curl extrema are positive in the northern hemisphere, in the region ofatmospheric cyclones. these are storms in the storm track regions of the pacific and atlantic oceans.associated with these storms are frontal systems, and these are almost washed out in the weather center products.this is the wind stress curl from the national centers for environmental prediction on a given day in 1996. thisparticular blending is for the nscat system. we do this on a regular basis for quickscat, and that data is availablefrom the data archive system from ncar. overlay the nscat swaths in the middle panel. that really doesn't show upwell. actually, i think this is in the abstract as well.the swaths from the satellite, all you can see is that the wave number properties in the satellite data are muchricher at high wave numbers than they arešdue to blow up in the north atlantic. so, here is that cyclone in the northatlantic. you are looking at it from the bottom of the ocean. here it is from the top. the frontal system is here inyellow. here are the overlying scatterometer plots. you can see that the scatterometer detects the very sharp spatialfeatures of the front, and the highamplitude wind stress curl that occurs there when it crosses. the blending procedure,because it is a spatial model, can't keep track of the propagation of this system. the spacetime model will,global and regional surface wind field inferences from spaceborne scatterometer data45statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.and we are working on that. because it is a spatial model, within the eight degree squares in the gap region, highamplitude wind stress curls, commensurate with the wind stress curl that occurs in the front, is distributed. that isimportant for driving a global ocean model. in fact, in 1999, we wrote a paper that showed that the ocean models thatwe used for climate forecasting, for climate stimulations, were very sensitive to this change in the surface wind fields.so, you drove a model with the weather center winds, and we are not getting this high wave number forcing. thishigh wave number forcing turns out to be important. this is the annual mean response after a spin of three degreesglobal ocean model. here is the difference with respect to a calculation that was done with the weather center model.so, it is the blended winds minus the weather center winds.you can see, up to 7 meters per second, 3 1/2 knot differences in the currents of the upper oceans. moreimportant, there is a big divergence pattern in the tropics. this is the region of the el niño southern oscillationsignal. so, the divergences there have big implications for cold water at the surface, and the propagation or not of an elniño signal into the eastern pacific.so, there is a reason to do this for climate. i am going to shift now to regional applications, and perhaps a littlemore deep statistical modeling. this comes from my learning at the feet of berliner and wikle.this is an avhr, a radiometer image. it is basically a temperature in the infrared and visible regions of thespectrum, of the labrador sea. this is the coast of labrador, here is the southwestern coast of greenland. this is oneof a few regions in a world ocean where the socalled ocean deep convection process occurs, and this is critical toclimate. this is a place where the properties of the surface atmosphere are subducted to great depth and for very longtimes into the ocean. this is the socalled thermal haline circulation, or the global conveyor belt, that you might haveheard of.the labrador sea is one place. the eastern mediterranean is another, and a few points around antarctica are theother places where the atmosphere and the deep ocean are in contact, very briefly, during these very brief oceanconvection events, and they drive the redistribution of heat on the planet. the ocean part of that redistribution happenshere. so, those convective triggers are associated with weather patterns of the kind we see here. this is called a polarlow. the lowpressure system is centered around the middle of the basin.the winds that are associated with this, the surface winds that are associated with this signal, drag dry, coldcontinental air across the relatively warm sea surface and exchange those properties that i talked about in the beginningof the talkšheat, moisture and momentumšand superdensify the surface ocean and provide this plunging physicalmechanism. within an hour of this avhrr image, the nasa scatterometer, or nscat, a fragment of that swathoccurred, and you can see the signature of the polar low here. so, it is understanding these convective triggers, andcertainly the surface wind field associated with them, is sort of the target science problem that we dealt with.this is another polar low signal in the labrador sea. this sets up a bayesian hierarchical model that we use toretrieve a uniform surface wind field with estimates of uncertainty at each grid point, from the scatterometer data.so, we are going to build a posterior distribution for the wind at the diamond grid pointsšthat is our target gridšgiven the scatterometer data and a prior, based on aglobal and regional surface wind field inferences from spaceborne scatterometer data46statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.physical balance. this is the work of andy royal, who was another postdoc at gsp, and mark berliner and myself.so, i think, as a physicist, about bayesian models in stages. the first is the data model stage, or what you would callthe likelihood, and i think this is a very natural entity for satellite data. it is wrong to think of satellite data only asmoorings for balloon traces that happen to be right next to each other in space. instead, they inform probabilitydistributions. so, probabilistic models are an essential new technique, i think, that we need a great deal of help with inthe geophysical field. i think our sort of preliminary pilot studies show that there is a great deal of play here. thelikelihood model gives us a distribution for the data that naturally arises from measurement error models that comefrom every satellite mission that is ever launched.we do what are called calibration validation studies. calibration validation studies will inform the likelihooddistribution to excellent precision and allow the satellite datašthe volume of itšto actually speak to the posterior veryclearly. in the prior process model, we used heritage and geophysical fluid dynamics that go back for generations. wedeveloped essential dynamical descriptions of processes of interest, and there is a whole branch of atmospheric andoceanographic science to do just this. we can blend these two heritages, the observational side of our field and theprocess model side of our field, to develop very useful posterior distributions. of course, the analytic derivation of theposterior is out of range because the normalizer is intractable. so, we rely on the advances in your field and gibsampling and mark up chain monte carlo algorithms.the data statement for this particular problemšbayesian formalism, as i hope i will get to at the end of the talkšis very amenable to multiplatform observation. in fact, we have a prototype model here that uses altimeter andscatterometer for the same problem. the altimeter measures the seasurface height, from which we can infer oceansurface currents. so, what we use as a data statement involves an incidence matrix, and this is another problem thatsatellite data introduced, and that the statistical community can readily address, changes of support.we have a single point observation, perhaps, within the swath of a satellite data. we want to infer somethingabout a process in the grid cell for a model. you people know how to do this, and we are beginning to deal with that.that is the kind of information that goes into this incidence matrix, k. on the other hand, when we are given theabundance of data that comes with the satellite overpass, the incidence matrix need not be very sophisticated, we havefound, and we have simple nearestneighbor algorithms at present that will yield the results that i am about to show.then, as i said before, the measurement error models are the calibration validation studies.for the process model, we use what we call stochastic geostrophy. this is a fundamental balance between thegradient of a pressure field and the implied velocity. because we are on a rotating planet, any gradient in pressure orpotential will initiate a flow from high potential to low potential but, because we are rotating, the resultant flow, whichaccounts for this rotation vector, will be along the lines parallel to the gradient. this is called the geostrophic relation,and we can translate this differential expression for the geostrophic relation into a probabilistic statement.so, for our priors, we say that the zonal wind, given some pressure, some hidden process pressure and variance, isdistributed normally, and the mean of that normal distribution is proportional to the gradient of the pressure, and thevariance is expressedglobal and regional surface wind field inferences from spaceborne scatterometer data47statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.in terms of the covariance of the wind field that we might know about from our observations. the second level, weprescribed a pressure field. the pressure, you know, is a good news/bad news sort of field. it is the reason why wecan't do massively parallel calculations very efficiently in climate and weather forecasting, because it is quasielliptic.so, the perturbations in the pressure from remote places have a very important impact on the pressure at the grid pointof interest. so, it is bad in that sense. it is good in the following sense: since it is quasielliptic, it is relatively smooth,and it is well approximated by harmonic operators.it turns out it is a good thing to hide in a bayesian hierarchical models, because there are analytic expressions forsurface pressure that fit well with meteorological processes, and we have done some regional studies with drifters inthe region that give us space and time scales of variability for the pressure field there. so, we can prescribe a pressureprocess solely in terms of its covariant structure for models of that kind. building that prior distribution, building thedata distribution, using a gibb's sampler, we generate the following posterior mean distribution for the surface winds.there is already an important result here. had the prior dominated the flow, as i told you, should be parallel to theisobar. in fact, the wind, the posterior mean wind here, is crossing isobars, and that means that the satellite data hasspoken. the bayesian formalism requires that we get a distribution for not just the dependent variable of interest in thedeterministic sense, but also all the parameters and the hidden processes. so, in addition to the surface wind, we have aposterior distribution for surface pressure as well.the righthand panel shows what the weather center forecasts for this particular time also, and came up with in adeterministic model. this was a single realization from a forward model. all they came up with is the followingpressure distribution. when we overlay the original satellite data it shows, in fact, they misplaced the lowpressurecenter. so, their region of ocean deep convection triggering would have been in the wrong place and, in fact, theintensity was considerably weaker than it is in the posterior mean distribution from the bayesian hierarchical model.we have done a similar and more sophisticated bayesian hierarchical model to retrieve surface winds in the tropics.in fact, thanks to tim hoar we are providing 50 realizations of the most specifically reasonable surface windsfrom the indian ocean to the dateline, 40 degrees north and 40 degrees south, four times a day. that is going to beavailable, and it will be interesting to see what the geophysical community does with it. i know what i am going to dowith it, but there is a great deal that can be done with 50 realizations in terms of putting error bars on the deduction ofweather and climate processes that we study.typically, for example, john bates mentioned the maddenjulian oscillation. well, what we typically have to doto study the process of the maddenjulian oscillation, which takes about 10 days to propagate across the indian oceanand into the western pacific, is average several events. these events happen every 40 to 50 days when they happen,and then they don't happen for a while. so, the background flow system is completely different in the situations thatyou have to composite, in some sense, to get an idea of what the generic maddenjulian oscillation looks like. now,with 50 realizations of the surface winds for a single maddenjulian oscillation, we have a different concept of whatthe error bars are going to be on in relationships between, for example, surface convergence and propagation of thiswave. that is an aside. the bayesian hierarchical model that describes that wind blending in the tropics has beenpublished in jasa.global and regional surface wind field inferences from spaceborne scatterometer data48statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.there is a wikle et al. (2001) paper that i would refer you to, and also chris wikle's web page. that is one of the30 or 40 recent publications on his page. what i would like to talk about now is the prototype atmosphere ocean modelthat is also set in the bayesian hierarchical context.this is an analog, a probabilistic analog, for the centerpiece tools for climate analysis and climate forecast. peoplewho analyze climate run massivešwe really do mean massive nowšatmosphereocean coupled simulations on thelargest supercomputers that they can find, and they provide a single deterministic realization at the end of the day. ithink that this community can guide well those kinds of calculations, which are very expensive, by building theessential pdf, that whatever formal models of simulation they choose to run have to go through in some mode or someparameter sense. i can talk more about that in the breakout session.what we did was combine the atmosphere model that i have just described for the labrador sea and an oceanmodel with slightly more sophisticated physics for the prior. we separated in the data stage or the likelihood the errorswith respect to the atmospheric process, and the scatterometer data from the errors in the altimeter data from the oceanprocess. so, those were independent. in the process model stage, we simply factored the joint distribution between theatmosphere and ocean processes.in the atmospheric process times an ocean process, we come up with the posterior interests, which are a processfor atmosphere and oceans, all the parameters, given scatterometer and altimeter data. then the horrible normalizer, ofcourse, is the simulation method, which is here. the simulation method is particularly clever, and this was markberliner's design, and i will come to that, i hope, in the end. so, the process model was built on now a dynamicdifferential equation that has proved itself. it is the original ocean model, actually. it is called quasigeostrophy.we have terms for the evolution of the ocean stream function, , nonlinear effects, convection, planetaryvorticity, forcing by the surface wind, bottom friction and internal friction. the first step that a deterministic modelerwould take would be to discretize these on a grid of interest and form matrix operators, and that is done here. this ischanging a differential equation into a difference equation, a very standard technique.you will notice that it is also very markovian. we have matrix expressions operating on the previous timelevelstream functions to give us an estimate of the next timelevel stream function.we also separate out the boundary conditions which, when we jump to probabilistic state, will become aboundary process, and that is a very big issue in geophysical modeling of limited area domains. so, here is the leap toprobablistic ocean stream function, and these operators are modeled directly after their finitedifference counterparts inthe deterministic world. we have the linear operators on the previous time level, the nonlinear operator, surface windstress, boundary process, and we have added a model misfit term. this model misfit term is the only account for modelerror that forward model data simulation systems can make. in contrast, we have distributional parameters, whichmake these random variables in front of every term here.so, we have a termbyterm management of uncertainty, and the uncertainty can interact, in the way that invectionuncertainty should interact with the diffusion uncertainty in the dynamic. along with this come several economies.because theglobal and regional surface wind field inferences from spaceborne scatterometer data49statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.deterministic system is stiff, elliptic, difference equation, they are constrained to take very small time steps that are notrelevant to the physics of interest.it is not important to a polar low, what is happening on 15 second time intervals but, because it is an ellipticsystem, the way it is written in deterministic space, they are constrained to take 15second time steps, and this drives ahuge expense. they are also constrained to take very small spatial steps in their models. in a probabalistic model, weare not so constrained.there must be a constraint that makes physical sense of some kind in the probabilistic world, and this is the sortof theoretical problem that i think this community could pursue and make large contributions. nonetheless, we cantake six hour time steps and three times the grid space in our airsea hierarchical bayesian model. the algorithmš ican't go through it given timešis a clever combination of markov chain monte carlo for the atmosphere, andimportant sampling monte carlo for the ocean. that is atmosphereocean physics in probabilistic space. theimportance weights are the likelihood distributions for the ocean data.so, i build a catalog of forcing from the atmosphere. i go ahead and generate the ocean stream functions thatcome from every member of that catalogue. then, i say the important ones have to do with how theyšthe datadistributions from the ocean sensor, the altimeter oriented.we tested this model in what is called an observing system simulation experiment. we had a truth simulationfrom primitive equations, a more sophisticated physical set, very high resolution, and compared the bayesianhierarchical model, posterior distribution, with that truth simulation over 10 days. first, we had to spin up the truth. so,for a year we forced the box that looked like the labrador sea with this kind of wind, and generated this kind of oceanstream function equivalent in primitive equations.there is a cyclonic eddy in the southwestern corner, and a rim current that is similar to the labrador sea, and thenclosed eddies on the interior of that rim current. then, we idealized the surface forcing of a polar low, and sampled itas thought we had a scatterometer, and sampled the ocean as though we had an altimeter, corrupted those data withproper measurement noise, fed those to our data stages in the bayesian hierarchical models.so, these are days one, three, five and seven of the simulated data. you can see the ocean stream functionevolving underneath the altimeter. the altimeter tracks are here. this is representative of the topec system. thesimulated scatterometer is representative of the sea wind system, and you can see the polar low, which is perfectlycircular and propagating perfectly zonally across this box, sampled, and then depart from the domain.this is a comparison of truth simulation on the left and the bayesian hierarchical model posterior meandistribution on the right, for the same four days, one, three, five and seven of a 10day simulation. i will show you adifference map in a minute, but the main difference is that the bayesian hierarchical model is actually more responsive,in a posterior mean sense, to this polar low, than was the primitive equation model.as a physicist, my conception of statistical models always used to be, yes, they were generally right, but man,they were really sluggish and smooth, and the real detail that i needed wasn't available. well, that seems to be quite theopposite in a posterior mean, let alone the realizations from the posterior distributions.global and regional surface wind field inferences from spaceborne scatterometer data50statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.here are the difference plots, and i draw two columns here. one is the difference for the full bayesianhierarchical model. the other is the difference when i exclude that importance weighting. so, here, on the righthandcolumn, is a bayesian hierarchical model for which we did not supply altimeter data in a separate data stage. you cansee that this allows us to quantify, in a distributional sense, the value added of the altimeter data to the atmosphereocean problem.interestingly, all of the differences isolate with features of interest. so, the cyclone in the southwestern corner is aplace where differences exist and, because we have a posterior distribution, it is a place where uncertainty exists. thisis a map of the standard deviation as a function of space for day seven, day five, day three, day one. notice also thatthe boundary process is emerging as a source of uncertainty, and that is very consistent with the experience in forwardmodeling in limited area domains in the atmosphere and ocean. i am going to skip my last slide, which is a summaryslide for this model that is in the abstract, and get to my conclusions.the regional and global surface wind data sets from space are important. with two sea wind systems, there willbe eight times 105 surface vector wind retrievals every 24 hours. that is a level 2 product. a level 1 product is anorder of magnitude bigger than that. those are the backscatter observations. i would never use a level 3 productš ihate to say thisšbecause level 3 depends very much on what you want to do, and i will build my own level 3products, as i have seen. i have blended winds from the weather center and the satellite.[question off microphone from audience.]mr. milliff: level 3 is for eye candy. it makes for the prettiest slides and animation and things, but you can'tdo science. the problem that polarorbiting and even equatorialorbiting satellites pose for geophysicists is that theydon't appear on regular grids, they don't have uniform spatialšthey don't leave a global field with uniform spatial andtemporal resolution. so, that is a key issue that amy brought up.we have used physical constraints to drive a process to build those uniformly distributed spatially and temporallyvarying grids. multiresolution wavelets to impose this spectral constraints. bayesian hierarchial models exploit themassive remote sensing data sets. i think what i expect to hear, in parts of this meeting, is that we have a problem oftrying to find a needle in a haystack.i think what geophysicists need to say is that, wow, there is a haystack that we can use. never mind the needles.we used to just put the needles out there as a few moorings in a few places. now, in fact, there is a whole probabilitydistribution that needs to be exploited that comes from these satellite data.bayesian hierarchical models are amenable and readily adaptable to multiplatform data. the modern oceanobserving system will involve remote sensors from space and in situ drifting, autonomous systems. the changes ofsupport and distributional interactions of the uncertainty in the signals from those data, i think, are readily handled bythe bayesian hierarchical model approach. there has been a demonstration of air sea interaction through a markovchain monte carloimportant sampling monte carlo linkage. thanks.global and regional surface wind field inferences from spaceborne scatterometer data51statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.global and regional surface wind field inferences given spaceborne scatterometer dataralph f.milliffcolorado research associates (cora) division,northwest research associates (nwra)collaborators:l.mark berlinerohio state universitychristopher k.wikleuniversity of missouridoug nychkatim hoarnational center for atmospheric researchjan morzelcora/nwraresearch support:nasa ese ocean vector winds science teampresentation to:nrc committee for applied and theoretical statisticsworkshop on massive data streams13œ14 december 2002washington, dcglobal and regional surface wind field inferences from spaceborne scatterometer data52statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.global and regional surface wind field inferences from spacebornescatterometer dataralph f.milliffcolorado research associates (cora) division,northwest research associates (nwra)collaborators:l.mark berliner (ohio state university)christopher k.wikle (university of missouri)doug nychka (national center for atmospheric research)tim hoar (national center for atmospheric research)jan morzel (cora)the global ocean surface wind field transfers momentum, and modulates the transfers of heat and materialproperties (e.g. fresh water, co2, etc.), between atmosphere and ocean. momentum inputs create and sustain the winddriven general circulation of the ocean, and heat and fresh water exchanges drive the thermohaline general circulation;both of which have important implications for earth climate. on regional scales, the surface wind field is an indicatorof synoptic variability affecting weather forecasts.the surface wind vector field over the ocean has been observed by active scatterometer systems in space withincreasing precision, coverage, and resolution since 1978. table 1 indicates characteristics of past, existing, andplanned scatterometer missions since sustained earth observing missions began in 1991. in scatterometry, radar pulsesof known frequency and polarizations are directed at the ocean surface where they are scattered by capillary waves.the spaceborne sensor detects the backscatter signal, from several different geometries (e.g. look, azimuth, andincidence angles) and across two polarizations, to return a normalized radar backscatter cross section, or !o. the !o arespatially averaged within socalled wind vector cells (wvc) that form an array spanning the satellite ground trackalong its orbit. for each wvc, a geophysical model function is fit to relate averaged !o to wind speed and direction.the nasa quikscat (qscat) satellite bears the first seawinds scatterometer instrument. the wvc areordered at 25 km resolution across an 1800 km swath, along a polar orbit that is declined 8º. roughly 1,000,000 surfacewind vectors are retrieved over the global ocean in about 14 orbits every 24 hr by qscat. a second seawindsinstrument is planned for launch aboard the adeos2 satellite of the japanese space agency (nasda) on53global and regional surface wind field inferences from spaceborne scatterometer datastatistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.14 december 2002 (i.e. the same day this talk is scheduled for delivery at nrc/cats!). for the first time, tandemscatterometer missions will return true synoptic resolution of the global ocean surface wind field.global and regional applications of surface wind data from scatterometer systems often require regularly griddedsurface vector wind fields with physically consistent treatments for missing data (e.g. due to rain contamination and/orattenuation of the radar signals). computer models for the simulation of the ocean general circulation have been shownto be sensitive to surface wind forcing on diurnal time scales. the implied spacetime requirements do not match wellwith the native organization of surface vector winds from scatterometer systems occurring in swaths from westwardprecessing orbits. a variety of statistical models have been developed to infer global and regional surface vector windfields from scatterometer observations, on regular grids, and at diurnal temporal resolution.blending qscat and weathercenter analysis windsa statistical model has been developed to blend scatterometer surface vector winds with surface wind fields fromweathercenter analyses to create global ocean datasets, 4times per day, at 0.5º resolution (http://dss.ucar.edu/ds744.4). the blending methodology is constrained by an approximate powerlaw relation that is observed, withregional and monthly variability, in wavenumber spectra for surface winds from scatterometer systems. the weathercenter winds are used only in swath gaps and missing data regions with respect to the qscat orbits. they must beaugmented at high wavenumbers to retain the regional and seasonal powerlaw relation that is observed. theaugmentation is implemented in a multiresolution wavelet procedure designed by chin et al (1998).figure 1 depicts three panels of the global wind stress curl field for 24 january 2000 at 1800 utc. the windstress curl is a scalar summary of the surface vector wind field that is useful to illustrate the blending method. the toppanel shows the wind stress curl from the weathercenter analyses field. analysis fields combine the latest forecastfield with relevant surface observations that accrue over the forecast interval (e.g. between initialization andverification times). in the middle panel, the wind stress curl derived in the qscat swaths for this time period aresuperposed on the analysis field. note the higher wavenumber content in the surface wind stress curl from the satelliteobservations. the blending method operates to augment the higher wavenumber content of the weathercenter analysessuch that the54global and regional surface wind field inferences from spaceborne scatterometer datastatistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.blended field is consistent with powerlaw spectral properties observed by the qscat. the third panel shows the windstress curl for the blended field.the blended winds have been used to drive regional and global ocean model simulations. milliff et al. (1999)demonstrated realistic enhancements to the response of a relatively coarseresolution ocean general circulation model(ogcm) to the higherwavenumber winds in the blended product. higher resolution ogcm experiments are inprogress now.bayesian inference for surface winds in the labrador seathe labrador sea is one of a very few locations in the world ocean where surface exchanges of heat, momentumand fresh water can drive the process of ocean deep convection. ocean deep convection can be envisioned as theenergetic downward branch of the socalled global ocean conveyor belt cartoon for the thermohaline generalcirculation that is important in the dynamics of the earth climate. the energetic exchanges at the surface are oftenassociated with polar low synoptic events in the labrador sea.a bayesian statistical model has been designed to exploit the areal coverage of scatterometer observations, andprovide estimates of uncertainty in the surface vector wind inferences for the labrador sea. here, the scatterometersystem is the nasa scatterometer or nscat system that preceded qscat. it has proved convenient to organize thebayesian model components in stages. data model stage distributions are specified almost directly from preciseinformation that naturally arises in the calibration and validation of satellite observing systems. the prior model stage(stochastic geostrophy) invokes a simple autonomous balance between surface pressure (a hidden process in ourmodel) and the surface winds. the posterior distribution for the surface vector winds is obtained from the output of agibbs sampler.an application of the labrador sea model for surface winds will be described at the end of this presentation. thefirst documentation of this model appears in royle et al. (1998).bayesian hierarchical model for surface winds in the tropicsthe bayesian hierarchical model (bhm) methodology is extended in a model for tropical surface winds in theindian and western pacific ocean that derives from chris wikle's postdoctoral work (wikle et al., 2001). here, thedata model stage reflects measurement error distributions for qscat in the tropics as well as for the surface windsfrom the ncep analysis. the prior model stage is prescribed in two parts. for large scales, the length scales55global and regional surface wind field inferences from spaceborne scatterometer datastatistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.and propagation of the leading modes of the equatorial "plane are used. at smaller scales, once again, we invoke awavelet decomposition constrained by the powerlaw behavior for wavenumber spectra in the tropics.a recent implementation of this model generates 50 realizations of the surface wind field, 4times per day, at 50km resolution, for the domain 20° n to 20° s, 40° e to 180° e, for the qscat data record for the calendar year 2000.figure 2 depicts snapshots of five randomly selected realizations for zonal wind and divergence fields for 25december 1999 at 0000 utc. differences are smallest in regions recently sampled by qscat. this implies that theuncertainty in the observations is smaller than the uncertainty in the approximate physics assigned in the prior modelstage.surface convergence in the tropics is a critical field in the analysis of the atmospheric deep convection process.however, single realizations of this field are rarely useful because divergence is an inherently noisy field. theproduction of 50 physically sensible realizations can begin to quantify the spacetime properties of the signal vs. noise.the first use of this dataset will be to diagnose surface convergence patterns associated with the maddenjulianoscillation (mjo) in the regions where the mjo is connected to the surface by atmospheric deep convection.a bayesian hierarchical airsea interaction modelthe bayesian hierarchical model methods extend naturally to multiplatform observations and complex physicalmodels of airsea interactions. berliner et al (2002) demonstrate a prototype airsea interaction bhm for a test casethat mimics polar low propagation in the labrador sea, given both simulated altimeter and scatterometer observations.hierarchical thinking leads to the development of a prior model distribution for the surface ocean streamfunction thatis the product of an ocean given atmosphere model, and a model for the atmosphere. the prior model stage for theatmosphere is a model similar to the labrador sea wind model introduced above.figure 3 compares the evolution of the ocean kinetic energy distribution in the airsea bhm with a case fromwhich all altimeter data have been excluded. the bhm resolutions are 3 times coarser in space and o(1000) timescoarser in temporal resolution with respect to a highresolution ﬁtruthﬂ experiment also shown in the comparison. also,the ﬁtruthﬂ fields are computed in a physical model that incorporates more sophisticated physics than those that formthe basis of the prior model stage in the airsea bhm. implications of this methodology to data assimilation issues incoupled general56global and regional surface wind field inferences from spaceborne scatterometer datastatistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.circulation models will be discussed.referencesberliner, l.m., r.f.milliff and c.k.wikle, 2002: ﬁbayesian hierarchical modelling of airsea interactionﬂ , j. geophys. res., oceans, in press.chin, t.m., r.f.milliff, and w.g.large, 1998: ﬁbasinscale, highwavenumber sea surface wind fields from multiresolution analysis ofscatterometer dataﬂ, j. atmos. ocean. tech., 15, 741œ763.milliff, r.f., m.h.freilich, w.t.liu, r.atlas and w.g.large, 2001: ﬁglobal ocean surface vector wind observations from spaceﬂ, inobserving the oceans in the 2#st century, c.j.koblinsky and n.r.smith (eds.), godae project office, bureau of meteorology,melbourne, 102œ119.milliff, r.f., w.g.large, j.morzel, g.danabasoglu and t.m.chin, 1999: ﬁocean general circulation model sensitivity to forcing fromscatterometer windsﬂ, j. geophys. res., oceans, 104, 11337œ11358.royle, j.a., l.m.berliner, c.k.wikle and r.f.milliff, 1998: ﬁa hierarchical spatial model for constructing wind fields from scatterometerdata in the labrador sea.ﬂ in case studies in bayesian statistics iv, c.gatsonis, r.e.kass, b.carlin, a.cariquiry, a.gelman,i.verdinelli, and m.west (eds.), springerverlag, 367œ381.wikle, c.k., r.f.milliff, d.nychka and l.m.berliner 2001: ﬁspatiotemporal hierarchical bayesian modeling: tropical ocean surface windsﬂ,j. amer. stat. assoc., 96(454), 382œ397.figure captionstable 1. past, present, and planned missions to retrieve global surface vector wind fields from space (from milliffet al., 2001). the table compares surface vector wind accuracies with respect to insitu buoy observations. launchdates for seawinds on adeos2 and windsat on coriolis have slipped to 14 and 15 december 2002, respectively.figure 1. three panel depiction of the statistical blending method for surface winds from scatterometer andweathercenter analyses. panel (a) depicts the wind stress curl for the weathercenter analyses on 24 january 2000 at1800 utc. wind stress curl from qscat swaths within a 12hour window57global and regional surface wind field inferences from spaceborne scatterometer datastatistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.centered on this time are superposed on the weathercenter field in panel (b). panel (c) depicts the wind stress curl forthe blended field. derivative fields such as wind stress curl are particularly sensitive to unrealistic boundaries in theblended winds.figure 2. a bayesian hierarchical model is used to infer surface vector wind fields in the tropical indian andwestern pacific oceans, given surface winds from qscat and the ncep forecast model. five realizations from theposterior distribution for (left) zonal wind and (right) surface divergence are shown for the entire domain on 30january 2001 at 1800 utc. the two panels in the first row are zonal wind and divergence from the first realization.subsequent rows are zonal wind differences and divergence differences with respect to the first realization. thedifferences are for realizations 10, 20, 30, and 40 from a 50 member ensemble of realizations saved from the gibbssampler.figure 3. summary plots for the airsea interaction bayesian hierarchical model (from berliner et al., 2002). thebasin average ocean kinetic energy distributions as functions of time are compared with a single trace (solid) from aﬁtruthﬂ simulation described in the text. the posterior mean vs. time (dashed) is indicated in panel (a) for the full airsea bhm, and in panel (b) for an airsea bhm from which all pseudoaltimeter data have been excluded. panels (cf)compare bhm probability density function estimates at days 1, 3, 5, and 7.58global and regional surface wind field inferences from spaceborne scatterometer datastatistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.59global and regional surface wind field inferences from spaceborne scatterometer datastatistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.60global and regional surface wind field inferences from spaceborne scatterometer datastatistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.61global and regional surface wind field inferences from spaceborne scatterometer datastatistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.missionmeasurementapproachswath (km)daily cov.resolution(km)accuracy(wrtbuoys)url(http://)ers1/2 ami4/91œ1/01cbandscatt.500/41%50 (~70)1.4œ1.7 m/srms spd 20ºrms dir~2 m/s randomcomp.earth.esa.intascat/metopcbandscatt.2×550/68%25 50better than ersesa.int/esa/progs/www.metop.htmlnscat 9/96œ6/97kubandscatt. (fanbeam)2×600/75%(12.5) 25 501.3 m/s (1œ22m/s) spd 17º(dir)1.3 randomcomp.winds.jpl.nasa.gov/missions/nscatseawinds/quickscat7/99œpresentkubandscatt. (dualconical scan)1600/92%(1400)12.5 251.0 m/s (3œ20m/s) spd 25º(dir)0.7 randomcomp.winds.jpl.nasa.gov/missions/quickscatseawinds/adeos2 2/02kubandscatt. (w/uwave rad.)1600/92%(1400)(12.5) 25better thanquickscatwinds.jpl.nasa.gov/missions/seawindswindsat/coriolis3/02duallookpol. rad.1100/~70%25±2 m/s or 20%spd±20°??www.ipo.noaa.gov/windsat.htmlcmis/npoess 2010?singlelookpo. rad.1700/>92%20±2 m/s or 20%spd±20°?? (5œ25m/s)www.ipo.noaa.gov/cmis.html62global and regional surface wind field inferences from spaceborne scatterometer datastatistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.summary1. global and regional surface wind datasets from spaceborne scatterometers are ﬂmassiveﬂ and importantfor climate and weather. applications require: regular grids uniform spatial o(10 km) and temporal o(diurnal) resolution2. blended scatterometer and weathercenter analyses provide global, realistic highwavenumber surfacewinds impose spectral constraints via multiresolution wavelets3. bayesian hierarchical models to exploit massive remote sensing datasets measurement error models from cal/val studies (likelihoods) process models from gfd (priors) advances in mcmc4. tropical winds example (wikle et al. 2001)5. bayesian hierarchical model for airsea interaction (berliner et al 2002) multiplatform data from scatterometer and altimeter stochastic geostrophy (atmos) and quasigeostrophy (ocean) priors mcmc to ismc linkage for posteriors termbyterm uncertainty realistic covariance structures63global and regional surface wind field inferences from spaceborne scatterometer datastatistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.report from breakout groupinstructions for breakout groupsms. kellermc nulty: there are three basic questions, issues, that we would like the subgroups to comeback and report on.first of all, what sort of outstanding challenges do you see relative to the collection of material that was in thesession? in particular there, we heard in all these cases that there are real specific constraints on these problems thathave to be taken into consideration. we can't just assume we get the process infinitely fast, whatever we want.the second thing is, what are the needed collaborations? it is really wonderful today. so far, we are hearing froma whole range of scientists. so, what are the needed collaborations to really make progress on these problems?finally, what are the mechanisms for collaboration? you know, amy, for example, had a whole list ofsuggestions with her talk.so, the three things are the challenges, what are the scientific challenges, what are the needed collaborations, andwhat are some ideas on mechanisms for realizing those collaborations?report from atmospheric and meteorological data breakout groupmr. nychka: the first thing that the reporter has to report is that we could not find another reporter except forme. i am sorry, i was hoping to give someone the opportunity, but everybody shrank from it.so, we tried to keep on track on the three questions. i am sure that the other groups realized how difficult that was.let me first talk about some technical challenges. the basic product you get out of this is a field. it is maybe avariable collected over space and time. there are some just basic statistical problems of how you summarize those interms of probability density functions, if you have multiple samples of those, how you manipulate them, and also dealwith them. also, if you wanted to study, say, like a particular variable under an el niño period versus a la niñaperiod, all those kinds of conditioning issues. so, that is basically, sort of very mainstream spacetime statistics.another important component that came out of this is the whole issue of uncertainty. this is true in general, andthere was quite a bit of discussion about aligning these efforts with the climate change research initiative, which is avery high level kind of organized effort by the u.s. government to study climate. uncertainty measures are animportant part of that, and no surprise that the typical deterministic geophysical community tends to sort of ignorethese, but it is something that needs to be addressed.there was also sort of the sentiment that one limitation is partly people's backgrounds. people use what they arefamiliar with. what they tend to do is limited by the tools that they know. they are sort of reticent to take on newtools. so, you have this sort of vicious circle that you only do things that you know how to do. i think an interestingthing that came out of thisšand let me highlight this as a very interesting technical challenge, and it is one of thesecurious things where, all of a sudden, a massivereport from breakout group64statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.data set no longer becomes very massive. what john was bringing up is that these large satellite records typically havesubstantial nonzero biases, even when you average them. these biases are actually a major component of using these.so, a typical bias would be simply change a satellite platform that is measuring a particular remotely sensed variable,and you can see a level shift or some other artifact. in terms of combining different satellites, you need to address this.these biases need to be addressed empirically as an important problem.the other technical challenge is reducing data. this is another interesting thing about massive data sets, that partof the challenge here is to make them useful. in order to make them useful, you have to have some idea of what theclientele is. we have had some discussion about being careful about that, that you don't want to sort of create somekind of summary of the data and have that not be appropriate for part of the user community. the other thing is,whatever summary is done, the assumptions used to make it should be overt, and also there should be measures ofuncertainty along with it.collaborations, i think for this we didn't talk about this much, because i think they were so obvious. obviously,the collaborators should be people in the geophysical community that actually work and compile this data with thestatisticians.some obvious centers are jpl, ncar, noaašralph, do you volunteer cora as well?audience: sure.mr. nychka: john, ncdc, i am assuming you will accept visitors if they show up.audience: sure will. it is a great place to be in the summer, between the blue ridge and the great smokeys.mr. nychka: okay, so one thing statisticians should realize is that there are these centers of concentrations ofgeophysical scientists, and they are great places to visit. the other collaboration that was brought up is that there needsto be some training of computer science in this. the other point, coming back to the climate change research initiative,is that this is another integrator, in terms of identifying collaborations. in terms of how to facilitate thesecollaborations, one suggestion wasšthis is post docs in particularšpost docs at jpl.i tried to steer the discussion a little bit, just to test the waters. what i suggested is some kind of regular processwhere there are meetings that people can anticipate. i am thinking sort of along the interface model or researchconference model. it seems like the knee jerk reaction in this is simply, people identify an interesting area that theydeclare, let's have a workshop. we have the workshop, people get together, and then that is it. it is sort of the finalpoint in time. i think john agreed with me, in particular, that a single workshop isn't the way to address this. so, i amcurious about pursuing a sort of more regular kind of series of meetings. okay, and that is it.report from breakout group65statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.david scott, chair of session on highenergy physicsintroduction by session chairtranscript of presentationbiosketch: david scott is the noah harding professor of statistics at rice university. he earned his ba inelectrical engineering and mathematics in 1972 and his phd in mathematical sciences in 1976, both from riceuniversity.dr. scott's research interests focus on the analysis and understanding of data with many variables and cases. theresearch program encompasses basic theoretical studies of multivariate probability density estimation, computationallyintensive algorithms in statistical computing, and data exploration using advanced techniques in computervisualization. working with researchers at rice, baylor college of medicine, and elsewhere, he has published practicalapplications in the fields of heart disease, remote sensing, signal processing, clustering, discrimination, and time series.with other members of the department, dr. scott worked with the former texas air control board on ozoneforecasting, and currently collaborates with rice environmental engineers on understanding and visualization ofmassive data.in the field of nonparametric density estimation, dr. scott has provided a fundamental understanding of manyestimators, including the histogram, frequency polygon, averaged shifted histogram, discrete penalizedlikelihoodestimators, adaptive estimators, oversmoothed estimators, and modal and robust regression estimators. in the area ofsmoothing parameter selection, he has provided basic algorithms, including biased crossvalidation and multivariatecrossvalidation. exciting problems in very high dimensional data and specialized topics remain open for investigation.dr. scott is a fellow of the american statistical association (asa), the institute of mathematical statistics, theamerican association for the advancement of science, and a member of the international statistics institute. hereceived the asa don owen award in 1993. he is the author of multivariate density estimation: theory, practice,and visualization. he is currently editor of the journal of computational and graphical statistics. he is past editor ofcomputational statistics and was recently on the editorial board of statistical sciences. he has served as associateeditor of the journal of the american statistical association and the annals of statistics. he has also held severalintroduction by session chair66statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.offices in the statistical graphics section of the american statistical association, including program chair, chairelect,chair, and currently past chair.introduction by session chair67statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.transcript of presentationmr. scott: this is a very statistically oriented committee, but we were very much interested in bringing inresearch scientists to help us understand the data opportunities out there, and to bring a good description of problemsthat might be available for research.certainly, our second topic today falls into this category in a nice way. it deals with highenergy physics. wehave three outstanding speakers, two physicists and a computer scientist, to lead us in the discussion.we want to remind everybody that we intend, in some sense, to have a question or two during the talks, ifpossible, as long as it is for clarification and hopefully in the discussion at the end, when we come back together, youwill have a chance to sort of express your ideas as well. we would like to capture those.i am editor of jcgs and again, i would like to extend an invitation to the speakers to consider talking with meabout putting together a small research article for the journal, a special issue of the journal, later this yearšnext year.with that, i would like to turn it over to our first speaker, who tells me that, as all physicists, he has traveled allaround the world. he is now at berkeley.introduction by session chair68statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.robert jacobsenstatistical analysis of high energy physics datatranscript of presentation and powerpoint slidesbiosketch: robert jacobsen obtained a bsee from massachusetts institute of technology in 1978. he spent1976 through 1986 working in the computer and data communications industry for a small company that wassuccessively bought out by larger companies. he left in 1986 to return to graduate school in physics, obtaining his phdin experimental high energy physics from stanford in 1991.from 1991 through 1994, he was a scientific associate and scientific staff member at cern, the europeanlaboratory for nuclear physics, in geneva, switzerland. while there, he was a member of the aleph collaborationconcentrating on b physics and on the energy calibration of the lep collider. he joined the faculty at berkeley in 1995.statistical analysis of high energy physics data69statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.transcript of presentationmr. jacobsen: my name is bob jacobsen, and i am starting these three talks. so, i am going to lay some ofthe groundwork for what some of my colleagues will follow with.my science is different from what you have just been hearing about because we actually have a chart whichexplains it all. because we have that chart, which is really what we call a standard model, it affects how we do ourscience. it affects what questions are interesting and what we look at in an important way.so, i am going to start with an introduction that actually talks about this science, the goals of it and how we studyit. then i am going to talk about a very specific case, which is something that is happening today, an experiment thathas been running for two years and will probably run for three or four more yearsšthat is a little bit uncertainš whatwe do and how we do it in the context of the statistical analysis of highenergy physics data. i am going to leave to youthe question at the end, as to whether we are actually analyzing a massive data stream or not. then, i am going to endwith a few challenges, because i think it is important to point out that, although we get this done, we are by no meansdoing it in an intelligent way.so, the standard model of particle physics explains what we see at the smallest and most precise level in terms oftwo things, a bunch of constituents, which are things like electrons and neutrinos and quarks, and a bunch ofinteractions between them, whichstatistical analysis of high energy physics data70statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.you can think of as forces, but we tend to actually think of as particles that communicate with each other by theexchange of other particles. depending on how you count, there are basically three types of stuff, fermions, and thereare basically four forces and we omit the fact that we don't actually understand gravity. we just sweep that under therug and ignore it, and we do our experiments on electricity, magnetism, the strong force and the weak force.it is important to point out that, just like you can't seešin the talks that we have just been hearingšyou can'tactually see the underlying processes that are causing weather. you can only see the resulting phenomenon. we can'tsee any of the things in that fundamental theory. what we actually see are builtup composite structures like a proton,which is made up of smaller things, and protons you can actually manipulate and play with.more complicated ones are the mesons, which are easier for us to experiment with, so we generally do, but eventhen, we have to use these techniques to do our experiments. we have no other instruments except the forces we havejust described for looking at these particles. so, what we do is, we interact them through things that look like this bigblob on, for you, it would be the righthand side.for example, you bring an electron and antielectron together. they interact through one of these forces,something happens and you study that outgoing product. this particular one is the annihilation of an electron and itsantiparticle to produce two particles called b's, which is what we study. the actual interactions of those particles iswhat my experiments study. in general, when you put together approximately a dozen constituents, four differentforces, many different possibilities for how these can happen, there is a very large bestiary of things that can happen.statistical analysis of high energy physics data71statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.now, my wife likes to say that this is not actually science. this is a cartoon. i get a little bit defensive about that.it actually is backed up by a formal mathematical structure that lives on a little card that is about this big, which lookslike that, and i am not going to go through it.the advantage that we have with this actual physical theory is that we can make from it predictions. we canšthere are a few little asterisks here, and we won't go over the details very much, but in many cases, we can actuallymake predictions to the level of four significant digits of what will happen in a particular reaction.now, these predictions have been checked many different ways, but there are some open questions that remain.the two basic categories of this are that we have the equivalence of turbulence. we have things that we cannotcalculate from first principles. they happen for two reasons.one reason is that the mathematics of this theory are, like the mathematics of turbulence, beyond us to solve forfirst principles. we do not yet know how to do that. we know how to model it, we know how to approximate it, weknow how, in many cases, to pick regimes in which the thing simplifies to the point where we can make precisecalculations.if you walked up to us with a physical situation, we may not be able to calculate the answer to that. further,because of the way these processes workšand this is a cartoon here very similar to the one that i just talked about, theinteraction between an electron and an antielectron, which is very simple but, as time gets on, it gets more and morecomplicated. it starts as a very highenergy pure reaction but, as that energy is spread among more and more particlesand their reaction products, the situation becomesstatistical analysis of high energy physics data72statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.more and more highly dimensional, becomes more and more complicated, just like the cascade of energy andturbulence. by the time you get down to the smallest level, you are dealing with something that is inherentlyprobabilistic.in our case, it starts probabilistic at the very topmost level. these interactions could go any one of a number ofways when they happen.audience: could you tell us how much energy you are talking about?mr. jacobsen: it depends on the experiment. this particular picture was made for one that was 100 gigaelectron volts in the center mass. my experiment is actually 10. so, it is many times the ras mass of the particles thatwe see in the final thing, and you are making a lot of stuff out of energy.the other problem here is that, like every physical theory, there are some underlying parameters. depending onhow you count, there are approximately 18 numbers that are in this theory that are not predictable by the theory. theyare just numbers that are added in. now, we have measured them many different ways. we are a little sensitive aboutthe fact that 18 is very large.you know, is that too many? wouldn't you like a theory of everything that has the zero or one numbers in it. is 18too many? i just say, thousands of chemical compounds were originally explained by earth, wind and fire but, whenpeople worked on it for a long time, they discovered they really needed 100 elements. that was good, because wecould explain 300 elements in terms of protons, neutron and electrons but, when they really looked into it, thoseprotons, neutrons and electrons but when they really looked into it, those protons, neutrons and electrons of the 1930sbecame hundreds of mesons and particles in the 1950s.those were eventually explained in terms of five things, the electron, the neutrino and three quarks but, when youlook really into it, it turns out there are 88 of those. the modern version of the standard model particle physics has 88distinguishable particles in it. so, no matter what we do, we seem to keep coming back to approximately 100.this gives us a question as to whether or not there is another level of structure. right now, we don't know theanswer to that. the standard model doesn't need another level of structure, but it is certainly something we are lookingfor.so, with these 18 parameters, we measure them. that is what we do. is this complete? is there anything morethere? we don't know the answer to that. we know that some measurements that were just announced over the lastcouple of years and just updated about a week ago require us to add at least a couple more numbers.statistical analysis of high energy physics data73statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.well, that is bad. the theory is getting more and more complicated, but we are still explaining all of the data, andthere is one part where we have experimental evidence completely lacking, for whether the theory is really working theway it is supposed to. including people who will talk to you later on in this session, we are working very hard on thatissue.my current goal is to check a very specific hypothesis, which is that this theory, in its structure, as it stands today,is correct in describing. it may not be the entire storyœ but is it right? while we are at it, we are going to look for hintsof how to extend it, but mostly, i am interested in how it is right.the usual method of doing this in science, which was invented by galileo, was that you measure things 50different ways. if they are all explained by one theory with one underlying parameter, the first one tells you nothing,because all you have done is measure the parameter. the second one, though, should be consistent, and the third oneshould be consistent and the fourth one should be consistent.the theory of gravity predicts that all bodies attract to the earth at the same acceleration. the first object you droptells you nothing, because you have just learned what that acceleration is. it is the second one that tells you something.so, this is an example of a whole bunch of measurements with their error bars made by different experiments ofdifferent quantities, all of which are related by this underlying theory, to a singular parameter.we express these are measurements of the underlying parameter, but that is not why we are doing it. nobodyactually cares what that number is. what we are trying to do is determine the theory which is saying, all those thingsare related. they are all different except for the fact that some of them are replicated from different experiments. theyare described by one single underlying phenomenon, and in this case, they are disgustingly well distributed. the chisquared of all those things is about .05 per degree of freedom. that is a separate problem.statistical analysis of high energy physics data74statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.okay, so, how do you study it? the problem is that we can't actually do an experiment in the classic sense. likeastronomers or people observing weather, we can't go out and say, i want a storm here, right now. that is not going tohappen. so, it is an observational science. we collide particles in our machines to put a lot of energy in a smallvolume, and then we let these reactions take over and everything that is permitted by nature will eventually happen bynature. some might be more common than others.the typical reaction, though, is a little bit hidden from us. if i want to study the box that i showed you earlier, ican arrange for it to happen, but i cannot see the direct result. i can't see the interaction happen. that happens over a 10!29 of a second. i can't even see the things that it produces, because their lifetimes are just picoseconds. they will beproduced. they will go some tiny little distancešthat is a couple of microns.they will decay into things. some of those things will decay further into other things and, if i am lucky, i may beable to see all of these things and try to infer backwards what actually happened.i have no control over this downstream process. i have only limited control over what i can measure from thethings that come out of it. there are practical difficulties. what i want to see is what actually happens inside these littleblocks.that is the introduction to highenergy physics. now, i want to specialize in what we do. we are trying tounderstand why matter and antimatter are different. the world has a lot more of one than the other.in this mathematical structure of the theory, this is intimately associated with the idea of time. particle versusantiparticle, time and parity, lefthandedness versus rightstatistical analysis of high energy physics data75statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.handedness are all tied together in the mathematical structures in ways that have only really been elucidated in therecent past. i consider 15 years to be the recent past.for those of you who are statisticians and, therefore, very familiar with numbers, there is a deep connectionbetween the appearance of complex numbers in this theory that represent a wave, a propagating wave, as ei timessomething. this connection between changing parity, which changes one direction to another, like looking in a mirror,that just changes the sign of your x coordinates. time reversal, time going forward and backwards changes the sine ofyour time coordinates. these waves become their own complex conjugates, under these reversals, under thesesymmetries.the bottom line is that we are trying to measure parameters, not just in terms of amplitudes, but also in terms ofthe complex nature of them, in a theory. we are interested in measuring complex numbers that nature seems to have.there are very few constants of nature that are complex. the gravitational constant, mass of the electron, tax ratešwell, maybe not the tax ratešnone of these are intrinsically complex.the standard model has a 3by3 matrix of complex numbers that just appears in it as a mathematical construct.because it is a matrix that is relating how things transform into others, it is inherently unitary. everything you startwith becomes a one of three possibilities. that means that these three amplitudes for doing thisšfor example, one ofthese rows, this thing could become one of those three things that is the sum of these.when you actually put the mathematics of unitary together, what you get isš basically dotting these matrices intoeach otheršyou get an equation that looks like that. these rows are orthogonal to each other. you can represent thatas a triangle. these are three complex numbers that sum back to zero, so they are a triangle.the important point that you need to carry away from this is that these constants are controlling physicalreactions. so, by measuring how fast some particular physical reaction happens, i can measure this piece or i canmeasure this piece or i can measure this piece. i am measuring things that go into this construct, the triangle. bymeasuring a very large number of reactions, i canšaudience: what does ckm stand for?mr. jacobsen: cabibbokobayashišcharlie will now give you the names of the three physicists.charlie: cabibbokobayashimaskawa.mr. jacobsen: the three people who built this part of the theory. that wasstatistical analysis of high energy physics data76statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.before my time.you build a large number of measurements and you try to overconstrain this. you try to see whether thehypothesis of this theory is correct, which says that, all those measurements, when combined properly, will result in atriangle that sums to zero. that is what we want to know.now, other experiments will have similar things they are trying to do. this is the cartoon of what actuallyhappens. let me remind you that we can't measure the underlying reaction or the intermediate products. we can onlymeasure the things in the final state and, from that, we have to infer it.now, even worse, our experiment does not tell us everything that we want to do. the properties of these outgoingparticles, their momenta or even their type, are only measured with finite resolution and it is not as good as we need itto be.the information is incomplete in two senses. one is that sometimes things are not seen by the detector. there arepipes that carry particles in and cooling water in and information out. particles that go in there are not measured, justlike the satellites don't always tell you what you want at the time you want to see it. our sampling of these events isimperfect.it is often confused as to how to interpret. this is not from my experiment, but it is a great example. this is whathappened in the detector during one of these collisions. there is no actual picture here, where the computer readoutsensors inside the thing. the yellow green lines are where the computer said, ah, a particle went there because i canstatistical analysis of high energy physics data77statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.line up all these little dots that correspond to measurements, and there is some energy deposited out here.you will notice that there is one sort of going down toward 5:00 o'clock here that is not color. the computer saysthat that is not a real particle. no real particle made that line. the reason is, it has got a little tiny kink in it, part wayalong it. you can't really see it, but there is a missing dot and a kink right there.what it probably did was bounce off an atom and deflect. instead of making a nice smooth trajectory, it hit anatom and deflected. that doesn't happen very often, but it doesn't fit my hypothesis that something just propagatedthrough the deflection. we will come back to this kind of problem. in this case, what the machine knows about whathappened here is missing an important piece.i didn't expect to do a mine is bigger than yours kind of slide, but let me start. twentytwo terabytes a second. wedon't keep it all, because most of it isšwe don't keep it all. most of it is stuff that people understood 200 years ago.so, we aren't interested in that. the fact that like charges repel and opposite charges attract, or maybe it is the otherway around, we have mastered that. we don't need to study that any more.so, most of the 250 million collisions a second are just that, and there is nothing more to them than that. about athousand result in things being thrown into the detector in ways that indicate something may have happened. so, thehardware detects that, drops this data stream down, both in space and time, fewer bytes per crossing, because you don'tread out the parts of the detector that have nothing in them, and fewer per second.then, there is software that takes that 60 megabytes per second down to about 100 per second that can't bedistinguished from interesting events. it turns out, we will talk later about how many events, but we record about 600gigabytes a day of these events that are interesting. this is what we analyze.because there are so many things that can happen in this decay chain, and because only certain reactions areinteresting, we need a lot of events, and i will talk about those statistics later on. so, we run as much as we can and wetypically keep everything together for about twothirds of always, and we are now looking at a few billion of theseevents, 500 terabytes, but i will come back to that number later on.statistical analysis of high energy physics data78statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.the process of dealing with it is that you take the data that starts in this little hexagon, which is the detector itself,you shove it onto disks. then you have a file of processors that looks at it and tries to convert it into usable data.i don't quite understand the terminology of earth sensing, but i think this is level 2. it is where you have taken itfrom raw bits, as to wire seven had so many erts, and you converted it to something went thataway, with a certainamount of precision. you try to get not just this precise measurement of that particular particle went thataway, butalso an estimate of how precise that is. it is strictly misr statistics estimates. the rms of the scatter on this isexpected to be [word missing].we also do a tremendous amount of simulation which is very detailed. the simulation knows about all thematerial in the detection, it knows about all the standard model, it knows how to throw dice to generate randomnumbers. it is generating events where we know what actually happened because the computer writes that down, verysimilar to your simulations.we try to have at least three times as much simulated data as real data but, when you are talking hundreds ofterabytes, what can you do? we feed these all through farms and what comes out the bottom of that is basically in realtime. we keep up with that. then it goes through a process where i am not sure where it fits into the models that wehave described before, but it is very reminiscent of what we heard the nsa does. we look for what is interesting.we have had the trigger where we have thrown stuff away. which phone you tap is sort of the analog of that. butnow we take all the stuff that we have written down and individual people doing an analysis, looking for someparticular thing they want to measure, will scan through this data to pick out the subset they look at.the ones who are smart will accept something like one in 10!5. the real things they are looking for are 10!6, 10!7,but they accept a larger set. people who don't put a good algorithm in there will accept 5 percent, and will have a hugedata sample to do something with.audience: [question off microphone.]mr. jacobsen: the skins, the actual running through the thing, is supposed to be done three times a year. it isreally done sort of every six months. it is a production activity.audience: [question off microphone.]mr. jacobsen: they will pick 10 million events out of this store, and then they will go away and study themfor an extended period of time and write a paper. thestatistical analysis of high energy physics data79statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.process of actually picking from billions of events is a largescale computer project. so, they willši will talk moreabout how they look over and over again at these things, but the actual project running to do the selection is scheduledand done as a collaborative effort several times a year.audience: [question off microphone.]mr. jacobsen that is about right. i will show you some plots.so, our task involves thousands of processors, hundreds of terabytes a disk, millions of lines of code, hundreds ofcollaborators, and i remind you that we still screw up a lot. we are doing the best we can but, for example, fixing thealgorithm that didn't find that track involves interactions on all these levels, lots of code, more computer time, lots ofpeople signing it off as the right thing to do.i have to show this. i love this. we are in the state of this poor guy. the task has grown and our tools have not,and this limits what we can do in science. this goes directly to our bottom line.so, once you have this sample of some number of events, what do you do with it? you are looking for a particularreaction. you are looking for this decayed to that, because you want to measure how often that happens, to see theunderlying mechanism. you want to find out how many storms produce how much something or other.so, you remove events that don't have the right properties. the events you are looking for will have a finitenumber of final state particles. the electric charge has to add up to the right thing, due to charge conservation, etc. youcut everything away thatstatistical analysis of high energy physics data80statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.doesn't match that. since the detector makes mistakes, you will have thrown away events that really were what youwere looking for.a typical efficiency in terms of that kind of mistake for us is 20 percent. we can discuss whether it is 10 percent,25 percent, but it is routine to throw away the majority of events that actually contain what you are looking for becauseof measurement errors and uncertainty. we will come back to that.in the end, you usually pick a couple of continuous variables like the total energy and the invariant mass.invariant mass is just the idea that if you have something that blows up from the products, you can calculate how bigthe thing was that blew up. everything is right, but you are still not certain they are the right ones, and you plot thosetwo properties on these two axes for each of those events. you will get something that looks like this.this is actually a simulated run. in this bigger box are 1,100 events, approximately, and in this little bluepurplebox, are 13.2 events. you get fractional events, because this is simulated data and you have to adjust for the variablenumber, and we don't have exactly as many real data as simulated data.so, all the ones that are in the blue box are most likely to be the right thing, because that is centered on the rightvalues of these quantities. this big box is not centered on the right quantities. it is the wrong ones. it is ourbackground. those are accidentals that get through all of your cuts, that something else happened.it is easy to see what you have to actually do. you just count the events in those two boxes, take the stuff in thebig box, and project how much of the stuff in the little box is background, and you have to worry about the shape andall the other stuff. do the subtraction, and that is your answer, and then you write the paper, nobel prizes are awarded,etc.the way you do the extrapolation and the correction for efficiency is with these stimulations. they are not perfectbut, to the extent that they are only small corrections, how big an error can you make on a small correction? so, youhave to add systematic errors for that. historically, though, there has been a tremendous concern, because everybodywants to be the first person to discover a particular reaction.statistical analysis of high energy physics data81statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.so, they tend to sort of put the lines in the place that gets that last event in. the babar has adopted a policy,which is followed about 85 percent of the time, called blind analysis. blind but not dumb, is the phrase we actually use.the analysts basically promise to make all their plots on their screen with this little box over the signal, whilethey are doing all their tuning. there is nothing that we can do to enforce that. these tools are on the desktop, but theydo it. then, once they have got all the details of their analysis final and they promise not to change anything, you popthat off and you look at it. this seems to be very effective, because nobody worries about it any more.as time goes on, we will get more complicated analyses. we will do a maximumlikelihood fit that has somelinear combination of probability density functions. to the statisticians these probably aren't probability densityfunctions, but that is what we call them, to find the rates of many things that are going on.maybe that fit will have directly in it the standard model parameters that we are looking for. maybe there will beparameters that describe a resolution. how much something is smeared out on the plot may actually be fit for, as itgoes along.a typical fit has 10 parameters in it, but i have seen ones that have up to 215. the basic problem here is that, ifyou are fitting in a maximumlikelihood fit, you want to understand the distribution. we do not understand most ofthese distributions on first principles.statistical analysis of high energy physics data82statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.we can extract them from simulations that we don't trust, or we can look at the data to try and figure out whatthey are. now, we are not smart about it. this is a fit probability density function to some particular thing. you can'tread this, because it is one of our tools, but it is the best fit by the sum of two gaussians.so, we parameterize it as the sum of two gaussians and feed it into one of these huge fits. the real reason that it isthe sum of two gaussians is there are two distributions there. there are actually two populations.in this plot down here, the highenergy physicist will motivate the fact that they are actually two types of events,but we do not separate them. it is very unusual, it was considered a major breakthrough for somebody to actually do afit by doubling the number of parameters separating these two populations. that is how you get up to 215, a bunch ofsimultaneous fits.usually, we just parameterize these. the reason is subtle. the reason is that, even though we know it is twopopulations, we are still going to have to fit for two gaussians, because we don't have a first order understanding ofwhat those things should actually look like. so, even though we calculate the errors on many quantities, we calculatethem in a very simplistic gaussian way, and we don't have any way of representing the more complicated things thatare going on.to come back to your question, there are typically 10k to 100k events in that final selection that was done inproduction, and that fits in a desktop computer, and that is really what sets how many of those are.the analyses are done with very small scale tools. they are physicistwritten,statistical analysis of high energy physics data83statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.generalpurpose tool boxes, but users write their own algorithms for selecting events. the analysis turn around time isanalogous to the attention span. that means that they will write code that is just complicated enough that it willcomplete before they come back with a cup of coffee.now, my question to youšwe don't have to answer this nowšis i am not sure that we are doing statisticalanalysis of massive data streams at this point. the only way we know how to do it right now is to get it ontosomebody's desktop, somehow make it fit, and in the next talk we may hear about better ways to do this.this is the situation we are in. we have bigger jobs, and our tools are just not up to it. every time we attempt it,we get embarrassed.look at this picture, actually taken at a home depot. you will notice the engine is actually running. this guytried to drive home.in my last couple of slides, i want to turn this around the other way. i want to say something to the statisticians inthe office, what i perceive the real challenges in your area to be. all of them are this idea of capturing uncertainty. wedo a tremendously bad job of dealing with the fact that our knowledge is poor, in many different ways.what we do basically now is, if we suspect that we have something funny going on, we punt. we throw the eventaway, we run the detector for an extra week, month, year, whatever. so, if only parts of the final state are presentbecause of a measurement error, if, when we look at the event, we could interpret it multiple ways, which can happen,there was this eventšthis cartoon is to remind you of the marginal track.statistical analysis of high energy physics data84statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.if something is marginal because maybe something a little bit more complicated than the first order thing i amlooking for has happened, we throw it away. you can easily do back of the envelope estimations that we are talkingabout factors of three in running time for experiments that cost $60 million by doing this. physicists, particularly highenergy physicists, think that we have completely mastered gaussian statistics, and refuse to admit that there are anyothers, although secretly we will mention poisson.we don't know how to propagate detailed uncertainty. we don't know how to deal with the fact that the realdistribution of error on one of these tracks is, 98 percent of the time it is an misr of this width and 2 percent of thetime it is a gaussian of that width. we don't know how to combine all that throughout all the stages of this analysis toget out something that makes sense in the end.so, we choose to get rid of those special cases out of a fear of tails in our distribution. when we can't do that, wemodel it with these pdfs, which we don't really understand, and which are throwing away all the correlationinformation between those two, because we can't see that at all.this is accepted now, because it causes a lot of statistical power. in the absence of a way of dealing with itšidon't want to say formally, i want to say principled way of dealing with it, we do not know how to assign a systematicerror if we allow these things to come in.it is not even so much that the systematic error from these will be large. we don't know what it should be. so, wedon't really know how to balance off the complexity of our true information and the statistical power of our trueinformation.as you go through the literature in highenergy physics, you will find people who will address this in oneparticular place in an analysis where it is useful.there has been no systematic approach, which is where the real gain is. the real gain will come from ten 5percent gains, not from one 5 percent gain. now, sometimes, every data matter. when what you are doing is the realfinal result, taking that triangle which is shown in cartoon form here to remind you, and combining measurements thattook hundreds of people years to make, because different experiments are measuring different things, and trying to seewhether this is right, you can't throw out the work of 500 people because there is a tail on the distribution. you will getlynched.so, the way that we do that now is that we do it manually. people called theorists get together at conferences andthey argue.this particular plot is the result of six theorists for three weeks, trying to deal withstatistical analysis of high energy physics data85statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.the statistics of combining data with nongaussian error. the way that you interpret this is that each little color band orhatched band is a different measurement from a different experiment, and this little eggshaped thing here, the yolk, isthe best estimate of where the corner of the triangle is in this plane. the triangle actually lies along the horizontalmidplane and sort of extends up to there. as these measurements get smaller and smaller error bars on them, this willtighten up until eventually, hopefully, some of them will become inconsistent with each other.audience: [question off microphone.]mr. jacobsen: no, actually. what we do, remember, i said that this combination of all possible processes hasto obey unitarity. everything has to become only one possible thing. so, unitarity also implies that certain things don'thappen. some things sum to zero. so, we have this triangle which each side of it is related to certain rates. to make iteasier to deal with, since we don't know any of them, we divide all the sides by the length of this and put it in anarbitrary zero to one space.so, we call these things an " and #, but they really are physical measurements. everything is put in terms of onehighprecision measurement. so, that is actually one of the things that is really spurious. that is not a good word to use.one of the things that is very hard about this is that everything, with the possible exception of some of the bandsthat appear in the theory, and one of the plotsšbut not all of the plotsšis subject to experimental verification. wehave no scale in this plot that is not subject to experimental verification.from here to hereši am not sure where the center isšthere is the center, and there is the side. that is actually anexperimental number. the width of that is actually secretly an experimental number.as fundamental physicists, we worry about that. when you learned electricity and magnetism, you probablylearned it in terms of the charge of the electron. that did not tell us the charge of the electron. somebody had to go outand measure it and, every time that measurement changes, it changes everything. it is now known to nine significantdifferences, so it doesn't change your electric bill, but at this level, it has to be taken into account that everything issubject to experimental verification.it is an unfortunate fact that there is nothing that is really well known in that plot. in 15 years, we will be doingthis in high school, but right now, it is a very hard thing because there is a lot of uncertainty in that plot.mr. scott: can you just briefly, when the theorists are sitting around in this room deciding how to draw it,what exactly are they using for this?mr. jacobsen: leaving the names out, there is one guy who says, the only way you can possibly do this istake all the likelihood curves from all of these measurements, pour them into a computer and have it throw numbers,and then make a scatterplot of the possible outcomes.then, i always discuss whether we draw contour lines or not. someone else will say, but that is completely bogusbecause there are large correlations in the systematic errors that these experiments have made. they have madecorrelated assumptions, and then how do you deal with that?his response was, well, we vary the underlying assumptions in one of these computer models. someone else says,no, they told us it is plus or minus something in their abstract, that is all i know, that is what i am going to draw.statistical analysis of high energy physics data86statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.in fact, the one that actually won was the first one that i described. they put all this stuff in, they ran a computermodel that put dots on a screen. the question is, if you have got a whole bunch of dots on the screen, where do youdraw the 90 percent? one has to enclose 90 percent of those. which 90 percent? i am not a statistician. my job is tomake that little angled line in there. i don't understand, really, this, but i know that they don't either, because i canshow you six other plots, and they are not the same.one last thing. i promised i would get done close to on time. everybodyši teach freshman physics a lot.everybody believes that science starts with ﬁeurekaﬂ and archimedes running down the street naked. not my science.my science starts with, ﬁhmm, i wonder what went wrong there.ﬂ let me give you an example of this.this is a famous event. it has actually been redrawn with a different depictographic package, so it is actually asimulated event. the real event that looked like this led to the discovery of the $, nobel prizes were awarded, etc., etc.,etc. what this event was, was this can't happen. in the standard model, as it was established at that time, the rate forthis process was identically zero. zero does not fluctuate to one.this was not noticed by a computer program that was looking for it. why do you look for something that can'thappen? that is not a great way to get measurements. it was noticed by someone flipping through events and saying,ﬁoops, what is that?ﬂ the process that i have outlined, which is the modern process of the last five years, is not theprocess of flipping through a significant fraction of events. you can't look through a significant fraction of a billionevents.so, we are, in fact, quite weak on the fact of finding things that don't belong. this is particularly difficult becausenowadays, although we have a theory that predicts this, one of my students checked. approximately 15 percent of theevents that look like this are due to mistakes in the detection.anomalies can be due to mistakes. the standard models predict that an electric charge is 100 percent conservedall the time. if you could prove that an electric charge was not conserved, automatic nobel prize. a lot of our eventsdon't conserve charge, because we will offset the charged particle. something disappeared and we didn't see it. so, ifyou naively add up the charges, it doesn't sum to zero. it sums to something else. so, how do you look for anomalies inthe presence of mistakes is something that we don't know how to do.the way we do it now is say, ﬁoh, this weird thing that can't happen, could be happening.ﬂ let's go look for it asa clean signature. we search positively. let's see ifstatistical analysis of high energy physics data87statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.electric charge is conserved. we do a long, complicated analysis, and the bottom line is that, with such and suchconfidence level, an electric charge is conserved. it is a search paper, and there are a lot of those. they are all searchesfor specific things. it is very hard today to notice an anomaly.okay, here are my conclusions. i tried to give you a feeling for how we do highenergy physics with these data. inthe discussions, i really hope to hear about better ways, because our successors definitely need one.i am in the position of these guys, who are back at the invention of the kite, and they are working with thetechnology they know and understand, and they are just going to use it as much as they can, but they are not going tosucceed. so, thank you very much.mr. scott: while our second speaker is setting up, i want to thank bob very much, and we do have a fewminutes for questions.audience: why do you do this parametrically, if you don't know what your distribution is?mr. jacobsen: what other choice have i got?audience: nonparametrics.mr. jacobsen: i am embarrassed to say, i don't know how to do that, but let's talk. how do i put this? a lot ofwhat we do is because we learned to do it that way. so, new ideas, in fact, distribute themselves in a very nonlinearfashion in highenergy physics. neural networks are a classic example. neural networks for a long time, the basicreaction of the highenergy community was something like this: keep this away from me, i will get out the garlic.then a few people showed that they actually could be understood. once that had been shown, you don't evenmention it any more. so, methods that we don't understand we resist until suddenly they are shown to be worthwhile.audience: where are they used?mr. jacobsen: neural networks? they are used everywhere now, but they are predominantly used ascategorizations of large groups, at least at the rejection steps before you get to the precision measurement.for example, the skins that are sorting out samples for people? i guess about a third of them probably use somesort of neural network recognition. it takes many variables to try to make a guess about these things. we do apply themon a large basis. we have simulated ones. we don't have neural network chips. we have neural networkstatistical analysis of high energy physics data88statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.algorithms.audience: what are the number ofš [off microphone]?mr. jacobsen: ten is thešplus or minus two.audience: and the categorization isš [off microphone.]mr. jacobsen: yes no.statistical analysis of high energy physics data89statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.paul padleysome challenges in experimental particle physics datastreamsabstract of presentationtranscript of presentation and powerpoint slidesbiosketch: paul padley is a professor of physics at rice university. his research interests lie in experimentalelementary particle physics. he conducts much of his research at hadron collider facilities such as the tevatron atfermilab and the large hadron collider at cern, the world's largest particle physics laboratory. dr. padley earnedhis bsc from york university in 1981 and his phd from the university of toronto in 1987.some challenges in experimental particle physics data streams90statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.abstract of presentationsome challenges in particle physics data paul padley, rice universityexperimental particle physics attempts to understand the most basic constituents of matter and the forces that actupon them. the research is carried out at national and multinational laboratories, such as fermilab and cern, by largeinternational collaborations. the next generation of experiments will produce data at a rate of 40 tb/s, which will bereduced with realtime hardware. the resulting 800 gb/s data stream will then be processed in real time with a 10**6mips computer cluster and reduced to terabytes of data per day which will subsequently be analyzed offline. each stepin the process involves the statistical analysis of the data to search for the signatures of interesting (but possiblyunanticipated) physics.some challenges in experimental particle physics data streams91statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.transcript of presentationmr. scott: a decade ago, steve sain, who is sitting in the back corner here, was trying to write his thesis.these physicists kept bugging him to do these nonparametric methods, and finally found the top quark. paul padleyhas very graciously agreed to sort of give us a talk that really illustrates statistical challenges explicitly that are outthere and, without any further ado, i will turn it over to paul.mr. padley: i am going to do my introduction, too. the most important thing i want you to leave with todayis, i come from rice, and it was ranked the coolest university in the united states by seventeen magazine. while mydaughters will challenge with me on that issue, who am i to argue with seventeen magazine?so, i am doing particle physics which you just heard about, and my brief summary of what we tried to do isanswer the following questions. what are the elementary constituents of matter, and what are the basic forces thatcontrol their behavior at the most basic level?some challenges in experimental particle physics data streams92statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.in the process of doing that, we asked some very weighty questions. i mean that literally. so, one of theoutstanding problemsšand bob may reference thisšis this bit of the theory that is marked, not been checked, or notbeen confirmed.the bit of the theory that has not been confirmed is the thing that tells us why we have mass. i assume we havemass, you have mass. we don't know where that comes from. a point particle, the top quark, has the mass of 175protons. a proton is a big, extended object. it has quarks and things in it. so, why is this point particle, that is infinitelysmall, have a mass? so, there are some pretty big, weighty questions that haven't been answered by the standardmodel. so, we don't know why things have mass.we don't have a quantum theory of gravity, which is some sort of clue, since gravity is the thing that interactswith mass. so, to do that, we do weighty experiments.here is the nextgeneration experiment that we are building at cern. it will weight 12,500 tons. notice theperson, for scale. you can think of this as a big instrumented device. those 12,500 tons of detector are instrumented.some challenges in experimental particle physics data streams93statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.to do this, we need big international collaborations. there is an experiment i currently work on at do. there is asmall subset of the people who worked on building that experiment, which is running and taking data now. it is one ofthe two experiments that discovered the top quark. a free beer if you can find me in there. i am there. another freebeer if you can figure out which client was upside down.you can think of a big particle physics detector as a smart device. it is a realtime smart device doing realtimeanalysis. the nextgeneration experiment will, in principle, produce data at about 40 terabytes of data a second. whatwe do is use this big smart device to selfselect what data is interesting.we will knock it down to something like 800 gigabits per second in realtime on the detector itself. we never tryto get the 40 terabytes of data off. i, for example, work on the bit out in the edge that tries to self identify whether whathappened in there was interesting or not. i, for example, work on this bit of the detector which has artificialintelligence on it to say, ﬁhey, wow, something interesting happened here; let's think about it some more.ﬂwe then shift the data out in realtime computing, and we will reduce the data to something like 10 terabytes ofdata a day. each step in that process is a statistical process.some challenges in experimental particle physics data streams94statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.you can ask, given this heavy need to use statistics, one can ask the question, how do particle physicists usestatistics? bob made reference to the fact that we all use gaussian statistics. in fact, if you went and polled a group ofparticle physicists and asked them, if two hypotheses have a good chisquared, which hypothesis do you take, the vastmajority would say, the one with the lower chisquared, and not answer that both are valid. your typical particlephysicist, when you say, ﬁwell, actually, both hypotheses are valid, what are you going to do with that?,ﬂ they willlook at you dumbfounded.it was announced last year the observation of neutrinoless beta decay and they published it. they got it published.okay, there is an effort to rectify this. here is a headline from what i will call a trade magazine, the cerncourier, a standard popular magazine about a business: ﬁphysicists and statisticians get technical in durham.ﬂ therewas a big conference in durham last march on applying statistical techniques to particle and astrophysics data.how many statisticians even attended this conference? none. they then go on proudly to say, ﬁthis teaming ofphysicists and statisticians.ﬂ there they all are. look for your colleagues in there. almost 100 physicists and 2professional statisticians gathered. that is a direct quote.some challenges in experimental particle physics data streams95statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.okay, we have a problem. before i go on, some of us have actually tried to rectify this situation. i am going totalk about some work that has gone on, and i am representing other people who have done the real work. the results iwill show come from my graduate student, andrew askew, and a former student at the rice, bruce knuteson, acolleague, hannu miettinen, another colleague, sherry towers at fermilab.then, i owe a lot of thanks to david scott, because he is always there and i can come and ask him my naivequestions like, ﬁyou mean you don't take the lowest chisquared?ﬂ he will patiently answer them for me.first, a little bit of what we do. we talked about how we collide protons, for example, and look at what comespouring out into our detector event by event, and then we statistically analyze that looking for new physics or knownphysics.it is a little bit like taking two strawberries, smashing them together. you get this bowl of energy that then turnsinto a bowl of fruit, and we look at the fruit coming out.some challenges in experimental particle physics data streams96statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.here is a specific example. this is something where you get a trip to sweden. this would be a proton antiprotonšproton proton collision, say the two photons. what we do, this is what we see in the detector, and we have tostatistically analyze that to figure out what went on.then we will make an invariant mass distribution of these two photon signals over a large statistical event to lookfor a bump. that would be the discovery of the higgs signal.of course, this phrase, ﬁselect all the events,ﬂ with the correct apologies, that is a very complex step. so, here isan event. what i have done is altered the detector outlook and energy deposited in the detector.some challenges in experimental particle physics data streams97statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.there are still obvious big things there with missing energy and an electron. then, as i look at the event i find,well, there is a jet from the fundamental quark, and another one and another one and another one. those things getpretty hard to fish out in the data. so, we need pretty good tools for doing that.so, we have sort of a twostep problem. we look at each event and try to figure out, event by event, what hashappened, and that is a complex pattern recognition problem with lots of cluster finding and other things that we needto do, to track fitting and identifying things. then we have to take a cohort of events and statistically analyze them andlook for physics. it gets pretty complex.in the nextgeneration experiment, what you will actually see, here is an event with four muons coming out. ifyou were looking at it, all you would see is that, and you have to find the tracks in that mess. i have made it worse thanit really is, because i have compressed it all into two dimensions, a threedimensional thing.some challenges in experimental particle physics data streams98statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.it is not just those bumps we look for. sometimes, what we are looking for are differences from the standardmodel that are a little bit more subtle. for example, this is a simulation distribution of two parametersšit doesn'tmatter whatšfor our standard model particle physics.here is simulated distribution, assuming there were extra dimensions to the universe. the fact that we could useparticle physics experiments to look for extra dimensions was mindboggling to me, not something i had expected.here is what we saw in the data. then we have to ask the question, well, are we seeing extra dimensions in that data ornot. clearly, we are not in this case, but sometimes it is a little bit more subtle.so, i want to talk about two particular attempts to try and go a little bit beyond the sort of analysis that bob wastalking about. he made reference there about using neural networks, and we have been trying to use kernel densityestimation. then, a method for looking for new unexpected things.some challenges in experimental particle physics data streams99statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.neural networks have become commonly used as a pattern recognition tool, but the black box nature of themworries many in the field. i mean, physicists don't really like being able to visualize what they have seen in that.so, a group at rice, named herešis sain here? there he is. i have never met him, this happened before i got atrice. they developed a method called pde that was formulated to look for top quarks. it is a multivariate approachwhere you can plot things to understand what is going on.so you want to form a discriminate function that can discriminate signal from background. so, you can have ageneral form like this, where x is some vector of parameters that you have measured. so, you have a functiondescribing the signal andsome challenges in experimental particle physics data streams100statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.then the background, and you try to make it discriminate.so, we need to formulate these feature functions. what they did was they used kernel estimation. so, for each datapoint in the signal or background, they put in a function, typically a gaussian functionšactually, we have only everused gaussian functions.this function describes a surface and end dimensions, and you form the signal and background surfaces using thismonte carlo simulated data of the signal you are looking for in the background. so, you can just think of these assmooth surfaces representing the data. it is much more straightforward to think of than thinking about what the neuralnetwork is doing.some challenges in experimental particle physics data streams101statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.it reflects the covariance of the data. you construct a transformation, so that one class of the data has a covariancematrix that is a unit matrix, and the other is diagonal. that is something that you can do, and then you write it inmathematical form.there is a free parameter that enters.so, by following a recipe where we make these kernel functions, it can make a discriminate function and make agraphical cut, something where you can visualize what you are doing.here is just a monte carlo, arbitrary parameter signal, that we wanted to look forsome challenges in experimental particle physics data streams102statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.distribution, and here is a background.then, you apply this technique and you get a model of the signal,and a model of the background,and then here is this discriminate function, and you would make a cut somewhere here, and then be able to pickthe signal out.some challenges in experimental particle physics data streams103statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.we have modified that original method a little bit. one of the problems with that method is if you have outliers.the outliers sort of get a little gaussian put around them. so, we have made a way to adapt that and smooth it out.so, we have introduced another parameter.our colleague at fermilab, sherry towers, independently came up with a similar method that uses a localcovariance method.here is a comparison of these methods for particularly hard signal and background samples, where a neuralnetwork just does a terrible job and our methods do a good job.so, that is one thing. so, one thing that we have tried to do is, in applying thesesome challenges in experimental particle physics data streams104statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.cuts and sort of making cuts and boxes, as was described before, try and useša lot of people are using neuralnetworks, but there is a group of people who are trying other techniques, like support vector machines, internal densityestimations and that.another problem we come up with continuously is, you have background, and here, background means thestandard model. what you see in the data is this. so, have we found new physics in these points out here or have wenot? that is a common problem.in fact, if you were to take a poll, as a particle physics community, as to what the new physics would be, youwould get a whole pile of answers. so, we have this standard model that works beautifully and explains all the knowndata. i think if you made a poll of the particle physics group, nobody would believe that the standard model is right.there are all these free parameters, you don't really understand mass in this context, and there are a lot of ideas outthere for what the new physics would be, new bosons, all these things that are sort of meaningless. a big contingentwould say something else. so, this was just a straw poll then.some challenges in experimental particle physics data streams105statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.even if you pick the most popular model, supersymmetry, of what you are looking for, there are 105 preparameters in the theory. so, how do you possibly know š and changing those parameters changes what you will seein the detector.so, the problem, in a nutshell, that we face is that we have a welldefined standard model. we need to look forinteresting physics in the datašthere is lots of data. we need to statistically analyze these events to determine if wehave found some physics of interest, but we probably don't know what the physics of interest is that we are looking for.so, we are looking for something unknown in this vast mass of data.now, the method that was described to you before is, typically, what you do is select a model to be detected, youfind a measurable prediction and a couple ofsome challenges in experimental particle physics data streams106statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.parameters of that model, and you go check those predictions against the data. that is fine, when you have a finite setof models to test.in this huge plethora of possible models and every variation of the parameters, and saying supersymmetry is adifferent model with different consequences for the experiment, this becomes a real problem. so, at the doexperiment, a generic search was tried. this is something that was called sleuth.so, typically, what is done in a physics analysis is that you have some class of model, minimal supergravity,supersymmetric supergravity, with some particular set of parameters, and you can go and try and look for that case.you can consider looking for some larger set of the parameters. what we really want to do is try to make our searcheswhere we are looking for something new in general.so, the typical physics analysis done in a particle physics experiment is done at 1.5 on this scale, and we wouldlike to be up here at 6.0, searching through the data, looking for new things.the other problem that comes up all the time is you get one unique event. well, bob showed you a unique eventbefore. well, how do you find those unique events and decide that they are unique. so, we would like our method to beable to do that in an unbiased way.some challenges in experimental particle physics data streams107statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.so, what we did is basically tried to boil the data down into a finite set of fundamental things that you canmeasure. we looked at different combinations of the particles, what they are, and then we tried to see what it is aboutthose particles that we are measuring that is interesting.in our case, we have the advantage that the known physics happens at low energy. so, at low transverse energy inthe experiments. so, if we look for things that happen at high energy, they are likely to be interesting.so, we picked a set of parameters which is just basically the momentum in the perpendicular direction of thethings that we were looking for.some challenges in experimental particle physics data streams108statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.then we go one more step. we do a variable transform.so, say we have our data gathered on a space like this. we basically push that data out into a unit box, and thenmap the data onto a uniform grid. we are taking a simulated model background data, and we map it out onto a uniformgrid in a unit box, and dimensions.we can go back to that example that i gave before and look at what happens to the signal or not.some challenges in experimental particle physics data streams109statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.the way we set up our variables, it would tend to push those events up into a cluster in the corner.then we have to ask the question, is that an interesting cluster that we are looking at? so, what we do is, wecreate what are called voronoi regions, which is just a cluster of data points as the set that hasšthe region is the set ofall values of x closer to a data point in that cluster than to any other data point in the sample.so, you break the data up into regions and then you look through those regions and try to look for an x set in thatspace.some challenges in experimental particle physics data streams110statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.basically, you can assign a probability as, what is the probability that you will see something that is moreinteresting than what you saw.so, the data contain no new physics. these are just fine p's that are random between zero and one. if the data doescontain new physics, then you hopefully find p to be small. this method was tested on known physics. for example,the search for the top quark, which was a big discovery, was reproduced using this. it did findšthis message did findthe top quark, but the price you pay for this general search is that you have less sensitivity to the new thing that you arelooking for.what was amazing with this is, here is a list of all of the channels and the limits we could set, looking for newphysics in all these channels. in the traditional sort ofsome challenges in experimental particle physics data streams111statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.analysis that bob was describing, and normally done, is a graduate student would come and pick a channel and take amodel, and test that channel against some model. with this method here, we were able to search down through a wholelist of channels in one fell swoop, going through our masses of data.now, one of the things that made this possible is, at the time of this analysis, it was mature data. so, we had verygood simulations of the detector in that. so, we really could model the standard model in it very well. it was verymature, wellunderstood data. so, that made it easier to go searching through. we think in here, there is this hint wherewe have this general problem of looking for unknown things.so, just to conclude, i think particle physics presents a number of challenges of interest. you have just seen a littletaste, a little scratch of the many places, every step along the way, we face statistical challenges. we certainly havelarge streams of data that we must search in realtime, and offline, for signals of interest, that are possiblyšin fact, themost interesting ones would be unanticipated. we have the advantage of a welldefined standard model to test against,and actually, techniques to generate the data that we use for that will be talked about in the next talk.there are people in the community who have actually talked to a statistician now. that is a step in the rightdirection. of course, we always have this hurdle, we know we are smarter than everybody else, so we try to reinventthings for ourselves from scratch. there is a small group of people who have actually spoken to at least twostatisticians. so, we know it happens. so, we are in the very early infant stages.i have shown some of what i call baby steps that have gone on at our experiment at do which are unique. i mean,the number of people even within the experiment who would understand the phrase kernel density estimation would bevery small, or even know what a kernel is. so, there is a lot of progress that needs to be made.the statistics conference that occurred is going to be a continuing series, it looks like. there is another onescheduled for next year. so, there is at least a recognition in the community that we should be talking to the statisticscommunity, and that dialogue should start. hopefully, we will get smarter about how we do analysis. i will finish there.audience: [question off microphone.]mr. padley: yes, i thinkšthat has to be done individually for each combination of particles. in fact, really,you don't need to do it. i mean, really, if you are smart, you could bypass that whole step. i think next time aroundmethods will be used to bypass that. it is the idea of trying to findšthe problem you always get is you havesome challenges in experimental particle physics data streams112statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.some distribution that is fine with exponential pay off.you have two events way out here, far away from the standard model. so, is that interesting? i mean, carlorubbia made a career out of discovering things out there that, as they took more data, the distributionš [offmicrophone.]mr. scott: i have a question, paul. you have 6, 7, 10 dimensions of data. are these real measurements orderived measurements, typically?mr. padley: that is almost level three in this case, which is one of the problems. so, you know, part of whatmade that analysis possible at the end is, what we started off with is about a million measurements per event. you thenhave to distill that down into, say, vectors, and that will representšthere will be hundreds of those in an event. wehave tried to knock it down to five or six or ten parameters that characterize the events. that is like level three. thatwas really only possible because of the maturity of the data and, by the time that analysis was done, there was a lot ofconfidence in the other steps that went into informing that data set.some challenges in experimental particle physics data streams113statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.miron livnydata grids (or, a distributed computing view of highenergy physics)transcript of presentation and powerpoint slidesbiosketch: miron livny is a professor of computer science at the university of wisconsin at madison. hisinterests are in high thoroughput computing, visual data exploration, experiment management environments, andperformance evaluation. he received his phd in computer science from the weizmann institute of science, inrehovot, israel, in 1984.highthroughput computing is a challenging research area in which a wide range of techniques is employed toharness the power of very large collections of computing resources over long time intervals. his group is engaged inresearch efforts to develop management and scheduling techniques that empower high throughput computing on localand wide area clusters of distributively owned resources. the results of these efforts are translated into production codeand are incorporated into condor, a widely used, highthroughput computing system. the worldwide user communityof condor plays an important and active role in dr. livny's research, and researchers from a wide spectrum ofscientific disciplines collaborate with his group in the development and evaluation of condor.in the area of visual exploration of information, dr. livny's group works on developing a framework and tools forintuitive graphical interaction with collections of multimedia data. his framework is based on a declarative approach tothe creation of active visual presentations of tabular data. he implements his tools in java and works closely withdomain scientists on testing and evaluating them with real data. some of these are stored in scientific databases that areconnected to realtime and offline data sources.data grids (or, a distributed computing view of high energy physics)114statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.transcript of presentationmr. scott: the next speaker is very appropriate. if you are going to have massive data sets, you need massivecomputing and ﬁgridﬂ is the buzzword. we are very fortunate to have one of the leaders in the field to talk to us aboutthe relationship between grid computing and highenergy physics.mr. livny: thank you. since i really didn't know what was expected from me in this presentation, i will try tocommunicate sort of threešaddress three areas. one is sort of our view from the computer science perspective of whathighenergy physics is. the other one is to touch upon what i believe is a huge challenge, is how to do realinterdisciplinary work. so, what does it mean to change a title from a computer science professional to a computerscientist. that was not easy, and we are still working on it. the third one is sort of to give you some update ontechnology and what can be done and how it works because, at the end of the day, a lot of what was described earlierdepends on computing resources and stuff like that. so, we will see how far we can go with all of that.i will give you a little bit of background because i think it is important to understand (a) where we are comingfrom, and (b) what we have experienced so far.data grids (or, a distributed computing view of high energy physics)115statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.so, at wisconsin we have been running the condor project now for over 15 years, and we probably are the proudholders of the title of the oldest computer science project that is still doing the same thing.this is the good news and the bad news. the good news is that there is a lot of work to be done there. the badnews is that it is really hard to do it right. when i say to do it right, i think it is important to understand that, if youwant to do it, then you have to do it for real, which means that you have to develop new software, and you have to faceall the challenges of software engineering, middleware engineering, whatever you want to call it, because it has to runon all the platforms and it has to do it right.you have to become part of these collaborations, and it is not that easy to be this dot in the picture that you sawearlier, and to survive there. we definitely, in computer science, think that a collaboration of three scientists is a hugeeffort. suddenly realizing that we have to work in these larger collaborationsšand i will talk about politics later š itis an important part of it, and has a huge implication. so, we have to learn how to do it, and it is not simple.the other part of it is that we have to work with real users. so, we cannot come and say, ﬁyes, we would like tohelp you but can you change your distribution a little bit? i mean, you knowšif the distribution would have been alittle bit different, it would have been much easier for us.ﬂ the same thing is true for us as computer scientists.the other part of it is really this business of practicing what you preach. if you develop a method or you develop atool, if you are not actually using it and figuring out how it works and when it works and when it doesn't work, then ithink there is very little hope that it will be accepted by an end user.to remind you, the highenergy physics community, as an example, was very selfcontained until recently. i thinkwhat you have been hearing here regarding statistics, and what we have experienced on the computer science side, isthat they realized that they need some help or can use help.this has not been an easy process, again, and we have to develop things that actually work. if we come andprovide something and say, this is the solution, and it falls on its face, the next time doing it becomes very, verydifficult.now, the good news is that today what we are doingšdistributive computing gridsšall this is very fashionableand, therefore, we are getting a lot of support. it is good news and bad news because, when something is veryfashionable, everyone is stepping to the plate, and people who have been doing a lot of very different things suddenlyare experts in distributive computing, and that can be very dangerous.data grids (or, a distributed computing view of high energy physics)116statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.so, sort of one message i want to leave with you, something we have learned over the last 15 years, is that whenwe build a distributive computing environment, we have to sort of separate it into three main components, one thatrepresents the clients, the consumer. this is not the trivial part of the equation. then, there is the other part thatrepresents the resource. again, this is a very intelligent component, especially when we get into all the political issuesof distributed ownership.what we have realized has worked extremely well is that we interface these two with a framework that we callmatchmaking, that allows providers of resources and requesters of resources to sort of come together in the way thatwe, as humans, come together.i think one of the big messages that we have as a result of all of our work is that we should look at thesecomputing environments more as communities rather than a computing system, where nanoseconds and picosecondsare what matter. the actual activity of why are you part of this community and what do you contribute and what doyou get out of it becomes much more important than the latency of the network.in the mid1990s, the grid concept came to the front of our activities, and this is the bible, and we are working onthe new testament now, the second version of the grid book.the main message there, we are trying to create this pervasive and dependable computing facility, and as i said,there is a lot of activity on this. i can give you another talk to two hours to show it is related to distributive computing,and there are a lot of concepts that are sort of resurfacing, and there is a lot of stuff that goes back to what wedata grids (or, a distributed computing view of high energy physics)117statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.have been doing 20 and 30 years ago.so, if you look at this notion of the grid, there is the user who wants to do some analysis, who wants to createsome kind of a histogram or scatterplot or what have you, or ask a question, or generate a million events, and i willshow you a little bit about that.that leaves the fabric that has to do the work, and in between there is this thing that we call the grid of themiddleware, that has to bring them together.actually, following the division of labor that i showed you earlier, we have been doing, on our side, and havebeen sort of contributing and moving it into the highenergy physics community, is to comment and say there is whatis called the globus tool kit, which is the interdomain capability that allows you to cross domains and create them intoa single domain, and then we have taken the technology that we have developed and attached it on one side to thefabric and on the other side to the computing environment, and this is what we have to add to it in order to make thewhole thing work.now, what i will do today is focus more on the application, user side, because i think that this is much moreapplicable to what we are talking about here. one of the questions that we have to ask at the end of it is, how do wewrite applications, how do we develop interfaces that can actually take advantage of these capabilities. that, i think, iswhere some of the algorithmic and software development issues are coming into play.now, we are even in a much worse situation than what you have seen earlier, because you can see only thehandles on that side, but when we are getting into a grid effortšin this case, the particle physics data grid which is adoe activity, we have to deal with several of these experiments, and we have to deal with several softwaredata grids (or, a distributed computing view of high energy physics)118statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.contributors, in order to make the whole thing work and in order to understand what is in common and what is different.this is, as i said, one example and it is chopped on the left. by the way, if you are looking for the use of compus,logo generation is a huge industry these days. there is a continuous stream of new logos, and i am sure there is a lot ofmoney in that also.now, the hardware is also pretty challenging. this is one grid, in terms of infrastructure. this is the thoroughgrid.i think it is a $45 million or $50 million effort of nsf.part of it is, okay, if you want to do highenergy physics calculation, here are a few flops that you can use and afew pedabytes to store the data. the question is, how do you get to that, and how do you get your results back?let me try to generalize or abstract and say, this is the way we look at it, and that is the way we are trying todevelop tools to help highenergy physics. so, the two examples that i have, one is from highenergy physics and theother one is from the sloan digital sky survey, which is more in the astronomy side.data grids (or, a distributed computing view of high energy physics)119statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.so, this is a simplified view of what is going on. these are the phenomena down there that can either be describedby the detector or described by a set of models. that is sort of the sort of information about the real world. then it goesthrough this pyramid of data refinement and, eventually, at the end, what they wanted is statistics.we have to deal with the flow of data up, and we have to deal with the issue of a query going down. when ascientist comes in with a question about statistics, which is basically, in most cases, ﬁgive me a histogram of thesevalues on these events, under these conditions,ﬂ then it is going down.one of the challenges here is how far you have to go down depends on the question. so, the projection question ischallenging because you might get it from the top or you may have to go down. as you saw earlier, the amount of datainvolved here are huge and, in many cases, the computation needs is huge as well.what makes this more interesting is that all this is happening in a distributed environment. the distribution is intwo dimensions. it is what we are used to, which is the physical distributionšnamely, we have networks, we havemachines that are interconnected by these networks, and these networks can be local area and these networks can bewide area, or can be wireless, or whatever they are.so, the machines are in different places. they are heterogeneous and all these kinds of wonderful things which,by the way, brings up one of the biggest challenges of this environment, is if you don't have a single reboot button. it isso wonderful when one of these wonderful machines misbehaves. you reboot it and you bring it back to a stable stateand you can keep going.when you have distributed distribution, you can really never have the system in adata grids (or, a distributed computing view of high energy physics)120statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.stable state. the other principle that we learned from the physicists, and that is the importance of time. what you knowis what happened in the past and, by the time you react on it, it is even later. so, never assume that what you are doinghas anything to do with reality.the second part, which is even more challenging, is the distributed ownership. since computing is so cheap, andso powerful, we get more and more, a smaller and smaller organization owning more and more computing power.these computing resources are autonomous, are managed by their own local software, but are also managed by theirown local administrators that reflect local policy.so, if you want to operate in this environment, you have to be prepared to deal with all that, which really meansthat you ought to take an opportunistic view of the world. you ought to go and use whatever is available for as long asit is, and then be ready to back off when this is gone, and be able to make sure that you behave according to the rules.so, what is driving the fact that it is distributed? cost, obviously. commodity computing is here to stay with us, ithink, for a long time, at least, and if we have to compute on desktops or we have to compute on play station orwhatever, the computational needs, as you saw them earlier, are so huge.i heard earlier that they would like to have threefold monte carlo. i heard earlier that you would like to get 10fold monte carlo. some people tell me that you can barely get today onefold monte carlo, in many cases.so, if we can make it available to the hep community and the othersšby the way, the biologists are coming inwith even more than that, i just talked earlier this week with one biologist who wanted to do pairwise comparison of120,000 proteins that exist today, and another 300,000 that are coming down the pike later this year. so, we have to goafter commodity, whatever it is, and it is distributive in nature.the other part of it is performance, because we need to be close to where we collect the data. we want to be atcern or we want to be at slac where the detector is, we want to be close to where the scientists are, in order to dothe analysis.it is not only the performance, but it also brings in the availability issue. why? because if i have the data here andi have the computing here, i am in charge. i touch the mouse, all this is mine, even if it is not as big as it could havebeen if everyone would have gone to the single place.that is also the politics. so, you have these international efforts and, for example,data grids (or, a distributed computing view of high energy physics)121statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.the united kingdom now has invested a huge amount of money in e science, and they want the resources to be in theunited kingdom. all of a sudden, babar has more of a presence in the united kingdom, not because that is whatmakes sense, but that is where suddenly the money is. so, that is the politics.the other one is the sociology. i want to have control, i want to show that i am a contributor, i am doing it. if wedon't understand it from the outset, and we really build everything around itšthis also goes back to my previouscommentsšwe have to understand how to operate in a large collaboration.it is getting even more difficult when it is interdisciplinary, when we come in with different cultures and adifferent way of thinking and we have, eventually, to solve the same problems.while we are doing computer science and they are doing physics, at the end of the day, they have to find one ofthese particles, which i have been trying to give them particles along and here it is, and let's forget about all these otherworlds, but they don't want my particles. they are looking for another one.now, what can we bring to the table? there are a lot of principles we have learned over the years in computerscience. i think one of them, which is actually coming from the database world, is the value of planning. one of thenice things about databases is that you come in with a logical request. you don't know anything about the physicalimplementation of the system or the data. then, you let somebody do the planning for you. there are a lot ofchallenges in doing the planning, and there are a lot of challenges in convincing the user that what you did is right.for example, a typical physicist will not trust any piece of software. coming to them to say, ﬁtrust me, give me ahighlevel request and here is the histogram,ﬂ they will say, ﬁno, no, where was the byte, when was it moved, bywhom was it generated, by which operating system, which library?ﬂ šall these kinds of things, because they havelearned over the years that they are very sensitive to it.now, whether it has to be this way or not is an interesting question which i think has also statistical implicationsbecause, on the one hand, everything is random. on the other hand, you want to know exactly what the voltagedistribution of the machine was when you ran it.so, that is getting intošthe second item here is data provenance. there is a lot of work today in understanding, ifthis is the data, where did it come from? how much do we have to record in order to convince the young scientist thatthis data is valid, or thatdata grids (or, a distributed computing view of high energy physics)122statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.these two data sets are the same? in the database world, whether it has materialized or not is left to the database, andthat is connected to the other concept that i have here, which is virtual data.we have the whole it output, which is called griffin, which is dealing with virtual data, which is again coming tothe end user and saying, ﬁtell me what you want and i will do it for you. write it in sequel, and i will decide what joinmethod i should use and where i should use it, and whether i materialized it earlier or not is up to me.ﬂ there is a hugetrust issue here, when you allow somebody else to do the planning for you.now, the main issue in the planning is to figure out what is the research requirement. as i point out later, a hugequestion is how much is the space requirement of an operation, because this is sort of a bottleneck and a source ofdeadlock, if you cannot write your data or bring your data in when you are trying to really run a large operation.there is this question of when to derive and when to retrieve. if you want a million events that were simulated,what is cheaper, to go and retrieve them from cern, or to rerun it on your local farm. now, if you can guarantee thatthe two are statistically equivalent, then maybe i should reproduce them on the local farm, rather than wait for the datato be shipped from cern.we have not solved, i think, even in databases the question of when to move the data and when to move thefunction. given the amount of data involved and the amount of resources, we are facing this problem on a larger scale.where do we do it, how long do we wait for the data to come, and where do we move the data? can we push selectiondown to where the data is?for the issues that are coming from the database world, from computer science, we have to apply them not in thestandard way, which is the way we have been doing it all along.the other part of it is that we really have a huge data workflow problem here that we have to manage and control,because if we screw up, it can be very bad.if you really want to live in this brave new world where we have grids and we have computing and we can dothings everywhere, then we have this continuous movement of data, of computing. we are talking about tens orhundreds of thousands of things that we want to do in this environment. if we don't keep track of what is happening, orsomebody really misbehaves or loses control, we can grind the wholedata grids (or, a distributed computing view of high energy physics)123statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.system to a halt.so, when we move the data from the phenomenon to the data, when we measure it, so we have a real timeconstraint if the data is coming from an instrument. whether it is a telescope or a detector, we have to make sure thatthis data goes in and we cannot lose anything, because this is real data.we also have to deal with the production of data coming from the monte carlo production, which is, again, astream of data coming in.i think many of us have focused on the problem of how to optimize read. the problem of how to maintain apipeline where there are rights and data has to go in is still an open question.on the web, we are all focusing, again, on how can i get the document quickly. we are not dealing with how do iinject a lot of data into a common space.now, when we are doing the data to data, then we have this multistage, where what we are doing is, we are doingfeature extraction, we are doing indexing, we are extracting metadata, we are doing compression. it is basically thesame, depending on how you want to look at the output of it.we have to deal with all these stages. we have to keep track of them. we have to know how the data wasproduced, the data provenance, and we have also to record how it is done, so that if we want to redo it on the fly, wecan do it automatically. in the end, what we have to do is, again, we have to select, we have to project and we have toaggregate.now, the selection may involve a lot of distributed patches. so, i have to figure out where the data is. the indexcan tell me where it is, but it is distributed all over. maybe some of it has to be reproduced, but eventually, i get thedata. at what level i project, again, it depends. sometimes i want an attribute which is in the metadata, and sometimesi have to go deeper, even into the raw data, in order to get the attribute that i want.so, the typical approach that we have, that we have sort of the whole thing and we do the selection and then theselection. again, we need something that is more of the semijoined structure, that we look at the attributes and what isgoing on, and then we go to the real couples, and the real couples may be very deep in the hierarchy and may requirequite a lot of computing to get the data out.so, let me try to give you sort of a simple example of what is involved in doing what i would view the most basicoperation on the grid, and try to make it abstract.data grids (or, a distributed computing view of high energy physics)124statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.let's assume i have an x. i want to apply on it an f. i get the result, which is a y, and i want to store it somewhere,in location l, which can be my screen or can be a cern archive. i don't care, but eventually, i have to park itsomewhere. what i think we have to keep in mind here, which traditional computing, definitely high performancecomputing, has ignored is that moving this data in and out is an integral part of the problem, not just computing.so, getting a very fast computation on a highperformance machine, but then waiting several weeks to get the dataon and off the machine is not the solution here.so, we have to bring in the data placement activity as part of the end to end solution. here are sort of the six basicsteps that we have to carry out in order to do this y equal f effects or 2f.so, first of all, we have to find some parking space for x and y. as i pointed out earlier, how do we know how bigx is? that is relatively easy. how big is y is getting even trickier, because it can depend on a lot of internal knowledge.then we have to move it from some kind of a storage element to where we want to actually move it to move x. thenwe may have to place the computation itself, because the computation itself may not be a trivial piece of software thatresides anywhere in this distributed environment. then, we have the computation to be done. then, we have to movethe results to wherever the customer orders us and, in the end, we have to clean up the space.just doing this right is tough. i can assure you that you don't do it right, even today, on a single machine. howmany of you, when you open a file or you write to adata grids (or, a distributed computing view of high energy physics)125statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.file, check the return codes of the write, whether it succeeded or not?i am sure that most of your applications will die if the disk is full, and it will take root intervention, in many cases,just to recover it. we cannot afford it in a distributed environment because it has to work in an autopilot with a lot ofapplications.so, what we really have here, if you think about it, it is really a dag, a simple dag in this case, although ashishkabob. do this, do this, do this, do this.keep in mind that we have to free the space, even if things have failed in between, which creates some interestingchallenges here.this has to be controlled by the client, because you are responsible for it. somebody has to be in charge of doingall these steps and, again, you can look at if, if you really want to, as a transaction that has to go end to end.i think i am sort of running out of time here. here is a list of challenges. how do we move the data? the closestwe can get to it is a quota system on some machines, but even that doesn't guarantee you that you can actually write thedata when you need it.data grids (or, a distributed computing view of high energy physics)126statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.so, i already talked a little bit about it, because i want to move faster to the examples to show you that we canactually do something with all that, but the approach that we have been taking is, first of all, to make data placementfirstclass citizens. that means that when you write an application, when you design a system, make sure that gettingspace, moving the data, releasing it, is a clear action that is visible from the outside, rather than buried in a script thatnobody knows about it and, if it fails, it really doesn't help us much. we have to develop appliances that allow us touse a managed storage space in this environment in a reasonable way, and then create a uniform framework for doing it.so, let me show you what we have been able to do so far. the first one is, how we can generate the simulatedevent, with millions and millions of simulated events.data grids (or, a distributed computing view of high energy physics)127statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.this is sort of the highlevel architecture of what we have deployed out there. so, the application is generating ahighlevel description of what has to be done.this is getting into what we callšthis is the directed acyclic graph manager that is responsible for controlling it.now, for some of you, if it reminds you of the old days of jcl, yes, it is like jcl, at the higherlevel, but then itgoes to what we call condorg, which is the computational part, and we have a data placement schedule that uses theother tools to do that.so, i am not going to talk about that since i have four minutes, and justšthedata grids (or, a distributed computing view of high energy physics)128statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.physicists have way too much free time on their hands. so, they can generate this wonderful animation.so, here is the way it works. we have a master side. impala is the cms. they have an even bigger detectorthan the babar detector, that is generating the events themselves. this is the master side. then we have all these othersides where we send out the computations, get the data back, publish it, move the data in and we keep going.data grids (or, a distributed computing view of high energy physics)129statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.basically, each of these jobs is a dag like this, and then we move them all to larger dags that include somecontrols before and after, and that is the way it works.so, here is an application that this graph is, after 900 hours, this is hours since it started. so, this is one of thecms data challenges, and this is the number of events that we have to generate. so, a job is two months, and it has tokeep going, and we have to keep generating the event. this is what we have been generating using that infrastructure.let me show you another example of what happens when you have to do it in a more complex environment.data grids (or, a distributed computing view of high energy physics)130statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.that is where we are putting in the planning. so, it is the same architecture as i showed you earlier, but there is aplanning box there that is trying to make a decision on when to do it, how to do it, and what are the resources weshould use for this.this is based on work that is actually done at argonne national labs and the university of chicago as part of thegriphyn project, and that was because of the data system that includes higherlevel information about the derivationsthat are formally defined and, from the derivation, we create transformations, which are the more specific acts thathave to be done.this is creating the dags, but they are not being executed by the architecture. as we go through, we go back tothe virtual system where we come and say, tell me, now, what to do.data grids (or, a distributed computing view of high energy physics)131statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.so, this is what we have to do there. we have to aggregate information. we have all these images. we have topull them together. we have to create distributions, as i showed you, of galaxy sizes or whatever it is.so, this is sort of the dag that is going up rather than going out. this is an example of one job. this is anexample of a collection of these jobs that we are actually executing. each of the nodes in this dag is a job that can beexecuted anywhere on the grid, and this is where we start.data grids (or, a distributed computing view of high energy physics)132statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.this is the computing environment that we use to process the data, and these are the statistics.i will leave you with that, that if you want to write applications that work well in this environment, (a) be logical.the other one, you have to be in control, because if you don't get the right service from one server, you should beprepared to move on to somebody else, if you want to use it effectively. everyone wants lunch.data grids (or, a distributed computing view of high energy physics)133statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.report from breakout groupinstructions for breakout groupsms. kellermc nulty: there are three basic questions, issues, that we would like the subgroups to comeback and report on.first of all, what sort of outstanding challenges do you see relative to the collection of material that was in thesession? in particular there, we heard in all these cases that there are real specific constraints on these problems thathave to be taken into consideration. we can't just assume we get the process infinitely fast, whatever we want.the second thing is, what are the needed collaborations? it is really wonderful today. so far, we are hearing froma whole range of scientists. so, what are the needed collaborations to really make progress on these problems?finally, what are the mechanisms for collaboration? you know, amy, for example, had a whole list ofsuggestions with her talk.so, the three things are the challenges, what are the scientific challenges, what are the needed collaborations, andwhat are some ideas on mechanisms for realizing those collaborations?report from highenergy physics breakout groupgroup two presenter: i only took a few notes, so i am trying to stall, but i am glad to see mark hansenhas arrived.so, we talked about experimental physics. what is interesting is that there is sort of a matrix in my mind of whatwe discussed. i think paul had mentioned there was a conference in durham earlier this year in march, in which therewere 100 physicists and two statisticians starting to scratch the surface of issues. there is a followup meeting instanford in september. somebody named brad efron is the keynote speaker. so, presumably, there will be at least onestatistician.i think what was clear is that, sort of in the current context of what experimental physics is doing, there is a list ofvery specific questions that they think they would like answered. what we had discussed went beyond that. we werereally looking, gee, if we had some real statisticians involved, what deeper issues could we get into.i think that, after a good round of discussion for an hour, we decided there were probably a lot of really neat, coolthings that could be done by somebody who would like to have a career changing event in their lives. alan wilkes isfeeling a little old, but he thinks he might be willing to do this. i think on the good note is what you have, which is oftenšon another good notešcollaborations are clearly in their infancy. there are only a few statisticians in the world, issort of my observation. so, there is a reason why there are not a lot more collaborations than there should be, perhaps.if you look at doug's efforts in climatology, there are really some very established efforts. if you look at astronomy,you have had some efforts in the last four years that have really escalated to the next level, and i think physics is highon the list of making it to the next step. i think there are probably a lot of agencies here in this town that would helpmake that happen.the thing that gets more to sort of the issue at hand here is that there are a wholereport from breakout group134statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.lot of statistical things involved in what are called triggering. so, things are going on in this detector and the thing iswhen to record data, since they don't record all 22 terabytes a second, although they would like to, i guess, if they could.the interesting statistic that i heard was, with what they do now, they think they get 99.1 percent of theinteresting events among all the billions of ones that turn out not to be interesting. so, 99.1 is perhaps not a badcollection ratio. so, much of the really interesting statistics that we have talked about is sort of the offline type. inother words, once you have stored away these gigabytes of data, there are lots of interesting patternrecognitionproblems and stuff. sort of on the realtime data mining sort of issue, we didn't sort of pursue that particular issue verydeeply. what struck everybody was how timesensitive the science is here, and that the way statisticians do science issort of at the dinosaur pace and the way physicists do it is, if they only sleep three hours a night, the science would getdone quicker, and it is a shame they can't stay up 24 hours a day. there is lots of discussion about magic tricks to makethe science work quicker.all in all, i think the conversation really grew in intensity and excitement for collaborations, and almosteverybody seemed to have ideas about how they could contribute to the discussion. i think i would like to leave it thereand ask anybody else in the group if they wanted to add something.report from breakout group135statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.daryl pregibonkeynote address: graph miningšdiscovery in largenetworksabstract of presentationtranscript of presentation and powerpoint slidesbiosketch: daryl pregibon is head of the statistics research department at at&t shannon research labs.his department is responsible for developing a theoretical and computational foundation of statistics for very large datasets. he has been with the labs for over 20 years. he has interest in and has made contributions to the three main areasof statistics: modeling, data analysis, and computing. his specific contributions include data analytic methods forgeneralized linear and treebased models, incorporating statistical expertise in data analysis software, and designingand building applicationspecific data structures in statistical computing. he is very active in data mining, which hedefines as an interdisciplinary field combining statistics, artificial intelligence, and database research.dr. pregibon received a phd in statistics from the university of toronto in 1979 and an ms in statistics from theuniversity of waterloo in 1976. he is a fellow of the american statistical association and has published over 50articles in his field. he was coauthor of the best applications paper, ﬁempirical bayes screening for multiitemassociation in large databases,ﬂ at knowledge discovery and data mining 2001 (kdd2001) and the best researchpaper, ﬁhancock: a language for extracting signatures from data streams,ﬂ at kdd2000. he is the past chair ofcats (committee on applied and theoretical statistics, national academy of sciences). he was cochair of kdd97and has been either a special advisor or member of the kdd program committees for the past 3 years. his is cofounder of saias (society for artificial intelligence and statistics). currently he is a member of cnstat(committee on national statistics, national academy of sciences), a member of the sigkdd executive committee, amember of the steering committee of ida (intelligent data analysis), and a member of the editorial board of datamining and knowledge discovery.keynote address: graph miningšdiscovery in large networks136statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.abstract of presentationgraph mining: discovery in large networks daryl pregibon (with corinna cortes and chris volinsky), at&tshannon labslarge financial and telecommunication networks provide a rich source of problems for the data miningcommunity. the problems are inherently quite distinct from traditional data mining in that the data records,representing transactions between pairs of entities, are not independent. indeed, it is often the linkages between entitiesthat are of primary interest. a second factor, network dynamics, induces further challenges as new nodes and edges areintroduced through time while old edges and nodes disappear.we discuss our approach to representing and mining large sparse graphs. several applications intelecommunications fraud detection are used to illustrate the benefits of our approach.keynote address: graph miningšdiscovery in large networks137statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.transcript of presentationmr. pregibon: i guess i want to think of the words to apologize. i am listed as a keynote speaker, but i thinkany of the presentations we heard this morning, and probably this afternoon and tomorrow probably would qualify forthis lunchtime keynote speech. the only thing i can think that it is marked as keynote is maybe it has caught thefunder's eyes the most. so, with nsa being the major funding agency, this is probably an area that they are quiteinterested in. that is my excuse for why someone has named this a special talk. otherwise, i think what we will see isthis morning, this afternoon and tomorrow, there are many different shapes and forms for this large data problem andlarge data streams.so, what you are going to hear is something completely different than what you heard this morning. this isn't bigscience. this isn't science at all. in fact, i am a bit jealous. it is nice to have science that you can at least hope toexplain what is going on, in some phenomenon in your large data.i am not sure that what i am going to talk about today is going to make any contribution to big science. the onlything i could think of is, i might donate my laptop to the scientific community because i think it holds some newphysics in it. it is very sensitive to the characteristics of electrons and, if i just touch it the wrong way, it is going to dieand reboot. it only weighs seven pounds. so, it is not a couple ton accelerator, but i really think there are some secretsin here that the physics community might be able to unlock.keynote address: graph miningšdiscovery in large networks138statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.let me get on to the topic. we are going to talk about network data. i will give you a brief introduction of whatsort of network data i want to talk about. then i want to go into a little bit of a spiel where i want to show you somenetwork data. the reason i am doing that is to basically guide how we think about such data, and you are going to findout that, throughout the talk, there aren't a lot of statistics per se in the talk, but i think you will be able to see howstatistical thinking guided a lot of sort of the ideas and the things we migrated to. by virtue of describing this networkdata, ideas of a dynamic graph pop up. so, we will talk a little bit about defining a dynamic graph, necessarily how weapproximate that, and sometimes how we postprocess that and then use it in some applications.so, let me jump right in to say what the topic of my talk is versus other sorts of network data that might exist. so,generally speaking, with regard to networks, you can sort of think of static versus dynamic. i will think of networktypology as a somewhat static beast. even though the number of switches in a telephone network and the number ofpieces of copper or fiber connected to them, they do change through time, they are relatively static.they are, in many cases, physical objects. in the cases of web pages, maybe they don't have a physical entity oridentity, and web links come and go, but they don't come and go as fast as other sorts of things. by that i mean, thething that i am going to be talking about is actual traffic on a physical network. that is the sort of data that makes upthe topic of my talk. so, the things of interest here are the entities that are using the physical network. they are usingthis physical network to conduct something š communication, finance, commerce, it could be transportation.these transactions involve other entities. they could be voice data transactions, investment transactions, andretail purchases. these things are totally out of the sort of control of the person collecting the data.keynote address: graph miningšdiscovery in large networks139statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.now, the data that are associated with these transactions isn't the content of the transaction. largely speaking,they are just a few bites of information that describe some salient characteristics of the transaction, such as the date andtime stamp that it took place, possibly encoded in the metadata is the location for the transaction, maybe somethingšdefinitely in my case, and i think many of these casesšsome identifiers for the transactors, and often some element ofsize. it is the number of bytes involved in the transaction, the number of minutes the transaction lasted. maybe in afinancial or a commerce application it could be the size in dollars of the transaction. so, it may not say what waspurchased, but you would know the dollar amount associated with it.now, traditionally, these data aren't new. they have been around forever, and they have been around for a darnedgood reason, because people make money, or this captures records that are used for billing. this is the thing thatactually paid, and continues to pay, for the collection of such data. what we are going to talk about is trying to usethese datašthis morning we talked about level 0, level 1, level 2 data, and for this talk, you might think of these aslevel 0 data.in fact, as the fields that i have described them, they are probably level 1. the level 0 data that we get off thenetwork are pretty ugly. it is a raw, unstructured form called ama format that we process and put into a differentformat called level 1. we have our own name for it.as we go up the food chain, i think what we try to do is add value as we go up the levels. so, this talk is almoston, as you go up, and with large data sets, you do have to filter your data. if you can think of adding value as you go upthis level stack, that is kindkeynote address: graph miningšdiscovery in large networks140statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.of what we are talking about, or one of the messages i want to get across to you in my talk.maybe the lesson here is that each record has just a few bytes of information, who talked to who, when and howlong. so, they are pretty trivial. if you put together a couple hundred million of those a day, and you do that day in andday out, you can get a very good picture of what is going on in your network. so, that is kind of the message here.i think i used graphs in the title of my talk, and anyone who is associated with network data, this is a nobrainer.graphs are a convenient representation of the data and, in the applications that i am going to be talking about, we aretalking about call graphs.so, every node in a network, the network identifier is a phone number, and the edges we are talking about arephone calls or aggregate summaries of phone calls between telephone numbers.some of the questions you might ask of such data, are there interesting nodes in your network? how close are twonodes to each other, not in geography, but in sort of the network topology, where the network is not the physicalnetwork, but it is the communication network. other sorts of nodes or other questions that are interesting to meconcern the temporal nature of the network. is the node that i am looking at today behaving similarly to the way itbehaved in the past. then you might actually talk about subgraphs of the large graphs and how you might capture themand classify them.so, to try to motivate some of the ways we think about the data, let me show youkeynote address: graph miningšdiscovery in large networks141statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.some actual data. it is going to be from what we will call a lowvolume service. we get data all the time. so, it is notquite an accelerator application, but people are making phone calls and we bill them for their phone calls. so, we havea continuous stream of data.this is going to be a stream of data from one of our services. i picked it up for 22 weeks. what i am going to do isjust give you an idea of the size, shape and scope of network data. let me just go through a series of graphs, andhopefully you will be able to see some of these features.so, this is a plot. it is a scatter plot and, for each of the 154 days in the study period, i have a point. here is apoint, there is a point, and there are a bunch of points. that point contains the information on the number of nodes thatwere active in this service on that day, and the associated number of edges for that day.so, generally, on a weekday, we have roughly half a million transactors on our network, and they are transactingwith two others. so, there are about a million edges a day, and about a half million nodes a day. on the weekends, wesee it quite different, maybe only a third of a million nodes, and half a million edges.there are some outliers that correspond to special events, holidays, etc. so, this gives you an idea, a little bit, ofhow many transactors we are seeing on the network, how many transactions, and also gives you a little hint of thesparsity of the graph.if you have a graph that has n nodes, roughly, there are n2 edges. this is hardly n2. i mean, it is a factor of two.so, the graphs that we are talking about, and are going to talk about, are very, very sparse, and that is a good thing, orcan be a good thing.keynote address: graph miningšdiscovery in large networks142statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.this gets at the sparsity in a slightly different way. this is a plot of the number of edges versus the number ofdays on which they were observed. this is a typical plot you might see in an analysis of text and other things. mostedges occur only once. so, over a million of these edges, you only see once, over about half a million of the edges youonly see twice over the study period. way down here, you see some of the edges every day.so, this gives you a little sense of the temporal nature and how frequently you observe things. these are the samedata presented slightly differently, and it just shows you the cumulative frequency.so, 95 percent of the edges have occurred on six or fewer days, of a 154day study period. so, that gives you anidea of the episodic nature with which these edges come and go.keynote address: graph miningšdiscovery in large networks143statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.this is a different way to view the data. this talk is about the lifetime of the edges. so, we have an edge and, ifwe have only seen it once, its lifetime is a day.some edges we have seen twice. so, we say, have we seen them today and tomorrow, or did we see one the firstday of the study period, and the second time we saw it was the last day of the study period.what this display is meant to convey is that roughly, say, 75 percent of the edges came and went within a week.eighty percent of the edges came and went in about two weeks. eightyfive percent came and went within a month,and 90 percent of them, the first time we saw them and the last time we saw them was about two months, which is lessthan half the study period. so, these things come and go and you may never see them again, and their half lives aregenerally very short.this is one of my favorite plots, and someone criticized me for saying it is my favorite one because it takes solong to explain, and a good plot, you shouldn't have to explain it, but let me make a crack at it. what i want to get at inthis plot is just the fact that nodes come in and go out through time, and the numbers are pretty staggering.what i did was take the entire study period, and i got the complete list of network identifiers. there were about6.5 million of them. then i said, okay, let's start at day one, or how many are unique on day one. well, they are allunique on day one, because that is where you are starting. on the second day, how many new ones did you see? on thethird day, how many new ones that you hadn't seen the first couple of days and so forth.after a while you sort of get over this initial starting period and you get to a sort of steady state. i have fitted atrend line to this steady state up here. the slope of that corresponds to the fact that every day we see 16,000 new nodesin our graph. then, i did the same going in reverse. i said, okay, we have all these nodes and now the steady state isgoing to be the reverse process.when was the first day of the study period? for how many nodes was that the last day i saw that node? thesecond day of the study period, for how many nodes was that the last day that i saw it, and fitted a trend line to thebeginning of that, because the truncation for that will occur over here. generally speaking, the slope of that trend lineis that we are losing 27,000 new nodes a day. so, this is not a good business to be in.again, what we are trying to capture is, nodes come in and go out and there are lots of them. it is not trivialnumbers. so, you have to take this into account.keynote address: graph miningšdiscovery in large networks144statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.audience: [question off microphone.]mr. pregibon: they haven't converged yet. the question was, do these lines converge? my response was theyhaven't converged yet.audience: [question off microphone.]mr. pregibon: the second question was, what would happen if i had run this for more weeks. i believe whatwould happen, i would see the same slopes, and i think the lines would be closer to intersection than they are right now.this business, by the way, i don't think it is that proprietary. it is our calling card business. calling cards arebuying out by virtue of cell phones and prepaid cards. so, this is a business that we are essentially harvesting. we arenot investing in it, people aren't using it, and this is the rate at which that business is, you know, going in the toilet.so, the final thing is a similar plotši guess it is not the final one, there is another plot after thisšthe same sort ofbusiness on the edges. what is a little interesting to me is that the attrition of edges is nearly linear. i can't explain that.this is the addition of edges.the numbers here are also big. the slopes of these lines, we are seeing 300,000 new edges a day that we havenever seen before in the steady state, and we are losing 400,000 edges a day that we will never see a again. that givesyou sort of an idea of both the node set and the edge set.this is the final graph in this set. this is speaking toward, again, the connectivity or the sparseness of these data. ihave done it in a cumulative plot and i think i have triedkeynote address: graph miningšdiscovery in large networks145statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.to capture the 90th percentile. so, 90 percent of the nodes on this graph have an in degree of eight or fewer edges, andi think 90 percent of the nodes on this graph had an out degree of 13 or fewer edges, and this is over the entire studyperiod.so, that is, again, a bit of a background for, when you are in this business of studying graphs or traffic ortransactions on a network, that is the type of volatility that you have to deal with.now, let's get into how that data motivated sort of the structures we used to capture this graph. i keep saying thisgraph as if there is a graph. it is changing all the time. so, therein lies the problem. we have to come up with adefinition of what we need.so, i will introduce a microsoft operator. i found that in the symbol table. it looks like a little plus sign. that is agraph operator.basically, all we are going to say is that we can take two graphsšgraph one and graph twošand combine themor take a linear combination of them, such that the resulting graph will have the union of all the nodes and edges ofgraphs one and two, but that we are just going to take a linear combination of the weights along those edges to find anew graph, or graph three.i will show you how that is used on this viewgraph. again, there are many different ways to define what we meanby the network graph. i am putting most of these up here as a straw man to say, none of these are the ones we chose,but these are allkeynote address: graph miningšdiscovery in large networks146statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.legitimate choices for legitimate purposes.again, you can just define the graph to be the network traffic you see on some period, say, today. that could bethe graph you are interested in. given the volatility that we see, 300,000 new edges today that we didn't see, and400,000 are going away, that graph may be viewed as being too narrow for some applications, of too high variance.the sort of complement of that is to say, don't throw anything away, and let your graph just grow over time anddon't sort of delete anything, or just keep adding stuff to it. that possibly, for some applications, is too broad. if youare using this graph for inferential purposes, you may not want traffic from a year ago to affect inferences on yourgraph today. for some applications, you may exactly want that. for some of the things we do, we don't want that.something closer to what we want is this. we want the graph to be defined as what is happening recently with thenetwork. you can think of this as simply a moving window. so, we have traffic coming over the network. we aregoing to window this.the only problem we have with this is that, to maintain this, we have to maintain too many little graphs. so, inorder to update this graph tomorrow, we have to throw this guy away, and then add another one on the other end. then,depending on how wide this window isšif it is a monthšwe are going to manage 30 graphs all the time.so, it is not a surprise to think of where i am going with this. i sort of said, well, we like this windowing operatingbut we don't like to pay for the storage and maintenance of all these graphs.well, one way to get a benefit of that is to simply do an exponential average where we take a convex combinationof the graph like it was through yesterday, and then just add in the graph for today, using sort of a factor of $, which isgoing to sort of guide how wide of a window you are looking at for your application. again, just doing the simplealgebra, you can expand the recursion and basicallyšthis isn't anything new š most people who study this type ofdata, i think, this would be a natural definition for them.the benefits of this, from the recursion you see that recent data would have the most influence. from acomputational point of view, it is great, really because only one graph needs to be stored. you have yesterday's graph,you put today's data in, and you immediately have the new graph for today.keynote address: graph miningšdiscovery in large networks147statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.that parameter, $, is, again, adjustable. i just show you some values of $ that we use for different applications.if we said $ equals about .85 and you follow that curve down, for our applications, if we do daily processing, thatmeans an hour phone call will last, in our analysis, for roughly about a month. so, it won't wash out for a month.for other values of $, i think that is a $ of .4, if we set the value of .4, that one hour call will wash out in about aweek. again, it is completely adjustable. it really depends on your application. in telecom, phone numbers, when theygo out of existence, they are reassigned. typically, they can be reassigned within a month. so, we typically use a 30day value and a $ of .85. so, we don't blend activity on the new user of a phone number with activity on the previousowner.so, let's talk about the graph again. so, that captures the dynamic nature of our graph. now, how are we going torepresent it?one way that we like to represent the graphšand you will sort of see the applications that motivate thisšis in aconstructive sense, and we are going to think of it as a union of all subgraphs, where the subgraphs are, for every nodein the networkšso, this is important here, and we will get to this again lateršfor every node in the graph, we keep theset of edges going out of it or coming into it, the set of edges going out of it, and that defines a diameter one subgraphcentered on that node, and we are going to do that for every node. then the graph, by definition, is just a union of allthose subgraphs.now, there is a big penalty here, because i have stored every edge twice. if i callkeynote address: graph miningšdiscovery in large networks148statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.amy up, that edge is going to be in my subgraph, and it is also going to be in amy's subgraph. so, this is prettyredundant.the main reason we do this is, it allows for fast expansion of subgraphs centered on every node. by that i mean,in a lot of the applications we are interested in not just who called me and who i called, but we want to take that out.for every one of the people i called, who called them and who did they call. by building this redundant structure,it is very easy to go from this down to this. so, it is very easy to traverse this data structure to build subgraphs ofarbitrary depth, and literally within a second. so, that is the main reason why we like this constructive representation.we don't use that, though, in practice. what we actually do is approximate the graph. the nature of theapproximation kind of builds off this constructive definition.so, we are going to approximate our large graph by sacrificing something. now, i am not going to sacrificeinformation on the node set. i want information on every node in my graph. what i am going to sacrifice is some of theedges. so, i am only going to retain edges in my graph that are sort of big enough or important enough.that can be defined by the user, but i am going to truncate my edge set, say i am only going to keep the k edgeswith the largest weights on them going on. the k edges with the largest weights going out, and now i can define mysubgraph now, centered on every node, and it is going to be approximate. then i can define the big graph to be theunion of all these subgraphs.now, through sleight of hand, this graph isn't necessarily redundant. the reasonkeynote address: graph miningšdiscovery in large networks149statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.for that is sort of asymmetry. amy might be very popular. when she calls me, i might fall out of her subgraph becauseshe only spoke to me for a minute, where she talks to her family members and friends for quite long. i might beunpopular, so her edge may stick around in my subgraph for a long time. so, even though there is a possibility ofredundancy in this representation, it may not occur.the other thing we try to do in our approximation is the following. we threw away edges, but we are going to tryto maintain the total weight of the subgraph by maintaining the weight in a sort of funny node for each node in thegraph, and we call it slot other. every node will have two slot others, that sort of catch the carry over, and let me showyou how this works.this is, let's say, my subgraph. i guess i labeled it me in the middle. bill gates is responsible for rotating some ofthese names up here. i just typed them in and i must have twisted something when i put in my circles, but when i wentto type in the names, that is what it wanted to do and i couldn't undo it. so, if bill gates is responsible for doing it, it ismy responsibility for not knowing how to undo it. so, anyway, this is my subgraph.now, suppose i wanted to do a top four approximation for this. what this means is that i want to maintain onlyfour of the nodes going in and four of the nodes going out.i am going to do that by saying, i am going to drop off the nodes with the smallest weight. if we look around here,it looks like these guys have the smallest weight. so, when my boss calls, or when paul calls me, these guys are goingto be left, and when ikeynote address: graph miningšdiscovery in large networks150statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.call bill, i don't call him enough, so i am going to whack that node.what i have done is, i have replaced those nodes here with two other nodes, just labeled ﬁother.ﬂ so, they are notgoing to carry the names associated with who made those calls, or the number of calls that were in there. so, if i triedto mimic amy's animation, i can do that. so, you can see how i am collapsing several nodes down to one, and thenreplacing them with node ﬁother.ﬂaudience: [question off microphone.]mr. pregibon: so, the question is, what are the weights. so, the weights typically would be associated withsome characteristic of the transaction. so, the weight might be the dollar value of a transaction, length of a phone call,number of bytes of the connection between this ip address and that ip address. so, sorry for the confusion.anyway, the top four approximation for my subgraph, again, only maintains the top four in and out directions,plus overflow nodes that i have thrown away the labels on, but i have maintained the weight. so, my approximatesubgraph has ever node in the network in it. it has the total weight as the original subgraph. all i have done is prunedsome of the edges. so, that is the beast we have chosen to work with.audience: [question off microphone.]mr. pregibon: so, the question was, it is not really the top four because of the temporal nature of the data. icould have had something in my top four that slid off the top four but now is coming back in and would get in my topfour.that is the segue into this view graph, is how do we update this thing. you arekeynote address: graph miningšdiscovery in large networks151statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.absolutely right. if something falls off, it will go into other, and it has to fight its way back into the top four.the way it works, sort of at the atomic level, is that we are going to update this graph by updating all of thesubgraphs at the atomic level. so, we may have the node set from yesterday, we have today's calls, and we take theconvex combination of the weights, and then, when we do that, we then do the sort and only the top k nodes in that sortare retained.if you were in my top four and you then fall into other, i have got to keep calling you in order for you to fightyour way back into my top four. so, then the approximate graph, then, through time, is then defined by that operation.so, the final thing i will talk about is something that we are going to do day in and day out. as the data streams in,we are going to construct a network topology of the data streaming in and maintain that. you can think of this as kindof a database. basically, we are building what you might call a materialized view of the raw transactions, and this viewis the network view of the data that can be queried very, very fast.again, you put in a seed node and, within a second, you can get a subgraph out surrounding this node. when youabstract that subgraph, you may want to enhance it. this is just a fact of life, and reflects the fact that you data reallyaren't everything that you would like.for instance, this is something that we brought on ourselves. we threw away some stuff. so, we might want tothink about ways that, at least when we analyze data, that we account for the fact that we threw away some stuff. then,there are these two cases where we would have liked other stuff, but we didn't get it. so, you know, in the good olddays, at&t was a monopoly, so we had all the network transactions. so, our graph was complete. today, we are not amonopoly. so, there are edges that we don't observe because those calls aren't carried on our network.now, in building these graphs, for security's sake, you may want to have all these edges in, even though your datamay not have observations on these edges. so, that is a potential problem. another one is that, you know, you may becapturing intercepting data at one level in the network stack, and there are communications going on beneath that, thatyou won't see.so, in my application, at&t is primarily a longdistance company. so, i will see longdistance phone calls. if icall my parents in ohio and my brother, who also lives in my same town calls them, the at&t network data will seecalls going to my parentskeynote address: graph miningšdiscovery in large networks152statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.from each of our numbers. they won't see me calling my brother, because we live in the same town and that is a localcall.you can think of it the same in ip space. there are routers, in different parts of the internet. you may see trafficcrossing some of the routers, but you are not going to see stuff below. if you are really trying to build up networkconnectivity for all of these network elements, that may be important to you.we are exploring several approaches to enhancing the graphs. i will show you examples of both of them. one isstrictly algorithmic, the other probabilistic.so, the algorithmic ones first. what we try to do here is capture some of the richness that our data doesn't bring tous that we would like to bring back in. so, in a deterministic fashion, we will add some edges and then we will dosome pruning to get rid of them.again, through a cartoon, how we do that, this might be my diameter two subgraph, again, showing the categoryother, and then for the nodes i called, or that called me, who they conversed with.keynote address: graph miningšdiscovery in large networks153statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.i may add some edges in. so, for some reason or other, these edges weren't in my data, but i wanted to add them in.then, after i added them in, i probably have too much junk in there and i will get rid of them. one algorithm weuse to get rid of things is just running strongly connected components or some other fast graph algorithm, to basicallyprune out stuff.maybe i should say something here that is obvious to anyone who has looked at these graphs. graphs are likeclassification trees. people think of them, oh, they are simple, they are easy to interpret. classification trees, likegraphs, are easy to interpret if they are small. classification trees, regression trees and graphs are really hard tointerpret if they are big. so, pruning is almost necessary in every application, because these things get big. we seldomgo out more than a radius of two in our graphs. you just bring in too much junk. even at diameter two, you have toprune away junk.keynote address: graph miningšdiscovery in large networks154statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.the second way we add stuff into our graphs is using probability models. the class of models we are using hereare loglinear models, which are quite nice, because they give nice interpretable parameters, both for the graph overall,and also parameters at the node level. so, if you wanted to think about computing these models for every node, youcan then cluster your parameters, to cluster your nodes. it is a very flexible type of model.the way we account for the fact that we are missing data is to put some priors on some of these parameters, andyou can decide which ones you want priors on. then, once you have those, you can use an n type algorithm toestimate the parameters, and this is one of the things we are experimenting with.generally, we are at the point now for about a 200 node diameter two subgraph in the r language, we cancompute these models in about two minutes. that is an interpretive language. so, we can't do it for a large number ofsubgraphs. if we rewrote that in c, we think we could get it down to about two seconds per subgraph, but we are stillnot satisfied with it at the level of modeling that we are doing.the other nice thing about these models, as a statistician, we would like to think about the parameters of themodel. there are also the ultimate fitted probabilities. so, in the model that i put up before, we are actually modelingsort of the probability of an edge. you can think of then using the output of the statistical model as a way to prune andenhance your graph.for instance, this is actually, i think, a very small subgraph around me. i think there are only six nodes in thissubgraph. so, these are the six nodes, and these might bekeynote address: graph miningšdiscovery in large networks155statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.my observed data. if i was going to draw a graph of that data, this might be the threshold version of data that i use formaking that graph.if i fit a model to that data, this might be the probabilities associated with the edges on that model and, if ithreshold this thing, this might be the graph i plot. so, this is a way that we are going to be using to observe data, addin edges probabilistically, and then prune them back to something that we hope we can understand, and that capturesthe salient features of the data.let me, in the last five minutes or so, go over the applications and give you an idea for the volumes we are talkingabout. i mentioned before, we are a longdistance company. so, this is the data from the longdistance network.we see about 350 million edges a day. that is a reasonably big number by this morning discussion. the biggernumber is the number of nodes that we see through time, nearly half a billion nodes through time. so, it is a pretty biggraph when you look at it through time. in our work, we tend to use the top nine approximation. that is for historicalreasons. we have been changing that over the past year.i will give you an idea about the sizes of these materialized views. each direction, inbound and outbound, areabout 7 gig. it takes us about two hours to process the data daily. if we wanted to scan each of these maps and computesomething for each, it takes about 20 minutes. it is easily under a second to expand a diameter two subgraph for anynode.when we want to do the enhancement, flavor it and then render it on someone's web site, it is basically about 10seconds from ﬁgive me the informationﬂ to seeing the subgraphs. those are some of the operating characteristics thatwe are dealing with.keynote address: graph miningšdiscovery in large networks156statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.keynote address: graph miningšdiscovery in large networks157statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.okay, so without further ado, let's talk about the applications. the primary use of these tools is in fraud detection.i will outline one type of fraud that we deal with, and those in the audience with similar problems, they probably havesimilar versions of this. they aren't necessarily for fraud.customers subscribe for service. we like that. they don't pay us, we don't like that, and there are a lot of reasonswhy this happens. some of it is our own doing. it is a competitive market. we work hard to get customers. we don'tscreen them, maybe, as well as we should. other times, people out there who want to be bad are pretty good at beingbad. they will steal identities. no matter how good we check on their credentials, it comes up good because this is aperfectly good credit record that this individual has.keynote address: graph miningšdiscovery in large networks158statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.the only problem is that it is not the person.then, the problem gets exacerbated, because not only did we give them service, we basically gave it to them for acouple of months. they use the service for 30 days, we send them a bill. thirty days later, we find out they didn't paythe bill. so, we start reminding them, gently at first and more vigorously later. eventually, we lose our patience, butliterally, 60 to 90 days. so, it is not just getting service, but it is service over an extended period. so, this is somethingwe don't like.in a security operation, you may not want bad guys floating around for 60 or 90 days without you knowing aboutthem. you would like to know about them within minutes, days, weeks, rather than 60 or 90 days.so, part of the plan we have is, you know, bad guys don't work in isolation, or you count on the fact that theydon't. so, there is going to be some clustering of bad behavior in your network, and we will exploit this to come upwith ways to describe this affinity group, and just to rank suspects by who they are associated with.so, for instance, i think i have outlined a node up here. this is a suspect. we can compute their diameter twosubgraph, add some value to it and thenši won't go into detail on what the nodes of different shapes and sizes mean.think of some as cellular handsets and others as land lines, thickness and color of the lines depicting the weightor the strength of that association. color is the main thing here. any node that is colored was associated with fraudvery recently. so, thiskeynote address: graph miningšdiscovery in large networks159statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.particular phone number that was a suspect, in their community of interest, has five other bad guys associated with it.this is what we want to bring to the attention of the security staff, that this would be someone good to investigate.so, the way we do this is, for every new account we see in our network for a particular type of service, we willcompute their subgraph seven days after we sight it. seven days is, again, just some made up number, a waiting period.you want the calling circle to mature, depending on the level of activity. it could be one day, it could be six days, butseven is what we settled on.we will do some enhancement of those subgraphs and then threshold them. we will go to a database and look forrecent fraud cases. so, we will color those subgraphs. we will rank them and put them on a web site and the securityassociates then just go through and look at this, and do their investigations.this is a result of what we saw from a trial when we first developed this. on this axis is the number of nodes in adiameter subgraph that are associated with previous fraud cases. this was the outcome of new cases post investigation.the number plotted at each plotting position is the number of cases. so, there were 13 cases that we presented tosecurity that had six bad guys in the community of interest, and about 45 percent of those turned out to be fraud.out here, there were six cases that we presented that had about nine bad guys in it. over 80 percent of thoseturned out to be bad. as soon as you get beyond that, it iskeynote address: graph miningšdiscovery in large networks160statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.almost 100 percent. if you are surrounded by badness, by golly, you are bad, no doubt about it.where the action, though, is down here where you don't see. this is the mess we were cleaning up when westarted this. now, when you see one or two bad guys around a new person, that is a big indicator that things are bad.so, by rolling this out and having active investigation, we are able to really clean up the mess.the second thing i will talk about, and just close on this, is tracking bad guys. you can think about this as accountlinkage. people who are bad, as i said, they are good at it. just like customers, they exhibit loyalty. if you have got agood product, they are going to be loyal. bad guys are the same way. if you have got a bad product, they will be loyalto you.if you let them abuse your network for a year without catching them, they will come back tomorrow and abuse itfor another year because you are not hassling them. fraud detection versus some of the security things, we don't haveto be perfect. we just have to be better than the competition. the fraud is not going to go away. it is going to go tosome network. we just don't want it to be ours.so, the nature of the game is to really hassle these guys. so, we want to hassle them, but again, they have theirown tricks. so, identity theft is not quite science yet, but there are some people that are darned good at it.so, you know, you can knock them off your network and because they don't costkeynote address: graph miningšdiscovery in large networks161statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.you as much as you would like to, they will come back on your network. if you are dumb enough not to know they areback, they are going to abuse you again. so, it is the same old game except, you know, burned once, you don't want toget burned again.so, even though the node identifieršso, the phone identifier on your network is new, is it possible that the personbehind that identity is the same. so, we are after them. basically, we want to exploit the fact that you are who you call.you may be burning your cell phone or a prepaid card at a fast rate, but maybe your friends and family, yourgirlfriends or whatever, they are not doing the same thing.so, their lifetime in your network has a different halflife than yours, and we want to exploit that fact. so, whatwe do is, we keep a library of subgraphs for these baddies and then new guys come along and we want to do a graphmatching.we want to say, are any of these newbies likely to be any of these ones we have caught before. so, it is a bigproblem. again, it is a graph matching problem, and they don't necessarily come back on your network the same daythat you bumped them off. so, you are collecting a pool of baddies with a pool of new customers, trying to do graphmatching and then ranking these pairs to present to security.again, there is a lot of engineering involved, and what do you mean by graph matching? so, basically, you aretaking a graph and a subgraph, overlapping them and saying, you know, how good is the overlap. you know, this isjust something we have come up with. it mimics, i think, a little bit of what goes on in text analysis, some of thescoring methods that are used there. you want to account for the fact that big weightskeynote address: graph miningšdiscovery in large networks162statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.associated with edges are good, but if a node is very common, it is not very discriminatory, you want to down weight it.the fact that both the newbie and the old guy call lands' end isn't very informative. if they both called a numberin yemen, maybe that is informative, if no one else called that number. so, that is what this is getting at.i am not going to go into details. i guess this is a cartoonšit is actually a real case of this. anything that is in pinkor red are the overlaps on two subgraphs. the network ids for the subgraphs we are overlaying are both in here.anything that is green is associated with only the top number here, anything in blue the bottom one. this just showsthat these two identities that are appearing on your network at different times are quite likely the same person behind it.quickly, just to show you the results of implementing this in the investigative sense, giving a bunch of leads toour spot associates, based on the deciles of predicted match, and then looking through time to say how many of thosecases that were presented turned out to be true matches, and that the pairs that we presented were actually matchedpairs.it just shows we are reasonably well calibrated. this thing would be perfectly on a 45 degree, on a perfectcalibration. it is showing that, when we think it is a match, generally, after investigation, it is a match. the thing that,to us, makes this so powerful, again, speaks to a number of things that we are seeing today.keynote address: graph miningšdiscovery in large networks163statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.generally speaking, we are getting tens of thousands of new customers on our network a day, tens of thousands ofbaddies of different sorts, not paying their bills or whatever, a day. so, the fact that we are able to distill this down toless than a thousand things for our fraud team to investigate, it is a big crunching of this down.in a physical sense, as a physicist, you have got all of this data. where do you look? so, these tools are to guideour physicists to show, this is where you have got to look.i am just going to close things by saying this is ongoing work, a lot going on, and thank you for your patience andyour time. thanks.ms. kellermc nulty: while we are getting john hooked up for the next session, if there are a couple ofquestions?[question off microphone.]mr. pregibon: the question is, do i ever use spectral techniques for these graphs. i think possibly what youmean there is some of the hudson authority type computations?[comments off microphone.]mr. pregibon: no, we have not. we should probably talk off line. there is some mathematics associated withanalysis of graphs in the literature, with argon analysis. those sorts of spectral methods are used to characterize andprocess findings of special nodes on the graph. for people who are familiar with the work of john kleinberg fromcornell, his hudson authority work is of that ilk.keynote address: graph miningšdiscovery in large networks164statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.sallie kellermcnulty, chair of session on integrated datasystemsintroduction by session chairtranscript of presentationbiosketch: sallie kellermcnulty is group leader for the statistical sciences group at los alamos nationallaboratory. before she moved to los alamos, dr. kellermcnulty was professor and director of graduate studies atthe department of statistics, kansas state university, where she has been on the faculty since 1985. she spent 2 yearsbetween 1994 and 1996 as program director, statistics and probability, division of mathematical sciences, nationalscience foundation. her ongoing areas of research focus on computational and graphical statistics applied tostatistical databases, including complex data/model integration and related software and modeling techniques, and sheis an expert in the area of data access and confidentiality. dr. kellermcnulty currently serves on two nationalresearch council committees, the cstb committee on computing and communications research to enable betteruse of information technology in government and the committee on national statistics' panel on the research onfuture census methods (for census 2010), and chairs the national academy of sciences' committee on applied andtheoretical statistics. she received her phd in statistics from iowa state university of science and technology. she isa fellow of the american statistical association (asa) and has held several positions within the asa, includingcurrently serving on its board of directors. she is an associate editor of statistical science and has served as associateeditor of the journal of computational and graphical statistics and the journal of the american statisticalassociation. she serves on the executive committee of the national institute of statistical sciences, on the executivecommittee of the american association for the advancement of science's section u, and chairs the committee ofpresidents of statistical societies. her web page can be found at http://www.stat.lanl.gov/people/skeller.shtmlintroduction by session chair165statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.transcript of presentationms. kellermcnulty: our next session has to do with integrated data streams. actually, it has been alludedto in the sessions prior to this as well, the multiplatforms, how do you integrate the data?we are going to start off with a talk that is sort of overview in nature, that is going to present some pretty broadproblems that we need to start being preparedšwe need to start to prepare ourselves how to address. that is going tobe by doug season, who is one of the deputy lab directors in the threat reduction directorate at los alamos. he hasbeen involved with different presidential advisors at ostp throughout his career, for both clinton and bush, has a longhistory of looking into and being interested and doing, himself, science in this whole area.that is going to be followed by a talk by kevin vixie, who will look at some hyperspectral analyses, kind offocus in on a piece of this problem. he is a mathematician at los alamos.finally, our last speaker will be john elder, who has been looking hard at integrating models and integrating data,and both hardware and software methods to do that.introduction by session chair166statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.j.douglas beasonglobal situational awarenessabstract of presentationtranscript of presentationbiosketch: douglas beason is the director of the international, space and response (isr) division at the losalamos national laboratory, responsible for over 400 professionals conducting research and development inintelligence, space, sensor, and directed energy programs. he has over 26 years of r&d experience that spansconducting basic research to directing applied science national security programs and formulating national policy.dr. beason previously served on the white house staff, working for the president's science advisor in both thebush and clinton administrations. he has performed research at the lawrence livermore national laboratory;directed a plasma physics laboratory; taught as an associate professor of physics and director of faculty research; wasdeputy director for directed energy, usaf research laboratory; and is a member of numerous national review boardsand committees, including the usaf science advisory board and a vice presidential commission on spaceexploration. he retired as a colonel from the air force after 24 years, with his last assignment as commander of thephillips research site, kirtland afb, new mexico.dr. beason holds phd and ms degrees in physics from the university of new mexico, an ms in nationalresource strategy from the national defense university, and is a graduate of the air force academy with bachelor'sdegrees in physics and mathematics. the author of 12 books and more than 100 other publications, he is a fellow of theamerican physical society, a distinguished graduate of the industrial college of the armed forces, a recipient of thendu president's strategic vision award, and a nebula award finalist.global situational awareness167statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.abstract of presentationglobal situational awarenessdouglas beason, los alamos national laboratorybattlefield awareness can sway the outcome of a war. for example, general schwarzkopf's ﬁhail maryﬂ feint inthe gulf war would not have been possible if the iraqis had had access to the same overhead imagery that wasavailable to the alliance forces. achieving and maintaining complete battlefield awareness made it possible for theunited states to dominate both tactically and strategically.global situational awareness can extend this advantage to global proportions. it can lift the fog of war byproviding war fighters and decision makers capabilities for assessing the state anywhere, at any timešlocating,identifying, characterizing, and tracking every combatant (terrorist), facility, and piece of equipment, from engagementto theater ranges, and spanning terrestrial (land/sea/air) through space domains. in the world of asymmetric warfarethat counterterrorism so thoroughly stresses, the realtime sensitivity to effects (as opposed to threats from specific,preidentified adversaries) that is offered by global situational awareness will be the deciding factor in achieving adominating, persistent victory.the national need for global situational awareness is recognized throughout the highest levels of our government.in the words of undersecretary of the air force and director of the national reconnaissance office peter teets,ﬁwhile the intelligence collection capabilities have been excellent, we need to add persistence to the equationyou'dlike to know all the time what's going on around the face of the globe.ﬂglobal situational awareness is achieved by acquiring, integrating, processing, analyzing, assessing, andexploiting data from a diverse and globally dispersed array of ground, sea, air, and spacebased, distributed sensors andhuman intelligence. this entails intelligently collecting huge (terabyte) volumes of multidimensional and hyperspectraldata and text through the use of distributed sensors; processing and fusing the data via sophisticated algorithmsrunning on adaptable computing systems; mining the data through the use of rapid featurerecognition and subtlechangedetection techniques; intelligently exploiting the resulting information to make projections in multipledimensions; and disseminating the resulting knowledge to decision makers, all in as near a realtime manner as possible. 168global situational awarenessstatistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.transcript of presentationmr. beason: thanks, sallie. this will be from the perspective of a physicist. while i was flying out here, i satby a particle physicist. i, myself, am a plasma physicist. i found out that, even though we were both from the samefield, we really couldn't understand each other. so, good luck on this.global situational awareness is a thrust that the government is undertaking, largely due to the events surroundingseptember 11 of 2001. basically, it is a thrust to try to give decision makers the ability to be able to assess thesocioeconomic or tactical battlefield situation in nearreal time. the vision in this is to be able to do it anywhere anytime, and this is versus everywhere all the time. now, everywhere all the time may never be achieved, and we maynever want to achieve that, especially because of the legal ramifications. the vision to be able to monitor nearlyeverywhere any time, what i am going to do is to walk you through some of the logic involved.first of all, what does it mean by that? what does it mean by some of the sensors? then, really get to the core ofthe matter, which is how do we handle the data. that really is the big problem, not only the assimilation of it,understanding it, trying to fuse it together. we will mine it, fuse it, and then try to predict what is going to happen.here is an outline of the talk. what are the types of capabilities that i am talking about in the concept ofoperations? then, i will spend a little bit of time on the signatures. that is kind of the gravy on here. again, i am aphysicist, and this is where the fun part is. what do we collect and why, a little bit of the physics behind it, and how dowe handle the data, how do we mine it, and then how do we fuse it? what scale of problem am i talking about? if wejust purely consideršif we try to decouple the problem from the law enforcement to the space situational awareness,and just look, for example, at the battlefield awareness, what people are talking about in the defense department issome kind of grid on the order of 100 by 100 kilometers. so, that is 104 kilometers. then, up to 50 kilometers high,and knowing the resolution down to a meter. that is like 1014 points. this is just the battlefield itself. so, it juststaggers your mind, the problem. so, let me give some examples, and what do we mean about the global capabilities.first of all, the argument is being made that it is more than visible. that is, it is more than looking at photographs.it is more than looking at imagery. it includes all types of sensors, and i am going to walk you through this in a minute.it also includes cyberspace, web sites, email traffic, especially if there is a flurry of activity. what you would like todo is, you would like to have the ability to be able to look at what we call known sites, and to visit these in a timewhere, first of all, things don't change very much. that is, it could be a building going up and you may only have torevisit this site perhaps weekly, daily or even hourly, if you would like. these are sites where something may be goingon, or even web sites, but you know that the delta time change is not very much, so you don't have to really revisit ittoo much.the second thing is that you really want to have the capability for monitoring for specific events. if there is anuclear explosion, if missiles are being moved around, if terrorists are meeting somewhere, you want to have thosespecific events. you want to be able to telescope down to them, to be able to tap into them. you want to be able to doit on a global scale. second of all, for those kinds of activities, you may have to haveglobal situational awareness169statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.some kind of a tipoff. you don't know, either through any kind of intercepts, like telephone conversations or visualintelligence. you may have to have human intelligence direct you to what is going to be happening. so, that is whatyou would like on a global scale. on a local scale, you would want very specific things to occur.for example, perhaps when equipment is being turned on or off, if this is a terrorist that you have located that youare communicating, you want to be able to not only geolocate the terrorist, but also to determine some of theequipment that they may be using. this thing of dismounts, right now, this is one of darpa's largest problems. adismount is, if you can imagine a caravan going through the desert and you are tracking this caravan and all of asudden somebody jumps off the caravan, you don't want to divert your observation asset away from that caravan, butyet, this person who jumped off may be the key person. so, you would want to have the capability of not onlyfollowing that caravan, but to follow this person across the desert, as they jump into a car, perhaps, drive to an airport,jump in a plane and then fly somewhere to go to a meeting.so, how do you do something like that? again, it is not just visual. if you can imagine an integrated system ofsensors that combine, say, acoustic sensors that are embedded in the ground that can follow the individual, and thenhand off to some kind of rfša radiofrequency sensoršthat could follow the car, that could, again, follow the planethat the person may go into.so, what type of sensors are needed, and how do you integrate this in a way so that you don't have a bunch ofscientists sitting in a room, each person looking at an oscilloscope saying, okay, this is happening and that is happeningand then you are going to hand it off to the next person. what type of virtual space do you need to build to be able toassimilate all this information, integrate it and then hand it off. so, these are some of the problems i will be talkingabout.the traditional way of looking at this problem is to build a layered system of systems. that is, you have tobalance everything from sensitivity resolution, coverage and data volume. i will give you a quick example. back in thebosnian war, the communications channels of the military were nearly brought to their knees. the reason was notbecause of all the high information density that was going back and forth on the communications channel. it wasbecause, when people would send out, say, air tasking orders or orders to go after a certain site, they would send themon powerpoint slides with 50 or 60 emblems, each bit mapped all around the powerpoint.so, you had maybe 20 or 30 megabytes of a file that had maybe 20 bits of information on it. so, you have to besmart in this. so, the point there is that when you are making these studies, the answer is not just to build bigger pipesand to make bigger iron to calculate what is going on. you have to do it in a smart way. again, this is part of theproblem, and now what i am going to do is walk you through, first of all, some of the sensors and some of the waysthat people think we may be able to attack this problem.first of all, there is more to the sensing than visual imagery. let me walk you through some examples of these.the case i am trying to build up here is that, for this global situational awareness, the problem is not really inventingnew widgets. it is the information, and the information is really the key. it is where the bottleneck is.so, i am going to walk you through just some examples of some sensors that already exist. some of them arealready being used. it is not, again, a case of building new technology all the time. on the lower lefthand side, whatyou are looking at is aglobal situational awareness170statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.defense threat reduction agency. so, this is why i am here, is to translate that, our project. it is a hard, deeply buriedtarget project.we are basically looking at an underground cavern and trying to determine where assets are in this undergroundcavern. of course, that is a timely question today. you can do this by using acoustic sensors. that is, you know theresonances that are built up in these threedimensional cavities. just like you can calculate the surface of a drumheadwhen it is struck, in a threedimensional cavity, if you know where the resonances are located, what you can do is backout of that where the assets are, if you know that trucks are in there, for example, or that people are walking around.it is kind of like a threedimensional pipe organ. this just shows some unique characteristics that arise from thepower spectrogram of that. on the upper righthand side it is just showing that, believe it or not, there is a difference inthe acoustic signatures of solid state and liquid fuelšnot solid state, but solid propellant liquid fuel rockets.you can back out what the differences are, and you can identify whether or not somebody has shot off, not onlywhat type of rocket, if it solid or liquid fueled, but also the unique rocket itself from that. so, there are other types ofsensors, using sonicsšand i will talk a little bit more here when i talk about distributed networks. if you can have theability to be able to geolocate your sensors in a very precise manneršsay, by using differential gpsšthen what youcan do is correlate the acoustic signatures that you get. you can, for example, geolocate the position of snipers. youcan imagine, then, that those distributed sensors don't even have to be stationary, but they could also be moving, if youhave a timeresolution that is high enough.what are the types of sensors that we are talking about? well, radiofrequency sensors. for example, on the lowerlefthand side, it shows a missile launcher that is erecting. most of the examples i am using for global situationalawareness are military in nature, but that is because of the audience that this was pitched at.what occurs in a physics sense, any time you have a detonation that happens in an engine, you have a very lowtemperature plasma that is created. any time you have a plasma, plasmas are not perfect. that is, they are not idealnhd plasmids. you have charge separation, which means that you have radiofrequency emissions. you are able topick that up. in fact, the missions are dependent upon the cavity that they are created in. so, there is a unique signaturethat you can tag not only to each class of vehicle, but also the vehicle itself that you can pick out.up on the righthand side, it shows the same type of phenomenology that is being used to detect high explosiveswhen they go off. i am not talking about megaton high explosives. i am talking about the pound class, 5 to 10poundclasses of explosives. again, it creates a very low temperature plasma. an rf field is generated, and you can not onlydetect that rf field, but also, what you can do is, you can geolocated those. these are, again, examples of sensors thatcan be added to this global sensor array. we have two examples here of spectral type data. on the righthand side isdata from a satellite known as multithermal imager. it is a national laboratory satellite, joint effort between us andsandia national laboratory. it uses 15 bands in the infrared. it is the first time that something like this has been up andhas been calibrated.what you are looking at is the actual dust distribution the day after the world trade center went down. this isfrom the hot dust that had diffused over lower manhattan. i can't tell you exactly what the calibration is on this, but itis extremely low.global situational awareness171statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.on the lower lefthand side, what you are looking at is an example of another sensor, which is a one photon counter.what this sensor does, it works in either a passive or an active mode. this is in an active mode where we have used asource to illuminate a dish. this is a satellite dish. it was actually illuminated from above from an altitude of about1,000 meters. what we are looking at are the returns, the statistical returns from the photon system that came back.the technical community now has the ability to count photons one photon at a time and to do so in a timeresolved way with a resolution of less than 100 picoseconds. what that means is that we can now get not only a twodimensional representation of what a view is, but also, with that timeresolution, you can build up a threedimensionalrepresentation as well. what you are looking at, the reason you can get the pictures from the bottom side, it is througha technique called ballistic photons. that is, you know when the source was illuminated, and you can calculate, then,on the return of the photons the path of each of those individual photons. so, basically, what this is saying is that youcan build up threedimensional images now. you can, in a sense, look behind objectives. it is not always true, becauseyou need a backlight for the reflection. again, there is more to sensors than visual imagery. that is kind of the fun partof this, as far as the toys and being able to look at the different things we are collecting.the question then arises, how do we handle all this data. finally, how do we go ahead and fuse it together. iwould like to make the case ofši talked about a paradigm earlier about one way to collect data or to build bigger pipesand to make bigger computers to try to run through with different kinds of algorithms to assess what is going on.another way to do this is to let the power of the computer actually help us itself by fusing the sensor with the computerat the source.it is possible now, because of a technique that was developed about 10 years ago in field programmable gatearraysšthat is, being able to hardwire instructions into the registers itself, to achieve speeds that are anywhere from ahundred to a thousand times greater than what you can achieve using software, that is because you are actuallyworking with the hardware instead of the software, to execute the code. since these things are reprogrammablešthatis, they are reconfigurable computersšthen you can do this not only on the fly, but also, what this means is that youcan make the sensors themselves part of the computation at the spot, and take away the need for such a high bandwidthfor getting the data back to some kind of unique facility that can help process the information.plus, what this gives you the ability to do is to be able to change these sensors on the fly. what i mean by this is,consider a technology such as the software radio. all you know is that radio basically is a receiver, and then there is abunch of electronics on the radio to change the capacitance, the induction. all this does, the electronics, really, is tochange the bandwidth of the signal, to sample different bits in the data stream, and that type of thing. it is now possibleto go ahead and makešbecause computers are fast enoughšand especially reconfigurable computersšto go aheadand make a reconfigurable computer that can do all the stuff that the wires and years ago the tubes and, now, thetransistors do.what this means is that, if you have a sensor, say, like a synthetic aperture array, and you want to change thenature of the sensor because it is detecting thing in an rf, to say an infraredometer, you can do it on the fly. what thisprovides people the power withglobal situational awareness172statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.is thatšor if you have platforms that you are going to put out in the field, be they groundbased, sea, air or spacebased, you don't have to figure out, 5 to 10 years ahead of time, what these sensors are going to be. that is, if it isgoing to be an rf sensor, then all that is important is the reception of this, and the bandwidth that you have for thereconfigurable computer. you can change the nature, the very nature, of the sensor on the fly.that is a long explanation of what this chart is, but what this shows is that, by putting the power of thecomputation next to the sensor, then what you do is greatly reduce the complexity of the problem and the data streamsthat you need. you are still going to need the ability to handle huge amounts of data, because remember, i was talkingabout 1014 different nodes. what this does is help solve that problem. i don't want to go too much longer on that, butyou all know about the distributed arrays. i talked a little bit about the power of that earlier. basically, having a noncentralized access to each of these, once you have the positions of these things nailed down in a wayšsay, by usinggps or, even better, differential gpsšthen they don't even have to be fixed, if you can keep track of them.what is nice about distributed networks is that every node on this should automatically know what every othernode knows, because that information is transmitted throughout. so, it degrades very gracefully. what this also givesyou the power to do is not only to take information in a distributed sense, but also, if you know the position of thesesensors well enough, then you will have the ability to phase them together, and to be able to transmit.what this means is, if you have very low transmitters in here at each of these nodes, say, even at the watt level, byphasing them together, you get the beauty of phasing, if you can manage to pull this off. it is harder to do at the shorterwavelengths, but at the longer wavelengths, it is easier to do. once you have all this data, how are you going to move itaround in a fashion where, if it is intercepted, then you know that it is still secure? when using new technologies thatare starting to arise, such as quantum key distribution, this is really possible. for example, two and only two keys arecreated, but the keys are only created when one of the wave functions is collapsed, of the two keys that exist. this issomething that arises from the epr paradoxšeinstein, polinski, rosen š and i would be happy to talk to anybodyafter this about it. it involves quantum mechanics, and it is a beautiful subject, but we don't have really too much timeto get into it. anyway, keys have been transmitted now 10 kilometers here in the united states, and the brits have,through a collaboration, i think, with the germans, have transmitted keys up to, i think, 26 kilometers through the air.we also have the ability to use different technologies to transmit the data that aren't laser technologies. why lasertechnologies? well, laser energy is very opaque to certain atmospheric conditions.we know that rf can transmit, especially where there are holes in the spectrum, through the atmosphere. so, tobe able to tap into regions of the electromagnetic spectrum that have not been touched before, in the socalled terahertzregimešthis is normally about 500 gigahertz up to about 10 terahertzšis possible now, with advances in technology.what i have shown here is something that was initially developed at slac. it is called the clistrino, which is based ontheir clistron rf amplifier.the key thing here is that the electron beam is not a pencil beam but, rather, a sheet beam which spreads out theenergy density. so, you don't have a lot of theglobal situational awareness173statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.problems that you used to have for the oldertype tubes. so, we talked a little bit about handling and we talked aboutthe sensors. let me talk about the data mining.what is envisioned here is having some kind of what we call distributed data hypercube. that is, it is amultidimensional cube with all sorts of data on it. on one axis, you have probably heard in the news, this ispoindexter's push at darpa, tapping everything into credit cards on one axis to airlines transactions on another axis,another axis being perhaps telephone intercepts, another axis being rf emissions, another axis perhaps visualinformation or human intelligence. tapping into that, and being able to do so in a legal wayšbecause there are largelegal implications in this, as well as things that are prohibited by statute, as it turns out, especially when you talk aboutintelligence databasesšto be able to render that using different types of algorithms and then be able to compute thatand then feed that back in does two things.first of all, it is to give you a state of where you are at today and, second of all, it is to try to predict what is goingto happen. i will give you a very short example of about five minutes here of something that is going on, that is takingdisparate types of databases to try to do something like this. so, that is kind of the mining problem, and there arevarious ways to mine large types of data. let me talk to you about two examples here where, again, you are not relyingjust on the algorithm itself, but you are relying on the computer to do this for you.this is an example of a program called genie that, in fact, was developed by a couple of the individuals that arehere in this room. it is a genetic algorithm that basically uses an array of kernels to optimize the algorithm that you aregoing to render, and it does so in a sense where the algorithm evolves to find you the optimal algorithm. it evolvesbecause, what you do is, you tell the algorithm or the onset, or tell the computer, what are the features, or some of thefeatures, that you are looking for.on the lefthand side, this is an example ofšthis is an aerial overhead here of san francisco bay. what you wantto do is look for golf courses on this. let's say that we paint each of the known golf courses green and then we tell thealgorithm, okay, go off and find those salient characteristics which define what a golf course is. now, as it turns out, itis very tough to do because there is not a lot of difference between water and golf courses. a lot of golfers, i guess,think that is funny.the reason is because of the reflectivity, the edge of the surface, which has no straight lines in it. so, it is kind oftough. especially when you are talking about something like global situational awareness, if you find a golf course,you have got to make absolutely sure that it is a golf course. you can imagine that this could be something else thatyou are going after. what you do is, you let the computer combine those aspects, especially if you have hyperspectraldata. that could be information in the infrared that may show the reflectivity, for example, of chlorophyll. it could beinformation about the edges. the computer assembles this data, using again, this basis of kernels that you have, andcomes up with and evolves a unique optimized algorithm to search for these things.now, this is different from neural nets because you can actually go back through and you can, through adeconvolution process, find out what the algorithms are, and it does make sense, when you look at it. basically, it is byusing the power of computation to help you itself, what you do is, you are reducing the complexity of the problem.now, if you have taken that, you can go to the next step, and you can accelerate that algorithm,global situational awareness174statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.not just to run on a computer, but if you hardwire it into a reconfigural computer, with that floating point gate array itold you about, what i will do is, i will show you an example of how you can do things on aši don't know if i amgoing to be able to do this.say that you have streaming data coming in as video arrays. what is occurring here is that we have asked it tolocate the car that you will seešthat is the one in the green on the righthand sidešat rates that are approaching 30frames per second. the point is that, by marrying different types of technology, we will be able to help out and helpyou determine to do things in a nearrealtime manner. other techniques for pulling very small or highšvery low, ishould say, signaltonoise data out. what i have shown here is pulling some data out by using a template on the lowerlefthand side. on the righthand side, it is looking at some spectrographic data, and being able to pull out somechemical species. so, these are all examples of data fusing techniques.let me really wrap this up and leave time for some questions here. the whole goal of this is to be able tosynthesize what we call a complete view of a situation from disparate databases. just trying to pull things together togive people a wide range of ability, be it from the national scene to the person, it could be a law enforcement officer,who may only want to know things that are happening 30 or 40 feet around him. what i have done is, i have putdoubleheaded arrows on there, to show that there should be a capability for those sensors to be tasked themselves,which kind of makes the people who run the sensors kind of scared.on the other hand, if you don't have that ability, then you don't have the ability to allow feedback into the system.there is an example of something going on like this, that is not nearly as complex, where some forest service data isbeing combined on wild fires, where they start, how they originate, combine that with climatology data, looking at soil,wetness, wind datašwhat else, soil information. then, combine that with department of justice data on knownarsonists, where they started forest fires before and where they are located now.what this is attempting to do, with this very small database, is to combine these disparate databases to predictšfirst of all, to give you the situation where we are now, and then perhaps to be able to predict if an arsonist were tostrike at a particular place. so, this is a real, no kidding, national security problem, forest fires, because you canimagine šwell, we, at los alamos ourselves, had devastating fires two years ago, that nearly wiped out a national labwith that. so, by using small test problems like this, we will show not only the larger problems that will arise, but alsohopefully that doing something like this is not merely a pipe dream.in conclusion, i think it has been determined that a need for a global situational awareness really exists. again,this is a synthesis and an integration of space situational awareness, battlefield situational awareness, law enforcementsituational awareness, to be able to be used in antiterrorist activities. a lot of work needs to be done. this is not a onelab or a oneuniversity project. it is something that i think will really tap the s&t base of the nation. the key here isseamless integration. it is the data and it is not really the sensors, but it is integrating it in a way to be able to show it ina way that makes sense in a seamless sense. so, that is the talk. it is accelerated about 10 minutes faster than i normallygive it. might i answer any questions you might have?audience: in that golf course example, you actually had trained data, spectral data from real golf courses, andtrained the model on that and predicted other golfglobal situational awareness175statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.courses?mr. beason: actually, what we did on that was, we had that picture and we locatedšwe knew where the golfcourses were, and we painted those with whatever technique we used, and then let the computer itself use that as itstraining aid. so, we allowed it to pick outšwe didn't give it any information a priori as to what might be a course. welet it decide itself. you did see some errors. so, there is an effort to push down the number of errors involved.also, we were able to findšfor example, we went back and looked at some forest fire data that had occurredaround los alamos, and what we were able to find was that there were three instances where the forest service hadstarted fires before and they had not told us about it, and we were able to pick those out. that really ticked us off, whenwe found that out.ms. kellermc nulty: i would like to point out that nancy david and james theiler over here are thegenie genies, if some people have some questions about that.audience: i was going to ask a question about this term, data fusion. is that the same as data assimilation?mr. beason: i am not sure what you mean by data assimilation. i think i know. fusion, that is all in a smartway, because you can't just bring things together. you have to know what the context is.global situational awareness176statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.kevin vixieincorporating invariants in mahalanobis distancebasedclassifiers: applications to face recognitiontranscript of presentationtechnical paperbiosketch: kevin vixie is a mathematician in the computational science methods group at los alamosnational laboratory. his research interests are in inverse problems; image analysis and datadriven approximation;computation; and modeling. more specifically, he is interested in the following main areas: data analysis techniquesinspired by ideas from partial differential equations, functional analysis, and dynamical systems; nonlinear functionalanalysis and its applications to real world problems; geometric measure theory and image analysis; high dimensionalapproximation and data analysis; and inverse problems, especially sparse tomography. the problems he is interested intend to have a strong geometrical flavor and a focus not too far from the mathematical/real data interface.incorporating invariants in mahalanobis distancebased classifiers: applications to facerecognition177statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.transcript of presentationmr. vixie: thanks for your patience. so, the problems we are interested in range over a lot of different kinds ofdata, including hyperspectral data.the problem we chose to look at, for the methods i am going to tell you about, is space data, because not only hasit been worked on a lot and it is hard, but there are nice sets of data out there, and there is also some way to make verynice comparisons among a bunch of competing algorithms.the people involved in this are myself, andy frazier, nick hengartner, who is here, brendt wohlberg. a littlemore widely, there is a team of us that is formulating that includes also other people who are here, like peter swartz,james and, of course, nick.the big picture is that we have these very large data sets. to do the kind of computations that we need to do, weneed to do dimension reduction. the classical, of course, is pca, and we like to look at nonlinear variance of thatapproach. the challenge we chose to address was how to build metrics which are invariant to shifts along surfaces inthe image space which represent changes of, like scaling, rotation, etc., that don't change the identity, but they dochange the image.this is preliminary work, like a couple of months of work, but the fivedimensional space we are working on isrepresented by the shift in x, shift in y, scale in x, scale in y and rotation. the next steps are to look at threedimensional rotation and change elimination.so, the prior work that we believe is relevant is on the order of 160 papers that we looked through. this morning,out of curiosity, i did a site search, also an inspec search, just to see what would come up when i typed in ﬁfacerecognition.ﬂ isis gave me about 900 hits and inspec gave me about 3,200. that is not too far from believable that 5percent may be kind of poor.so, the couple of papers that we found that we thought were of help to us was this eigenfaces versus fisher faces,and then a paper on using tangent approximations to increase the classification rates for character recognition. the datathat we used was the feret data from the feret database.the test bed we chose was the colorado state university test bed. we chose this because, in this test bed, theyhave a very uniform way of testing many different algorithms. so, including ours, there are 13 algorithms there, sowith ours, it was 14. there is a standard preprocessing that is done, and everybody trains on the same data with thesame preprocessing and there is a standard test. that allowed us to compare, in a very fair and unbiased mannerdifferent algorithms.here is a sampling of, this is the performance. i will explain this a little bit. the idea is that these algorithms trainon the training data, and then they end up with an algorithm that is a trained algorithm that you can hand data to, and itwill hand you back a distance matrix.so, if we hand you 100 faces, you end up with about 100 matrices, and this is the distances between the faces.then, based on that, you can rank results and say, if i have two images of the same person, what i would like to happenis that, in the column corresponding to image one of this person, the image two is ranked number one. it is the closest.this is sort of a monte carlo experiment, a histogram of how often, if you handincorporating invariants in mahalanobis distancebased classifiers: applications to facerecognition178statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.an algorithm 100 faces, it is going to get the right person in rank one. so, it ranges. the bottom is the pca base,putting in distance, which i will explain a little bit, and the top is this linear discriminate analysis based on acorrelation angle.audience: so, if you train on 100 faces, why doesn't it cover those 100 faces?mr. vixie: first of all, it is a different training and testing, and i will get to the ethos of how many it is trained on.now, the preprocessing is done to all the training data and all the test data; you shift, scale and rotate based oneye coordinates. so, these slides aren't actually the very latest. i made some changes last night after i came and they arenot showing.so, this is two pictures of the same guy. there is a different facial expression. you can also see at the bottom,these are the preprocess. these are the raw images.so, you scale, shift and rotate based on eye coordinates and location. then you mask. you use an elliptical mask.then you equalize the histogram. this is standard in image processing, and then you shift and scale until you get amean zero, standard deviation one.for training, 591 images were used, and that was 197 individuals, three pictures each. just a little flavor of whatthese different methods do, for pca, of course, what you do is simply take the images, form the empirical covariancematrix, do the hagandie composition, and then project down to some specified number, and here it was 60 percent. so,you end up with a 354dimensional space.now, that is actually a very big reduction in dimension, because the images are on the order of maybe a quartermillion pixels. so, it is a big reduction in dimension.the lda is justšprobably everybody here knows about thisšthe fisher basis. in this case, you have aknowledge about what is the withinclass variance and what is the betweenclass variance. so, you know whatdifferences correspond to differences between the same individual and what differences correspond to differencebetween different individuals. you try to pick a basis that can differentiate betweenindividual differences and withinindividual differences, optimally.for the linear discriminate analysis, we simply take the withinclass. so, there are 591 pictures. so, you have thedifferences. for each set of three images, you take all those differences, and you throw them together, build acovariance matrix, and that gives you the withinclass covariance matrix.for the euclidian distance, this is very simple. you just project into the pca basis, and then take the coefficients.this is image a and image b. the distance between those images is just simply the difference of the coefficientsquared sum dot square root. that corresponded to actually the worst performance in that set of algorithms.the correlation that was the best is an anglebased classification. here, after you project it on an lda basis, youtake the mean and subtract them out and simply take an inner product. subtract that from one. so, if the end product ismaximized, then they are close. you want that to be a distance. you subtract that from one and it turns into somethinglike a distance.so, what did we do? well, what we did was dictated not only by some interest in faces, but more in an interest inmany kinds of data, including hyperspectral, voice, etc.so, we said, well, we would like to know, in a principal way, how to include in the distance metric the notion thatthere are differences that don't really matter.incorporating invariants in mahalanobis distancebased classifiers: applications to facerecognition179statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.so, the blue curve represents the low dimensional manifold. in our case, it is going to be fivedimensionalbecause there are five parameters we are modifying the image by.that represents sort of an orbit of the face in the image space that represents all the same individual. so, what iwould like to do is build a metric that says, if there are two pointsša point here and a point therešthere is no distancebetween them, if they are the same individual. so, these manifolds are nonlinear and it is not necessarily easy tocompute them. so, as sort of a first thing, we do a linear approximation.so, this just represents the tangent manifold approximation to that surface. so, what we are going to now is, weare going to try to modify this covariance matrix, which uses a kernel to build the distance, or the inverse of thatkernel, modify it with the knowledge that differences along this direction don't matter.now, of course, that is not quite true, because if this is highly curved, and if i go too far out, that linearapproximation isn't very good. so, we want to use the secondorder information, which we can also compute, to limithow widely we let ourselves move along this direction with no penalty. we built this new covariance matrix, and itenabled us to use the classification method.now, notice that the key feature here is that, in fact, when you do this, you end up with different modifications ateach point. even though we start with the same withinclass covariance, we end up with different modifications of that.so, it is a localized modification of something that seems to be nonlinear.i will come back to some of the details and show you the results. so, this is the same graphic as before with somefaces removed. so, this was the worst, this is the best, this is what we got. what we got was an improvementšthe nexttothelast curve was what we did without the tangent modification. so, we got this big improvementšagain, ourperformance is untuned, and the thing you have to understand about the face stuff is that often tuning makes a very bigdifference.so, we were encouraged because untuned performance, and then we got this big jump. sort of the next step is toadd that to the angle base metric, in which you get an improvement there. again, really, the real goal for this isn't to dofaces the best of anybody. it is really to have a tool that is flexible, fast, and generally applicable.here are some details. there is this nice picture, again. i like this picture. so, you imagine there are a bunch ofindividuals and you have a space of images. so, this plane i am drawing represents the space of images.if you have 100 by 100 pixelated images, you get 10,000 in actual space. then, i imagine that i have a space ofparameters that controlsšthat this individual controls where i am on this manifold.this is the transformation that maps you from individual and parameter to the image space. what you can do isšto make a sensible little explanationšassume the image equals this %(f, $) but i have got some noise. i am going toassume that is distributed normally, and that the $ is also distributed normally, which is a helpful fiction that is not toofar off.you can sort of convince yourself that it might be reasonable because all of this face recognition is done whereyou are first trying to locate them the same but you make at least subpixel errors. so, you might expect that the errorsyou make are going to have something that might be something like a gaussian distribution.incorporating invariants in mahalanobis distancebased classifiers: applications to facerecognition180statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.so, if we expand $ in a taylor series, then we get this. in our case, % is a fivedimensional vector, and g is goingto be the number of dimensions you are using by five, and then we have the secondorder term here.this is just a note that, because of linearity, if $ distributes according to this distribution, then gf $ is just that.so, let me give an example of the shifts. now, when i show this to people first they say, what is the big deal? youare just shifting it. the deal is that in images this is nonlinear. the shift in image is up and the pixel space is nonlinear. i will give it to you a little later. you want to convolve these images with the kernels to smooth them before youtake the derivative.this is the image you start with and then you try to shift it up nine pixels, and then this is the secondorder ofcorrection.now, notice, you can see artifacts here, not that well, but there are some artifacts. it gets a little too light here anddark there. so, there are some artifacts with this secondorder of correction, because we have shifted it a little bit pastthe validity of this combination of kernels and image. so, we have to tune the sort of combination of how big a kernelwe use to smooth it and how far we shift it.audience: [comment off microphone.]mr. vixie: yes, so let's take an example. if you have a very simple image and you have just one pixel that isblack and everything else is white, and you shift it once, we are going to do it with quiet differences.you shift it one pixel. now, take the difference between those pixels. one is the new spot and minus one is theold spot. add that to the old image to get the new image. it is shifted one pixel. now, i would like a fivepixel shift.so, how about if you multiply that little difference vector by five? are you getting a fivepixel change? no. does thatmake sense now?audience: yes, but that is not usually whatš [off microphone.]mr. vixie: what i mean is i want to shift thingsš [off microphone.]audience: when you say linear, do you mean linear operating on the image, not on the location?mr. vixie: yes. yes. okay, some details. so, it is a secondorder error, if everything after zero and the firstorder turn is small, then the distribution for s and f is going to be normal, according to this distribution. it is going tohave a mean at $ after zero, and this covariant.so, the maximum likelihood of classification is simply this minimizing over i, this distance, where the kernel isnow modified with the derivative.okay, so, the question is, how do you pick the $. we have conflicting goals here. do you want the !($) large, doyou want to move out on that approximating manifold, but you want the secondorder to be small?so, the solution is to maximize the determinant of !($), while constraining the secondorder error.if you do that, it is fairly straightforward. you end up that !($) equals & times the inverse of this modified secondorder term.what we have done here is, we have modified it so that it looks strictly positive. you might think that, okay, thisweights all pixels evenly. so, this is the sum of the pixels.incorporating invariants in mahalanobis distancebased classifiers: applications to facerecognition181statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.the way to view this secondorder term is as a thirdorder tangent because at each pixel you have, in our case, afivebyfive matrix.so, what i would like to do is actually make it so that the secondorder errors are discounted, or it is not paid asmuch attention to if those secondorder errors are in directions which the a within, or the withinclass covariance isignoring anyway.so, then, what we do is, we decompose the withinclass variances and we get the components of this tangent ineach of those directions, and then simply weight those, because this final withinclass covariance by the inverse squareroot of the iš [off microphone.]now we have the sense that, if there is a secondorder error but it is in a direction that the withinclass covarianceis totally ignoring, then we can go much farther out. again, we used the switch and we the constrained optimizationbefore.okay, now just a couple of notes. the trick that makes this thing work quickly is the fact that, when you do thesmoothing and just taking derivatives, so you can transfer the derivatives to the kernel, and then you can use the factthat even when you getšin this case, with the shift, you don't get a spatially varying kernel, but in other cases,especially when you are doing the secondorder derivatives, you get a lot of spatially varying kernels, and that is hardto do with the fft.so, we simply preprocess the image by multiplying by that spatially ordering term, and then we can do the fft.that is what makes this work very quickly.i guess that is all the slides i have here. i had more in here. the main points that i wanted to close with are thatthis method of looking at modeling data is generally applicable. we are not interested, for a couple of reasonsšlegalreasonsš [off microphone].i might feel bad aboutši don't think i am going to do this, but if i created something that was perfect at spatialrecognition, maybe i would have a conscience issue.at any rate, this is something that is actually applicable to a very wide range of data. it is fast to compute and it isheaded toward the use of knowledge that we have about what sorts of transformation of identity or classificationsš[off microphone.]the things we want to do next would be, first of all, add the change and modifications to the angle base. also,look at more detailed, or more difficult, transformations, like threedimensional locations would be much moredifficult. lighting is also an issue. thank you.ms. kellermc nulty: are there some questions while john is getting set up?audience: how do you find theš[off microphone.]mr. vixie: we let the csu test bed do that, simply because we wanted our comparison to be absolutelyunbiased with the other test cases. so, we used their algorithm. i didn't take care of that part. my job was the code tocompute all the derivatives and secondorder transforms. i didn't do that piece of it or recode that piece of it.ms. kellermc nulty: other questions? do you want to make a comment on the data fusion, dataassimilation question earlier?mr. vixie: yes, somebody asked about data fusion. well, classically, when i think of data assimilation, i thinkof common filtering, something where you have a dynamic process and you want to get back to a state space, but thatstate space is not fullyincorporating invariants in mahalanobis distancebased classifiers: applications to facerecognition182statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.measured.i have a tendimensional space and i am taking onedimensional measurements, and the idea is that over time ibuild up those onedimensional measurements, and i know about the dynamic connections. then i can get back to thestate. i preserve the state. when i think of assimilation, i think of that. in fact, i think that is what is commonly meantwhen you say data assimilation. you can correct me if you know better.data fusion could include something like that. in essence, to do fusion, you might approach it this way, where youhave an idea that there is some big invariant state space that is like hidden, and fusion is enabling you to get back tothat hidden space.audience: [off microphone.]mr. vixie: fundamentally, there is no difference in the way i stated it. i think the way people often think aboutit, and what you find written about it is quite different. i mean, i have a way of thinking about it that makes sense to me.incorporating invariants in mahalanobis distancebased classifiers: applications to facerecognition183statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.incorporating invariants in mahalanobis distancebased classifiers: applications to facerecognition184incorporating invariants in mahalanobis distance based classifiers:application to face recognitionandrew m.fraserportland state university and los alamos national laboratorynicolas w.hengartner, kevin r.vixie, and brendt e.wohlberglos alamos national laboratorylos alamos, nm 87545usaabstractšwe present a technique for combining prior knowledge about transformations that should beignored with a covariance matrix estimated from training data to make an improved mahalanobis distanceclassifier. modern classification problems often involve objects represented by highdimensional vectors orimages (for example, sampled speech or human faces). the complex statistical structure of these representationsis often difficult to infer from the relatively limited training data sets that are available in practice. thus, wewish to efficiently utilize any available a priori information, such as transformations of the representations withrespect to which the associated objects are known to retain the same classification (for example, spatial shifts ofan image of a handwritten digit do not alter the identity of the digit). these transformations, which are oftenrelatively simple in the space of the underlying objects, are usually nonlinear in the space of the objectrepresentation, making their inclusion within the framework of a standard statistical classifier difficult.motivated by prior work of simard et al., we have constructed a new classifier which combines statisticalinformation from training data and linear approximations to known invariance transformations. when testedon a face recognition task, performance was found to exceed by a significant margin that of the best algorithmin a reference software distribution.i. introductionthe task of identifying objects and features from image data is central in many active research fields. in this paperwe address the inherent problem that a single object may give rise to many possible images, depending on factors suchas the lighting conditions, the pose of the object, and its location and orientation relative to the camera. classificationshould be invariant with respect to changes in such parameters, but recent empirical studies [1] have shown that thevariation in the images produced from these sources for a single object are often of the same order of magnitude as thevariation between different objects.inspired by the work of simard et al. [2] [3], we think of each object as generating a low dimensional manifold inimage space by a group of transformations corresponding to changes in position, orientation, lighting, etc. if thefunctional form the transformation group is known, we could in principle calculate the entire manifold associated witha given object from a single image of it. classification based on the entire manifold, instead of a single point leads toprocedures that will be invariant to changes in instances from that group of transformations. the procedures wedescribe here approximate such a classification of equivalence classes of images. they are quite general and we expectthem to be useful in the many contexts outside of face recognition and image processing where the problem oftransformations to which classification should be invariant occur. for example, they provide a framework forclassifying near field sonar signals by incorporating doppler effects in an invariant manner. although the proceduresare general, in the remainder of the paper, we will use the terms faces or objects and image classification forconcreteness.of course, there are difficulties. since the manifolds are highly nonlinear, finding the manifold to which a newpoint belongs is computationally expensive. for noisy data, the computational problem is further compounded with theuncertainty in the assigned manifold.to address these problems, we use tangents to the manifolds at selected points in image space. using first andsecond derivatives of the transformations, our procedures provide substantial improvements to current imageclassification methods.ii. combining within class covariances and linear approximations toinvarianceshere we outline our approach. for a more detailed development, see [4]. we start with the standard mahalanobisdistance classifierwhere cw is the within class covariance for all of the classes, µk is the mean for class k, and y is the image to beclassified. we incorporate the known invariances while retaining this classifier structure by augmenting the withinclass covariance cw to obtain class specific covariances, ck for each class k. we design the augmentations to allowexcursions in directions tangent to the manifold generated by the transformations to which the classifier should beinvariant. we have sketched a geometrical view of our approach in fig. 1.denote the transformations with respect to which invariance is desired by (y, ), where and arethe image and transform parameters respectively. the second order taylor series for the transformation iswhere r is the remainder,statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.incorporating invariants in mahalanobis distancebased classifiers: applications to facerecognition185fig. 1. a geometrical view of classification with augmented covariance matrices: the dots represent the centers µkabout which approximations are made, the curves represent the true invariant manifolds, the straight lines representtangents to the manifolds, and the ellipses represent the pooled within class covariance cw estimated from the data.a new observation y is assigned to a class  using  the novelaspect is our calculation of  where is a parameter corresponding to a lagrange multiplier, and is a function of the tangent and curvature of the manifold (from the first and second derivatives respectively) withweighting of directions according to relevance estimated by diagonalizing cw.we define(1)where c,k is a dim()×dim() matrix. we require that c,k be nonnegative definite. consequently  isalso nonnegative definite. when  is used as a metric, the effect of the term  is to discountdisplacement components in the subspace spanned by vk, and the degree of the discount is controlled by c,k. wedeveloped [4] our treatment of c,k by thinking of  as having a gaussian distribution and calculating expected valueswith respect to its distribution. here we present some of that treatment, minimizing the probabilistic interpretation.roughly, c,k characterizes the costs of excursions of . we choose c,k to balance the conflicting goalsbig:we want to allow  to be large so that we can classify images with large displacements in the invariantdirections.small:we want  to be small so that the truncated taylor series will be a good approximation.we search for a resolution of these conflicting goals in terms of a norm on  and the covariance c,k. for theremainder of this section let us consider a single individual k and drop the extra subscript, i.e., we will denote thecovariance of for this individual by c.if, for a particular image component d, the hessian hd has both a positive eigenvalue 1 and a negative eigenvalue2, then the quadratic term th is zero along a direction e0 which is a linear combination of the correspondingeigenvectors, i.e.  we suspect that higher order terms will contribute to significant errorswhen  min so we eliminate the canceling effect by replacing hd with its positive square root, i.e. ifan eigenvalue of hd is negative, replace it with . this suggests the following mean root square norm(2)consider the following objection to the norm in eqn. (2). if there is an image component d which is unimportantfor recognition and for which hd is large, e.g. a sharp boundary in the background, then requiring  to be smallmight prevent parameter excursions that would only disrupt the background. to address this objection, we use theeigenvalues of the pooled within class covariance matrix cw to quantify the importance of the components. if there is alarge within class variance in the direction of component d, we will not curtail particular parameter excursions justbecause they cause errors in component d.we develop our formula for c in terms of the eigendecompositionas follows. break the dim()×dim()×dim() tensor h into components(3)then for each component, define the dim()×dim() matrix(4)and take the average to get(5)define the normstatistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.incorporating invariants in mahalanobis distancebased classifiers: applications to facerecognition186given h and cw, one can calculate  using equations (3), (4), and (5). then by using the determinant |c| toquantify goal big: (allow to be large) and using  to quantify goal small: (keep small), we getthe constrained optimization problem:maximize the determinant |c|subject to(6)where  is a constant.the solution to the problem is(7)where , which is a function of , is a constant that balances the competing goals.to verify that eqn. (7) indeed solves the optimization problem, note:in the coordinates that diagonalize  eqn. (6) only constrains the diagonal entries of c. of the symmetricpositive definite matrices with specific diagonal entries, the matrix that has the largest determinant is simply diagonal.so c and must be simultaneously diagonalizable, and the problem reduces tothe lagrange multipliers method yields eqn. (7).summary: given a new image y, we estimate its class withwhere  we have derived the parameters of this classifier by synthesizing statisticsfrom training data with analytic knowledge about transformations we wish to ignore.iii. face recognition resultswe tested our techniques by applying them to a face recognition task and found that they reduce the error rate bymore than 20% (from an error rate of 26.7% to an error rate of 20.6%). we used an analytic expression fortransformations in image space and developed procedures for evaluating first and second derivatives of thetransformations. the transformations have the following five degrees of freedom: horizontal translation vertical translation horizontal scaling vertical scaling rotationto implement the test, we relied on the feret data set [5] and a source code package from beveridge et al. [6],[7] at csu for evaluating face recognition algorithms.version 4.0 (october 2002) of the csu package contains source code that implements 13 different facerecognition algorithms, scripts for applying those algorithms to images from the feret data set, and source code formonte carlo studies of the distribution of the performance of the recognition algorithms. following turk and pentland[8], all of the csu algorithms use principal component analysis as a first step. those with the best recognition ratesalso follow zhao et al. [9] and use a discriminant analysis. for each algorithm tested, the csu evaluation procedurereports a distribution of performance levels. the specific task is defined in terms of a single probe image and a galleryof ng images. the images in the gallery are photographs of ng distinct individuals. the gallery contains a singletarget image, which is another photograph of the individual represented in the probe image. using distances reportedby the algorithm under test, the evaluation procedure sorts the gallery into a list, placing the target image as close tothe top as it can. the algorithm scores a success at rank n if the target is in the first n entries of the sorted list. the csuevaluation procedure randomly selects ng×10,000 galleryprobe pairs and reports the distribution of successfulrecognition rates as a function of rank.restricting the test data set to those images in the feret data that satisfy the following criteria: coordinates of the eyes have been measured and are part of the feret data. there are at least four images of each individual. the photographs of each individual were taken on at least two separate occasions.yields a set of 640 images consisting of 160 individuals with 4 images of each individual. thus we use ng=160.of the remaining images for which eye coordinates are given, we used a training set of 591 images consisting of 3images per individual for 197 individuals. the testing and training images were uniformly preprocessed by code fromthe csu package. in [6] the authors describe the preprocessing as,ﬁall our feret imagery has been preprocessed using code originally developed at nist and used in the feretevaluations. we have taken this code and converted it–spatial normalization rotates, translates and scales the images so that the eyes are placed at fixed points in theimagery based on a ground truth file of eye coordinates supplied with the feret data. the images are cropped to astandard size, 150 by 130 pixels. the nist code also masks out pixels not lying within an oval shaped face regionand scales the pixel data range of each image within the face region. in the source imagery, grey level values areintegers in the range 0 to 255. these pixel valuesstatistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.incorporating invariants in mahalanobis distancebased classifiers: applications to facerecognition187are first histogram equalized and then shifted and scaled such that the mean value of all pixels in the face region iszero and the standard deviation is one.ﬂeach recognition algorithm calculates subspaces and fits parameters using the preprocessed training images andknowledge of the identity of the individuals in the images. then, using those parameters, each algorithm constructs amatrix consisting of the distances between each pair of images in the testing set of 640 images. thus, in the trainingphase, one can calculate the mean image, µk, of an individual, but in the testing phase, the algorithm has noinformation about the identity of the individuals in the images.we developed three recognition algorithms: the first consists of the general techniques of section ii combinedwith minor modifications to fit the test task. we developed the second two algorithms after observing that the csualgorithms based on angular distance perform best (see fig. 2). in section ii we supposed that we would have severalexamples of each class, making an estimate of each class mean µk plausible, but for the task defined by the csuevaluation procedure, we must simply provide 640×640 interimage distances.the most obvious method for fitting our classification approach within this distancebased framework is to definethe distance between image yk and yl as the mahalanobis distancenote, however, that this distance is not symmetric, since the augmented covariance is only relevant to one of thetwo images. consequently, the symmetrized distanceis used for the distance matrix. after observing that of the csu algorithms, those based on angular distanceperform best (see fig. 2), we developed two additional algorithms. the ﬁmahalanobis angleﬂ distance iswith symmetrized versioninstead of symmetrizing d1(yk, yl), we also define the symmetric distancewhereevaluating each of the first two distances on the test set of 640 images takes about 30 minutes on a 2.2 ghzpentium iii. we found that the second distance performed better than the first. because we estimated that evaluatingthe third distance would take about 160 hours, we instead implemented a hybrid, constructed by computing and then computing only for those distance below some threshold (further detail may be found in [4]).each of our algorithms operates in a subspace learned from the training data and uses an estimated covariance,associated with each image yk. we list the key ideas here: use the training data (which includes image identities) to calculate raw withinclass sample covariances, .regularize the raw covariances as follows: (1) do an eigenvalueeigenvector decomposition to find (2) sum the eigenvalues,  (3) set cw= which has no eigenvalues less than s. conceptually convolve the test image with a gaussian kernel that has mean zero and variancewhere h is an adjustable parameter in the code that must be an odd integer. change variables to transferdifferentiation from the image to the kernel. evaluate the matrices vk and  by convolving (using fftmethods) differentiated kernels with the image.thus , , and h are three adjustable parameters in the estimate of ck. we investigated the dependence of theperformance on these parameters [4], and chose the values =100, h=11, and =0.0003. our experiments indicated thatthe classification performance was not sensitive to small changes in these choices.results are displayed in fig. 2 and fig. 3. each of our algorithms performs better than all of the algorithms in thecsu package.iv. conclusionswe have presented techniques for constructing classifiers that combine statistical information from training datawith tangent approximations to known transformations, and we demonstrated the techniques by applying them to aface recognition task. the techniques we created are a significant step forward from the work of simard et al. due tothe careful use of the curvature term for the control of the approximation errors implicit in the procedure. for the facerecognition task we used a five parameter group of invariant transformations consisting of rotation, shifts, and scalings.on the face test case, a classifier based on our techniques has an error rate more than 20% lower than that of the bestalgorithm in a reference software distribution.the improvement we obtained is surprising because our techniques handle rotation, shifts, and scalings, but wealso preprocessed the feret data with a program from csu that centers, rotates, and scales each image based onmeasured eye coordinates. while our techniques may compensate for errors in the measured eye coordinates orweaknesses instatistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.fig. 2. approximate distributions for the rank one recognition performance of the algorithms. for each algorithm, agaussian is plotted with a mean and variance estimated by a montecarlo study. note that the key lists thealgorithms in order of decreasing mean of the distributions; the first three are the algorithms described in sectioniii, and the remainder are those implemented in the csu software distribution.fig. 3. the mean recognition rate and 95% confidence intervals as a function of rank for the following algorithms: hybrid (the hybrid of  and ), (the symmetrized mahalanobis angle with tangent augmentation), (the symmetrized mahalanobis angle with no tangent augmentation, illustrating the benefit obtainedfrom the regularization of ), (the symmetrized mahalanobis distance), and lda correlation (the bestperforming algorithm in the csu distribution).incorporating invariants in mahalanobis distancebased classifiers: applications to facerecognition188statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.the preprocessing algorithms, we suspect that much of the improvement is due to similarities between thetransformations we handle and differences between images. for example, a smile is probably something like a dilationin the horizontal direction.v. acknowledgmentthis work was supported by a lanl 2002 homeland defense ldrder (pi k. vixie) and a lanl 2003 ldrddr (pi j. kamm).references[1] a.s.georghiades, p.n.belhumeur, and d.j.kriegman, ﬁfrom few to many: illumination cone models for face recognition under variablelighting and pose,ﬂ ieee transactions on pattern analysis and machine intelligence, vol. 23, no. 6, pp. 643œ660, june 2001.[2] p.y.simard, y.a. l.cun, j.s.denker, and b.victorri, ﬁtransformation invariance in pattern recognitionštangent distance and tangentpropagation,ﬂ in neural networks: tricks of the trade, g.b.orr and k.r.muller, eds. springer, 1998, ch. 12.[3] p.y.simard, y.a.cun, j.s.denker, and b.victorri, ﬁtransformation invariance in pattern recognition: tangent distance and propagation,ﬂinternational journal of imaging systems and technology, vol. 11, no. 3, pp. 181œ197, 2000.[4] a.fraser, n.hengartner, k.vixie, and b.wohlberg, ﬁclassification modulo invariance, with application to face recognition,ﬂ journal ofcomputational and graphical statistics, 2003, invited paper, in preparation.[5] p.j.phillips, h.moon, p.j.rauss, and s.rizvi, ﬁthe feret evaluation methodology for face recognition algorithms,ﬂ ieee transactions on pattern analysis and machine intelligence, vol. 22, no. 10, oct. 2000, available as report nistr 6264.[6] j.r.beveridge, k.she, b.draper, and g.h.givens, ﬁa nonparametric statistical comparison of principal component and lineardiscriminant subspaces for face recognition,ﬂ in proceedings of the ieee conference on computer vision and pattern recognition, 2001. [online]. available: http://www.cs.colostate.edu/evalfacerec/index.html[7] r.beveridge, ﬁevaluation of face recognition algorithms web site.ﬂ http://www.cs.colostate.edu/evalfacerec/, oct. 2002.[8] m.turk and a.pentland, ﬁface recognition using eigenfaces,ﬂ in proc. ieee conference on computer vision and pattern recognition, maui, hi, usa, 1991.[9] w.zhao, r.chellappa, and a.krishnaswamy, ﬁdiscriminant analysis of principal components for face recognition,ﬂ in face recognition:from theory to applications, wechsler, phillips, bruce, fogelmansoulie, and huang, eds., 1998, pp. 73œ85.incorporating invariants in mahalanobis distancebased classifiers: applications to facerecognition189statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.john elderensembles of models: simplicity (of function) throughcomplexity (of form)transcript of presentation and pdf slidesbiosketch: john elder is chief scientist of elder research, inc. (eri), a consulting firm with offices incharlottesville, virginia, and washington, d.c. (www.datamininglab.com).dr. elder earned electrical engineering degrees from rice university and a phd in systems engineering from theuniversity of virginia, where he is currently an adjunct professor, teaching optimization. he spent 5 years in hightechdefense consulting, 4 years heading research at an investment management firm, 2 years in rice university'scomputational and applied mathematics department, and has led eri since 1995.since 1995, dr. elder has led eri's projects in credit scoring, direct marketing, sales forecasting, stock selection,image pattern recognition, drug efficacy estimation, volatility forecasting, fraud detection, biometrics, and markettiming. he writes and speaks widely on pattern discovery techniques and is active on statistical and engineeringjournals and boards.dr. elder is active in statistics and engineering conferences and boards and is a program cochair of the 2004knowledge discovery and data mining conference. since the fall of 2001, he has served on a congressionallyappointed panel guiding technology at a division of the national security agency.ensembles of models: simplicity (of function) through complexity (of form)190statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.transcript of presentationmr. elder: i am going to talk today on two topics, ensembles of modelsšthat is, combining competing modelstogether to improve accuracy, and then the complexity of doing that. it tends to help you to combine models on newdata, which is sort of counter to the intuition or religion of simplification being important for generalization. so, i wantto explore that.combining models almost always improves generalization accuracy. the mechanisms for this aren't completelyknown, but there are a lot of intuitive reasons for that. i will mention a couple of methods of combining models, whichi call bundling. you will notice that all good methods have to start with the letter ﬁbﬂ.the question it raises, though, is why, if you take a model that is relatively complex already, and perhaps evenslightly overfit, based on your experiments, and combine it with other models that are of a similar ilk, you don't geteven further overfit. it certainly goes against the minimum description length principle for model selection, which ifind very elegant and beautiful, that a modelšthat is related to the communications world by saying, i have to transmitan output variable to you.we all have the same input variables, and i can send that as a signal and then send the noise. since the errordistribution of the noise, which has to be sort of completely described, is tighter, given a good model, if i am able todescribe the gist of the data very well, then i will only have to send a little bit of correction for you to reproduce thedata. however, if my model is not very good, the error component will be much larger. so,ensembles of models: simplicity (of function) through complexity (of form)191statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.this is very appealing and it gets people into counting bits in terms of how to describe trees and so forth. the modelensembles really mess with this whole thing because they are very difficult to describe, and yet, generalize well. so,that is a puzzle.i think an answer is, to measure complexity differently than the form of the model. if it looks complex, it isn'tnecessarily, to look at the function of the model, to look at its behavior rather than its appearance. one of the metricsthat does this was recently introduced by jianming ye and i want to describe that briefly, if i can. then, a very simpleexperimental example to look at how it works out in practice, and then conclude with just some factors that affectcomplexity and, ergo, generalization. so, you can see i am still wedded to the occam's razor viewpoint, which hasbeen under attack recently, and for good reason. so, let's see if this is a partial answer to that.here is an experiment that steven lee at the university of idaho and i performed, using five different algorithms,on six different wellstudied problems. most of these data sets have fewer points in the data set than they have papersthat have been written with them, so these are, in some sense, not necessarily representative. in particular, theinvestment problem is completely made up there on the right. they are ordered on the xaxis in terms of increasingdifference on absolute terms between the estimates.so, there is a higher variance of differences on the rightmost data set than there is on the leftmost. what i haveplotted are the relative errors. so, the very worst would be near one at the top, and the very best would be at thebottom. so, we can see that this selection of five algorithms, which is not my five favorite algorithmsšthis is the fivewe could get a hold of in s at the time, yet they are very different.we see that the ageold question of which algorithm is best really has just the kind of answer that we want. itdepends on the problem.from this small subset of data, probably the neural nets would be the one. again, this is on outofsample data, sothis was after the fact. i don't have shown here what we knew about the training data beforehand. after the fact, itlooks like neural nets, the red line, is closest to the zero line in terms of relative error. you can see that most techniquesare competitive in at least one of the problems.the reason we wanted to do all these was to introduce a way of combining models. so, we looked at that, but thenhad to swallow our own medicine and look at simpler versions of it. we were introducing a technique called advisorperceptron, and that is the red one here, and it does very well. simple model averaging, although it outperforms oneach of the problemsšthat is the yellow linešit still does just fine andensembles of models: simplicity (of function) through complexity (of form)192statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.it is a whole lot easier.so, our point was to try to introduce a new technique, and it looks like splitting hairs compared to the big issue,which is virtually any reasonable method of model combinations. so, this is on the same scale as the previous slide,and you can see that the error variance, if you will, of choosing any one particular favorite technique is much greaterthan building five models and averaging the outputs together, and you get a much more robust answer. in fact, therewas no prefiltering on the individual models. they didn't have to reach any kind of performance threshold to beincluded in the average. they were just all averaged together.so, this is exciting. it is not five times more work to build five models. it is maybe 10 percent more work, becauseall the work is getting the data ready, and actually the fun part is running models. so, you get to do more fun things,and you get five times as much output, which is good, when you have a client or boss, and yet, your answer is betterand you avoid the hard choices of which one.now, i think this isn't done much, because we tend to specialize, and people tend to become experts in a particulartechnique, and they don't really feel confident in other techniques. perhaps this problem has been alleviated somewhatby the commercial tools which keep becoming more and more improved, keeping it a little easier, the learning curve. itis certainly easier than it used to be in terms of using something.i remember when mars was freeware, it took about a month to get it up and running. now it is a commercialproduct, and i would rather pay a few thousands than spend the month. only graduate students would rather spend themonth than actually pay real money. they don't realize their time is money, even for graduate students.the good news is i think it is going to be possible for people to run more algorithms on the same problem, with 5percent, 10 percent more effort. that is the fun part and the payoff is really good. so, that is a plug for modeling.ensembles of models: simplicity (of function) through complexity (of form)193statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.just stepping back, bundling consists of two basic steps. first, build a variety of models and, secondly, combinethem in some way. i have just shown here that there are a lot of different options for how you can do those two steps.you can use different case weights, which is a generalization of bootstrapping, or you can modify the data values.you can just run with the same data and change your guidance parameters, or you can use different subsets of thevariables.so, you can think of data coming from different sources and building models that are specialized from that sourceand later combining, without having to merge the entire data sets and start whole. that is a possible way.then, combining them, you can vote or weigh or do other fancy things, or partition the space and have modelsthat are experts in one area gradually hand over their authority to models that are experts in overlapping areas.now, all of these have been attempted. i am just going to list here a handful of the major ways of combiningmodels, and we have some experts here in a number of these areas. i also want to mention, greg ridgeway and i did atutorial on this, and the notes from that are available on our web site, greg from rand.perhaps the one that has captured the most attention, bagging, is perhaps the easiest one. it is just bootstrapaggregating. you take the same set of data and you build bootstrap replicates of the data, which i kind ofšwhich arecertainly easy to do, and build models on those replicates and then aggregate them together, or vote.one that is a much more sophisticated, complex and so forth is boosting, where the accuracy of the firstšyoubuild a model, and the accuracy of that model is evaluatedensembles of models: simplicity (of function) through complexity (of form)194statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.for all the particular data points. initially, everyone starts out with the same weight, every observation. theobservations that you are able to nail get very low weight, and the ones that you have errors on get more weight, andyou do a second pass.then, the things that you still have trouble with, the weight continues to go up and the weight gets less and less,and you get these obsessed models, as you build these stages, that are looking more and more focused on fewer andfewer things. sort of the things that you do right are taken for granted andšit is really like marriage, really. the sockson the floor really become important. these models are a little crazy. you don't use that final obsessed model. you usea weighted sum of the whole sequence of models, and this, astonishingly, has very good performance.so, a number of good statisticiansšit wasn't invented by statisticians. they would never come up with anythingquite that crazy, but it was looked at after the fact as to why it is working, and jerry friedman and dick shraney andhasty have done some good analysis of that, perhaps whyšonce you sort of analyze why something works, you canstart to twiddle with it and change parameters, and get papers out of it. so, that is a good thing.there are a lot of different bundling techniques. again, the distinction between them is probably less importantthan just doing them. my favorite way, which is still rather rare, my favorite way of generating variety is to usecompletely different modeling methods.this is a surface representation of a decision tree. this is a nearest neighbor surface representation, where a datapoint looks for its closest known point by some metric, and takes it answer as its estimate.that gives you similar surfaces to a decision tree, but not rectangular, but convex shapes. this is a piecewiseplaner approximation method, a polynomial network, which is similar to a neural network, but smooth anddifferentiable, and then a kernel estimation surface.so, you can see that the strength, sort of the vocabulary, of these different methods are different, and they havedifferent types of problems that they will work well on, or different regions of the same problem that they will workwell on. so, it is a good source of diversity to have different modeling methods.ensembles of models: simplicity (of function) through complexity (of form)195statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.one more example, pro bundling, we did a fairly sophisticated longterm project on credit scoring performance,and i am just showing five of the dozen or so techniques that we utilized. here is a tree. it is scalable, there areproperties that can handle categorical variables, real variables, they are fast, they are interpretable, at least in verysmall dimensions. they only have one problemšaccuracy. it is like a really wellwritten book about a veryuninteresting subject.so, trees alone are up here somewhere, but when you bundle them, it suddenly becomes competitive with muchmore complex techniques, but still, was the worst of these five. what we did is, we said, okay, let's try all pairwisecombinations of these methods. this is out of sample performance, so you wouldn't necessarily have known, well,sure, let's employ mars and neural nets together and we will do better than either one.this is the distribution of the pairwise performance, and then, of course, the threeand four and fivewayperformance. you can see that there is a trend here. the single best model is a combination of the three networkmethods. it is possiblešin this case, stepwise regression, neural nets and treesšcombine to give you something worsethan any of the individual models. so, it is not a silver bullet, but statisticiansšor statistician wannabees, as i amšyoulook for the trend, you look for the thing that will help you probabilistically.just to highlight that, i will show the box plots of these distributions as the degree of integration of the models, inthis case just averaging the outputs, as it increases. you can see that the mean and the median of the error improve witheach other model that isensembles of models: simplicity (of function) through complexity (of form)196statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.combined. we took this out to a dozen or so. obviously, at some point, it saturates and has some noise, but the trendwas definitely a good one, and it is one that i have seen over and over.of course, it is a great research area to say, under what criterion should you include a model and a bundle andhow can you estimate error rates and so forth, and that is very much an open issue.well, this brings up the whole problem of occam's razor, which basically has been taken to mean that simplicityis going to be a virtue in a sample. pedro domingos here got the best paper award a few years ago at the knowledge,discovery and data mining conference for highlighting failings of the razor, and highlighting failings of thisassumption, including, besides the performance of various kinds of model ensemble techniques, showing that, if youbuilt an ensemble and then estimated itšif you built one model that estimatedšthat would improve your accuracyover building that one model to start with.this perverse thing, that i haven't had the guts to read the paper, in 1996, apparently taking trees that wereperfectly fit and then grafting excess nodes to them, improved generalizability. that makes no sense, and i also pointout some work by david jensen about how much of overfit isn't really overparameterization but is excessive search.this brings up a number of issues with respect to model selection and model complexity. typically, what hasbeen done most often to control complexity is byensembles of models: simplicity (of function) through complexity (of form)197statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.counting and penalizing terms.it certainly works for regression, where the number of terms is equal to the number of degrees of freedom that themodel is using, and you can generalize a version of crossvalidation, which is a function of a penalty times the numberof parameters in the error. so, if the error improves with complexity, the complexity goes up and you look for the besttradeoff between the two and select a model at that point. it is fast, allows you to use all your data for training, insteadof having to reserve some.it has been long known that single parameters can have fewer or greater degrees of freedom, and i just includedsome references here from the 1970s and 1980s, where, for instance, if you have a neural net, you can often have moreparameters than you have data points and yet, not necessarily be overfit. it is not making full use of those parameters.the parameters themselves are weak, they don't have full power.you can have a tree or mars or something like that, that has the equivalent of three or four degrees of freedomfor every parameter. there is a lot more search going on, a lot more flexibility. so, counting terms is certainly not thestory.also, the search, if you look at a model, you don't know how much work went into getting that model. if you havea threeterm model, did you look at 10,000 variables to choose the three or 10 variables to choose those three. thatmakes a big difference.hjorth, in 1989, said the evaluation of an effective model cannot be based on that model alone, but requiresinformation about the class of model and the selection procedure. so, we find ourselves in a situation where, if wetruly want to evaluate the complexityšthat is, degree of overfitšwe have to take into account the model selectionprocess in coming up with a model selection metric.so, we need to take into account the extent of the search over model space, how thorough the algorithm was. infact, a number of people hypothesized that, well, we know that greedy step wise regression is suboptimal, but it helpsus avoid overfit. if we looked at all optimal subsets, that would be more likely to overfit. that is actually false, ibelieve, but there is the acknowledgment that the thoroughness of the algorithm search is a factor in overfit.how diverse and how many inputs there are matter. the power of the parameters, and often overlooked, thedegree to which the problem itself may be easy or hard.there is clear structure in the data. then, a number of adaptive techniques will latch onto it. if it is much harder tofind, you have much more chance for mischief.so, a model selection technique will have to be empirical and involve reensembles of models: simplicity (of function) through complexity (of form)198statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.sampling. we may be able to get back to a place where we can do some of that, but we lose speed with that. we maybe able to regain some of it if we do some early estimations and refine our complexity parameter approach from that.so, it is heavily computerdependent, but we may be able to do it just in some early stages, and more intelligentlyselect models from the vast search that data mining involves.data mining, you come up with a final model and people interpret it. i always have trouble with that, because youhave looped over a finite data set with noise in it. you have searched over billions of possible combinations of terms.the one that fell to the bottom, or that won, just barely beat out hundreds of other models that had perhaps differentparameters, and the inputs, themselves, are highly correlated. so, i tend to think of models as useful and not try tothink of the interpretability as a danger, but i know that is not the case with everyone here, in terms of the importanceof interpretability.well, there are a couple of metrics that have these properties. jianming ye introduced generalized degrees offreedom, where the basic idea is to perturb the output variable, refit your procedure, and then measure the changes inthe estimates, with the idea that the flexibility of your process is truly complex.if your modeling process was to take a mean, then that is going to be fairly inflexible. it is certainly more subjectto outliers than a median or something, but it is a whole lot less flexible than to changes in the data than a polynomialnetwork or a decision tree or something like that. so, if your modeling procedure can respond to noise that you inject,and responds very happily, then you realize it is an overfit problem.this reminds me, when i was studying at the university of virginia and one of the master's students in my groupwas working with some medical professionals across the street at the university of virginia hospital. he had a graphthat they were trying to measure heart output strength for young kids with very simpletoobtain features like therefresh rate for your skin when you press it. that is not the technical word for it, but how quickly it becomes pinkagain after you squeeze it, or the temperature of your extremities and so forth, and see if they could have the sameinformation that you could get through these catheters that are invasive and painful.they determined that they could, but i remember a stage along the way when he was presenting someintermediate results on an overhead. this is one of the dangers of graphs on overheads. the nurse and the head nurseand the doctor were going over it and saying, ﬁi see how temperature rises and you get this change.ﬂ my colleaguerealized, to his horror, that he had the overhead upside down and backwards. he changed it and itensembles of models: simplicity (of function) through complexity (of form)199statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.took them about five seconds to say, ﬁoh, i see how that works, too. it is completely the opposite.ﬂso, we want to believe and, if we are extremely flexible with resultant changes to the data, then maybe we are tooflexible, too easily fooled, too much overfit.so, another method i won't look at in detail, but i will just mention, is tibshirani and knight's covariance inflationcriteria. instead of randomly perturbing the outputs, they shuffle the outputs and look at the covariance between oldand new estimates. the key idea here is to put a loop around your whole procedure and look at its sensitivity.i remember that the first person i heardšjulian fairwayšin an interface conference in 1991 doing that in hisrat regression analysis tool, at the time, a twosecond analysis took two days to get resampling results on, but ithought it was a very promising approach. i saw him later. he was one of those statisticians trapped in a mathdepartment. so, he had gone away from doing useful things and doing more theoretical things out of peer pressure.anyway, i was excited about this and it was, i think, a good idea.so, let me explain a little bit about what generalized degrees of freedom are. with regression, the number ofdegrees of freedom is the number of terms. if we extrapolate from that, you see that you count the number ofthresholds in a tree, the number of splines in a mars or something. people had noticed that the effect can be more likethree, as i mentioned before, for a spline or even less than one for some particularly inefficient procedures.well, if we, instead, generalize from a linear regression in a slightly different way, if we notice that the degrees offreedom are also the trace of the hat matrix, which is the sensitivity of the output, to sensitivity estimate of changes tothe output, then we can use that as a way of measuring. we can empirically perturb the output, and then refit theprocedure. this is similar to what is done with splines, i understand.ensembles of models: simplicity (of function) through complexity (of form)200statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.so, put a loop around the process. that is nice, because the process itself can be a whole collection of proceduresšoutlier detection, all sorts of things can be thrown into this spline. so, graphically, we have a modeling process andwe have inputs and we have an output variable. we add some noise to it, and we record the output variable and thenew forecast based on that perturbed output variable.i kind of like the fact that the output ye also spells the name of the fellow that came up with it.so, you have 100 observations. i know that is a heretically small number for this gathering. you have 100observations and 50 perturbations. then, you would record the perturbed output and its change, and you would look atthe sensitivity of change in the output and change in the estimate. if i were doing it, i would just naturallyšeachperturbation experiment, i would calculate a different number.i do want to point out that ye, i think, had a good idea, and he took the matrix and sliced it this way and said,well, i am going to look at the effect on observation one of changes over time, which seems to be a little bit morerobust than measuring up all of these within an experiment.also, interestingly, you can then assign complexity to observations. now, i excluded that graph from this, but ifyou are interested, i can show, on a sample problem, where we did that.i will conclude, in this last few minutes, here with a very simple problem. this is a decision tree, and this is adecisionmaking mechanism for the data. naturally, the tree is going to do pretty well on it. it is amazing how manytest problems for trees involveensembles of models: simplicity (of function) through complexity (of form)201statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.trees as their source.here is where we have added noise to it. the noise is at a level that you can see it obscuresšpretty much obscuresšthe smallest structural features, but doesn't obscure others. so, there are some features that are easy to discern andothers that are harder, and that seems to be picked up very nicely by the gds metric.now, out of this surface we have sampled 100 training samples, and the other experiments with 1,000 and soforth. i am just going to show you sort of the very initial experiments, and make only a few points. this is very muchongoing.out of those samples, we built trees and we also built bootstrap and put five treesensembles of models: simplicity (of function) through complexity (of form)202statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.together, that went down different amounts. they either had four leaf nodes or eight leaf nodes in this example. youcan see, if we built five fourleaf trees and put them together, you still get a tree. you still get something that could berepresented as a single tree. it would look rather complex.so, i can show what that looks like, if you are interested, but you can see here, that the bagging procedure doesgentler stairsteps than a raw tree. that tends to be good for generalization, the smoothness. so, bagging helps tosmooth trees and that is, i think, the major reason for generalization improvement.you can see that, when you go to more complex trees, eight leaf nodes would be eight estimation surfaces. thereare only five in this data. so, four is sort of underpowerful and eight is overpowerful, and you can see that itcompletely misses the smallest piece of structure up here. this one picks up on it, but on it, but it also picks up onanother structure that isn't there. so, it is the bias variance type of tradeoff.here is, in essence, the end result. we also did experiments with eight noise variables being added in. so, treestructure depends on two variables, but now you have thrown in eight distracting noise variables. so, how does thataffect things. then, we have, on this slide, the regression as well.as the number of parameters increases, the measured generalized degrees of freedom for regression is almostexactly what gary would tell you. it is basically one for one.interestingly, here, with the trees, a single treešthis purple line herešit has an estimated rough slope of four,meaning that, on this problem, each split that was chosen with the tree algorithm has roughly the equivalent descriptivepower of four linear terms, or it is using up the data at four times the rate of a linear term. if you bag those trees, itreduces to a slope of three. if you add in noise variables, it increases the slope to five, and then, if you bag those treesthat are built on noise variables, it brings it back to four.so, just to summarize, adding in distracting noise variables increases the complexity of the model. the modellooks exactly the same in terms of its structure and its size and how long it takes to describe the model. because itlooked at eight variables that could only get in its way, it was effectively more complex.then, the other point is that the bagging reduces the complexity of the model. so, the bagged ensemblešfive treesšis less complex than a single tree, and that is consistent with occam's razor idea that reduced complexity willincrease generalizability after you reach that sort of saturation point.ensembles of models: simplicity (of function) through complexity (of form)203statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.to summarize, i have a lot of little points to make. bundling almost always improves generalization, and i thinkthat different model families is a great source of diversity that you need for the bundling. if we measure complexity asflexibility of the procedure, then we sort of revive or answeršpartially answeršsome of the problems with generalintuition of complexity being related to overfit. so, the more a modeling process can match an arbitrary change madeto its output, the more complex it is by this measure.complexity clearly increases with distracting variables. these are variables that are considered, during the modelsearch project, which may not appear at all in the model at the end. actually, it would have to appear, at leastsomewhat, to affect the behavior, but the size of the model could be the same betweenštwo candidate models couldhave the exact same number of parameters and so forth, but one could be more complex because of what it looked at.the complexity is expected to increase, obviously, with parameter power, the thoroughness of your search, anddecrease with the use of priors and shrinking, and if there is clarity in the structure of the data. in our earlyexperiments, i certainly thought the complexity would decrease as you had more data. that seems to be the case withsome methods and not with others. it seems to actually increase with decision trees and decrease with neural nets, forinstance.by the way, neural nets, on this same problem, had about 1.5 degrees of freedom per parameter. it could be thatlocal methods somehow are more complex with more data, and methods that fit more global models are not. so, that isan interesting question. model ensembles usually have effective complexity less than their components by thisempirical measure.the next thing about the gdf is you now can more fairly compare very diverse procedures, even multistepprocedures, as long as you can put a loop around it. that concludes my talk. thank you.ms. kellermc nulty: questions?question: i have one. when you talk about the complexity of flexibility, are you meaning robustness? i amconfused about the local methods being more complex.mr. elder: i am, too. it is confusing. the idea behind gdf, the idea behind these empirical measures ofcomplexity is that you want to be able to measure the responsiveness of your procedure. obviously, you don't wantsomething that, every time you change the task a little bit, it is good for that one thing.ensembles of models: simplicity (of function) through complexity (of form)204statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.you know, it is the universal health oil. i really have hair problems. well, it is good for that, it is good for yourliver, too. obviously, it is overfit.you know, where is that tradeoff? so, the measure is, if you arbitrarily change the problem, or you change theproblem a little bit, how responsive is it to suddenly get the same accuracy and so forth?again, it was a little bit confusing to me, but the trees, they are only concerned aboutšonce they partition thedata, they are only concerned about their little area, and they are not paying attention to the big, whereas, models thatare basically fitting a global model, data everywhere helps, and it tends to rein it in a little bit, is my best guess. it iscertainly an interesting area.question: do you have any extrapolation from your results? [comments off microphone.]mr. elder: the question, how things might change with many variables and massive data sets. i would like torevisit a problem we did.we participated in the kdd cup challenge, i guess, two summers ago, and that had 140,000 variables, which is alot for us, but only 2,000 cases. the final model, the one that won the contest, used three variables. well, threevariables seems like a simple model, but it was a huge search. it would be nice to put that under this microscope andsee what happens.again, my intuition was that more data would actually help reduce the complexity, but it is kind of wide open.audience: [question off microphone.]mr. elder: here, i have just talked about combining the estimates, and some techniques need a little squeezingto get them into the right shape.take a tree. you might need to do some kind of robust estimation. if it is a classification problem, you might get aclass distribution and use that to get a real value š to turn into a real value so that you can average it in.what i haven't talked about is, there are whole areas for collaboration amongst these different techniques that wehave explored in the past. for instance, neural nets don't typically select variables. they have to work with whateveryou give them, but other methods, like trees and stepwise regression and polynomial regressions select variables as amatter of course. typically, you can do better if you use the subset they select, when you hand it off to the neural nets,and some methods are very good at finding outliers.there are a lot more methods that can go on between the methods. up here, i talked about them just in terms oftheir final output, but yes, you do have to work sometimes to get them in a common language.audience: [question off microphone.] i am not sure how what you are talking about here answers that.mr. elder: good point. criticisms of occam's razor work with any measure of complexity, is pedro's point.the one that bothered me the most is about bundled. they are obviously more complex. they just look more complex,and yet, they do better. how can that be?in fact, it is very easy to get many more parameters involved in your bundle than you have data points and nothave overfit. they are not all freely simultaneously modified parameters, though. so, there is a difference. under thismeasure, the thingsensembles of models: simplicity (of function) through complexity (of form)205statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.that are more complex in gdf tend to overfit than things that aren't. so, it is a vote on the side of occam's razor.i haven't addressed any of the other criticisms, but in the experiments we have done here, it is more like youwould šaudience: like, for example, when you approximate an ensemble with one model back again, this model isstill a lot more complex, because a lot of this complexity is apparent. it just seems that you still get the more complexmodels in that.mr. elder: but it is apparent complexity.audience: yes, but when you replace the apparent complexity with the actual complexity, not just measuringto find the number of parameters. [comment off microphone.]mr. elder: i certainly would be eager to see that. thanks.ensembles of models: simplicity (of function) through complexity (of form)206statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.report from breakout groupinstructions for breakout groupsms. kellermc nulty: there are three basic questions, issues, that we would like the subgroups to comeback and report on.first of all, what sort of outstanding challenges do you see relative to the collection of material that was in thesession? in particular there, we heard in all these cases that there are real specific constraints on these problems thathave to be taken into consideration. we can't just assume we get the process infinitely fast, whatever we want.the second thing is, what are the needed collaborations? it is really wonderful today. so far, we are hearing froma whole range of scientists. so, what are the needed collaborations to really make progress on these problems?finally, what are the mechanisms for collaboration? you know, amy, for example, had a whole list ofsuggestions with her talk.so, the three things are the challenges, what are the scientific challenges, what are the needed collaborations, andwhat are some ideas on mechanisms for realizing those collaborations?report from integrated data systems breakout groupms. kellermc nulty: our discussion was really interesting and almost broke out in a fist fight at onepoint, but we all calmed down and got back together.so, having given you the three questions, we didn't really follow them, so let me go ahead and sort of take youthrough our discussion. when we did try to talk about what the challenges were, our discussion really wandered intothe fact that there are sort of two ways that you can kind of look at these problems.remember, our session had to do with the integration of data streams. so, you can kind of look at this in astovepipe manner, where you look at each stream independently and somehow put them together, hoping thedependencies will come out, or you actually take into account the fact that these are temporally related streams ofinformation and try to capture that. the thought is that, if one could actually get at that problem, that is where somesignificant gains could be made. however, it is really hard, and that was acknowledged in more ways than one as well.that led us into talking about whether or not the only way to look at this problem domain is very problemspecific. isevery problem different, or is there something fundamental underneath all of this that we should try to pull out?in particular, should we be trying to look at, i am going to say, mathematical abstractions of the problem and theinformation, and how the information is being handled, to try to get at ways to look at this? what are the implicationsand database issues, database design issues, that could be helpful here? there clearly was no agreement on that,ranging on, there is no new math to be done, math isn't involved at all, to in fact, there is some fundamentalmathematics that needs to be done. then, as we dug deeper into that and calmed down a little bit, we kind of got backto the notion that, what is really at issue here is how to integrate the fundamental science into the problem.report from breakout group207statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.if i have two streams of data, one coming from each sensor, if i am trying to put them together, it is because thereis some hidden state that i am trying to get at. neither sensor is modeling perhaps the physics of that hidden state. so,how do i start to try to characterize that process and take that into account? so, that really means that i have tosignificantly bring the science into the problem. so, then, we were really sounding quite patriotic from a scientificperspective.one of our colleagues brought up the comment that, you know, this philosophy between, am i modeling the dataor am i modeling the science and the problem, you know, has been with us for a long time. how far have we come inthat whole discussion and that whole problem area since 1985? that had us take pause for a minute, like, where are wecompared to what we could do in 1985, and how is it different? in fact, we decided, we actually are farther ahead incertain areas. in fact, our ability to gather the data, process the data, to model and actually use tools, we clearly arefarther ahead. a really important issue, which actually makes the powerpoint comment not quite so funny is that ourability and communication, remote communication, distributed communication, modes of communication, actuallyought to work in our favor in this problem area as well. however, the philosophical issue of how to integrate scienceand technology and mathematics and all these things together, it is not clear we are all that much farther ahead. it is thesame soap box we keep getting on.then, it was really brought out, well, maybe we are a little bit farther ahead in our thinking, because we haverecognized the powerful use of hierarchical models and the hierarchical modeling approach, looking at going from thephenomenology all the way up through integrating the science, putting the processing and tools together. the fact thatit is not simply a pyramid, that this is a dynamic pyramid, that if we take into account the changing requirements of theanalyst, if you will, the end user, the decision maker, we have to realize that there is a hierarchy here, but it is ahierarchy that is very dynamic in how it is going to change and move. there are actually methods, statisticalmathematical methods, that have evolved in the last 10 or 15 years, that to try to look at the hierarchical approach. so,we thought that was pretty positive.there is a really clear need, as soon as we are going into this mode of trying to integrate multiple streams, torecognize that expertise, the human must be in the loop and the decision process, the decision environment back to thedomain specificity of what you are trying to do, is needed. in a couple of the earlier sessions, we actually heard aboutthe development of serious platforms for data collection, without any regard to how that information was going to beintegrated, or how it was going to be used, through some more seriously collaborations that i will get into in a second.maybe we can really influence the whole process, to design better ways to collect information, better instruments,things that are more tailored to whatever the problem at hand is.i thought there was a really important remark made in our group about how, if you are really just looking at asingle data stream and a single source of information, that industry is really driving that single source problem. theyare going to build the best, fastest, most articulate sensor. what they are not going to probably nail is the fusion of thisinformation.if you couple that with the fact that, if you let that be done ad hoc, that you are now going to have just randommethods coming together with a lot of false positives, and then we got into the discussion of privacy invasion, and howdo you balance all of that,report from breakout group208statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.that we really need the serious thought, the serious integration, multidisciplinary collaboration, to be developing themethods, overseeing the methodological development, as well as being able to communicate back to the public what isgoing on here. so, i thought that was kind of interesting. so, collaboration, there needs to be very close collaborationin areas like systems engineering, hardware software design, statistics, mathematics, computer science database typethings, and basic science. that has to come together. now, that is not easy because, again, we have been saying thatforever that this is how we are going to solve these problems.then that comes into play, what are the mechanisms that we can try to do that? we didn't have a lot of goodanswers there. one idea was, is it possible to mount certain competitions that really are getting at serious fusion ofinformation that would require multidisciplinary teams like this to come together. there was a suggestion that, at someof our national institutes, such as samsi, that is science and applied mathematics institute, one of the new, not solelynsffunded, but one of the new nsffunded institutes, perhaps some sort of a focus here. i think that gets back todoug's comment, which i thought was really good, that regular meetings as opposed to one up workshops is the waywe are probably going to foster relationships between these communities. clearly, funding is required for those sorts ofthings. can we get funding agencies to require collaborations, and how do they then monitor and mediate how thathappens.then, one comment that was made at the end was the fact that, if we just focus in on statistics, and statisticsgraduate training, there is a lot of question as to whether we are actually training our students such that they can reallybegin to bite off these problems. i mean, do they have the computational skills necessary and the ability to do thecollaborations. i think that is a big question. my answer would be, i think in some of our programs we are, and inothers we are not, and how do we balance that?just one last comment. you know, we spoke at very high level and just at the end of our timešand then we sortof ran out of timešit was pointed out that if you really think of a data mining area and data mining problems, thatthere has been a lot done on supervised and unsupervised learning. i think we understand pretty well that these aremethods that have good predictive capabilities. however, it seems that the problem of the day is anomaly detection,and i really think that there, from a data fusion point of view, we really have a dearth of what we know how to do. so,the ground is fertile, the problems are hard, and somehow we have got to keep the dialogue going.report from breakout group209statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.mark hansenuntitled presentationtranscript of presentationbiosketch: mark hansen is a professor of statistics at the university of california at los angeles, with a jointappointment in design and media arts. his fields of expertise include statistical methods for data streams, text miningand informational retrieval, information theory, and practical function estimation.before joining the faculty at ucla, dr. hansen was a member of the technical staff at bell laboratories. hespecialized in web statistics and other large databases, directing a number of experiments with sound in support ofdata analysis.he has five patents and is the author of numerous publications as well as serving as an editor for the journal ofthe american statistical association, technometrics, and statistical computing and graphics newsletter. he hasreceived a number of grants and awards for his art installation listening post. listening post produces a visualizationof realtime data by combining text fragments in real time from thousands of unrestricted internet chat rooms, bulletinboards, and other public forums that are then read (or sung) by a voice synthesizer and simultaneously displayed acrossa suspended grid of more than 200 small electronic screens.dr. hansen received his undergraduate degree in applied mathematics from the university of california at davisand his master's and phd degrees in statistics from the university of california at berkeley.untitled presentation210statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.transcript of presentationmr. hansen: [speech in progress]. that involves artists like rauschenberg and even andy warhol. the ideawas to try to pair, then, mostly engineers and artists together to see what kind of useful forms of artistic expressionmight come out. in a very selfconscious way, i think, the approach was to try to revive this tradition with the arts and,hence, was born this arts and multimedia program.it was actually an interesting event. the idea, as i said, was to very selfconsciously pair artists and researcherstogether, and this will actually get to streaming data in a moment, i promise. so, what happened was, they organized atwoday workshop where 20 or so media artists from new york city and 20 or so invited researchers in the labs met inthe boardroom, then, of lucent, and each got 10 minutes to describe what they do. i had 10 minutes to kind of twitchand talk about what i do, and the artists got some time to have some very beautiful slides, and have a very bigvocabulary and talk about what they do. then we were supposed to pair up, somehow, find somebody and then put aproposal together, and they would fund three residency programs where the project would get funded.ben and i put together perhaps the simplest thing given our backgrounds, him being a sound artist and me being astatistician. we put together a proposal on data sonification, which is a process by which data is rendered in sound, forthe purpose of understanding some of its characteristics that may not be immediately obvious in the visual realm. so,instead of visualizing a data set, you might play a data set and get something out of it. this is sort of an old idea, and itseems like everything i have done john chambers has done many, many years ago. so, i have kind of given up ontrying to be unique or novel in any way.he was working with perhaps the father of electronic music, max mathews, at bell labs. this was back in 1974.he developed something that bell labs at the time gave the title mavis, the multidimensional audiovisualinteractive sensifier. the idea was that you would take a data set or take a matrix and you would map the first column,say, the pitch, to the second column, the timbre, the third column, the volume. then there would be some order to thedata somehow and you would just play it. john said you got a series of squeaks and then a squawk, perhaps, if therewas an outlier, and that was as far as it went. he said it wasn't particularly interesting to listen to, but maybe there wassomething that could be done to kind of smoke out some characteristics in the data. actually, this kind of mapping,when ben and i were talking, we thought that this kind of mapping might be able to withstand underground bombblasts and earthquakes. apparently, this problem was motivated by suki, who was involved in the soviet test bandiscussions. at least, i am getting this now all from bill.i thought i could give you an example of what some of this early sonification sounds like. a friend of mine at thegmd has developed a program on earthquake sonification, and here is what the kobe quake sounds like, if you speedit up 2,200 timesuntitled presentation211statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.its normal speed at a recording station in california. [audio played.] it sort of gets amusing if you listen to otherplaces. here is what it sounds like, where a few plates come together, but it also happens at the end of the water.[audio played.] i am told that it has nothing to do with the fact that there is water all around and has everything to dowith the fact that you have three plates coming together there, but i am not going to talk this evening about sort ofthose early examples of sonification. instead, i am going to start to talk about some recent work we have been doing inanalyzing communication streams. most of this work was done, again, through this bell labs/brooklyn academy ofmusic program. not surprisingly, then, a lot of it was inspired by communication processes or workmediatedtransactions.our group had been looking at things like call detail records. i am sure daryl must have talked about that earliertoday. i have personally been looking at web log data, or proxy log data. so, you get records of who requested whatfile when from the network. then we sort of slip in the end into like online forms, chat rooms, bulletin boards, that sortof thing, where the fundamental data consisted of who posted, what did they post, which room, what kind of content. ifyou think about it, in some sense, the model of this web as being this place where you kind of go out and youdownload a page is sort of fading in favor of these sort of these more usergenerated, or sort of connectionbasedcommunication processes. if you think about the number of emails per year, hal varian at berkeley estimated thatsomething like 610 billion emails are sent a year, versus only about 2 billion new web pages are made every year.audience: that is the wrong thing to count. you count how many times they are downloaded.mr. hansen: i guess you are right.audience: half of them are spam.mr. hansen: i am not going to go into my favorite spam story. it is kind of a mixed audience. then, there areother ubiquitous public forums. irc cracked its halfmillion user mark this year.in a sense, there is something to sort of these usergenerated communication streams, and that is what we aregoing to try to get at with this project with ben. so, our initial work, which seemed, i guess, a little wonky at the time,but at least produced some cool musicši don't know how practical it wasšfocused just on web traffic. the idea wasthat we were going to develop sort of a sound that you could use to passively monitor a system. so, we were going tolook at people's activity on a web site.we looked at lucent.com, which was then organized into a large number of businesses, now not so manybusinesses. below each business directory there was a series of products and directories and then, below those productdirectories, there were white papers and then, below those white papers, you would have technical specifications. so,the deeper you went in the directory structure, the more detailed material you were downloading.so, the idea was to create a sound that somehow became more expressive as moreuntitled presentation212statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.people on the site were browsing deeper and getting more interesting things. so, we created a kind of mapping of sortsthat generated drones for sort of highlevel browsing. so, if you were somewhere in the microelectronics areašwell,you won't be now, but if at the time you were in the microelectronics area, you would be contributing to the volume ofsome overall drone.as you went deeper, you would be contributing to, let's say, a pulse or some other sound that was all at the samepitch. then, the tonal balance of the sound would vary based on the proportion of people who were at different parts ofthe site. so, here is the kind of mapping that we used, just to cut to the chase. this is what lucent.com sounds like at6:00 in the morning. [audio played.] just to give you kind of a lonely feeling. at 6:00 o'clock in the morning, there areprobably 15 people rattling around the site. at 2:30 in the afternoon, we get most of our visitors and it sounds morelike this. [audio played.] so, the deal was to make that somehow kind of pleasant and easy to listen to, and it mightinform you of something.the idea of the sound at some level, the unique featureši see a lot of skeptical faces. this is a crowd ofstatisticians and you are supposed to have a lot of skeptical faces. i was with david cox at the spring researchconference called the cautious empiricists or something. the ﬁcautiousﬂ is the important thing. the idea of it was thatsound somehow gives us the capabilityšyou can attend to the sound in a way that you don't attend to sort of the visualsystem, or you can background sound and you can't really do that with the visual system. so, you can have somethinggoing on in the background and you can attend to changes in that something, in the musical track, without reallyhaving to listen to it hard. the visual system requires you to watch a display or watch a plot. so, we came up with thissort of general map of the web site activity, and then a graduate student at tufts wrote me that he didn't really like thistonal balance that we got, he thought it was maybe a little too ravy or a little too something and he didn't really care forit, and he preferred more natural sounds.so, he created sounds like thisš [audio played.] što give this, which is telling you something about the networktraffic. the patter of the water increases in volume with the more users who are on the system. the one of the birdsounds is incoming mail. so, you can kind of get a sense of what is going on. anyway, he seemed to think that wasmore listenable. at some level we decided that these experiments in sonification were interesting, were certainlycreating some music that we didn't mind listening to, but they weren't particularly practical. also, they didn't speak tomany people, because very few people care about any one given web server. i mean, the number of people who wouldcare about the traffic on lucent.com is quite small. if you think about most web servers, that is going to be the case.so, we decided that we needed to find something that had perhaps a little more social relevance. so, we decided wewould keep to kind of the communications realm and look at online communications.in some sense, as i pointed to before, with the amount of email traffic and such, the web is really just a bigcommunications channel. our thought was, perhapsuntitled presentation213statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.aggressively, perhaps we were a little too whatever, but could we characterize the millions of conversations that weretaking place right now. you had people who were in chat rooms, hundreds of thousands of people in chat rooms,people posting to bulletin boards. can you say something about what all these people are talking about. in some sense,these chat rooms and bulletin boards represent new spaces for public discourse. if you take them together, theyrepresent a huge outpouring of realtime data, which is kind of begging to be looked at. there is a lot of structure here.there are sorts of chat sessions that are kind of daytoday things. in the morning, it is what are you having forbreakfast and in the middle of the day it is, my boss is riding my back. at the end of the day it is, this is a great day, iam off to bed. in between, you have got lots of sort of, not just cycles about sort of daily things, what is going on thismorning or what is going on at work, but political arguments about terrorism or afghanistan or something like that. so,our thought was that we would try to create some kind of tools to give us a better understanding, or tap into this bigstream and sort of experience this in some way that perhaps is a little bit more accessible to the general public than justa plot or a graph or something like that.so, here is the kind of data that we are basically looking at, and we have lots of it. so, you get some sense of asource, in this case, suppose all of these are irc chat rooms. so, you get the room, the name of the room, and then youget the user name and what they posted. we have agents, and i will talk about that in a little bit, who go out and sort ofsample from these rooms. so, we will attach to a network and sample from tens of thousands of rooms, to kind of getan overall sense of what people are talking about.so, the interesting thing about this project is that not only has there been some statistical contactšand i havegiven talks about some of this stuffšbut there has also been the opportunity for public performances or public eventsaround it. the first thing we did with this chat space was a performance at a place in new york city called thekitchen, which is a fairly well knownšpeople like laurie anderson and stuff got their start at the kitchen.it is in chelsea and at that point we were looking at about 100 chat rooms and bulletin boards. we were looking atsort of news chat, sports, community. there was a beautiful room on the care and feeding of iguanas. i have told thisstory a lot. the beautiful thing about this room is that, after sort of monitoring it off and on for three or four months,they only used the word iguana like five or six times in that period. so, they don't sort of refer to iguanas as iguanas. itis like baby or honey or my little something or other. i found it sort of amusing that you couldn't really tell what peoplewere talking about, if you didn't include something you knew about the room itself.from there, we were monitoring for topics, looking at activity levels, that kind of thing. now, the display that weput out, because this was a performance base, it was part of their digital happy hour series. so, we got a big room,perhaps a bit bigger than this, extraordinarily tall ceilings, and we had the picture at the bottom as a sort of layout ofthe rooms. there were round tables that people sat around because it was a digital happyuntitled presentation214statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.hour. there were four speakers, one in each corner of the room, and in the white bar at the top was a very large screen,about 20 feet tall, 20 feet wide. so, the display that we picked involved streaming the text along four lines at the top,and then each line of text was being read by one of the speakers in the room.i can tell you what that looks like. i have a small movie of very bad quality, but we get better stuff later. here youget the full lines of text. [tape played.] the sound is a tone, and then there is a voice that is speaking at that same tone,in a monotone, so you get sort of a chant effect. there is an algorithmic structure that is used to generate the pitches. itwas picked according to the length of the post. so, we wanted to have it clear. if it was very short, it would take thevoice only a short amount of time. so, that was, first of all, the texttospeech was horrible. that was kind of thestandard mac texttospeech voice.we only had, like i said, we only thought we had about 100 rooms, but we thought the structure was nice, havingthe text up there, having the texttospeech to guide you, and having the compositional element helped to keep people'sattention. they were sort of watching this thing. at that point, there wasn't a lot of organizational structure put to it.we just sort of randomly selected representative phrases from the individual chat room and let whatever collide,collide, in terms of meaning or content or whatever. so, that seemed to work out reasonably well. so we posed forourselves another challenge which was, could we in fact display sort of largescale activity in realtime from not just100 rooms, but tens of thousands of rooms? as ben keeps saying, we aspire to listen to it all. the ﬁaspireﬂ word meansthat we don't actually have to achieve it, but that is where we are headed.again, because we were, for some reason or another, extraordinarily lucky for sort of public performances to keepmoving this along, we were part of the next wave festival sponsored by the brooklyn academy of music last year in2001. i will show you some pictures of this. here, instead of having the one large screen with four lines of text, wecreated a 7foottall, 10footwide grid of small text displays, fluorescent vacuum displays, about the size of a hersheybar, each one. there were, like i said, 110 of them and they could show text that we were generating. instead of havingjust four voices at a time, we used the speech engine, which would allow us to have up to 45 voices speaking in theroom at a time on eight channels of audio.so, this was the little sign that they put out in front. this is what the installation space looked like. therockefeller foundation kicked in and we were able to build an installation space. you see the hanging grid of smalldisplays, and then the room itself, the silver panels conceal, in some cases, speakers, and in other cases just acousticinsulation, so you don't get a lot of flutter echo from the walls. here is what each of the little gizmos look like. this isa standard noritake display and we had a printed circuit board designed with a microcontroller on board, so that wecould communicate with this. this is rs45, for those who care. the two wires on the left are carrying communicationand the two wires on the right are carrying power. so, you see that these two things areuntitled presentation215statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.hanging from the same things that we are talking to them on and powering them on. so, here is another view. we havethis very tight text, sort of four lines of 20 characters, and then we also have this big character mode where we canstream text across the screens. here are some pictures that were taken at the time. this is the back. the back has anled on it. in fact, in the bam room, the brooklyn academy of music room, you enter it in the back.this wedge over here is the doorway, and you would enter in the back. what you would see is the leds are thepattern. so, you come into a very sort of abstract space, and then you move around and see all the text. the piece itselfwas organized in a series of scenes or phases that were all meant to highlight some different aspect of the stream. insome cases, they are quite simple. all they are doing is giving you a tabulation of all the posts by length, and streamingit by, so you not only get a sense of scale, like how much conversation is going on because things are streaming byfairly quickly, but you also get a chance to see the short posts, a lot of hi, hey, hi, and the longer posts werešat thattime, there was a whole lot of talk about afghanistan and john walker.i think there was one friday when we were up at bam when wynona rider was arrested. if only we timed itbetter, this time we would have been able to see her being sentenced. anyway, that was a very simple one, but thesecond scene tries to organize things by content and creates kind of a dynamic version of a selforganizing map. youget large regions that are all referring to the same topic, and the regions grow in response to the proportion of peoplewho are talking about that particular topic.so, if afghanistan is popular in the news and lots of people are talking about it, that region will grow quite large,and that depends quite heavily on the texttospeech that you are using. then there are other things i won't have time toillustrate. this is the kind of thing that end up coming up from the map scene that i will show you in a minute.to generate all this, i guess i should give a little bit of talk about the stream itself. we have a series of java andpearl clients that are on the protected side of the lucent firewall, that are going out and pulling things from chat roomsand bulletin board. then, on the other side, in the display space, we have four computers, one running sort of thesounds in the room, one running a texttospeech engine, one running the individual displays themselves, and then onekind of overseeing all of it. the unfortunate thing is that all of those machines are running on a different operatingsystem. if you can think of another operating system, we would be happy to include it.so, it is all about interprocessor communication somehow. on the lucent firewall side, we have two linuxservers and two class c networks that give us the capacity to look like about 500 different ip addresses. the chancethat we are going to get somehow spotted and caught and turned off seems small, although, every time i say that i am alittle nervous. so, we have upgraded the texttospeech engine as well. we are using lucent commercial heavydutyspeech engine, that can give us access to about 100 voices in the room, for this whitney exhibit that i will show in aminute. [audio played.]can we show the dvd now? i am going to show a couple of examples of theuntitled presentation216statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.installation. then i have some production pictures. the construction just started at the whitney. i apologize in advancethat this is going to be a little hard to see. [dvd shown.] this is just that simple tabulation where, when things are veryshort, the screens are less bright and the sounds are very soft. i will just show a little of the next scene. this is the onethat has the content, or builds up a map dynamically, and here we will have some voices. [dvd shown.]so, as i said, to get to that point, there is, like i said, a series of scenes that this thing alternates through. eachtime, because the stream is live, the text that is spoken and the scenes you experience of it are different, because thedata are always changing. we are trying to get the buffering now to about 15 minutes, so that everything you see willhave been typed just 15 minutes ago. so, there was a series of things that had to happen to pull things from the chatstream and, given the time, i am not going to go into too much detail. there are things like trying to characterize topicand track features by person and room and do some clustering and what have you.so, from the brooklyn academy of music, we went on tošactually, the friday after we closed at the brooklynacademy of music, we were sort of summoned to a morning at the whitney and an afternoon at the museum ofmodern art in new york, where we met with curators who were, well, let's talk about having our piece here, whichwas a little humbling and frightening.in the end, we were fortunate enough tošwell, we are opening at the whitney museum of american art, anexpanded version of the piece is opening in just a few days. it is an enhanced display space. so, the display is not,instead of having 10 feet by 7 feet, with 110 displays, which was 11 rows and 10 columns, we now have 11 rows and21 columns, which spans 21 feet. so, the thing is big, and it is in a soft curve. we had a workshop residency, ordevelopment residency with a performing arts organization in seattle called on the boards. they gave us like a studio,a kind of stage. you can see part of this grid, the curved grid, and then we got to sort of litter the place with ourcomputers. this is what the thing wound up looking like in the end when it was up in seattle.we started construction at the whitney as of monday. this is the whitney lobby. this, right there, is where weget to go, which again is an awesome thing, to think that there will be people walking by. so, here is the inside of thespace as construction is going on. the first thing they had to do was put up a wall that is going to have all of ourpanels, the concealed panels. this is the curved beam from which the text displays are going to be suspended. this ismy collaborator, ben. i am very happy that the beam is up in the air. it was a nontrivial problem to get it attached tothe ceiling. this is part of the process. now, speakers are mounted behind the beam, so that voices are spatialized. thisis where i left it when i got on the plane earlier this evening. the carpet guys had just arrived and were puttingcarpeting in the room. so, it was kind of a lonely little place. i guess where this project goes, we have been looking atkind of a stream of data, a stream of text data. i don't know how much sort of text has been talked about here, but it isuntitled presentation217statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.messy. our audience isn't a technical expert, per se, but the general public, and how you create kind of data analysesbases, in a way, that speak immediately to the general public.some other applications that we have been working, we have begun a joint project with bill seaman, who was atucla and now is at risd, jointly with ucla. we are tiling a large room at ucla with these sorts of condensednetworks. so, it is a sensor network, which i have heard people already talk about, but we will have sort of wonkythings like speech recognition and temperature and things on these sensors, for an inside of the room. then, all of themwill report wirelessly back to a central place, where we will be dealing with the streams. we have also looked withsome lucent people at perhaps network operations. when we first started this, we were talking to some people in themanufacturing lines.an interesting application, we were approached by an architect, rem koolhaas, to help for a building he wasputting up at the iit. the idea was, he was going to give us access to just streams of facilities data, occupancy sensors,data from boilers and what have you, and that we would create a sound in the foyer of this building. with repeatedexposure to the foyer of this building, people would be able to know exactly what was going on in the building, just bythe sound when they walked in. this is sort of a look š it is sort of a wonky artistic look at what the building issupposed to look like.there is a bowling alley in the space and a store, and we were going to get access to all of those data in realtime.anyway, i guess i should summarize because i need to get on a plane, because tomorrow we have to start putting up,now that the carpet people are done.so, it began as a collaboration between, somehow, the arts and sciences, and there was an interesting interplay ofviewpoints between the two. what i am finding is that there is a community of sort of artist folks who are trying toreach out for collaborations with the scientists. the collaboration is really highly valued and they are looking at andstudying, kind of, how collaboration works and why it does, and how you can make it successful and how you promoteboth art and science through that collaboration, and not have just sort of one side of things. my work with ben hasbeen motivated, in a way, by new and complex data sources, large quantities of dataši suppose that is the massive partšand there is a strong time component. i guess that is the streaming part.ultimately, the goal is to create kind of new experiences with data and, in particular, to create public displays ofdata that somehow speak to the general public.with that, i will thank you for spending your digesting time with me.audience: how long are you at the whitney?mr. hansen: three months. we open the 17th and we are up until march 9. if anyoneši have invites to theopening, if anyone would like to come. the opening is on the 20th, the party.audience: [question off microphone].mr. hansen: that is exactly thešthat kept coming up again and again. we had a formal critique, actually,with some curators at moma and some artists, and thatuntitled presentation218statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.was extremely important to them, that it be live, and that it bešthat the process from data to display was legible, andyou weren't kind of tampering with it.there was some notion of unbiasedness that came out there that they didn't really have the words for, but it wasdefinitely there. there is no filtering. it is funny. even if there is no filtering, the bad words don't get people. it is thereally bad thoughts, somehow. chat is like really bad talk radio, but it is even more anonymous, because you don'teven give your voice. so, you can type just any horrible thing that is off the top of your head. that is what seems to getpeople, when something comes across that is just like really hateful. it is not even clear to me how you would filter forthat kind of hateful stuff, because it is not just the four letter words or whatever, which would be easy.audience: in terms of interaction with the public, what sort of things have š [off microphone.]mr. hansen: there were some things, actually. we have another scene that we have just put in. so, thisdocumentation video has four scenes, one that goes dee, dee, dee, dee, dee, one that has the spinning and the talking.there is another where we kind of blast along the streams, and then we have another that just gives a listing of the usernames. we have added a few more. one of them was looking at howševery few hours we look at the most frequentways in which people start their posts. inevitably, aside from welcome back, which we kind of toss outševeryone getsstop lists. so, we toss our welcome back.after that, it is i'm or i am, is the most frequent way that people start their posts. so, we have started kind of alitany of those as one of the scenes. our host in seattle, who is this sort of semijaded art curatorial type, was in tearsover this scene. i wasn't prepared for it. you know, you kind of present this in a kind of human way because, at the endof the day, it is about people communicating. if you present this in a reasonably straightforward way, i think it has animpact, and that sort of surprised me. i should say, in seattle, a very strange thing happened. so, we were there forthree weeks. the first week we were just setting up. the second week, we were open to the public, and the third weekwe were open to the public. the third week, we got written up in the seattle times and what have you, but we startedsort of marching up this crazy attendance curve. like, the wednesday, we had like 50 people and the thursday it was90 and the friday was 150 and the saturday was 311 and the sunday it was 652, who came and just sat in this room foran hour, hour and a half, at a time. it blew my mind. people would come up afterwards and tell me what they thought itwas doing and what they thought about it, and that was very surprising to me, that it would be sort of well received likethat. it made me very nervous, at the same time.audience: [comment off microphone.]mr. hansen: i heard someone mention anomaly detection earlier. you people talked, could you use this toscoop up lots of chat and then find a terrorist or something. i think our approach has been to like sample and give a bigpicture, like what is sort of the broadši don't know that there is any practical use for it, really. i mean, there is a lot ofuntitled presentation219statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.data analysis that i think is interesting to pursue but, like practical, i don't think so.audience: i guess i disagree with that. if you think of an analyst that has to try to assimilate all the informationthat is coming in, if you are actually moving in a directionš [off microphone.] šoptions that they have, to make iteasy forš [off microphone.]mr. hansen: we thought about it. for those sorts of systems, like for the sonification, we thought, would be anatural for that, because you could hear a process drifting before an alarm would be set off in a statistical process, likein a control chart of some kind. so, we thought about that and we kind of toyed with that. then we were quicklyderailed with this text stuff and kind of went off in a different direction. i think that is an application, that you will beable to hear shifts in the background. even something you are not directly attending to, you will be able to hear kind ofshifts. so, for realtime process monitoring, i think it has got some applications for sure.audience: [question off microphone]mr. hansen: we do that all the time with our laptops and say, oh, this is a problem.audience: i would point out that the acceleratorš [off microphone] šif it deviates one little bit, you notice it.if it is something phenomenal, you would hear that.mr. hansen: there isši mean, we do a lot of analysis in thešwe do a lot of information gathering with ourauditory system in our regular life. we start up the car and immediately we know if there is a problem. i mean, we aredoing that all the time. the question is, can you apply that in a data sense.audience: i was wondering if you had spoken with some of the people who are working on sonification ofthings like web pages, and mathematics.mr. hansen: we have been to a couple of these icat meetings. so, there is an international community forauditory display and they have a meeting every year in some very exotic places. when it was in finland, i rememberthere was a sessionšit was crushing to see how primitive that technology was, about how blind people were forced tokind of mouse over a page until they hit the radio button or something. it was horrifying to see what kind of state of theart there was at that point. that is a huge area for work that i don't know whošdavid and i were at some digitallibraries meeting at dimax.i think one of the things thatšwe were supposed to propose things that people should be doing as more and moredata libraries are keepers of more and more digital data. one of the things we were pushing for was assistivetechnologies like that. horrifying is the word, the kinds of things that people have to do. maybe i am more sensitive toit, because my mom is slowing losing sight. i am trying to get her to use speech recognition and things like that, and itseems like a really good research area.audience: different kinds of voices, i didn't hear any different voicesš [off microphone] švoices in differentlanguage.mr. hansen: it is the typical kind of male engineer response when you go, well, why aren't there any womenvoices.untitled presentation220statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.so, we asked the people who make the texttospeech engines. well, we have had a lot of problems with her.they can't like get it. i mean, they have a lot of thešthere are two sort of voice qualities. at the highquality end, theyonly have the male voice. at the lowquality end, they have several males and several females. we really wanted thehighquality one, because it just sounds so much better. they have one now that we just started getting working as wewent to seattle. we are hoping we can get it wedged into the whitney show. that was one criticism that we had. whenyou get these 45ševen though they are british inflectedšwhen you get these 45 voices going at once, it is a verymale space, and sometimes it can be very heavy. the female voice is quite nice. it sounds something like an nprannouncer. she just keeps crashing. she will like crash thešwe had a problem with the male voice initiallyšactually,this is a nice story. we had a problem with the male voice and that is that it would stay up foršwe had a set of testdata. we weren't running it on live data. we had a set of test data. inevitably, after two hours it would crash. justbefore we were going to seattle, this kept us debugging and working and figuring out. we had, you know, theengineers from the lucent speech thing. i mean, they were like in the studio until like 2:00 and 3:00 o'clock in themorning.in the end, it was the word, abductor. there was something about the word abductor that brought the whole thingdown. they had to bring in some new thing or whatever. i thought it was beautiful that it was the word, abductor. itkept us in the studio for a very long time. there was a fix, and we think something like that can fix the female voice,but as of last thursdayšbecause these things always happen on thursdayšthe last of the texttospeech people atbell labs were laid off. we are hoping that we will be able to get something like that going. they have french, theyhave italian. they have spanish. we stayed away from other languages, because i can barely speak english. so, i canbarely do what i need to do and see that it is working right in english, much less in these foreign languages. there isthat capacity. if, somehow, we find someone with a better grasp of languages, we can try that.audience: this being a collaboration with artists, was there any piece that made it really difficult to understandcertain parts, given the real mathematical sorts of things šmr. hansen: we are a slightly weird pair. i took a lot of art classes at undergraduate. my collaborator took alot of computer science classes as an undergraduate. to the extent that kind ofšthe stats on the computer science i canfind someplace to overlap. we did have some very difficult discussions around the concept of sampling. in fact, thiscame up at the critique, where the curators kept using the word privilege, why are some data points privileged oversome others. it is not that they are privilegedšit is sort of a hard thing to get over. we had some really sort of rockyevenings where we had to explain, we don't need to take all of the data and throw it at some collection port. udpprotocol has no guarantee. so, packets will get dropped all over the place. so, rather than sending sort of a millionmessages at this poor port and just grabbing whatever sticks, we could send as many as we needšthat was a conceptuntitled presentation221statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.that was just really hard to get through. then, like i said, there was this privileging concept, and legibility seems reallyimportant. initially, there wasn't a lot of trustšnot my collaborator, but the curators and stuffšwhat is this all doing.something like this would be too easy to twist. if we only went to like aryan nation web sites, the thing would have atotally different character than it does now.so, there has beenšthe other thing i have noticed, and i am sorry to be kind of yammeringšbut the other thingthat i have noticed is that these media artists are a lot more savvy technically than we give them credit for, maybe notstatistically, but software and hardware wise, they run circles around them. not my collaborator in particular, but a lotof them will run circles around us. that is kind of whyšso, my position in ucla that i am starting in april, is jointbetween media arts and statistics, and i will be teaching joint classes. i think that it will be interesting to have mediaarts students with stats students, in the sense that the stats students aren't going to have the same kind of computingskills that the media arts students will, and the art students just won't know what to compute.so, it is going to be kind of an interesting interplay, i think, of approaches to problem. introducing to both a groupof media arts students and statistics students the concept of a database and how a database works and all that, there is abig thrust now about database aesthetics, not just the politics of databases, but there is an aesthetic to them as well. so,i think that that is going to be kind of interesting. i suppose the last comment i want to make is that my collaboratorhas this interestingševerything should be doable, and that kind of pushes me a little farther. of course, we should beable to string these 231 displays and, of course, we should be able to update the entire grid 25 times a second. that hasbeen the other thing, that of course we can do it and we just haven't hit kind of the limits yet.thank you for your time.untitled presentation222statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.wendy martinez, chair of session on network trafficintroduction by session chairtranscript of presentationwendy martinez is a scientist in the probability and statistics division at the office of naval research.introduction by session chair223statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.transcript of presentationms. martinez: welcome back to the second day of the workshop on massive data streams. without furtherado, i will introduce the first speaker, who is bill cleveland. i am not going to take up any of his time. so, we will turnit over to him.introduction by session chair224statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.william clevelandfsd models for openloop generation of internet packettrafficabstract of presentationtranscript of presentation and pdf slidesbiosketch: william cleveland is a distinguished technical staff member at bell laboratories. he specializesin internet engineering, visualization, model building, visual perception, consumer opinion polling, and bayesianstatistics.his professional service has included editorial positions with technometrics, the journal of the americanstatistical association, the wadsworth probability and statistics series, and the collected works of john w.tukey. heis a former member of cats.dr. cleveland received his phd in statistics from yale university. he is the author of two books on visualizationand analysis of data: the elements of graphing data (chapman and hall, 1994) and visualizing data (hobart press,1993).fsd models for openloop generation of internet packet traffic225statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.abstract of presentationfsd models for open loop generation of internet packet trafficwilliam s.cleveland, bell laboratories (with jin cao and don x.sun)abstract: the packet traffic on an internet link is a marked point process. the packet arrival times are the pointprocess, and the packet sizes are the marks. the traffic arises from connections between pairs of computers; for eachpair, the link is part of a path of links over which files are transferred between the computers; each file is broken upinto packets on one computer, which are then sent to the other computer where they are reassembled to form the file.packets arriving for transmissions on the link enter a queue. many issues of internet engineering depend heavily on thequeuelength distribution, which in turn depends on the statistical properties of the packet process, so understandingand modeling the process are important for engineering. these statistical properties change as the mean connectionload changes; consequently, the queuing characteristics change with the load.while much important analysis of internet packet traffic has been carried out, comprehensive statistical modelsfor the packet marked point process that reflect the changes in statistical properties with the connection load have notpreviously been developed.we introduce a new class of parametric statistical models fraction sumdifferent (fsd) models for the packetmarked point process and describe the process we have used to identify the models and to then validate them. themodels account for the changes in the statistical properties through different values of the parameters, and theparameters are modeled as a function of the mean load, so the modeling is hierarchical.the models are simple, and the simplicity enhances the basic understanding of traffic characteristics that arisefrom them. the models can be used to generate synthetic packet traffic for engineering studies; only the traffic loadand certain parameters of the size marginal distribution that do not change with the load need to be specified. themean load can be held fixed for the generation or can be varied. fsd models provides good fits to the arrivals andsizes provided the mean connection loadšthe mean number of simultaneous active connections using the linkšisabove about 100. the models apply directly only to traffic where packets on the link input router delay only a smallfraction of the packets, about 15 or less; but if delayed traffic is needed, it can be very simply generated by putting thesynthetic model traffic through a queue.c code is available for generation as well as an implementation in the widely used ns2 simulation system.fsd models for openloop generation of internet packet traffic226statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.transcript of presentationmr. cleveland: thanks. i am going to be talking about statistical models for internet packet traffic. themodels contribute in adding to the basic understanding of internet packet traffic, which was very much in need ofunderstanding, and also provide a mechanism for openloop generation of packet traffic for engineering studies.so, what i am going to do is, i am going to start out by giving you just some information about internettechnology. i need to do this. otherwise, the issues that i will be raising and tacking into modeling just won't be clear.so, we will go through that. i will talk, then, about packet arrivals and sizes, which constitute the traffic, as we havemodeled it. it is a mark point process. i will describe previous work in looking at the statistical properties of thistraffic, and also i will describe the importance of the modeling of these properties for engineering the internet.i am going to tell you about a new class of statistical models that provide very good fits to the packet mark pointprocess. as i said, they do provide help in a basic understanding of packet traffic. they also help to reverse a verycritical central conventional wisdom about packet traffic, and can be used to generate traffic for quantitativeconclusions.we have c code, and we have also implemented this in a very widely used ns2 network simulator that is usedby the internet research community.fsd models for openloop generation of internet packet traffic227statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.so, the technology. internet communication consists of transfers of files between pairs of computers. it istransferred in the following way. the file, sitting on the source computer, is broken up into packets. these packets are1,460 bytes or less.here is an abstract of a talk i gavešaudience: bill, is there any discussion of increasing theš [off microphone.]mr. cleveland: no, that is the max, that is the rules.audience: it is such a stone age thing. technology has changed.mr. cleveland: okay, well, the answer is no. there is no discussion of that whatsoever, because we have gotmillions of computers out there sitting in peopled homes where that is the max. so, there is a lot of inertia. one doesn'thear that, no.okay, so, what happens is, the file is broken up into these packets, 1,460 bytes or less. here is an abstract of a talkof reasonable size. that just fits into 1,460 bytes. so, if you want to calibrate how much information goes in, it is areasonable abstract's worth of information.to this packet, a 40byte header is added. that header has a lot of information about the transfer. it has the sourcecomputer address, the destination address, the size of the data going into the packetšthe amount of information in thepacket, and a host of other variables.the packets are transmitted from the source to the destination, across the internet, and then they are reassembledon the destination computer.now, to carry out this transfer, the two computers establish a connection. that is the word that is used. inaddition, 40byte packets, which are all header, and no data, arefsd models for openloop generation of internet packet traffic228statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.used to manage the connection.each sends the other control packets. the nature of this connection is just simply software daemons running onthe two computers, listening for arrivals from the other computer.it begins when one computer sends a packet to the other saying it would like to open up the connection, and itends when control packets are sent back and forth between the two computers, agreeing to close the connection.let's take a look at an example here. so, i am sitting at bell laboratories with my laptop, actually, this one righthere, and i download the uc santa cruz applied math statistics web page, because i am going to be going there togive a talk and i need to see directions to drive down from berkeley. so, i download that web page and the connectionis set up. here is what the connection looks lie. so, here is my laptop sitting here at bell laboratories.a packet coming from my laptop and going to uc santa cruz goes through this series of routers. so, what we areseeing here are devices which are routers, and every successive device here is a link, over which the package is sent.so, we start out here at my computer. here is a first router at bell laboratories, and then we continue on and hitworldcom routers, then we go through crest routers, and then we hit the california educational network, and thenfinally get to uc santa cruz.so, there are two computers herešmy laptop and their serveršthere are 18 routers in between us, and there are19 links connecting these devices.fsd models for openloop generation of internet packet traffic229statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.now, when it first starts out, the packet is at the edge of the internet. the link speeds are low. there are fewerconnections using the links at any given time, just less traffic generally.as we move into the core and hit the service providers, the link speeds are going up, the usage of the links goesway up. so, this is the core, higher link speeds, more connections. then, as we travel on farther, getting close to ucsanta cruz, we are at the edge, lower link speeds, fewer connections.what i want to do, i want to take a look at two routers here. this one, svlcore, and svledge, and let's take acloser look at them.so, we have packets coming in on different links, and then we have packets going out here on this link, and thenthis particular link here that goes to the next router that is in the pack from my packets.now, at any given time, on that link, there are a number of simultaneous active connections using the links, notjust mine, of course. there are others sharing the link.the packets of the active connections are intermingled on the links. so, if we š say, down here, let's number theconnections one through n. so, a packet comes from one and then another from one, and then from two and from oneand three and so on. so, they are intermingled on the link.in the packet network literature, they describe this as statistical multiplexing. in the statistical literature, they referto it as superposition. you might think it would be the other way around, but it is not. i will tend to use statisticalmultiplexing in this talk.fsd models for openloop generation of internet packet traffic230statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.now, the packet is arriving at the first svlcore router, destined for this router over this link in question. they gointo a queue, which is a buffer, to wait for transmission. of course, if there are no other packets in the queue, they justgo right off. there is no waiting. if the buffer is full, the packet is dropped.now, once a packet gets to the front of the queue, it is put on the link at a certain link speed. actually, link speedis the wrong word. i don't know why anybody ever invented that word, because it is not the speed of the link. i mean,things go down the link at the speed of light.it is actually the speed of the device. it is the speed at which the device puts the bits on the links. so, it is really adevice transmission speed.now, supposedly the link speed is 100 megabits a second. this is a common speed that you would find inethernet networks, local networks, perhaps at your university or work location.so, 100 megabits per second, that is 100 bits per microsecond. so, if we take a maximumsize packet, which is1,500 bytes or 12,000 bits, the service time to put the packet on the link is 120 microseconds.for these control packets, the smallest possible packets, the service time is 3.2 microseconds. so, we have got aqueue. that is a key issue in internet technology. we will get back to that shortly.now, if we want to understand internet traffic, one extremely useful and effective measurement scenario is toattach a device to a link. so, we want to understand the packet traffic on a link. we attach a device to the link thatmirrors all the transmission.when a packet arrivesšthat means when the first bit goes on the link, you write a time stamp, so you know itsarrival time, and you capture the header contents. now, some people might want to capture the content. we don't, andmost people doing internet traffic research don't capture content. we don't want to know what is in the packets. a lotof it is encrypted anyway. it is not our business, and it doesn't contribute to the kinds of engineering questions thatneed to be asked. well, some of it actually would, but we still don't get it. it is too tricky.fsd models for openloop generation of internet packet traffic231statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.a packet trace consists of the headers and the time stamp. that is the term that is used. so, the data are a packettrace on the link over some period of time. now, in our study, we have put together traces from 17 links around theinternet. the durations of these traces from the 17 links are from anywhere from one hour to four years.the link speeds go from 100 megabits per second to 2.5 gigabits per second. the highest packet rate of collectionis this 2.5 gigabit per second link, which is the link of a service provider out on the west coast. the rate there is80,000 packets per second.so, for this presentation, i am going to be showing you a couple of pictures that show 2,072 subtraces from thiscollection of traces. they are broken up from 15 second to 5 minute intervals duration, because that is a standard waywe analyze it, because we want to break things up to keep characteristic stationary within the interval of study, andthen watch how that changes through time.question: how did you pick the links?mr. cleveland: we picked the links to get a diversity of traffic rates. so, when we started studying them, westarted at the edges. the first data we got we collected ourselves, and we started analyzing that.there were lots of characteristics that were quite interesting, but we realized that, if we wanted to reallyunderstand the traffic, we had to be in places where the amount of multiplexing was much higher, because we startedseeing things higher, as the number of active connections increased.so, that was a major factor. we also picked the linksšsorry, some of these data are already existing measurementprograms. it is not as if we went off and measured a service provider.we began establishing relationships with people who actually do internet traffic measurement, and we knew weneeded to get to links where, as i said, we knew the links were high. also, we needed to get data that were extremelyaccurate, because we pushed the time stamp accuracy to a very high degree, and much that we do is highly dependenton that.question: so, everything you are analyzing is a highaccuracy time stamp?mr. cleveland: the accuracy actually varies. it is sufficiently high, yes, for the kinds of things we need todo. we threw out a lot of data, like data from harvard. maybe i shouldn't say that. anyway, people were collectingdata at harvard, and we looked at it and we said, this just won't do.the unc data doesn't have especially accurate time stamps for us to do our work, and we knew that when we setit up. we knew it would not have it.fsd models for openloop generation of internet packet traffic232statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.i mean, there are other purposes you can use these data even without the highly accurate time stamps. there are alot of other tasks that can be carried out if your time stamps are actually quite inaccurate, because there are other thingsto be done.audience: you alsoš [off microphone.]mr. cleveland: the packet headers contain information about the order. i mean, they have to, because whenit gets on the receiving computer you have to collect them and put them back in the correct order. so, the packetheaders carry all that information. so, we have that, yes.audience: [remark off microphone.]mr. cleveland: i am sorry, i am only talking about one specific problem here in terms of when one analyzesinternet traffic. i mean, there are a whole host of problems that one can attack. i am telling you about one specific, veryimportant, but one specific problem. so, these variables don't represent all the variables that one can get off the packetheader.all right, the packet arrivals and sizes are a mark point process. the arrival numberši will let the u be the arrivalnumber, u=1, that is the first packet, the second packet is u=2, and so forth.so, i will let au be the end arrival times, and tu=au+1)au, are the end arrival times.i will let qu be the sizes. so, qu is the size of the packet arriving at time au. now, we are going to need some othervariables here, also, in addition to this.i need a measure of multiplexing, a magnitude of multiplexing, and here is one measure we use. there are severalof them, but here is one. it is just simply the number of connections that are active at a given point in time. now, overan interval of time, if we want a summary measure for a tracešsay a 15minute tracešthen you just take the averageof the number of active connections over the 15 minutes. so, that will be c.audience: so, you can't know that in real time.mr. cleveland: actually, you can know it in real time. you have a time. the devices like firewalls have tokeep state on connections. so, you do your best to try to figure it out. you can do better offline, of course, but you justdo your best if you have to do online things like security.so, here are the traffic variables that we are going to be studying today, a few of many.i told you there were 2,072 traces i was going to be describing to you. let's takefsd models for openloop generation of internet packet traffic233statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.a look at the connection loads.so, i have got the log of the average active connection load for 2,072 traces. so, this is log base two, and this is aquantile problem.so, we are going from 24, actually 23, about 8, up to 214, so that is 16,000. so, we are going from an average of 8connections, active connections, at any given time up to 16,000. so, we have got a very wide range of traffic rates.again, that was a goal in being able to get data for this purpose here.here is one of these traces, a 5minute trace. so, i have taken the log base 10 now, or the end arrival time, andapply it against the arrival number.so, it is something like 7,000 packets arriving during this particular 5minute interval on this link. so, you see thelog goes from a little bigger than !6. so, 10!6 would be a microsecond. so, the smallest end arrival time isšwell, it isactually 3.2 microcycles. it is the arrival time of the smallest packet on the link. it goes up to about a second.so, we are going through nearly six orders of magnitude in these end arrival times. we do have accuracy in thisparticular case down to that level.fsd models for openloop generation of internet packet traffic234statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.here are the sizes, the packet sizes plotted against the arrival number, except i have had to take the 40bytepackets, of which there are many, and the 1,500 byte packets, of which there are many, and jitter them a bit, so that weget some better resolution on the plot.so, here they are, plotted against time as well. now, if you look at it you say, gee, that actually looks fairlyrandom.if you look a little closer, though, you see they are actually bunching up together, aren't they. so, we get endarrival times that seem to come in little bursts here, and there are sort of striations that you can see. so, just from thistime plot, you see that there must be time relationships, time correlation. well, it looks noisy. anyway, let's stoptorturing ourselves. let's look at the autocorrelation function. here is the autocorrelation function of the log end arrivals.fsd models for openloop generation of internet packet traffic235statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.what you see is that the correlations are all positive, and they fall off very slowly. so, we don't have too muchcorrelation at lag one, but we still have a fair amount left at lag 100. if we looked at lag 500, we would still see a fairamount of correlation. the same is true of the packet sizes.the sizes in the air arrivals are longrange dependent. that is a critical factor of the internet traffic that has amajor impact on the engineering of the internet.so, let's take this one 5minute trace. i am sorry, i actually didn't define for you the byte count. actually, i am notgoing to be looking at those variables, but i do have an important comment to make about them.so, the packet counts, you take an interval, say, of 10 milliseconds, and you count the number of arriving packetsin 10 milliseconds through time. so, you are getting counts rather than arrivals and sizes.for the byte counts, a similar process, except that, instead of just counting the number of packets during theinterval, you add up the sizes of all the packets arriving during that interval. so, packet counts and byte counts, theyare all longrange dependent, sizes and arrivals by packet counts.the autocorrelation falls off slowly, like k2d!1 between 0 and .5. keep in mind the arrivals would be poisson, ifthe end arrivals were independent and identically and exponentially distributed.the arrivals are not poisson because they are neither independent, nor are they, at least on this particular 5minutetrace, nor are they exponential, and the sizes aren't independent. so, nothing is poisson and independent.fsd models for openloop generation of internet packet traffic236statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.now, why do we need to model the traffic mark point process? the packet queuing delay and service time add tothe transfer time of files on the internet. so, when you click on a web page and it takes an immense amount of time toarrive, it can well be that packets are being delayed in routers along the path. it could also be that the server is justslow. that is another source of pages slowing down on the internet, but the congestion, when it gets bad, it gets reallybad. so, that is a major factor in how long it takes you to get a web page.packet loss, if it is full and the packet gets dropped, it is even worse. the sources detect this in many cases, andstart slowing down the sending rates of the packets.now, all this depends on the queuing characteristics. the queuing characteristics depend on the statisticalproperties of the mark point process.the queue links are much longer for dependent, longrange dependent traffic than they are, for example, forpoisson and independent.for example, one qos problem is bandwidth allocation. if i have traffic at v bits per secondšsorry, if i have alink speed of l bits per second, how much traffic in bits per second can i put on that link for a fixed loss and a fixeddelay as quality criteria? so, this is a problem that needs to be attacked, and depends heavily on the statisticalproperties of the traffic.the history of internet traffic study has largely been one of the study of packet and byte counts. as i said, they arelongrange dependent. this has been historically the driver for intuition theory and the driver for empirical study. withenough aggregation,fsd models for openloop generation of internet packet traffic237statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.of course, the packet byte counts are a gaussian series.so, they make some things easier. it is also data reduction methods, so you go from gigantic files to smaller files,and for some people, that is an advantage.there has been widespread wavelet modeling of packet byte counts, and fractional brownian motion is a popularmodel.arrivals and sizes, the mark point process, the real thing that the routers see, very little empirical study as fineprocesses.enough that the longrange dependence has been established, as you can see almost immediately when you lookat the data, a few wavelet modelings, but no comprehensive generation model, and that is what we set out to fix.multiplexing, what happens when the rates go up? the conventional wisdom arose, even though there was almostno study of what actually happens, empirical study and conventional wisdom arose that said that the longrangedependence was unabated or even magnified.for example, in ieee communications magazine in the year 2000, there was a statement by somebody talkingabout architecting the future internet, traffic on internet networks exhibits the same characteristics, regardless of thenumber of simultaneous sessions on any given physical network. that was the conventional wisdom that dominatedengineering design, both network design and device design.so, let's start doing some modeling. here is a weibull quantile plot of the end arrival times of one particular lowload trace.fsd models for openloop generation of internet packet traffic238statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.so, here is the sixth root taken because they are highly skewed and that sort of messes up the plot, so here issomething to just make the plot work and make the data more nearly symmetric.so, sixth root of the packet end arrival times. we could have taken a log section, but six through of the arrivalweibull quantiles with a shape parameter of .48. so, we estimated the shape parameter, found the weibull quantiles ofthat shape parameter, and made a plot. so, this looks pretty good, actually, as these things go.this is a model. you have to be forgiving, of course, because when you have an arbitrarily large amount of data,of course, nothing ever fits. so, weibull in fact, turns out to be an excellent approximation of the distribution of theend arrival times.fsd models for openloop generation of internet packet traffic239statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.here is a weibull quantile of sizes for one very highload trace. now we start to see a little bit of deviation downhere at the low end of the distribution, not enough, though, to jeopardize modeling these things by weibull.by the way, there is a minimum end arrival time. you can't have an end arrival time any less than thetransmission time of the smallest packet appearing on the link. so, that is a truncation.so, you might say, strictly speaking, it is a truncated weibull, truncated at the bottom, but the amount oftruncation is extremely small. this vertical line here shows that it is 1 percent of the data. so, you have actually got avery small amount of data, in this particular case, being terminated.so, weibull turns out to be an excellent approximation. by the way, the shape parameter in this case is one, whichis an exponential.so, weibull distribution as a model for the end arrival times. so, t, tu, ' is the shape and a will be the scale.how about the sizes? the sizes, here is the same thing, a quantile plot of the sizes themselves. you see there issomething like 35 percent of the packets are the 40byte packets, control packets, and something, about the samefraction, of the packets are 1,500 bytes.then, sort of down through here, things are reasonably uniform, and then there is a bit of a turnaround here.this is this one lowload trace. here is the highload trace. the same sort of thing, except that we see now there isan accumulation of packets of 576 bytes.so, this is a case where somebody is even more behind the times and has configured some device, either a serveror a client, so that the maximum packet size allowable on the connection is 576 bytes and not 1,500. so, it can even beworse.fsd models for openloop generation of internet packet traffic240statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.in any case, the same sort of things, the accumulation at a small number of fixed points in the distribution andthen, elsewhere, sort of reasonably uniform up here, and then some curvature down here.so, the marginal distribution of the sizes of the model as a discrete continuous distributionši guess everything ispretty much discrete in continuous distribution, but one picks out particular sizesšsay 40 bytes, 576 bytes, what wejust saw here 1,500 bytes.actually, the way we model is to take things to be uniform on a set of variables from 40 bytes to 1,500 bytes and,oftentimes, just one interval suffices.for the purposes at hand, if you take the packet size distribution to be uniform, it is a little crude. you find thatthings don't really change too much in simulation. if you want to do a little better job of containing things, then youcertainly can do that. so, something on the order of three or four intervals is usually just fine and you get anexceedingly close fit.now, to model these data, here is what we do. let's suppose that xu is a time series to be modeled, and i want tolet f be the marginal cumulative distribution, and bias some unknown parameters.i am going to let g(z) be the cumulative distribution function of the normal, with mean zero and variance one.so, what we did was, we said, all right, we got a good handle on the marginal distribution and now we are goingto transform the time series. we are going to transform it so that it is marginal with gaussian. well, that is slightlytricky because thatfsd models for openloop generation of internet packet traffic241statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.factor size distribution has got some atoms. it is true. so, you just do a little bit of randomization and you are off withthe gaussian.so, here is it. so, half of the data where f is a fit of distribution and then g!1 of that, and that gives you gaussian.now, that time series has got a gaussian margin, a normal margin, but we can't suppose that that is a gaussiantime series.of course, this has to be checked, it has to be true, but the idea is that we transform it to try to get our best shot atturning it into gaussian.of course, to generate the xu, once we have a model for zu to generate xu, we transform zu and then transform itback to the original scale.everything works if i am taken to a gaussian time series. then, of course, everything is perfectly legitimate.now, why do we need this transformation? the reason is that the end arrival times and the sizes are grossly nongaussian and grossly nonlinear. so, if you attack them directly with the wavelet modeling, you are going to seeimmense complex structure.the transformation actually removes most of that compound structure, as it turns out.so, t* will be tu transformed to normality, and q* will be qu transformed to normality.now, here is the power spectrum. we take those transformed variables. here is the power spectrum for a lowloadtrace of the sizes.what you see is, there is a rapid rise at the origin of the power spectrum. this is the longrange dependenceexhibiting itself. that is the way it comes out of the power spectrum, is a rapid rise at the origin.fsd models for openloop generation of internet packet traffic242statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.this is the lowload trace. here is the highload trace. you see there is the same rapid rise at the origin, but thingshave flattened out immensely. keep in mind that is the spectrum of white noise.what we found was that an exceedingly simple model described this behavior. once transformed to gaussian, wefound a very simple model described the relation structure for longrange dependence of the sizes.it is a linear combination of two time series, one of them white noise, *, and one of them a very simple, longrange dependent series, mixed according to this parameter $.so, $ goes between zero and one. if $ is one, then we get nothing but white noise. if $ is zero, then we get nothingbut this longrange dependent series. otherwise, we are mixing the two together.fsd models for openloop generation of internet packet traffic243statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.that, of course, is what is happening here. as we mix in more of the white noise, the spectrum at higherfrequencies heads toward flat.no matter how much we add, if there is still a little longrange dependence left, eventually we wind up going toinfinity at the origin.so, we never quite get rid of longrange dependence, but its influence is reduced more and more through time.for the end arrivals, it is a similar story, except that the end arrivals head off to, it turns out, the first order ofmoving average process.so, you need a little bit of extra stuff in there to account for the observedfsd models for openloop generation of internet packet traffic244statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.behavior. so, instead of the nu being white noise, it is the first order moving average, with actually quite a small $.actually, the other reason why we need to throw this in is that estimating the parameter $ is a bit sensitive to the". so, it has to be in there for the estimation purposes.then, when it comes time to actually generate the traffic, we just ignore ".audience: so, you ignore "?mr. cleveland: yes. " tendsšwhen we estimate ", we have had no " bigger than .3. that is shorttermcorrelation and it just isn't salient for the queuing characteristics. you could put it in, but we have done that, and youlook at the results and you look at the simulation results and queuing out and so on, and it just looks identical. so, aftera while you say, forget it. everything is supposed to be as simple as possible, you know, occam's razor always works.so, we have these generation models. there are a number of parameters here, but actually we have been able toreduce this down so that, when you do packet generation for engineering studies, the only thing you have to specify isthe size distribution, you have to say what that is, and then you have to pick a traffic frame, c, the average number ofbackup connections.what we found was that the shape parameter of the weibull and those two $'s for the end arrivals and the sizes,change with the connection load.what we decided to do is to sort of fit the behavior. so, for example, here are the three parameters plotted againstthe log of the connection load.so, here is the $ for the end arrivals changing with the connection load heading up toward one, because those endarrivals are tending to independent, and the same thing with the sizes. so, we fit these curves and they become afunction of c in the model.by the way, this immense variability here is because we have taken the intervals to be small. if we took theintervals to be larger, then the variability would go down, but the problem is that now we are beginning to risk nonstationary, because the connection load changes.so, if the connection load changes appreciably, then we actually get different physical characteristics.so, the phenomenon is really the curve going through. by the way, this wasn't fitted. this is actually what youmight call a theoretical curve that we put together, to see how everything worked out, to see if the models wereconsistent. as you can see, it is doing a very good job of fitting the data.fsd models for openloop generation of internet packet traffic245statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.in any case, one thing about the change in parameters with load sets is that the end arrivals are tending towardindependent. that is what this is telling us, as the connection load goes up.the shape is tending toward exponential. sorry, the end arrival marginal is tending toward exponential. the shapeparameter is tending toward one. so, that means that the end arrivals are tending toward poisson, and the end arrivalsare tending toward independent. so, this reverses that conventional wisdom that held for so long on the internet. it isjust not true.by the way, this is supported by the superimposition theory of mark point processes. so, some of you might saythis should be no surprise. it is not quite that. obviously, it wouldn't been an issue. it doesn't necessarily have to be thecase that the assumptions regarding the poisson are true.we did an immense amount of validation of these models. we did a huge amount of what you might calltraditional model checking.you take these fits. of course, these fits are blindingly simple. so, there isn't any issue about gobbling up hugenumbers of the degrees of freedom. we really only have a few parameters in effect. actually, we have got moreparameters for the size marginal distribution than we do for the whole rest of the model.still, we did a lot of plotting of bits and data and residuals and things like that. we did other things. we didconsistency checks, you might say. in fact, that is what i was referring to when i showed you that curve there.what we did was, we said, well, look, if these models work, we ought to be able to take them and generate awhole bunch of traffic streams at very low rates, and then multiplex them from the generated model so that you get ahigh rate, and then fit parameters and look at it, and that ought to correspond to what the model says it should be forthat high rate. we did that and it worked out fine. you saw the results on that plot.in running our queuing studies for quite some time now we have been doing it side by side, by sticking live tracesin. we stick the models in with match to those traces, and any queuing results are consistently coming out to be aboutthe same.what we are hoping is that eventually people will allow us to use that and not get more data. we have, sort ofsitting off somewhere on the west coast, data on that same link, that same highspeed link, but now with 300 times asmuch traffic.so, it is 300 gigabytes of data, and we would just as soon people would let us get away with extrapolating to agigabyte of traffic rather than having to analyze the data to prove, again, that everything fits. but we will see whathappens.in any case, what about the domain of validity from all this extensive validation? the models fit, provided thenumber of active connections is about the same as a number of, say, eight.fsd models for openloop generation of internet packet traffic246statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.once you start to get too low a load, the communication protocols that are doing the transfers are starting to makethemselvesštheir footprint is very visible in the statistical characteristics. you are just chasing endlessly after detailthat you are never really going to account for in any reasonable way. you really need to see it at low rates like that, ataccess links, for example. then, somehow you just have to come to grips with modeling directly the communicationprotocols. there is no point in trying to do the statistics.all of this actually supposes that there is a small ši mean, the modeling was done under a supposition of a smallamount of queuing. so, it really tells you what are the statistical characteristics of the power packets arriving at thequeue as opposed to coming out of the queue. however, if you said, you know what? i actually need packets wherethere is delayšyou know, they have been delayed in a queue. then, it is that easy. you can just take the output of themodel and put it through just a simple personal computer.so, for generation purposes, let's say, there is no problem creating packets that have felt the effects of more thanjust a small percentage of packages being related in a queue. that is pretty much it. i think i am just at the end here.ms. martinez: i think we have time for at least one or two questions, and the next speaker can come up andswitch.audience: it seems like if everybody sent files that were of some small size, you would seek poisson right offthe bat.the sort of twopart questions are, is the reason it is not poisson at a lower rate just because long files arereduced, and if that is the case, as people start downloading bigger and bigger files, will the poisson go away?mr. cleveland: it is not just the file size. actually, that was thought for a long time. it was thought that thelongrange dependence on the internet was exclusively as a result of, as you are saying, the heavytailed file sizedistribution.there is another thing which is generating, which may actually be more important. there is another raging debategoing on right now about this issue. so, there is another conventional wisdom under attack right now. so, it is not asole creator of longrange dependence.another reason for it is the transfer protocol. it tends to ramp up its sending rate and then drop off and then rampup its sending rate and drop off. that creates a kind of persistence of longrange dependence as well.that may well turn out, in the end, to be the more salient factor. so, we will see. there is going to be a specialissue of papers in statistical science on internet traffic, andfsd models for openloop generation of internet packet traffic247statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.we will have a paper on this topic. so, let's see what the authors come up with.audience: bill, just thinking about your results, i was sort of struck that you are characterizing traffic duringnormal loads.when you engineer a network, you engineer it so that it performs well at heavy loads, and you tend not to carewhat happens in normal loads, because you are engineering from an extreme point of view.i am sort of curious, it sort of seemed like, to get traction with these results, you really have to move into the casewhere you are in overload.mr. cleveland: steve just said something extremely important. he says, well, look, you are trying to designa network, and it is when you get up to highload that it matters, and you modeled it when it was at lowload. so, whatare we going to do about highload.the answer is, well, our modeling says everything works in the model in describing the arrival process. the keything here is that it models the way the router sees it as it is about the enter the queue, is really what we have modeled,although strictly speaking, the data aren't that.actually, this is another thing. we took data where the queuing wasn't actually large. so, what we are saying is,we did it in the queue. we really did modeling as it comes into the queue.now, you want to push your queuing as far as you can. everything will be okay in terms of putting that into asimulation study, what we have modeled, so long as the drop is small, and that is where matching will be the case forhigherspeed load.so, you run an openloop queuing, you stuff our stuff in, in the queuing simulation, and everything is fine so longas the packet loss is a small fraction.today, packet loss is not likely allowed to be high on service provider networks at all. it is likely to be in numberslike .1 and .5 percent. the question is, how high can we get before that happens, and that is what the simulation tellsyou, and that is what nobody knew, by the way.service providers just didn't know how toši am sorry, i am taking too long for this question. anyway, let's talkabout it offline.audience: as sort of a followup question, ultimately the engineering issue is how do we relate the size of thequeue to the speed of the link. when you have higherspeed links you need longer queues.mr. cleveland: the size of the queue is also dictated by quality of service criteria. you can't make the queuearbitrarily large. otherwise, the line packs a long time and then your customer comes and says, you owe me a lot ofmoney because you broke the rules.so, the queuingši mean, there are two things. there is packet loss, and then there is how long you delay. forexample, if you want to do voice over ip, you can't delay the packets too long and create a lot of jitter, and the queuingdoes that.audience: the question really is, is what you are doing informing you of that?mr. cleveland: yes, absolutely. we have solved the bandwidth allocation problem under the assumptionthat the packet loss is going to be low. i will be talking about that at los alamos on february 24. so, i would like toinvite all of you to come and hear my talk.ms. martinez: okay, let's thank our speaker again.fsd models for openloop generation of internet packet traffic248statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.audience: [question off microphone.]mr. cleveland: that is a good question. hopefully, 2003, probably summer and fall. the authors, none ofthem are here, but anyway, it is up to the authors at this point.fsd models for openloop generation of internet packet traffic249statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.johannes gehrkeprocessing aggregate queries over continuous data streamsabstract of presentationtranscript of presentationbiosketch: johannes gehrke is an assistant professor in the department of computer science at cornelluniversity. he obtained his phd in computer science from the university of wisconsin at madison in 1999; hisgraduate studies were supported by a fulbright fellowship and an ibm fellowship.dr. gehrke's research interests are in the areas of data mining, data stream processing, and distributed datamanagement for sensor networks and peertopeer networks. he has received a national science foundation careeraward, an arthur p.sloan fellowship, an ibm faculty award, and the cornell college of engineering james andmary tien excellence in teaching award. he is the author of numerous publications on data mining and databasesystems, and he coauthored the undergraduate textbook database management systems (mcgrawhill, 2002,currently in its third edition), used at universities all over the world.dr. gehrke has served as program cochair of the 2001 acm sigmod workshop on research issues in datamining and knowledge discovery, tutorial chair for the 2001 ieee international conference on data mining, areachair for the twentieth international conference on machine learning, cochair of the 2003 acm sigkdd cup, andhe is serving as program cochair of the 2004 acm sigkdd conference.dr. gehrke has given courses and tutorials on data mining and data stream processing at international conferencesand on wall street, and he has extensive industry experience as a technical advisor.processing aggregate queries over continuous data streams250statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.abstract of presentationprocessing aggregate queries over continuous data streams johannes gehrke, cornell universityin this talk, i will describe techniques for giving approximate answers for aggregate queries over data streamsusing probabilistic ﬁsketchesﬂ of the data streams that give approximate query answers with provable error guarantees.i will introduce sketches and then talk about two recent technical advances, sketch partitioning and sketch sharing. insketch partitioning, existing statistical information about the stream is used to significantly decrease error bounds.sketch sharing allows one to improve the overall space utilization among multiple queries. i will conclude with someopen research problems and challenges in data stream processing.part of this talk describes joint work with al demers, alin dobra, and mirek riedewald at cornell and minosgarofalakis and rajeev rastogi at lucent bell labs.processing aggregate queries over continuous data streams251statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.transcript of presentationms. martinez: our next speaker is johannes gehrke, and he is from cornell university, and he is going totalk somewhat on the same topic.mr. gehrke: hello. i am johannes gehrke from cornell university. my last name is german. i get these callsfrom telemarketers who say, hello, may i please speak to mr. jerk.so, my talk is going to be a little bit different from the previous talk. i am going to talk about some technicaldetails, but i thought i would give you a broad picture of some of the techniques that i am going to talk about here,where they apply, and how they fit in the larger picture.so, what has been our work motivator, especially since 9/11, is this notion of information spheres. this notion ofinformation spheres actually comes really from the intelligence community. there is a local information sphere withineach intelligence agency that presents challenging problems, and there is a global information sphere that would like toenable intelligence agencies to work together in a collaborative way. so, these local information spheres, really, arewithin each local agencies or even businesses, and there you have some of the problems that are addressed here in thissession.you have to process highspeed data streams. you have to have variation of thousands of triggers you might seton events as they might happen, and you also have to worry about storage and archiving of the data. in addition, youalso have this global information sphere where you would like to share data or collaborate between differentintelligence agencies. at the same time, you have legal restrictions that do not allow you to share the actual data. so,privacy preserving computations in the setting are a very important part.so, let me give you sort of a little bit of a background on this work. so, in a local information sphere, really whatwe are building is this distributed data stream event processing data mining system. the technical challenges are, if wewould like to process physically distributed highspeed data streams, it has to be scalable because we do not reallyanticipate, we don't really know what kind of questions others are going to ask. there will be a highspeed archivingcomponent there. we have a graphbased data model, and we also look into some other issues like data mining modelmanagement, and built a support for especially data provenance.since i am going to talk about the problems with highspeed data streams from a data management point of view,let me first step a step back and ask, well, why can't we use existing approaches? why can't we use existingapproaches? if you think about databases, people have created this multibilliondollar industry with oracle and ibmand microsoft in this market. why can't we use traditional database systems to process highspeed data streams? thatis the question to us.so, it is looking at sort of using natural approaches that you would take. so, the first thing would be, so databasesystems have this technical component which is called a trigger. a trigger is basically like a condition that you can seton a table. if this condition is true, then a certain event is going to happen. this might be a notification; this might beanother insertion for another table, what have you. the only problem is that these triggers, they reallyšthere is atrigger developed usually for every insert into the table. so, if you do a linear number of triggers, you can see that yourperformance degradesprocessing aggregate queries over continuous data streams252statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.linearly with the number of triggers. there is some documentation out there, like in the air force jdi, but they reallystill require index maintenance on each publication.so, what would be another possibility, if we look at the bottom thing? you could use bulk loading for archival inthis local information sphere. there is another component which is very important here, if you want to archive data,but if you think about current database systems, they are really not made for highspeed appendonly relations. forexample, oracle has something, what is called a transportable table space. the transportable table space is where youtake an existing table, you dump it out in its existing physical format. you can read it, actually, very fast. if you thinkabout it, well, how did i get it first into this physical existing format? to do that, actually, i first have to load the tableand then dump it out and then reload it again. actually, this is really a reload of the existing table. you know,modeling constructionšactually, pedro domingos is going to do a very nice talk about online construction of datamining on this.then, the last point that i think is actually very important as we talk here about streams and monitoringapplications is this notion that i think, in order for this technology to be acceptable by the public, we actually have tomake advances from a technology point of view. that means that we actually have to have techniques that allow, forexample, to collect data and, at the same time, be able to build models, but not being able necessarily to refer this databack to the individual. so, we really need to have techniques for private data sharing in this global information sphere.so, these are sort of the two main goals of our research processes, and i am actually going to concentrate now on sometechniques in the first partšthis is distributing streamlining and monitoring in a processing system.so, really to the ultimate of my talk is i am going to give you a little bit of background of modelingconsiderations. i am going to actually switch again to sort of a highlevel mode and talk about sort of one excitingapplication of these kinds of techniques in sort of a different setting, which is a network setting. then i am going totalk a little bit about the actual techniques we are using, which are actually random projections. they are also calledsketches, which allow us to process network data streams at highspeed rates, and with very small space.so, the highlevel model that we are consideringšthis is a model that the previous speaker talked already muchabout. we have a bunch of routers. so, there is some kind of network operation center, and we can't afford to actuallystream all the data through the central operations center. this might be a measurement allowance. these might beother questions that you might ask about the status of the triggers, that may be based on certain events, like a serviceattack. the kind of queries that i am going to concentrate on here, and my techniques, are these kind of data streamjoin queries. these data stream join queries, conceptually, the way you can think about it is that there are sets of eventshappening at these different routers distributed through the network.what you would like to know is how many of these events actually match. so, i am a database person, so anotherway of thinking about this is that you have conceptually three different relationships that are distributed at threedifferent routers. what i would like to find out is the site of the join between these numbers, that is, the number ofelements that actually match these three different data streams.again, my objective is to do this without actually sending all the data to a central site. so, that really is the goal. iwould like to do this with a very small amount of space.processing aggregate queries over continuous data streams253statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.the main technique that i am going to use is that i am going to say that, well, if you are looking for trends, or if youwant to analyze behavior of this network, you are not caring about the count of single packets. what you care about ishighlevel trends. therefore, approximate answers are sufficient for a large class of applications. i am not saying for allapplications. for a large class of applications, you want to have fast classification of actual highlevel events and, inthis case, approximate answers are good enough. actually, a lot of applications are out there. there is one applicationwhich is out there, which is sensor networks, and actually, later in the talk, i am going to switch and talk just a little bitabout that work, because i think it fits very nicely here into the scope.so, in computations over streaming data, the model really is that i have this synopsisšessentially, you have thisstream processing engine, yet these different streams are streaming through the stream crossing engine, and the streamcrossing network is physically distributed over different parts of the network. you have this synopsis of the relation ofthe different streams that you are building. really, the stream processing engine really keeps some smallspacesummaries of these relations, and there is a certain workloadš [off microphone] šso that is the model. so, thecomputational model is a single pass, each parameter is examined once, and a fixed order that it arrives. at the sametime, we would like to use a very small space. small space can actually be in sort of two dimensions. it can be small inthe length of the stream as well as small in the size of the domain of the attribute which you would like to match. i amgoing to give you an example, actually, what i mean by this. so, this is the setting that we are talking about, and let megive you sort of an idea of the class of queries to consider.the class of queries to consider is some aggregates over join. so, this is database technology, so let me tell youwhat really a join is. so, the join between these two different relations are one through r, is the following. in thesimplest case, you have one attribute between sort of a pair of relations. so, there is this linear chain of relations. ineither pair, you would like to find only those records that match actually on this pair of attributes. for example, one ortwo may join here on this little attribute. then, r2 and r3, again, might be joining on a different attribute. what youwould like to find out is š you would like to find out some sort of aggregates over this join. for example, in thesimplest case, how many doubles are actually in this join. we would like to do this, again, in a distributed fashion in avery small amount of space in an online scheme. so, another way of specifying these queries is the count of thisrelation here is just the sum of ones where the doubles on the records of the output of this join. another way ofthinking about this is sort of the job product of the frequency vectors, and actually, i will give you an example of thisas well.so, this is the class of queries. let me give you some examples of these queries and actually how this works. so,for example, a little twoway join. we would like to find the size of this join. so, how does this work? you have thesetwo streams, f and g, that are streaming in. essentially, what i would like to do is, i would like to build thesefrequency vectors here on the top rightšnamely, f and gšwhere, for each value in the domain i have the frequencythat it actually occurred. the size of the join is the dot product of these two frequency factors. that is what i wouldlike to ask it. again, i would like to do this without shipping the actual frequency vectors to a central site, because thedomain of these attributes might be extremely large. that is what i would like to work on. in this case, if you get thedot product of f and g, you get the single caseprocessing aggregate queries over continuous data streams254statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.for these two streams. this would be a twoway join somewhere. so, now, again, i match up the two events, the twoevents, the two relations at these two different sites, but i am summing over a third attribute, or a third variable.so, now, here, for streaming for f and g, i have two different variables a and b. i match them up on a and then,for those, wherever i have a match, i sum over b. again, this is now the sum of the join, which i modify the vector ofg', which is now 30, 10 and 40, and the overall answer is 180. the frequency matrix in g now has the frequency of theactual attribute value. then i also have, for each of the values in b. so, it is now a twodimensional matrix. for everyvalue in g, how often does it appear for every attribute value in a.again, i want to do the match on a and i want to sum over b. that is the set. again, another way of thinkingabout this, i match these two relations on a. every match has a certain value of b. i want to sum over all of thesevalues. that is the answer to the query that i would like to compute.you can also extend this to more than two streams. here are three streams, stream f, g and h. now, stream g hastwo different attributes, again, a and b. a is the connection to f, and b is the connection to h.what i would like to point out is, what is the size, again, of the join between all these three relations. actually,again, in the middle you have this matrix, in this case it is just 24. so, this is the setting i would like to compute. so,think about it in a distributed setting. i would like to find out the number of matches of attribute values.so, there is a lot of previous work that i think actually some of the experts here in this area who have done a lot ofwork on this here in the audience. so, there has been a lot of work on conventional data summaries. one of theproblems that they have is that they do not really work well for foreign key joins, or they require a lot of space, or theycan't cross acrossš [off microphone.] i can't say that this technique that i am going to talk about is the main techniquethat should be used in all these applications, but is one technique that works well for this class of query.let me give you an introduction to sketches, and then let me actually tell you about some work that we did, sketchpartitioning, sketch sharing, if time permits. again, let's go back to the examples. again, the setting is that there aredistributed data streams. i would like to find out the number of matches. that is the simple query. again, these are myexamples. i have these streams coming in. what i would like to find out is the count of the join. the join is the numberšexcuse me, those doubles were actually matching pairs.so, the main idea that we are going to use is called a sketch, which was developed by mateas and lewis in apaper in 1996. this is a work where they centered this in a single join in 1999. so, the idea is the following. as youhave seen in the previous slide, you can have these frequency vectors, and i can actually compute the size of the joinexactly. so, one way of doing the join would actually be, well, i computed the frequency vectors at every point in thenetwork, shipped these frequency vectors to a single site, and then compute the size of the join.now, the problem is, if i have an attribute, something like, say, my key address, this attribute has a huge domain.so, these frequency vectors can actually be extremely, extremely large. so, the problem is, depending on the size of thedomain, there might be a polynomial, or the size of the stream at the polynomial, let's say, in the size of theprocessing aggregate queries over continuous data streams255statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.domain, actually. so, in this case, these frequency tables are really too large to ship to a central site. so, the main ideais that i would like to summarize these frequency vectors and the way i can summarize them is as follows. i take thefrequency vector and project it onto a random vector. this is what is called a sketch. let me tell you how this is done.so, you build this random vector of ±1 value. then what you do isšthe size of the domain is n. what you do is, youtake the frequency vector and you multiply with this random vector.now, this product that comes out is a single number. i do this on both f and g. now, if these random vectorshave certain properties, let's look at what comes out and i am going to multiply these two sketches. now, when imultiply these two sketches, the expectation is now f times these two in the middle. now, i have these two items inthe middle, and take the frequency vector and take them to each site. because these are plus minus vectors,classification independent, such as the diagonal actually has one, and the expectation of the diagonals is zero. then ican actually get the size of the join out. what this means is that, i have now a construction that, in expectation, givesme exactly the size of the join with a single number on both 's.again, with this random projection, what this allows me to do is summarize this frequency vector into a singlenumber. i have the expectation that i am going to take the product of these two numbers and actually get exactly thestatistic that i would like to estimate. again, this is not a technique that we developed. this is a wellknown techniquefrom the algorithms in theoretical literature, actually.so, let's see how this actually does. so, we have this vector at site one, site two, site three. let's assume that ourdomain set is three. let's assume it is {!1, +1, !1}.then what i do is, for f, for the sketch of the stream f, i just take the frequency vector of 3.2 and multiply withthis vector +1 or !1. i get !4. i do the same thing on g. i get !5. i take the product of them. i get 20, which is sort ofabout 13, which you can see, is a little bit off, but in explication it is actually correct. so, you can see, the variancemight be a problem. actually, i will show you actually a couple of hopefully interesting techniques how to actuallyreduce the variance.again, the only property you need from this site actually is estimates of the variance. four is independence, andthere are techniques out there to generate them with small seeds where the  vector actually is not stored. well, what ihave to do at each of the sites, i have to store another  vector of ±1's. that, again, is big as the size of my frequencyvector. so, what have i gained.the main idea, again, is that i can generate these entries on the  vector through a very small seed, basicallythrust through a small function, which i give the actual attribute value in the domain, and it spits out ±1.so, how is this actually working? there is a seed s that generates the  family, and if i have a stream 1, 2, 1, 3,the main thing i do, i sort of take my function h, take my seed and i give it seed and one, what outcomes, ±1, take thesame for the next function, take the seed in one, put in the function, what comes out is ±1. the following or thecorresponding entry in my vector.as you can see, i can do this online in a streaming factor. as i add my individual elements here, what comes outis actually online in a one pass algorithm exactly the sketch for f and xf. the counters now, you can actually see, theyonly need log space actually in the size of the domain, because really they are notšand the size of the256about this pdf file: this new digital representation of the original work has been recomposed from xml files created from the original paper book, not from the original typesetting files. page breaks are trueto the original; line lengths, word breaks, heading styles, and other typesettingspecific formatting, however, cannot be retained, and some typographic errors may have been accidentally inserted. pleaseuse the print version of this publication as the authoritative version for attribution.processing aggregate queries over continuous data streamsstatistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.stream. so, as you can readily see, the estimation of this count from single step is too noisy. so, there are sort ofstandard techniques where you take medians of averages to actually reduce the error and to increase the confidence.so, basically what you do is, you have these seeds here in the middle. you take f and g. for the same seeds, you dosketches of f and you do sketches of g. then you take these independent copies of x, you average them, you take themedian of the average to get the confidence up.so, again, this is not my contribution. this is sort of a nice set of techniques that i am now going to use in astreaming fashion to get highquality estimates, or the estimates. again, in a picture, how does this actually work? theestimation of this count is, you take this stream, you build a sketch, as the total stream in a single pass online.you take these two sketches, you multiply them. you do this for a bunch of sketches, you take median averages,and what comes out is an estimator that is unbiased and has a low variance. so, that is sort of a warm up of how to usesketches.mr. nychka: where does the probability statement come from? is that exact? you have ±+ with probably of 1!,.mr. gehrke: that comes from basically how many sketches i average. this comes basically from the numberof sketches that i use here. what i do is, i take the average of a number of sketches and this gives me basically a smallamount of error because the variance of the average is much smaller than the average of the individual sketches. then,to get the confidence, i actually take the median of a bunch of these averages. that only comes from this picture here.so, let's do a simple extension. a simple extension, let's look at multiple joins first. first of all, you can alreadysee that conceptually easily i can do sum. what i do is, instead of just summing +1's or !1's, i actually take the actualvalue of these and i compute it into one of these sketches that i have. then, if you actually look at the math š again, iam not going to do thisšwhat comes out is actually an estimate of the sum exactly.so, again, for sum, the only difference is that i build a sketch. when i build the sketch, instead of just addingxii's, i also add all the attribute value that i need.let's look at another example. here, i have three different streams. so, how does this work? again, i take nowtwo different  families, one between f and g, and another one between g and h. i can use the same construction and,again, sort of f and g and h factor out nicely, and cxi and transports in the middle, because of independence, again,a factor out and what i have here in explication is exactly the size of the join.this is schematically what is happening. from the lefthand , i build f, around the middle i built g, whichnow i have to take the product of the corresponding 's for the one attribute and the other attribute and add it to thiscorresponding sketch. now you can sort of see this actually extends through multiple joining. you can actually see thisvariance has this sort of unfortunate property that actually grows up exponentially with the number of joins. it is sort ofa really bad property, if you look at it.again, in expectation, i still get exactly what i am trying to estimate, but my variance is really getting extremelylarge and hopefully, i have a little bit of time, and i will tell you now about some techniques how to reduce thevariance, and then hopefully some techniques how to do multiple characterization for a set of sketch queries.so, again, to bring this point home, the sketch making algorithms is really as simple as this, on a stream. so, i takeeach double, for each double, i take my function h 257about this pdf file: this new digital representation of the original work has been recomposed from xml files created from the original paper book, not from the original typesetting files. page breaks are trueto the original; line lengths, word breaks, heading styles, and other typesettingspecific formatting, however, cannot be retained, and some typographic errors may have been accidentally inserted. pleaseuse the print version of this publication as the authoritative version for attribution.processing aggregate queries over continuous data streamsstatistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.and my seed, look at the attribute value in the domain, find out whether i am +1 or !1, and add that to my sketch. it isa relatively simple operation that i can do online, on a stream, at very high speed. there are actually very nice andparallelizable, very nice computing in a distributed setting.for example, at the stream level, i can compute sketches for each stream independently in a distributed way ateach point in the network. at a sketch level, i can even maintain each sketch independently. even at the data level, ican partition the sketch into fragments, and then the sketch of the whole stream is the sum of the parts. actually, i amgoing to use this next, when i am going to show you a technique that actually shows you how to reduce this variance.you can see, so far it looks nice. in expectation, i get exactly what i want, but my variance really has this badproperty that really seems to blow up completely, especially with the number of joins, because there is this badexponential factor there in front of the variance. if you look at the technique, how i get my confidence and errorbounds, this sort of decreases my variance by some linear factor. i actually have to pay for this linear factor with thenumber of copies that i am going to get. so, this is not really exactly what i am going tošthis is not going to help meagainst this exponential growth that i have in front of my variance.i think i am going to skip over some experiments that basically show that sketches are really good, and let me justgive you sort of an intuition of one of the techniques that you can use to actually reduce the variance.so, the problem really is, if you have a large variance, you really get bad estimation guarantees. it is really so badthat your variance, for example, can be just much, much larger than the quantity that you are actually trying toestimate. so, basically, you don't really get anything. so, what would be one solution? one solution is to use thisprevious technique of keeping lots and lots of copies, trying to drive the average down through some conventionaltechniques.if you think about what we are trying to do, we are really trying to estimate the size of the join. the size of thejoin is sort of payindependent. if, for one type of event i have many matches and for another type of event, there arefew matches, this is really getting to my estimate, because my estimate is for the number of matches of events. whatthis actually tells me is that maybe by actually looking at what is happening inside the join, this should give me someinsight on how potentially to drive down the variance.so, here is the idea. let me give you an example. for example, consider this gray down here, this count of f andg, and these are fi and gi on the frequency vectors. these are the frequency of the different attribute values forattribute values one through four.what i do is build a single sketch, but the variance is this huge number, you can see, 35,000. so, it is justridiculous, and this technique doesn't really help me here at all. it is 357,000, yes. so, what would be the idea? theidea is that maybe what i can do is, i can use the concept that i can split the domain, the different parts.i compute the drawing on that part of the domain and, in the end, i just add these two sketches up. hopefully thevariance on these different partitions is much, much smaller than the actual overall variance of the way i constructed itover the whole part.what i can do is i can split the sketch up into f1 and f2 and g into g1 and g2. i can start x1 and x2. then, myfinal sketch is just the sum of these two sketches. now,processing aggregate queries over continuous data streams258statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.this now assumes that i have some previous knowledge about the distribution of the stream, actually, of the attributevalues for the two parts of the stream, and you might be able to use some historic knowledge, of you can actually do anestimation of this.conceptually, what is happening here, it is a little bit hash joined. you have put the relation into two packets. thesum of your join is the sum of the one packet plus the sum of the other packet. what is happening is, if i do thispartitioning here, i compute the variance on one part and the variance on the other part. what this actually allows is todrive the error down by a factor of 2.4.actually, we have seen a dramatic decrease about an order of magnitude on real data, because real data is actuallyusually quite skewed. what this allows us, it allows us to have many sketches for the heavy part of the distribution andfew sketches for the loose part of the distribution. so, the main idea is really that, by splitting the main of the joinattribute, we get a reduction in error. so, this is not a nice sort of organization problem that you can formulate. so, imean, you want to partition the domain into two parts, or 2k parts, such that it minimizes the sum of the variances, andyou get the sum of the variances, and the variance is sort of a product between cell join sizes. we can actually showthat you should actually allocate this space in proportion to the variance, and we only have to look at partition as sortof the order of the ratios of the two frequencies, and it actually follows from sort of a nice theory that actually comesfrom a decision tree construction of a data mining problem called raymond's theorem. so, it gives you this fastsolution to this problem. actually, you can do some more extensions. you can do kr splits and multiple joins, etc.i think i am running out of time. let me give you just some idea how this actually performs. for example, here,we are using some census data as sort of an example of real data. you can see, as the number of partitions increases,the relative error here goes down by about a factor of two. here, again, a join on two different attributes, actually, it issort of a correlation between the attributes. again, the variance goes down by about a factor of two. the aggregategoes down by a factor of two.so, i have maybe about two minutes left. what i want to do, in the end is, i want to show you another applicationof these kinds of techniques, again, in a streaming network environment. that example is sensor networks. youprobably have seen there is sort of an evolution happened. actually, one of the speakers yesterday talked about thesame thing, namely, that small scale embedded devices are going to be everywhere.in 1965, wilson did an estimate of 1016 or 1017 ants on earth. in 1997, we produced one transistor per ant. youcan really see what kind of computational power this actually puts in the space surrounding us within a few years. so,what we really need are techniques to deal with the slew of data that is streaming at us from these different sensors. iwould like to give you one idea of one potential solution or one potential approach for how you would handle this data.traditionally, apparently how these sensor networks manage a program, the idea is that you have some kind ofprogram that addresses these sensor. say, sensor 17 sends something to sensor 25. then, it sends something to sensor83 and then sends it back.we believe that actually the right way of programming these sensors would be through declarative queries. whatyou should have is these queries or these large scale sensor networks that give you the extraction of a single virtualdatabase. let me just take one minute. so, the more distributed database system, and you can now ask, what is theprocessing aggregate queries over continuous data streams259statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.distribution of chemical x in this quadrant, or in which area is the concentration of chemical x higher than the averageconcentration in this area. the nice thing is that really you have this declarative highlevel tasking that shields awayfrom network cracking and the system optimizes resources.again, this is a streaming problem because these sensors continuously generate data. some of the techniques thati actually talked about and similar techniques can be used in this example. so, i can just give you sort of an example.so, what user would have, a user would have some kind of minigui and array a set of network resources.i think i sort of went over a lot of stuff. let me justšthere are sort of lots of problems in the sketching model. ithink actually there are several people here in the audience who have also done fundamental work on the sketches.instead of running more over, let me just ask if you have any questions and thank you.audience: it seems possible in some cases you could get additional acceleration by allowing your two datasources to do a little bit of negotiation between themselves before they start sending off lots of information. forinstance, they could initially show each other what their most frequent items are and, if they tend to agree on differentitemsš [comment off microphone.]mr. gehrke: this is actually a very nice observation. one possibility is, for example, if you look at where thevariances areš [off microphone]. another extension of this idea would be to look at actually streams that aresomewhat annotated.instead of just sending blocks of data over to you, maybe i can do some computation on my side, or the originatorof the stream, where there is some annotation.annotate the stream, for example, with saying this is partially sorted, or maybe you could sum it, that wouldactually allow the computer of the actual computerš [off microphone.]processing aggregate queries over continuous data streams260statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.edward wegmanvisualization of internet packet headersabstract of presentationtranscript of presentation and powerpoint slidesbiosketch: edward j.wegman is the bernard j.dunn professor of information technology and appliedstatistics, the chair of the department of applied and engineering statistics and the director of the center forcomputational statistics at george mason university. he received his ms and phd in statistics from the university ofiowa. he spent 10 years on the faculty of the statistics department at the university of north carolina.dr. wegman's early career focused on the development of aspects of the theory of mathematical statistics. in1978, he went to the office of naval research (onr), where he was the head of the mathematical sciences division.in this role, he was responsible for a variety of crossdisciplinary areas, including such projects as mathematicalmodels of biological intelligence, mathematical methods for remote sensing, and topological methods in chemistry. dr.wegman was the original program director of the basic research program in ultrahighspeed computing at thestrategic defense initiative's innovative science and technology office (ﬁstar warsﬂ program). as the sdi programofficer, he was responsible for programs in software development tools, highly parallel architectures, and opticalcomputing.dr. wegman came to george mason university with a background in both theoretical statistics and computingtechnology, with knowledge of the considerable data analytic problems associated with largescale scientific andtechnical databases. in 1986, he launched the center for computational statistics and developed the ms in statisticalscience degree program. more recently he has been involved with the development of the institute for computationalscience and informatics and the new phd program in computational sciences and informatics at george masonuniversity.he has been consultant to a variety of governmental and private sector organizations, organized some 15 majorworkshops and conferences, and served as associate editor of the journal of the american statistical association,statistics and probability letters and communications in statistics. he presently serves on the editorial boards of thejournal visualization of internet packet headers261statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.of statistical planning and inference, the naval research logistics quarterly, the journal ofnonparametric statistics,and computational statistics and data analysis.visualization of internet packet headers262statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.abstract of presentationvisualization of internet packet headersedward j.wegman, george mason university (with don r.faxon, jeffrey l.solka, and john rigsby) .abstract: we have launched a project with the agreement of the university's cio to capture all headerinformation for all internet traffic in and out of the university. this includes tcp, udp, snmp, and icmp packets.we have installed sniffer and analysis machines and are capable of recording up to a terabyte of traffic data.preliminary experiments within our small statistics subnet indicate traffic of 65,000 to 150,000 packets per hour.indications are that we will have terabytes of data traffic daily universitywide, 35œ40 megabytes of header traffic perminute, or approximately 50œ60 gigabytes of header information per day in the larger university context. much of thepacket traffic is administrative traffic from routers. ultimately, we are interested in realtime detection of intrusionattacks so that analysis methods for streaming data are necessary. in this talk i will describe our project, includingsome background on tcp/ip traffic, indicate some recursive methods capable of handling streaming data, illustrate adatabase tool we have developed, and give some suggestions for visualization procedures we are in the process ofimplementing. this report is very much a preliminary report. in data mining, 80% to 90% of the effort involves gettingthe data in shape to analyze, and this project does not deviate from this pattern.visualization of internet packet headers263statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.transcript of presentationms. martinez: our next speaker is professor ed wegman from george mason university, and he is going tocontinue to talk about internet traffic and how can we analyze it.mr. wegman: i must say, it is always a formidable challenge to come after bill cleveland, who was mycolleague once upon a time in chapel hill, when we were both quite a lot younger. bill was looking at internet trafficfrom sort of the global perspective. what i would like to do is have a discussion of our sort of internal structure. weare particularly concerned with issues of intrusion into our systems. don faxon and i work at george masonuniversity. jeff solka and john rigsby are colleagues of ours that work at the naval surface warfare center.one of the issues of interest is, how intrusions in a military setting are different from intrusions in an academicsetting, and is there sort of a qualitative or quantitative characteristic difference between the internet traffic in thesetwo settings? so, we are working relatively closely with these guys.what i would like to do is give a little background on tcp/ip. i realize probably a lot of people in the audiencealready know a lot of this. this is, you know, partly, for me, but i hope that some people may not know as much aboutthe nuts and bolts of it, which are relatively important in understanding how intrusions are done.i would like to talk a little bit about our project, do a little demo. then, i wouldvisualization of internet packet headers264statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.like to talk about streaming algorithms and maybe some graphics proposals that we have.now, bill talked about ip addresses, but didn't go into any detail. technically, these are called ip version 4addresses.ipv addresses are 32bit numbers, usually represented as four dotted fields. field one, two, three or four, orsometimes called octet one, two, three and four. in general, these ip addresses uniquely identify a machine, althoughthat is not entirely true, because ips can be dynamically assigned. so, they may not uniquely identify a machine.there is also a number called a mac number, which is a manufacturer's number, which does essentially uniquelyidentify the machine. so, in principle, it is better to find the mac number than the ip address, if you are trying toidentify and locate individual machines.each of these fields is 256. it is an eightbit field. so, if you multiply that you, you come out with approximately 4billion addressable machines.there are a number of different types of networksšclass a, class b, class c. most of us are probably associatedwith class b networks, where the first two fields identify the network, and the second two fields identify specifichosts. field three is often used as the identifier of the subnet.in a class a network, field one is always smaller than 127, 127 is the loopback number. so, there are relativelyfew, there can only be 126 or so class a networks, and there are a few. i noticed in bill's presentationšbillcleveland's presentationšnone of the networks were identified as class a networks.visualization of internet packet headers265statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.you can also have class c networks, in which field four identifies the host and the first three fields identify theparticular network. so, class c networks are relatively small networks, they only have 256 hosts in the network.just a word on ip addressing. the tcp/ip addressing is normally regarded as a layered system. so, at the highestlevel, you have sort of the application layer, which is the data itself. attached to that, application header. attached tothat is a protocol header. the protocol that we are typically familiar with is tcp/ip, although there are three or fourother kinds of protocols that are relatively common.the discussion that bill was talking about principally is the ip layer, which is the ip header, and then finally thehardware layer attached to this. so, in order to get the application data, you have all this other stuff, which is basicallymetadata about routing and so on, and what kind of stuff is in the ip address.so, here is the basic structure of the ip header. it is the version, ip version four is the common version. ip versionsix is in the wings. the length, the type of service, total length, some identification, flags, fragment offset, time to live.each packet has a maximum possibility of 255 lifetimes, and each time it transits a router, the time to live getsdecremented. so, if it goes through too many routers, it ceases to exist. there is information about the protocol andcritical information of source ip and destination ip and other options.visualization of internet packet headers266statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.one of the things that is of significant importance is the flag. of course, what really is important is the data, butthat almost sort of gets lost in the, i guess, noise. some flag types are acknowledgment flags acks, psh flags that saythe data should be pushed into the application as soon as possible, a reset, a syn, or synchronization of a connection,and a fin, which is the finish of the connection.so, a possible ip session might be that the host 1, or the computer that is seeking data will send out a synpacket. these are the 40byte packets that bill is talking about. host two will send out a syn/ack, will acknowledgethe fact that it has received this packet. so, one possibility for discovering intrusion is if you are receiving a lot ofsyns with no acks, no syn/acks, then somebody is probing the system, and they are not probing the right ports,so they are not getting any acknowledgments.so, one of the ways of discovering that you are being probed and things maybe aren't quite what they should be islooking for the ratio of syn that is in ack packets.once there is an acknowledgment, then the host one acknowledges the fact that it received the syn/ack packet,and then started sending out the data. so, essentially, those are the psh things. when that stuff is received, the hosttwo sends the acknowledgment. it pushes out some data. the host one will acknowledge it and say, i am done, and thehost two can send out a fin/ack, which is the final acknowledgment. it is an acknowledgment of his fin.host 2 may not be done, so he may send out some more data. finally, you get to an acknowledgment of that. thenfinally, the sign off handshake is that host 2 will say, i am done, send the fin packet, and then the other host will sendthe fin/ack, and thatvisualization of internet packet headers267statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.ends the session.clearly, this is a very abbreviated kind of session. if you are getting internet traffic from email or, for example,from web pages, there is maybe lots more of this going on, but this is sort of the typical prototype.just to give you a scale of things, an ipv version 6 address is a 128bit address, arranged as eight groups of 16bitnumbers, separated by colons. so, it is hexadecimal and so a typical ipv6 address may be configured something likethat. in general, leaving zeros may be omitted. so, instead of writing all these zeros in, you can just put the final zeroin, and so you can compress the address quite a lot by omitting leading zeros. if you have a sequence of zeros, then thatcan be replaced by a double colon. so, the address can be compressed even further.now, it turns out the last two fields are sufficient to fit in current ipv4 addresses. so, for example, an address thatis 1310345 can be compressed as this. the way that is, is that 130 in decimal is 82 in hex, so the 130 becomes 82, the103 in decimal becomes 67 in hex. so, this part is the first two fields of an ipv4 address, and then the second part,2805, corresponds to the party in five in the ipv 4 address.since everything else would be leading zeroes, we can simply put the double colon in. so, this is the ipv6 versionof the ipv4 address, and there are sort of hybrids allowed, so that you can put in the single point instead of a colon toget something.now, a question is, how many hosts are possible? one reason i am sort of going through this is that, if you areinterested in visualization, you are interested in how manyvisualization of internet packet headers268statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.things you can see. if you sort of multiply all that junk out, you get 3.4 times 1038, which roughly means that every 30atoms can be having its own ipv6 address. so, there are a lot of these addresses.audience: it is still a factor of 30 short.mr. wegman: i was thinking on the way in, because i have sort of stupid random thoughts while riding thesubway, that one of the things that, you know, in star trek they have these transporters. they got it wrong in star trekbecause they always had these pattern buffers. see, this would be a sufficient amount to address every molecule inyour body or, in fact, every atom in your body. so, if you had an ipv address for every molecule or atom in your bodyand you transmitted just the location and type of atom it was, you could clearly, as long as you had a streamingalgorithm, you could clearly implement the transport. in fact, i was thinking even further that, if it lost a few packets, iwouldn't mind, you know? [laughter.]so, in ipv4, there are basically 4 billion, and so, visualization of everything is hard to see, even with ipv4.now, in addition to the issues of sort of looking at all possible internet addresses, each machine has 216 or about65,000 ports. so, looking at ports is an interesting thing. 65,000 is something that is visualizable. so, looking at ports isan interesting thing from the point of view that there are some standard ports, but attacksšintrusion attacksš oftenattempt to go after ports that are not properly closed off or not properly unused. so, one thing that can happen is thatports can get scanned to see if something is misallowed with those things.so, looking at attacks that scan ports and seeing if you can do that visually is interesting. i put some popular portsdown. ftp is 21, simplemail is 25, http is typically 80, it is also 8080. top three servers, or mail servers are 110.nfs is 2049. one of the things that i thought was interesting is that even direct tv and aol have standard ports thatthey use.visualization of internet packet headers269statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.so, in order to analyze the traffic data, you have to basically capture the traffic data. this is donešat least in ourcasešwith something called sniffers. i think bill had a less provocative name for such things. anyway, these things sitoutside the firewall in our case. we have implemented a couple of these things and i will show you a little bit aboutthis. the idea is that the program will capture something like tcp delta's program that captures the header informationof all data flowing through a given point.ours is implemented in such a way that all traffic in and out of george mason university is routed throughšnotrouted through, but also sent to this machine. the sniffer is actually a covert machine. it is not visible on anybody'ssystem. you can't tell that it is there, basically.so, our network at george mason university is a class b network, and just to give you some calibration, the datain and out, the traffic in and out of the class b network, which is a sort of mediumsized network, is in the multiterabyte range daily. we can collect, when we do, on the order of 35 to 45 megabits of header information per minute,something like 50 to 60 gigabytes of header information per day. even within our relatively small statistics subnet wehave eight faculty members that sit on statistics subnet, plus a secretary or two.last week, just in getting ready for this, we were collecting a little data just to see what kind of things were goingon. last week was final exam week, so we didn't have as much activity as we usually do. usually only the faculty thatare giving final exams are around. even in this relatively small subnet, we were getting 65,000 to 155,000 packets perhour.visualization of internet packet headers270statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.so, some observations. with the scale of traffic, even though things are usually thought of, in computer terms, asbeing discrete, for many purposes, they are essentially continuous. if i tried to look at all ip addresses, somehow icouldn't do that in any reasonable discrete form. if i tried to do graph theory associated with that, it would be tough.clearly, the storage of all header data is not possible. we have a terabyte storage capability and that would run outin not too long a time. so, streaming algorithms and methods are essentially, and essentially, recursive algorithms areof great interest. i think, in general, for streaming data, that is something we would like to make a comment on.the good news is that not every computer system talks to every other computer system. so, the networks arerelatively sparse. even so, the visualization methods typically are stretched to the limit. of course, the nature of trafficchanges during the day. within george mason university, we have very little traffic between about 3:00 and 5:00 inthe morning, comparatively little, very heavy traffic up to about 10:00 o'clock when people are sorting out, i guess,their email. traffic tapers off a little bit. george mason has a lot of evening students, so traffic builds up againrelatively heavily in the evening. as students go to their dorms and surface naughty web sites, we get a lot of trafficafter 10:00 o'clock.by the way, i worked for a while at the bureau of labor statistics. the bureau of labor statistics has a filter thatcuts off any ip that seems to have nasty stuff on it. one of the discussions, when i was working at bls, was that thewashington post web site was cut off because it apparently had offensive material in it.my friend, don faxon, who is working with me on this project, went to the isle of crete during the summer. mywife was in the hospital and i couldn't go to give a talk, so i sent him to give a talk. he started calling this projectknossos. i don't know why, because that is a city in the isle of crete. i was not fond of this name.visualization of internet packet headers271statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.i thought, as long as you are going with greek mythology, certainly cerberus would probably be a good name forthis. cerberus is the threeheaded dog that guards the gates of hell. the comment is, after all, they do call it a firewall,and the question is, which side of hell is on our gatewayšis it on our side or is it on the outside?i thought maybe a better name is to call it st. peter because he guards the gates of heaven. but for a streamingdata set, this is no good, because in theory, st. peter keeps the records forever. so, you are potentially in trouble. hestores too much data.so, i decided it would be project santa claus, keeping with the season. he wants to find out who is naughty ornice, but he discards the data after a year, so he is clearly a streaming data analyst.visualization of internet packet headers272statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.so, let me toggle over to show you a little bit about this. this is just a little bit of stuff that we have put together.it has some interesting things in it. you can go to a glossary and you can look up any of these acronyms that you canimagine what they might actually mean. you can do things like look up port numbers. so, if you are interested infinding out which port is which, you can do such things. you can look up route zones. so, if you want to know what asuffix is and where that place is, you can do that. one that has been advertised a lot lately is ﬁtv.ﬂ so, if you want toknow where ﬁtvﬂ actually is, it is in tuvalu, wherever that is. so, you can do some things like that.we have two stations, ariadne, which is the sniffer, which sits outside the firewall, has roughly a terabyte ofstorage capability, and it runs the tcp dump and other kinds of stuff. you can find out some stuff about that, if you areinterested. theseus is the analysis station which sits in side of our lab. by the way, as bill pointed out, these are sort ofsensitive issues. the content of what is being looked at is an interesting privacy issue. there is an assumption ofprivacy in general on the internet, although it is not so clear that that is maintained. we could, in principle, track theactual stuff that people are downloading and certainly the ip addresses that they are downloading. one of theinteresting things in the state of virginia is that faculty are not allowed to look at xrated web sites, but students are,and so are secretaries. it is against the law for me to look at anything bad, unless i have a project like this.steve marin was asking me why i do this instead of using somebody else's database. let me show you a little bitof sort of what data looks like. so, here is some data that comes out ofšthis is what amy would call sort of level 2data, i guess. this is sort of semiraw data that comes out ofšthat is slightly processed coming out of something liketcp dump.we can look at substructure data. one variable of interest is the source manufacturer, and source serial number.so, we can track things to specific machines. this is the time stamp that is put on by our machine, when the packetcomes through it. so, we do have a time stamp associated with it.as somebody pointed out the other day, no talk of ed wegman's would be complete without some kind of graphicparallel coordinate display. so, here is a scatterplot matrix of things that we might be interested in. we could increasethe size of the pixels. one of the interesting things is to go to the parallel coordinate display. source manufacturer is aninteresting thing.if we don't know the mac number of the machine, it continues to record the ip address. so, these things overhere on the righthand side are things where we couldn't identify the mac number of the machine, but those are thingsthat have an ip address. just for purposes of discussion, we can get rid of those things, and we see a couple of thingsthat are of interest.audience: ed, could you just be clear about that? do you mean you don't have a mac number?mr. wegman: we don't have the mac number for it. we have the ip but we don't have the mac. so, here area couple of things that are of interest.so, the thing i just colored in green, let me do one other quick little thing here. we can drag this down. so, thething i just colored in green, you will notice, is one machine source manufacturer and one source serial number. weonly have one machine and one serial number. that, in fact, is our router. what we have here is, this is deltavisualization of internet packet headers273statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.time, this is time. the frame is essentially, in an ip there is a sequence number.so, what is happening is there is a lot of data coming in. there is a batch of data coming in here, and it is comingin through this time frame. so, these are the sequence numbers that are associated with the data that is coming in.here is another probably web page that is coming in, and here are the frame numbers associated with that. here isa third item, and the frame numbers that are associated with that. so, those are big files.the red thing also has only one ip, one source manufacturer number, and one source serial number. so, that turnsout to be our web server. so, sad to say, we get less traffic. people aren't interested in us as much as they could be, iguess, but occasionally we get some hits. just to give some other interesting things, we have a lot of dell machines andwe have a number of gateway machines. so, we can identify those.this is all stuff that is internal to our subnet. so, that is a quick tour of something that you could do. i am going toexit santa and go back to the powerpoint.so, i wanted to say a word on recursive algorithms, because recursive algorithms are the algorithms that youbasically need to deal with streaming data.clearly, things like means and moments can be formulated recursively. the idea with recursive algorithms is thatyou don't want to store data any longer than you have to. so, you want to keep whatever quantity you have and do anupdate. so, things like counts, means, variances, covariances, other moments, are easily computed recursively.probably less well known is that there are recursive forms of kernel densityvisualization of internet packet headers274statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.estimators. so, for the people at rice university who think that probability density estimation was invented, there aresome recursive forms of kernel estimators.this is one and this is another one. these have essentially the same properties as ordinary kernel densityestimators, the same kinds of convergence rates. they can be extended to multivariate settings as well. so, if you areinterested in streaming data and collecting density information, you can do that this way.i thought daryl pregibon brought up an interesting idea yesterday, which is the idea of exponentially weightedaverages. if you have something that is sort of an exponentially weighted average, that takes some data and creates anobject, then you can do this recursively, and you can do this with almost any old object. so, if you have a startingobject and you have some data that updates it in the proper way, you can do streaming data with this as well.visualization of internet packet headers275statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.i just wanted to give you a couple of references. this paper, i told someone the other day of the rusty stapleprinciple which is, if the staples on your reprint are rusty, it is time to cycle the research, because clearly everybodyhas forgotten about it. i did some work with ian davies in the late 1970s that did this stuff on recursive densityestimation, and gave exact rates of almostšcarey priebe, who is in the audience here, did the stuff on adaptivemixtures, and adaptive mixtures has a formulation that is a recursive formulation as well. i had a student a few yearsago, mark sullivan, who did stuff on correlation and spectral estimators in a recursive fashion, so, computationallyefficient stuff.some of the things we are planning to do is some waterfall graphics, where we scan ports and ip addresses. oneof the things we are planning to do is transient geographic mapping.visualization of internet packet headers276statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.we are interested in both highintensity traffic being persistent and lowintensity traffic being persistent. the ideais that people who are trying to intrude often do it very, very quickly. so, what we would be interested in doing islooking at a geographic map that had the locations of ips that are very shortlived.we, for example, at george mason have had a lot of trouble with people in mainland china probing our systems.there are just a couple of graphics that i will hurry through. again, as someone pointed out, one has to have aparallel coordinate plot.visualization of internet packet headers277statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.so, some additional referencesšand a lot is due to dave marchette, who is also in the audience, who wrote thiswonderful book on computer intrusion detection and network monitoringšstevens and this leidenwilensky book,are both useful in terms of understanding tcp/ip addressing and so on.some of the pictures that i just hurried through were from solka, marchette, brad wallett, all three of whom weremy students.just for acknowledgment, our work here and the work on surveillance is supported by a critical infrastructuregrant from the air force. i also work on the darpa isp program with carey priebe as the principal investigator and,as i said, dave marchette plays a key role. so, that is it.ms. martinez: while we are switching, we have time for one question.visualization of internet packet headers278statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.audience: ed, it doesn't strike mešlooking at these packets, it doesn't strike me as a visualization problem.so, i am sort of curious why you framed it that way.mr. wegman: i am not sure why you don't think it is that. so, i guess i am curious the other way. i guess i seea lot of things as visualization problems.one of the issues that we are particularly interested in is having a monitoring system that can detect, very, veryquickly that we are getting intrusions into the system. we want to be able to cut off things quite rapidly, i guess. i amnot sure that is an adequate address any more than my answer to steve marin. i guess i am one of those guys who likesto do it myself.visualization of internet packet headers279statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.paul whitneytoward the routine analysis of moderate to largesize datatranscript of presentation and powerpoint slidesbiosketch: paul whitney is a scientist in the statistical sciences group at pacific northwest nationallaboratory. his research interests currently include the analysis of data objects associated with the contents of theinternet and data analyses associated with computer simulations. he has developed information retrieval methods,exploratory analyses algorithms, and software for these, notably for image and text data. he has contributed to thedevelopment of a variety of information visualization methods. currently, dr. whitney is exploring data analysischallenges associated with agentbased models and developing methodologies for exploring and retrieving structuredinformation such as transactions and relations.toward the routine analysis of moderate to largesize data280statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.transcript of presentationmr. whitney: good morning. i don't know that i am going to start out as ambitious as looking at the wholenetwork.i am from pacific northwest national laboratory, a group of about 40 statisticians there. we do all kinds of dataanalysis. there is this phrase that describes a fair amount of what we do, probably because of the situations we live in,and it is fighting our tools. they just don't quite do everything you want them to do. we have to do a lot of customwork.i really wish i would have recorded the sound of my hard drive swapping, just to play for you guys. i hear it allthe time. it is really a key thought. if you have heard your own hard drive swapping, just remember that we are goingthrough some of these analyses, some of the stories of what we have done.i have a lot of colleagues. here is a list of some that are related to the stories i am going to tell. i think one personhere is in the audience. i think there are 15 people on that list, and 4 of them are statisticians. it turns out that there is afellow there who is a risk analysis for things like nuclear reactors. there is a fellow there who is a remote sensor, somesoftware engineers. there are people here who i really don't know what their technical background is, but we do haveuseful technical interaction. it just never quite comes up, what they did in school. here is some data. it also happens tobe where i am from.this is the columbia river. this is some satellite image i downloaded from spot. it was a few years ago and imight have the name wrong. it is low resolution, buttoward the routine analysis of moderate to largesize data281statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.you can see a lot of what is going on. this is the city here. there is the snake river going into the columbia, theyakima you can't quite make out. there it is. i work right about there. this is agriculture, and it is blurry, probably notbecause the measurement is that bad, but you get an image in one form, you put it in another form, you put it intopowerpoint, and god knows what has happened by now, but there are a lot of those circles for irrigation. the reasonthat you don't see anything like that here is that that is the hanford site. they don't do agriculture there. there are afew installations actually here that we will look at. i haven't got a good idea about why there is this demarcation. isuspect a fence. this is rattlesnake mountain.okay, i am going to describe some data analysis stories. just for background, i would like to describe what ourcomputing environment looks like, just so you can get some idea about the size and difficulty of challenge that we arefacing here. fortysome odd people, two to three computers per person, typically a newer one, an older one, someunix, some mac, some pc.our breadandbutter software tends to be these guys here, because of the flexibility they give you. some otherlanguages, less and less fortran and c over time. i don't think i have written anything in those languages in a longtime. then, packages, just depending.we have got the potential for a lot of storage, the afs share kind of thing, and the network is good. it is reliable.you know, a lot of pcs. the demographics associated with macintoshes are interesting. here is the ram size. this isgetting back to the swapping thing. there is one lucky dog who has got a couple of gigabytes of ram and,toward the routine analysis of moderate to largesize data282statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.you know, my computer is right there, and then my portable is right there, pretty typical, good stuff, when you thinkabout it.also, another thing to keep in mind is the computing model. it is the single work station computing model is whatwe tend to deal with on a daytoday basis.here is an outline. i thought i would start at the end. so, the computers are good. they are great, and stuff stillbreaks. the reason stuff breaks has to do, i believe, with the software model that we are using. the realization of thatis nothing deep, and the solution is just work, that you have to keep track of the ram you are using, and that will havea lot of implications, i think, in the future.it is an observation echoed very strongly by ed, and johannes is worried about memory, also, very explicitly inhis talk.this failure of tools happens not only on streaming data or massive data, but ordinary data sets of a size you couldpresumably fit in memory will cause the computers to die with the tools that we typically use.then, there is another type of complexity that is a whole lot slipperier. i think it is hard to describe, but if youthink about the potential complexity underlying a large data set, and how you start to get your arms wrapped around it,it starts to become kind of daunting. for instance, let's pretend that you did a cluster analysis of a homogenous data set,and you had some nice formula that said what the optimum number of clusters might be, and if you can get that downto 3,000, because that is the optimum, well, you are just not done. that is still way too many for the human to absorb.so, somehow the complexity isn't just in the data set. it is in the communicationtoward the routine analysis of moderate to largesize data283statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.of the information to the user, and that is a tricky thing, a very slippery thing. we spend way too much time handlingdata. i am sure that happens to everybody, and i don't have a deep recommendation there to get to. it is just that wework through it like everybody else does.so, here is what we do. here is a good strategy to consider, and think of the data as being something like thatimage of the hanford area. first off, you have got this data. it is in a digital format, but we want to be able to use ourstandard data analysis tools for it. so, the first thing that we do, we make a vector out of it. there is a ton of ways to dothat, and there is a lot of good work out there that can be used.for instance, if you are dealing with a collection of documents, this isn't what you would use, but you could useas fundamental information just word frequency counts. the world has moved on from that, but that is a nicemeasurement to think about. if you are working with an image or a collection of images, you could imagine looking attextures, edges, various bits of color.it turns out that both of those things, while being simple and waytoobrieflystated there, they eventually can bemade to work very well. it is kind of surprising and gratifying.the characteristics of those are indicated here in these two bullets. one is sort of a social characteristic, thebottom one, and the upper one is pretty interesting. each coordinate in that vector that you are using to construct asignature can be very uninformative. for instance, if you are making a vector of word frequencies to represent adocument, and you have multiples of those because you have multiple documents, thetoward the routine analysis of moderate to largesize data284statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.number of times that the word signature appears, that is a pretty low level of information about the objects in question.it turns out, you put that together and you can start to do useful data analysis. similarly, with an image: a colorhistogram, by itself, might not tell you a lot, but you put that together with other information and you start to getsomewhere.this social characteristic is important. guys like me can start making up sensors, virtual sensors, basically. youcan, too. that basically changes the nature of our profession and our jobs as data analysts, i think, in a good way.finally, why would you care about a strategy like this? well, people have been coming up with data analysisalgorithms forever. it is great. if you take a strategy where you encode some data object as a signature vector, thatmeans that you can borrow algorithms from all kinds of staff packages, neural net packages, whatever, to try things.so, it is an effective strategy, basically, for running through a lot of analysis potential.here is a way too busy showing of a collection of documents. let me back off that one and go to something alittle simpler. suppose you got 400 documents. you want to know what is in there. and you have only got 30 minutes.so, you can't read them all, that is out of bounds at this point. well, there are tools out there that will basically look atthose, summarize what, say, the key words are, and do a visual clustering of the collection. then, even better, start tolabel the various regions in ways that you can hopefully begin to understand what the generic contents of thatcollection are.so, the technology. you make one of those signature vectors. it turns out you need the coordinates to bemeaningful.you do a little nonmetric multidimensional scaling. there is a lot of artistic license in this particular one, and yougo. so, it is a good functional thing, and you can start to build analytic functionality on top of that as well.this is a dangerous picture in the sense that i have never been able to explain it very well in a public forum, or aprivate forum either, probably. the picture suggests there is some sort of relation going on. let me indicate broadlywhat it is. you have matched pair data. think sophomore statistics here. it turns out that one measurement is one ofthese text vectors for the english version of the document, and another measurement is the text vector for the arabicversion of the document.you have got, then, in some sense this regression problem you can do from english to arabic or arabic toenglish. that part, by the way, has just become standard data analysis. it is regression and this is a very looseygooseyregression display. by thetoward the routine analysis of moderate to largesize data285statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.way, it does show that there is some potential for mapping there. the reason you might care is that maybe you don'tspeak one of those languages. so, you say, okay, i am going to calculate my vector for thisšyou know, i speak arabicand somebody gave me this darned english document. so, i will calculate my vector for it, i will plug it into theregression formula, i will find what arabic part of the space it lies in and then say, oh, that is just like these otherdocuments.so, it gives you a way to do that kind of problem, again, based on this simple idea of calculating these signatures,using standard data analysis procedures, and then mapping back to the problem space.another type of data object, image and video. it turns out that there are walls of books of how to do imageanalysis. so, we don't have to do that. the science is done.what they don't have done is things like, well, i have 10 of these images in my shoe box. i would like to sort themout. i have never actually organized my photo collection. how do i go about doing that? well, if they are digitized,you can calculate a vector representation for each of those images. you can do one of those visual clustering typethings and do a multidimensional scaling and show basically this organized collection of images, and that is one wayto go.what this shows is something similar for a small videoclip. i am assuming 80 percent of you can recognize whatvideoclip this is. the calculations are really simple here, that led to this picture.we did one of those signature vectors for each frame. we did a cluster analysis for the signature vectors. we tookthe picture with the vector nearest the representertoward the routine analysis of moderate to largesize data286statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.vector, and then just show it and say, well, there is a label you can slap on that tape that gives you some idea of thecontent in a quick and dirty fashion.the idea just goes on. here is the hanford site, a little part of it. the 200 area is what it is affectionately knownas. it is an iconus shot of it, so it is one of the bands of that. you can see, there are roads, some buildings, god onlyknows what, and so on.what this picture is, is what happens if you calculate a little vector that describes the content, just sort of there andthere and there and there, and get a little subpicture around that vector location and say, well, i am going to do amultidimensional scaling type thing for those vectors.to indicate the content, i will just show the little pictures near each vector. then you say, okay, i have got this.have i got anything interesting from an exploratory analysis or even classification point of view, even though it was anunsupervised thing? so, you know the data base has been around for a while and this data has been around for a while,so you can start doing brushing ideas.so, you grab a bunch of these little pictures here in this region of this multidimensional scaling thing and see whatyou grab over here, just to get an idea of whether you have done anything useful. okay, we have got a lot of these sortof benign regions, fields, as it were. actually, it would be more like gravel fields there.toward the routine analysis of moderate to largesize data287statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.let's grab another region and see what we get. well, we got roads, when you look at it.so, it becomesšyou develop the potential for building some interesting data analytic tools with that strategy, andit kind of gets you to the place of this, you know, all of these things, it is either the statistician manifestation orstatistician hubris, that everything is data.you have got all of these objects. you have got this strategy. you can use it and see what happens. so, let's talkabout some more data.let's back up a second. so, you have got network traffic. you have got economic models. i was going to also talkabout an error reporting data base, very diverse data objects.toward the routine analysis of moderate to largesize data288statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.network traffic, you have seen some information on the format of that, but we have got a strategy forsummarizing the content, potentially. so, we will show how that begins to play out.an economic model, it turns out we spent some time analyzing the content of the output of an internationaleconomy, just a simulation of it. we have no idea, actually, if the model was any good. there is probably a joke inthere somewhere. it is, again, a whole other data type and how do you get your arms around it. well, let's just dive in.we are focusing on the content. i mean, there is a lot of work on the packet. you have seen a lot of that heretoday, and some indication of why you might be interested in that from a network design point of view, but we wantedto look at the payloads.our model is, we were going to look at the contents going by at a particular point. so, you imagine, if this is anetwork cable, i am just going to keep track of what goes by here, something that has that type of mental model.there are tons of challenges. you have the ethical and legal issues associated with privacy. data handling isalways a problem. there are tools out there to help you get by that, but you have to learn how to deal with the tools. iam sure folks have gone through that learning curve here as well.then, it is streaming data, and we have kind of been challenged by data sets on the order of the size of memory inour computer, given our current tools. so, streaming data is a whole other level of difficulty.so, what do we do? well, we have got a strategy. so, let's try our strategy and see what happens. what can you dofor a signature on streaming network data?it is not just text going by and it is not just images going by. it is who knows what going by in those payloads. so,you have immediately got a challenge of, well, i can't just read the image analysis literature. i can't just go to thecomputational linguistics literature. it is everything. i have got to do something that handles everything.well, you know, there are some really straightforward things that have worked in the past for text that have atleast the mathematical extension to a bite, to just digital data. so, we decided to look at bytebased ngrams and acouple of simple means of summary.toward the routine analysis of moderate to largesize data289statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.ngrams are fairly well known. i thought i would take a minute, just in case there were a couple of people thatdidn't know what those guys were.an ngram signature for a text document is basically a frequency table of successive runs of characters in thedocument. so, if you have got this type of text, and you are doing a three gram [on the phrase ﬁan ngramﬂ], then youhave anspace, you have got nspacen, nspacedash, and so on. you just accrue those things.it turns out a fellow named danocheck did some very nice work showing that you could use those as a basis of asignature to distinguish among language types. then, subsequently, folks figured out that, son of a gun, you couldactually do a fair job of information retrieval based on those crude measurements.this, again, emphasizes a point, that this, as the basis of the coordinate vector, anspace, isn't awesomelyinformative, all by itself, and the vector gra is not buying you a lot either, all by itself. if you take that weakcollection of measurements together, you can start to do stuff. there is nothing new about this.toward the routine analysis of moderate to largesize data290statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.here are some scanned xeroxes from cover and joy's information theory book, showing what some of thesemodels actually do from a generative point of view. if you just start thinking about what an ngram might mean, well,it kind of goes with this markov model thing. so, you estimate some frequencies, you generate some data from themodels.this is a secondorder one, sort of like dealing with two grams, and okay, hey, there is a word, there is a word,and that is not bad. it gets kind of funny down here. this is a fourgram. this could be a word, and so on. so, it doesseem to have something to do with language. so, it is an okay measurement.we decided to just make a leap of faith and see if it would have anything to do with generic digital data objectswhen we went to bytebased ngram things. here is a summary of how this might work. eventually what we did was,we used a bunch of stuff from my web cache. we broke it up into size 1,500 bytes for reasons that are clear at thispoint in the day, and just started categorizing things in an unsupervised setting. this is a nice plot from r. you can seethat, even though it was unsupervised, we did a really good job of isolating these guys.these guys are really strong in this cluster, but spread among other clusters. so, it is not an overwhelmingconnection between type of file and cluster, based on that signature, but it is not random either. it is better than random.the big challenge here, by the way, was just how do you semantically represent the content of something thisgeneric. we didn't address that challenge in this particular exercise, but we were just going to use exemplar objects asopposed to things like words and subimages.toward the routine analysis of moderate to largesize data291statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.this second bullet was our t stumbling block. my computer kept making that noise, basically, during thisexercise, using the types of tools i showed you. it turns out that if we just took a little time and rewrote one of theclustering algorithms to just not use so much memory, we did much better. basically, the compute time went fromimpossible to, okay, that is a cup of coffee.i am going to go right to the end. ed wrote a very nice paper that appeared in the mid1990s that laid out one wayto organize your computations to fit on work stations. one of my favorite lessons learned was that my work station ispretty good. i can solve a lot of problems with my work station.even taking that advice, and just working with, say, a 100megabyte data set in a 500megabyte work station, andthe kinds of tools that we typically use, stuff happened. bad stuff happened. the computer made the noise. we chargeour time by the hour. it is bad.it turns out that this would be good. a bounded ram would be really good, and recursive formulations, there area lot of things out there. use statistics are good, the common filter is good.once you know what you are looking for, you can do a web search and start finding good theory out there in thecomputer science literature in the database area, saying how to organize your calculations to achieve this. i think theyare just getting started there as well. there is a ton of good work that a lot of people could do.let's think about that together here a second. there is some, as i said, some work out there. there is somecommercial software, actually, that is thinking along these lines,toward the routine analysis of moderate to largesize data292statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.probably some i don't even know about.one of our spin off companies took some time to worry about keeping good control, the amount of ram used intheir calculations, but there is theory that you could do, too.i mean, you could do the relative efficiency of a bounded ram statistic versus an unbounded ram statistic forvarious problems.you could imagine rearchitecting some of our standard tools, r, say, to start taking advantage of some of theseideas.toward the routine analysis of moderate to largesize data293statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.there is a lot of good work a lot of people could do that would get us not only to moderate data sets, but i thinkget us all going in a streaming data set sense.this second recommendation is slippery. i don't think i have made a great case for it. if you think about thecomplexity that ought to be inherent in some of these larger data sets, and how much trouble we have communicatingsome basic ideas, i believe there is a lot of effort that we need to expend in that area, and it is going to take a lot offolks, in part, just because they are all the type that we are going to be communicating with, and in part, because thestatistics community isn't going to have all of those answers.ms. martinez: it is lunchtime. one question.audience: what type of clustering algorithms have you had the best luck with, and have you developedalgorithms specifically to deal with streaming data?mr. whitney: i tend, just by default, to use a kmeans, and it works pretty good. it is simple, it is fast. weplayed with variants of it, a recursive partitioning version, with various reasons why you would recurse.the version that i ended up writing for that network problem wasn't for streaming data. it was for data basicallydefined so you could pass through and repass through it and repass through it. so, we had an explicit data structure thatrepresented that type of operation. i hope to use that as a basis for lots of other algorithms, though.toward the routine analysis of moderate to largesize data294statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.leland wilkinson, chair of session on mining commercialstreams of dataintroduction by session chairtranscript of presentationbiosketch: leland wilkinson is an adjunct professor of statistics at northwestern university and senior vicepresident of spss, inc. his research interests include statistical graphics and statistical computing.dr. wilkinson received his ab from harvard college in 1966, his stb from harvard university in 1969, and hisphd from yale university in 1975. in addition to his statistics background, dr. wilkinson has also served as a lecturer,visiting scholar, and professor of psychology at yale university, the israel institute of applied social research, andthe university of illinois at chicago.one of dr. wilkinson's many accomplishments is the development of systat, a statistics and statisticalgraphics package that he designed in the late 1970s and then incorporated in 1983. an early feature of systat wasmystat, a free statistical package for students. systat and spss became the first statistical software companies tomarket fullfeatured windows versions of their software. in 1995, systat was sold to spss, which was in turn soldto cranes software international in 2002.introduction by session chair295statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.transcript of presentationmr. wilkinson: all right, this session is on mining commercial streams of data, with lee rhodes, pedrodomingos and andrew moore. only one of our speakers is from a company although the other two, as you know, areinvolved in developing procedures, highdimensional searches, and mining and other areas that are highly relevant towhat is done by businesses.i just want to highlight the three major market shares of applications of streaming data analysis, and these arequite large. monitoring and process control involves such applications as general electric with its turbines worldwide.there are many, many turbines and, to shut down a turbine, can cost millions of dollars per day in their system. so,they need to maintain continuous multivariate data stream monitoring on those turbines, and they have real needs fordisplay and alert and analysis capabilities.ecommerce goes without saying. we all know pretty much where that lies. many are putting ecommerce dataand web logs into databases, but amazon and other companies are analyzing these in realtime.financial is another huge area for streaming data. i thought i would give you a quick illustration of how that getsused.this is a java application called dancer that is based on the graphics algebra, and the data we are putting into itnow, we happen to be offline, of course, but this data feed is simulating a natural stream coming in.these are microsoft stock trades, and these are coming in at roughly 5 to 10 per second. on the right, you see thelist of trading houses, like lehman brothers, and so on. these trades, the symbol size is proportional to the volume ofthe trade. up arrow is a buy, down arrow is a sell order, and then a cross trade is a rectangle. these traders want to beable to do things like alter time, back it up, and reverse it. those of you who have seen the tivo system for tv, video,know that these kinds of manipulations of time can be critical.this application, by the way, is not claiming this as a visualization. it is actually doing the calculations as soon asthe realtime feed comes in. notice all the scaling is being done on the fly. you can speed up the series. if you speedthis up fast enough, it is a time machine, but i won't go into that. i will show you just one more aspect of realtimegraphics, and these are the kinds of graphics that you plug into what the rest of you guys do.when you develop algorithms, you can plug them into graphic displays of this sort. this one simulates the way ibuy stock. actually, i don't buy stock for this reason. it is just a simple exponential forecast.you can see the behavior. this is trading in oracle and sbss. this type of a forecast represents exactly what i doand probably some of you as well which is, as soon as it starts going up a little bit, buy.what is being done here, the model is being computed in realtime. so, you get, in this kind of a system,anywhere from 10 updates a second to 10,000 data events per second, and 90 percent of the effort in developingsoftware in this area is in the data handling. how do you buffer 10,000 events per second and then render in roughlyframes per second using the graphic system? so, the rendering system is a lot simpler than the actual data handlingsystem.introduction by session chair296statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.so, now we are going to see some presentations that will highlight how these systems work, and we will beginwith lee rhodes from hewlettpackard, who will tell you about data collection on the web.introduction by session chair297statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.lee rhodesa stream processor for extracting usage intelligence fromhighmomentum internet datatranscript of presentationtechnical paperbiosketch: lee rhodes is chief architect for analysis in hewlettpackard's management softwareorganization. during his career at hp he has led numerous research and development efforts in a broad range oftechnology areas, including fiber optics, integrated circuit design, highperformance graphics software and hardware,cpu architecture, massively parallel processing systems, multimedia and video streaming software, communicationssoftware systems, and two and threedimensional visualization software.since 1996, mr. rhodes has been heavily involved in the development of operational systems software for thecommunications network service provider industry. he has invented a suite of technologies in the area of realtimeanalysis software that enables internet service providers to quickly extract key statistical information about subscribers'usage behavior that is critical to security, network operations, capacity planning, and business product planning. heassembled and managed the r&d team that developed this technology into a commercially successful product.mr. rhodes' formal educational background includes a master's degree in electrical engineering from stanforduniversity where his emphasis was on integrated circuit design and solidstate physics. his undergraduate degree wasin physics from the california state university at san diego.a stream processor for extracting usage intelligence from highmomentum internet data298statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.transcript of presentationmr. rhodes: this should go flawlessly because of our advance planning. first, i want to tell you how honoredi am to be here, and i want to thank the organizers of this conference for putting this setting together. i particularlywant to thank lee wilkinson for being a great mentor and friend and guiding me along the way.i am going to talk about a stream processor that we have developed at hp that is currently on the market. we sellthis product. before you do any kind of measurements in terms of what we are talking aboutši just want to be clearthat we should be calibrated.this is not science. this is engineering. our role, my role, at hp is to develop advanced software. our statisticalsophistication is very low. i am learning and, with the help of lee wilkinson, i have learned an immense amount. ihated statistics when i was in college, but now, i am really excited about it. so, i am really having fun with it.in isolation, much of the technology you will see here has been written about before in some form. nonetheless, ithink you will find it interesting for you.the context of this technology is that we develop software for communications service providers. so, this softwarešand particularly internet, although not exclusively internet providersšthose are our customers.how we got started as a startup within hp about five years ago was exclusively focused on the internet segment,and particularly broadband internet providers. we are finding that the technology we built is quite extensible toneighbor markets, particularly telephony, mobile, satellite and so forth.now, the network service providers, as i am sure you know, have some very serious challenges. the first one ismaking money. the second one is keeping it.in terms of making money, marketing 101 or business 101 would tell you that you need to understand somethingabout your customers. the real irony here is that few internet service providers do any measurements at all about whattheir customers are doing. in fact, during the whole dotcom buildup, they were so focused on building infrastructures,that they didn't take the time, or invest in, the systems that would allow them to understand more about customerbehavior.that even goes for the isps that are part of big telephone companies. of course, telephone companies have a longhistory of perusing the call detail records and understanding profiles of its customers.there are some real challenges here, not only understanding your customers, but understanding what thedifferentiating services are. it is very competitive. what kind of services are going to make money for you.another irony is pricing this stuff. it is not simple. it is not simple now, and it will get even more complex,because of this illusion that bandwidth is free. that won't survive. it is not free.so, there have to be some changes and, as i go through the talk a little bit later, i think you will see why pricing issuch a challenge, particularly for broadband. certainly, you want to keep your own subscribers are part of yournetwork, but you are also concerned about use, fraud, theft, and other kinds of security breaches.now, when you go and talk to these service providers, they own the big networks. what you find is like in anybig organizations. they have multiple departments and, ofa stream processor for extracting usage intelligence from highmomentum internet data299statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.course, the departments don't communicate very well. this is not a surprise. we have the same problem at hp.nonetheless, the sales people are interested in revenue. so, they are really interested in mediation systems whichcollect the data about the usage of other subscribers so that they can bill for it in some way. this is an emerging trendand will continue to be.they are interested in not just bytes, but they are interested in what types of traffic it is, time of day. for instance,they want to be able to track gamers, say, to a local gaming host on the network, because their network bits are cheaperthan peering agreements out on the open networks. so, understanding who the people are who are using games and soforth would be of interest to them.product development. these are the folks who come out with the new services. so, they need to have some senseof, well, is this going to make money or not, what is attractive.network operations needs understanding of utilization and performance on a daybyday basis. they tend to bevery focused on servers, on machines, on links, to make sure they are operating properly.product planning is often in a different department. these are the ones who are interested in future capacity, howcan i forecast current behavior forward to understand what to buy and vend.realize that a lot of quality of service, if you can call it that, on the internet, today is accomplished by overprovisioning. so, if i have bodacious amounts of bandwidth, nobody tends to notice. of course, ip is particularly poorat quality of service, but there is work being done to do that.so, the technology challenges for the service provider, there are many, but here are some of the few key ones.they would like to capture the data that would service all these different needs once. they are expensive tocapture usage data, and the tendency is, among vendors such as hp, is to go in and say, oh, great, we have this widget.we will just sample your key core routers with snp queries and get all this valuable data for you.of course, every other vendor comes in and wants to do the same thing. so, they end up with 50 different devicesquerying all their routers and virtually bring the routers down.economic storage and management of the internet usage data is a severe problem. of course, they want theinformation right away and, of course, it has to scale.so, i am talking about some of my back of the envelope kind of analysis of this problem of data storage andanalysis challenges.starting withšthis is what i call a cross over chart. what i did is very simplistic calculations saying internettraffic, particularly at the edges, is still growing at about doubling about every, say, 12 months. at times it has beenfaster than that. over the past several years, it seems to be pretty stable.one of the interesting things is that the traffic in the core of the internet is not increasing as fast as it is at theedges, and a lot of that has to do with private peering agreements and caching that is going on at the edge, which iskind of interesting.the next thing i plotted was aerial density of disk drives. in the disk industry, this is one of their metrics, is howmany millions of bits per square inch of magnetic surfacea stream processor for extracting usage intelligence from highmomentum internet data300statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.can they cram onto a disk. that has been doubling about in a range of 15 months. so, it is a little bit slower. thenmoore's law, which doubles about every 18 months.so, the axes had no numbers on them. they don't need it. it doesn't matter where you originate these curves, youare going to have a cross over.if this continues to grow at this rate, then at some point thešchoose your measure. the traffic on the internet isgoing to exceed some value. i think we can help with this one by better collection strategies and using statistics.audience: i have to admit, i am really confused here by comparing internet traffic volumes to disk drivedensities.mr. rhodes: it is just a very simplistic assumption. it says that, if i am receiving traffic and i need to storeinformation about that traffic that is proportional to the nontraffic, i have got to put it someplace.audience: what does it mean that they are equal?mr. rhodes: i am just saying choose a value. suppose you can store so many trillion or terabytes of datatoday. if the ability to store economically their data doesn't increase as fast as the traffic increases and the need to storeit, you may have a problem.audience: so, where is the traffic coming from, if people can't store it?mr. rhodes: that is on your own machines. remember, the internet is still growing. there are people joining.now, the other crossing is moore's law, which says if the traffic continues to increase faster than intel canproduce cpus that keep up with it, or cisco can produce processors that keep up with it, you just have to add morehorsepower.audience: well, isn't the traffic consumed? if i am watching a video, i consume that traffic, i don't store it.audience: some people might want to store it.mr. rhodes: okay, at the service provider, they are not storing the actual traffic. what they are interested inare the summary records, which are called usage data.the usage data are summaries of flows. at least, that is very common in the service providers. it is a fraction ofthe actual traffic, but as a fraction, it stays about the same. so, as a service provider, the tendencyšand this may seemstrangešis to serve all of it. those who have telecom backgrounds sometimes save their call detail records (cdrs) forseven years. sometimes there are regulatory requirements.saving the internet traffic, number of summary records for a session which you might have on the record, is farhigher, orders of magnitude higher, than a single phone call. if you make a phone call, one record is produced. if yousit hitting links on the internet, you are producing sometimes hundreds of sessions, as far as the way these sessions arerecorded.the second graph is also a back of the envelope calculation. this is based on some measurements that we havedone, which is the storage required.now, presume that you wanted to store each of these just usage records. one of the factors that we have measuredon broadband internet is the number of what we call flows. these are micro flows, really, per second per subscriber ina broadband environment. it is around .3, and varies, depending on time of day from about .1 up to about .3.now, you multiply that through times the size of a storage record, and they don't want to store just the flowinformation, they usually also need to put information like thea stream processor for extracting usage intelligence from highmomentum internet data301statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.subscriber id and some other key information. you assume a couple hundred bytes per record. all of a sudden, youare talking about pedabytes or exabytes of storage, if you want to store it for any kind of period.so, these represent different numbers of subscribers, different scales. the dark red one is about a millionsubscribers and, as a service provider we are working with today that saw this coming and realized that they had aproblem.the other one is also a back of the envelope calculation, time to process this stuff. say you get it all into adatabase. you have got to scan it once. that can take a long time.there, i just projected different database systems, depending on how many spindles and how sophisticated youwant to get, in terms of how many records per second can you process, and how much money do you want to spend onit.so, we are talking about years, sometimes, if you wanted to scan the whole thing. so, there is a problem here andit has to do with inventory, if you just have too much inventory of data. handling it is a severe problem.so, this is a somewhat tongueincheek illustration, somewhat exaggerated to make a point, but a lot of our majorcustomers are very used to having very big data warehouses for all their business data. data warehouses aretremendous assets. as soon as you start trying to plug these into the kinds of volume we are talking about, it no longermakes that kind of sense.what we have developedšthis is just a short cutšis a sense of how can we capture information on the fly, andbuild not just a single model, but hundreds or thousands of small models of what is going on in the network.then, we have added the capability of essentially a realtime lookup, where the user here can, using a navigationscheme, can select what data they want to look at and then they look at, for instance, the distribution statistics of thatintersection.this is the productši promise i am not trying to sell anything, but i just want to say this is the architecture of theproduct that is the foundation of this. it is called internet manager. it is an agent based technology. these representsoftware agents. it is these three things together here, encapsulator, rule engine, and a distributed data store.in a large installation, you can have scores to hundreds of these, and the whole idea is putting a lot of intelligenceright up close to the source of this high speed streaming data.we have different encapsulators. these are all plugins. the encapsulator is like a driver. it basically connectswhatever the unique source, type, or record type or whatever that these various sources produce to internal format.then, this is a rule engine, which i won't talk about. basically, the flow is generally to the right, although this issomewhat simplistic, so it represents a kind of pipeline.so, these rule engines process rules, and they scale in three dimensions. one is the horizontal parallelization,which you have with many agents. the second is the size of the machine you put these one. the third is you can defercertain rules downstream. so, you can spread your intelligence processing.now, a lot of times, and where we initially got started, was supplying basically data in database form, or fileform, to various other business systems like rating, billing, reporting operations and so forth. that is how we got started.now, to give you an idea of what the datašhere is an example of one format of hundreds that we read. this is anet flow, version five record format. you can see all thea stream processor for extracting usage intelligence from highmomentum internet data302statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.different types of information that comes out. basically, it is summary information of the headers that you have beenhearing about in the previous talks.source destination addresses, source destination ports, bytes, packets, and a lot of very valuable information here.it is of a flow. a flow is a group of packets that is matched to a source and destination ip address and a sourceport, sometimes even an added destination port, and sometimes even a source port. so, it is really nailed down to theparticular transaction that is going on.so, what do we do with this? each of our engines, each one of them, can pull in records from anywhere fromaround 50,000 to 100,000 per second. the first task is to normalize these, collect them and normalize them. thesecond task is the normalization and i like to think of as a vector, which was also spoken of earlier.this is a set of arbitrary attributes. think of them as columns in a database, but it comes in as a single record andactually can be variable in the number of attributes, and dynamic.now, once these come in, and we can have multiple streams coming in, usually we know quite a bit about thesestreams. we might have a stream coming in from an authentication service like a dhcp or combination dhcp,sometimes radius, sometimes dns, that basically authenticates a user.so, the service provider knows it is a legitimate describer, as well as the usage information coming from therouter itself.what we call them is normalized metered events. it is sort of the most atomic information about usage. so, theseentities come in just like a record, and they are processed in this rule change, and a stream processing engine can't haveloops. so, no four statements and stuff like that. we basically can't afford it.it travels down and you can have f&ltype statements. the other interesting thing is, we have a statement whereeach of these, as the data is processing through, it looks at each of the field based on what rule it isšand this is allconfigurable, what rule you put in, about several hundred.there is an association with a data tree. one of the things that this data tree can be used is in sorting. as the metravels through, decision are made, there is a natural selection going on based on a certain field. then we can do simplesumming, for instance.so, summing on a variable or even a group of variables is very straightforward, doing very much the jointsomething that was spoken about earlier. this all occurs in realtime.the other use of this data tree, we call itšand it doesn't have to be just a tree, it can be a number of differentpointsšis each one of these triangles is a structure that can have an arbitrary container, and we can put data in it.so, one of the ways that we do stream correlation in realtime is that we effectively have like a switch, where wecan select information coming from what we call a session correlation source.it will load information into the tree that is used for matching, and then virtually all you have to do is now, as thenew entities come through, they correlate dynamically to information that you want. for instance, it could be the ipaddress to a subscriber, or you could do all different kinds of correlation.a stream processor for extracting usage intelligence from highmomentum internet data303statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.now, in any one engine you couldšso, i am using a symbolic representation of what you just saw, is this littletriangle of a tree here, and you can have multiple ones.so, we can do fan out. so, you can have a single source because the same data needs to go to differentapplications and needs to be processed by different sets of rules. so, you can parallel them, going to differentapplications, or you can put them into sort of sequential themes for more sophisticated rule processing.so, the work that i have been doing has developed what i call capture models. so, as this data is flying by, iwould like to collect more than just a sum. in fact, i would like to capture distributions of these variables or other kindsof characteristics. i think there are lots of things that you can došjacobeans, i haven't seen the need for thatšbutthere is the opportunity.a capture model can have child models associated with it, but one of the rules of the capture model is that thenme that goes in left goes out of the right, because you can have a series of these in a row. so, you can have multipleof these capture models plugged together.i tend to look at this like a matrix. inside any of these capture models you have ainside there is a matrix whereyou have a number of different variables that you can track. if you are doing binning, then the other axis is the bins.so, now you can put these, instead of doing just simple summing, now you can do sorting of your data, and it feedsright into this capture model.you can put them in layers and do sequential summing. so, you create all these little matrices, and they are notvery big, a few kilobytes, the largest eight to ten kilobytes. so, you can have thousands of them.now, the endtoend architecture looks something like this, where you may have some free staging, for instance,some basic correlation going on. then you put it directly into the models. that is one thing our customers are doing, oryou can have these models directly on the raw data.so, you can be binning of it and making decisions as the data is flying by. what we do, then, is we store just themodels. of course, the nice thing about these capture models is that they don't really grow with volume. the number ofthem is proportional to the size of your business problem that you are trying to deal with. then, on the right here, youhave the clients.this is an examplešit is not a great example, but it is one example of a distribution that we collected. i don't havea good example of truly realtime, but this kind of data can be collected in realtime. it represents the usage ofsubscribers over a 30day period. this thing is just constantly updating as the data is flying by.red represents the actual number of subscribers and the red axis is the amount of their usage. now, this is abroadband internet. so, you will see, i have a subscriber out here with 23 gigabytes of usage for that period, all theway down to tens or hundreds of bytes. so, there is a huge dynamic range.if you think about it, like electric utilities or other types of usage services you might have, very few of them havethis kind of wide, dynamic range. now, i fitted this, and it fitted pretty nicely to a log normal. plotting this on a linearaxis doesn't make a lot of sense.in fact, what we do in the distribution models is do logarithmic binning. this data fits that very, very nicely. it isvery probable in terms of binning.a stream processor for extracting usage intelligence from highmomentum internet data304statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.now i can see up to 90 percent of my subscribers now. there are two plots here. this is the subscribers at aparticular usage, and this is the traffic that they create. one of the things it took me a while to figure out is why thisrighthand side of this is so noisy. notice it is jumping around quite a bit.part of that is not just noise. part of that is the fact that subscribers only come in unit quantities. so, a subscriber at10 gigabytes of usage also creates big deltas out at the righthand edge of this.the other reason is the actual binning. so, they may not fall in a particular bin. so, you will see some oscillationand it is actually easier to see the oscillation between bins on this graph.i did somešafter reading bill cleveland's book, i tried the qq plot, but i did a reverse qq plot because i havealready got bytes on my x axis, and these are the standard normal quantiles on the left. what is interesting is that thefit on this is very, very good, over about four orders of magnitude.i didn't bother doing any fancier fitting at the top or the bottom of this. the users at the bottom are using morethan the models would predict, of course, and at the high end, they are using less. i find that, in looking at about adozen of these from different sites, that the top ones slop around a bit.this is an asymmetry plot, which you read a lot about in the press. actually, here, it is quantified. you can lookat, for instance, that 20 percent of the subscribers, the top 20, are using 80 percent of all the traffic. that happens to bethe way this distribution fell out. what they don't talk about is, 80 percent of the users are only using 20 percent, whichis the obverse of that, which means they have got a real severe pricing and fairness problem, but i won't go into that.some extensions of this basic technology we are doing now, and actually deploying with one of our customers, isusing this kind of technique for security, abuse, fraud and theft. we are doing a lot of learning in how to do this, but iam convinced that, once you have a distribution of a variable and you can normalize it, say, over some longer period oftime for the standard population, then you can very quickly see changes in that distribution very quickly.if all of a sudden something pops up, like a fan in, fan out, which is the number of destination ip addresses, ordestination ports all of a sudden explodes, then you know someone is scanning ports.these terms mean different things, but in the service provider industry, fraud and theft are different. theft is whenthey are losing money. fraud is only when someone is using your account, because you are still paying. then, abuse isbasically violation of the end user agreement that you signed when you signed up with the service provider.now, the other thing i am working on is dynamic model configurations, where you can dynamically refocus amodel, a collection model, on different variables, different thresholds, what algorithms are actually used and so forth,do that dynamically.that allows you to do what i call drill forward. so, instead of having to drill down always to the history, you seesomething anomalous. it is likely to come back. this is not like looking for subatomic particles.so, if someone is misbehaving, more likely it will occur again, and you want to zoom in on that and collect moredata, and more detailed data. so, that is what i call drill forward.a stream processor for extracting usage intelligence from highmomentum internet data305statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.now, some back burner stuff, something that is interestingši haven't found a real business application for thisšnow that i have got this multidimensional hypercube, so to speak, of all these collections of models, and each one ofthese circles can be different kinds of models, it sort of represents, or can represent, the business of what the serviceprovider's customers are doing.i thought it would be kind of interesting to take that and do a reverse transform of it, and then create a randomstream of usage events that looks exactly like what your subscribers would look like. it is random, but it has exactlythe same distribution behavior as the stuff coming in, and it would be multiple distributions. i figured out thealgorithms for this, but i haven't found anybody that needs it yet.so, some of the paradigm shifts that i find are challenging when i talk to service providers is really, the knee jerkreaction is, oh, i want to store everything, and it is just prohibitively expensive.i find that i have to be a business consultant and not just a technologist when talking to people. what is thebusiness you are in, do you really want to keep this stuff for this long. this belief that i have that you have to analyzethis highvolume data as a stream, and not trying to store it first, do it on line, in the stream, can reduce it first.then, consider drilling forward rather than always wanting to drill back into the history. drill forward for moredetailed analysis.we are very interested in collaboration with research laboratories. we have research licenses for this softwarewith qualified laboratories that would like to take advantage of this kind of a rale engine. some of the things that ithink would be very interesting is capture model development for us in other kinds of purposes that i will never eventhink of.certainly, we need more robust statistical approaches, and visualization work, how to visualize this stuff.the last thing i want to bring up, this is a client. it is not hooked to the network, so you can't see the realtimegraphs changing. you can see, this is actual data. you can see, for example, this is a broadband supplier. if i looked atdata, say, from one hour, it is pretty noisy and, as you increase the time, in about 30 days, it turns into a real nice shape.this is what i would be doing here if i were hooked to the network, is navigating along the different axes of thathypercube, where i am choosing different service plans or user pricing plans and so forth, that the service provider haschosen.the last thing here, i actually took advantage of the fact that i have a distribution of usage for a population and, ifi know their pricing function, i can compute the value of the traffic, and do that virtually instantaneously.that is what this does. i am not going to demonstrate it, but basically i can help the product planners for theservice provider figure out what the volume of the traffic is, without having to go back through millions and millionsof records and basically try to model their whole subscriber base. basically, you have it all here. thank you very much.mr. wilkinson: while pedro domingos is setting up, we have time for a question.mr. cleveland: lee, i just would ask if you could give some idea of where you have set this up so far.mr. rhodes: in terms of commercial deployments? we have a number ofa stream processor for extracting usage intelligence from highmomentum internet data306statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.pilots.mr. cleveland: are most of your experiences at the edges with 80sl and cable?mr. rhodes: yes, most of these are the same with edges. so, it is 80sl cable and we did one backbone, mediabackbone service provideršwell, they dealt with commercial clients. so, they had a few thousands, but very largepipes.i would say most of ouršin fact, our current deployment that we are working on is a very large sized serviceprovider in canada.a stream processor for extracting usage intelligence from highmomentum internet data307statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.a stream processor for extracting usage intelligence from highmomentum internet datalee rhodesthe data streams of the internet are quite large and present significant challenges to those wishing to analyzethese streams on a continuous basis. opportunities for analysis for a network service provider include understandingsubscriber usage patterns for developing new services, network demand flows for network operations and capacityplanning functions, and early detection of network security breaches. the conventional analysis paradigm of store first,then analyze later has significant cost and latency issues when analyzing these highmomentum streams. this articlepresents a deployed architecture for a general purpose stream processor that includes dynamically configurablecapture models that can be tailored for compact collection of statistics of the stream in real time. the highlyconfigurable flow processing model is presented with numerous examples of how multiple streams can be merged andsplit based on the requirements at hand.key words: dna; ium; realtime statistics; statistical preprocessing.1. introductionin 1997 a small r&d group was formed inside of hewlettpackard's telecommunications business unit todevelop internet usage management software for network service providers (nsps). the services offered by thesensps ranged from internet backbone to internet access. the range of access services included residential andcommercial broadband (cable and xdsl), dialup, mobile data, as well as numerous flavors of hosting and applicationservices. early on our focus was the processing of usage data records (e.g., netflow® or sflow®) produced byinternet routers. however, it quickly broadened to include convergent voice call detail records (cdrs) as well as theability to collect and process data from a very broad range of sources such as log files, databases, and other protocols.the diverse technological histories (and biases) of the different segments of the communications industry created forus interesting challenges in creating a software architecture that waslee rhodes is chief scientist/architect, ium/dna, hewlettpackard co. (email: lee.rhodes@hp.com).©2003 american statistical association, institute of mathematical statistics, and interface foundation of north americajournal of computational and graphical statistics, volume #2, number 4, pages 927œ944 doi: #0.##98/#06#860032706a stream processor for extracting usage intelligence from highmomentum internet data308statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.robust, quickly adaptable, and scalable in order to meet the broad range of requirements for fast data collection andanalysis. it was in the process of meeting this challenge that the concept of a general purpose stream processor emerged.there are two related suites of technologies discussed in this article. internet usage manager (ium) (http://openview.hp.com/products/ium/index.html) is the platform technology that provides the basic stream processingcapability. dynamic netvalue analyzer (dna) (http://openview.hp.com/products/dna/index.html) technology extendsthe ium platform to enable stream statistical analysis capabilities.2. business challenges for the nspswith the recent spectacular collapse of some major nsp players, the challenge within the industry of creating aprofitable return on their sizable infrastructure investments could not be more visible (mcgarty 2002; sidak 2003).during the technology buildup of the late 1990s, many of the nsps invested heavily in rapid expansion of networkcapacity at the expense of infrastructure for metering and analyzing subscriber behavior. in the frenzy of thetechnology hype and market buildup with cheap capital readily available it was easy to believe that bandwidth wasfree, should be free, or would become free. why bother to measure usage? and given the wide publicity of the everexpanding bandwidth of optical fiber, a superficial examination of the issues could lead one to that conclusion.unfortunately, the real bandwidth limitations are not in the internet backbone, but in the access networks (the ﬁlastmileﬂ), where the upgrade costs are high. it is ironic that even those isps that were operating units of larger, wellestablished telecommunications companies, which had developed extensive telephone subscriber data collection andanalysis capability over the past 20 years, did not make substantive investments in measuring and understandingsubscriber usage behavior. this is rapidly changing today.the business motivations for understanding usage behavior on the revenue side include various usagebasedcharging models for billing and subscriber segmentation for marketing and product planning. additionally, because ipbased services are still young, having a statistical basis for trialtesting potential pricing models for these services iscertainly better than having no data at all. the motivations on the expense and investments side are equally strong.without the ability to measure or analyze the impact of various usage behaviors, whether they are legitimate or not, thetasks of network management, security, performance, and capacity planning reduce to a guessing game.some of our early r&d work focused on the usage mediation and tracking in support of billing for telstra'sbigpondž cable and dsl internet services (http://www.bigpond.com). in australia, as was the case in many parts ofthe world outside the u.s., the cost of international transit fees, based on bandwidth usage, represented a significantvariable cost that was constantly increasing on a per subscriber basis but not transferable to subscribers who werebilled, at that time, only on a flat, allyoucanuse pricing model. data collected at telstra from nine different dsl andcable broadband internet services revealed that the distribution of subscriber usage can be fitted very closely by alognormal (with a shapea stream processor for extracting usage intelligence from highmomentum internet data309statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.factor of ~0.7). the top 20% of subscribers generate ~80% of all traffic. the top 5% of the subscribers generate ~50%of all traffic. another way to look at this is that 95% of the subscribers can end up subsidizing the top 5%. simple flatrate pricing plans for unlimited usage broadband services will naturally force the nsp to charge high monthly fees,which naturally restricts the economic accessibility and uptake of the broadband services.3. sources and types of datadata sources can also be grouped by different device types, which vary considerably by the specific application.device examples include network equipment (routers, switches, and gateways), application servers (web, email,game servers), general purpose computers, network probes, and database management systems (dbms). we havefound it useful to classify the types of data sources into usage, session, and reference categories based on how the dataneeds to be processed. for many realtime sources of data we have defined the term metered event (me) as an atomicdata structure that encapsulates information about or relevant to usage of a service at a specific point in time or withina specific window in time.3.1 usage mesusage mes contain metadata, which are data about data. at the lowest level of collection usage mes are oftengrouped into small records where each of the fields contain basic statistics about an atomic usage event such as a singlephone call or an internet data transfer. typical fields that are often found in usage mes are source, destination, usagevolume, start time, and end time. depending on the context and applications involved, a usage me may also includefields such as service type, quality of service level, termination conditions or error codes.in telephony a common usage me is the call detail record (cdr) that is produced by the originating switch andrecords key information about the calling number (source), the called number (destination), and the length of the call inminutes (usage) among other fields. it is from cdrs that telephone companies construct their billing records andperform extensive analysis of subscriber behavior.in the internet context a single me might capture the usage details of a large file download or a small gif imageof a button. because web pages can be containers for references to many other web pages or objects, clicking on a fewpages of a complex web site can result in hundreds of me events. considering this ﬁsessionﬂ of browsing a web siteas roughly comparable to a telephone call, it is easy to see that the number of mes generated will be considerablyhigher than the single cdr produced from a phone call.3.2 session messession mes provide accounting and state information about the user originating aa stream processor for extracting usage intelligence from highmomentum internet data310statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.particular stream of traffic at a point in time. internet session mes are dynamic and usually create an associationbetween an ip address (or cookie) and a responsible account id or subscriber id. depending on the service definition,a session me may also provide information about session state, for example, logged on or off, authorization level,location, and so on. in telephony, the usage information and session information are often combined into the samerecord. in the internet, however, these data are acquired from different sources and must be timecorrelated together innearreal time. sources for session data for internet services include dhcp, dns, and ddns, as well asauthentication, authorization, and accounting (aaa) services such as those provided by radius.3.3 reference datareference data, defined by the nsp, is merged with the realtime streams of incoming mes in order to facilitateadditional downstream processing and analysis. network operational examples include network topological, physical,or routing information (e.g., autonomous system numbers). business examples include subscriber segmentation andclassification information useful by product planning. security examples include thresholds or patterns useful foridentifying abuse, fraud, hostile, or attack traffic.the actual collection and interpretation of mes from real devices is complex because of its diversity and thelegacy of old devices still in use. in the future, this arcane and timeconsuming development process could befacilitated by the adoption of abstract event and services models such as those being developed by jeff meyer (seehttp://www.circumference.org) for the ipdr (http://www.ipdr.org). the proposed model has a simple threelayerstructure. the top layer is the data model, which defines the service or data represented in the me. this is preferably amachinereadable file (e.g., w3c xmlschema), however, for legacy reasons a humanreadable document can do thejob. the middle layer is the data encoding model, which defines how the data are represented as a serialized stream ofbits. the bottom layer is the transport model, which defines how to get the data from point a to point b. the transportmodel is often a hierarchy of protocol layers, but includes concepts of filebased exchange, streaming data, and othertransports.4. data streams and riversthe data streams of the internet are huge. even though usage mes will be a couple of orders of magnitude less,the volume of usage events can still present significant design challenges for general purpose collection systems. wehave begun to characterize these me flow volumes from data that have been shared with us from several nsps. wehave measured me streaming rates of 0.2 to 0.5 mes/subscriber/sec for cisco ios® netflow (http://www.cisco.com/go/netflow) enabled routers. for a moderatesized network supporting 1 million subscribers, 0.3 me/sub/sec representsan input rate of 300k mes/sec. netflow version 5 uses udp packets of 30 me records of 48 bytes each plus a singleheader of 24 bytes. at 50 bytes average per me this is a line speed of about 120mb/s. but stored into a databasea stream processor for extracting usage intelligence from highmomentum internet data311statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.it can represent approximately 4 terabytes per day (allowing 3x for the inefficiencies of relational db storage). thisis assuming, of course, that you are willing to pay for a database that can handle a continuous input stream of 300krecords per second and perform useful analysis work at the same time.figure #. simplistic approaches of store first then analyze later when applied to highmomentum streams can leadto very large storage requirements (high infrastructure costs) and long analysis latencies.how long do you wish to keep these records? the left side of figure 1 is a simple backoftheenvelopecalculation (botec) that computes the database storage requirements as you scale up in number of subscribers and inlength of storage time. the heavier line represents the 1 million subscriber case above. three months of storage at 3million subscribers is already a petabyte!the other consequence of these large datasets is the time required to process them. the chart on the right above isanother botec that illustrates the time it would take to do a single pass of a dataset as a function of the dataset size interabytes and the record processing speed of the database. another way to think about this is to consider the ratio of thecontinuous input record rate to the record processing rate once the data have been stored into the database. if the queryrequires complex processing while it is completing its scan it may not be much faster than the input data rate. at a ratioof 1:1 it will take as long to perform a single pass on the data as it took to capture it. this could be unacceptably longto obtain some of the key results hidden in the data.for highmomentum streams it is common to have hardcoded preprocessors that perform either simpleaggregation or sampling to reduce the data down to a rate that can be absorbed by conventional databases. however,the use of either of these techniques involve making major a priori assumptions about the nature of the data and whatpotential queries will be made on the reduced data.assuming, for a moment, that the nsp can afford the dbms infrastructure required to capture all of this raw data,advanced data reduction techniques have been developed for obtaining quick approximate answers from largedatabases. the paper edited by hellerstein (hellerstein et al. 1997) provides an excellent survey of these techniques,which include singular value decomposition, wavelet, regression, loglinear, clustering, index tree, and sampling. butthe choice of these techniques also heavily depends on the nature of the dataa stream processor for extracting usage intelligence from highmomentum internet data312statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.and queries anticipated. unfortunately, internet data can have a high number of dimensions, the variables can be highlyskewed in both frequency and value, and some of the events or patterns of high interest can be very rare (e.g., a slowaddress scan by a potential intruder). to make matters worse, with the constant evolution of viruses and worms, thepriority of what is important to examine is constantly changing. these complicating factors make the selection of datareduction techniques somewhat of an art form.broadband service providers find themselves between a rock and a hard place. they need much richerinformation about their subscriber usage behavior with strong business rationale on both the revenue and the cost side.the rock is the very high cost of building and managing these large datasets. the hard place is that most generalpurpose data analysis tools presume that the data to be analyzed exists or will exist in a database. no database, noanalysis.what if you could extract some meaningful information about a data stream before you had to aggregate andcommit it to hard storage? this idea, by itself, is not exactly new. but what is needed in a number of these highmomentum, complex data stream situations is a highperformance, flexible, and adaptive stream processing andanalysis platform as a preprocessor to longterm storage and other conventional analysis systems. in this context, highperformance means the ability to collect and process data at speeds much faster (>10x) than most common databasesystems; flexible implies a modular architecture that can be readily configured with new or specialized components asneeds evolve; adaptive implies that certain key components can change their internal logic or rules onthefly. thesechanges could be as a result of a change in the input stream, or a detected change in the reference data from theenvironment, or from an analyst's console. starting in 2000 we set out to build a platform with these goals in mind.the remainder of this article discusses the progress we have made.5. ium highlevel architecturefigure 2 is a highlevel view of the ium architecture. streams of data flow left to right. the purple boxes on theleft represent different sources of raw data within a service provider's network infrastructure. the blue boxes on theright represent the target business applications or processes that require distinctly different algorithms or rule setsapplied to the streams of data. the gold triad of a sphere, rectangular prism, and a cylinder represent a single instanceof an ium server software agent that we call a collector. each collector is capable of merging multiple streams ofinput data and producing multiple output streams, each of which can be processed by a different set of rules.the basic unit of scalability is the collector. the first dimension of scaling is horizontal (actually front to back inthe graphic) in that different input streams can be processed in parallel by different collectors on the left. the seconddimension of scale can be achieved though the processing speed of the hardware hosts. the third dimension of scalecan be achieved by using pipelining techniques that partition the overall processing task for the various targetapplications into smaller sequential tasks that can execute in parallel. thea stream processor for extracting usage intelligence from highmomentum internet data313statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.ium platform has been implemented in java, which enables multiplatform operation. the architecture has beendesigned with high modularity and configurability from the start. upon startup each of the collectors obtains its ownconfiguration from a central configuration server and then builds itself with the proper components required.figure 2. hp's internet usage manager (ium) enhanced with dynamic network analysis is a distributed agentarchitecture.6. stream collection and normalizationthe input streams are captured from the source devices by plugin encapsulator (figure 3) componentsrepresented in two different colors. the gradient shaded purple to gold ones are configured to interpret the data fromspecific source device types. the gold encapsulators are configured to read normalized data.over the past few years we have developed many preconfigured encapsulators for a wide range of devicesmentioned above as well as different collection modes that include realtime streams, (e.g., netflow, sflow, ddns),polled data (e.g., snmp), files and directories, and databases (via jdbc).figure 3. encapsulation plugins are tailored to collect different types of input streams.a stream processor for extracting usage intelligence from highmomentum internet data314statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.figure 4. the events of the stream are converted into normalized metered events and passed directly to a ruleengine.once collected, the data of an me are normalized into a common data structure called a normalized meteredevent (nme, see figure 4), which is an array of attributes that contain different data types similar to a db record. theonly attribute that is required is an endtime stamp. all the other attributes can be configured to suit the processingneeds of the application. unlike db record schemas the number and type of attributes can change dynamically as itundergoes processing. in this stream processing context new attributes can be dynamically ﬁadornedﬂ to the nme andbe used as intermediate variables, which travel along with the nme in the stream, and then disposed of when no longerneeded. (the word ﬁtravelﬂ is only a metaphor. the nme object doesn't actually move; its position in the stream istracked by passing a small reference or ﬁpointerﬂ to the nme object from rule to rule.)7. stream rule processingonce normalized, the nmes move directly into a rule engine, which has been specifically designed for merging,processing, and splitting streams.the input streams can be independent or related (such as a usage stream and a session stream). in the rule engine(figure 5) there can be multiple rule chains that operate on the input streams. it is possible to have a single streamprocessed by multiple rule chains each producing distinctly different output streams, or multiple similar streamsprocessed by a single rule chain, or combinations of the above.there are several forms that the output streams can take: (1) during processing of an nme, attributes within thenme are added, modified, or deleted and the result nme is forwarded immediately to a downstream collector forfurther processing. (2) a cyclic aggregation time interval is configured into the collector and rule chains areconfigured toa stream processor for extracting usage intelligence from highmomentum internet data315statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.produce aggregates of particular attributes. at the end of the aggregation time interval the aggregates are flushed eitherto a persistent store for recovery operations, or the aggregates can be fed directly to a following collector for furtherprocessing (or both). (3) certain rules allow a real time query of current results. this is particularly valuable to gaininsight into ongoing statistics. for example, examining the current empirical distribution of a variable. this kind offunctionality is enabled with dna discussed later on.figure 5. the rule engine can be viewed as an inline agent that performs the work of merging, analyzing, andsplitting streams. the output streams of a rule engine use the same nme structure, which enables the creation of afabric of chained stream processors.8. rule chains and associated data structuresa rule chain is sequence of configured operations on the flowing nmes of an input stream. example rules includestandard threeaddress arithmetic and logical operators of the form op(a1, a2, a3) where the first two parametersrepresent nme attributes as operands, and the third attribute is the result. flow control rules include conditionals (e.g., ifthenelse) that can change the path of the rules based on the nme attributes, but no looping rulesšnot a good idea ina stream! in addition we have developed a rich set of lookup rules, filter rules, adornment rules, and other special rulesfor more complex operations. the rules are written in java and there is a developer's kit that enables users to createtheir own rules. however, the current rule library is extensive with more than 100 rules, which is satisfactory for mostapplications.the right side of figure 6 illustrates a simple rule chain on the left with a single ifthenelse rule followed bysome sequential operations.although the processing logic applied to an nme as it flows through is contained in the rule, there is also areference to a data structure that travels with the nme and can also be changed by the action of the rule logic. thisdata structure can hold state information that can interact with subsequent nmes traveling through the rule engineand can be flushed to a datastore at regular intervals. this parallel data structure is usually organized as a tree shownon the right.hash rules are an example of the special rules that operate on the references to the data tree. each node of a levelof the tree associated with a hash rule can contain a hash table (cormen, leiserson, and rivest 1999; knuth 1973)where the entries contain a key and a pointer to a child node. a hash rule is configured to use an attribute of the nmeas a key and then either chooses the successor data node based on the result of the hash if it exists, or create one if itdoes not exist. each data node can be an nway branch (a binary tree316about this pdf file: this new digital representation of the original work has been recomposed from xml files created from the original paper book, not from the original typesetting files. page breaks are trueto the original; line lengths, word breaks, heading styles, and other typesettingspecific formatting, however, cannot be retained, and some typographic errors may have been accidentally inserted. pleaseuse the print version of this publication as the authoritative version for attribution.a stream processor for extracting usage intelligence from highmomentum internet datastatistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.structure is used in the diagram for graphical simplicity). new nodes of the tree get created as new values appear in thedata. if the last rule in the chain is configured as an aggregation rule, and the collector is configured to flush the leafnodes flushed at periodic intervals, the resulting dataset would be the same as an sql aggregating groupby operationexcept the grouping (essentially routing) occurs as the nmes flow, not as a batch operation. the following tablecontrasts the stream processor rules that produce the same operation as the batch sql statement on the right.figure 6. a simplified view of how rule chains can interact with dynamic data structures. a single rule engine canhave multiple of these rule chains in various series and parallel configurations.stream processor rulesbatch sql statement(in a specific rule chain context)select srcip, dstip, sum(usage)hash srcip, dstip;from <table>aggregate sum(usage);group by srcip, dstipbecause there can be multiple rule chains, as explained earlier, the specific rule chain where the hash rules appearis analogous to the ﬁfrom <table>ﬂ clause in sql. however, in a stream processing context we have the opportunity toperform operations that would be much more cumbersome in an sql environment. this will be further discussed insection 9.3an nlevel hash in this structure is analogous to a hypercube in the sense that the leaf nodes of the tree can bemapped to coordinates of an ndimensional hypercube. however, there are differences. one is that the tree structurecan be sparse in that nodes are created only when the combination of data actually exists in the input stream. anotherdifference is that the cell of a hypercube usually contains only one value. in this data structure the nodes arecontainers that can contain their own complex data structures internally.a stream processor for extracting usage intelligence from highmomentum internet data317statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.figure 7. capture models are sophisticated rules that can perform complex, binning, filtering, and associativeprocessing. these capture models can be dynamically configured to examine different statistical properties of theincoming stream.the tree structure can be also used to create a result stream that is an associative merge of two different inputstreams. suppose we have two streams a and b. for stream a we mark each nme so that instead of traveling throughthe entire tree an anme travels to the coordinate node of the tree specified by a's hash attribute values and drops offits associative attributes values at that node, which are stored there until they are replaced with more current data forthat coordinate, again from stream a. stream b is processed as discussed earlier. as a bnme travels through the treeit is routed to the coordinate node specified by b's hash attribute values. as it passes by, the bnme picks up theassociative data, which was previously dropped by the anme. in this way, a multidimensional set of associationsbetween streams can be performed in realtime.other forms of associations can be performed by lookup rules, which are designed to perform fast specializedlookup algorithms for associations with reference data. an example of this is finding the longest qualified prefix of anip address given a reference routing table.9. statistics from streamsdna builds on the platform discussed above and extends the stream processing capabilities in several ways.9.1 capture modelsas the stream of nmes pass through a node in the tree above it is possible to collect richer statistics as well(figure 7). the motivations are several, but a capture model of a few kb can extract selected characteristics of a largestream very economically.capture models (or just models) are similar to rules. models are contained in a special modeling rule that actsas a manager and container for multiple models. when a modeling rule is inserted into a rule chain it will spawncapture models into the associated data nodes of the tree as they are created. consider a node of the hash tree as arepresentation of the intersection of a set of business coordinates such as customer, service, and geography. eacha stream processor for extracting usage intelligence from highmomentum internet data318statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.node can contain multiple capture models, which can collect different views of the data passing by. one capturemodel might be an adaptive histogram on one variable, another could be a topn model of a different variable fromthe stream.one way to think of a capture model is that its input is a stream of vectors (nmes) and its output can be a matrixof values defined by the capture model:capture models can be configured with an integration interval (minutes to days) that defines the amount of timethat statistics are collected. at the end of the integration interval, the result matrix is usually flushed to a persistentstore or to another downstream rule engine.there is no fundamental restriction on how a capture model is designed as the output can be any data structurethat can be contained in a java object. for example, creating a correlation matrix model would be relativelystraightforward. defining conventions, like the matrix form above, has allowed us to create additional functionalitysuch as model aggregation mentioned in the following.the most common capture models include log distributions, linear distributions, topn, history (time series), andother specialty models for security and capacity planning flow analysis.as an example, the distribution capture model performs dynamic binning on the values that fly by for aconfigured attribute. for improved accuracy, particularly for rare events, the model defines two vector variables, sumand hits, both of which are dimensioned by the number of bins. the order of the above matrix becomes n×2, where nrepresents the current number of bins. the bins need not be contiguous and are only created based on actual datavalues that appear in the stream.the result of the distribution capture model, when queried, is again an nme. the first several attributes define thecoordinates of the model and then a single object attribute that is a compact form of the empirical distribution of thevariable. this result nme can be output either at flush time of the aggregation tree or obtained by a real time queryfrom the dna client application.a stream processor for extracting usage intelligence from highmomentum internet data319statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.9.2 capture model aggregationa large stream with a lot of variables can create a lot of models based on how you choose to configure the dnacollector. a single distribution model consumes about 2kb with 100 bins. to monitor bandwidth distributioncharacteristics of stream flows at 10,000 points in your network amounts to only 20mb of memory, but how does oneexamine 10,000 distributions, (or 100,000 distributions)? this leads to the concept of model aggregation.returning to the navigation discussion, the tree structure could be leveraged again and produce one type of modelaggregation where capture models would reside at each of the interior nodes of the tree in addition to the leaf nodes.these interior models create a hierarchy of models where an interior model in an upper level of the tree represents theaggregate statistics of all the child nodes below it. order is important, however. using the "*" to represent theaggregation of all the coordinate values for a dimension you could create navigation coordinates like (al.*.*.*),(a1.a2.*.*), or (a1.a2.a3.*), where the coordinates are in topdown order. this does not allow aggregations of the form(*, a2, a3, a4), which diminishes this strategy's usefulness. instead we have provided an internal query capabilitywithin the dna collector server that can traverse the inmemory tree and collect data from nodes with an arbitraryquery of the form (*, a2 op x, *, a4 op z), where op is a qualifying operator.so far these aggregations have been inside a particular collector. a large deployment may have hundreds ofcollector agents widely distributed geographically. the second mechanism we have developed for model aggregationallows the model data from widely dispersed dna collectors to be merged as long as some basic rules are followed.the ability of a model to be aggregated with other models depends on the definition of the model. history models anddistribution models can be combined as long as the data was collected during the same aggregation time interval andthe events of the different models are independent. an example is two sets of subscriber usage distributions collectedper hour in san francisco and los angeles. as long as the subscribers generating traffic in san francisco are not thesame subscribers generating traffic in los angeles and both datasets are from the same day and hour of the day thedistributions can be aggregated. to facilitate this kind of model aggregation, the above dimensions of statistics datacollection are marked with an independence parameter. this simple facility protects the user from accidentally creatingmodel aggregations that would be meaningless.9.3 drill forwardone of the important capabilities of these capture models is that they can be dynamically configured by the user,or some other agent, including the type of model and all of its configuration parameters. this leads to an importantconcept in stream analysis i call drill forward.most of us are familiar with the concept of drill down when dealing with multidimensional online analyticalprocessing (molap) or relational online analytical processing (rolap) analysis systems. clicking on a bar of a barchart creates a new window of the historical detail displaying the next level deeper components that made up theselected bar.a stream processor for extracting usage intelligence from highmomentum internet data320statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.note the word historical, because the presumption of drill down is that there is a database of history behind the datayou see. unfortunately, constructing such a history of data for massive data streams may not be economically practical,or take too long for the reasons discussed previously.drill forward is simply a different name for what we all do when something draws our attention. we focus in andlook more closely, discarding a vast majority of the other data pummeling our senses. but we are moving forward intime not backward. when we are dealing with massive data streams, the same technique can be used to investigatepatterns.in a streamprocessing context, a few key variables could be monitored to establish normative behavior. if there isa sudden change (exceeding a percentile threshold or the change in shape of a distribution, etc.) the rule logic could bedynamically restructured to collect more detailed data about a reduced, but focused subset of the stream where theexception occurred. for example, the appearance of certain traffic patterns may be a precursor to a hostile attack on thenetwork. if this particular pattern occurs, it is desirable to collect additional detail on that substream. a simple exampleof this can be accomplished with a conditional hash rule, which is a variation of the hash rule above:in this example, if a single event flowing to (or from) a particular ip address shows traffic activity on one of a listof trojan ports, a flag triggers aggregation of traffic by port in addition to aggregation by ip address. once this porthas been hashed into the table data continues to be collected for this port for a defined interval of time because italready exists in the hash table. this avoids having to collect high granularity data all the time for all substreamsresulting in significant data reduction and efficient processing of these data in downstream systems.as another example, assume a capture model has been configured to measure the distribution of the number ofunique destination addresses per subscriber for outgoing traffic on a routine basis. a large spike of activity at the 99thpercentile may signal a subscriber performing address scans on the network. based on this abnormal event the capturemodels can be reconfigured with filters to focus in on only the portion of the distribution where the spike occurs, thenstart exporting additional information about the suspect traffic such as protocol and destination port, which will helpidentify the type of traffic. the ability to establish normative distributions of various characteristics of a stream andthen dynamically explore deviations from the norms adds considerable analysis capability. this technique is ideal fordetecting and exploring patterns in a stream, but not for discovering onceinalifetime events.a stream processor for extracting usage intelligence from highmomentum internet data321statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.figure 8. capture models can be configured for realtime queries, which enable interactive snapshot views of thestatistical data captured in memory. the above screenshot reveals the lognormal distribution of subscriber usage.9.4 user interaction with streaming modelsthe collection and processing of these streams forms the foundation, but users need graphical and visual tools forexploring this space. wilkinson (1999) has done some extraordinary work in this area. this is a challenging area in itsown right and where we will be investing more r&d going forward. the dna technology suite includes both abrowserbased client and a java application client for more sophisticated viewing and analysis.figure 8 is a real data example of the analysis screen examining a subscriber usage distribution. this kind of datacan be pulled up from a dna server using the realtime query mechanism mentioned earlier.what is interesting is that this usage distribution follows a lognormal distribution over five orders of magnitude(90kb/mo to 22gb/mo) with a shape factor of ~0.67.transforming this into a cdf is trivial (figure 9, top), which gives marketing folks information on how tosegment their subscribers based on usage. the graph on the bottom is a percentilepercentile plot of percent subscribersusing what percent of the overall traffic. this graph shows that this distribution follows the 80:20 rule, the top 20% ofsubscribers generate 80% of the traffic. the top 5% generate 50% of all traffic!to demonstrate how capturing statistics from a stream can generate valuable businessa stream processor for extracting usage intelligence from highmomentum internet data322statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.insight, figure 10 is from the dna financial modeling tool that uses empirical distribution data collected from thedna server to compute the estimated dollar value of subscriber traffic modeling different pricing scheme scenarios.givenb=bytes of usage per months(b)=density function: # subscribers at b$(b)=pricing function: $ paid by asubscriber with total usage b for the month.the revenue in dollars for all subscribers with monthly usage between b0 and b1 isfigure 9. from the empirical distribution, multiple parameters can be derived and various transforms applied.a stream processor for extracting usage intelligence from highmomentum internet data323statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.figure #0. certain types of traditionally tedious computations can be performed very quickly once the underlyingempirical distribution of the data is known. this tool takes advantage of that fact for performing interactivefinancial analysis of the value (in currency) of a stream based on pricing models input by the user. the empiricaldistributions can either be extracted in realtime or archived for later comparison and analysis.because of the compactness of the models this kind of computation can be performed by the client in a fewmilliseconds, which enables ﬁwhatifﬂ modeling based on actual or forecastextended distribution models of subscriberusage behavior.other tools currently in development include network analysis and forecasting for capacity planning as well as asuite of security analysis tools. a more complete discussion of how these more advanced tools take advantage ofstreaming analysis will be the subject of followon papers.10. summaryhighmomentum data streams and rivers can be expensive to store and require long processing times to analyzeusing the traditional store first, then analyze later techniques. although some types of analysis will always require thistimeproven approach, we are discovering that a great deal of valuable insight can be extracted from these streamsprior a stream processor for extracting usage intelligence from highmomentum internet data324statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.to other datareduction processes and commitment to storage and yield significant cost and latency reductions as well.key paradigm shifts that we have had to make (and are still making) in our own thinking have been in areas such asdynamic interaction with streamoriented programming languages, distributed stream processing architectures andvisualization of streams.acknowledgmentsi would like to thank some key individuals who have supported me and contributed to this program: lelandwilkinson, senior vp, spss, who has personally given me strong encouragement to get this material published; ericbuatois, hp vp, who helped fund early research on these concepts; jeff meyer, hp chief architect for ium, who hasbeen the thought leader and creator of many of the key concepts of the ium platform; ying he, hp softwaredeveloper, who has always been open to changes and yet more changes and has contributed extensively to the dnaserver architecture; eric peterson, hp software developer, a great communicator and developer who is primarilyresponsible for the dna frontend architecture; scott lamons, hp r&d project manager, who has been a tremendousasset to the smooth workings of our team and a strong supporter of the program. cisco ios® netflow is a patentedtechnology of cisco systems, inc. (http://www.cisco.com). sflow® is a registered mark of inmon corporation (http://www.inmon.com).[received april 2003. revised october 2003.]referencescormen, t.h., leiserson, c.e., and rivest, r.l. (1999), introduction to algorithms, cambridge, ma: the mit press.hellerstein, j.m., et al. (1997), ﬁthe new jersey data reduction report,ﬂ bulletin of the ieee computer society technical committee ondata engineering, 20, 4.knuth, d.e. (1973), the art of computer programming (vol. 3), reading, ma: addisonwesley.mcgarty, t.p. (2002), the imminent collapse of the telecommunications industry, the merton group, http://www.mertongroup.com/collapse%20of%20telecom%2002.pdf.sidak, j.g. (2003), ﬁthe failure of good intentions: the worldcom fraud and the collapse of american telecommunications afterderegulation,ﬂ yale journal on regulation, http://www.aei.org/doclib/ 20030403ssrnid335180code021001500.pdf.wilkinson, l. (1999), the grammar of graphics, new york: springerverlag.a stream processor for extracting usage intelligence from highmomentum internet data325statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.pedro domingosa general framework for mining massive data streamstranscript of presentation and pdf slidestechnical paperbiosketch: pedro domingos is a professor in the department of computer science and engineering at theuniversity of washington. he received a master's degree in electrical engineering and computer science in 1992 fromthe institute superior técnica (ist) in lisbon and a second master's degree in 1994 and a phd in 1997 in informationand computer science from the university of california at irvine. he spent 2 years as an assistant professor at istbefore joining the faculty of the university of washington in 1999.dr. domingos is the author or coauthor of over 100 technical publications in topics related to machine learningand data mining. he is also the associate editor of jair, a member of the editorial board of machine learning, and acofounder of the international machine learning society. dr. domingos was program cochair of kdd2003, and hasserved on the program committees of american association for artificial intelligence (aaai), internationalconference on machine learning (icml), international joint conferences on artificial intelligence (ijcai),knowledge discovery and data mining (kdd), the world wide web consortium (www), and others. he hasreceived an nsf career award, a sloan fellowship, a fulbright scholarship, an ibm faculty award, two bestpaper awards at kdd, and other distinctions.a general framework for mining massive data streams326statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.transcript of presentationmr. domingos: my talk is about a general framework for mining data streams. this is joint work that i havedone with jeff hockney at the department of nuclear science and engineering at the university of washington.so, this talk is basically about building things like classification models, regression models, public informationmodels and messages.here is what i am going to do. first, i am going to describe what the problem is that we are trying to solve. then iam going to present the general framework we have for solving this problem. then i will describe an exampleapplication of this framework. [comments off microphone]. then i will conclude with some comments.a general framework for mining massive data streams327statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.so, what is the problem we are trying to solve? a lot of the classification questions and so forth, algorithms, atleast when most of them were developed, you know, the number of data points wasn't very large.over the last 10 years, people have made a lot of progress in the sense that there is a lot of hard workš [offmicrophone] šdecision tree learners on data sets. i think there is great achievement, several orders of magnitude. thespeed at which the data rates are going up actually exceeds the speed at which we are speeding up our algorithms. so,we are losing the rates. i have a few random examples that i probably don't need to go through because people alreadyknow most of them. the bottom line is that, in most domains, in any given day, you can easily collect tens of millionsor hundreds of millions of records.for example, if you want to do a decision tree, doing a decision tree on, say, 20 million records, even with today'sbest algorithms, takes more than a day. so, in spite of all the progress that we have made, we are actually losing therace. the fraction of the available data that we actually use to build our models is actually dwindling to zero as timegoes forward. so, there is something wrong here. we need to do something about it. the thing that we need to doabout it is one of mining databases to one of mining data streams.you know, databases and data streams are different in many respect, but the idea that i have in mind here is thatthe database can be very large, but it is of a fixed size. at the end of the day, you know that you have so many records,you have so many samples that you can learn from. in the data scheme, it is open ended. in essence, we have infinitedata. how would we modify any of the algorithms if we actually had internet data available, because that is what wehave in the data stream. we get another 100 million, and another 100 million records. so, we need to change ourmodel of what we are doing from databases to data streams. what i am going to describe here is a framework toaddress these decisions. so, what are they?a general framework for mining massive data streams328statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.first of all, our algorithms need to require only a small amount of time per record. if the time requirement goes upwith the number of data points that you have in your past, then you are lost. sooner or later, you run out of breath.also, we need to be looking more at statistics than main memory. clearly, storing all the data in a main memory is notan option.we want to do a scan of the data. we can't assume that you are going to be able to store your data and go backand look at it. so, we want to be able to do everything we want, by looking at each data point, at most, only once.notice i say at most. maybe we can do things in even less than one scan of the data, and that is part of what i am goingto be talking about here.we also want the net results available at any time. again, the traditional model is that you collect your data andthen you run your algorithms on it and then, at some point in the future, your algorithm isn't running and you haveyour model. that isn't going to work here, because you have to wait forever. so, you want to have a model that getsbetter and better as time goes by but, at any given point, you can push the button and see what you already have, giventhe data that you already looked at.another very important thing is the following. we would like to ensure the results that we get are, impossible,equivalent to what you would get if you were actually just running your regular algorithm on a regular, but infinitesized database with a computer with infinite resources. it is very easy to satisfy those requirements if you compromisethe quality of the results that you are producing. the whole challenge is to actually guarantee those things whileproducing, say, decision trees or things that are not different from the ones that you get if you ran the algorithms thatwe know, and that we have, and whose properties we know.finally, we also want to be able to handle time changing phenomena. in a typical database, we just assume thatthe data is iid, so it doesn't matter what order they are in. in large data streams that last over months or years, veryoften, the phenomenon that you are looking at is actually changing over time. so, the model that you ran a year agoisn't necessarily valid now, and you also want to be able to deal with that.so, these are the criteria that we want to satisfy. in fact, when we started doing this work a couple of years ago, itsort of sounded overly ambitious. where are we going to have to compromise? however, to our amazement, we foundthat we were actually able to satisfy all those criteria. in fact, what we now have is, we have a framework that meets allthose criteria, and we have successfully applied it to several algorithms, including decision tree learning, networklearning, image clustering.a general framework for mining massive data streams329statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.we are able, using this, to deal with millions of examples a day using existing hardware. for example, the bestdecision trees we were able to mine perhaps a million examples per day, and now we are able to mine on the order of abillion examples per day.what we are doing now is, we are developing the general purpose library such that, if you write your algorithmsusing this library, you won't have to worry about scalability. you can just write them the same way as you alwayswrote them, and they will run on data streams without your having to focus on it. you just focus on the problem thatyou are trying to solve. so, this is not available yet, but it is like our next target that we are dealing with. so, i am notgoing to have time in this halfhour to actually talk about how we meet each of those problems. i will just focus onethe most salient ones. those are the problems with trying to ensure that you learn the same model, in real time, asquickly as possible, that you would run on infinite data, and a little bit at the end about what happens when the datagenerating process is not sufficient.a general framework for mining massive data streams330statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.what do i mean by discretion and how much data is enough? in a traditional learning algorithm, what you do is,the algorithm is a sequence of steps of some kind, and each of these steps look like the data in some way. think ofyour favorite algorithm and this is how they work. in the traditional setting, each of these steps, in general, looks at thewhole data before doing what it wants to do. now, in a data stream setting, that is not going to work because youwould never get past the first step, because you would wait forever for all your data to arrive.so, what you need to do is, you need to make the decision about how much data to use in each step. in essence,you want to use as little data as possible in each step, so that your model gets better as quickly as possible. i want tohave the best possible model as soon as possible, and this means minimizing the amount of data that i am using at eachstep.now, if i reduce the amount that i use at each step, i probably will pay a price for that in terms of the quality ofthe model that i get. so, the goal of what we are doing here, in a sense, one of the basic features of our framework, itenables you to minimize the amount of time that it takes to learn on the data stream, while guaranteeing that you arestill getting the same result that you would get if you waited for all the data up to eternity to arrive.some of you may be wondering, well, how can that be possible? to persuade you that that can be possible, iwould like to give you the basic idea of what we are doing. think about the simplest possible model that anybodywould want to build. say you just want to know what the average of some quantity is. what is the numberšforexample, in the network application, i want to know what the average number of packets is that you get from somesource, or whatever you need to find. suppose that i have, say, a billion samples of this. i don't necessarily need tolook at that billion, if i assume that they are iid, to form a good estimate of the mean. this is what pollsters do whenthey use 3,000 people as a sample for, say, 200 million. i mean, there are many kinds of results that you can use todecide how much data you need. one that we have used a lot is a hooking balance, and what happens in a hookingbalance is this. suppose we use variable x and range r. then we have n independent observations.what this guarantees is that, with a probability of 1!,, the variable is within + of x, where + is given by thisexpression. it depends on the range. it goes down with the number of data points that you have, and it actually onlygoes up with the log of one over the area. so, what this formula does is that, suppose you want to find the mean of somea general framework for mining massive data streams331statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.variable, but you are willing to have that mean be off by +, by at most +, with the probability at most ,. then, i will tellyou how many data points you need to gather. then, after that, you don't have to worry. it doesn't matter if you have atrillion points or infinite points. you don't need to look at them, because you already have the quantity that you wantwith the tolerance that you want. so, the + and , are the quality parameters that you have given.in essence, all we are doing, at least in the basis of our framework, all that we are doing is this. we arebootstrapping this idea to not just one quantity, but to a whole model, for example, a whole density estimation modelwith all of its parameters, or a whole decision tree with all of its nodes.so, we take these kinds of things for the individual things that you are doing and, by doing some analysis, youactually come up with a guarantee on the quality of the whole model.so, in summarizing one slide, here is the approach that we have. it goes in three steps. the first step is that youhave an upper bound on the time complexity of your algorithm, on how long it takes to run, as a function of thenumber of examples that you use in each step.in many cases, it is just the sum of the number of examples that you use in each step, multiplied by something thatis a constant. this is actually quite easy to do. so, we figure out how the running time of the algorithm varies with thenumber of examples that you use at each step.part two, we divide the upper bound on the loss between the model you get with finite data and the model thatyou would get with infinite data, as a function of the number of examples that you use when you are doing this withfinite data. so, you give me your loss function, the loss function that you are going to use to compare to differentmodels. so, if two models are very different by your loss function, then you know your loss function should have alarge value. this could be easier or harder, depending on the algorithm of lost function. we figure out how this lossvaries when you are comparing what you would get with infinite data, with what you are getting using a finite numberof examples.the final step is to minimize this. so, to minimize this, subject to the user, given constraints on this, meaning thatwe have a constrained optimization problem. what we are trying to do is to minimize the running time of the algorithmas a function of the number of examples we can expect, subject to the constraint that the loss that you get ata general framework for mining massive data streams332statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.the end of the day, the loss between finite and infinite data, is at the most +, with the probability of at least 1!l. so, thisis the general approach that we have.one of the surprises that we found is that we are actually able to do this for very broad classes of algorithm. westudied industry decision trees, and then we did it for clustering, and generally we saw that we could generalize this tomany different things.notice that, in some sense, what we are effectively doing with this is learning from infinite data in finite time,because we are getting, in finite time, the same results with the same tolerance that you would get if you waitedforever, until your data stream gave you infinite data.so, that is our general idea. there are lots of different methods that i am not talking about here, but that is thebasic idea of our framework. let's see how it applies to the case of decision tree induction, which is actually aparticularly simple one, perhaps in some ways not the most interesting one, but it is the first one that we did, and theeasiest one to talk about.so, in decision tree induction, what happens with the time? remember, the first step was to figure out how thetime varies within the examples that are used in each step.in the decision tree induction, which i assume most people here are familiar with, what happens is that you have aseries of nodes. each node tests the value of some attribute. depending on that value, it sends it to another node, and atthe least you have a classification.the basic problem in decision tree induction is deciding which tests, which attributes you pick to test in each node.the amount of time that we need to do that is proportional to the number of examples that you have. what youbasically need to do is, you need to gather some sufficient statistics that have to do with how often each value of eachattribute goes with each class.audience: is the number of nodes fixed?mr. domingos: no, the number of nodes is not fixed. so, the first thing we are going to have to do with ouralgorithm is that potentially, as time goes to infinity, the size of your tree could grow infinitely as well. in practice, itusually doesn't, but in principle, it could. so, when you have a model that is of unbounded size, potentially, the modeltakes an infinite amount of time to learn.what we are concerned with is what is the time that each step takes, and we are going to try to minimize, in thiscase, the sum of those times, which means that you runa general framework for mining massive data streams333statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.the model faster.so, notice, traditionally, and even in some of the very fast optimized algorithms for learning decision trees onvery large databases, they use all the data to take each test. if you unleash those algorithms on a data stream, in somesense, they never even pick, because they keep waiting for the rest of the data to come up, before they make their firstdecision. intuitively, you know, we should be able to make that decision after we see so many examples, and then goon to the next node, and that is exactly what we did.so, what about the lost function? how are we going to compare the decision tree we are building with finite datawith the decision tree that we would build with infinite data. you can do this in many ways. we are actually going touse a very stringent criteria which is, i am going to take the loss as being the probability that the decision trees disagreeon the random example. so, this is much more demanding than just we find that the tree be at least nearly as accurateas the other tree, or that they make nearly the same decisions.for the two trees to be an example, i am going to require that, from the point of view of that example, the twotrees are indistinguishable, meaning the examples see the exact same sequence of tests, and winds up with the sameclass prediction at the end of the day. so, this is going to be my loss function.audience: it doesn't seem that the tests matter, if it gets to the same answer.mr. domingos: maybe it does, maybe it doesn't. for some applications, it matters. the point is that, once youget guarantees on this loss function, you get guarantees on all the others, since we can do this at no extra cost.so, the final step, of course, is to try to minimize the limits on this loss. without going over a lot of details, whatis going to happen here is, what we are going to do, we are going to use the minimum number of examples to fit eachtest at each node such that, at the end of the day, the probability of disagreement is going to be bounded, and we aregoing to see the algorithm, and then we are going to see one example of the kinds of guarantees that we can getthrough the algorithm.so, here is our algorithm for mining massive data streams. we call it drpt. again, at a very high level, it hastwo arguments. the stream, which is an infinite sequence of examples, and , is the limits that we want to impose onthe disagreements between the two trees. we want to learn on that stream as fast as we can, subject to the concerns thatthe disagreement between that tree and what we would have with infinite data is, at most, ,.i beg your pardon, this , is actually not the disagreement between the whole trees. we are going to call that .this is the probability of getting each test wrong, and we are going to see how the two relate. it is the probability ofactually making the wrong decision on any given step.a general framework for mining massive data streams334statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.so, how does our algorithm work? we start with a tree that just has the roots. we initialize to suggest the count ofthe number of times that each value generated with each attribute i occurs with each class, k.now, for each example, x is the attributes and y is the class. we sort that example through a leaf. we are goingto have a partial tree at any given point. then we update the concept technique. then, the information gain on a linearor any other matters, we compute it for each attribute. then, if the data of the best attribute is better than the data of thesecond attribute by at least epsilon, when epsilon is a function of this ,, then at this point we know that, withprobability 1!,, we are picking the same attribute that we would be picking if we waited until we saw infinite data. so,at that point, we just split on that leaf, and now we start sending the new examples in the stream down to the childrenof that leaf.so, we start with the root. after some of our examples, we go through the roots to the leaves, and then theexamples get routed to its children, and then we collect examples of those guys and at some point we pick the test data,and we keep building it in this way.so, notice, if you think about it for a second, this satisfies everything that we were talking about. it does requirean amount of memory that is independent of a number of examples that you have seen and so forth. it is any time,because at any time you have a partial tree built and so forth. we have a few questions here.audience: [question off microphone.]mr. domingos: no, we deal with continuous inputs. in the particular network we are talking about here, weactuallyš [off microphone.] again, we can deal with continuous attributes.audience: [comments off microphone] šat many levels?mr. domingos: yes. that is actually not a problem with the levels that you have in the attributes. you justhave more statistics to start with.audience: it appears to be a greedy treebuilding algorithm. from that standpoint, it would perhaps lead to aninferior tree when compared to one that is pruned back with something else.mr. domingos: the basic algorithm that we are trying to use isn't really a decision tree algorithm. so, we aredoing the same thing. [comments off microphone.] you can stream data forward. so, this is the basic algorithm. themain claim that i made to you is that, running a decision tree in this way, in some sense, you will learn it in as littletime as you can.what you will get is a tree that disagrees very little with the tree that you get ona general framework for mining massive data streams335statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.infinite data. well, how come i can make that claim? there are actually various kinds of results that we can show here.the one that i am going to show you is select a better bound. to get that better bound, we have to make oneassumption about the data, which is almost always true, and that is why i am presenting it here.we can also get results without making that assumption. the assumption i am going to make is that some fractionof your data, at any given level of the tree, winds up in a leaf. again, in all the dozens of domains that i have applieddecision trees, i have never seen one where this doesn't happen. what this means is that the tree is somewhatunbalanced. you don't have all your leaves at the last level. if that happens, we will have to use a different kind ofguarantee than the one i am going to give here.again, for purposes of analysis, i am just going to assume that that probability is the same at every level, which itdoesn't need to be, and i am going to call it p, the leaf probability.you will recall that in our last measure there was some disagreement between some trees, and that i define as theprobability, that the path of the example through the first tree differs from the path that the example takes in the second.the two trees that i am going to compare is the decision tree that i learned using the algorithm that i just saw,with parameter ,, and the decision tree that i would learn in batch mode that i would learn using an ordinary decisiontree algorithm, with infinite memory and infinite time, waiting to see infinite samples.here is what we can guarantee. we can guarantee that the expected disagreement between the two trees is boundby ,/p. so, the , number is the value that we use at each node. i guess what this means is, if i get a smaller ,, call likethis , the one that you guarantee. you know the leaf probability. then immediately you know which lowercase , youneed to use at each local decision to get the result you want.obviously, the lower ,, the smaller , gets, the lower the leaf gets. not surprisingly, the lower the leaf probability,the bigger the tree you are going to grow. so, the smaller it has to be to get to the big ,.audience: it is noteworthy that ht, is also on an infinite sequence.mr. domingos: i will get to that in a second. actually, i will get to that right now. one correlate of thistheorem is that, on finite data, what our tree is going to be is a subtree. that is the best you could ever hope for, again,with the same probabilistic guarantee. then, it is like a twostep process to see how that correlator comes about.without actually proving the theorem here, it is actually quite easy to give youa general framework for mining massive data streams336statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.some idea of the basic intuition as to why we are able to have a guarantee like that.the reason is actually just the following. notice that if you have an example going down a certain path in the twotrees, and the probability of that example bumping into another one that is different is bounded by , because wedesigned it that way.so, if the path is of a certain length, l, then by the union of the probability that the data disagrees anywhere in thepath is, at most, , times l. this is, in fact, a loose bound. so, this is the problem when you get two paths of l data.now, the other side of this is, if we have a leaf probability of p, then the probability that a random example falls into aleaf at level i is the probability that it doesn't fall into a leaf at any of the p levels, so it is (1!p)i!1p, which is theprobability that it falls into a leaf at this level.although we need to consider computer expected disagreement, this is just the sum of the probability of these twoguys over all possible levels. the expected disagreement is the sum of all possible levels of the probability that theexample falls into that level, and the expected disagreements for that level.so, if you take this expression and you sum it, you just come out with the , that we have. like i said, there is thisnice corollary that applies to, in finite time, basically what you get is a subtree of the decision tree that you would getwith infinite data, again, with at most this disagreement.the bottom line here, though, is the following, that people in competition line theory have a lot of bounds thathave this flavor, but they are incredibly loose. they are nice for intuition but, from a practical point of view, they areuseless.a general framework for mining massive data streams337statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.the results that we are getting here is very different, because we are not looking at the size of the models toconsider. we are actually only looking at the concrete number of decisions that you make. the really nice thing, is thatthe number of examples that you need actually only goes up logarithmically with a , that you want to assure.putting it another way, as the number of examples grows linearly with time as you see more of the stream, yourguarantees get exponentially better. the fact that your guarantees get exponentially better means that, with realisticnumbers of examples, we can get disagreements that are extremely low. so, this is just an example to make things easy.let's say that your leaf probability is 1 percent, which basically means that your tree is huge, because only 1percent of your examples runs into a leaf at any given level. so, this is a very large tree. let's suppose that you have725 examples per node. it is not a very large number, if you think of the data stream of millions of numbers coming atyou. this is enough to guarantee disagreement of less than 1 in 10,000, .01 percent.so, here is one graph of results out of the many that we have produced, just to give you a flavor of what we areable to do. this is the fpt framework, 2.5 billion examples. using an ordinary pc, it took us a couple of days to dothis. in fact, the time to learn is completely dwarfed by the time just required to read the data from this.so, the best application of this algorithm is actually not unlike the kind of stuff that people have been talkingabout all day today, where you never even study an example. the algorithm actually takes an order of magnitude lesstime to run than it actually takes to just read them from this.so, we see a standard decisiontreerunning algorithm, and this is it running on the maximum number of examplesthat we could store in memory, which was 100,000. it had about 66 percent accuracy. without going into details of thedata set, it was composed of 100 attributes. so, this is a 100dimensional data set.what we see here is that the fdt, it is able to expecting more and more from the data right up to 100 millionexamples. actually, between there and the 2.5 billion, we don't get anything, a slight amount of increase. you couldget god knows how many, but the fdt wouldn't have any trouble with that. again, this is just one result with a verylarge number. let me just mention the issue of time changing data.everything i have talked about here assumes what those bounds assume. what those bounds assume is that thedata are independent. in fact, the hawking model isn't identically distributed, but it is significantly independent. whatthis means is that you data has to be generated by a stationary path, but then it has to be exchangeable. it can't bechanging over time.as long as that happens, in some sense, we don't need memory, because the stream itself is a memory, and that iskind of what makes our algorithms work, is that at some point you don't have to see more, because it doesn't containany more information with respect to your model than you have already seen.however, in a lot of applications of interest, what happens is that the data generating hypothesis is stationary. it ischanging over time. so, what do we do in that case?a general framework for mining massive data streams338statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.a general framework for mining massive data streams339statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.again, our framework handles this for very broad classes of algorithms. for an easy explanation, i am just nowtalking about how we do it in the context of decision trees. so, the standard technique to handle time changing data isto use a sliding window, or use some kind of weighted behavior examples.so, let's look at what is considered the sliding window case and generalizing it to theš [off microphone.] whatyou do is that, the model that you have at any given point is always the model run onš [off microphone.]the problem we have is that, at the data rates that we are looking at, you don't want to have to start a slidingwindow. it would just be like that. your sliding window might be a day. a day is like 100 million examples. we can'tstore that. also, you don't want to relearn your model every time a new example comes up in the sliding window.however, what you can do in our framework, almost every learning algorithm under the sun basically runs bycomputing some sufficient statistic, the decision tree algorithm that we just saw. so, all that we need to do, in order toeffectively run on the sliding window, is to be able to forget examples.that means that, if we want to run on a sliding window, when a new example comes up, as well as adding thatexample to the sufficient statistics, we subtract the oldest example in the window from those statistics. if the data isstationary, then the new examples and the old ones are equivalent and nothing changes. however, if the data is notstationary, then what happens is that at some point, what looks like the best is no longer the best way. at that point,what we start doing is growing a new subtree, and we leave those two until the new one is doing better than the oldone. so, we actually keep the old stuff around as long as it is still useful.suppose that the route suddenly becomes outdated. you don't want your performance to be bad because it is theentire tree. so, we let that tree stay there. we start growing a new tree and, when the new one becomes better than theold one, we switch it again. again, i am glossing over a lot of details here, but this is the basic idea.let me just conclude. i will skip over some of the future work that we are thinking of doing. i hope i haveconvinced youšand you probably don't need convincingšthat we need systems that mine openended streams of data.a general framework for mining massive data streams340statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.what i described here is a framework that satisfies a very stringent set of algorithms that learn on massive datastreams.you know, the basic idea behind this framework is to minimize the amount of time that you need to grow amodel, or you build a model on the data stream subject to guarantees that that model is going to be almost the same asyou get with infinite data.we have applied it to a wide variety of learning algorithms by now, and we have actually developed now in alibrary that hopefully we will make available in a few months or a year, that anybody who is interested in doingclassification or clustering or regression estimation can use these primitives.what we guarantee is that, as long as you access to data is encapsulated in the structures that we give you, it willscale to large data streams without your having to worry about it.audience: could you share your thoughts on 4.5 versusšat 100,000 or 105. so, it is clear that you areultimately going to win, and in the larger applications, that is the way to go clearly.is there a concern that you are not as efficient as sort of spitting out data than you would beš [off microphone.]mr. domingos: quite so. the reason it is doing better is becauseš [off microphone.]. the reason thatbasically c.45 does that is it is using each example multiple times, whereas we are only using each sample once.so, you can modify algorithms by using each sample multiple times. so, what we are seeing here is the crudestversion of this algorithm.the other thing that we can do is, we can bootstrap our algorithm with c.45 running on the number of examplesthat we are picking at random, and then start for there.c.45 actually makes lots of bad decisions. so, at the end of the day, we are going to offset the window. there ismore stuff that i can tell you about it.a general framework for mining massive data streams341statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.a general framework for mining massive data streamspedro domingos geoff hultendepartment of computer science and engineeringuniversity of washingtonbox 352350seattle, wa 98185œ2350, u.s.a.{pedrod, ghulten}@cs.washington.eduabstractin many domains, data now arrives faster than we are able to mine it. to avoid wasting this data, we must switchfrom the traditional ﬁoneshotﬂ data mining approach to systems that are able to mine continuous, highvolume, openended data streams as they arrive. in this extended abstract we identify some desiderata for such systems, and outlineour framework for realizing them. a key property of our approach is that it minimizes the time required to build amodel on a stream, while guaranteeing (as long as the data is i.i.d.) that the model learned is effectivelyindistinguishable from the one that would be obtained using infinite data. using this framework, we have successfullyadapted several learning algorithms to massive data streams, including decision tree induction, bayesian networklearning, kmeans clustering, and the em algorithm for mixtures of gaussians. these algorithms are able to process onthe order of billions of examples per day using offtheshelf hardware. building on this, we are currently developingsoftware primitives for scaling arbitrary learning algorithms to massive data streams with minimal effort.1 the problemmany (or most) organizations today produce an electronic record of essentially every transaction they areinvolved in. when the organization is large, this results in tens or hundreds of millions of records being producedevery day. for example, in a single day walmart records 20 million sales transactions, google handles 150 millionsearches, and at&t produces 275 million call records. scientific data collection (e.g., by earth sensing satellites orastronomical observatories) routinely produces gigabytes of data per day. data rates of this level have significantconsequences for data mining. for one, a few months' worth of data can easily add up to billions of records, and theentire history of transactions or observations can be in the hundreds of billions. current algorithms for mining complexmodels from data (e.g., decision trees, sets of rules) cannot mine even a fraction of this data in useful time.a general framework for mining massive data streams342statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.further, mining a day's worth of data can take more than a day of cpu time, and so data accumulates faster than it canbe mined. as a result, despite all our efforts in scaling up mining algorithms, in many areas the fraction of the availabledata that we are able to mine in useful time is rapidly dwindling towards zero. overcoming this state of affairs requiresa shift in our frame of mind from mining databases to mining data streams. in the traditional data mining process, thedata to be mined is assumed to have been loaded into a stable, infrequentlyupdated database, and mining it can thentake weeks or months, after which the results are deployed and a new cycle begins. in a process better suited to miningthe highvolume, openended data streams we see today, the data mining system should be continuously on, processingrecords at the speed they arrive, incorporating them into the model it is building even if it never sees them again. asystem capable of doing this needs to meet a number of stringent design criteria: it must require small constant time per record, otherwise it will inevitably fall behind the data, sooner or later. it must use only a fixed amount of main memory, irrespective of the total number of records it has seen. it must be able to build a model using at most one scan of the data, since it may not have time to revisit oldrecords, and the data may not even all be available in secondary storage at a future point in time. it must make a usable model available at any point in time, as opposed to only when it is done processing thedata, since it may never be done processing. ideally, it should produce a model that is equivalent (or nearly identical) to the one that would be obtained bythe corresponding ordinary database mining algorithm, operating without the above constraints. when the datagenerating phenomenon is changing over time (i.e., when concept drift is present), the modelat any time should be uptodate, but also include all information from the past that has not become outdated.at first sight, it may seem unlikely that all these constraints can be satisfied simultaneously. however, we havedeveloped a general framework for mining massive data streams that satisfies all six (hulten & domingos, 2002).within this framework, we have designed and implemented massivestream versions of decision tree induction(domingos & hulten, 2000; hulten et al., 2001), bayesian network learning (hulten & domingos, 2002), kmeansclustering (domingos & hulten, 2001) and the em algorithm for mixtures of gaussians (domingos & hulten, 2002).for example, our decision tree learner, called vfdt, is able to mine on the order of a billion examples per day usingofftheshelf hardware, while providing strong guarantees that its output is very similar to that of a ﬁbatchﬂ decisiontree learner with access to unlimited resources. we are currently developing a toolkit to allow implementation ofarbitrary stream mining algorithms with no more effort than would be required to implement ordinary learners. thegoal is to automatically achieve the six desiderata above by using the primitives we provide and following a fewsimple guidelines. more specifically, our framework helps to answer two key questions:a general framework for mining massive data streams343statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved. how much data is enough? even if we have (conceptually) infinite data available, it may be the case that wedo not need all of it to obtain the best possible model of the type being mined. assuming the datageneratingprocess is stationary, is there some point at which we can ﬁturn offﬂ the stream and know that we will not losepredictive performance by ignoring further data? more precisely, how much data do we need at each step ofthe mining algorithm before we can go on to the next one? if the datagenerating process is not stationary, how do we make the tradeoff between being uptodate andnot losing past information that is still relevant? in the traditional method of mining a sliding window of data,a large window leads to slow adaptation, but a small one leads to loss of relevant information and overlysimple models. can we overcome this tradeoff?in the remainder of this extended abstract we describe how our framework addresses these questions. furtheraspects of the framework are described in hulten and domingos (2002).2 the frameworka number of wellknown results in statistics provide probabilistic bounds on the difference between the true valueof a parameter and its empirical estimate from finite data. for example, consider a realvalued random variable xwhose range is r. suppose we have made n independent observations of this variable, and computed their mean .the hoeffding bound (hoeffding, 1963) (also known as additive chernoff bound) states that, with probability at least 1!,, and irrespective of the true distribution of x, the true mean of the variable is within of , whereput another way, this result says that, if we only care about determining x to within + of its true value, and arewilling to accept a probability of , of failing to do so, we need gather only  samples ofx. more samples (up to infinity) produce in essence an equivalent result. the key idea underlying our framework is toﬁbootstrapﬂ these results, which apply to individual parameters, to similar guarantees on the difference (loss) betweenthe whole complex model mined from finite data and the model that would be obtained from infinite data in infinitetime. the highlevel approach we use consists of three steps:1. derive an upper bound on the time complexity of the mining algorithm, as a function of the number ofsamples used in each step.2. derive a upper bound on the relative loss between the finitedata and infinitedata models, as a function ofthe number of samples used in each step of the finitedata algorithm.3. minimize the time bound (via the number of samples used in each step) subject to userdefined limits onthe loss.344about this pdf file: this new digital representation of the original work has been recomposed from xml files created from the original paper book, not from the original typesetting files. page breaks are trueto the original; line lengths, word breaks, heading styles, and other typesettingspecific formatting, however, cannot be retained, and some typographic errors may have been accidentally inserted. pleaseuse the print version of this publication as the authoritative version for attribution.a general framework for mining massive data streamsstatistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.where successful, this approach effectively allows us to mine infinite data in finite time, ﬁkeeping upﬂ with thedata no matter how much of it arrives. at each step of the algorithm, we use only as much data from the stream asrequired to preserve the desired global loss guarantees. thus the model is built as fast as possible, subject to the losstargets. the tighter the loss bounds used, the more efficient the resulting algorithm will be. (in practice, normal boundsyield faster results than hoeffding bounds, and their general use is justifiable by the central limit theorem.) each datapoint is used at most once, typically to update the sufficient statistics used by the algorithm. the number of suchstatistics is generally only a function of the model class being considered, and is independent of the quantity of dataalready seen. thus the memory required to store them, and the time required to update them with a single example, arealso independent of the data size.when estimating models with continuous parameters (e.g., mixtures of gaussians), the above procedure yields aprobabilistic bound on the difference between the parameters estimated with finite and infinite data. (byﬁprobabilistic,ﬂ we mean a bound that holds with some confidence 1!,*, where ,* is userspecified. the lower the ,*,the more data is required.) when building models based on discrete decisions (e.g., decision trees, bayesian networkstructures), a simple general bound can be obtained as follows. at each search step (e.g., each choice of split in adecision tree), use enough data to ensure that the probability of making the wrong choice is at most ,. if at most ddecisions are made during the search, each among at most b alternatives, and c checks for the winner are made duringeach step, by the union bound the probability that the total model produced differs from what would be produced withinfinite data is at most ,*=bcd,. for specific algorithms and with additional assumptions, it may be possible to obtaintighter bounds (see, for example, domingos & hulten (2000)).3 timechanging datathe framework just described assumes that examples are i.i.d. (independent and identically distributed). however,in many data streams of interest this is not the case; rather, the datagenerating process evolves over time. ourframework handles timechanging phenomena by allowing examples to be forgotten as well as remembered.forgetting an example involves subtracting it from the sufficient statistics it was previously used to compute. whenthere is no drift, new examples are statistically equivalent to the old ones and the mined model does not change, but ifthere is drift a new best decision at some search point may surface. for example, in the case of decision tree induction,an alternate split may now be best. in this case we begin to grow an alternative subtree using the new best split, andreplace the old subtree with the new one when the latter becomes more accurate on new data. replacing the old subtreewith the new node right away would produce a result similar to windowing, but at a constant cost per new example, asopposed to o(w), where w is the size of the window. waiting until the new subtree becomes more accurate ensures thatpast information continues to be used for as long as it is useful, and to some degree overcomes the tradeoff implicit inthe choice of window size. however, for very rapidly changing data the pure windowing method may still producebetter results (assuming it has time to compute them before they become outdated, which may not be the case). anopen direction of research that we area general framework for mining massive data streams345statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.beginning to pursue is to allow the ﬁequivalent window sizeﬂ (i.e., the number of time steps that an example isremembered for) to be controlled by an external variable or function that the user believes correlates with the speed ofchange of the underlying phenomenon. as the speed of change increases the window shrinks, and viceversa. furtherresearch involves explicitly modeling different types of drift (e.g., cyclical phenomena, or effects of the order in whichdata is gathered), and identifying optimal model updating and management policies for them. example weighting(instead of ﬁall or noneﬂ windowing) and subsampling methods that approximate it are also relevant areas for research.4 conclusionin many domains, the massive data streams available today make it possible to build more intricate (and thuspotentially more accurate) models than ever before, but this is precluded by the sheer computational cost of modelbuilding; paradoxically, only the simplest models are mined from these streams, because only they can be mined fastenough. alternatively, complex methods are applied to small subsets of the data. the result (we suspect) is oftenwasted data and outdated models. in this extended abstract we outlined some desiderata for data mining systems thatable to ﬁkeep upﬂ with these massive data streams, and some elements of our framework for achieving them. a morecomplete description of our approach can be found in the references below.domingos, p., & hulten, g. (2000). mining highspeed data streams. proceedings of the sixth acm sigkdd international conference onknowledge discovery and data mining (pp. 71œ80). boston, ma: acm press.domingos, p., & hulten, g. (2001). a general method for scaling up machine learning algorithms and its application to clustering.proceedings of the eighteenth international conference on machine learning (pp. 106œ113). williamstown, ma: morgankaufmann.domingos, p., & hulten, g. (2002). learning from infinite data in finite time. in t.g.dietterich, s.becker and z.ghahramani (eds.),advances in neural information processing systems #4, 673œ680. cambridge, ma: mit press.hoeffding, w. (1963). probability inequalities for sums of bounded random variables. journal of the american statistical association, 58, 13œ30.hulten, g., & domingos, p. (2002). mining complex models from arbitrarily large databases in constant time. proceedings of the eighthacm sigkdd international conference on knowledge discovery and data mining (pp. 525œ531). edmonton, canada: acm press.hulten, g., spencer, l., & domingos, p. (2001). mining timechanging data streams. proceedings of the seventh acm sigkddinternational conference on knowledge discovery and data mining (pp. 97œ106). san francisco, ca: acm press.a general framework for mining massive data streams346referencesstatistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.transcript of presentationmr. domingos: my talk is about a general framework for mining data streams. this is joint work that i havedone with jeff hockney at the department of nuclear science and engineering at the university of washington.so, this talk is basically about building things like classification models, regression models, public informationmodels and messages.here is what i am going to do. first, i am going to describe what the problem is that we are trying to solve. then iam going to present the general framework we have for solving this problem. then i will describe an exampleapplication of this framework. [comments off microphone]. then i will conclude with some comments.a general framework for mining massive data streams347statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.so, what is the problem we are trying to solve? a lot of the classification questions and so forth, algorithms, atleast when most of them were developed, you know, the number of data points wasn't very large.over the last 10 years, people have made a lot of progress in the sense that there is a lot of hard workš [offmicrophone] šdecision tree learners on data sets. i think there is great achievement, several orders of magnitude. thespeed at which the data rates are going up actually exceeds the speed at which we are speeding up our algorithms. so,we are losing the rates. i have a few random examples that i probably don't need to go through because people alreadyknow most of them. the bottom line is that, in most domains, in any given day, you can easily collect tens of millionsor hundreds of millions of records.for example, if you want to do a decision tree, doing a decision tree on, say, 20 million records, even with today'sbest algorithms, takes more than a day. so, in spite of all the progress that we have made, we are actually losing therace. the fraction of the available data that we actually use to build our models is actually dwindling to zero as timegoes forward. so, there is something wrong here. we need to do something about it. the thing that we need to doabout it is one of mining databases to one of mining data streams.you know, databases and data streams are different in many respect, but the idea that i have in mind here is thatthe database can be very large, but it is of a fixed size. at the end of the day, you know that you have so many records,you have so many samples that you can learn from. in the data scheme, it is open ended. in essence, we have infinitedata. how would we modify any of the algorithms if we actually had internet data available, because that is what wehave in the data stream. we get another 100 million, and another 100 million records. so, we need to change ourmodel of what we are doing from databases to data streams. what i am going to describe here is a framework toaddress these decisions. so, what are they?a general framework for mining massive data streams348statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.first of all, our algorithms need to require only a small amount of time per record. if the time requirement goes upwith the number of data points that you have in your past, then you are lost. sooner or later, you run out of breath.also, we need to be looking more at statistics than main memory. clearly, storing all the data in a main memory is notan option.we want to do a scan of the data. we can't assume that you are going to be able to store your data and go backand look at it. so, we want to be able to do everything we want, by looking at each data point, at most, only once.notice i say at most. maybe we can do things in even less than one scan of the data, and that is part of what i am goingto be talking about here.we also want the net results available at any time. again, the traditional model is that you collect your data andthen you run your algorithms on it and then, at some point in the future, your algorithm isn't running and you haveyour model. that isn't going to work here, because you have to wait forever. so, you want to have a model that getsbetter and better as time goes by but, at any given point, you can push the button and see what you already have, giventhe data that you already looked at.another very important thing is the following. we would like to ensure the results that we get are, impossible,equivalent to what you would get if you were actually just running your regular algorithm on a regular, but infinitesized database with a computer with infinite resources. it is very easy to satisfy those requirements if you compromisethe quality of the results that you are producing. the whole challenge is to actually guarantee those things whileproducing, say, decision trees or things that are not different from the ones that you get if you ran the algorithms thatwe know, and that we have, and whose properties we know.finally, we also want to be able to handle time changing phenomena. in a typical database, we just assume thatthe data is iid, so it doesn't matter what order they are in. in large data streams that last over months or years, veryoften, the phenomenon that you are looking at is actually changing over time. so, the model that you ran a year agoisn't necessarily valid now, and you also want to be able to deal with that.so, these are the criteria that we want to satisfy. in fact, when we started doing this work a couple of years ago, itsort of sounded overly ambitious. where are we going to have to compromise? however, to our amazement, we foundthat we were actually able to satisfy all those criteria. in fact, what we now have is, we have a framework that meets allthose criteria, and we have successfully applied it to several algorithms, including decision tree learning, networklearning, image clustering.a general framework for mining massive data streams349statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.we are able, using this, to deal with millions of examples a day using existing hardware. for example, the bestdecision trees we were able to mine perhaps a million examples per day, and now we are able to mine on the order of abillion examples per day.what we are doing now is, we are developing the general purpose library such that, if you write your algorithmsusing this library, you won't have to worry about scalability. you can just write them the same way as you alwayswrote them, and they will run on data streams without your having to focus on it. you just focus on the problem thatyou are trying to solve. so, this is not available yet, but it is like our next target that we are dealing with. so, i am notgoing to have time in this halfhour to actually talk about how we meet each of those problems. i will just focus onethe most salient ones. those are the problems with trying to ensure that you learn the same model, in real time, asquickly as possible, that you would run on infinite data, and a little bit at the end about what happens when the datagenerating process is not sufficient.a general framework for mining massive data streams350statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.what do i mean by discretion and how much data is enough? in a traditional learning algorithm, what you do is,the algorithm is a sequence of steps of some kind, and each of these steps look like the data in some way. think ofyour favorite algorithm and this is how they work. in the traditional setting, each of these steps, in general, looks at thewhole data before doing what it wants to do. now, in a data stream setting, that is not going to work because youwould never get past the first step, because you would wait forever for all your data to arrive.so, what you need to do is, you need to make the decision about how much data to use in each step. in essence,you want to use as little data as possible in each step, so that your model gets better as quickly as possible. i want tohave the best possible model as soon as possible, and this means minimizing the amount of data that i am using at eachstep.now, if i reduce the amount that i use at each step, i probably will pay a price for that in terms of the quality ofthe model that i get. so, the goal of what we are doing here, in a sense, one of the basic features of our framework, itenables you to minimize the amount of time that it takes to learn on the data stream, while guaranteeing that you arestill getting the same result that you would get if you waited for all the data up to eternity to arrive.some of you may be wondering, well, how can that be possible? to persuade you that that can be possible, iwould like to give you the basic idea of what we are doing. think about the simplest possible model that anybodywould want to build. say you just want to know what the average of some quantity is. what is the numberšforexample, in the network application, i want to know what the average number of packets is that you get from somesource, or whatever you need to find. suppose that i have, say, a billion samples of this. i don't necessarily need tolook at that billion, if i assume that they are iid, to form a good estimate of the mean. this is what pollsters do whenthey use 3,000 people as a sample for, say, 200 million. i mean, there are many kinds of results that you can use todecide how much data you need. one that we have used a lot is a hooking balance, and what happens in a hookingbalance is this. suppose we use variable x and range r. then we have n independent observations.what this guarantees is that, with a probability of 1!,, the variable is within + of x, where + is given by thisexpression. it depends on the range. it goes down with the number of data points that you have, and it actually onlygoes up with the log of one over the area. so, what this formula does is that, suppose you want to find the mean of somea general framework for mining massive data streams351statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.variable, but you are willing to have that mean be off by +, by at most +, with the probability at most ,. then, i will tellyou how many data points you need to gather. then, after that, you don't have to worry. it doesn't matter if you have atrillion points or infinite points. you don't need to look at them, because you already have the quantity that you wantwith the tolerance that you want. so, the + and , are the quality parameters that you have given.in essence, all we are doing, at least in the basis of our framework, all that we are doing is this. we arebootstrapping this idea to not just one quantity, but to a whole model, for example, a whole density estimation modelwith all of its parameters, or a whole decision tree with all of its nodes.so, we take these kinds of things for the individual things that you are doing and, by doing some analysis, youactually come up with a guarantee on the quality of the whole model.so, in summarizing one slide, here is the approach that we have. it goes in three steps. the first step is that youhave an upper bound on the time complexity of your algorithm, on how long it takes to run, as a function of thenumber of examples that you use in each step.in many cases, it is just the sum of the number of examples that you use in each step, multiplied by something thatis a constant. this is actually quite easy to do. so, we figure out how the running time of the algorithm varies with thenumber of examples that you use at each step.part two, we divide the upper bound on the loss between the model you get with finite data and the model thatyou would get with infinite data, as a function of the number of examples that you use when you are doing this withfinite data. so, you give me your loss function, the loss function that you are going to use to compare to differentmodels. so, if two models are very different by your loss function, then you know your loss function should have alarge value. this could be easier or harder, depending on the algorithm of lost function. we figure out how this lossvaries when you are comparing what you would get with infinite data, with what you are getting using a finite numberof examples.the final step is to minimize this. so, to minimize this, subject to the user, given constraints on this, meaning thatwe have a constrained optimization problem. what we are trying to do is to minimize the running time of the algorithmas a function of the number of examples we can expect, subject to the constraint that the loss that you get ata general framework for mining massive data streams352statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.the end of the day, the loss between finite and infinite data, is at the most +, with the probability of at least 1!l. so, thisis the general approach that we have.one of the surprises that we found is that we are actually able to do this for very broad classes of algorithm. westudied industry decision trees, and then we did it for clustering, and generally we saw that we could generalize this tomany different things.notice that, in some sense, what we are effectively doing with this is learning from infinite data in finite time,because we are getting, in finite time, the same results with the same tolerance that you would get if you waitedforever, until your data stream gave you infinite data.so, that is our general idea. there are lots of different methods that i am not talking about here, but that is thebasic idea of our framework. let's see how it applies to the case of decision tree induction, which is actually aparticularly simple one, perhaps in some ways not the most interesting one, but it is the first one that we did, and theeasiest one to talk about.so, in decision tree induction, what happens with the time? remember, the first step was to figure out how thetime varies within the examples that are used in each step.in the decision tree induction, which i assume most people here are familiar with, what happens is that you have aseries of nodes. each node tests the value of some attribute. depending on that value, it sends it to another node, and atthe least you have a classification.the basic problem in decision tree induction is deciding which tests, which attributes you pick to test in each node.the amount of time that we need to do that is proportional to the number of examples that you have. what youbasically need to do is, you need to gather some sufficient statistics that have to do with how often each value of eachattribute goes with each class.audience: is the number of nodes fixed?mr. domingos: no, the number of nodes is not fixed. so, the first thing we are going to have to do with ouralgorithm is that potentially, as time goes to infinity, the size of your tree could grow infinitely as well. in practice, itusually doesn't, but in principle, it could. so, when you have a model that is of unbounded size, potentially, the modeltakes an infinite amount of time to learn.what we are concerned with is what is the time that each step takes, and we are going to try to minimize, in thiscase, the sum of those times, which means that you runa general framework for mining massive data streams353statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.the model faster.so, notice, traditionally, and even in some of the very fast optimized algorithms for learning decision trees onvery large databases, they use all the data to take each test. if you unleash those algorithms on a data stream, in somesense, they never even pick, because they keep waiting for the rest of the data to come up, before they make their firstdecision. intuitively, you know, we should be able to make that decision after we see so many examples, and then goon to the next node, and that is exactly what we did.so, what about the lost function? how are we going to compare the decision tree we are building with finite datawith the decision tree that we would build with infinite data. you can do this in many ways. we are actually going touse a very stringent criteria which is, i am going to take the loss as being the probability that the decision trees disagreeon the random example. so, this is much more demanding than just we find that the tree be at least nearly as accurateas the other tree, or that they make nearly the same decisions.for the two trees to be an example, i am going to require that, from the point of view of that example, the twotrees are indistinguishable, meaning the examples see the exact same sequence of tests, and winds up with the sameclass prediction at the end of the day. so, this is going to be my loss function.audience: it doesn't seem that the tests matter, if it gets to the same answer.mr. domingos: maybe it does, maybe it doesn't. for some applications, it matters. the point is that, once youget guarantees on this loss function, you get guarantees on all the others, since we can do this at no extra cost.so, the final step, of course, is to try to minimize the limits on this loss. without going over a lot of details, whatis going to happen here is, what we are going to do, we are going to use the minimum number of examples to fit eachtest at each node such that, at the end of the day, the probability of disagreement is going to be bounded, and we aregoing to see the algorithm, and then we are going to see one example of the kinds of guarantees that we can getthrough the algorithm.so, here is our algorithm for mining massive data streams. we call it drpt. again, at a very high level, it hastwo arguments. the stream, which is an infinite sequence of examples, and , is the limits that we want to impose onthe disagreements between the two trees. we want to learn on that stream as fast as we can, subject to the concerns thatthe disagreement between that tree and what we would have with infinite data is, at most, ,.i beg your pardon, this , is actually not the disagreement between the whole trees. we are going to call that .this is the probability of getting each test wrong, and we are going to see how the two relate. it is the probability ofactually making the wrong decision on any given step.a general framework for mining massive data streams354statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.so, how does our algorithm work? we start with a tree that just has the roots. we initialize to suggest the count ofthe number of times that each value generated with each attribute i occurs with each class, k.now, for each example, x is the attributes and y is the class. we sort that example through a leaf. we are goingto have a partial tree at any given point. then we update the concept technique. then, the information gain on a linearor any other matters, we compute it for each attribute. then, if the data of the best attribute is better than the data of thesecond attribute by at least epsilon, when epsilon is a function of this ,, then at this point we know that, withprobability 1!,, we are picking the same attribute that we would be picking if we waited until we saw infinite data. so,at that point, we just split on that leaf, and now we start sending the new examples in the stream down to the childrenof that leaf.so, we start with the root. after some of our examples, we go through the roots to the leaves, and then theexamples get routed to its children, and then we collect examples of those guys and at some point we pick the test data,and we keep building it in this way.so, notice, if you think about it for a second, this satisfies everything that we were talking about. it does requirean amount of memory that is independent of a number of examples that you have seen and so forth. it is any time,because at any time you have a partial tree built and so forth. we have a few questions here.audience: [question off microphone.]mr. domingos: no, we deal with continuous inputs. in the particular network we are talking about here, weactuallyš [off microphone.] again, we can deal with continuous attributes.audience: [comments off microphone] šat many levels?mr. domingos: yes. that is actually not a problem with the levels that you have in the attributes. you justhave more statistics to start with.audience: it appears to be a greedy treebuilding algorithm. from that standpoint, it would perhaps lead to aninferior tree when compared to one that is pruned back with something else.mr. domingos: the basic algorithm that we are trying to use isn't really a decision tree algorithm. so, we aredoing the same thing. [comments off microphone.] you can stream data forward. so, this is the basic algorithm. themain claim that i made to you is that, running a decision tree in this way, in some sense, you will learn it in as littletime as you can.what you will get is a tree that disagrees very little with the tree that you get ona general framework for mining massive data streams355statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.infinite data. well, how come i can make that claim? there are actually various kinds of results that we can show here.the one that i am going to show you is select a better bound. to get that better bound, we have to make oneassumption about the data, which is almost always true, and that is why i am presenting it here.we can also get results without making that assumption. the assumption i am going to make is that some fractionof your data, at any given level of the tree, winds up in a leaf. again, in all the dozens of domains that i have applieddecision trees, i have never seen one where this doesn't happen. what this means is that the tree is somewhatunbalanced. you don't have all your leaves at the last level. if that happens, we will have to use a different kind ofguarantee than the one i am going to give here.again, for purposes of analysis, i am just going to assume that that probability is the same at every level, which itdoesn't need to be, and i am going to call it p, the leaf probability.you will recall that in our last measure there was some disagreement between some trees, and that i define as theprobability, that the path of the example through the first tree differs from the path that the example takes in the second.the two trees that i am going to compare is the decision tree that i learned using the algorithm that i just saw,with parameter ,, and the decision tree that i would learn in batch mode that i would learn using an ordinary decisiontree algorithm, with infinite memory and infinite time, waiting to see infinite samples.here is what we can guarantee. we can guarantee that the expected disagreement between the two trees is boundby ,/p. so, the , number is the value that we use at each node. i guess what this means is, if i get a smaller ,, call likethis , the one that you guarantee. you know the leaf probability. then immediately you know which lowercase , youneed to use at each local decision to get the result you want.obviously, the lower ,, the smaller , gets, the lower the leaf gets. not surprisingly, the lower the leaf probability,the bigger the tree you are going to grow. so, the smaller it has to be to get to the big ,.audience: it is noteworthy that ht, is also on an infinite sequence.mr. domingos: i will get to that in a second. actually, i will get to that right now. one correlate of thistheorem is that, on finite data, what our tree is going to be is a subtree. that is the best you could ever hope for, again,with the same probabilistic guarantee. then, it is like a twostep process to see how that correlator comes about.without actually proving the theorem here, it is actually quite easy to give youa general framework for mining massive data streams356statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.some idea of the basic intuition as to why we are able to have a guarantee like that.the reason is actually just the following. notice that if you have an example going down a certain path in the twotrees, and the probability of that example bumping into another one that is different is bounded by , because wedesigned it that way.so, if the path is of a certain length, l, then by the union of the probability that the data disagrees anywhere in thepath is, at most, , times l. this is, in fact, a loose bound. so, this is the problem when you get two paths of l data.now, the other side of this is, if we have a leaf probability of p, then the probability that a random example falls into aleaf at level i is the probability that it doesn't fall into a leaf at any of the p levels, so it is (1!p)i!1p, which is theprobability that it falls into a leaf at this level.although we need to consider computer expected disagreement, this is just the sum of the probability of these twoguys over all possible levels. the expected disagreement is the sum of all possible levels of the probability that theexample falls into that level, and the expected disagreements for that level.so, if you take this expression and you sum it, you just come out with the , that we have. like i said, there is thisnice corollary that applies to, in finite time, basically what you get is a subtree of the decision tree that you would getwith infinite data, again, with at most this disagreement.the bottom line here, though, is the following, that people in competition line theory have a lot of bounds thathave this flavor, but they are incredibly loose. they are nice for intuition but, from a practical point of view, they areuseless.a general framework for mining massive data streams357statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.the results that we are getting here is very different, because we are not looking at the size of the models toconsider. we are actually only looking at the concrete number of decisions that you make. the really nice thing, is thatthe number of examples that you need actually only goes up logarithmically with a , that you want to assure.putting it another way, as the number of examples grows linearly with time as you see more of the stream, yourguarantees get exponentially better. the fact that your guarantees get exponentially better means that, with realisticnumbers of examples, we can get disagreements that are extremely low. so, this is just an example to make things easy.let's say that your leaf probability is 1 percent, which basically means that your tree is huge, because only 1percent of your examples runs into a leaf at any given level. so, this is a very large tree. let's suppose that you have725 examples per node. it is not a very large number, if you think of the data stream of millions of numbers coming atyou. this is enough to guarantee disagreement of less than 1 in 10,000, .01 percent.so, here is one graph of results out of the many that we have produced, just to give you a flavor of what we areable to do. this is the fpt framework, 2.5 billion examples. using an ordinary pc, it took us a couple of days to dothis. in fact, the time to learn is completely dwarfed by the time just required to read the data from this.so, the best application of this algorithm is actually not unlike the kind of stuff that people have been talkingabout all day today, where you never even study an example. the algorithm actually takes an order of magnitude lesstime to run than it actually takes to just read them from this.so, we see a standard decisiontreerunning algorithm, and this is it running on the maximum number of examplesthat we could store in memory, which was 100,000. it had about 66 percent accuracy. without going into details of thedata set, it was composed of 100 attributes. so, this is a 100dimensional data set.what we see here is that the fdt, it is able to expecting more and more from the data right up to 100 millionexamples. actually, between there and the 2.5 billion, we don't get anything, a slight amount of increase. you couldget god knows how many, but the fdt wouldn't have any trouble with that. again, this is just one result with a verylarge number. let me just mention the issue of time changing data.everything i have talked about here assumes what those bounds assume. what those bounds assume is that thedata are independent. in fact, the hawking model isn't identically distributed, but it is significantly independent. whatthis means is that you data has to be generated by a stationary path, but then it has to be exchangeable. it can't bechanging over time.as long as that happens, in some sense, we don't need memory, because the stream itself is a memory, and that iskind of what makes our algorithms work, is that at some point you don't have to see more, because it doesn't containany more information with respect to your model than you have already seen.however, in a lot of applications of interest, what happens is that the data generating hypothesis is stationary. it ischanging over time. so, what do we do in that case?a general framework for mining massive data streams358statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.a general framework for mining massive data streams359statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.again, our framework handles this for very broad classes of algorithms. for an easy explanation, i am just nowtalking about how we do it in the context of decision trees. so, the standard technique to handle time changing data isto use a sliding window, or use some kind of weighted behavior examples.so, let's look at what is considered the sliding window case and generalizing it to theš [off microphone.] whatyou do is that, the model that you have at any given point is always the model run onš [off microphone.]the problem we have is that, at the data rates that we are looking at, you don't want to have to start a slidingwindow. it would just be like that. your sliding window might be a day. a day is like 100 million examples. we can'tstore that. also, you don't want to relearn your model every time a new example comes up in the sliding window.however, what you can do in our framework, almost every learning algorithm under the sun basically runs bycomputing some sufficient statistic, the decision tree algorithm that we just saw. so, all that we need to do, in order toeffectively run on the sliding window, is to be able to forget examples.that means that, if we want to run on a sliding window, when a new example comes up, as well as adding thatexample to the sufficient statistics, we subtract the oldest example in the window from those statistics. if the data isstationary, then the new examples and the old ones are equivalent and nothing changes. however, if the data is notstationary, then what happens is that at some point, what looks like the best is no longer the best way. at that point,what we start doing is growing a new subtree, and we leave those two until the new one is doing better than the oldone. so, we actually keep the old stuff around as long as it is still useful.suppose that the route suddenly becomes outdated. you don't want your performance to be bad because it is theentire tree. so, we let that tree stay there. we start growing a new tree and, when the new one becomes better than theold one, we switch it again. again, i am glossing over a lot of details here, but this is the basic idea.let me just conclude. i will skip over some of the future work that we are thinking of doing. i hope i haveconvinced youšand you probably don't need convincingšthat we need systems that mine openended streams of data.a general framework for mining massive data streams360statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.what i described here is a framework that satisfies a very stringent set of algorithms that learn on massive datastreams.you know, the basic idea behind this framework is to minimize the amount of time that you need to grow amodel, or you build a model on the data stream subject to guarantees that that model is going to be almost the same asyou get with infinite data.we have applied it to a wide variety of learning algorithms by now, and we have actually developed now in alibrary that hopefully we will make available in a few months or a year, that anybody who is interested in doingclassification or clustering or regression estimation can use these primitives.what we guarantee is that, as long as you access to data is encapsulated in the structures that we give you, it willscale to large data streams without your having to worry about it.audience: could you share your thoughts on 4.5 versusšat 100,000 or 105. so, it is clear that you areultimately going to win, and in the larger applications, that is the way to go clearly.is there a concern that you are not as efficient as sort of spitting out data than you would beš [off microphone.]mr. domingos: quite so. the reason it is doing better is becauseš [off microphone.]. the reason thatbasically c.45 does that is it is using each example multiple times, whereas we are only using each sample once.so, you can modify algorithms by using each sample multiple times. so, what we are seeing here is the crudestversion of this algorithm.the other thing that we can do is, we can bootstrap our algorithm with c.45 running on the number of examplesthat we are picking at random, and then start for there.c.45 actually makes lots of bad decisions. so, at the end of the day, we are going to offset the window. there ismore stuff that i can tell you about it.a general framework for mining massive data streams361statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.andrew moorekd r ball and ad trees: scalable massive science dataanalysistranscript of presentation and pdf slidesbiosketch: andrew moore is the a. nico habermann professor of robotics and computer science at theschool of computer science, carnegie mellon university. his main research interest is data mining, using algorithmsfor finding all the potentially useful and statistically meaningful patterns in massive sources of data. he is mostinterested in learning graphical models efficiently, probabilistic models of personperson interactions, spatiotemporalalgorithms for biosurveillance, active learning, new kinds of searches for interesting interactions between variables,and any kind of spatial data structure for caching sufficient statistics.dr. moore began his career writing video games for an obscure british personal computer. he rapidly became athousandaire and retired to academia, where he received a phd from the university of cambridge in 1991. heresearched robot learning as a postdoc working with chris atkeson and then moved to a faculty position at carnegiemellon.dr. moore's research group, the auton lab, works with astrophysicists, biologists, marketing groups,bioinformaticists, manufacturers, and chemical engineers and is funded from industry and research grants from thenational science foundation, nasa, and, more recently, the defense advanced research projects agency. hisresearch applications are in biosurveillance (he is part of a project led by mike wagner of the university of pittsburgh)and intelligence analysis. dr. moore collaborates closely with jeff schneider and chris atkeson, who was hispostdoctoral advisor at mit.kd r ball and ad trees: scalable massive science data analysis362statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.transcript of presentationmr. moore: this is joe monk with a couple of physicists, bob nichol and andy connolly, and also with mycolleague jeff schneider, another computer science, and two statisticians from carnegie mellon, chris genovese andlarry wasserman.i am going to be talking about a whole bunch of data structures which are very promising for allowing us to doapparently rather expensive statistics on large amounts of data, as opposed to trying to speed up cheap statistics, tryingto do some of the apparently very expensive algorithms.kd r ball and ad trees: scalable massive science data analysis363statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.kd r ball and ad trees: scalable massive science data analysis364statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.i am going to begin at this point. there are many pieces of statistics that want to do work of the sort of magnitudeof looking at all pairs of records in a big database. in fact, there are plenty of them that need to look at all triples or allquadruples, and that is unpleasant, computationally.there are also many pieces of statistics which need to look at all pairs of all variables, covariants in a database.even building a covariance matrix requires you to do that, and in some cases, there are things that you need to look atall triples and all quadruples of covariances.so, these things which scale quadratically or cubically with the amount of data over the number of attributes arevery frightening.i am going to show you some tricks from basically borne out computational geometry which allow us to survivethat. so, let's just review a few operations which people need to do involving all pairs.here, if you are using something like a k nearest neighbor classifiers lots of times to make lots of classifications,you would end up doing something quadratic in the size of your database.if you are trying to do kernel regression or, something more familiar, kernel density estimation, on a large amountof data, then if you implemented naively, then every one of your queries needs to ask questions of lots of other datapoints lying around it.kd r ball and ad trees: scalable massive science data analysis365statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.another related example is if you are doing something like a gaussian mixture model cluster, where you have alarge number of records and a large number of clusters. certainly, if you are using something like an aic or a bactype of a measure for creating your model, you would notice that the number of gaussians that you created in yourmixture model grows as the number of records grows. again, you actually end up with a quadratic kind of problemthere.there are many other examples. i am happening to show you examples that we have needed to use in reality, butthere are many examples that we haven't, ourselves, had to use so far.spatial anomaly detection, just finding out those, and highlighting those data points which are in dense regions ofspace.kd r ball and ad trees: scalable massive science data analysis366statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.there are many others including various nonparametric regressions, some other kinds of point processes. onethat is not up on here, which we are doing a lot of work with at the moment, spatial scan statistics for spotting clustersof disease.asking questions about all pairs of attributes is important, asking for things like finding the most highly correlatedpairs of covariates in your database.for building dependency trees or bayesian networks, two now very popular things to do in commercial data andon scientific data, they also require you to look over not just pairs, but also ntuples of attributes.i am going to frighten you here by talking about the kinds of algorithms which are the exact antithesis to the sortsof algorithms that we are talking about today. these are algorithms which need you to not just do a whole pass overthe data or several passes. they need you to do the equivalent of, in some cases, hundreds of thousands of passes overthe data.so, at first sight, they might look like the kind of algorithms that you can do on your small pc, if it all fits inmemory, but you could never hope to apply to large amounts of data. as a computer scientist, it is a very interestingquestion to see, is that true, or is there some other way of organizing these things so that you can get away with themon large amounts of data.kd r ball and ad trees: scalable massive science data analysis367statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.now, i am going to focus here on a very, very conceptually simple statistic, which looks like it requires you tolook at all pairs, and it is computing part of something called the two point spatial function of a distribution.the computationally expensive part of it is the following. you end up wanting to ask the question, given a set ofdata, how many pairs of data points are within a certain distance of each other? so, how many pairs of data are close toeach other? you have got a data set. all you are getting out of it in this case is one number. how many pairs are closeto each other? the reason you need that is that there are many spatial statistics where knowing those kinds of numberscharacterizes the clumpiness of the distribution for you.so, let's look at how we can do this operation. you could just say, well, i am going to buy a fast computer and dothis, but we just can't afford to do that. if you try to live this way, then one day you get 1,000 times as much data, youralgorithm is going to run a million times more slowly.instead of preparing slides like a professional, i am just going to run a program to show you what we are going todo about this.here we have a very tiny little data set. i am actually imagining that i am part way through this process ofcounting all pairs of points which are within that distancešyou see that little arrow up at the topšwithin that distanceof each other, so within .4 units of each other.i am actually showing you this part way through running the program. i am not at the beginning of the program orthe end. i am partway through, whereas a subroutine, akd r ball and ad trees: scalable massive science data analysis368statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.recursion, i happen to be in a situation where i am asking, how many pairs of data points have one data point in thisleft rectangle and one data point in this right rectangle, and are within distance .4 of each other.now, the way i am going to answer that is by breaking that single question into two smaller questions. i am goingto pick one of these rectangles, and in this case i am going to pick the larger rectangle, the right one. i am going tobreak it into two, and i am going to ask that question about the top half of the right rectangle, working with the leftone, and then later on i am going to ask it about the bottom half of the right rectangle working with the left one. so,let's see what happens.it looks like i begin with the bottom one here, and then i recursively ask the same thing. every time, i am going tochoose the larger of the two rectangles, break it in half, do the first half first, and then later on recursively come back tothe second half. so, i jump down there. now i am asking the question, how many pairs of data points have one guyfrom here, one guy from here, and are within distance .4 of each other.when i recurse again, i get to an interesting position here, and you can probably see what has happened. i can doa simple piece of geometry on these two squares and prove that it is impossible for any pair of data points to be withindistance .4 of each other, because the shortest distance between these rectangles is greater than .4. so, i can savemyself some work by not looking at any of those children.audience: if you were askedšit seems to me there is a valuešif you use something different from the .4, .9,you might not be able to use that geometry; is that correct?mr. moore: we are going to see an answer to that question just almost immediately. so, at this point, we backup and go somewhere else. we look at the other child. instead of that one, we look at this one.now we come to the pruning. so, we just carry on with the algorithm. we jumped on to there, had to carry on.now, here we get something that addresses your point. here, we can do some different geometry, and we can alsoprove, without having to do anything expensive, that every pair of point where one point comes from here and onepoint comes from here, must be within distance .4 of each other.so, now, instead of actually explicitly iterating all of those points, i can simply multiply the product of thenumber of points in this box with the number of points in this box, and adding that to my running count that i amaccumulating. it is very simple. so, that is another kind of pruning that i can do. i can just carry on doing that now. icarry on running through the data table and you see an occasional pruning of something.just to give you an idea of what this means, i am going to run this problem now on a data set with 40,000 datapoints on it. you see it running there without the graphics, which means that it is faster. it still needs to do quite a lot ofcomputations. remember, the 40,000 data points is 1.6 billion pairs of data points to consider. in 11 seconds, we havegot the answer. it turns out there are 296 million pairs of data points in that particular data set within distance .4 ofeach other.kd r ball and ad trees: scalable massive science data analysis369statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.it turns out that this takes about 10 minutes to run if you were to use the standard correcting algorithm. so, let'slook back, for instance, if we have gone from 10 minutes to about 10 seconds. that is a 60fold speed up, but i am notreally excited about that. we have had been able to run this on data sets with 14 million records, and that would havetaken over a year to run otherwise, and in our case, it took 3 hours.so, that is one example of very simple geometry helping do something we wanted, and we didn't resort to anyapproximation there. we have got exactly the same algorithm that we wanted, the same count that we got from thenaive method.i won't show you how to do it now, but if you prefer, you can ask this kind of algorithm the following question.you can say, give me back a number that is within onetenth of a percent of the correct number. if you tell it that youare going to allow it to make a certain error, if it will promise you that it will, at most, make that error, then thisalgorithm could be adapted to take advantage of that, and you usually get an extra magnitude or two of speed up.audience: that is not a probabilistic promise?mr. moore: it is not a probabilistic promise. it is an exact hard bound promise. so, it doesn't matter how weirdthe distribution of data is. so, if we do that, we are getting desperate.sometimes it turns out we are running these things on data sets where we are doing lots and lots of randomizationtesting, so having to run this same operation a thousand times. that is when we get desperate and have to do thosekinds of approximations.now, interestingly, it turns out that the reason people run two point functions is to test whether two spatialdistributions have got the same flavor to them. for instance, if you have got a theory of the universe, you write asimulation of the universe, which produces a simulation distribution of galaxies, in order to ask the question, does thesimulated distribution of galaxies have the same flavor, the same clumpiness, the same stringiness as the truedistribution of galaxies that i have observed. that is the kind of place you actually run this algorithm.kd r ball and ad trees: scalable massive science data analysis370statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.sometimes, doing it with pairs of data points isn't enough. if two distributions have got the same power in them,they will have the same two point statistics, no matter what. so, you are not able to distinguish between anything.for that reason, people move up to three point functions where they ask questions like, how many triplets of datapoints were already distant .5 of each other.that is something that previous physicists had only run out to about 1,000 data points and, even there, it wouldtake them weeks of cpu time. they were planning on building a supercomputer to do this, until we demonstratedrunning it on a laptop in half an hour on a million data points.audience: for this particular applicationš [off microphone.]mr. moore: it is true that sometimes, if you are comparing two point functions at two different distributions,you might just be asking the question, is one of them larger than the other one. when you do that, you get morepruning opportunities and you can go even faster. so, there is an additional opportunity for us to speed up there.if you decide that you are not trying to answer the question, give me the counts, but instead, answer the question,find out if the counts are different, those are two slightly different questions, but it does give you the option to furtherspeed up, and we do do that, mostly because physicists want to get the actual numbers out that we have experienced.kd r ball and ad trees: scalable massive science data analysis371statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.i am going to show you now how this generalizes. it seems like i was talking about a very specialized problem.there is a more general question.suppose you have this as a two dimensional gaussian and i am just showing you the covariance matrix to giveyou an idea.suppose for some reason we want to compute the likelihood of all these data points. obviously, these are rathersurprising data points, if we say they are generated by that gaussian, and i will go into the reason for that later.kd r ball and ad trees: scalable massive science data analysis372statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.one thing we can do is, if we have this bounding box on our data points, then it is quite easyšit is not trivial, butit is quite easyšto compute what position in this box a data point would have to be to have the maximum possibleload likelihood and where it would have to be to have the minimum load likelihood.having done that, if we know the bounding blocks, we know how many points are in the box, we can put boundson the load likelihood of the points in that box.what you can then do, if you have got a whole room full of data and you want to find this load likelihood, youcan play the same kind of game where you have a box that you keep splitting in half, looking at smaller and smallerregions. you stop doing that whenever you get down to a level where the box is virtually certain about what the loadkd r ball and ad trees: scalable massive science data analysis373statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.likelihood is from that.you can do a couple of extra tweaks on top of that to do some other things. so, we have now got software thatwill do these same kinds of tricks very quickly for kernel density estimations, lqh's, principal components. there aremany algorithms in the kernel density literature which are down to the kernel sizes, and they are more complex. youcan do various kinds of clustering and filament tracking and so forth.it has got to the stage now where it is boring for us to do more of these, but if anyone has any requests, we arealways happy to do it.i want to show you one of these. i have got a problem, which is i can't see on my screen what i am doing. okay,we have a data set here. you can't see it very clearly. in fact, i am only drawing up a tiny sample of the data set. this isa data set with i think 50,000 data points in it. you can see these little blue dots lying around. these form the data set. iwish i could have shown you that more clearly, but i guess i can't.so, 50,000 data points. we are going to run gaussian mixture models on it with 15 centers. we see actually a tinycopy of the data point here. in fact, i think you have 20,000 data points here in the distribution. i have got a small copyof the data set just here, with the same distribution. it is very, very tiny embedded there.i am going to run gaussian mixture modeling and i am going to run 15 iterations. so, the thing i just wanted toshow you is that, by using tricks like that, you can do things like that to your mixture models very quickly.kd r ball and ad trees: scalable massive science data analysis374statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.kd r ball and ad trees: scalable massive science data analysis375statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.kd r ball and ad trees: scalable massive science data analysis376statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.forty thousand data points, typically, with a typical implementation, you can expect each of those iterations to besomewhere between a few minutes and an hour. you can do 50 iterations in a few seconds. i will show you a slightgeneralization of that.audience: and that is the exact calculations or approximate?mr. moore: that is exact.now i am going to show you something a little more heuristic, which is a version of the mixture models gaussianbuilt on this technology, which is also doing adaptive splitting and merging of gaussians using various pieces oftesting, while it is running.it is probably going to run unnecessarily low, so i shall have to talk through this. the distribution which generatesthis table is actually not a mixture of gaussian, but it was designed to make these three. that is cald. that stands forthe center for automated learning and discovery, which is a group at carnegie mellon university.all it does, it doesn't do anything very clever to choose whether to split or merge gaussian. all it does is, ittemporarily splits them. watch what happens to them. it does a bac test to find out whether it was worth splitting thegaussian, and then undoes the split.it is slowing down a little bit now. it is up to about 50 gaussians. notice when i stopped it ended up with farfewer gaussians that we saw it running with most of the times. that is because this was the best model according tothe scoring.now we are at the stage for least pass. this is an operation that had previously taken days and we can quickly dothis now on the fly.i should mention that we can go to higher dimensional data for this. we are notkd r ball and ad trees: scalable massive science data analysis377statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.just doing this in two dimensions. it is still pretty tractable in 10 or 20 dimensions. we hit statistical problems beforewe hit computational problems. doing a fully general mixture of gaussians in 20 dimensions is a very dangerousthing. there are so many parameters.i just wanted to show you one thing you can do with this, but it is going to require some contortions with a mouse.one thing we often find a mixture of gaussians useful for is just a nice density estimator. without showing youthe density estimates, we are going to just jump down here to the high resolution. i am going to home in on thatparticular region of the data set. remember, it is still a copy of the data set down there. if we go down closer, we canseešone of the brilliant properties that i love with mixtures of gaussian is the way they manage to do this clusteringwith lots of different resolutions of interest.this isn't something that a kmeans or a hierarchical model has been able to do, to spot the small things. justdown there at a lower resolution, are the big things.now i am going to get back to some more theory. here is another question. now, we are going to be going moretoward the data queue types of questions that we have seen in a couple of talks today.suppose we are dealing with a big data set with categorical attributes, not real value attributes and, for somereason, we are constantly having to ask counting queries. for instance, given a data set, we might be asking, does theaverage wealth of innercity bluecollar suv owners seem to be correlated with their health. that is a question whichkd r ball and ad trees: scalable massive science data analysis378statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.you can answer by going into a database, getting a hold of appropriate records, getting some sufficient statistics fromthem, and then doing a computation.suppose that we know that we are not just about to be asked to do one of them, but we are going to be asked to dolots and lots of them. for example, you end up wanting to do this if you are building a bayesian network. you end upwanting to do it if you are, for instance, using a very nice algorithm by dumichelle and pregibon, that they introduceda couple of years ago based on doing lots and lots of empirical bayes tests on subsets of the data.those are just two examples. we have had about 10 examples of situations where you want to have a big databaseand ask billions of those kinds of questions. you want to ask them quickly.this is a crazy idea. in order to make this answer a question like that, why don't we build a secondary databasewhere we precomputed the complete sets of possible questions you could ask, and we precomputed the answers forall of them.the reason that is a good thing is, if we had that database, then i could say, i have a constant time algorithm,independent of the amount of data, for doing those kinds of statistical questions. the only problem with it is, a, itwould take a huge amount of memory. in fact, on a typical database that we have been working with, we computed itwould take about 1040 bytes of memory to contain this secondary database.the other problem is, it would take a very long time to build the secondary database. the universe would haveended by the time we built it.although it is a crazy idea to do this, sometimes we can just push our luck and get away with it if we can veryintensively compress the secondary database while we are building it.ad is a way to compress these databases. i am going to try to give you a sketch in about three minutes as to howwe did it.kd r ball and ad trees: scalable massive science data analysis379statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.here is a data structure which actually implements this impossibly expensive database i mentioned. it is a treestructure.every node in it contains a countšfor instance, this node contains a count of four. what does that mean? thisnode is answering the question, how many records have actually been run in sector four and attribute two we don'tcare. so, it turns out there are four records of that form. how many records have four two, we end up looking at thisclass of data sets. a1 equals four and the children of that are actually a2, because you come down here and you see thatthere are three such records.so, that is a database in which we can access any counts we like. if you like, it is an implicit representation of abig hypercube combinatory index by all combinations of attributes. we can do it with two attributes, but even with justa mere 20 attributes, this thing would take up too much memory to store on your disk drive, let alone your memory.so, you can try to save memory while you build this data structure.kd r ball and ad trees: scalable massive science data analysis380statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.the first thing you can do, whenever you get a count of zero, you don't need to store that record and you don'tneed to store any of the children in that record. any node here with a count of zero, any specializations of thosequeries, also count as zeroes.so, that saved us a little bit of data, but not much. the example i described before, it went down from 1040 downto about 1030 bytes of memory. so, although we have decreased our memory requirements 10 billionfold, it doesn'thelp us much.now, here is something else at this thing. any node on this thing puts a tail on considering all possible values ofthe attribute a1. i am going to mark in green the most common value of a1. here, a1 contains the value four, and thathas the highest count. that happened four times. the others happened less often.kd r ball and ad trees: scalable massive science data analysis381statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.everywhere on the tree, whenever i am instantiating a variable, i take the most common value of it and delineatethat in green, and delete all of those from the database. all i am going to do is leave behind this little thing saying mostcommon value. this was the most common value before i deleted it.now, there are two consequences of that. it turns out that you save a very, very large amount of data here. thereason is because it is always the most common value on all levels that you are getting rid of. so, you might get rid of,if you are lucky, 40 percent of all the nodes on this level. then, of the remaining 60 percent, you get rid of 40 percentof those on this level. of the remaining whatever it is, 20 percent, you get rid of 40 percent of those on the next level.we can prove the amount of memory you need really goes down. in the example i was describing we then end uptaking 106 bytes in this thing. it is much, much smaller. so, that helps us a lot.the other good piece of news is that you haven't lost any information you really needed. you can reconstruct anycounts that you had been able to get before through this prudent data structure.i will give you an intuition of why that is true. i am going to try to do this in four minutes.i will use this notation of a contingency table of a set of attributes. it is just thisš well, you know, you areprobably all familiar with what a contingency table is. in this data set, a contingency table at a1 and a2 is very similarto the distribution over a1 and a2.kd r ball and ad trees: scalable massive science data analysis382statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.it is a table recording the fact, for instance, that 2, 2 occurs once in this database. there it is.here is the marginal contingency table over a2, just as there is a histogram over the values of a2.this is a conditional contingency table. this is the conditional contingency table of a2 among those records inwhich a1 has the value two. so, if you can get your head around this, among records in which a1 has the value two, a2takes the value of one twice there. those are the two records in which a1 has a value two, a2 takes the value of onetwice. these are just objects you need to know about.there are two properties that contingency tables have, and they are very similar to the properties that you get fromthe axioms of probability, only this one is based on counts. one of them, the marginal contingency table over a2, i canjust rowwise add together the conditional contingency tables over each of the values of a1.second fact, if i want to make the joint contingency table over a1 and a2, i can do that by gluing togethercontingency tables of a2, conditioned on each of the values of a1.kd r ball and ad trees: scalable massive science data analysis383statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.so, let's hold those two facts in our memory, and then i will show you how you can do reconstruction. so, here isour mutilated data structure. i am going to show you we can recreate the joint contingency table over a1 and a2, fromthis mutilated data structure.i am going to have a recursive call, a program which progressively runs over this tree. it seems to take a node, inthis case, the root note, and a list of attributesšin this case a1 and a2.to build the joint contingency table of a1 and a2, it is going to be just what i said before. it is going to computerthe conditional contingency tables, conditioned on each value of a1, from each of these nodes. to do this one, it justdoes a recursive call to the same algorithm. to do this one, it just realizes it has to create a table full of zeroes. this isthe place where it looks like we are getting stuck, because this is the place where we don't have any data structures torun down to compute this conditional contingency table.we can use the other facts i told you about contingency tables to save us. i know that i do a rowwise addition ofthese four contingency tables, i should get the marginal contingency table on a2.kd r ball and ad trees: scalable massive science data analysis384statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.if i do another call, recursive call on my data structure here, asking it to create for me just the marginalcontingency table for a2, once i subtract these three contingency tables, that allows me to create this conditionalcontingency table, which i glue together with the remaining three, and that gets me the joint. did i succeed inconvincing you in four minutes?audience: actually, i believed it when you said it.mr. wilkinson: you could have run into a stack problem; right?mr. moore: the memory requirements for this are most proportional to twice the size of the contingency tableyou create, multiplied by the number of attributes in the data. it is small.now, it turns out that we can only do this practically forši think our largest case was 500 attributes, which is stilla fairly respectable size, but we are not talking about doing this on tens of millions of attributes. of course, we can doit on a daytoday basis, in one case we did it on a billion records, but the number of attributes were limited to a fewhundreds.kd r ball and ad trees: scalable massive science data analysis385statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.so, when we actually do use this for various things, we have to search, in this case, over a relatively smalldatabase, just 1.5 million records, and 27 attributes, so 27 covariates. we also search over all fivedimensional tables.there are 27, choose five of those. previously, we would have done a pass through data set for each of those 27 things.we don't have to do that any more. instead of taking 1.2 months of cpu time, it takes 21 minutes of cpu time to do it.the final closing comments, what happens as we have been developing these algorithms? i apologize because ihave been showing you algorithms that we actually developed a couple of years ago.i didn't show you our recent work because i didn't have time to get to that in this talk. what happened after wereduced those algorithms was a bunch of basically government places and also some commercial places who had runinto the problem they had a piece of statistics they really wanted to run, but they really could run it using off the shelftools, came to us. so, we had a consulting company which now routinely turns these things out, in special cases, thatwe do things. we have found it is a very popular thing. we are turning away business rather than having to seek it out.so, i strongly recommend that this is a good business to be in, the computer science of making statistics go fast.kd r ball and ad trees: scalable massive science data analysis386statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.just to ground it a little bit, some of the main academic things we are working on at the moment, is applyingspatial statistics for biosurveillance of possible bioterrorist attacks.we have a system running monitoring western pennsylvania, another one monitoring utah. it was in operationduring the olympic games, looking for hot spots in the sales of overthecounter pharmaceuticals, emergencydepartment registrations. we are currently trying to take that national to the stage where we are still going to be, byyour standards, quite small. we will be getting in 10 million records a day, so not the billions of records that some ofyou have been talking about, and hoping to apply that in those cases.i have shown you some of the physics applications. this has also been very useful in some large, highthroughputscreening work with pfizer.the coolest algorithm is, we are putting something on caterpillar diesel engines and, this is a long shot, but if itcould become a product in 2008, when new emissions will come into place, which will monitor everything going on ina diesel engine realtime, to be doing realtime monitoring of emissions as a function of the timing of the diesel engine.we are also following this up using this for a number of intelligence applications. so, that is it. with goodcomputational geometry, you can sometimes get even apparently impractical problems to a practical size, and papersand software at that web address. that is it.mr. wilkinson: time for one or two questions.kd r ball and ad trees: scalable massive science data analysis387statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.mr. domingos: going back to the pairwise distances problem, they were all guaranteed to be within .4 orfarther than .4. what if that never happens? do you just go down to single problems?you could always have two boxes where the smallest distance is less than .4 and the largest distance is morethan .4.mr. moore: in the exact version of the algorithm there will be some cases where you have to go down toindividual points in the database. you can show that the number of times that you will have to do that is the square rootof the number of pairs in the database. so, the square root of the number of pairs in the database is linear, so the wholething ends up being linear in the database size.if you tell the system that you are prepared to accept, say, a 1 percent error in your final count, usually it willnever go down.audience: what about the geometric points are scaled to higher dimensions?mr. moore: good point, i should have mentioned that. these are based on a data structure called a kd tree, forwhich jerry friedman was one of the inventors. kd trees typically don't work very well above about 10 or 20dimensions. so, some of these algorithms, like the mixture of gaussian ones, we get into computational problems withabout 20 dimensions.some of the other things, like the kernel methods or the two point counts we have done, we actually these daysrun them in a different data structure called a metric tree, where everything is stored in balls instead of hyperrectangles. under some circumstances, we have been able to run those in thousands of dimensions. in one particularcase, we ran it in a million dimensions. that is not generally possible. if our distribution of the data is uniform, youcould prove that you should not be able to do this efficiently.in empirical data, there are correlations among the covariates. even if they are nonlinear correlations, then youexpect it to be practical, which is why we have been able to do this in thousands of dimensions.kd r ball and ad trees: scalable massive science data analysis388statistical analysis of massive data streams: proceedings of a workshopcopyright national academy of sciences. all rights reserved.concluding commentsamong the research topics presented at this meeting were remote sensing for climate modeling, computer ornetwork intrusion detection monitoring, records from communication networks or web logs, ecommercerecommendation systems, and realtime imaging problems arising in robotic vision. by design, none of thepresentations is so broad that it touches all five major areas of research. however, common threads did emerge, such asstatistical modeling, visualization, and the need to store, move, and manipulate massive data streams, which arenontrivial challenges.the workshop explored both theoretical aspects of dealing with massive data streams as well as practical meansof effective analysis of such streams. for many massive data streams, there is no model of the data and little a prioriknowledge, and the workshop presenters demonstrated the vital role that statistical modeling, analysis, andvisualization play in enabling the essence of otherwise hidden information to be distilled from these data streams.this is an exciting new realm for statisticians, motivated by a rapid increase in streaming data. the committee onapplied and theoretical statistics (cats) has already made plans for a workshop on visualization of uncertaininformation, to be held in the winter of 2005, and the committee will explore other dimensions of these challenges inthe coming years. please check out the cats web site for future events.concluding comments389
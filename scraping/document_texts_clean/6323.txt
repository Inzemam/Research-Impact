detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/6323funding a revolution: government support for computingresearch302 pages | 6 x 9 | paperbackisbn 9780309062787 | doi 10.17226/6323committee on innovations in computing and communications: lessons from history,national research councilfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.government support for computing researchcommittee on innovations in computing and communications:lessons from historycomputer science and telecommunications boardcommission on physical sciences, mathematics, and applicationsnational research councilnational academy presswashington, d.c. 1999funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.notice: the project that is the subject of this report was approved by the governingboard of the national research council, whose members are drawn from the councilsof the national academy of sciences, the national academy of engineering, and theinstitute of medicine. the members of the committee responsible for the report werechosen for their special competences and with regard for appropriate balance.the national academy of sciences is a private, nonprofit, selfperpetuating society ofdistinguished scholars engaged in scientific and engineering research, dedicated to the furtherance of science and technology and to their use for the general welfare. upon theauthority of the charter granted to it by the congress in 1863, the academy has a mandatethat requires it to advise the federal government on scientific and technical matters. dr.brucealberts is president of the national academy of sciences.the national academy of engineering was established in 1964, under the charter of thenational academy of sciences, as a parallel organization of outstanding engineers. it isautonomous in its administration and in the selection of its members, sharing with thenational academy of sciences the responsibility for advising the federal government. thenational academy of engineering also sponsors engineering programs aimed at meetingnational needs, encourages education and research, and recognizes the superior achievements of engineers. dr. william a. wulf is president of the national academy of engineering.the institute of medicine was established in 1970 by the national academy of sciencesto secure the services of eminent members of appropriate professions in the examination ofpolicy matters pertaining to the health of the public. the institute acts under the responsibility given to the national academy of sciences by its congressional charter to be anadviser to the federal government and, upon its own initiative, to identify issues of medicalcare, research, and education. dr. kenneth i. shine is president of the institute of medicine.the national research council was organized by the national academy of sciences in1916 to associate the broad community of science and technology with the academyspurposes of furthering knowledge and advising the federal government. functioning inaccordance with general policies determined by the academy, the council has become theprincipal operating agency of both the national academy of sciences and the nationalacademy of engineering in providing services to the government, the public, and the scientific and engineering communities. the council is administered jointly by both academiesand the institute of medicine. dr. bruce alberts and dr. william a. wulf are chairman andvice chairman, respectively, of the national research council.support for this project was provided by the national science foundation under granteia9529482. additional support was provided by the association for computing machinery and the institute of electrical and electronics engineers computer society. any opinions, findings, conclusions, or recommendations expressed in this material are those of theauthors and do not necessarily reflect the views of the sponsors.library of congress catalog card number 9888131international standard book number 0309062780additional copies of this report are available from:national academy press (http://www.nap.edu)2101 constitution ave., nw, box 285washington, d.c. 2005580062462422023343313 (in the washington metropolitan area)copyright 1999 by the national academy of sciences. all rights reserved.printed in the united states of americafunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.iiicommittee on innovations incomputing and communications:lessons from historythomas hughes, university of pennsylvania and massachusettsinstitute of technology, chairgwen bell, the computer museumerich bloch, council on competitivenessrobert bressler, sun microsystems, inc.paul david, oxford university and stanford universitymarvin denicoff, office of naval research (retired)david hounshell, carnegie mellon universityamos e. joel, jr., lucent technologies, inc. (retired)timothy lenoir, stanford universitydouglas mcilroy, dartmouth collegeemerson pugh, ibm corporation (retired)charles seitz, myricom corporationcharles thacker, microsoft corporationspecial advisordaniel j. kevles, california institute of technologystaffjerry r. sheehan, senior program officer (study director afterfebruary 1997)marjory s. blumenthal, director (study director throughfebruary 1997)david mindell, cstb consultantjed gordon, research aideleslie m. wade, research assistant (through march 1997)david padgham, project assistant (starting august 1998)mickelle rodgers, project assistant (through august 1998)synod p. boyd, project assistant (through december 1997)funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.ivcomputer science andtelecommunications boarddavid d. clark, massachusetts institute of technology, chairfrances e. allen, ibm t.j. watson research centerjames chiddix, time warner cablejeff dozier, university of california at santa barbaraa.g. (sandy) fraser, at&t corporationsusan l. graham, university of california at berkeleyjames gray, microsoft corporationbarbara j. grosz, harvard universitypatrick hanrahan, stanford universityjudith hempel, university of california at san franciscodeborah a. joseph, university of wisconsinbutler w. lampson, microsoft corporationedward d. lazowska, university of washingtondavid liddle, interval researchbarbara h. liskov, massachusetts institute of technologyjohn major, qualcomm incorporateddavid g. messerschmitt, university of california at berkeleydonald norman, nielsen norman groupraymond ozzie, groove networksdonald simborg, knowmed systemsleslie l. vadasz, intel corporationstaffmarjory s. blumenthal, directorjane bortnick griffith, interim director, 1998herbert s. lin, senior scientistjerry r. sheehan, senior program officeralan inouye, program officerjon eisenberg, program officerjanet briscoe, administrative associatenicci dowd, project assistantrita gaskins, project assistantdavid padgham, project assistantfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.vcommission on physical sciences,mathematics, and applicationspeter m. banks, erim international, inc., cochairw. carl lineberger, university of colorado, cochairwilliam browder, princeton universitylawrence d. brown, university of pennsylvaniamarshall h. cohen, california institute of technologyronald g. douglas, texas a&m universityjohn e. estes, university of california at santa barbarajerry p. gollub, haverford collegemartha p. haynes, cornell universityjohn l. hennessy, stanford universitycarol m. jantzen, westinghouse savannah river companypaul g. kaminski, technovation, inc.kenneth h. keller, university of minnesotamargaret g. kivelson, university of california at los angelesdaniel kleppner, massachusetts institute of technologyjohn r. kreick, sanders, a lockheed martin companymarsha i. lester, university of pennsylvaniam. elisabeth patcornell, stanford universitynicholas p. samios, brookhaven national laboratorychanglin tien, university of california at berkeleynorman metzger, executive directorfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.viiprefacecomputing technology is widely touted as fast moving. generationsof products and their underlying electronics are introduced at intervals of18 to 24 months, and the number and variety of computer and communicationsbased goods and services are growing. technology and industryexperts believe that the doubledigit rates of improvement experienced inthe last couple of decades can be sustained for computerbased technologies over at least another decade if appropriate investments are made, butit is not clear what those investments should be and on what they depend.similarly, there is little understanding of how to relate a seemingly strongand steady flow of new technology to the slower and more diffuse processes of assimilating new technology into the economy.as described in evolving the highperformance computing and communications initiative to support the nationõs information infrastructure, alsoknown as the brookssutherland report,1 part of the reason for the tremendous advances in information technology since world war ii hasbeen the extraordinarily productive interplay of federally funded university research, federally and privately funded industrial research, and entrepreneurial companies founded and staffed by people who moved backand forth between universities and industry. to a degree that appears1computer science and telecommunications board (cstb), national research council.1995. evolving the high performance computing and communications initiative to support thenationõs information infrastructure. national academy press, washington, d.c.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.viiiprefaceuncommon in all but a few other disciplines, there has been a mix ofpeople and ideas that highlights the limitations of the linear model ofinnovation, which posits that innovation proceeds sequentially from laboratory research to product development to manufacturing and sales. thedynamic nature of the process is evidenced by the fact that many oftodays leading computer technology firms did not exist 20 years ago;many innovative firms that did exist have failed as businesses, but theirinnovations have endured or become the bases for subsequent developments; many familiar products and businesses can be traced back to federally funded research, often conducted at universities; and the ebb andflow of individual firms is fueled by the movement of people amonguniversities, government laboratories, and private companies. understanding this interplay and the ways the private sector has leveragedpublicly funded activities is important for sustaining success in this arena.understanding the changes in these elementssuch as downward pressures on research support in industry and governmentand the potentialimplications of such change is important for directing federal researchand development efforts.the committee and its chargeto better understand these issues, the national science foundation(nsf), along with the association for computing machinery and the institute of electrical and electronics engineers, asked the computer science and telecommunications board (cstb) of the national researchcouncil to initiate a study of lessons to be learned from the history ofinnovation in computing and communications technology. the committee was charged to expand on the analysis in the brookssutherland report to understand the way federal research funding affects the economyand creates new industries. the study was to address questions such asthe following:how did the u.s. computing and communications industriesachieve developmental fertility? on what have they built, and on whatdoes their continuation depend?what are the interactions among players in academia, government,and industry? what is special or unique about these players and interactions compared to other technologies? where are the frictionswherehave the interactions foundered, and why?how can success be calibrated? how often are there unexpectedsuccesses and how well are they tracked? what are notable instances offailure, what were the underlying factors, and what has been learned?how well can we assess causality, as opposed to associations?funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.prefaceixwhat are the key lags, to what are they attributable, and how constant are they? how long does it take for an advance to show up as acommercial productand how long does it take from commercial introduction to market acceptance?to conduct this study, cstb assembled a committee of 13 membersand one special advisor with experience in both computing and communications technology and relevant social sciences. members included (1)individuals involved in developing key computer and communicationstechnologies who had experience in academic research, government research and development, and industrial research, development, and commercialization and (2) economists, historians, sociologists, and others withinsight into the history of technology and the analysis of economic impacts of technology. this was a project in which experience, judgment,and expert interpretation were needed to produce balanced presentationsof events and formulation of lessons. the study was strengthened byinvolving social science experts in relevant forms of data gathering, analysis, and interpretation.the committee met six times between july 1996 and june 1998 to planits course of action, meet with relevant experts, deliberate over its findings, draft its final report, and respond to reviewer comments. in order tocombine a broad understanding of the major trends in computing andcommunications with more indepth knowledge of particular fields andinnovations, the committee took a twopronged approach to the study.first, it examined the broad history of computing and communications,extending from early attempts to design and build computers in the postworld war ii era to the present. the goal was not to document eachinnovation in computing and communications, but rather to identify thekey trends in each historical era and identify the primary governmentactivities that contributed to the industries development. data were gathered on federal and industrial funding levels for research and development in computing technology, as well as investments in research infrastructure and human resources.second, the committee developed case studies of five specific areas:relational databases, the development of the internet and the world wideweb, theoretical computer science, artificial intelligence, and virtual reality. these areas were selected because of the expertise of individual committee members and because they were believed to represent a broadrange of federal roles in the innovation process. the case studies were notintended to be exhaustive histories of the topics investigated, but rather toprovide illustrative examples that could inform the committees attemptto discern lessons regarding the role of federal research funding in computing. as a result, they differ significantly in length, structure, and tone.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.xprefacenevertheless, the committee derived overarching themes from seeminglydiscrete events regarding the relationship between public and privateinvestment, the roles of federal research funding in stimulating innovation, and characteristics of effective government support for research.additional information for the study was gathered through a series ofinterviews with key leaders in federal science and technology policy makingand in computing research: claude barfield (american enterprise institute), gordon bell (formerly with the national science foundation), georgebrown (u.s. house of representatives), mel ciment (national sciencefoundation), fernando corbato (massachusetts institute of technology),tice deyoung (national aeronautics and space administration), howardfrank (defense advanced research projects agency), juris hartmanis(national science foundation), charles holland (air force office of scientific research), anita k. jones (department of defense), john lehmann(national science foundation), john machado (naval electronic systemscommand), steven squires (corporation for national research initiatives),john toole (national coordination office for computing, information,and communications), bruce waxman (university research foundation),gilbert weigand (department of energy), and patrick winston (massachusetts institute of technology). these interviews provided considerable guidance on policy debates surrounding federal funding of researchand served to inform the committees evolving set of conclusions. theinterviews revealed a broad consensus regarding the importance of thefederal government in funding research in computing and communications. regardless of their political affiliations and different roles in theresearch enterprise, the experts interviewed for this study confirmed thevalue of federal funding in computing research, especially federal supportfor university research.this report attempts to summarize, as concisely as possible, the mainconclusions of the study while providing needed justification and support. as such, this report is not a comprehensive history of computing,nor is it a complete accounting of federal involvement in computing.rather, it provides an overview of the innovation process in computingtechnology based on a select set of seemingly representative examplesand buttressed by more comprehensive data. the lessons derived regarding the federal role in computing and communications will, it is hoped,provide relevant guidance for continued efforts in these fields.acknowledgmentsthis report represents the cumulative and cooperative efforts of manypeople. the study committee, itself a blend of technologists, historians,and social scientists, worked tirelessly to overcome differences in culturalfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.prefacexiperspectives and predilections to form a more unified view of the historyof computing and the governments role in supporting it. committeemembers contributions to the case studies and their deliberations formedthe backbone of this project. special thanks are due to david mindell,assistant professor in the science, technology, and society (sts) programat the massachusetts institute of technology (mit), who as a consultantto this project assisted in all aspects of its developmenthelping to definethe study and its scope, participating in committee discussions, and drafting sections of the final document. jed gordon, an undergraduate in thests program at mit, played a key role in collecting and analyzing data onthe contributions of various government organizations to computing research and in writing brief histories of specific federal research programs,such as project whirlwind and project mac. he also analyzed federalstatistics on research funding in computing and educational support ofcomputer science students. hui zeng, a graduate student in computerscience at george mason university, assisted in compiling and analyzinginformation about federal funding of computing research and development of human resources. laura ost, editorconsultant, helped to turnthe original manuscript into a readable text.beyond those directly affiliated with the project were many otherswho contributed valuable information to the report. jennifer sue bond,john jankowski, margaret machen, ronald meeks, and raymond wolf atnsf were instrumental in providing a wide range of data on federal andindustrial support for computing and communications. john lehmann atnsf opened his historical files to the committee, making available awealth of information about nsf programs in computing and communications. david gries at cornell university provided historical data fromthe taulbee surveys, tracking the growth of academic computer scienceactivities. francis narin and anthony breitzman at chi research, inc.,generated special tabulations of patent and citation data in computing.john warwick, a computer science student at carnegie mellon university, built a web crawler to gather data on u.s. patents in artificial intelligence. margaret taylor of carnegie mellon universitys department ofengineering and public policy helped to design the search and to sort andanalyze the data.the committee is also grateful to those who took time to meet with itsmembers and provide relevant briefings: john alic (then with the johnshopkins university school of advanced information studies), paul ceruzzi(national air and space museum), kenneth flamm (brookings institution), john hennessy (stanford university), robert kahn (corporation fornational research initiatives), nils nilsen (stanford university), paulromer (stanford university), ivan sutherland (sun microsystems, inc.),and william wulf (national academy of engineering). their input profunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.xiiprefacevided basic historical material upon which the committee could draw andhelped to create a framework for interpreting historical information.finally, special thanks are due to paul young and john cherniavskyfrom nsf, whose insight, interest, and support made this project possible.thomas hughes, chaircommittee on innovations incomputing and communications:lessons from historyfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.xiiithis report was reviewed by individuals chosen for their diverse perspectives and technical expertise, in accordance with procedures approved by the national research councils (nrcs) report review committee. the purpose of this independent review is to provide candid andcritical comments that will assist the authors and the nrc in making thepublished report as sound as possible and to ensure that the report meetsinstitutional standards for objectivity, evidence, and responsiveness tothe study charge. the contents of the review comments and draft manuscript remain confidential to protect the integrity of the deliberative process. we wish to thank the following individuals for their participation inthe review of this report:robert aaron, at&t bell laboratories (retired),john armstrong, ibm corporation (retired),william aspray, computing research associates,daniel bobrow, xerox palo alto research center,lewis n. branscomb, harvard university,donald chamberlin, ibm almaden research center,lynn conway, university of michigan,stephen cook, university of toronto,john l. hennessy, stanford university,richard herman, university of maryland,robert lucky, bellcore,arthur norberg, university of minnesota,acknowledgment of reviewersfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.xivacknowledgment of reviewersfernando pereira, at&t laboratories research,alex roland, duke university,richard rosenbloom, harvard university,herbert simon, carnegie mellon university,ivan sutherland, sun microsystems, inc.,john swets, bbn corporation, andkeith uncapher, corporation for networking research initiatives.although the individuals listed above provided many constructivecomments and suggestions, responsibility for the final content of thisreport rests solely with the study committee and the nrc.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.xvexecutive summary1part i: the federal role in computing research151introduction17using history as a guide, 18the computing revolution, 23effects on the economy, 24technological roots, 26sources of u.s. success, 27research and technological innovation, 28federal policy toward research funding, 31other mechanisms for federal support of innovation, 33issues related to federal support of research, 34organization of this report, 36notes, 362economic perspectives on publicsupport for research40the economic rationale for public support of civilian r&d, 40information and knowledge as commodities, 41capturing the benefits of research investments, 42technical standards as public goods, 43secrecy and intellectual property rights, 44contentsfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.xvicontentscommon pool problems, patent races, and potentialoverinvestment in r&d, 45the benefits of public support of research, 46direct contributions to the scientific knowledge base, 47indirect effects of governmentsponsored research, 49intellectual assistance, 49research as training, 50notes, 513federal support for research infrastructure52federal research funding, 53levels of federal support, 53sources of federal support, 55comparisons to industrial research funding, 59human resources, 62computer facilities, 71university computing centers, 73departmental computing, 74highperformance computing, 76network infrastructure, 77effects of federal investments in research infrastructure, 79conclusion, 81notes, 814the organization of federal support:a historical review8519451960: era of government computers, 86the governments early role, 87establishment of organizations, 88military research offices, 88national bureau of standards, 89atomic energy commission, 90private organizations, 91observations, 9519601970: supporting a continuing revolution, 96maturing of a commercial industry, 96the changing federal role, 98the advanced research projects agency, 98arpa and information technology, 99arpas management style, 101national science foundation, 10519701990: retrenching and international competition, 107funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.contentsxviicomputer science, computer technology, 107the changing political context, 111science and politics in the 1970s: a changed climate, 111policy for the 1980s: industrial research andcompetitiveness, 112changes in the organization of federal research support, 113changes at arpa, 113very large scale integrated circuits, 115strategic computing initiative, 122making a science, funding a science: the nsf inthe 1970s and 1980s, 124other federal agencies in the 1970s and 1980s, 126sematech, 129highperformance computing, 1301990 and beyond, 132notes, 1345lessons from history136the benefits of federal research investments, 137providing the technology base for growing industries, 138maintaining university research capabilities, 139creating human resources, 140accomplishing federal missions, 141characteristics of effective federal support, 142support for longrange, fundamental research, 142support for efforts to build large systems, 145building on industrial research, 146diverse sources of government support, 147strong program managers and flexible managementstructures, 150industryuniversity collaboration, 152organizational innovation and adaptation, 153concluding remarks, 155notes, 155part ii: case studies in computing research1576the rise of relational databases159background, 160emergence of computerized databases, 160early efforts at standardization, 161emergence of the relational model, 162codds vision, 162funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.xviiicontentssystem r, 163ingres, 164diffusion and commercialization of relational databases, 165lessons from history, 167notes, 1687development of the internet and theworld wide web169early steps: 19601970, 170expansion of the arpanet: 19701980, 173from arpanet to internet, 174local area networks, 174integrated networking, 176standards and management, 176closing the decade, 177the nsfnet years: 19801990, 177emergence of the web: 1990 to the present, 179lessons from history, 181notes, 1828theoretical research: intangiblecornerstone of computer science184machine models: state machines, 186computational complexity, 189verifying program correctness, 191cryptography, 193lessons from history, 196notes, 1979developments in artificial intelligence198the private sector launches the field, 199the government steps in, 201darpas pivotal role, 204success in speech recognition, 205shift to applied research increases investment, 209artificial intelligence in the 1990s, 216lessons from history, 221notes, 22210 virtual reality comes of age226launching the graphics and virtual reality revolution, 228seeding the academic talent pool, 229funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.contentsxixvirtual reality in the private sector: approach withcaution, 233synergy launches the quest for the holy grail, 235graphics hardware: risc technology, 238biomedical applications, 240virtual reality and entertainment: toward a commercialindustry, 242the right mix: virtual reality in the 1990s, 244lessons from history, 247notes, 248bibliography250appendix: committee biographies267funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.xxiboxeses.1why a historical approach?3es.2case studies of innovation in computing61.1drawing conclusions from case studies191.2analogy in technological innovation221.3early industrial efforts in computing294.1project whirlwind and sage924.2project mac and computer timesharing1034.3roots of the personal computer1094.4accomplishments of darpas very large scaleintegrated circuit program1194.5computer engineering at the national science foundation1258.1the formal verification process1928.2rivestshamiradleman cryptography1959.1the development and influence of lisp2029.2dragon systems profits from success in speech recognition2089.3pioneering expert systems2109.4darpas current artificial intelligence program21910.1what is virtual reality?22710.2community building23010.3the rise and fall of atari23310.4real3d emerges from militarycommercial linkage245boxes, figures, and tablesfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.xxiiboxes, figures, and tablesfigureses.1federal and industry funding for computing research,1977199641.1illustrations of the role of governmentsponsoredcomputing research and development203.1federal funding for research in computer science,19761995543.2federal funding for research in electrical engineering,19711995553.3federal funding for scientific research, 19741995563.4federal funding for research in computer scienceby agency, 19761995573.5federal funding for research in electrical engineeringby agency, 19721995573.6federal funding for basic research in computer scienceby agency, 19761995583.7federal funding for basic research in electrical engineeringby agency, 19721995593.8federal and industrial funding for computing research,19771996603.9r&d intensity in computerrelated industries, 19751996633.10bachelors degrees awarded by field, 19661995653.11masters degrees awarded by field, 19661995663.12doctoral degrees awarded by field, 19661995673.13federal funding for university research in computerscience, 19761995693.14federal funding for university research in electricalengineering, 19751995693.15portion of university research funding provided bythe federal government, 19731995703.16computer science and electrical engineering graduatestudents supported by the federal government, 19851996713.17expenditures for research equipment in computer science,19811995743.18expenditures for research equipment in electricalengineering, 19811995758.1simplified state diagram for supervising a telephone line1879.1artificialintelligencerelated patents awarded per year,197619962179.2ph.d. dissertations submitted annually in artificialintelligence and related fields, 195619952209.3number of ph.d. dissertations submitted annually in aiand related fields and in computer science, 19561995221funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.boxes, figures, and tablesxxiiitables1.1worldwide deployment of computers in 1995241.2sales and employment in the information technologyindustry, 1996251.3historical improvement in microprocessors and memories263.1funding for industrial r&d and research in office andcomputing equipment, 19751979613.2funding for industrial r&d and research incommunications equipment, 19651990623.3employment, by sector, for new ph.d. recipients incomputer science and engineering, 19701995683.4university expenditures for computing equipment,maintenance, and operations (in millions of dollars),1988723.5national science foundation obligations for institutionalcomputing services (in thousands of dollars)733.6national science foundation expenditures on thecoordinated experimental research and computingresearch equipment programs (in millions of dollars),19771985763.7authorship and source of financial support for computerrelated papers cited in u.s. patents granted in 19931994804.1computing and related equipment as a share of thenational economy1114.2representative vlsi technologies and resultingcommercial products1184.3growth in the national science foundations computerand information sciences and engineering directoratebudget (millions of dollars), 198719961279.1total federal funding for artificial intelligence research(in millions of dollars), 198419882159.2federal funding for basic research in artificialintelligence by agency (in millions of dollars), 198419882159.3federal funding for applied research in artificialintelligence by agency (in millions of dollars), 198419882159.4leading holders of patents related to artificialintelligence, 1976199721810.1select alumni of the university of utahs computergraphics program231funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.1executive summaryat a time when the u.s. style of competitive market capitalism attracts the worldõs attentionñeven its envyñand u.s. computer firmsdominate the global marketplace, it is difficult to recall and acknowledgethat the federal government has played a major role in launching andgiving momentum to the computer revolution, which now takes pride ofplace among the nationõs recent technological achievements. federalfunding not only financed development of most of the nationõs early digital computers, but also has continued to enable breakthroughs in areas aswide ranging as computer timesharing, the internet, artificial intelligence,and virtual reality as the industry has matured. federal investment alsohas supported the building of physical infrastructure needed for leadingedge research and the education of undergraduate and graduate studentswho now work in industry and at academic research centers.the computer revolution is not simply a technical change; it is asociotechnical revolution comparable to an industrial revolution. thebritish industrial revolution of the late 18th century not only broughtwith it steam and factories, but also ushered in a modern era characterized by the rise of industrial cities, a politically powerful urban middleclass, and a new working class. so, too, the sociotechnical aspects of thecomputer revolution are now becoming clear. millions of workers areflocking to computingrelated industries. firms producing microprocessors and software are challenging the economic power of firms manufacturing automobiles and producing oil. detroit is no longer the symboliccenter of the u.s. industrial empire; silicon valley now conjures up visionsfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.2funding a revolutionof enormous entrepreneurial vigor. men in boardrooms and gray flannelsuits are giving way to the casually dressed young founders of startupcomputer and internet companies. many of these entrepreneurs had theirearly handson computer experience as graduate students conducting federally funded university research.as the computer revolution continues and private companies increasingly fund innovative activities, the federal government continues to playa major role, especially by funding research. given the successful historyof federal involvement, several questions arise: are there lessons to bedrawn from past successes that can inform future policy making in thisarea? what future roles might the government play in sustaining theinformation revolution and helping to initiate other technological developments? this report reviews the history of innovation in computing(and related communications technologies) to elucidate the role the federal government has played by funding computing research and to identify factors that have contributed to the nationõs success in this field.1 itdraws on a series of case studies that trace the lineage of innovations inparticular subdisciplines of computing and on a more general historicalreview of the industry since world war ii. the lessons derived from thisexamination are intended to guide ongoing efforts to shape federal policyin this field (box es.1).governmentindustryuniversity interactioninnovation in computing stems from a complementary relationshipamong government, industry, and universities. in this complex arrangement, government agencies and private companies fund research that isconducted primarily in university and industry research laboratories andis incorporated into myriad new products, processes, and services. whilethe contributions of industry to the computing revolution are manifest inthe range of new products, processes, and services offered, those of thefederal government are harder to discern. nevertheless, federal fundingof major computing initiatives has often contributed substantially to thedevelopment and deployment of commercial technologies. commercialdevelopments, similarly, have contributed to government endeavors.the federal government has played a critical role in supporting theresearch that underlies computerbased products and services. from lessthan $10 million in 1960, federal funding for research in computer scienceclimbed to almost $1 billion in 1995. federal expenditures on research inelectrical engineering (which includes semiconductor and communications technologiesñnecessary underpinnings for computing) have fluctuated between $800 million and $1 billion since the 1970s. such fundinghas constituted a significant fraction of all research funds in the computfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.executive summary3ing field (figure es.1). the vast majority of this funding has been awardedto industry and university researchers, where it has supported innovativework in computing and, to a larger extent, communications (see chapter3 for detailed information on spending patterns).federal research funding plays an important role in supporting university efforts in computing. federal support has constituted roughly 70percent of total university research funding in computer science and electrical engineering since 1976. this funding has had several effects. first,it has promoted advances in fields such as computer graphics, artificialintelligence, and computer architecture: algorithms for rendering threedimensional graphics images, expert systems for assisting in drug design,and timeshared computing systems all derive from federally funded university research. beyond these direct contributions to the technologybase, federal funding for universities has had other benefits as well. it hasplayed a critical role in educating students in the computing field. incomputer science departments at universities such as the massachusettsbox es.1why a historical approach?science and technology policy issues are usually approached in an analyticaland quantitative way that projects the future from the present by extrapolating fromquantitative data. a historical approach, as used in this report, provides a differentperspective. history offers empirical evidence of the success and failure of pastpolicies and allows patterns to be discovered that can inform future decisions. itallows analogies to be drawn between events that occurred decades apart but thatmay be applicable in the future. furthermore, historical narrative can accommodatemessy complexity more easily than can a tightly structured analytical essay, and itallows reflection on longterm process development and evolution. the case studiesin this report present finely nuanced accounts that convey the ambiguities andcontradictions common to reallife experiences.of course, history is limited in its ability to serve as a guide to the future. historycannot suggest what would have happened if circumstances had changed in thepast. for example, history can show the influence of federal funding on historicalinnovations in computing, but it cannot suggest what directions might have beentaken without federal support. in addition, teasing out lessons from history that caninform the future is a difficult task. past outcomes are often tied to specific circumstances. the success or failure of specific research programs, for example, may beinfluenced as much by the particular people involved as by the amount of fundingavailable. the case studies presented in this report attempt to overcome some of thelimitations of history as a guide by examining events that occurred at various pointsin time and identifying lessons that many, if not all, of the cases offer. in this way,they can contribute to judgments about basic policies that are effective in differentcontexts.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.4funding a revolutioninstitute of technology (mit), stanford university, the university ofcalifornia at berkeley (ucberkeley), and carnegie mellon university,over half of all graduate students receive financial support from thefederal government, mostly in the form of research assistantships. inaddition, most of the funding used by academic computer science andelectrical engineering departments to purchase research equipment comesfrom federal agencies. by placing computing equipment in engineeringschools and universities, the government has made possible handsonlearning experiences for countless young engineers and scientists and hasenabled university researchers to continue their work.the effects of federal support for computing research are difficult toquantify but pervasive. patent data, although a limited indicator of inno05001000150020002500300019771978197919801981198219831984198519861987198819891990199119921993199419951996yearmillions of constant 1995 dollarsindustry researchfederal researchfigure es.1 federal and industry funding for computing research, 19771996.industry research, as shown, consists of companyfunded research in the computing and office equipment industry; it does not include companyfunded research in other computingrelated industries such as communications equipment,semiconductors, or computing and communications services. governmentfundedresearch, as shown, consists of total federal funding for research in computerscience. industry research data for 1978, 1980, 1982, 19851987, and 1989 wereestimated from data on total research and development expenditures and fromthe ratio of expenditures for research to expenditures for research and development in years for which actual data were available.source: federal research funding from nsf (1998b), table 25; industry researchfunding compiled from the 19791998 editions of the annual national sciencefoundation report research and development in industry.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.executive summary5vation, provide strong evidence of the links between governmentsupported research and innovation in computing. more than half of thepapers cited in computing patent applications acknowledge governmentfunding (see chapter 3).2 more specific evidence of the value of federallyfunded research derives from a close examination of particular innovations. each of the major areas examined in the five case studies presentedin part ii of this reportñrelational databases, the internet, theoreticalcomputer science, artificial intelligence, and virtual realityñbenefitedfrom federal research funding (box es.2). such funding provided a meansfor sustaining research in universities and industry and complementedresearch expenditures by industry.lessons from historywhy has federal support been so effective in stimulating innovationin computing? although much has depended on the unique characteristics of individual research programs and their participants, several common factors have played an important part. primary among them is thatfederal support for research has tended to complement, rather than preempt, industry investments in research. effective federal research hasconcentrated on work that industry has limited incentive to pursue: longterm, fundamental research; large systembuilding efforts that require thetalents of diverse communities of scientists and engineers; and work thatmight displace existing, entrenched technologies. furthermore, successfulfederal programs have tended to be organized in ways that accommodatethe uncertainties in scientific and technological research. support forcomputing research has come from a diversity of funding agencies; program managers have formulated projects broadly where possible, modifying them in response to preliminary results; and projects have fosteredproductive collaboration between universities and industry. the lessonsbelow expand on these factors. the first three lessons address the complementary nature of government and industrysponsored research; the finalfour highlight elements of the organizational structure and managementof effective federally funded research programs. greater elaboration ofthese lessons is provided in chapter 5 of this report.1. government supports longrange, fundamental research that industrycannot sustain.federally funded programs have been successful in supporting longterm research into fundamental aspects of computing, such as computergraphics and artificial intelligence, whose practical benefits often takeyears to demonstrate. work on speech recognition, for example, whichfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.6funding a revolutionbox es.2case studies of innovation in computingthe case studies contained in chapters 6 though 10 of this report provide detailed accounts of innovation in particular areas of computing: relational databases,the internet, theoretical computer science, artificial intelligence, and virtual reality.representing a range of technologies and time frames, the cases demonstrate significant interaction among industry, universities, and government in developing andcommercializing new computing technology. the lessons learned from these caseshighlight the variation and similarities in the interactions, as well as key elements ofthe innovation process. the following brief summary of the case studies includeslimited examples of the results of federal investments in research. readers aredirected to the full case studies for a more complete description of federal involvement in these areas.relational databasesdevelopment of relational database technologyñnow a billiondollar industrydominated by u.s. companies such as informix, sybase, ibm, and oracleñrelied onthe complementary efforts of industry and governmentsponsored academics. although originating within ibm, relational database technology was not rapidly commercialized because it competed with ibmõs existing database products. the national science foundation (nsf) funded the ingres project at the university ofcalifornia at berkeley, which refined and promulgated the technology, thus spreading expertise and rekindling market interest in relational databases. many of thecompanies now producing relational databases have on their staffsñor were foundedbyñparticipants in ingres.internetdevelopment of the internet grew largely out of governmentsponsored research,development, and deployment programs. building on research conducted by paulbaran and donald davies, the defense advanced research projects agency(darpa, during certain periods called arpa) funded the development of a packetswitched network, the arpanet, by industry and academia. it subsequently supported creation of the protocols used for interconnecting networks across the internet. to further its goals of supporting research and educational infrastructure, nsffunded development of networks for research and educational uses and, in effect,laid the groundwork for todayõs internet. the world wide web and browser technology currently used to navigate the internet were devised by timothy bernersleeat cern and marc andreesen, then a student at the nsfsponsored national centerfor supercomputing applications at the university of illinois at urbanachampaign.theoretical computer sciencetypically viewed as the province of academia, theoretical computer science hasbenefited from the efforts of both industry and university researchers. although someadvancesñsuch as number theory and cryptologyñhave translated directly intopractice, many others (such as finite state machines and complexity theory) haveentered engineering practice and education more subtly, influencing the way researchers and product developers approach and think about problems. progress intheory has both informed practice and been driven by practical developments thathave challenged or outpaced existing concepts.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.executive summary7was begun in the early 1970s (some started even earlier), took until 1997 togenerate a successful product for enabling personal computers to recognize continuous speech. similarly, fundamental algorithms for shadingthreedimensional graphics images, which were developed with defensefunding in the 1960s, entered consumer products only in the 1990s, thoughthey were available in higherperformance machines much earlier. thesealgorithms are now used in a range of products in the health care, entertainment, and defense industries.industry does fund some longrange work, but the benefits of fundamental research are generally too distant and too uncertain to receivesignificant industry support. moreover, the results of such work are generally so broad that it is difficult for any one firm to capture them for itsown benefit and also prevent competitors from doing so (see chapter 2).not surprisingly, companies that have tended to support the mostfundamental research have been those, like at&t corporation and ibmartificial intelligencework in artificial intelligence broadly addresses capabilities for enabling machines (computers) to exhibit characteristics of human intelligence, such as understanding language, learning, and problem solving. support for research in artificialintelligence (ai) over the past three decades has come largely from government agencies, such as darpa, nsf, and the office of naval research (onr). firms thatinitiated ai research programs in the 1960s scaled back their programs once theyrealized that commercial applications lay many years in the future. continued federal investments allowed a number of advances in areas such as expert systems,speech recognition, and image processing. for example, speech recognition systems, which had been the focus of darpa funding in the early 1970s, finally enteredthe marketplace in the mid1990s. many other ai technologies have been commercialized and embedded into a range of new products.virtual realityresearch in virtual reality attempts to develop technologies for creating computergenerated environments that are indistinguishable from real ones. innovation invirtual reality stems from the convergence of advances in numerous interrelatedfields, such as computer graphics, psychology, computer networking, robotics, andcomputer hardware. it has been both pushed by technological advances in theseunderlying areas and pulled by creative attempts to devise particular applications,such as flight simulators, entertainment, virtual surgery, engineering design, and toolsfor molecular modeling. much of the underlying research has been conducted byuniversities, with federal support from agencies such as darpa, nsf, and the national aeronautics and space administration, but industry has played an importantrole in commercializing technologies and identifying key research needs. interdisciplinary research efforts have been the norm in this field, as exemplified by the collaborative research effort between the computer graphics laboratory at the university ofnorth carolina at chapel hill and hewlettpackard.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.8funding a revolutioncorporation, that are large and have enjoyed a dominant position in theirrespective markets. as the computing industry has become more competitive, even these firms have begun to link their research more closelywith corporate objectives and product development activities. companiesthat have become more dominant, such as microsoft corporation andintel corporation, have increased their support for fundamental research.2. government supports large systembuilding efforts that have advancedtechnology and created large communities of researchers.in addition to funding longterm fundamental research, federal programs have been effective in supporting the construction of large systemsthat have both motivated research and demonstrated the feasibility ofnew technological approaches. the defense advanced research projectsagencyõs (darpaõs) decision to construct a packetswitched network(called the arpanet) to link computers at its many contractor sitesprompted considerable research on networking protocols and the designof packet switches and routers. it also led to the development of structures for managing large networks, such as the domain name system, anddevelopment of useful applications, such as email. moreover, by constructing a successful system, darpa demonstrated the value of largescale packetswitched networks, motivating subsequent deployment ofother networks, like the national science foundationõs nsfnet, whichformed the basis of the internet.efforts to build large systems demonstrate that, especially in computing, innovation does not flow simply and directly from research, throughdevelopment, to deployment. development often precedes research, andresearch rationalizes, or explains, technology developed earlier throughexperimentation. hence attempts to build large systems can identify newproblems that need to be solved. electronic telecommunications systemswere in use long before claude shannon developed modern communications theory in the late 1940s, and the engineers who developed the firstpacket switches for routing messages through the arpanet advancedempirically beyond theory. building large systems generated questionsfor research, and the answers, in turn, facilitated more development.much of the success of major systembuilding efforts derives fromtheir ability to bring together large groups of researchers from academiaand industry who develop a common vocabulary, share ideas, and createa critical mass of people who subsequently extend the technology. examples include the arpanet and the development of the air forceõssemiautomatic ground environment (sage) project in the 1950s. involving researchers from mit, ibm, and other research laboratories, thesage project sparked innovations ranging from realtime computing tofunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.executive summary9core memories that found widespread acceptance throughout the computer industry. many of the pioneers in computing learned throughhandson experimentation with sage in the 1950s and early 1960s.3 theysubsequently staffed the companies and laboratories of the nascent computing and communications revolution. the impact of sage was feltover the course of several decades.3. federal research funding has expanded on earlier industrial research.in several cases, federal research funding has been important in advancing a technology to the point of commercialization after it was firstexplored in an industrial research laboratory. for example, ibm pioneered the concept of relational databases but did not commercialize thetechnology because of its perceived potential to compete with moreestablished ibm products. national science foundation (nsf)sponsoredresearch at ucberkeley allowed continued exploration of this conceptand brought the technology to the point that it could be commercializedby several startup companiesñand moreestablished database companies (including ibm). this pattern was also evident in the development ofreduced instruction set computing (risc). though developed at ibm,risc was not commercialized until darpa funded additional researchat ucberkeley and stanford university as part of its very large scaleintegrated circuit (vlsi) program of the late 1970s and early 1980s. avariety of companies subsequently brought riscbased products to themarketplace, including ibm, the hewlettpackard company, the newlyformed sun microsystems, inc., and another startup, mips computersystems. for both relational databases and vlsi, federal funding helpedcreate a community of researchers who validated and improved on theinitial work. they rapidly diffused the technology throughout the community, leading to greater competition and more rapid commercialization.4. computing research has benefited from diverse sources of governmentsupport.research in computing has been supported by multiple federal agencies, including the department of defense (dod)ñmost notably the defense advanced research projects agency and the military servicesñthenational science foundation, national aeronautics and space administration (nasa), department of energy (doe), and national institutes ofhealth (nih). each has its own mission and means of supporting research. darpa has tended to concentrate large research grants in socalled centers of excellence, many of which over time have matured intosome of the countryõs leading academic computer departments. thefunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.10funding a revolutionoffice of naval research (onr) and nsf, in contrast, have supportedindividual researchers at a more diverse set of institutions. they haveawarded numerous peerreview grants to individual researchers, especially in universities. nsf has also been active in supporting educationaland research needs more broadly, awarding graduate student fellowships and providing funding for research equipment and infrastructure.each of these organizations employs a different set of mechanisms tosupport research, from fundamental research to missionoriented researchand development projects, to procurement of hardware and software.such diversity offers many benefits. it not only provides researcherswith many potential sources of support, but also helps ensure explorationof a diverse set of research topics and consideration of a range ofapplications. darpa, nasa, and nih have all supported work in expert systems, for example, but because the systems have had differentapplicationsñdecision aids for pilots, tools for determining the structureof molecules on other planets, and medical diagnosticsñeach agency hassupported different groups of researchers who tried different approaches.perhaps more importantly, no single approach to investing in research is by itself a sufficient means of stimulating innovation; each playsa role in the larger system of innovation. different approaches work inconcert, ensuring continued support for research areas as they passthrough subsequent stages of development. organizations such as nsfand onr often funded seed work in areas that darpa, with its largercontract awards, later magnified and expanded. darpaõs project mac,which gave momentum to timeshared computing in the 1960s, for example, built on earlier nsfsponsored work on mitõs compatible timesharing system. conversely, nsf has provided continued support forprojects that darpa pioneered but was unwilling to sustain after themajor research challenges were resolved. for example, nsf funds themetal oxide semiconductor implementation service (mosis)ña systemdeveloped at xerox parc and institutionalized by darpa that providesuniversity researchers with access to fastturnaround semiconductormanufacturing services. once established, this program no longermatched darpaõs mission to develop leadingedge technologies, but itdid match nsfõs mission to support university education and researchinfrastructure. similarly, nsf built on darpaõs pioneering research onpacketswitched networks to construct the nsfnet, a precursor totodayõs internet.5. strong program managers and flexible management structures haveenhanced the effectiveness of computing research.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.executive summary11research in computing, as in other fields, is a highly unpredictableendeavor. the results of research are not evident at the start, and theirmost important contributions often differ from those originally envisioned. few expected that the navyõs attempt to build a programmableaircraft simulator in the late 1940s would result in the development of thefirst realtime digital computer (the whirlwind); nor could darpa program managers have anticipated that their early experiments on packetswitching would evolve into the internet and later the world wide web.the potential for unanticipated outcomes of research has two implications for federal policy. first, it suggests that measuring the results offederally funded research programs is extremely difficult. projects thatappear to have failed often make significant contributions to later technology development or achieve other objectives not originally envisioned.furthermore, research creates many intangible products, such as knowledge and educated researchers whose value is hard to quantify. second,it implies that federal mechanisms for funding and managing researchneed to recognize the uncertainties inherent in computing research and tobuild in sufficient flexibility to accommodate midcourse changes andrespond to unanticipated results.a key element in agenciesõ ability to maintain flexibility in the pasthas been their program managers, who have responsibility for initiating,funding, and overseeing research programs. the funding and management styles of program managers at darpa during the 1960s and 1970s,for example, reflected an ability to marry visions for technologicalprogress with strong technical expertise and an understanding of theuncertainties of the research process. many of these program managersand office directors were recruited from academic and industry researchlaboratories for limited tours of duty. they tended to lay down broadguidelines for new research areas and to draw specific project proposalsfrom principal investigators, or researchers, in academic computer centers. this style of funding and management resulted in the governmentstimulating innovation with a light touch, allowing researchers room topursue new avenues of inquiry. in turn, it helped attract topnotch program managers to federal agencies. with close ties to the field and itsleading researchers, they were trusted byñand trusted inñthe researchcommunity.4this funding style resulted in great advances in areas as diverse ascomputer graphics, artificial intelligence, networking, and computerarchitectures. although mechanisms are clearly needed to ensure accountability and oversight in governmentsponsored research, historydemonstrates the benefits of instilling these values in program managersand providing them adequate support to pursue promising researchdirections.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.12funding a revolution6. collaboration between industry and university researchers has facilitated the commercialization of computing research and maintained itsrelevance.innovation in computing requires the combined talents of universityand industry researchers. bringing them together has helped ensure thatindustry taps into new academic research and that university researchersunderstand the challenges facing industry. such collaboration also helpsfacilitate the commercialization of technology developed in a universitysetting. all of the areas described in this reportõs case studiesñrelationaldatabases, the internet, theoretical computer science, artificial intelligence,and virtual realityñinvolved university and industry participants. otherprojects examined, such as sage, project mac, and very large scale integrated circuits, demonstrate the same phenomenon.collaboration between industry and universities can take many forms.some projects combine researchers from both sectors on the same projectteam. other projects involve a transition from academic research laboratories to industry (via either the licensing of key patents or the creation ofnew startup companies) once the technology matures sufficiently. asthe case studies demonstrate, effective linkages between industry anduniversities tended to emerge from projects, rather than being thrust uponthem. project teams assembled to build large systems included the rangeof skills needed for a particular project. university researchers oftensought out productive avenues for transferring research results to industry, whether linking with existing companies or starting new ones. suchtechniques have often been more effective than explicit attempts to encourage collaboration, many of which have foundered due to the oftenconflicting time horizons of university and industry researchers.7. organizational innovation and adaptation are necessary elements offederal research support.over time, new government organizations have formed to supportcomputing research, and organizations have continually evolved in orderto better match their structure to the needs of the research and policymaking communities. in response to proposals by vannevar bush andothers that the country needed an organization to fund basic research,especially in the universities, for example, congress established the national science foundation in 1950. a few years earlier, the navy foundedthe office of naval research to draw on science and engineering resourcesin the universities. in the early 1950s during an intense phase of the coldwar, the military services became the preeminent funders of computingand communications. the soviet unionõs launching of sputnik in 1957funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.executive summary13raised fears in congress and the country that the soviets had forged aheadof the united states in advanced technology. in response, the u.s. department of defense, pressured by the eisenhower administration, established the advanced research projects agency (arpa, now darpa) tofund technological projects with military implications. in 1962 darpacreated the information processing techniques office (ipto), whose initial research agenda gave priority to further development of computersfor commandandcontrol systems.with the passage of time, new organizations have emerged, and oldones have often been reformed or reinvented to respond to new nationalimperatives and counter bureaucratic trends. darpaõs ipto has transformed itself several times to bring greater coherence to its research effortsand to respond to technological developments. nsf in 1967 establishedthe office of computing activities and in 1986 formed the computer andinformation sciences and engineering (cise) directorate to couple andcoordinate support for research, education, and infrastructure in computer science. in the 1980s nsf, which customarily has focused on basicresearch in universities, also began to encourage joint academicindustrial research centers through its engineering research centers program.with the relative increase in industrial support of research and development in recent years, federal agencies such as nsf have rationalized theirfunding policies to complement shortterm industrial r&d. federal funding of longterm, highrisk initiatives continues to have a high priority.as this history suggests, federal funding agencies will need to continue to adjust their strategies and tactics as national needs and imperatives change. the cold war imperative shaped technological historyduring much of the last halfcentury. international competitivenessserved as a driver of government funding of computing and communications during the late 1980s and early 1990s. with the end of the cold warand the globalization of industry, the u.s. computing industries need tomaintain their high rates of innovation, and federal structures for managing computing research may need to change to ensure that they are appropriate for this new environment.conclusionas this report demonstrates, the federal government has played asignificant role in the development of the computing industry. althoughdifficult to quantify precisely, the returns from federal investments incomputing and communications have been tremendous. many of theleading concepts being exploited todayñfrom virtual reality to theinternetñderive from research funded by federal agencies. as the industry has grown, the role of the government has evolved, but it has refunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.14funding a revolutionmained essential in supporting longterm research and efforts to buildlarge systems. the computing industry has advanced at an astonishingrate, driven by competition and commercial reward. researchñfundedby the government and privatelyñhas made that remarkable progresspossible.policymakers attempting to develop sound science and technologypolicies and promote the continued vitality of the computing industrycan find useful guidance in history. the explorations of meriwether lewisand william clark suggest an analogy. they drew on numerous storiestold by others, including native americans and fur traders, who had tentatively explored the lands west of the mississippi. from these historiesthey imaginatively created with broad brush strokes a picture of the frontier and prepared for the host of contingencies that they might encounter.so, too, can the stories contained in the case studies in this report provideillustrations to help policymakers address the challenges they face as computing enters the next millennium.notes1.a variety of other federal policies have shaped the computer industry andinfluenced computing research. these include enforcement of antitrust laws,patent policy and intellectual property protection more generally, and assistancein developing technical standards. the granting of a monopoly to at&t in thetelephone industry exerted great influence on research in communications. doehas also stimulated advances in highperformance computing through procurement of supercomputers. this report focuses on federal research funding, notbecause these other factors are not important, but because of the range of publicpolicy issues currently surrounding federal research investments (see chapter 1).2.this estimate is based on an analysis of patent citations in the computingfield conducted specifically for this project by francis narin and anthony f.breitzman at chi research, inc., haddon heights, n.j.3.the lessons implicit in the sage project can be compared to the learningexperiences associated with the construction of the erie canal early in the 19thcentury. engineers then referred to the erie as the leading engineering school inthe united states.4.the degree of trust between office or program managers and the researchcommunity was facilitated by the many common bonds they shared. in the 1960sand 1970s, licklider, sutherland, and roberts, for example, all had ties to mit.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.part ithe federal role incomputing researchfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.17the latter part of the 20th century has witnessed a revolution in computing and related communications technology. as earlier eras witnessedtransformations wrought by steam power, internal combustion engines,and electricity, the 1990s have seen the development, elaboration, anddiffusion of a generalpurpose technology that is transforming society.computing technology has infiltrated all corners of society, from theworkplace and the laboratory to the classroom and the home, changingthe way people conduct business, govern, learn, and entertain themselves.the computer revolution is predicated on 50 years of effort by industry, universities, and government. together, these entities have createdan innovation system that has vastly improved the capabilities of computerrelated technologies, from semiconductors to computers, and fromsoftware to data communications networks. realtime, online operatingsystems, graphical user interfaces, the mouse, the internet, highperformance computers, and microprocessors are all offspring of the productiveinteraction among government, universities, and industry in the innovation process. understanding the interplay among industry, government,and universities in developing new computing technology is an important step in framing both public and private policies that will shape futureresearch activities. as the nation attempts to maintain its leadership incomputing, business leaders, policymakers, and university researcherswill need to understand the sources of their past success.this report examines the history of innovation in the field of computing and related communications technologies with emphasis on the role1introductionfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.18funding a revolutionof the federal government in supporting computing research.1 it provides an overview of the federal governmentõs investments in the nationõsresearch infrastructure for computing and, through a series of historicalcase studies, illustrates the ways in which these investments have influenced the field. as such, the report is not a comprehensive history ofcomputing, but rather an attempt to provide insight into the role of federal research funding in the innovation process for computing. it is hopedthat the lessons learned from this report will provide guidance topolicymakers attempting to plot the course of federal research investments over the next several decades.using history as a guidehistorical analysis is one means of informing debates over the role ofthe federal government in computing research. history provides empirical evidence of the success and failure of different policies over time, andit offers evidence from which patterns can be seen and conclusions drawnabout the funding process in particular and innovation in general. examining changes in government support for technology over many decadeshelps eliminate spurious events resulting from shortlived fads, politicaland technical fashions, and individual anomalies. it allows recognition ofthe often long time lags between initial funding of research and its subsequent incorporation into commercial products. similarly, it puts intoperspective the frequently long lag times between the implementation ofpolicies and the realization of their major and lasting effects.case studies are a standard tool of historical analysis, allowing one tomove more deeply into the mix of events, people, and organizations associated with the funding of computer research. case studies provide anintimacy with history akin to that experienced by persons who lived it.they present the messy details of reallife experiences not available inabstract, quantitative analysis. at the same time, case studies are limitedin their analytical capabilities. to some extent, the conclusions learnedfrom case studies are conditioned by the particular cases examined (seebox 1.1 for an example). as economist richard nelson noted after conducting case studies of seven major u.s. industries to derive lessons forfederal policy, broad analogies are difficult to identify and outcomes often are tied closely to special circumstances of the industry, a specifictechnical problem, or the policy approach taken. òit is very hard to teaseout from the historical record clear cut lessons that are applicable to future policy decisions,ó he concluded. nevertheless, he noted that it ispossible to make judgments about the kinds of policies that are feasibleand effective in different contexts (nelson, 1982, p. 454).this report uses a series of case studies, supplemented by a historicalfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.introduction19overview of federal involvement in computing, to derive lessons regarding innovation in computing technology. the cases in this report build onearlier work by the computer science and telecommunications board(cstb, 1995a) that identified the role of federal research funding in stimulating innovation in several areas of computing and communications (figure 1.1). the case studies are not intended to be definitive surveys of thevarious subjects; nor are they necessarily fully representative of the interactions of federal research funding with other elements of the nationõsinnovation system. instead, they are narratives that illustrate the kinds ofinfluences federal research funding has exerted on the innovation systemand that highlight the interactions among government, universities, andindustry. the cases represent a diversity of examples, differing in thetime periods they cover, the technologies they address, and the type ofbox 1.1drawing conclusions from case studiesthe selection of case studies can greatly influence the nature of the conclusionsdrawn from them. for example, examination of the development of voice telephonynetworks would suggest lessons different from those to be found in case studies ofdata networking. a case study of telephony might suggest that computing researchwould witness a decline in government funding and the rise of other types of government support, as well as industry support for research and development. it woulddemonstrate the role of the federal patent system in providing companies and individual inventors a means for protecting their innovations long enough to recoverresearch and development funds, stimulating further expenditures. it would suggestthe possibility of government support forñand industrial inclination towardñmergers, monopolies, and regulation. considering the negative attitude today towardgovernment regulation of private enterprises, the likelihood of a return to regulatedmonopoly seems unlikely. yet, contrary to conventional wisdom, private enterprisehas in the past favored government regulation under certain circumstances. duringthe first quarter of this century, for example, state governments supported the spreadof telephone service through the granting of natural monopolies coupled with government regulation.1 possession of a natural monopoly allowed at&t to levy a supplemental charge on customers that provided funds for research at bell laboratories.1a classic example from the electric power industry involves the commonwealthedison company of chicago asking the state of illinois to grant it a natural monopolyin a large region surrounding chicago. in exchange, in 1914 commonwealth edisonreadily accepted state regulation of price and service. previously, the city of chicago had regulated commonwealth edison and limited the companyõs area of supplyto the city limits. other urban utilities followed a similar policy. this resulted in acascading effect. the natural monopoly allowed the utilities to avoid competitionresulting from duplication of service.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.20funding a revolutionfigure 1.1 illustrations of the role of governmentsponsored computing research and development in the creation of innovative ideas and industries. raid,redundant arrays of inexpensive disks; risc, reduced instruction set computing.source: cstb (1995a), figure 1.2, p. 20.a few examplesctss, multics, bsdunixsds 9940, 360/67, vmssketchpad, utahgm/ibm, lucasfilme&s, sgiarpanet, internetethernet, pup, datakitdecnet, lans, tcp/iplisp machine, stanfordxerox altoapollo, sunsun, sgi, ibm, hpmead/conway, mosismanymanyilliac 4, c.mmp, hpcibm rp3, intelcm1, teradata, t3dberkeleystriping, datameshstar, mac, microsoftberkeley, stanfordibm 801englebart, rochesteralto, smalltalk1965 19701975 198019941985 1990gov't researchindustry researchindustry development$ 1b businesstransfer of ideas or peopletimesharinggraphicsnetworkingworkstationswindowsriscvlsi designraidparallel computingfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.introduction21government involvement. they range from limited discussions of particular projects and programs (such as relational databases and theinternet) to broader discussions of federal support for various fields ofinquiry (such as artificial intelligence and virtual reality).applying historical lessons to future policy making is a difficult exercise, one historians justifiably are reluctant to do. a stock answer is thathistory does not repeat itself, but this response is misleading. from everyday observation, professional historians and others know that comparable, recurring events are embedded in longterm trends and enduringfactors. the impressive ability of statisticians to predict the level of automobile accidents on national holidays, the accurate predictions of trendsin economics and demography, and the longterm forecasts of particularcyclical effects of climate changes on agricultural production and energyconsumption provide examples of the durability of trends and the persistence of circumstances. french historian fernand braudel in his study,the mediterranean and the mediterranean world in the age of philip ii, writespersuasively of the influence of the physical environment upon societyand the resulting slow but perceptible rhythms of social behavior: òahistory in which all change is slow, a history of constant repetition, everrecurring cyclesó (braudel, 1972, pp. 2122).historians use two processes to apply the past to the future: projection and analogy. projection, in a sense, moves the past into the future ina continuous, linear way. it assumes that conditions prevalent in the pastwill continue to exist largely unchanged in the future and that yesterdayõslessons apply equally well to tomorrowõs problems. analogies, on theother hand, presume a discontinuity between present and future. theyassume that the future will not be like the recent past, but may in factresemble the more distant past when circumstances differed. analogiesraise interesting and unorthodox questions that can inform policy makingand business strategy (see box 1.2 for an illustration of analogy in scientific thought), but the art of drawing analogies requires a sensitive touchin choosing what is comparable. analogies can be dramatically misleading if events and trends in the past are wrongly assumed to have arisenfrom conditions and contexts that will repeat themselves in the future.history is replete with examples of poorly applied analogies that resultedin poor decisions.2 thus, great care must be taken in extrapolating fromthe past to the future, and it must be recognized that reasoning throughanalogy may prove erroneous in detail, even if it allows anticipation ofevents and outcomes.clearly, the future of computing will differ from the history of computing because both the technology and environmental factors havechanged. attempts by companies to align their research activities moreclosely with product development processes have influenced the role theyfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.22funding a revolutionbox 1.2analogy in technological innovationanalogies are often used in the process of technological innovation. inventorsuse analogy to help them conceptualize new ideas. edison conceived of the quadruplex telegraph, perhaps the most elegant and complex of his inventions, òalmostentirely on the basis of an analogy with a water system including pumps, pipes,valves, and water wheels.ó1 later, continuing to reason by analogy, he conceived ofthe interaction of existing illuminatinggas distribution systems and the illuminating,incandescentlight system he intended to invent. the analogy stimulated him toinvent a system, not simply an incandescent lamp (friedel et al., 1986, pp. 6364).lee de forest, inventor of the triode vacuum tube, also inclined to analogy. observing under a microscope the flow of minute particles between electrodes in his wireless receiver, he imagined, òtiny ferryboats they were, each laden with its little electric charge, unloading their etheric cargo at the opposite electrode and retracing theirjourneyings or, caught by a cohesive force, building up little bridges, or trees withbranches of quaint and beautiful patternsó (de forest, 1950, p. 119). spurred on byanalogous thinking, he resolved to invent a flaming hotgas (ionized), or incandescentparticle, receiver, a search that culminated in his invention of a gasfilled, threeelement electronic tube (hughes, 1990; aitken, 1985).the emerging history of computer networks also reveals instances of invention byanalogy.2 j.c.r licklider, whose vision of the future of computing inspired theproblem choices and research and development activities of numerous of his contemporaries, opened his 1960 seminal paper, òmancomputer symbiosisó (licklider,1960), with a metaphor:the fig tree is pollinated only by the insect blastophaga grossorum. the larvaof the insect lives in the ovary of the fig tree, and there it gets its food. the treeand the insect are thus heavily interdependent: the tree cannot reproducewithout the insect; the insect cannot eat without the tree; together, they constitute not only a viable but a productive and thriving partnership. this cooperative òliving together in intimate association, or even close union, of twodissimilar organismsó is called symbiosis.3mancomputer symbiosis, he adds, is a subclass of manmachine systems. otherhumanmachine systems use machines as extensions of humans. still others deployhumans to extend machinesñto perform functions, for instance, that cannot yet beautomated. by contrast, mancomputer symbiosis depends on an interactive partnership of man and machine.mit professor john mccarthy, an early contributor to computer timesharing,suggested by analogy the potential of commercialized timesharing and computingutilities. in a 1961 lecture he predicted:if computers of the kind i have advocated [timesharing] become the computers of the future then computing may someday be organized as a public utilityjust as the telephone system is a public utility. . . . the computer utility couldbecome the basis of a new and important industry. (fano, 1979, p. 43)funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.introduction23may play in the innovation process. as the computing industry has grownand the technology has diffused more widely throughout society, government has continued to represent a proportionally smaller portion of theindustry. the lessons contained in this report attempt to discern crosscutting, pervasive themes and patterns regarding federal support for computerrelated research. as such, the report attempts to identify fundamental, enduring trends and relationships that will survive change. it ishoped that they will both help historians better understand developmentof a dynamic industry and provide technologists with a deeper appreciation of the heritage of their trade, as well as assist policymakers in makingmore informed judgments about federal support for computing.the computing revolutionthe united states is clearly a leader in the computing revolution.computing technology has diffused throughout the u.s. economy withfarreaching effects. over 36 percent of households in the united statesowned a personal computer in 1995, a number far exceeding that of othermajor regions of the world (table 1.1). spurred by advances in computafter a successful demonstration of the arpanet in 1972, other computer engineers and scientists saw the analogy. they no longer considered arpanet a research site for testing computer communications but a communications utility comparable to the telephone system. òit was remarkable how quickly all of the sitesreally began to want to view the network as a utility rather than as a research project,óalexander mckenzie, an arpanet pioneer, pointed out.4an analogy drawn between a conventional office and a future electronic oneprovided a metaphoric bridge for ingenious computer scientists and engineers atxerox parc (palo alto research center). in the 1970s they invented the òelectronicoffice,ó which they embodied in the alto computer system. not long afterward theparc group began to use visual analogies to introduce icons into the displays ofpersonal workstations.1edison, theodore m. 1969. òdiversity unlimited: the creative work of thomasa. edison,ó a condensation of a paper given before the mit club of northern newjersey, january 24.2the discussion of the use of metaphors by arpanet/internet pioneers is basedon a chapter on the arpanet in hughes (1998).3licklider quoting the definition for òsymbiosisó in websterõs new internationaldictionary (springfield, mass.: merriam company, 1958), p. 2555.4alexander mckenzie as quoted in an interview conducted by judy oõneill,charles babbage institute, university of minnesota, minneapolis, march 17, 1990.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.24funding a revolutioning power and data communications, government, industry, and homeusers moved onto the internet in record numbers to exchange electronicmail, buy and sell goods and services, gather and disseminate information, and browse the world wide web. recent surveys indicate that some58 million adults in the united states and canada are now online (nielsenmedia research, 1997). computers have become ubiquitous, with microprocessors running desktop and laptop computers, quietly controllingthe operation of aircraft and automobile engines, and adding functionality to common household devices, such as telephones, thermostats, andcoffeemakers.effects on the economythe effects of this revolution on the economy are pervasive. althoughproductivity gains from computing have remained difficult to measurequantitatively,3 the qualitative effects are manifest. many industries, frombanking to insurance to airline reservations, could not operate at currentlevels of activity without computing and communications systems. computerbased devices, such as automated teller machines, have dramatically altered the ways banks operate, and they enable banks to offer arange of new services to customers. electronic commerce is changing theway customers and vendors buy and sell goods. as individuals andbusinesses become more familiar with the technology and industry churnsout more innovative informationtechnology products, it is clear that theinfluence will be felt in ways that cannot yet have been foreseen.u.s. firms have led the computer revolution. companies such asinternational business machines (ibm), intel, and microsoft dominate global markets for computing devices and software. others, such as ciscotable 1.1 worldwide deployment of computers in 1995unitedrest ofworldstateseuropejapanworldpopulation (millions)5,7002644771254,834number of computers(millions)25796541889percentage of worldõscomputers1003721735percentage of regionõspopulation witha computer4.536.511.314.51.8source: petskajuliussen and juliussen (1996).funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.introduction25systems and lucent technologies, are leaders in the data communications field. computerrelated manufacturing represents a significant fraction of the nationõs economy. sales of computers, telecommunicationsequipment (including data networking equipment), software, and semiconductors by u.s. firms topped $280 billion in 1996 (table 1.2), a figurethat has grown at an average rate of almost 10 percent a year since 1960.taking into account the changing prices of informationprocessing equipment of equivalent performance, annual expenditures on informationprocessing equipment grew at an average pace of 9.7 percent per year inreal terms from 1970 to 1994. the corresponding figure for investments incomputers and peripheral equipment (monitors, disk drives, and so forth)increased at a rate of 27.5 percent per year (sichel, 1997, table 4.1). employment in these manufacturing industries stood at over 1 million in 1996,representing 6 percent of the total u.s. manufacturing workforce.related service industries have also blossomed. computer and dataprocessing firms generated close to $150 million in revenues in 1996, withrevenues from domestic telecommunications services climbing from $10billion to $320 billion between 1960 and 1996.4 employment in u.s. communications services and computer and dataprocessingservices companies topped 2.4 million in 1996 (u.s. department of labor, 1997, tableb12).the u.s. department of labor predicts that demand for information technology workers in all sectors of the economy will grow by 95,000 jobsannually between 1994 and 2005, with systems analysts posting the largest gains and the service sector absorbing most of these workers.5table 1.2 sales and employment in the information technologyindustry, 1996sales revenuesindustry sector(in billions of dollars)employmentit manufacturing computing and office equipment111254,700 communications equipment65263,000 softwarea36215,900 semiconductors 68257,000it services computing and data processing1441,037,300 communications services3221,404,000aincludes prepackaged software only, standard industrial classification (sic) code.source: sales revenues for multiple industries from bureau of the census (1997).employment data from u.s. department of labor (1997), table b12.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.26funding a revolutiontechnological rootsthe computing revolution is predicated on a series of technologicaladvances made since the end of world war ii. between 1945 and 1995,the power of computing devices increased at an exponential rate. whereasthe earliest largescale electronic computing machine in the united states,the electronic numerical integrator and computer (eniac), operated ata speed of 5,000 operations per second, the standard desktop computer in1995 operated at nearly 100 million instructions per second, and the fastestsupercomputers operated at 1 trillion operations per second. driving muchof this improvement were advances in integrated circuit technology. following the invention of the integrated circuit, as described in patents filedby jack kilby in february 1959 and robert noyce in july 1959, the numberof circuits fabricated on a single silicon chip has doubled every 18 to 24monthsña phenomenon known as mooreõs law.6 such advances enablesimilar advances in the processing speed of computers and in storagecapacity (or memory) that, over time, have a significant, cumulative effect. between 1971 and 1995, the speed of a standard, generalpurposemicroprocessor increased more than 7,000fold, from 60,000 to 440 millioninstructions per second, and the storage capacity of a dynamic randomaccess memory chip swelled from 4,000 to 256 million bits (table 1.3).concomitant reductions in the priceperformance ratios of computers,integrated circuits, and related devices have facilitated their diffusion.7other technological advances have allowed computing systems totake better advantage of the growing capability of microelectronic detable 1.3 historical improvement in microprocessors and memoriesyearmicroprocessorspeed (mips)transistorsmemory (bits of dram)197140040.062,3001 k19724 k197480800.606,000197516 k197880860.8029,000198064 k1982802862.70134,000256 k19853866.00275,0001 m19884 m199148613.001,185,000 16 m1993pentium100.003,106,0001995pentium pro440.005,500,000 64 mnote: mips, millions of instructions per second; k, kilobit (1,000 bits); m, megabits(1million bits); dram, dynamic random access memory.source: data from intel (1996); ota (1991).funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.introduction27vices. improvements in the density, cost, and performance of magneticdisk storage devices, for example, have kept pace with advances in integrated circuits, allowing computers (from small personal computers tolarge mainframes) to have rapid access to appropriately large amounts ofstored information at reasonable cost. additionally, advances in systemarchitecture have facilitated the transition from mainframe computers totimeshared minicomputers, personal microcomputers, and laptops connected by local and wide area networks. other architectural innovations,such as reduced instruction set computing and parallel processing, haveincreased overall processing speeds, especially in high and midrangemachines. new programming languages and data structures have facilitated development of applications as well as the storage and retrieval ofinformation. improvements in data communications have further enhanced the capability and utility of computers. development of packetswitched networks, the internet for instance, have allowed for more rapidcommunication of information among an expanding number of computers. such innovation has increased the effectiveness of computing systems in a variety of personal, business, and government activities.sources of u.s. successthat the united states should be the leading country in computingand communications was not preordained. early in the industryõs formation, the united kingdom was a serious competitor. the united kingdom was the home of the difference engine and later the analytical engine, both of which were programmable mechanical devices designedand partially constructed by charles babbage and ada, countess oflovelace, in the 19th century. basic theoretical work defining a universalcomputer was the contribution of alan turing in cambridge just beforethe start of world war ii. the english defense industryñwith alanturingõs participationñconceived and constructed vacuum tube computers able to break the german military code. both machines and theiraccomplishments were kept secret, much like the efforts and successes ofthe national security agency in this country. after the war, englishuniversities constructed research computers and developed computerconcepts that later found significant use in u.s. products. other european countries, germany and france in particular, also made efforts togain a foothold in this new technology.how then did the united states become a leader in computing? theanswer is manifold, and a number of external factors clearly played a role.the state of europe, england in particular, at the end of world war iiplayed a decisive role, as rebuilding a country and industry is a moredifficult task than shifting from a war economy to a consumer economy.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.28funding a revolutionthe movement of people among universities, industry, and governmentlaboratories at the end of world war ii in the united kingdom and theunited states also contributed by spreading the experience gained duringthe war, especially regarding electronics and computing. american students and scholars who were studying in england as fulbright scholarsin the 1950s learned of the computer developments that had occurredduring the war and that were continuing to advance.industrial prowess also played a role. after world war ii, u.s. firmsmoved quickly to build an industrial base for computing. ibm andremington rand recognized quite early that electronic computers were athreat to their conventional electromechanical punchedcard business andlaunched early endeavors into computing (box 1.3). over time, fiercecompetition and expectations of rapid market growth brought billions inventure money to the industryõs inventors and caused a flowering ofsmall hightech innovators. rapid expansion of the u.s. marketplace forcomputing equipment created buyers for new computing equipment. therapid postworld war ii expansion of civilianoriented industries andfinancial sources created new demands for data and data processing. insurance companies and banks were at the forefront of installing earlycomputers in their operations. new companies, such as engineeringresearch associates, datamatic, and eckertmauchly, as well as established companies in the data processing field, such as ibm and sperryrand, saw an opportunity for new products and new markets. the combination of new companies and established ones was a powerful force. itgenerated fierce competition and provided substantial capital funds.these factors helped the nation gain an early lead in computing that ithas maintained. while firms from other nations have made inroads intocomputing technologyñfrom memory chips to supercomputersñu.s.firms have continued to dominate both domestic and international markets in most product categories. this success reflects the strength of thenationõs innovation system in computing technology, which has continually developed, marketed, and supported new products, processes, andservices.research and technological innovationinnovation is generally defined as the process of developing and putting into practice new products, processes, or services. it draws upon arange of activities, including research, product development, manufacturing, and marketing. although often viewed as a linear, sequentialprocess, innovation is usually more complicated, with many interactionsamong the different activities and considerable feedback. it can be motivated by new research advances or by recognition of a new market need.8funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.introduction29box 1.3early industrial efforts in computingibm and remington rand were two early industrial pioneers in computing. bothwere engaged in electromechanical punchedcard machines at the close of worldwar ii, with ibm holding 90 percent of the domestic market and remington randhaving most of the rest. between them, they also had most of the much smallerforeign market. ibm chose to build its electronic computer business internally,whereas remington rand purchased two small computer companies that had gottentheir start primarily through government encouragement and funding. the first ofthese small companies was engineering research associates (era), which was established in january 1946 with the active support of military leaders and a promise oflucrative government contracts. initially eraõs only business was to design and buildtopsecret, electronic, codebreaking equipmentðða task that could no longer beaccomplished adequately in the navy once the war ended and technically trainedpeople were free to seek better opportunities elsewhere. by 1947 era had begun todesign generalpurpose, electronic, storedprogram computers because it was concluded that they would be more costeffective than the specialpurpose equipmentera had designed previously. the first of these computers, codenamed atlas, wasdelivered to the government in washington, d.c., in december 1950.the second of these small companies was the eckertmauchly computer corporation (emcc), which was founded in june 1946 by the chief designers of eniac.the business of emcc was to design and manufacture computers (of the von neumann rather than eniac design) and to sell them in the commercial market to displace punchedcard equipment in installations having very large data processingrequirements.short of money, despite a contract with the census bureau for its first largescalecomputer, emcc accepted an offer to be acquired by remington rand in 1950. justover 1 year later in march 1951, the companyõs first univac was accepted by thecensus bureau. one year later, remington rand acquired era, which needed additional funding to enter the commercial market with the computers it had previouslysold only to the government. thus, by 1952 the number two supplier of punchedcard equipment had become the leading supplier of largescale electronic computers.the decision of ibm to build its electronic capability internally was based on thebelief that it had a leadership position in applying electronic computing capability incommercial equipment. using electronic circuits developed in its endicott laboratory as the country was entering world war ii, ibm in 1946 introduced its 603electronic multiplier, the first commercial product to incorporate electronic arithmetic circuits. two years later in the fall of 1948, shipments of the ibm 604 began.containing over 1,400 vacuum tubes, its electronic circuits performed addition, subtraction, multiplication, and division, and could execute up to 60 plugboardcontrolled program steps between reading data from a card and punching out the result.beginning in 1949, the ibm cpc (cardprogrammed electronic calculator) wasshipped to customers. it combined the electronics of the 604 with other equipmentto permit the user to enter both data and program commands on cards. architecturally similar to the eniac, but much smaller, the ibm cpc was sometimes referred toas òa poor manõs eniac.óthus, ibm was first in the marketplace with electronic accounting and computingcontinued on next pagefunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.30funding a revolutionequipment. over 5,000 ibm 604s and 700 cpcs were shipped to customers duringthe first half of the 1950s when remington rand delivered only 14 univacs. ibmhad also begun work on large storedprogram computers to compete with those ofremington rand and of other companies drawn into the field by large governmentresearch, development, and procurement contracts. this growing competition forcedibm to make a major policy change in 1950. previously it had avoided governmentresearch and development contracts in electronics because it did not want to loseproprietary rights to its developments. finally recognizing that its own technical andfinancial resources were insufficient to compete with the countrywide effort the government was orchestrating, it began to seek government research and developmentcontracts in electronic computing. the first such project was the development ofnorc (a supercomputer for the navy), the design and construction of which wasauthorized early in 1951 and completed late in 1954. but without doubt, ibmõs mostimportant government contract put it in close collaboration with the massachusettsinstitute of technologyõs lincoln laboratory (beginning in 1952) to design and manufacture computers for the semiautomatic ground environment (sage) airdefensesystem (see chapter 4).thus began an era of vigorous competition that pitted ibm against remingtonrand, rca, general electric, ncr, honeywell, raytheon, philco, and many others.these companies vied with each other to lead in computerrelated technologies lestthey fall behind in the marketplace. they sought government research contracts,collaborated with government laboratories and agencies, and worked with people inuniversities. technical people published articles on their work in professional societyjournals and spoke at professional meetings where they could also talk informallywith people from other laboratories. although proprietary and classified informationwas carefully guarded by most participants, the information that could be exchangedwas invaluable in moving forward the governmentõs overall research and development effort. government funding of computer research, development, and procurement had dramatically stimulated the rapid growth of the computer industry.source: summarized from pugh (1995).government, universities, and industry all play a role in the innovationprocess.research is a vital part of innovation in computing. in dollar terms,research is just a small part of the innovation process, representing lessthan onefifth of the cost of developing and introducing new products inthe united states, with preparation of product specifications, prototypedevelopment, tooling and equipment, manufacturing startup, andmarketing startup comprising the remainder (mansfield, 1988, p. 1770).9indeed, computer manufacturers allocated an average of just 20 percentof their research and development budgets to research between 1976 and1995, with the balance supporting product development.10 even in thelargest computer manufacturers, such as ibm, research costs are onlybox 1.3 continuedfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.introduction31about 1 to 2 percent of total operating expenses.11 nevertheless, researchplays a critical role in the innovation process, providing a base of scientific and technological knowledge that can be used to develop new products, processes, and services. this knowledge is used at many points inthe innovation processñgenerating ideas for new products, processes, orservices; solving particular problems in product development or manufacturing; or improving existing products, for example.both industry and government fund research activities, with the research itself generally conducted by workers in industry or universitylaboratories. the computer industry has supported several large andhighly productive research facilities, such as ibmõs t.j. watson researchcenter, american telephone and telegraphõs (at&t) bell laboratories,and the xerox palo alto research center (parc). in 1996, computermanufacturers invested about $1.7 billion in research (out of $8.1 billionin total r&d), most of which supported research in their own facilities.12federal research expenditures in computer science totaled roughly $960million in 1995, approximately $350 million of which supported university research, the remainder supporting work in industry and government laboratories (see chapter 3 for a more complete discussion of federal investments in computerrelated research).traditionally, research expenditures have been characterized as either basic or applied. the term òbasic researchó is used to describe workthat is exploratory in nature, addressing fundamental scientific questionsfor which ready answers are lacking; the term òapplied researchó describes activities aimed at exploring phenomena necessary for determining the means by which a recognized need may be met. these terms, atbest, distinguish between the motivations of researchers and the mannerin which inquiries are conducted, and they are limited in their ability todescribe the nature of scientific and technological research. recent workhas suggested that the definition of basic research be expanded to includeexplicitly both basic scientific research and basic technological research(branscomb et al., 1997). this definition recognizes the value of exploratory research into basic technological phenomena that can be used in avariety of products. examples include research on the blue laser, exploration of biosensors, and much of the fundamental work in computer engineering.13federal policy toward research fundingfederal funding for research in computing technologies has beenbased on the rationale first enunciated by vannevar bush in his 1945report to thenpresident truman, science, the endless frontier (bush,1945a). drawing from the nationõs experience in world war ii, bushfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.32funding a revolutionargued that government funding of research was necessary to meet thenationõs needs in defense, health, and the economy in general. industry,he argued, had little incentive to support such work, but would pursuemore applied research geared toward developing new products, processes, and services. this policy set in place new government activitiesthat over the last 50 years have brought new agencies into existence, suchas the national science foundation, and made the u.s. research systemthe envy of the world.cold war policies of the united states aimed at military and politicalcontainment of the soviet union and other communist adversaries provided additional impetus for computing research. defense agencies, suchas the office of naval research, army research office, air force office ofscientific research, and defense advanced research projects agency,invested in computing research with longterm effects on military capabilities (and, indirectly, civilian capabilities). they, and other federal agencies, such as the national security agency (nsa), department of energy(doe), national aeronautics and space administration (nasa), andnational institutes of health (nih), have funded research in computingrelated to their own missions: maintaining national security, developingnew energy sources and nuclear weapons, exploring space, and improving human health. although these agencies have funded projects linkedto their own needs, they have also, to varying degrees, created technicalknowledge or specific products that have been adopted by the commercial marketplace (alic et al., 1992).many mechanisms have been used to support federal contributions tocomputing research. until the mid1980s, most federal support took theform of research grants or contracts. this included federal contracts forproduct development or procurement that, in turn, demanded significantresearch. in each of these arrangements, the government acts as thecustomer for research services, specifying a period of performance andprogram objectives. after 1985, a growing number of programs wereestablished that involved partnerships among government, universities,and industry. such programs tended to pool public and private monies tosupport research in a variety of organizations in industry, universities, andgovernment. in computing, such programs have included (1) sematech,a consortium of semiconductor manufacturers who, with their own andfederal funding, support research and development of semiconductormanufacturing equipment (see chapter 4);14 (2) semiconductor researchcorporation, which pools industry and some federal funding to supportuniversity research in semiconductor technology; (3) engineering researchcenters that require collaborative work between universities and industry on engineering problems of interest to industry (with some federalfunding); (4) cooperative research and development agreements (cradas)funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.introduction33between government laboratories and industry; and (5) extramural cooperative research programs sponsored by the national institute of standardsand technology (nist), such as the advanced technology program.15all of these mechanisms are considered in this report.other mechanisms for federal support of innovationfederal research support has been an important element of thenationõs innovation process, but other government activities have alsohad a significant impact on innovation in computing. federal procurement and standardization efforts, for instance, have also been highly influential. in a number of areas, ranging from semiconductors to supercomputers, governmentõs specialized needs for computing technologiescreated a market for highperformance devices and systems and underwrote the deployment of prototypes and core elements of new technologies in computing. federal procurement of integrated circuits (ic) for theapollo spacecraft and the department of defense (dod) minuteman intercontinental ballistic missile program, for example, was a major impetus for early investments in ic manufacturing capability. the needs ofdoe and its predecessors for highperformance computers for nuclearweapon development and testing drove early markets for supercomputers.in software, the federal government helped drive the marketplace towardthe american national standards instituteõs version of cobol by establishing it as a federal data processing standard. it also supported effortsto set a standard for messagepassing interfaces in parallel computingand supported the high performance fortran forum to extend thefortran programming language to parallel computers (ota, 1995).antitrust actions have also had a significant impact. for example, theantitrust suit brought against ibm in 1952 and settled in 1956 required thecompany (among other things) to sell as well as rent its equipment, tohelp others get into the business of servicing ibm equipment, and tolicense at reasonable rates all of its current and future patents on informationprocessing equipment, including electronic computers. the settlement of the ibm suit and a similar settlement reached with at&t one dayearlier (together with a suit then pending against rca) were described bythe chief of the justice departmentõs antitrust division as òpart of oneprogram to open up the electronics field.ó the manner in which thesesuits were settled facilitated the entry of other companies into the computer industry (pugh, 1995, pp. 254255). similarly, the modified finaljudgment of judge greene created competition in the longlines industry,which, together with computer inquiries i, ii, and iii of the federal communications commission, ensures the lowest prices for lease and resale offunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.34funding a revolutionlonglines carriage in the world. such actions were arguably as importantas research in advancing the telecommunications industry and the internet.issues related to federal support of researchdespite the wide range of influences on innovation in computing,federal research funding deserves particular attention, both because ofthe leverage it exerts over the entire innovation process and because ofthe policy issues currently under debate. throughout the 1990s, changesin the policy environment and in the industry itself have raised newquestions about the role of the federal government in funding computingresearch. the end of the cold war and increasing calls for fiscal stringency in government spending have renewed debates over federal funding of research in the united states, as well as in other industrially advanced nations. to some extent, these debates are not new: the secondhalf of the 20th century has seen numerous reviews of federal policies,programs, and institutions affecting research and education in scienceand engineering. the debates of the 1990s differ in that they represent thefirst time in which fundamental questions are being raised about theinfrastructural commitments and organizational principles that haveguided federal support for research.few challenge the appropriateness of government developing orsponsoring new technologies for its specialized needs, especially regarding national defense, but the arguments for government support for commercially relevant technology are less clear and their effects more controversial. although many believe that fundamental, knowledgeexpandingresearch, whose benefits are openly available through publication, is anappropriate course for government, support is not without question.these questions do not arise just out of budgetary considerations. evenas federal budget deficits have given way to promises of surpluses in thelate 1990s, and proposals have been made for increasing federal researchspending,16 congress initiated a study to determine the proper role ofgovernment in supporting science and engineering.17 such studies attempt to determine how federal monies can be most productively spentand, more generally, what role the federal government should play insupporting research and innovative activities.computing research poses an especially difficult challenge in thisregard. first, advocates of computing research must counter the claimthat computing technology has matured and that the industry is less dependent on fundamental research than it was in the past. why should thegovernment continue to support computing research that will yield onlyincremental improvements in the technology? answering this questionrequires an appreciation of the evolution of computing technology overfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.introduction35the past five decades and an understanding of the role research has playedin promptingñand responding toñnew advances and developments. itrequires an analysis of the ways in which pathbreaking innovations havedramatically altered the landscape of computing over time so that policymakers can appreciate the evolution of the industry as a whole.second, advocates of computing research must demonstrate why thefederal government should continue to support research when a healthyindustry exists that could develop its own technology. why would companies in such a highly competitive and profitable industry not fundcomputing research and develop new technologies on their own? clearly,the computer industry does fund research and does develop new technologies on its own. answering the question more fully requires a betterunderstanding of the interplay among industry, government, and universities in creating and applying new information technologies. federalpolicymakers must determine what role government plays in supportingsuch work and how federal efforts supplement, rather than duplicate ordisplace, those of industry. similarly, policymakers must understandhow federal needs differ from those of the commercial marketplace andhow federal needs can drive industrial innovation.furthermore, policymakers and federal research managers are underincreasing pressure to enhance the effectiveness of government researchprograms. the desire to streamline federal government operations hasled to renewed efforts to improve federal programs and their management, as manifested by passage of the government performance andresults act of 1993. this act requires federal agencies to account forprogram results through integrating strategic planning, budgeting, andperformance measurement.18 for agencies that support scientific andtechnical research, the act implies that methods be developed for measuring the results of federal research investments. doing so requires anunderstanding of the many different ways research influences the innovation process, the time delays involved, and the uncertainties inherent ininnovation. such a task would benefit from an examination of past federal research programs to identify examples of successful federal researchprograms and to provide guidance on the kinds of metrics, if any, thatcould be applied to federally funded research.these are the kinds of issues this report hopes to inform. the lessonscontained in chapter 5 attempt to answer questions about the role offederally funded research in the innovation process, the cycle of innovation, and the results of federal investments. they discuss the effects federally funded research had on industry and society as a whole and identify characteristics of effective federal research programs. with this kindof historical background, policymakers can be better informed to face thechallenges ahead.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.36funding a revolutionorganization of this reportthe remainder of this report examines the history of computing andcommunications to derive lessons for public policy. chapter 2 providesthe economic rationale for federal support of fundamental research. itidentifies the economic properties of research and discusses market failures in the support of research that justify a government role. chapter 3presents an overview of the federal role in creating the research infrastructure that supports the u.s. computing and communications industries. it reviews federal investments in research, education, and researchequipment over the past several decades. chapter 4 reviews the changingorganizational context of computing research in the united states, withan emphasis on federal funding agencies. it describes the changing political, technical, and organizational context in which innovation has occurred and contains minicase studies of particularly important innovationsñsuch as timeshared computing and very large scale integratedcircuitsñidentifying the federal role in each. chapter 5 contains a summary of the lessons learned from this study. it identifies general lessonsabout the role of federal funding in the innovation process and about thestructure of successful research programs. it is hoped that such lessonswill be useful to policymakers, researchers, and research managers. partiiof this report, chapters 6 through 10, contains the case studies that formthe backbone of this report. the cases represent a sampling of importanttechnologies that have had an enduring influence on the computing andcommunications industry and society: relational databases, the internet,theoretical computer science, artificial intelligence, and virtual reality.although by no means comprehensive, they cover a wide range of technologies, degrees of success, and interactions among government, universities, and industry.notes1.this report uses the term òcomputing researchó in a broad sense, to include work in semiconductors, software, and data communications, in additionto computer science and engineering. it does not include all research in telecommunications (such as voice communications), which has a very different historycharacterized by regulated monopolies for telephone services.2.many historians offer as a classic case of a dangerously misleading analogy the assumption that conditions in southeast asia in the 1950s were comparable to those in europe in 1939. this analogy led some policymakers in theunited states to assume that, if the country immediately and directly confrontednorth vietnam, it would result in a compromise like that offered hitler at munichand another largescale war would be averted. for a discussion of poor presidential decisions resulting from the misapplication of analogies, see may (1972).funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.introduction373.the socalled productivity paradox was first noted by economist robertsolow (hence it is often referred to as the solow paradox). explanations haveranged from measurement problems to lag times to the difficulties inherent inintegrating computing into the workplace. nevertheless, recent research suggests a correlation between higher levels of information technology capital andincreased productivity in large companies, especially in companies that use information technology to enhance customer service. see brynjolfsson and hitt (1996).for a discussion of the difficulties in measuring productivity gains associatedwith information technology, see cstb (1995b).4.revenues cited for the telecommunications services industry include bothvoice and data communications over a range of mediañwireline and wireless.data are from bureau of the census (1997).5.employment of systems analysts and computer scientists and engineers isprojected to increase 158 and 142 percent, respectively, in the service industriesbetween 1994 and 2005, versus 26 and 37 percent, respectively, in manufacturingindustries. the number of computer programmers in service industries is expected to grow 37 percent, versus a 26 percent decline in manufacturing. see u.s.department of commerce (1997).6.mooreõs law is named after gordon moore, who first noted the relationship and predicted its continuation in 1964. it is the result of two underlyingprocesses: continuous reductions in the size of individual circuits etched ontocomputer chips through advances in lithography and other manufacturing processes, and increases in the overall size of the integrated circuit (or chip) resultingfrom improvements in processing of silicon wafers and reductions in contaminants. see bashe et al. (1986), pp. 5658.7.between 1960 and 1995, the average unit price of computers sold in theunited states declined from $330,000 to $3,700, helping to propel growth in annual sales from 1,790 units to over 21 million units. see iti (1997). the priceperformance ratio of the typical computer during that time period also declinedby a factor of 100. the u.s. department of commerceõs (bureau of economicanalysis) hedonic price index for computer equipment for 19701994 implies thatthe priceperformance ratio was 1.9 percent of its 1970 level in 1994, an averageannual rate of decrease of 15.3 percent. estimating a slower rate of decline in the1960s of approximately 7.0 percent per year, the price performance ratio in 1994/1995 would have stood at approximately 1/100 of its 1960 level. see sichel (1997)table 41.8.for a more complete overview of the innovation process, see ota (1995).9.this figure has remained remarkably constant over the past several decades. a 1967 report from the department of commerce that relied on data fromthe previous 10 years found that product conception and design accounted for 15to 30 percent of the cost of new product introduction; manufacturing preparation, manufacturing startup, and marketing startup made up the balance. seeu.s. department of commerce (1967), p. 8.10.this estimate was calculated from data contained in the biennial report,national science foundation, research and development in industry. other datafrom the national science foundation show that 40 percent of all research anddevelopment expenditures in the united states supported research in 1995; thefunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.38funding a revolutionremaining 60 percent supported development. see national science board (1996),pp. 45.11.this figure assumes that about 20 percent of ibmõs total r&d expenditures support research. r&d was about 7 percent of ibmõs operating expenses(the sum of the cost of goods sold, r&d, and general, administrative, and salescosts) in 1997. see ibm (1997).12.this figure includes research expenditures for firms in the office andcomputingequipment industry only. it does not include expenditures by firmsin data communications, prepackaged software, or semiconductors. see nsf(1998a), table a24.13.the national science foundation, which is the source of most of the research funding data in this chapter, defines research as òsystematic study directed toward fuller knowledge or understanding of the subject studied.ó it defines development as òsystematic use of the knowledge gained from research,directed toward the production of useful materials, devices, systems, or methods,including design and development of prototypes and processes. it excludes quality control, routine product testing, and production.ó see nsf (1997a).14.in 1997, after 10 years of roughly even funding from industry and government, sematech became fully selfsupported, using only industry funding forits programs.15.nistõs advanced technology program (atp), for example, provides costshared funding to consortia of industry and university participants attempting toconduct precompetitive applied research. funding for the program peaked at$341 million in 1995 and stood at $192.5 million in 1998. funding history isavailable online at <http://www.atp.nist.gov/atp/budget.gif>.16.two bills were introduced in the senate in 1997 and 1998, calling for adoubling of the federal funding for basic scientific and precompetitive engineering research. in october 1997, senators gramm, lieberman, and 18 other cosponsors introduced the national research investment act of 1998. the plan calledfor the doubling of funds over a 10year period. the bill was referred to thesenate committee on labor and human resources. in june 1998, senator fristsubmitted similar legislation entitled the federal research investment act alongwith 26 cosponsors. in addition to doubling federal funding for research to 2.6percent of the federal budget, the bill also called for new evaluation processes toprovide better oversight of funding programs. the bill also called for the president to provide a strategic plan for proposed r&d funds as well as an analysis ofcurrent funds as part of the annual budget. the bill was referred to the committeeon commerce. a companion bill in the house was introduced in august of 1998.17.early in 1997, vernon ehlers, vice chairman of the house science committee, initiated the national science policy study, which was intended to provide a new rationale for federal funding of science. the study examined issues inmathematical and scientific education, funding for r&d, cooperation among government, industry, and the international community. the chair of the sciencecommittee, james sensenbrenner, hoped that the study would justify the proposed funding increases for research that were introduced in the senate in 1997and 1998. the final report was released on september 24, 1998 (committee onscience, 1998).funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.introduction3918.each agency was required to submit by september 1997 strategic plansthat outlined the agencyõs mission statement, goals and objectives, and strategiesit would use to achieve them. the first annual performance plans were due whenthe president submitted the 1998 budget to congress and were to include measures that the agency would use to gauge performance toward meeting thosegoals, and the resources to be used in doing so.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.this chapter examines the economic logic of public subsidies for research and development (r&d) activities in general. the first sectionnotes a number of serious theoretical objections that can be raised againstpublic support of r&d, and it reviews empirical considerations that reaffirm the general presumption that, without government support, marketfailures will result in too few resources being allocated to expanding scientific and technological knowledge. the second section takes up thespecial considerations that bear on the economic case for public supportof exploratory, open researchñthe sort that is usually designated as basicscience, however unsatisfactory that label may be. that discussion emphasizes the complementarities and guidance that such research createsfor privatesector, applicationsoriented, proprietary r&d, rather thanthe possibilities of spinoff products that may compete with results targeted by industrial research organizations. it also highlights the contribution federal funding makes to the education of the scientific and engineering workforce.the economic rationale for public supportof civilian r&dduring the past 30 years, economists have worked out cogent reasonswhy the price system and competitive markets should not be expected todo a good job in producing or distributing knowledge and informationñcertainly not by comparison with marketsõ performance in similarly allo2economic perspectives onpublic support for research40funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.economic perspectives on public support for research41cating resources in more conventional, tangible commodities such as fishor chips (of both the computer and the potato varieties).1 this conclusionrests on the fundamental insight that ideasñespecially ideas tested andreduced to codified scientific and technological information through r&dactivitiesñhave some important attributes found in public goods, goodsthat are widely available to individuals whether or not they paid forthem. correspondingly, they may be better understood by studying otherpublic goods, such as a smogfree environment or defense against nuclearmissile attack.information and knowledge as commoditiesan idea is a thing of remarkable expansiveness: it can spread rapidlyfrom mind to mind without any reduction in its meaning and significancefor those into whose possession it comes. thomas jefferson remarkedupon this attribute, which permits the same knowledge to be used jointlyby many individuals at once: òhe who receives an idea from me, receivesinstruction himself without lessening mine; as he who lights his taper atmine receives light without darkening me. . . .ó economists have pointedout that the potential value of an idea to any individual buyer generallywould not match its value to the social whole. the latter value, however,is not readily expressed in a willingness to pay on the part of all whowould gain from the illuminating idea. once a new bit of knowledge isrevealed by its discoverer(s), some benefits will instantly spill over toothers who are therefore able to share in its possession. commodities thathave the property of expansibility, permitting them to be used simultaneously for the benefit of a number of agents, are sometimes described asbeing nonrival in use: although the cost of the first instance of use of newknowledge may be large, in that it includes the cost of its generation,further instances of its use impose at most a negligible incremental cost.2this formulation ignores the cost of training potential users to be ableto use new information. although it is correct that there can be fixed costsof access to the information, these costs do not invalidate the propositionthat reuse of the information will neither deplete it nor impose furthercosts. it may be costly to teach someone how to read the table of theelements or use differential calculus, but any number of individuals thusinstructed can go on using that knowledge without incurring further costs.the second feature of ideas is that it is difficult, indeed costly, toretain exclusive possession of them while putting them to use. anotherdisadvantage of exclusivity is that results obtained by methods that arenot or cannot be revealed often are felt to be less reliable. of course, it ispossible to keep a piece of information or a new idea secret. producingresults not achievable otherwise, however, indicates the existence of afunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.42funding a revolutionmethod for doing so. even a general explanation of the basis for achieving the observable result jeopardizes the exclusivity of its possession, forknowing that something can be done is an important step toward discovering how it may be done.the dual properties of nonrival usage and costly exclusion of othersfrom possession define what is meant by a pure public good. the termòpublic goodó does not imply that such a commodity cannot be suppliedprivately, nor does it mean that government must produce it. but competitive market processes will not do an efficient job of allocating resources for producing and distributing pure public goods, because suchmarkets work well when the incremental costs and benefits of using acommodity are assigned to the users.capturing the benefits of research investmentsone may see the problem posed by the public goods characteristics ofknowledge by asking how ideas can be traded in competitive markets,except by having aspects of their nature and significance disclosed beforethe transactions are consummated. rational buyers of ideas, no less thanbuyers of coal, and of fish and chips, first want to know something aboutwhat they will be getting for their money. even if the exchange fellthrough, the potential purchaser would enjoy (without paying) some benefits from what economists refer to as transactional spillovers. these occurbecause there may be significant commercial advantages from acquiringeven general information about the nature of a discovery, or an inventionñespecially one that a reputable seller has thought it worthwhile tobring to the attention of people engaged in a particular line of business.this analysis leads to the conclusion that the findings of scientificresearch, being new knowledge, would be seriously undervalued werethey sold directly through perfectly competitive markets. some degree ofexclusivity of possession of the economic benefits derived from ideas isnecessary if the creators of new knowledge are to derive any profit fromtheir activities under a capitalist market system. firms can protect theirknowledge either by seeking patent or copyright protection or by tryingto keep it secret. patents and copyrights provide legally enforceablemeans of protecting knowledge, but they require that inventors publiclydisclose the workings of their inventions (e.g., through a patent application), enabling others to learn from their work and to find alternativemeans of achieving the same end (i.e., reverse engineering a particulardevice). keeping a trade secret (if done effectively) avoids public disclosure, but offers little means for legal recourse if others learn the secret(unless they use unlawful means to do so). industries vary in the degreeto which firms prefer to seek intellectual property protection versus keepfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.economic perspectives on public support for research43ing trade secrets. patents tend to be very important in pharmaceuticals,for example, but less so in computing. regardless of the mechanismchosen for protection, imposing restrictions on how ideas may be usedsaddles society with the inefficiencies that arise when monopolies aretolerated, a point belabored by economists ever since adam smith.technical standards as public goodstechnical standards also demonstrate characteristics of public goodsin that competitive markets often fail to produce them without publicassistance. technical standards acquire economic value for their possessors only as a consequence of being publicly disclosed and jointly used,and they actually grow in utility for the individual user in proportion tothe degree of universality in their adoption. many technological andengineering reference standards, such as those for the thread sizes of nutsand bolts, or the diameter of optical fiber (to permit splicing withoutdegrading the light signal that is propagated through the inner core),benefit buyers and vendors by reducing transactions costs and permittingeconomies of scale in production, especially when they are widely adopted.it should be noted that many other reference standards have emergedfrom the work of scientific communities, such as the units in which electrical current, resistance, and power are measured. the ampere, ohm,and watt, like the joule, angstrom, and countless other precisely specifiedunits, provide a standardized terminology that facilitates scientific communications. they thus enable individuals in a distributed research network to work together (i.e., become interoperable) in the way that compatibility standards enable interacting components of systems to achievegreater functionality. as is the case with other standards, market incentives are weak for producing and distributing scientific reference standards.firms that know of and wish to use technical standards would haveevery incentive to freely share that information, in order to encourageothers to follow suit. hence, an adequate supply of reference standardsand related technologies may not be forthcoming through individual private enterprise, as it may not be worthwhile for any single firm to undertake the cost of designing a reference standard that would be useful forthe industry as a whole and redistributed freely.governmental support for the collaborative development of reference standards, or direct funding of agencies such as national standardsinstitutes that undertake such work, constitutes a mechanism for rectifying this market failure. the alternative of using intellectual propertyrights protection to grant monopoly privileges to private developers ofsuch standards has a perverse effect. it tends to restrict the extent of thefunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.44funding a revolutionstandardsõ use, and therefore deprives even those who do pay themonopolistõs charges, imposed by licensing of patentprotected standards,from enjoying the added benefits that would accrue to all users fromenlarging the user community. this is a generic problem with standardsfor systems, such as telephone and other communication systems, whosevalue to individual subscribers is enhanced by being able to connect with,and be contacted by, a larger number of network members.secrecy and intellectual property rightssome suggest that the problems of incomplete appropriability of benefits from research are overstated, or indeed nonexistent, because industrial secrecy is sufficient to protect against some firms freeriding on ther&d investments of others. but other factors must also be considered.first, one has to consider what costs a strategy of secrecy imposes uponprivate enterprise, and whether such practices can be totally effective inthe face of the mobility of technical personnel and reverse engineering.second, one must look at the matter from the societal viewpoint. on thesupposition that extensive secrecy was a viable policy for firms engagedin research, what is the potential for wasting r&d resources by duplicating research, not to mention potential injury to consumers, were the developers of new products and processes actually able to maintain indefinite secrecy about their research results?the economic logic of providing intellectual property rights in science and technology is that this is a better choice, from the societal standpoint, than secrecy. modern economic analysis has come to view thegranting of patent and copyright monopolies as a sacrifice of shortrunconsumer interests that may be justified by far greater longrun gainsderived from giving creators of new, useful knowledge more secure pecuniary incentives to reveal it rapidly to the public. still, in order to pursueresearch profitably, it is necessary for firms to be able to control the flowof information about work that is in progress, and to build an inventory ofpotential future projects that they can expect to exploit, rather than seeingthese walk out the door with their research personnel.consequently, trade secrecy protections are in this respect complementary to intellectual property protection in the production process fornew research findings whose benefits the firm expects to be able to appropriate. this reinforces the argument made for strengthening intellectualproperty rights in patents, and their enforcement, on the grounds thatreliance upon secrecy is reduced thereby. although the disclosure ofcodified information is augmented by patent systems, so is the inducement to curtail the transmission of tacit knowledge that might reduce thecommercial value of the patents that are being sought.3funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.economic perspectives on public support for research45common pool problems, patent races, andpotential overinvestment in r&dmarket failures do not necessarily result in underinvestment in r&dby profitseeking firms. there also is a potential for excessive privateinvestment when expected private marginal rates of return are not matchedby the marginal social value expected to result from those expenditures.economists have been aware for some time of three main situations inwhich that is likely to be the case, and, although these are thoroughlytreated in the technical literature, they often go unmentioned in publictestimony on the subject. these overinvestment pathologies of marketcompetition through r&d go under the labels of business stealing, common pool problems, and racing behavior.business stealing refers to the situation that arises when research thatis directed toward displacing a competitor from the market entails developing a new product or process that largely duplicates the functions ofthings that already exist, but adds some distinctive additional features.achieving a marginal improvement in quality may be sufficient to capture a rivalõs share of the market, and so may justify the private investment in completely redesigning a product or system to accommodate thenew feature, or to overcome the barriers that an incumbent has erectedthrough secrecy or patent protection. but the social value of the addedfeatures for consumers may be much smaller than the private benefits ofa successful attack on the incumbentõs market position.common pool problems arise because individual competitors mayvie for market position based upon r&d without taking into account theeffect of their entry on the expected returns on the investments that othersare making. not every entrant will get a prize, but every entrant canbelieve in having just as good a chance, if not a better chance, for successthan the others. the result can be duplicative investment in areas inwhich the anticipated prizes are large. the rivalries for certain prescription drug markets in the pharmaceutical industry often are cited as aclassic manifestation of this problem: billions are spent to develop thenext blockbuster therapy, whereas little investment may be devoted toproducts of lesser commercial value.racing behavior is another form of duplicative investment and isdriven by the desire to beat oneõs rival to market. the value of being aweek earlier at the patent office window, or 6 months in advance of competitors to launch a new software application, can be very large in comparison with the incremental social value of letting consumers use theinnovation that much sooner. firms then have an incentive to structuretheir r&d programs for speed, rather than cost minimization. they try tofunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.46funding a revolutionbluff the opposition into quitting by establishing a lead and displaying acommitment to maintain it, whatever the cost.it is clear that such effects, like the appropriability problem, will leadto inefficiencies in the detailed allocation of privatesector research outlays: excess correlation of research strategies, excessively duplicativefunding in some areas, and inattention to other areas in which the marginal social value of new technologies may be quite high. what is lessclear is whether these tendencies to overinvestment are so powerful thatthey destroy the presumption that private markets will, on balance, fail toallocate enough to creating new scientific and engineering knowledge.some recent analytical work suggests this is not the caseñexcept in circumstances where the real interest rate is so high that the value of knowledge spillovers to future generations should, in fact, be heavily discountedby the present generation, and where the impact of additional r&d fundingon the creation of knowledge is rather weak (jones and williams, 1996).thus, the accrued wisdom from the economics profession regardingthe aggregate tendency to underinvestment, and the corresponding casefor government support of research as a stimulus to economic growth,still stands. but these qualifications point to the need for greater attentionto where the publicly funded research is to be directed.the benefits of public support of researchthe development of scientific and technological knowledge is a cumulative process, one that depends on the prompt disclosure of newfindings so that they can be tested and, if confirmed, integrated with otherbodies of reliable knowledge. in this way open science promotes therapid generation of further discoveries and inventions, as well as widerpractical exploitation of additions to the stock of knowledge.the economic case for public funding of what is commonly referredto as basic research rests mainly on that insight, and on the observationthat business firms are bound to be considerably discouraged by thegreater uncertainties surrounding investment in fundamental, exploratoryinquiries (compared to commercially targeted r&d), as well as by thedifficulties of forecasting when and how such outlays will generate asatisfactory rate of return.the proposition at issue here is quantitative, not qualitative. onecannot adequately answer the question òwill there be enough?ó merelyby saying, òthere will be some.ó economists do not claim that withoutpublic patronage (or intellectual property protection), basic research willcease entirely. rather, their analysis holds that there will not be enoughbasic researchððnot as much as would be carried out were individualbusinesses (like society as a whole) able to anticipate capturing all thefunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.economic perspectives on public support for research47benefits of this form of investment. therefore, no conflict exists betweenthis theoretical analysis and the observation that r&dintensive companies do indeed fund some exploratory research into fundamental questions. their motives for this range from developing a capability to monitor progress at the frontiers of science, to identifying ideas for potentiallines of innovation that may be emerging from the research of others, tobeing better positioned to penetrate the secrets of their rivalsõ technological practices (nelson, 1990).nevertheless, funding research is a longterm strategy, and thereforesensitive to commercial pressures to shift research resources toward advancing existing product development and improving existing processes,rather than searching for future technological options. large organizations that are less asset constrained, and of course the public sector, arebetter able to take on the job of pushing the frontiers of science and technology. considerations of these kinds are important in addressing theissue of how to find the optimal balance for the national research effortbetween secrecy and disclosure of scientific and engineering information,as well as in trying to adjust the mix of exploratory and applicationsdriven projects in the national research portfolio.direct contributions to the scientific knowledge basewhen asked to demonstrate the usefulness of exploratory researchthat is undertaken to discover new phenomena, or explain fundamentalproperties of physical systems, scientists often point to discoveries andinventions generated by research projects that turned out to have immediate economic value. many important advances in instrumentation, andgeneric techniques such as the polymerase chain reaction (pcr) and theuse of restriction enzymes in genesplicing, are such examples. these byproducts of the openended search for basic scientific understanding alsomight be viewed as contributing to the knowledge infrastructure requiredfor efficient r&d that might result in exploitable commercial innovations.occasionally, such new additions to the stock of scientific knowledge areof immediate commercial value and yield major economic payoffs.though few and far between, they can have farreaching consequences.there is no dearth of examples testifying to the practical value andcommercial benefits that have followed serendipitously from exploratory,or curiositydriven, scientific inquiries. the chance finding of bacteriasurviving in and near the thermal vents in yellowstone park may beoffered as a striking recent instance of a scientific discovery having animportant and economically valuable field of application that hardly couldbe anticipated. the bacteria in question turned out to be crucial in thedevelopment of the pcr process for replication of specific pieces of dna,funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.48funding a revolutiona generic technique that is now the basis of many commercial biotechnology applications, ranging from diagnostic kits to forensic medicine.what the developers of pcr required was an enzyme that would bestable at high temperatures, and the yellowstone bacteria produced justwhat was needed.the experience of the 20th century also testifies to the many contributions of practical value that trace their origins to large, governmentfundedresearch projects that were focused upon developing new enabling technologies for publicmission agencies (rosenberg, 1987). consider just afew recent examples from the enormous and diverse range that could benoted in this connection: airline reservation systems, packet switchingand the internet communication protocols, the global positioning system, and computer simulation methods for visualization of molecularstructures.at issue is whether a more directed search for the solutions to theseapplied problems would have been less costly and more expedient thanwaiting for scientists with quite different purposes in mind to come upwith these commercially useful findings. indeed, the theme of such spinoff stories is their unpredictability. the argument that the new applications are in some sense free requires that the research program to whichthey were incidental was worth undertaking for its own sake, so thatwhatever else might be yielded as byproducts was a net addition to thebenefits derived. yet, the reason those examples are being cited is theskepticism as to whether the knowledge that was being sought by exploratory science was worth the cost of the public support it required. perhaps this is why the many examples of this kind that scientists havebrought forward seem never enough to satisfy the questioners.the discovery and invention of commercially valuable products andprocesses are seen from the viewpoint of the new economics of science4to be among the rarer of the predictably useful results that flow from theconduct of exploratory, open science. without denying that researchsometimes yields immediate applications around which profitable businesses spring up, it can be argued that those direct fruits of knowledgeare not where the quantitatively important economic payoffs from basicscience are to be found. much more critical over the long run than spinoffs from basic science programs are their cumulative indirect effects inraising the rate of return on proprietary r&d performed by businessfirms. among those indirect consequences, attention should be directednot only to informational spillovers, but to a range of complementaryòexternalitiesó that are generated for the private sector by publicly fundedactivities in the sphere of open science, where research and training aretightly coupled.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.economic perspectives on public support for research49indirect effects of governmentsponsored researchfederally funded r&d provides a number of indirect benefits to private r&d beyond direct transfers of knowledge. these include intellectual assistance that can guide private r&d programs toward potentiallymore productive areas of inquiry and assistance in training researchers.although resources are limited, and research conducted in one field andin one organizational mode is therefore performed at the expense of otherkinds of r&d, exploratory science and academic engineering researchactivities support commercially oriented and missiondirected researchthat generates new production technologies and products. as such, public support of research in many ways complements, rather than competeswith, private r&d efforts.intellectual assistancefirst among the sources of this complementary relationship is theintellectual assistance that fundamental scientific knowledge (even thatderiving from contributions made long ago) provides to applied researchersñwhether in the public or private sector. from the expanding knowledge base it is possible to derive time and costsaving guidance as to howbest to proceed in searching for ways to achieve some prespecified technical objectives. sometimes this takes the form of reasonably reliable guidance as to where to look first, and much of the time it takes the form ofvaluable instructions as to where it will be useless to look. one effect thishas is to raise the expected rates of return and reduce the riskiness ofinvesting in applied r&d. gerald holton, a physicist and historian ofscience at harvard university, recently has remarked that if intellectualproperty laws required all photoelectric devices to display a label describing their origins, òit would list prominently: ôeinstein, annalen der physik17 (1905), pp. 132148.õó such credits to einstein also would have to beplaced on many other practical devices, including all lasers.the central point that must be emphasized here is that, over the longrun, the fundamental knowledge and practical techniques developed inthe pursuit of basic science serve to keep applied r&d as profitable aninvestment for the firms in many industries as it has proved to be, especially during the past halfcentury. in this role, modern science continuesin the tradition of the precious, if sometimes imprecise, maps that guidedparties of exploration in earlier eras of discovery, and in that of the geological surveys that are still of such value to prospectors searching forburied mineral wealth.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.50funding a revolutionresearch as traininga second and no less important source of the complementary relationship between public and private research is the nexus between university research and training. the profitability of corporate r&d is closelytied to the quality of the young researchers who are available for employment. seen from this angle, government funding of open exploratoryscience in the universities today is subsidizing the r&d performed by theprivate business sector. properly equipped research universities haveturned out to be the sites of choice for training the most creative and mostcompetent young scientists and engineers, as many a corporate directorof research well knows. this is why graduates and postdoctoral studentsin those fields are sent or find their own way to university laboratories inthe united states. it explains why businesses participate in (and sponsor)industrial affiliates programs at research universities. it also is part of thereason for u.s. industrial research corporationsõ broadly protective stancein regard to the federal budget for scientific research. acknowledgmentof it has had a great deal to do with the recent announcement by thejapanese government of a dramatic reversal of its former policies and theinitiation of a vast program of support for universitybased research.a key point deserving emphasis in this connection is that a great dealof the scientific expertise available to a society at any point in time remains tacit, rather than being fully available in codified form and accessible in archival publications. it is embodied in the knowledge of theresearchers about such things as the procedures for culturing specific celllines, or building a new kind of laser that has yet to become a standardpart of laboratory repertoire. this is research knowledge, much of it verytechnological in natureñin that it pertains to how phenomena have beengenerated and observed in particular, localized, experimental contextsñthat is embodied in people. under sufficiently strong incentives it wouldbe possible to express more of this knowledge in forms that would makeit easier to transmit, and eventually that is likely to happen. but, beingpossessed by individuals who have an interest in capturing some of thevalue of the expertise they have acquired, this tacit knowledge is transmitted typically through personal consultations, demonstrations, and themovement of people among institutions.the circulation of postdoctoral students among university researchlaboratories, between universities and specialized research institutes, and,no less important, the movement of newly trained researchers from theacademy into industrial research organizations, are therefore importantaspects of technology transferñdiffusing the latest techniques of scienceand engineering research. the incentive in this mode of transfer is a verypowerful one for ensuring that the knowledge will be successfully transfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.economic perspectives on public support for research51lated into practice in the new location, for the individuals involved areunlikely to be rewarded if they are not able to enhance the research capabilities of the organization into which they move.a similarly potent incentive may exist when a fundamental researchproject sends its personnel to work with an industrial supplier from whichcritical components for an experimental apparatus are being procured.ensuring that the vendor acquires the technical competence to producereliable equipment within the budget specifications is directly alignedwith the interests of both the research project and the business enterprise.quite obviously, the effectiveness of this particular form of usersupplierinteraction is likely to vary directly with the commercial value of theprocurement contracts and the expected duration and continuity of theresearch program.for this reason, big science projects or longrunning public researchprograms may offer particular advantages for the collaborative mode oftechnology transfers, just as major industrial producersñsuch as the largeautomotive companies in japanñare seen to be able to set manufacturingstandards and provide the necessary technical expertise to enable theirsuppliers to meet them. by contrast, the transfer of technology by licensing intellectual property is, in the case of process technologies, far moresubject to tensions and deficiencies arising from the absence of completealignment of the interests of the involved individuals and organizations.but, as has been seen, the latter is only one among the economic drawbacksof depending upon the use of intellectual property to transfer knowledgefrom nonprofit research organizations to firms in the private sector.notes1.economic theory describing the reasons industry will underinvest in research was first developed in the late 1950s and early 1960s. see nelson (1959)and arrow (1962).2.economists refer to this characteristic as a form of nonconvexity or anextreme form of decreasing marginal costs as the scale of use is increased.3.for further discussion of the inefficiencies of using intellectual propertyprotection to stimulate innovation (especially in regard to the adverse effects onthe use of existing knowledge that is relevant to research), see david and foray(1996).4.see, for example, dasgupta and david (1987, 1994), david et al. (1992), andgrossman and helpman (1991).funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.523federal support forresearch infrastructureresearch infrastructure consists of many elements. primary amongthem are research funding, human resources, and physical facilities forconducting research. historically, the u.s. government has been a partner with industry and universities in creating the infrastructure for manycritical new industries, ranging from agriculture to aircraft to biotechnology.1 computing is no exception. government, industry, and universities have all contributed to the research infrastructure that underlies theinnovative capacity of the nationõs computing industry. funding for theresearch infrastructure in computing comes largely from industry andgovernment sources, with small contributions from universities and nonprofit organizations. private industry invests in research, develops humanresources, and builds physical infrastructure for research and development (r&d) primarily to serve commercial purposes. public support forresearch infrastructure is, in contrast, intended to create a pool of resources that can be drawn upon by a variety of users in the private andpublic sectors. for example, substantial public investment is made inuniversities that train students, conduct research, and build research laboratories.this chapter explores the federal governmentõs contributions to theresearch infrastructure, examining the governmentõs support for research,human resources, and research equipment. although computing technology draws on research in a number of academic disciplinesñfromcomputer science, electrical engineering, mathematics, materials scienceand engineering, and cognitive science and psychologyñthis chapter exfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.federal support for research infrastructure53amines federal contributions in the areas of computer science and electrical engineering, which are the most directly relevant. computer scienceincludes work on the theory of computing; design, development, andapplication of computer capabilities to data storage and manipulation;information science and systems; programming languages; and systemsanalysis. research in electrical engineering includes work in communications, semiconductor technology, and electronic circuits, which is relevantto computing, as well as work in electric power, which is not.2 data onresearch funding is categorized according to the national sciencefoundationõs definitions of basic research, applied research, and development uses. although the distinctions among these categories are increasingly difficult to make in the computing industry, they reflect the mannerin which federal statistics are currently collected (see chapter 1).3federal research funding4levels of federal supportsince the end of world war ii, the federal government has been astrong supporter of computing research. between 1976 and 1995 (theearliest and latest years for which consistent data are available), federalfunding for research in computer science increased by a factor of five,from $180 million to $960 million in constant 1995 dollars (figure 3.1).growth has occurred in both basic and applied research, with basic research jumping from $65 million to $265 million and applied researchrising from $116 million to almost $700 million over the 19year period.roughly 35 to 45 percent of total federal research funding for computerscience has gone to universities, with industry and government laboratories garnering the remaining 55 to 65 percent; about 70 percent of the basicresearch funding went to universities during this period.5in contrast to computer science, federal funding for research in electrical engineering remained essentially flat between 1972 and 1995. froma peak of $1.1 billion (in constant 1995 dollars) in 1972, the real dollar levelof federal funding for research in electrical engineering dropped below$800 million in 1976 and, after exceeding the $1 billion mark again in 1987and 1989, dipped back below $800 million in 1995. despite the overalldecline, obligations for basic research in electrical engineering grew during this time frame, from about $130 million in the 1970s and early 1980sto about $200 million after 1985 (figure 3.2). as a result, the share of totalresearch funding in electrical engineering going to basic research increased from 12 to 25 percent, and the share of total research fundinggoing to universities rose from 10 to 23 percent.federal expenditures on computing research represent just a portionfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.54funding a revolutionof the federal budget for scientific and technological research. combinedfederal obligations for computer science and electrical engineering research climbed from just under $1 billion to $1.7 billion between 1976 and1995, growing from 5 percent to almost 7 percent of the federal researchbudget. several other fields, such as biology and physics, have historically maintained higher levels of federal investment than computer science and electrical engineering, although growth in physics research funding slowed after the mid1980s (figure 3.3).01002003004005006007008009001,000 1976 1978 1980 1982 1984 1986 1988 1990 1992 1994fiscal yearmillions of constant 1995 dollarstotal researchbasic researchfigure 3.1 federal funding for research in computer science, 19761995.source: nsf (1998b), tables 25 and 35.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.federal support for research infrastructure55sources of federal supportfederal funding for research in computer science and electrical engineering has come through several federal agencies whose roles and levelsof support have shifted over time. because of the emphasis it placed oncomputing as a means of enhancing u.s. military capabilities during thecold war, the u.s. department of defense (dod) has long been the largest funder of computing and communications research. early funding02004006008001,0001,200 1972 1974 1976 1978 1980 1982 1984 1986 1988 1990 1992 1994fiscal yearmillions of constant 1995 dollarstotal researchbasic researchfigure 3.2 federal funding for research in electrical engineering, 19711995.source: nsf (1998b), tables 25 and 35.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.56funding a revolutioncame from the army and office of naval research, but within 2 years ofestablishing its information processing techniques office in 1962, thedefense advanced research projects agency (darpa) became the dominant source of funding, providing more support for computer scienceresearch than all other federal agencies combined. between 1976 and1995, dod provided some 60 percent of total federal research funding incomputer science and over 75 percent of total research funding in electrical engineering (figures 3.4, 3.5).0.00.51.01.52.02.53.03.54.04.55.019741976197819801982198419861988199019921994yearbillions of constant 1995 dollarsbiology (excluding environmental biology/science) chemistry physics information technology (computer science + electrical engineering)figure 3.3 federal funding for scientific research, 19741995.source: nsf (1998b), tables 25 and 35.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.federal support for research infrastructure57figure 3.4 federal funding for research in computer science by agency, 19761995.source: nsf (1998c), table 1.02004006008001,0001,200 1972 1974 1976 1978 1980 1982 1984 1986 1988 1990 1992 1994fiscal yearmillions of constant 1995 dollarsdodnsfdoenasadocothers figure 3.5 federal funding for research in electrical engineering by agency,19721995.source: nsf (1998c), table 1.01002003004005006007008009001,000 1976 1978 1980 1982 1984 1986 1988 1990 1992 1994fiscal yearmillions of constant 1995 dollarsothersnasadoensfdodfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.58funding a revolutionfigure 3.6 federal funding for basic research in computer science by agency,19761995.source: nsf (1998c), table 2.050100150200250300 1976 1978 1980 1982 1984 1986 1988 1990 1992 1994fiscal yearmillions of constant 1995 dollarsothers doensfdodby the 1970s, the national science foundation (nsf) emerged as thesecond largest supporter of research in computing and communications,providing 20 percent of all federal support for computer science researchand 5 percent of federal support for electrical engineering research between 1976 and 1994. in contrast to dod, nsf has concentrated its effortson funding basic and university research in computer science, for whichits research expenditures have generally equaled or exceeded those ofdod (figure 3.6).6 with the exception of a 4year period between 1983and 1987, nsf has provided between 40 and 45 percent of all basic research funding in computer science, and it has consistently providedabout 40 percent of university research funding in computer science. inelectrical engineering, nsf contributed just under 30 percent of the funding for basic research and 30 to 40 percent of the funding for universityresearch, but it lagged behind dod by a wide margin (figure 3.7).funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.federal support for research infrastructure59comparisons to industrial research fundingfederal funding has supported a substantial fraction of all researchconducted in computing. in 1950, government funding for research anddevelopment dominated the computer world: it exceeded all industrialr&d spending on computing by a factor of three. as late as 1963, government still funded 35 percent of ibmõs r&d in computing, 50 percent atburroughs, and 40 percent at control data. but even by the 1960s thedistribution was uneven, and several commercial suppliers, notablyhoneywell and rca, financed most of their r&d internally. thus, theoverall percentage of computer r&d supported by government declineddramatically from the late 1960s, both because of an absolute decline ingovernment support and because of the rapid growth of the industry. inthe mid1970s, federal support represented only about 25 percent of computer r&d, and then shrank to a postwar low of 15 percent in 1979. withnew programs and the reagan administrationõs defense buildup, the levelwas restored to about 20 percent by 1983 (flamm, 1987, p. 102).these numbers alone, however, can be deceiving. very little r&dperformed in industry is research; most, in fact, counts as development.even applied research accounts for only about 10 to 15 percent of indus050100150200250 1972 1974 1976 1978 1980 1982 1984 1986 1988 1990 1992 1994fiscal yearmillions of constant 1995 dollarsothersnasansfdodfigure 3.7 federal funding for basic research in electrical engineering by agency, 19721995.source: nsf (1998c), table 2.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.60funding a revolutiontrial r&d in computing. flamm estimates that the ratio of developmentto research in the computer industry was about seven to one in the early1980s, and within the research category it was about seven to one ofapplied to basic (that is, basic research in industry is only about 2 percentof total r&d). thus, when one excludes development from consideration, government support represented about 40 percent of all computerresearch, and half of that was basic research (flamm, 1987, pp. 104105).direct comparisons between federal and industrial research fundingare hard to make because of differences in the way data are collected fromfederal and industry sources.7 nevertheless, a rough estimate of thefederal share can be made by comparing federal funding for research incomputer science to company funding for research in the office, computing, and accounting machinery industry.8 this comparison shows thatfederal funding constituted roughly onethird of total computerrelatedresearch funding in the late 1970s (figure 3.8). the federal share dipped05001000150020002500300019771978197919801981198219831984198519861987198819891990199119921993199419951996yearmillions of constant 1995 dollarsindustry researchfederal researchfigure 3.8 federal and industrial funding for computing research, 19771996.industry research, as shown, consists of companyfunded research in computingand office equipment industry; it does not include companyfunded research inother computingrelated industries such as communications equipment, semiconductors, or computing and communications services. governmentfundedresearch, as shown, consists of total federal funding for research in computerscience. industrial research data for 1978, 1980, 1982, 19851987, and 1989 wereestimated from data on industry research and development expenditures andfrom the ratio of research to research and development in expenditures in yearsfor which actual data were available.source: federal research funding from nsf (1998b), table 25; industry research funding compiled from the 19791998 editions of the annual national science foundation report research and development in industry.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.federal support for research infrastructure61to 15 percent in the early 1980s as industrial research funding expandedand federal funding stagnated, but by 1992 federal funding again constituted onethird of the total, owing to rapid growth in federal funding andrestructuring and cutbacks in industry support.9 not included in thisestimate are research expenditures financed by universities and nonprofitorganizations, which tend to be much smaller than the amounts providedby federal agencies or industry.government also directed significant research funding to industryñeven as the computer industry grew during the late 1970s. while theshare of the computer industryõs total r&d funds coming from government sources declined dramatically between 1975 and 1979, the share ofthe industryõs research funding coming from the federal government remained high, declining only from 47 percent to 37 percent (table 3.1).flamm estimates that federal funding accounted for 40 percent of totalcomputer industry research funding through the mid1980s (flamm, 1987,p. 104, table 45). in the communications equipment industry, the federalrole has been even larger and more pervasive.10 in 1965, federal fundsaccounted for 66 percent of the industryõs total r&d funding, a figure thatdeclined to 40 percent by 1990. as a percentage of total industry research,federal funds declined steadily from 49 percent in 1965 to 19 percent in1980, but then rebounded to account for half of all industry researchfunding in 1990 (table 3.2). in contrast, federal funding has played adeclining role in industrial r&d in the electronic components industry.11the percentage of industry r&d funding provided by government detable 3.1 funding for industrial r&d and research in office andcomputing equipment, 19751979r&dresearchtotal levelpercenttotal levelpercent(in millions of dollars)federal(in millions of dollars)federal19752,22022n.a.n.a.1976 2,40221269471977 2,65516 3134419782,88311n.a.n.a.19793,2148 45137note: funding levels indicate total support for r&d and research conducted by industry;expenditures for research conducted by universities are excluded; n.a., data not available.source: data compiled from the national science foundationõs biennial reports, researchand development in industry, issued between 1979 and 1992.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.62funding a revolutionclined from 38 percent in 1972 to 11 percent in 1990 as total r&d fundinggrew from $330 million to $4 billion.these figures suggest that federal funding continued to play an important role in the expanding computing industry. it created economicopportunities for industry to exploit and, as such, expanded the privateinvestments made to seize these opportunities. as new ideas emergedfrom federally funded research, companies capitalized on them. indeed,firms in computingrelated industries tend to spend a greater percentageof their sales revenues on r&d than do firms in most other industries(figure 3.9). roughly 10 to 20 percent of corporate r&d funds is spent onresearch as opposed to development.12 such expenditures tend to derivefrom, and result in, the fast pace of innovation characteristic of the field.human resourceshuman resources are essential to innovation, especially in knowledgeintensive fields like computing and communications. attractingand educating students to new areas of research opportunity (especially,but by no means exclusively, at the graduate level) is a vital taskñboth inmaintaining progress at the research frontier and in transferring newknowledge to industry by providing trained scientific and engineeringtable 3.2 funding for industrial r&d and research incommunications equipment, 19651990r&dresearchfundingpercentfundingpercent(in millions of dollars)federal(in millions of dollars)federal1965a,b1,91266425 491970a,b 2,57854 522411975b 2,38544569271979c 3,63544787191985 9,397451,674301990 5,928401,32151note: funding levels indicate total support for r&d and research conducted by industry;expenditures for research conducted by universities are excluded.aincludes funding for electronic components, which had $330 million in r&d funding in1972.bincludes funding from the communications services industry.cdata for 1979 are shown because complete data are not available for 1980.source: data compiled from the national science foundationõs biennial reports, researchand development in industry, issued between 1979 and 1992.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.federal support for research infrastructure63personnel. in the united states, graduate education is tightly connectedwith university research, and university research budgets are an important driving force for graduate enrollment. the federal government hasplayed an important role by supporting university research in computingand communications, which has directly and indirectly supported the024681012141619751977197919811983198519871989199119931995yearr&d expenditures as a percentage of total salescomputer and office equipmentcommunicationselectronic componentsall manufacturingfigure 3.9 r&d intensity in computerrelated industries, 19751996. data forcomputing and office equipment between 1992 and 1996 reflect the reclassification of firms considered to be part of the industry.source: nsf (1998a), table a18funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.64funding a revolutioneducation of graduate students and the creation of university departments in computer science.since 1965, the number of college and university departments in computer science and computer engineering has grown rapidly. the taulbeesurveys of u.s. and canadian computer science and computer engineering departments show a steady growth in ph.d.granting departments,increasing linearly from 6 in 1965 to 56 in 1975 and to 148 in 1995(andrews, 1997).13 along with the expansion of academic computer science departments has been growth in enrollments at all levels, from undergraduate through doctorate. between 1966 and 1986, the number ofbachelorõs degrees awarded in computer science skyrocketed from 89 to42,000, surpassing the number of bachelorõs degrees awarded in mathematics and electrical engineering (the largest engineering subdiscipline)in 1981 and in physics in 1982 (figure 3.10). electrical engineering alsoexperienced significant growth, expanding at an average annual rate of 4percent, from 11,000 to 27,000 during this period, while the total numberof bachelorõs degrees awarded in all academic fields rose at a 1 percentannual rate. between 1987 and 1995, the number of bachelorõs degreesawarded in both these fields declined precipitously, reflecting changingstudent preferences and shifts in the job market, as well as attempts bysome universities to relieve the burden on electrical engineering and computer science departments by shifting students to other academic departments.14 by 1995, the number of bachelorõs degrees awarded in computerscience and electrical engineering had declined to 25,000 and 18,000,respectively, although the decline showed evidence of leveling off.graduate student production also blossomed after 1965. in computerscience, the number of masterõs degrees awarded climbed steadily at arate of 14.5 percent a year between 1966 and 1995 (figure 3.11). in electrical engineering, the number of masterõs degrees remained relatively constant at 4,000 per year from 1966 to 1980, and then began growing at a 6percent annual rate. such growth occurred despite the fact that the number of masterõs degrees awarded in all fields of science and engineeringbegan to decline after 1977 and did not return to the 1977 level until 1990.at the ph.d. level, the number of degrees awarded by u.s. universities incomputer science grew from 19 in 1966 to over 900 in 1995, despite leveling off between 1976 and 1982 (figure 3.12).15 by comparison, the number of ph.d.s awarded in electrical engineering and mathematics declinedduring the 1970s, although both fields began growing again in the 1980sand 1990s. nevertheless, computer science has continued to lag behindboth electrical engineering and mathematics in the total number of ph.d.sawarded each yearñdespite leading in the number of bachelor and master degrees awarded. the percentage of ph.d. recipients choosing a firstjob in industry (as opposed to academia) grew steadily between 1975 andfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.federal support for research infrastructure651995, reflecting strong industrial demand for skilled computer scientists,and causing concern among universities about their ability to train thenext generation of computer scientists (table 3.3).foreign students have also played a large role in the growth of u.s.ph.d. programs. the taulbee surveys show that the percentage of ph.d.recipients in computer science who are nonresident aliens increased from050001000015000200002500030000350004000045000 1966 1969 1972 1975 1978 1981 1984 1987 1990 1993 yearnumber of degreescomputer scienceelectrical engineering mathematicsfigure 3.10 bachelorõs degrees awarded by field, 19661995.source: nsf (1997b), tables 30, 45, and 46.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.66funding a revolution20 percent in the early 1970s to 40 percent in the 1980s and 1990s. incomputer engineering the percentage reached as high as 64 percent. theunited states has attracted a large number of foreign nationals, most ofwhom were first trained abroad before they entered graduate educationin this country. these scientists and engineers have formed an importantpart of the nationõs workforce in computing. it is reasonable to expect 1969 1972 1975 1978 1981 1966 02,0004,0006,0008,00010,00012,000 1984 1987 1990 1993 yearnumber of degreescomputer scienceelectrical engineering mathematicsfigure 3.11 masterõs degrees awarded by field, 19661995.source: nsf (1997b), tables 30, 45, and 46.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.federal support for research infrastructure67that fewer foreign students will decide to remain in the united states inthe future as opportunities for employment in their home countries increase. by 1997, u.s. industry was already seeing shortages of qualifiedinformation technology personnel to fill job market vacancies, raisingquestions about the need for policies to expand the proportion of thelabor force entrants who possess computing and related skills.1602004006008001,0001,2001,4001,6001,800 1966 1969 1972 1975 1978 1981 1984 1987 1990 1993 yearnumber of degreescomputer scienceelectrical engineering mathematicsfigure 3.12 doctoral degrees awarded by field, 19661995.source: nsf (1997b), tables 30, 45, and 46.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.68funding a revolutionthe federal government has directly and indirectly supported thecreation of human resources in computing and communications. as earlyas the 1960s, federal agencies conducted or sponsored studies that identified human resource issues as matters of national concern.17 federalagencies have provided a number of fellowships for graduate students incomputer science, and nsf has worked to develop curricula for university programs.18 but the most important contribution has come indirectlythrough federal support of university research. between 1976 and 1994,federal obligations for university research in computer science expandedfrom roughly $65 million to $360 million (figure 3.13), and federal obligations for university research in electrical engineering more than doubledin real terms from $74 million to $161 million (figure 3.14). most of thisfunding has come from two sources, darpa and nsf. altogether,federal funding accounted for 70 percent of university research fundingfor computer science and between 65 and 75 percent of university research funding for electrical engineering from the mid 1970s through 1995(figure 3.15). the balance has come from a combination of industry,private foundations, state governments, and universitiesõ own resources.federal funds play a significant role in supporting graduate studentsin electrical engineering and computer science. data from the nationalscience foundation indicate that between 1985 and 1996, the percentagetable 3.3 employment, by sector, for new ph.d. recipients incomputer science and engineering, 19701995number employed (and percentage)sector19701975198519901995industry37 (36)69 (29)145 (35)355 (41)375 (48)government5 (5)12 (5)19 (5)28 (3)24 (3)academia58 (56)137 (57)191 (46)343 (40)285 (37)other3 (3)24 (10)38 (9)131 (15)92 (12)total103(100)242(100)412(100)857(100)776(100)note: years refer to the start of the academic year for 1985, 1990, and 1995 and to thecalendar year for 1970 and 1975. totals do not include unknown employment, which totaled 9 in 1970, 12 in 1975, 15 in 1985, 217 in 1990, and 139 in 1995. percentages may not addto 100 because of rounding.source: data compiled from annual taulbee surveys conducted between 1971 and 1996.see note 12.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.federal support for research infrastructure69050100150200250300350400 1976 1978 1980 1982 1984 1986 1988 1990 1992 1994fiscal yearmillions of constant 1995 dollarsothers nasansfdodfigure 3.13 federal funding for university research in computer science, 19761995.source: nsf (1998d), table 1.050100150200250 1975 1977 1979 1981 1983 1985 1987 1989 1991 19931995fiscal yearmillions of constant 1995 dollarsothers +doenasansfdodfigure 3.14 federal funding for university research in electrical engineering,19751995.source: nsf (1998d), table 1.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.70funding a revolutionof graduate students in u.s. computer science and electrical engineeringdepartments supported by federal funds (i.e., research assistantships,teaching assistantships, and fellowships) grew from 14 percent to 20 percent.19 over 75 percent of this support came in the form of researchassistantships; over half of all research assistants in u.s. graduate programs between 1985 and 1996 received federal support.20 in the nationõsfigure 3.15 portion of university research funding provided by the federalgovernment, 19731995.source: nsf (1998d), table 2.01020304050607080197319751977197919811983198519871989199119931995yearpercentagecomputer scienceelectrical engineeringfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.federal support for research infrastructure71top computer science departments, federal funding plays an even greaterrole (figure 3.16). between 1985 and 1995, approximately 56 percent ofthe graduate students in computer science and electrical engineeringdepartments at the massachusetts institute of technology (mit), carnegiemellon university, and the university of california at berkeley receivedfederal funding, with research assistantships alone supporting 46 percentof them. at stanford university, 27 percent of graduate students in electrical engineering and computer science received support from the federal government in 1997;21 it is estimated that 50 to 60 percent of stanfordph.d. students in these departments receive federal funds.22computer facilitiesresearchers need equipment and facilities with which to conduct theirwork. acquiring and maintaining such equipment is especially challenging in computing and communications research because of the rapidgrowth of the field since the 1950s, the concomitant rise in the number ofgraduate students and faculty conducting research, and the rapid rate atwhich computing equipment becomes obsolete.23 in industry, support0102030405060198519861987198819891990199119921993199419951996yearpercentage figure 3.16 computer science and electrical engineering graduate studentssupported by the federal government, 19851996.source: compiled from data in the national science foundationõs online database of sources of support for science and engineering graduate students. thedatabase is available via webcaspar at <http://caspar.qrc.com>.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.72funding a revolutionfor research infrastructure is provided internally: corporate funds areused to build new facilities and to equip them with computers, networking equipment, and other research equipment, as needed. universityinfrastructure, on the other hand, relies on a mix of support from federaland state governments, university funds, and donations of equipmentfrom industry. since the 1960s, the federal government has been thedominant source of support for computing and communications researchequipment.providing and supporting research infrastructure are expensive tasks.in 1988, for example, u.s. universities spent $187 million on equipmentfor academic computer centers and supercomputer centers, and an additional $334 million for maintenance, repair, and operations (table 3.4).computer science departments spent another $77 million on equipmentpurchases for research purposes and related maintenance, repair, andoperational costs. such expenditures are increasing faster than inflationas universities attempt to maintain stateoftheart research centers andmeet the demands of a growing pool of researchers. between 1981 and1995, expenditures for computer science research equipment alone (notincluding maintenance and operations) tripled in real terms from $25million to $75 million. in electrical engineering, research equipment expenditures doubled during this same period to $68 million.the federal governmentõs support for financing the purchase of computing equipment by universities has taken a variety of forms, rangingfrom funding for general computing resources for universities, to financing of researchgrantrelated equipment in computer science departments,to establishing large supercomputer centers. while the first two of thesemissions required scientific or engineering computers of modest capabilities, the third required specialized computers to address large, complextable 3.4 university expenditures for computing equipment,maintenance, and operations (in millions of dollars), 1988expenditurecomputer science departmentscomputer centersequipment45187maintenance and repair1784service contracts1258other (e.g., salaries, tools)519operations15250technician salaries12156other (e.g., supplies)394total 77 521source: nsf (1991), tables 1 and 8.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.federal support for research infrastructure73problems of interest to dod, other federal agencies (e.g., u.s. meteorological service), and academic research communities. all three requireddevelopment of a networking infrastructure capable of linking researchers with resources that were geographically separated.university computing centersamong the first federal efforts to provide computing resources foruniversities was nsfõs institutional computing services program, established in 1956 to provide universities with computers for general educational use. annual obligations expanded rapidly, and, between 1958 and1970, nsf provided $66 million for such centers (table 3.5). other agencies also supported computing facilities on campuses during the 1960s. infact, virtually all governmentfunded computer research included significant monies for equipment; one study estimated that in 1963 federal agencies were supplying about half the support for campus computing in thecountry. nsf support for computing facilities differed from that provided by other federal agencies because it was spread among a largenumber of universities and because it was not provided for use in anyparticular project sponsored by the government; rather, it supported general educational and scientific applications of computing. for example,nsf supported philip morse at mit in his early work on timesharingñatechnology intended to improve the efficiency of facilities that nsf wasalready supporting at academic centers, by making them available tomore users. darpa support, in contrast, was aimed at a limited numbertable 3.5 national science foundation obligationsfor institutional computing services (in thousands ofdollars)yearfunding1958 2001960 1,6721962 2,9751964 4,5171966 8,8991968 10,6041970 6,563total 65,913source: data for 19601967 compiled from the national sciencefoundationõs annual budget request to congress; data for 19681970compiled from the national science foundationõs annual reports,grants and awards.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.74funding a revolutionof select computer science departments (such as those at mit, carnegiemellon university, and stanford university) and was intended for use ondarpa projects, such as project mac and the arpanet.departmental computingother initiatives were targeted more specifically to computer sciencedepartments. between 1981 and 1995, the federal government fundedroughly 65 percent of the purchases of research equipment in computerscience departmentsððproviding 83 percent of such funding in 1985 (figure 3.17). in electrical engineering, the share of equipment funds comingfrom the federal government declined from its 75 percent level in 1982,but remained at 60 percent in 1995 (figure 3.18). many government agencies provided funds for equipment in research contracts with universities,but nsf established two programs specifically designed to provide infrastructure for computer science departments: the computer researchequipment (cre) program and the much larger coordinated experimental research (cer) program.the cre program, initiated in the 1970s, provided basic computersupport for computer science departments. annual expenditures on thecre between 1977 and 1985 grew to $1.4 million (table 3.6). with the01020304050607080198119821983198419851986198719881989199019911992199319941995fiscal yearmillions of constant 1995 dollars0%10%20%30%40%50%60%70%80%90%totalpercent from federal sourcesfigure 3.17 expenditures for research equipment in computer science, 19811995.source: compiled from data in the national science foundationõs online database of current fund research equipment expenditures for computer science between fiscal years 1981 and 1995. the database is available via webcaspar at<http://caspar.qrc.com>.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.federal support for research infrastructure7505101520253035404519811983198519871989199119931995fiscal yearmillions of constant 1995 dollars0%10%20%30%40%50%60%70%80%totalpercent from federal sourcesfigure 3.18 expenditures for research equipment in electrical engineering, 19811995.source: compiled from data in the national science foundationõs online database of current fund research equipment expenditures for electrical engineeringbetween fiscal years 1981 and 1995. the database is available via webcaspar at<http://caspar.qrc.com>.formation of the computing and information science and engineering(cise) directorate in 1986, cre became the cise research instrumentation program. program funding grew from $2 million to $3.8 millionbetween 1987 and 1996.the cer, started in 1981, was a response to growing concerns thatcomputer science departments were not producing enough ph.d.s in partbecause they lacked funds to pursue largescale experimental computerresearch (nsf, 1981a). the majority of cer funds was allocated to theexperimental computer research program, which provided òsupport ofspecial purpose equipment needed by more than one computer researchproject and difficult to justify on a single projectó (nsf, 1981b). thisprogram also paid for recruitment and retention of quality faculty andtechnicians for the new computer science centers.24 another portion ofthe cer, the csnet program (described in more detail below), althoughconstituting less than 10 percent of the cer budget, made major strides innetworking by linking computer science departments together to expedite research through a more open forum for ideas. the cer was renamed the cise institutional infrastructure program in 1986, and funding grew from $14 million to $23 million.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.76funding a revolutionhighperformance computingthe government has been the largest supporter of access to highperformance computers for researchers, especially those in universities.through the mid1980s, government funding of the ibm 701, univaclarc (livermore automatic research computer), stretch, and later boththe cdc and the cray series of computers created large systems that wereused for a variety of applications by researchers. in 1985, nsf launched aprogram of supercomputer centers to provide access to highperformancemachines and to encourage development of useful technology and applications. annual expenditures increased from $29 million to $71 million in1996. this funding originally created five centers nationwide that provide researchers in many disciplines with access to supercomputer time.25the centers were intended to allow for advanced computationally complex research that cannot be carried out on regular computers. over time,the centers became the early proving grounds of a longdeveloping newtable 3.6 national science foundation expenditures on thecoordinated experimental research and computing researchequipment programs (in millions of dollars), 19771985total budget ofcomputer sciencessection (19771983)experimentaland division ofcomputercomputer researchyearcrecerresearcha(19841985)1977 1.65 15.791978 1.55 16.631979 1.66 16.771980 1.97 18.171981 1.02 5.69 3.77 22.121982 1.21 8.55 7.10 25.591983 1.20 11.19 9.52 33.881984 1.3913.5012.74 33.791985 1.4614.9914.76 38.59total13.1153.9247.89221.33aexperimental computer research was the predominant source of infrastructure supportwithin the coordinated experimental research program. it does not include support forfaculty or csnet.source: data for 19771993 compiled from the annual summary of awards for the nationalscience foundationõs mathematical sciences section. data for 19841985 compiled from theannual summary of awards of the national science foundationõs division of computersciences.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.federal support for research infrastructure77architecture for highperformance computingñparallel computing. thecenters also play an important educational role for some computer sciencedepartments teaching parallel computing (cstb, 1992, p. 225), and theybecame the spur for additional supercomputer centers, paid for by stateand private sources, to be established in other universities. some computer scientists contend, however, that the supercomputers offered littlevalue to researchers in computer science and that their primary use wasby scientists in other disciplines. there has been a longstanding tensionin the computer programs about support for computer research and provision of computer facilities to support research in other scientific andengineering disciplines.nevertheless, numerous innovations emanated from these centers.they catalyzed work leading to modeling and visualization tools, motivated development of the browser technology for the world wide web,and introduced industry to largescale scientific and engineering calculation on an impressive scale. both university and government laboratorycomputer centers were in the forefront of availing themselves of newcommunications technology to link users and providers and to make moreefficient use of computer power on a national level. many of the centerswere used by researchers in the oil, automotive, and pharmaceuticalindustries whose companies had joined the centers as industrial partnersso that they might explore the benefits of supercomputers in their research,development, and manufacturing efforts. as such, the supercomputersites brought together academic and industry researchers to work on problems of mutual benefit and filled a muchneeded gap for computingresources. in doing so, the centers generated scientific and technicalbenefits as well as economic ones.network infrastructurefederal agencies have long supported development and deploymentof networking infrastructure to assist the research communities in computing and communications. as early as 1973, nsf initiated a programcalled networking for science, which provided between $600,000 and$750,000 per year to create computer networks for university researchers.more significant support for network infrastructure followed upon thedevelopment of packetswitched networking technologies by darpa inthe late 1960s and 1970s. this technology formed the basis of thearpanet, which connected researchers at universities supported bydarpa research funding (see chapter 7).use of the arpanet expanded to the computer science researchcommunity and other scientific research communities starting in the1970s. after management of the arpanet was transferred to the defunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.78funding a revolutionfense communications agency (now the defense information systemsagency) in 1975, a number of federally supported, disciplinespecific networks were established. these included (1) mfenet, funded by the department of energy (doe) to give academic physicists working on nuclearfusion access to supercomputers at lawrence livermore national laboratory; (2) hepnet, also funded by doe to support research in highenergy physics; and (3) space physics analysis network, funded by thenational aeronautics and space administration (nasa). in the early1980s, nsf established the csnet to link computer science researchers atdifferent universities who were not attached to the arpanet. csnetcombined access to arpanet, telenet (a commercial packetswitchedsystem run by a subsidiary of bolt, beranek, and newman), and phonenet(an emailonly system for other academic departments). by 1985, csnethad links to over 170 university, industrial, and government researchorganizations. in 1987, it merged with bitnet, another network servingusers from academic institutions. csnet operations were continued under the corporation for research and education networking until the fallof 1991 (cstb, 1994, p. 238). the success of the csnet convinced researchers of the value of a national computer network and therefore provided the impetus for nsfõs more notable networking project, thensfnet (hafner and lyon, 1996, pp. 241245).in 1986, nsf launched nsfnet, the backbone of a network that connected hundreds of colleges and universities in the united states withhighspeed links and was used by departments of all varieties, includingcomputer science and engineering. nsfnet linked nsfõs five supercomputing centers and, in coordination with the connections programs ofthe late 1980s, provided seed funding to allow regional networks (such asthe new york state education and research network, or nysernet) anduniversities to interconnect. the connections program provided 2 yearsof financial support, after which participants were expected to assumefinancial responsibility. under the federal governmentõs national research and education network program, different federal agencies, including nsf, nasa, doe, darpa, and the national library of medicine, launched or expanded separate, interconnected networking effortsthat served specific communities. nsfõs funding for nsfnet grew from$6.5 million in 1987 to $25 million in 1992, during which time the capacityof the backbone was upgraded several times. with the commercializationof the internet in 1993, nsfõs responsibility for managing the networkdeclined, but it continued to fund development and deployment of highspeed network infrastructure, including the very high speed backbonenetworking system and the nextgeneration internet. expenditures onsuch network infrastructure reached $42 million in 1996.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.federal support for research infrastructure79effects of federal investments inresearch infrastructurethe effects of federal investments in research infrastructure have beenfelt throughout the computing industry. many concepts that were developed by industry and designed into products received their initial impetusfrom governmentsponsored research and largescale government development programs. examples include core memories, computer timesharing, the mouse, packet switching, computer graphics and virtual reality, speech recognition software, and relational databases. chapter 4 andchapters 6 through 10 of this report trace the influences of federal researchfunding upon the development of the particular technologies describedabove.a more general sense of the broader linkages between federallyfunded research and innovation in computing can be derived from patentstatistics. although not an entirely satisfactory measure of innovation,patents can provide a rough measure of invention and, through the references cited within them, they can help in tracing the intellectual inputs toinventions.26 recent studies by chi research, inc., suggest a significantñand growingñlinkage between publicly funded research and patents (and by extension, innovation). between 1985 and 1994, the numberof scientific or technical papers cited in individual patents rose from 0.4 to1.4 in the united states.27 of these papers, almost 75 percent were written by publicsector researchers in the united states or abroad (the publicsector includes government laboratories, universities, and federallyfunded research and development centers). for the specific industriesanalyzed, reliance on public science was highest in drugs and medicines(79 percent of referenced papers) and lowest in electrical components (49percent of referenced papers). data for ibm indicate that only 21 percentof the papers referenced in its patents in 19931994 were written by ibmemployees; 25 percent referenced papers by researchers at u.s. universities (narin et al., 1997).similar figures hold for the computer industry. between 1993 and1994, 1,619 patents were issued in the united states containing referencesto papers published in computingrelated journals, such as ieee transactions on computers, the ibm journal of research and development, communications of the acm, and computer. despite the fact that 75 percent of thesepatents were issued to u.s. companies, the majority of the papers cited bythese patents were written by university or government researchers (table3.7). moreover, of the papers for which funding information is available,51 percent acknowledged funding from the federal government, whereas37 percent acknowledged industry funding. nsf support was acknowledged in 22 percent of the papers, darpa support in 6 percent.28 thesefunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.80table 3.7 authorship and source of financial support for computerrelated papers cited in u.s. patents grantedin 19931994numbernumber of acknowledgments per source of fundingsector ofof papersauthor(s)citedindustryuniversitygovernmentnonprofitforeignunknowntotalnsfadarpaaindustry34534403126739022university397113366101199787626281industry anduniversity8268082602158454governmentanduniversity74090041721total8315293673219151101,44131188percent37351118100226aas a subset of the number acknowledging funding by the federal goverment.source: based on patent citation, authorship, and funding data provided by francis narin and anthony breitzman, chi research, inc., haddonheights, n.j.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.federal support for research infrastructure81data are limited in that they reflect patenting behavior only during arecent 2year period. nevertheless, they suggest that federally sponsoredresearchñespecially that conducted at universitiesñcontinues to contribute to innovation in computing even as the computer industry has grown.conclusionas this chapter demonstrates, the federal government has played animportant role in helping to create the research infrastructure needed tosupport the nationõs computing industry. the federal government becamethe primary source of funding for university research in computer scienceand electrical engineering and for research equipment in these disciplines.it also became the primary supporter of graduate students studyingñandconducting researchñin these fields. such support complementedindustryõs efforts to build the much larger industrial infrastructure neededfor successful innovation in computing and industryõs contributions topublic infrastructure (through equipment grants, tuition reimbursement,and sponsored research). together, these investments created a publiclyavailable pool of resources for others to draw upon. as subsequent chapters of this report describe in more detail, people with ideas and trainingmade possible by public investments in research infrastructure helpedstaff the information revolution, disseminate its ideas, and chart its course.as part of the larger innovation process, they helped the nation to establish a dominant position in the international market for computing technology and to enjoy resulting social and economic benefits.notes1.in aircraft, the government established the national advisory committeeon aeronautics in 1915 to address both instrumentation and generic design in theform of a wind tunnel and the design of an aerodynamic foil or wing. the national aeronautics and space administration continues to play a role in aeronautics research. the former u.s. bureau of standards, now the national institute ofstandards and technology, has undertaken much research in developing scientific and technical standards in the fields of metallurgy, optics, and electronics, aswell as in computing hardware and software.2.the definitions of computer science and electrical engineering used inthis report derive from those used by the national science foundation (nsf) inits surveys of federal research expenditures. see nsf (1997a).3.nsf defines basic research as research in which òthe objective of the sponsoring agency is to gain more complete knowledge or understanding of the fundamental aspects of phenomena and of observable facts, without specific applications toward processes or products in mind.ó it defines applied research as workfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.82funding a revolutionin which òthe objective of the sponsoring agency is to gain knowledge or understanding necessary for determining the means by which a recognized need maybe met.ó see nsf (1997a).4.several shortcomings also exist in the data and statistics that follow. theyare somewhat incomplete as data for the early years of computing are eitherpoorly documented or intermixed with data from mathematics, electrical engineering, or other disciplines. some data are not generally available. for example,data on the national security agencyõs expenditures on computerrelated research, although early and extensive, are not publicly available.5.all data contained in this section derive from nsf (1997a) unless otherwise noted.6.it is notoriously difficult to distinguish among basic and applied research indod. while dod divides its r&d expenditures into several categories, with 6.1designating basic research, 6.2 designating applied research, and 6.3 designatingadvanced development, the classifications are often used in incompatible ways.some of the work classified as 6.2 is often claimed to result in fundamental breakthroughs. hence, comparisons among federal agencies are somewhat ambiguous.7.statistics on federal and industry research spending are difficult to compare because they are compiled through different surveys (both administered bynsf), and because relevant spending is classified differently. whereas federalresearch funding is classified by academic discipline (such as computer science orelectrical engineering), industry research funding is classified by industry (computing and office equipment versus communications equipment). the comparison shown herein does not include industryfunded research for communications, electronic components, or related services, nor does it include the portionof federal funding of research in electrical engineering that might be relevant tothose areas.8.office, computing, and accounting machinery is the industry defined inthe standardized industrial classification (sic) codes (used for classifying government statistics on industrial production, employment, trade, and so on) that ismost closely aligned with computing. it includes electronic computers, computerstorage devices, computer terminals, other computer peripheral equipment, calculating and accounting machines (except electronic computers), and other officemachines. it does not include communications equipment, electronic components, or software, which are classified as part of other industries.9.the sharp decline in reported industry research expenditures in 19921994 resulted, in large part, from a reclassification of several companies into otherindustries (typically in the service sector). the reported rise in research spendingbetween 1994 and 1996 reflects a combination of growing industry expenditureson research and the inclusion of several additional firms within the office andcomputing equipment industry category.10.the communications equipment industry, sic code 366, includes manufacturers of telephone, networking, radio, and television broadcasting equipment.it does not include communications service providers, such as telephone companies, radio and television broadcasting stations, and cable television companies,which are separately classified under sic code 48. historical data on r&d exfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.federal support for research infrastructure83penditures by communications service firms are not generally available, althoughthey are included in the communications equipment totals prior to 1976.11.the electronic components industry includes integrated circuits as well asdiscrete components, such as transistors, diodes, resistors, and capacitors. statistics on federal and industrial support for research (as opposed to r&d) in thissector are not available.12.this estimate is based on annual data compiled by the national sciencefoundation and contained in its series of publications, research and developmentin industry, between 1956 and 1998.13.the taulbee surveys of ph.d.granting departments were initiated andadministered by orin taulbee at the university of pittsburgh from 1970 through1984. they were administered subsequently by david gries and dorothy marshat cornell university through 1991 and are now administered by the computingresearch association with assistance from david gries. results were originallypresented in communications of the acm and now appear in computing researchnews.14.for example, in the late 1980s, mit established a program in mathematicswith a focus on computer science and another program in physics with a concentration in semiconductor devices and electronics as a means of reducing enrollments in its departments of electrical engineering and computer science.15.the leveling off of ph.d. production around 1980 caused considerableconcern in the computer science community.16.see, for example, u.s. department of commerce (1997).17.see, for example, nsf (1988).18.professional societies also played a role in developing curricula for computer science education. the association for computing machinery (acm),sponsored the first major work on curricula for computer science, curriculum 68,which influenced the undergraduate curriculum in many departments formed inthe 1970s. later, the acm and the institute of electrical and electronics engineers (ieee) computer society worked together on curriculum efforts and jointlycreated the computer science accreditation board, which accredits undergraduate departments of computer science.19.data compiled from the national science foundationõs database of sourcesof support for fulltime science and engineering students, by academic disciplinefor fiscal years 19721996. the database is available online at <http://caspar.nsf.gov/cgibin/webic.exe?template=/nsf/srs/webcasp/start.wi>.20.between 20 and 23 percent of all graduate students in u.s. computer science and electrical engineering departments were supported by research assistantships during the time frame indicated.21.personal communication from susan clement, stanford university, july9,1998. statistics reported to nsf by stanford university tend to underestimate therole of federal funding in supporting graduate students because they count onlystudents supported by fellowships, not research assistantships. the stanfordfigures cited in this chapter were provided directly by the university and countall forms of federal support.22.personal communication from john hennessy, dean of engineering,stanford university, june 22, 1998.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.84funding a revolution23.see van dam et al. (1991).24.personal communication with john r. lehmann, deputy division director for computercommunications research, national science foundation, july31, 1997.25.in 1997, nsf restructured the advanced scientific computing centersprogram into the partnership for advanced computation infrastructure (paci).under the paci program, each partnership operates a leadingedge site thatmaintains highend hardware systems that are one or two orders of magnitudemore capable than those typically available at a major research university. nonleadingedge partners are expected to contribute to access, outreach, training,and software development. two partnerships support two leadingedge centersand over 60 partners. these are the the national computational science alliance,which is anchored by the national center for supercomputing applications inurbanachampaign, illinois, and the national partnership for advanced computational infrastructure, anchored by the san diego supercomputing center incalifornia.26.invention refers to the creation of new products or processes that meet thetest of novelty and utility and are not obvious to experts in the field. innovationgenerally refers to the development and application of a new product, process, orservice. as a result, patent statistics suffer from a number of shortcomings as ameasure of innovation. patents register new inventions, not innovation. manyinventions are never commercialized, and many innovations are never patented.for example, a firm may decide to keep its innovation a trade secret rather thanfiling a patent, which requires a disclosure of the operation of the new product,process, or service. much technological progress emerges from incremental innovation, learning by doing, and adaptation of existing technologies. patent statistics do not provide any indication of the economic value of the invention patented.27.the vast majority of patents do not cite scientific or technical literature;they tend to cite previous patents, demonstrating the degree to which they represent incremental improvements to the state of the art.28.the estimates of patents and cited papers contained in this paragraphderive from data provided by francis narin and anthony breitzman at chiresearch, inc., in haddon heights, n.j.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.85rather than a single, overarching framework of support, federal funding for research in computing has been managed by a set of agencies andoffices that carry the legacies of the historical periods in which they werecreated. crises such as world war ii, korea, sputnik, vietnam, the oilshocks, and concerns over national competitiveness have all instigatednew modes of government support. los alamos national laboratory, forexample, a leader in supercomputing, was created by the manhattanproject and became part of the department of energy. the office ofnaval research and the national science foundation emerged in the wakeof world war ii to continue the successful contributions of wartime science. the defense advanced research projects agency (darpa) and thenational aeronautics and space administration (nasa) are products ofthe cold war, created in response to the launch of sputnik to regain thenationõs technological leadership. the national bureau of standards, anolder agency, was transformed into the national institute of standardsand technology in response to recent concerns about national competitiveness. each organizationõs style, mission, and importance have changedover time; yet each organization profoundly reflects the process of itsdevelopment, and the overall landscape is the result of numerous layersof history.understanding these layers is crucial for discussing the role of thefederal government in computing research. this chapter briefly sets out ahistory of the federal governmentõs programmatic involvement in computing research since 1945, distinguishing the various layers in the his4the organization of federal support:a historical reviewfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.86funding a revolutiontorical eras in which they were first formed. the objective is to identifythe changing role the government has played in these different historicalperiods, discuss the changing political and technological environment inwhich federal organizations have acted, and draw attention to the multiplicity, diversity, and flexibility of publicsector programs that have stimulated and underwritten the continuing stream of u.s. research in computing and communications since world war ii. in fulfilling this charge, thechapter reviews a number of prominent federal research programs thatexerted profound influence on the evolving computing industry. theseprograms are illustrative of the effects of federal funding on the industryat different times. other programs, too numerous to describe in thischapter, undoubtedly played key roles in the history of the computingindustry but are not considered here.19451960: era of government computersin late 1945, just a few weeks after atomic bombs ended world war iiand thrust the world into the nuclear age, digital electronic computersbegan to whir. the eniac (electronic numerical integrator and computer), built at the university of pennsylvania and funded by the armyballistic research laboratory, was americaõs first such machine. the following 15 years saw electronic computing grow from a laboratory technology into a routine, useful one. computing hardware moved from theungainly and delicate world of vacuum tubes and paper tape to the reliable and efficient world of transistors and magnetic storage. the 1950ssaw the development of key technical underpinnings for widespread computing: cheap and reliable transistors available in large quantities, rotating magnetic drum and disk storage, magnetic core memory, and beginning work in semiconductor packaging and miniaturization, particularlyfor missiles. in telecommunications, american telephone and telegraph(at&t) introduced nationwide dialing and the first electronic switchingsystems at the end of the decade. a fledgling commercial computer industry emerged, led by international business machines (ibm) (whichbuilt its electronic computer capability internally) and remington rand(later sperry rand), which purchased eckertmauchly computer corporation in 1950 and engineering research associates in 1952. other important participants included bendix, burroughs, general electric (ge),honeywell, philco, raytheon, and radio communications authority(rca).in computing, the technical cutting edge, however, was usuallypushed forward in government facilities, at governmentfunded researchcenters, or at private contractors doing government work. governmentfunding accounted for roughly threequarters of the total computer field.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.the organization of federal support: a historical review87a survey performed by the army ballistics research laboratory in 1957,1959, and 1961 lists every electronic storedprogram computer in use inthe country (the very possibility of compiling such a list says a great dealabout the community of computing at the time). the surveys reveal thelarge proportion of machines in use for government purposes, either byfederal contractors or in government facilities (weik, 1955, pp. 5761;flamm, 1988).the governmentõs early rolebefore 1960, governmentñas a funder and as a customerñdominatedelectronic computing. federal support had no broad, coherent approach,however, arising somewhat ad hoc in individual federal agencies. theperiod was one of experimentation, both with the technology itself andwith diverse mechanisms for federal support. from the panoply of solutions, distinct successes and failures can be discerned, from both scientificand economic points of view. after 1960, computing was more prominantlyrecognized as an issue for federal policy. the national science foundation and the national academy of sciences issued surveys and reports onthe field.if government was the main driver for computing research and development (r&d) during this period, the main driver for governmentwas the defense needs of the cold war. events such as the explosion of asoviet atomic bomb in 1949 and the korean war in the 1950s heightenedinternational tensions and called for critical defense applications, especially commandandcontrol and weapons design. it is worth noting,however, that such forces did not exert a strong influence on telecommunications, an area in which most r&d was performed within at&t forcivilian purposes. longdistance transmission remained analog, althoughdigital systems were in development at at&tõs bell laboratories. still,the newly emergent field of semiconductors was largely supported bydefense in its early years. during the 1950s, the department of defense(dod) supported about 25 percent of transistor research at bell laboratories (flamm, 1988, p. 16; misa, 1985).however much the cold war generated computer funding, duringthe 1950s dollars and scale remained relatively small compared to otherfields, such as aerospace applications, missile programs, and the navyõspolaris program (although many of these programs had significant computing components, especially for operations research and advanced management techniques). by 1950, government investment in computingamounted to $15 million to $20 million per year.all of the major computer companies during the 1950s had significantcomponents of their r&d supported by government contracts of somefunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.88funding a revolutiontype. at ibm, for example, federal contracts supported more than half ofthe r&d and about 35 percent of r&d as late as 1963 (only in the late1960s did this proportion of support trail off significantly, although absolute amounts still increased). the federal government supported projectsand ideas the private sector would not fund, either for national security,to build up human capital, or to explore the capabilities of a complex,expensive technology whose longterm impact and use was uncertain.many federally supported projects put in place prototype hardware onwhich researchers could do exploratory work.establishment of organizationsthe successful development projects of world war ii, particularlyradar and the atomic bomb, left policymakers asking how to maintain thetechnological momentum in peacetime. numerous new government organizations arose, attempting to sustain the creative atmosphere of thefamous wartime research projects and to enhance national leadership inscience and technology. despite vannevar bushõs efforts to establish anew national research foundation to support research in the nationõs universities, political difficulties prevented the bill from passing until 1950,and the national science foundation (nsf) did not become a significantplayer in computing until later in that decade. during the 15 years immediately after world war ii, research in computing and communicationswas supported by mission agencies of the federal government, such asdod, the department of energy (doe), and nasa. in retrospect, itseems that the nation was experimenting with different models for supporting this intriguing new technology that required a subtle mix of scientific and engineering skill.military research officescontinuity in basic science was provided primarily by the office ofnaval research (onr), created in 1946 explicitly to perpetuate the contributions scientists made to military problems during world war ii. incomputing, the agency took a variety of approaches simultaneously. first,it supported basic intellectual and mathematical work, particularly innumerical analysis. these projects proved instrumental in establishing asound mathematical basis for computer design and computer processing.second, onr supported intellectual infrastructure in the infant field ofcomputing, sponsoring conferences and publications for information dissemination. members of onr participated in founding the associationfor computing machinery in 1947.onrõs third approach to computing was to sponsor machine designfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.the organization of federal support: a historical review89and construction. it ordered a computer for missile testing through thenational bureau of standards from raytheon, which became known asthe raydac machine, installed in 1952 (rees, 1982). onr supported whirlwind, mitõs first digital computer and progenitor of realtime commandandcontrol systems (redmond and smith, 1980). john von neumannbuilt a machine with support from onr and other agencies at princetonõsinstitute for advanced study, known as the ias computer (goldstine,1972; rees, 1982). the project produced significant advances in computerarchitecture, and the design was widely copied by both government andindustrial organizations.other military services created offices on a model similar to that ofonr. the air force office of scientific research was established in 1950to manage u.s. air force r&d activities. similarly, the u.s. army established the army research office to manage and promote army programsin science and technology.national bureau of standardsarising out of its role as arbiter of weights and measures, the national bureau of standards (nbs) had long had its own laboratories andtechnical expertise and had long served as a technical advisor to othergovernment agencies. in the immediate postwar years, nbs sought toexpand its advisory role and help u.s. industry develop wartime technology for commercial purposes. nbs, through its national applied mathematics laboratory, acted as a kind of expert agent for other governmentagencies, selecting suppliers and overseeing construction and delivery ofnew computers. for example, nbs contracted for the three initial univacmachinesððthe first commercial, electronic, digital, storedprogram computersððone for the census bureau and two for the air materiel command.nbs also got into the business of building machines. when the univacorder was plagued by technical delays, nbs built its own computer inhouse. the standards eastern automatic computer (seac) was built forthe air force and dedicated in 1950, the first operational, electronic,storedprogram computer in this country. nbs built a similar machine,the standards western automatic computer (swac) for the navy on thewest coast (huskey, 1980). numerous problems were run on seac, andthe computer also served as a central facility for diffusing expertise inprogramming to other government agencies. despite this significanthardware, however, nbsõs bid to be a government center for computingexpertise ended in the mid1950s. caught up in postwar debates overscience policy and a controversy over battery additives, nbs researchfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.90funding a revolutionfunding was radically reduced, and nbs lost its momentum in the field ofcomputing (akera, 1996).atomic energy commissionnuclear weapons design and research have from the beginning provided impetus to advances in largescale computation. the first atomicbombs were designed only with desktop calculators and punchedcardequipment, but continued work on nuclear weapons provided some ofthe earliest applications for the new electronic machines as they evolved.the first computation job run on the eniac in 1945 was an early calculation for the hydrogen bomb project òsuper.ó in the late 1940s, the losalamos national laboratory built its own computer, maniac, based onvon neumannõs design for the institute for advanced study computer atprinceton, and the atomic energy commission (aec) funded similarmachines at argonne national laboratory and oak ridge national laboratory (seidel, 1996; goldstine, 1980).in addition to building their own computers, the aec laboratorieswere significant customers for supercomputers. the demand created byaec laboratories for computing power provided companies with an incentive to design more powerful computers with new designs. in theearly 1950s, ibm built its 701, the defense calculator, partly with theassurance that los alamos and livermore would each buy at least one.in 1955, the aec laboratory at livermore, california, commissionedremington rand to design and build the livermore automatic researchcomputer (larc), the first supercomputer. the mere specification forlarc advanced the state of the art, as the bidding competition requiredthe use of transistors instead of vacuum tubes (mackenzie, 1991). ibmdeveloped improved ferritecore memories and supercomputer designswith funding from the national security agency, and designed and builtthe stretch supercomputer for the los alamos scientific laboratory, beginning it in 1956 and installing it in 1961. seven more stretch supercomputers were built. half of the stretch supercomputers sold were usedfor nuclear weapon research and design (pugh, 1995; pp. 222223).the aec continued to specify and buy newer and faster supercomputers, including the control data 6600, the star 100, and the cray1 (although developed without aec funds), practically ensuring a marketfor continued advancements (pugh, 1995; p. 192). aec and doe laboratories also developed much of the software used in highperformance computing including operating systems, numerical analysis software, andmatrix evaluation routines (flamm, 1987, p. 82). in addition to stimulating r&d in industry, the aec laboratories also developed a large talentpool on which the computer industry and academia could draw. in fact,funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.the organization of federal support: a historical review91the head of ibmõs applied science department, cuthbert hurd, camedirectly to ibm in 1949 from the aecõs oak ridge national laboratory(hurd, 1994). physicists worked on national security problems with government support providing demand, specifications, and technical input,as well as dollars, for industry to make significant advances in computingtechnology.private organizationsnot all the new organizations created by the government to supportcomputing were public. a number of new private organizations alsosprang up with innovative new charters and government encouragementthat held prospects of initial funding support. in 1956, at the request ofthe air force, the massachusetts institute of technology (mit) createdproject lincoln, now known as the lincoln laboratory, with a broad charter to study problems in air defense to protect the nation from nuclearattack. the lincoln laboratory then oversaw the construction of the semiautomatic ground environment (sage) airdefense system (box 4.1)(bashe et al., 1986, p. 262). in 1946, the air force and douglas aircraftcreated a joint venture, project rand, to study intercontinental warfare.in the following year rand separated from douglas and became theindependent, nonprofit rand corporation.rand worked only for the air force until 1956, when it began todiversify to other defense and defenserelated contractors, such as theadvanced research projects agency and the atomic energy commission, and provided, for a time, what one researcher called òin some sensethe worldõs largest installation for scientific computing [in 1950].ó1 randspecialized in developing computer systems, such as the johnniac, basedon the ias computer, which made rand the logical source for the programming on sage. while working on sage, rand trained hundredsof programmers, eventually leading to the spinoff of randõs systemsdevelopment division and systems training program into the systemsdevelopment corporation. computers made a major impact on the systems analysis and game theoretic approaches that rand and other similar think tanks used in attempts to model nuclear and conventionalwarfighting strategies.engineering research associates (era) represented yet another formof government support: the private contractor growing out of a singlegovernment agency. with era, the navy effectively privatized its wartime cryptography organization and was able to maintain civilian expertise through the radical postwar demobilization. era was founded in st.paul, minnesota, in january 1946 by two engineers who had done cryptography for the navy and their business partners (cohen and tomash,funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.92funding a revolutionbox 4.1project whirlwind and sagetwo closely connected computing projects, whirlwind and sage, demonstratethe influence of federal research and development programs during the early days ofcomputing. they not only generated technical knowledge and human resources, butthey also forged a unique relationship among government, universities, and industry.the whirlwind computer was originally intended to be part of a generalpurposeflight simulator, but it evolved into the first realtime, generalpurpose digital computer. sage, an airdefense system designed to protect against enemy bombers,made several important contributions to computing in areas as diverse as computergraphics, timesharing, digital communications, and ferritecore memories. together, these two projects shared a symbiotic relationship that strengthened the earlycomputer industry.whirlwind originated in 1944 as part of the navyõs airplane stability and controlanalyzer (asca) project. at that time, the navy made extensive use of flight simulators to test new aircraft designs and train pilots; however, each new aircraft designrequired a separate computer specially created for its particular design. asca wasintended to negate the need to build individual computers for the flight simulators byserving as a generalpurpose simulator that could emulate any design programmedinto it. jay forrester, the leader of the computer portion of the asca project, soonrecognized that analog computers (which were typically used on aircraft simulators)would not be fast enough to operate the trainer in real time. learning of work inelectronic digital computing as part of eniac at the university of pennsylvania,forrester began investigating the potential for realtime digital computers for whirlwind. by early 1946, forrester decided to pursue the digital route, expanding thegoal of the whirlwind program from building a generalizable aircraft simulator todesigning a realtime, generalpurpose digital computer that could serve many functions other than flight simulation.pursuing a digital computer required dramatic increases in computing speedsand reliability, both of which hinged on development of improved computer memoryñan innovation that was also needed to handle large amounts of data about incoming airplanes. mercury delayline memories, which used sonic pulses to recordinformation and were being pursued by several other research centers, were too slowfor the machine forrester envisioned. he decided instead to use electrostatic storagetubes in which bits of information could be stored as an electrical charge and whichclaimed readandwrite times of a few milliseconds. such tubes proved to be expensive, limited in storage capacity, and unreliable. looking for a new memory alternative, forrester came across a new magnetic ceramic called deltamax and beganworking on the first magnetic core memory, a project to which he later assigned agraduate student, bill papian.the expansion of whirlwindõs technical objectives resulted in expanding projectbudgets that eventually undermined support for the project. forrester originallyplanned whirlwind as a 2year, $875,000 program, but he increased his cost estimate for the whirlwind computer itself to $1.9 million in march 1946 and to almost$3 million by 1947 (campbellkelly and aspray, 1996, pp. 161163). by 1949,whirlwind made up nearly 65 percent of the office of naval research (onr) mathematics research budget and almost 10 percent of onrõs entire contract researchfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.the organization of federal support: a historical review93budget (edwards, 1996, p. 79). as a part of a general department of defense initiative to centralize computer research in 1951, onr planned to reduce whirlwindõsannual budget from $1.15 million to $250 thousand in 1951, threatening the viability of the project (edwards, 1996, p. 91). support for the project was salvaged onlyafter george valley, jr., a professor of physics at the massachusetts institute of technology (mit) and chairman of the air defense system engineering committee, realized that whirlwind might play a critical role in a new airdefense program, sage,and convinced the air force to provide additional funding for the project, therebyadding to its credibility.in 1949, valley began lobbying the air force to improve u.s. airdefense capability in the face of the nationõs growing vulnerability to soviet bombers (freeman,1995, p. 2). valley was put in charge of the air defense systems engineering committee to investigate possible solutions. the resulting project charles summer studygroup recommended that the air force ask mit to build a laboratory to carry out theexperimental and field research necessary to develop a system to safeguard the united states (freeman, 1995, p. 6). in response, mit created project lincoln, nowknown as lincoln laboratory, to create the semiautomatic ground environment, orsage, system.through sage, the air force became the major sponsor of whirlwind, enablingthe project to move toward completion. by late 1951, a prototype ferritecore memory system was demonstrated, and by 1953, the whirlwindõs entire memory wasreplaced with core memory boasting a 9microsecond access time, effectively ending the research phase of the program. the air force subsequently purchased production versions of the computer (designed in a cooperative effort between mit andibm) to equip each of its 23 direction centers. each center had two ibmmanufactured versions of whirlwind: one operating live and one operating in standby modefor additional reliability. the machines accepted input from over 100 different information sources (typically from ground, air, and seaborne radars) and displayed relevant information on cathoderaytube displays for operators to track and identifyaircraft.the first sage direction center was activated in 1958, and deployment continued until 1963, when final deployment of 23 centers was completed at an estimatedcost of $8 billion to $12 billion. although a technical success, sage was alreadyoutdated by the time of its completion. the launch of sputnik shifted the most fearedmilitary threat to the united states from longrange bombers to intercontinental ballistic missiles. sage command centers continued to operate into the middle of the1980s but with a reduced urgency.all told, onr spent roughly $3.6 million on whirlwind, the air force, $13.8million. in return, whirlwind and sage generated a score of innovations. on thehardware side, whirlwind and sage pioneered magneticcore memory, digitalphoneline transmission and modems, the light pen (one of the first graphical userinterfaces), and duplexed computers. in software, they pioneered use of realtimesoftware; concepts that later evolved into assemblers, compilers, and interpreters;software diagnosis programs; timeshared operating systems; structured programmodules; tabledriven software; and data description techniques. five years after itsintroduction in whirlwind, ferritecore memory replaced every other type of comcontinued on next pagefunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.94funding a revolutionputer memory, and remained the dominant form of computer memory until 1973.royalties to mit from nongovernment sales amounted to $25 million, as mit licensed the technology broadly.1in addition, sage accelerated the transfer of these technologies throughout thenascent computer industry. while lincoln laboratory was given primary responsibility for sage, the project also involved several private firms such as ibm, rand,systems development corporation (the spinoff from rand), burroughs, westernelectric, rca, and at&t.2 through this complex relationship between academia,industry, and the military, sage technologies worked their way into commercialproducts and helped establish the industry leaders. sage was a driving force behindthe formation of the american computer and electronics industry (freeman, 1995, p.33). ibm built 56 computers for sage, earning over $500 million, which helpedcontribute to its becoming the worldõs largest computer manufacturer (edwards,1996, pp. 101102; freeman, 1995, p. 33). at its peak, between 7,000 and 8,000ibm employees worked on the project. sage technology contributed substantiallyto the sabre airline reservation system marketed by ibm in 1964, which later became the backbone of the airline industry (edwards, 1996, p. 102). kenneth olsen,who worked on whirlwind before founding digital equipment corporation, calledwhirlwind the first minicomputer and states that his company was based entirely onwhirlwind technology (old associates, 1981, p. 23).sage also contributed to formalizing the programming profession. while developing software for the system, the rand corporation spun off the systems development corporation (sdc) to handle the software for sage. sdc trained thousands ofprogrammers who eventually moved into the workforce. numerous computer engineers from both ibm and sdc started their own firms with the knowledge they acquired from sage.sage also established an influential precedent for organizational management.lincoln laboratory was structured in the same style as mit had run the radiationlaboratory during world war ii, in that it had much less management involvementthan other equivalent organizations. as a result, researchers had a large amount offreedom to pursue their own solutions to problems at hand. norman taylor, one ofthe key individuals who designed sage at lincoln laboratory credited the management style for the projectsõ successes:i think bob [everett] put his finger on one important thing: the freedom to dosomething without approval from top management. take the case of the65,000 word memory. . . . we built that big memory, and we didnõt go to thesteering committee to get approval for it. we didnõt go up there and say,ònow, hereõs what we ought to do, itõs going to cost this many million dollars,itõs going to take us this long, and you must give us approval for it.ó we justhad a pocket of money that was for advanced research. we didnõt tell anybody what it was for; we didnõt have to. (freeman, 1995, p. 20)this management style contrasted with the more traditional bureaucratic style ofmost american corporations of the time. it was subsequently adopted by digitalequipment corporation (under kenneth olsenõs leadership) and eventually imitatedbox 4.1 continuedfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.the organization of federal support: a historical review951979). the navy moved its naval computing machine laboratory fromdayton to st. paul, and era essentially became the laboratory (tomash,1973; parker 1985, 1986). era did some research, but it primarily workedon taskoriented, costplus contracts. as one participant recalled, òit wasnot a university atmosphere. it was ôbuild stuff. make it work. how doyou package it? how do you fix it? how do you document it?õó(tomash,1973). era built a community of engineering skill, which became thefoundation of the minnesota computer industry. in 1951, for example, thecompany hired seymour cray for his first job out of the university ofminnesota (era, 1950; cohen, 1983; tomash 1973).as noted earlier, the rand corporation had contracted in 1955 towrite much of the software for sage owing to its earlier experience in airdefense and its large pool of programmers. by 1956, the systems trainingprogram of the rand corporation, the division assigned to sage, waslarger than the rest of the corporation combined, and it spun off into thenonprofit systems development corporation (sdc). sdc played a significant role in computer training. as described by one of the participants, òpart of sdcõs nonprofit role was to be a university for programmers. hence our policy in those days was not to oppose the recruiting ofour personnel and not to match higher salary offers with an sdc raise.óby 1963, sdc had trained more than 10,000 employees in the field ofcomputer systems. of those, 6,000 had moved to other businesses acrossthe country (baum, 1981, pp. 4751).observationsin retrospect, the 1950s appear to have been a period of institutionaland technological experimentation. this diversity of approaches, while itby manyñif not mostñof the information technology firms that dot the suburbanboston and silicon valley landscapes. although not the first to pioneer this management style and the organizational ethos it engendered, lincoln laboratory had demonstrated its functionality in large computing systems development.1mit licensed the technology for core memories to several computer companiesñibm, univac, rca, general electric, burroughs, ncr, lockheed, and digital equipment corporationñand memory suppliers, including ampex, fabritek, electronicmemory & magnetics, data products, general ceramics, and ferroxcube. see oldassociates (1981), figure 2 and p. 3.2although at&t is a private company, much of its research was supported througha tax on customers. hence, its research is often considered quasipublic.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.96funding a revolutionbrought the field and the industry from virtually nothing to a tentativestability, was open to criticisms of waste, duplication of effort, and ineffectiveness caused by rivalries among organizations and their fundingsources. the field was also driven largely by the needs of governmentagencies, with relatively little input from computeroriented scientists atthe highest levels. criticism remained muted during the decade when themilitary imperatives of the cold war seemed to dominate all others, butone event late in the decade opened the entire system of federal researchsupport to scrutiny: the launch of sputnik in 1957. attacks mounted thatthe system of r&d needed to be changed, and they came not only fromthe press and the politicians but also from scientists themselves.19601970: supporting a continuing revolutionseveral significant events occurred to mark a transition from the infancy of information technology to a period of diffusion and growth.most important of these was the launching of sputnik in 1957, which sentconvulsions through the u.s. science and engineering world and redoubled efforts to develop new technology. president eisenhower elevated scientists and engineers to the highest levels of policy making.thus was inaugurated what some have called the golden age of u.s.research policy. government support for information technology took offin the 1960s and assumed its modern form. the kennedy administrationbrought a spirit of technocratic reform to the pentagon and the introduction of systems analysis and computerbased management to all aspectsof running the military. many of the visions that set the research agendasfor the following 15 years (and whose influence remains today) were setin the early years of the decade.maturing of a commercial industryperhaps most important, the early 1960s can be defined as the timewhen the commercial computer industry became significant on its own,independent of government funding and procurement. computerizedreservation systems began to proliferate, particularly the ibm/americanairlines sabre system, based in part on prior experience with militarycommandandcontrol systems (such as sage). the introduction of theibm system/360 in 1964 solidified computer applications in business, andthe industry itself, as significant components of the economy (pugh, 1995).this newly vital industry, dominated by òsnow whiteó (ibm) and theòseven dwarfsó (burroughs, control data, ge, honeywell, ncr, rca,and sperry rand), came to have several effects on governmentsupportedr&d. first, and most obvious, some companies (mostly ibm) becamefunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.the organization of federal support: a historical review97large enough to conduct their own inhouse research. ibmõs thomas j.watson research center was dedicated in 1961. its director, emanuelpiore, was recruited from onr, and he emphasized basic research. suchlaboratories not only expanded the pool of researchers in computing andcommunications but also supplied a source of applied research that allowed or, conversely, pushed federal support to focus increasingly on thelongestterm, riskiest ideas and on problems unique to government. second, the industry became a growing employer of computer professionals,providing impetus to educational programs at universities and makingcomputer science and engineering increasingly attractive career paths totalented young people.these years saw turning points in telecommunications as well. in1962, at&t launched the first active communications satellite, telstar,which transmitted the first satelliterelay telephone call and the first livetransatlantic television signal. that same year, a lessnoticed but equallysignificant event occurred when at&t installed the first commercial digitaltransmission system. twentyfour digital speech channels were timemultiplexed onto a repeatered digital transmission line operating at 1.5megabits per second. in 1963, the first stored program control electronicswitching system was placed into service, inaugurating the use of digitalcomputer technology for mainstream switching.the 1960s also saw the emergence of the field called computer science, and several important university departments were founded duringthe decade, at stanford and carnegie mellon in 1965 and at mit in 1968.hardware platforms had stabilized enough to support a community ofresearchers who attacked a common set of problems. new languagesproliferated, often initiated by government and buoyed by the needs ofcommercial industry. the navy had sponsored grace hopper and othersduring the 1950s to develop automatic programming techniques that became the first compilers. john backus and a group at ibm developedfortran, which was distributed to ibm users in 1957. a team led byjohn mccarthy at mit (with government support) began implementinglisp in 1958, and the language became widely used, particularly for artificial intelligence programming, in the early 1960s. in 1959, the pentagonbegan convening a group of computer experts from government,academia, and industry to define common business languages for computers. the group published a specification in 1959, and by 1960 rcaand remington rand univac had produced the first cobol compilers(acm sigplan, 1978). by the beginning of the 1960s, a number of computer languages, standard across numerous hardware platforms, werebeginning to define programming as a task, as a profession, and as achallenging and legitimate subject of intellectual inquiry.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.98funding a revolutionthe changing federal rolethe forces driving government support changed during the 1960s.the cold war remained a paramount concern, but to it were added thedifficult conflict in vietnam, the great society programs, and the apolloprogram, inaugurated by president kennedyõs 1961 challenge. new political goals, new technologies, and new missions provoked changes inthe federal agency population. among these, two agencies became particularly important in computing: the new advanced research projectsagency and the national science foundation.the advanced research projects agencythe founding of the advanced research projects agency (arpa) in1958, a direct outgrowth of the sputnik scare, had immeasurable impacton computing and communications. arpa, specifically charged withpreventing technological surprises like sputnik, began conducting longrange, highrisk research. it was originally conceived as the dodõs ownspace agency, reporting directly to the secretary of defense in order toavoid interservice rivalry. space, like computing, did not seem to fit intothe existing military service structure.2 arpaõs independent status notonly insulated it from established service interests but also tended tofoster radical ideas and keep the agency tuned to basic research questions:when the agencysupported work became too much like systems development, it ran the risk of treading on the territory of a specific service.arpaõs status as the dod space agency did not last long. soon afternasaõs creation in 1958, arpa retained essentially no role as a spaceagency. arpa instead focused its energies on ballistic missile defense,nuclear test detection, propellants, and materials. it also established acritical organizational infrastructure and management style: a small, highquality managerial staff, supported by scientists and engineers on rotation from industry and academia, successfully employing existing dodlaboratories and contracting procedures (rather than creating its own research facilities) to build solid programs in new, complex fields (barberassociates, 1975). arpa also emerged as an agency extremely sensitiveto the personality and vision of its director.arpaõs decline as a space agency raised questions about its role andcharacter. a new director, jack ruina, answered the questions in nouncertain terms by cementing the agencyõs reputation as an elite, scientifically respected institution devoted to basic, longterm research projects.ruina, arpaõs first scientistdirector, took office at the same time askennedy and macnamara in 1961, and brought a similar spirit to theagency. ruina decentralized management at arpa and began the tradifunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.the organization of federal support: a historical review99tion of relying heavily on independent office directors and program managers to run research programs. ruina also valued scientific and technical merit above immediate relevance to the military. ruina believed bothof these characteristicsñindependence and intellectual qualityñwerecritical to attracting the best people, both to arpa as an organization andto arpasponsored research (barber associates, 1975, chapter v). interestingly, arpaõs managerial success did not rely on innovative managerial techniques per se (such as the computerized project scheduling typical of the navyõs polaris project) but rather on the creative use of existingmechanisms such as ònoyear money,ó unsolicited proposals, solesourceprocurement, and multiyear forward funding.arpa and information technology.from the point of view of computing, the most important event at arpa in the early 1960s, indeed in all ofarpaõs history, was the establishment of the information processingtechniques office, ipto, in 1962. the impetus for this move came fromseveral directions, including kennedyõs call a year earlier for improvements in commandandcontrol systems to make them òmore flexible,more selective, more deliberate, better protected, and under ultimate civilian authority at all timesó (norberg and oõneill, 1996, p. 10). computing as applied to command and control was the ideal arpa programñithad no clearly established service affinity; it was òa new area with relatively little established service interest and entailed far less constraint onarpaõs freedom of action,ó than more familiar technologies (barber associates, 1975, p. v5). ruina established ipto to be devoted not to command and control but to the more fundamental problems in computingthat would, eventually, contribute solutions.consistent with his philosophy of strong, independent, and scientificoffice managers, ruina appointed j.c.r. licklider to head ipto. theharvardtrained psychologist came to arpa in october 1962, primarilyto run its command and control group. licklider split that group intotwo disciplineoriented offices: behavioral sciences office and ipto.licklider had had extensive exposure to the computer research of the timeand had clearly defined his own vision of òmancomputer symbiosis,ówhich he had published in a landmark paper of 1960 by the same name.he saw humancomputer interaction as the key, not only to commandand control, but also to bringing together the thendisparate techniques ofelectronic computing to form a unified science of computers as tools foraugmenting human thought and creativity (licklider, 1988b, 1960).licklider formed ipto in this image, working largely independently ofany direction from ruina, who spent the majority of his time on higherprofile and higherfunded missile defense issues. lickliderõs timing wasopportune: the 1950s had produced a stable technology of digital comfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.100funding a revolutionputer hardware, and the big systems projects had shown that programming these machines was a difficult but interesting problem in its ownright. now the pertinent questions concerned how to use òthis tremendous power. . . for other than purely numerical scientific calculationsó(barber associates, 1975).3 licklider not only brought this vision to iptoitself, but he also promoted it with missionary zeal to the research community at large. lickliderõs and iptoõs success derived in large part fromtheir skills at òselling the visionó in addition to òbuying the research.óanother remarkable feature of ipto, particularly during the 1960s,was its ability to maintain the coherent vision over a long period of time;the office director was able to handpick his successor. licklider choseivan sutherland, a dynamic young researcher he had encountered as agraduate student at mit and the lincoln laboratory, to succeed him in1964. sutherland carried on lickliderõs basic ideas and made his ownimpact by emphasizing computer graphics. sutherlandõs own successor,robert taylor, came in 1966 from a job as a program officer at nasa andrecalled, òi became heartily subscribed to the licklider vision of interactive computingó (taylor, 1989). while at ipto, taylor emphasized networking. the last ipto director of the 1960s, lawrence roberts, came,like sutherland, from mit and lincoln laboratory, where he had workedon the early transistorized computers and had conducted arpa researchin both graphics and communications.during the 1960s, arpa and ipto had more effect on the science andtechnology of computing than any other single government agency, sometimes raising concern that the research agenda for computing was beingdirected by military needs. iptoõs sheer size, $15 million in 1965, dwarfedother agencies such as onr. still, it is important to note, onr and arpaworked closely together; onr would often let small contracts to researchers and serve as a talent agent for arpa, which would then fund promising projects at larger scale. arpa combined the best features of existingmilitary research support with a new, lean administrative structure andinnovative management style to fund highrisk projects consistently. theagency had the freedom to administer large block grants as well as multipleyear contracts, allowing it the luxury of a longterm vision to fostertechnologies, disciplines, and institutions. further, the national defensemotivation allowed ipto to concentrate its resources on centers of scientific and engineering excellence (such as mit, carnegie mellon university, and stanford university) without regard for geographical distribution questions with which nsf had to be concerned. such an approachhelped to create universitybased research groups with the critical massand stability of funding needed to create significant advances in particular technical areas. but although it trained generations of young researchers in those areas, arpaõs funding style did little to help them pursue thefunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.the organization of federal support: a historical review101same lines of work at other universities. as an indirect and possiblyunintended consequence, the research approaches and tools and the generic technologies developed under arpaõs patronage were disseminatedmore rapidly and widely, and so came to be applied in new nonmilitarycontexts by the young m.s. and ph.d. graduates who had been trained inthat environment but could not expect to make their research careerswithin it.arpaõs management style.to evaluate research proposals, ipto didnot employ the peerreview process like nsf, but rather relied on internalreviews and the discretion of program managers as did onr. theseprogram managers, working under office managers such as licklider,sutherland, taylor, and roberts, came to have enormous influence overtheir areas of responsibility and became familiar with the entire field bothpersonally and intellectually. they had the freedom and the resources toshape multiple r&d contracts into a larger vision and to stimulate newareas of inquiry. the education, recruiting, and responsibilities of theseprogram managers thus became a critical parameter in the character andsuccess of arpa programs. arpa frequently chose people who hadtraining and research experience in the fields they would fund, and thuswho had insight and opinions on where those fields should go.to have such effects, the program managers were given enough fundsto let a large enough number of contracts and to shape a coherent researchprogram, with minimal responsibilities for managing staffs. programbudgets usually required only two levels of approval above the programmanager: the director of ipto and the director of arpa. one iptomember described what he called òthe joy of arpa . . . . you know, if aprogram manager has a good idea, he has got two people to convince thatthat is a good idea before the guy goes to work. he has got the director ofhis office and the director of arpa, and that is it. it is such a short chainof commandó (taylor, 1989).part of arpaõs philosophy involved aiming at radical change ratherthan incremental improvement. as robert taylor put it, for example,incremental innovation would be taken care of by the services and theircontractors, but, arpaõs aim was òan order of magnitude difference.ó4arpa identified good ideas and magnified them. this strategy oftennecessitated funding large, grouporiented projects and institutions ratherthan individuals. taylor recalled, òi donõt remember a single case wherewe ever funded a single individualõs work. . . . the individual researcherwho is just looking for support for his own individual work could [potentially] find many homes to support that work. so we tended not to fundthose, because we felt that they were already pretty well covered. instead, we funded larger groupsñteams.ó nsfõs peerreview processfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.102funding a revolutionworked well for individual projects, but was not likely to support large,teamoriented research projects. nor did it, at this point in history, support entire institutions and research centers, like the laboratory for computer science at mit. iptoõs style meshed with its emphasis on humanmachine interaction, which it saw as fundamentally a systems problemand hence fundamentally team oriented. in taylorõs view, the universityreward structure was much more oriented toward individual projects, soòsystems research is most difficult to fund and manage in a universityó(taylor, 1989). this philosophy was apparent in arpaõs support ofproject mac, an mitled effort on timeshared computing (box 4.2).arpa, with its clearly defined mission to support dod technology,could also afford to be elitist in a way that nsf, with a broader charter tosupport the countryõs scientific research could not. òarpa had no commitment, for example, to take geography into consideration when itfunded workó(taylor, 1989). another important feature of arpaõs multiyear contracts was their stability, which proved critical for graduate students who could rely on funding to get them through their ph.d. program. arpa also paid particular attention to building communities ofresearchers and disseminating the results of its research, even beyondtraditional publications. ipto would hold annual meetings for its contract researchers at which results would be presented and debated. thesemeetings proved effective not only at advancing the research itself butalso at providing valuable feedback for the program managers and helping to forge relationships between researchers in related areas. similarconferences were convened for graduate students only, thus building alongerterm community of researchers. arpa also put significant effortinto getting the results of its research programs commercialized so thatdod could benefit from the development and expansion of a commercialindustry for information technology. arpa sponsored conferences thatbrought together researchers and managers from academia and industryon topics such as timesharing, for example.much has been made of arpaõs management style, but it would be amistake to conclude that management per se provided the keys to theagencyõs successes in computing. the key point about the style, in fact,was its light touch. red tape was kept to a minimum, and project proposals were turned around quickly, frequently into multipleyear contracts.typical dod research contracts involved close monitoring and carefuladherence to requirements and specifications. arpa avoided this approach by hiring technically educated program managers who had continuing research interests in the fields they were managing. this realitycounters the myth that government bureaucrats heavyhandedly selectedr&d problems and managed the grants and contracts. especially duringthe 1960s and 1970s, program managers and office directors were notfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.the organization of federal support: a historical review103box 4.2project mac and computer timesharingthe development of computer timesharing and the advent of minicomputers setthe technological stage for the 1970s. timesharing systems divide computationpower cyclically between many users over a network. properly designed timesharing computers can switch among processes quickly enough so that users do notrecognize any delay, making it appear as though each user has the computerõs fullattention. such systems took advantage of design and manufacturing peculiarities ofmainframes that resulted in the power of a mainframe computer varying as the squareof cost of the computer.1 therefore, building one computer for twice the cost of asmaller machine created four times the power. timesharing systems took advantageof this phenomena by allowing several users to share a single larger computer instead of several smaller machines. development of such systems emerged from thecomplementary efforts of industry, universities, and government. key to these effortswere project mac and its predecessors, funded by the advanced research projectsagency and the national science foundation (nsf). while project mac was notresponsible for the first timesharing system, it played a significant role in the technologyõs development.project mac was started by ipto in 1963, with funding going to the massachusetts institute of technology (mit). mac stood for man and computer, machineaided cognition, and multiaccess computer. j.c.r. licklider chose mit as the sitefor project mac because of the large variety of computer disciplines being studied atmit. project mac brought together, for example, marvin minskyõs artificial intelligence work, douglas rossõs computeraided design systems, herbert teagerõs studies in languages and devices, and martin greenbergerõs work with humanmachinesystems. while the program was justified to the military as a commandandcontrolprogram, lickliderõs goal was much broader. he sought òthe possibility of a profound advance, which will be almost literally an advance in the way of thinkingabout computing.ó in an interview with the charles babbage institute, licklider said,òi wanted interactive computing, i wanted timesharing. i wanted themes like: computers are as much for communication as they are for calculationó (norberg andoõneil, 1996, pp. 9798). project mac would eventually receive $25 million intotal from 1963 to 1970 (reed et al., 1990, chapter 19, p. 14).the core of project mac involved the design of a timesharing computer system.project mac was not the first timesharing initiative, but it significantly pushed thestate of the art. timesharing systems had previously been developed in the mitcomputation center, at system development corporation, and at bolt, beranek andnewman. at first, project mac used the mit computation centerõs compatibletimesharing system (ctss), which had been designed under a grant from nsf. thesystem was built on an ibm 7090/94 and became operational in 1961. this was thefirst system enabling users to write their own programs online (reed et al., 1990, pp.192 to 193). in 1964, ctss was connected to 24 terminals across the mit campus.eventually, 160 terminals were in place and 30 could be in use at one time. however, the ctss still could not provide as much power as researchers desired, and itlacked necessary data access security.continued on next pagefunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.104funding a revolutionbeginning in 1965, project mac began to create a second system with the help ofgeneral electric and bell laboratories: multics (multiplexed information andcomputing service), was completed in 1969 and would eventually support 1,000terminals at mit with 300 in use at any one time (campbellkelly and aspray, 1996,pp. 214215). multics also incorporated a multiuser file system and a complexvirtualmemory system that allowed application programs to function as if availablememory were much larger than the memory actually attached to the processor. itfeatured an automatically managed threelevel memory system, controlled sharingand protection of data and programs accessed by multiple users, and the ability toreallocate its resources dyamically without interruption. multics had a multiuserfile system that allowed each user to work as if on an independent computer (flamm,1987, p. 58).project mac led to many advances beyond timesharing. mitõs artificial intelligence laboratory received $1 million in funding through project mac for work tofurther the objectives of interactive computing (of which timesharing was an integralpart) and intelligent assistance (norberg and oõneill, 1996). funds also went towardresearch in input/output devices. one of the earliest computeraided design systems,kludge, was developed through project mac. project macõs ability to composeand edit programs and documents online laid the groundwork for word processorsand interactive programming. the idea for the spreadsheet, later popularized bylotus 123 and subsequently microsoftõs excel, also came from two students whoworked on project mac. this idea spurred development of the first spreadsheet onthe personal computer, visicalc, from software arts. the first real networking of thepersonal computer (the first version of internet protocols for the pc) also came frommitõs project mac (renamed the laboratory for computer science by then), whichled to the company called ftp software. ftp sold the first internet protocol suite fordos.another lasting spinoff from project mac was the popular operating system,unix. the difficulty that bell laboratories had in developing the multics operatingsystem led to a new philosophy of software design stressing simplicity and elegance.in 1969, when bell laboratories realized that a commercial product was still manyyears away, it withdrew from project mac. over the next 5 years, bell researcherskenneth thompson and dennis ritchie, along with others who had been workingwith mac and had become frustrated with multicsõs complexity, developed unix,which was based on multics but was much simpler. it offered quick responses,had minimal system overhead, and ran on minicomputers instead of more expensivemainframes with special memory management systems.beyond the technical advances in timesharing, project mac influenced anindustrywide movement toward developing timesharing computers. when searching for a contractor to supply the hardware for multics, mit turned down its traditional supplier, ibm, and hired general electric (ge) because of ibmõs unwillingnessto modify their machines for the project. the early results of project mac, though,convinced ibm and other manufacturers that they would have to pursue timesharing(campbellkelly and aspray, 1996, p. 215). by 1967, 20 firms were competing for a$20 million industry providing timeshared computer services to businesses acrossthe nation including ge, telcomp, tymshare, keydata, and university computingbox 4.2 continuedfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.the organization of federal support: a historical review105bureaucrats but were usually academics on a 2year tour of duty. theysaw arpa as a pulpit from which to preach their visions, with money tohelp them realize those visions. the entire system displayed somethingof a selforganizing, selfmanaging nature. as ivan sutherland recalled,ògood research comes from the researchers themselves rather than fromthe outside.ó5national science foundationwhile arpa was focusing on large projects and systems, the nationalscience foundation played a large role in legitimizing basic computerscience research as an academic discipline and in funding individual researchers at a wide range of institutions. its programs in computing haveevolved considerably since its founding in 1950, but have tended to balance support for research, education, and computing infrastructure. although early programs tended to focus on the use of computing in otheracademic disciplines, nsf subsequently emerged as the leading federalfunder of basic research in computer science.nsf was formed before computing became a clearly defined researcharea, and it established divisions for chemistry, physics, and biology, butnot computing. nsf did provide support for computing in its early years,company. by the mid1970s, almost every mainframe computer sold incorporatedtimesharing technology (reed et al., 1990, pp. 914).project mac was largely responsible for bringing the computer out of the laboratory and business and leading it to the home. lickliderõs desire to create a ònew wayof thinkingó about computers succeeded. project mac developed technology andideas that allowed interactive computing to become a reality. . . .ó as a result ofproject mac and other computer timesharing research programs in the late 1960s,the concept of computer utilities became widely accepted in the computer and business world. in 1964, only one year after project mac began, martin greenbergerwrote, òbarring unforeseen obstacles, an online interactive computer service, provided commercially by an information utility, may be as commonplace by 2000a.d. as the telephone service is todayó (campbellkelly and aspray, 1996, p. 217).the image greenberger described is remarkably similar to the internet. before timesharing became a reality, computing remained available only to large businesses,academic institutions, and the government. however, as more users could simultaneously use a single machine, the cost of computing dramatically decreased, andusage increased accordingly. project mac played a large role in the publicõs changeof philosophy regarding the use of computers.1this relationship between cost and the power of mainframes was often referred toas groschõs law.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.106funding a revolutionbut this support derived more from a desire to promote computerrelatedactivities in other disciplines than to expand computer science as a discipline, and as such was weighted toward support for computing infrastructure (nsf, 1956, p. 57). for example, nsf poured millions of dollarsinto university computing centers so that researchers in other disciplines,such as physics and chemistry, could have access to computing power.nsf noted that little computing power was available to researchers atamerican universities who were not involved in defenserelated researchand that òmany scientists feel strongly that further progress in their fieldwill be seriously affected by lack of access to the techniques and facilitiesof electronic computationó (nsf, 1958, p. 103). as a result, nsf begansupporting computing centers at universities in 1956 and, in 1959, allocated a budget specifically for computer equipment purchases. recognizing that computing technology was expensive, became obsolete rapidly,and entailed significant costs for ongoing support, nsf decided that itwould, in effect, pay for american campuses to enter the computer age.in 1962, it established its first office devoted to computing, the programfor computers and computing science within the mathematical sciencesdivision (aspray and williams, 1994). by 1970, the institutional computing services (or facilities) program had obligated $66 million to university computing centers across the country.6 nsf intended that use of thenew facilities would result in trained personnel to fulfill increasing needsfor computer proficiency in industry, government, and academia.nsf provided some funding for computerrelated research in its earlyyears. originally, such funding came out of the mathematics division inthe 1950s and grew out of an interest in numerical analysis. by 1955, nsfbegan to fund basic research in computer science theory with its firstgrants for the research of recursion theory and one grant to develop ananalytical computer program under the mathematical sciences program.although these projects constituted less than 10 percent of the mathematics budget, they resulted in significant research.in 1967, nsf united all the facets of its computing support into asingle office, the office of computing activities (oca). the new officeincorporated elements from the directorates of mathematics and engineering and from the facilities program, unifying nsfõs research andinfrastructure efforts in computing. it also incorporated an educationalelement that was intended to help meet the radically increasing demandfor instruction in computer science (aspray and williams, 1994). theoca was headed by milton rose, the former head of the mathematicalsciences section, and reported directly to the director of nsf.originally, the ocaõs main focus was improving university computing services. in 1967, $11.3 million of the officeõs $12.8 million total budgetwent toward institutional support (nsf, 1967, pp. 5354). because not allfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.the organization of federal support: a historical review107universities were large enough to support their own computing centersbut would benefit from access to computing time at other universities, theoca also began to support regional networks linking many universitiestogether. in 1968, the oca spent $5.3 million, or 18.6 percent of its budget, to provide links between computers in the same geographic region(nsf, 1968). in the 1970s, the computer center projects were canceled,however, in favor of shifting emphasis toward education and research.beginning in 1968, through the education and training program, theoca began funding the inauguration of universitylevel computer science programs. nsf funded several conferences and studies to developcomputer science curricula. the education and training program obligated $12.3 million between 1968 and 1970 for training, curricula development, and support of computerassisted instruction.7although the majority of the ocaõs funding was spent on infrastructure and education, the office also supported a broad range of basic computer science research programs. these included compiler and languagedevelopment, theoretical computer science, computation theory, numerical analysis, and algorithms. the computer systems design programconcentrated on computer architecture and systems analysis. other programs focused on topics in artificial intelligence, including pattern recognition and automatic theory proving.19701990: retrenching andinternational competitiondespite previous successes, the 1970s opened with computing at acritical but fragile point. although produced by a large and establishedindustry, commercial computers remained the expensive, relatively esoteric tools of large corporations, research institutions, and government.computing had not yet made its way to the common user, much less theman in the street. this movement would begin in the mid1970s with theintroduction of the microprocessor and then unfold in the 1980s witheven greater drama and force. if the era before 1960 was one of experimentation and the 1960s one of consolidation and diffusion in computing,the two decades between 1970 and 1990 were characterized by explosivegrowth. still, this course of events was far from clear in the early 1970s.computer science, computer technologyby 1970, computer science was just emerging as a discipline. many ofthe major computer science departments were established (at places likestanford university, mit, and carnegie mellon university), but computer science did not yet have the academic legitimacy of the older fieldsfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.108funding a revolutionof physics, chemistry, and biology. was computer science really a science? although much theoretical work examined fundamental questionsof computability that are independent of computing hardware, manyproblems for computing research stemmed from experience with the construction and use of actual computers (manmade instruments as opposed to naturally occurring phenomena).8 during the 1970s, computerscientists would continue to answer these questions with a growing andmature body of theoretical work.technologically, the 1970s, like the 1950s, might be characterized as adecade of experiments. the unix operating system grew to prominenceduring this decade, at first in research environments and then increasingly in industry. although the minicomputer industry competed successfully with mainframes, it faced a threat of its own: intel delivered thefirst microprocessor, the 4004, in 1971, soon followed by the 8bit 8008, thebasis of the first personal computers. networking became an increasingfocus of research and systems: the arpanet, although formulated in the1960s, became an operational system in the 1970s: it had 4 nodes in 1970,23 in the next year, and was publicly demonstrated in washington in1972. in 1973, xerox unveiled its alto personal computer, a system ofboxes, each of which was controlled with a graphical user interface and amouse, with each box connected to others throughout the palo alto research center (parc) through an ethernet network. still, it would takealmost another 20 years before this visionary technologyõs prototype became the tangible reality of the world of business computing in the unitedstates.also during the 1970s, a veritable computer culture emergedñhobbyists who touted computer liberation and experimentation with small microprocessorbased machines, often outside of institutional environments.it took steve jobs, apple computer, and the computerized spreadsheet,however, to turn the hobbyist personal computer into the ubiquitous pieceof business equipment and consumer product it later became. popularmythology celebrates the independent entrepreneurs who produced thepersonal computer (pc) revolutionñsteve jobs at apple, mitch kapor atlotus, and bill gates at microsoft. these innovators built upon ideasdeveloped previously, many of them with government funding (box 4.3).ibm also played a critical role in making the new technology establishedand acceptable with its 1981 introduction of the ibm pc. packaged withlotus 123 and msdos, the ibm pc gave the business marketplace whatit wanted from a personal computer (malone, 1995).until about 1980, truly capable computers remained large boxes. thisbegan to change with the birth of the desktop workstation, based on themicroprocessor. after xerox built its alto computers, it donated 10 machines to stanfordõs computer science department. they inspired forestfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.the organization of federal support: a historical review109box 4.3roots of the personal computerthe development of the personal computer (pc) is illustrative of the symbiosisbetween government and industry in the evolving computer industry. while the pcstands as a monument to industrial innovation and the foresight and tenacity of individual entrepreneurs, federally sponsored research also played a role. the macintoshoperating system and microsoft windows, which trace their lineage to the alto computer developed by xerox between 1973 and 1978, incorporate concepts first explored by researchers working with federal support.in the 1960s, the advanced research projects agency (arpa) and the nationalaeronautics and space administration provided funding for douglas engelbart tocreate a new research program at the stanford research institute to work on improving humancomputer interactions. engelbartõs research concentrated on using computers to augment the abilities of an individual as opposed to automating those abilities. in 1968, at the joint computer conference, engelbart presented the nls(online system), a computerized office system that his group developed. the nlswas the first system to use a mouse and the first to use windows. the invention of themouse and its use as part of a graphical user interface represented a dramatic changefrom the standard commandline operation of computers. most mainframe and timesharing systems at the time relied on typed commands that computer novices foundcryptic and difficult to use. text on the screen could often be edited only by referencing the line number as opposed to changing the text in place. the use of a mouseand graphical user interface began the trend to make computers usable by anyone.designers at the xerox palo alto research center (parc) later incorporated engelbartõs advances into a graphical user interface for xeroxõs alto computer. thealto was designed for users including òchildren from age 5 or 6 and ônoncomputeradultsõ such as secretaries, librarians, architects, musicians, housewives, doctors andso onó (acm, 1993, p. 29). the alto also drew upon the ideas described in alankayõs doctoral thesis, work that was also supported by arpa while kay was at theuniversity of utah. kay described a computer called flex that would act as òaninteractive tool which can aid in the visualization and realization of provocativenotions. it must be simple enough so that one does not have to become a systemsprogrammer (one who understands the arcane rites) to use it. it must be cheapenough to be owned (like a grand piano). it must do more than just be able to realizecomputable functions; it has to be able to form the abstractions in which the userdeals. flex is an idea debugger and as such, it is hoped that it is also an ideamedia.ó1 kay envisioned this computer of the future to be the size of a notebook,one that could handle all of an individualõs personal information management andmanipulation needs. kay later called this computer the dynabook. kay was not ableto build an operational dynabook for his thesis, but the new computing context wasinfluential. òsince at first people shared computers, the idea that everyone shouldhave their own was a breakthroughó (acm, 1993, p. 31).robert taylor, the associate manager of the computer science laboratory (csl)at parc recruited alan kay for the xerox system science laboratory (ssl) in anattempt to integrate the ssl and csl in working toward a shared goal. taylor was aformer director of arpaõs information processing techniques office and used hiscontinued on next pagefunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.110funding a revolutionbaskett and student andy bechtolsheim to build a successor for engineering and scientific applications. the stanford university network (sun)developed new desktop computers with ethernet networking and highresolution, highspeed graphics, tapping into darpaõs very large scaleintegrated circuit (vlsi) program en route. in 1982, bechtolsheim, vinodkhosla, and scott mcnealy acquired venture capital to found sunmicrosystems, inc.by 1980, the sales of the computer equipment industry made up asignificant share of the value of all domestically produced goods andservices (gdp) (table 4.1). the share of gdp contributed by the computing and office equipment industry continued to grow over the next decade, and investments in computing, communications, and office equipment began to absorb more than half of all gross fixed business investmentin plant and equipment. the industry routinely built for commercialusers complex systems combining computing and communicationsñtechnology once reserved for the military. software became increasinglyprominent, as a massmarket industry selling shrinkwrapped products,and as a subject of intellectual and managerial inquiry as the òsoftwareknowledge of the field and the key researchers in it to staff the laboratory and providedirection. he followed the same principles he used at arpa: enlisting the mosttalented researchers and giving them the freedom to follow their own imagination.2taylor planned for the csl to create the hardware infrastructure for distributed personal computing and for ssl to design software and applications for it (smith andalexander, 1988, pp. 7071). while working in the ssl, kay developed the smalltalk language on which most of altoõs software was developed. smalltalk was thefirst objectoriented programming language.xerox was never able to market the alto successfully, but its influence is noticeable in most business and home computers in use today. in 1979, steve jobs wasinvited to tour xerox parc. jobs realized the potential for the alto system. he toldthe demonstrator of the system, larry tesler, òwhy isnõt xerox marketing this? . . .you could blow everything awayó (smith and alexander, 1988, p. 241). jobs thenincorporated many aspects of the alto into the apple lisa, first produced in 1983,and its successor, the macintosh. the popularity of graphical user interfaces grewrapidly. eventually microsoft introduced windows, beginning the conversion of x86pcs from the commandline operating system dos to the operating systems prevalent today.1alan kay as quoted in smith and alexander (1988).2taylor was not alone in his management style at ipto. other program managersand office managers at darpa, including j.c.r. licklider, used a similar style.box 4.3 continuedfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.the organization of federal support: a historical review111crisisó increasingly demonstrated the difficulty of bringing in large programming projects on time and within budget. as the computer industryexploded, traditional industrial research and development increased proportionally. but only the largest companies could afford broadbasedresearch efforts to rival those of universities and government laboratories.in 1984, for example, ibm still conducted 50 percent of the r&d (by dollarvalue) in the computer industry as a whole (flamm, 1987).the changing political contextwhile the 1970s and 1980s saw explosions in the growth of technology,they also witnessed a changing environment for governmentsupportedresearch. during the 1970s, the war in vietnam became the driving force,tending to redirect research toward military purposes and raising concerns about the effect of defense funding on university research. duringthe 1980s, traditional defense concerns gave way to industrial competitiveness as the primary driver of research policy. both these changes hada significant effect on the nature, structure, and direction of federallysponsored research in computing.science and politics in the 1970s: a changed climatetension over the vietnam war brought campus protests against thewar and against defenserelated research on campus, forcing some universities to change their policies. as the costs of the war escalated, research budgets were increasingly squeezed within the pentagon. in the1970s, despite the fabulous success of the apollo program in putting aman on the moon in 1969, a general skepticism about the role of science intable 4.1 computing and related equipment as a share of thenational economygross domesticsales of computing andcomputer equipmentproduct (gdp)related equipmentas a percentageyear(in billions of dollars)(in billions of dollars)of gdp1960 513 1.50.319701,010 10.51.019802,708 55.12.019905,546154.82.819957,117204.82.9sources: national science board (1996); iti (1997).funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.112funding a revolutionsocietyñand hence the role of scientific researchñbegan to emerge. divisions over vietnam, heightened distrust of authority in the wake ofwatergate, the oil crisis of 1973, and increased awareness of pollution andenvironmental damage all contributed to the changed role of science inthe public sphere. dod funding for mathematics and computer sciencereached a twodecade low in 1975. government support for science andtechnology, although not necessarily in crisis, would never again enjoythe same prominence it had in the previous decade; the golden age ofresearch support was over.politics intervened in other ways during this time, too. the nixonadministration, for example, did not think nsf should be in the businessof developing computer networks, seeing such activities as the provinceof private business. as a result, nsfõs activities were severely curtailed inthis area. the nixon administration also pushed for more directed research programs in computer science that addressed specific nationalproblems, such as education and environment, rather than letting theresearch community have most of the role in defining research directions.these sentiments were matched by similarly motivated actions in congress.in 1969, congress forbade military funding for any research that didnot have a òdirect or apparent relationship to a specific military functionor operations.ó this legislation, enacted into law as the mansfield amendment (named after its sponsor) to the defense authorization act of 1970(public law 91121), was short lived, but it sent a strong signal to theresearch community: it would have to demonstrate the military relevanceof its work. some program managers thought this would involve merelyrewriting project descriptions with an emphasis on applications, and nodoubt frequently they were correct. but the mansfield amendment, andthe mood that gave rise to it, had the longerterm impact of shortening thetime horizons for government research support in general and defenseresearch in particular. both arpa and nsf materially felt the effects ofthis new climate in their computing programs.policy for the 1980s: industrial research and competitivenessin the 1980s, fears were raised that the microelectronics and computerindustries seemed to be going the way of the auto industryñto japan.just as in the automobile industry, in which the japanese had masteredmanufacturing technology before turning their attention to design, thejapanese integrated circuit companies first captured a dominant percentage of the dynamic random access memory (dram) industry. they began with the processintensive memory chips and then turned their attention to moredesignintensive processors. as a result, many believed u.s.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.the organization of federal support: a historical review113industry to be in trouble in the early 1980s. compounding the alarm wasthe declining market share of the semiconductor equipment industry,which makes the intricate manufacturing equipment for chips: its share ofthe world market fell from 75 percent to 40 percent during the 1980s (alicet al., 1992). òcompetitivenessó became the keyword for u.s. technologypolicy in the 1980s.much of the vast literature analyzing the competitiveness problemfocused on the role of government and governmentsponsored research.japanõs ministry of trade and international development played a keyrole in bringing japanese companies together to cooperate in targetingnew markets and technologies. in the united statesñamid calls forgovernment actionñjoint ventures, cooperative agreements, universityindustry collaborations, and industry consortia began to emerge to fightthe japanese threat. the national cooperative research act of 1984 exempted research consortia from some antitrust laws and facilitated thesemergers. the microelectronics and computer technology corporation,formed in january 1983, was entirely privately funded (at $60 million to$70 million in 1985) by its 12 member companies. of these new initiatives,sematech, the semiconductor manufacturing technology consortium,was most significant as a governmentsupported venture.changes in the organization of federal research supportresponses to the changing policy environment echoed throughoutthe federal research establishment. significant changes in organizationand management occurred at darpa, nsf, and other federal agencies.new federal initiatives, such as sematech and highperformance computing, began to dominate the research and policy agenda. these changesalso reflected advances in computing technology and the evolution of thecomputing industry. new structures and missions allowed federal agencies to interact better with a growing industry that had an expandingrange of capabilities and needs.changes at arpaarpaõs name was officially changed to darpa (the defense advanced research projects agency) in 1972, presaging changes in iptoand its personnel as well. george heilmeier, director of darpa from1975 to 1977, came, unlike his predecessors, from an industrial background. heilmeier brought an emphasis on applications to darpa and amore formalized management style to the agency. as one program manager recalled, òduring the 1970s . . . there was tremendous pressure toproduce stuff that looked like it had a short applications horizon.ó9 thefunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.114funding a revolutionshortening time horizon had tangible effects, especially in ipto. j.c.r.licklider, who had started the office in the early 1960s, with a free handand in his own image, returned for a stint as program director in 19741975 and found it a changed place (norberg and oõneill, 1996, pp. 3738).after that, the agency had difficulty finding a successor to serve as director of ipto.these changes at darpa, and in particular at ipto, represented thenatural evolution of an organization as it matures. iptoõs funding morethan doubled from the $9 million of 1962 to $23 million in 1970, and itaccounted for most of dodõs basic research and about half of the appliedresearch in computing (norberg and oõneill, 1996, p. 55). in that senselicklider and his cohort had been victims of their own success: iptoleadership no longer had to evangelize and legitimate the field; theymerely administered the research of an established areañan equally important, if perhaps less entrepreneurial, endeavor. furthermore, mansfielderachanges did bring some benefits. at first iptoõs computer research had allbeen classified as 6.1, dod parlance for basic research. now the emphasis shifted to 6.2, or òexploratory development,ó which expanded. evenin the early 1970s, 6.2 constituted more than half of the ipto budget andafter 1971 was responsible for most of its growth. as mentioned above,the shift also had the effect of transferring much of the basic research fromdarpa to nsf.arguably, this change in the priority of applications and development, although potentially threatening to lickliderõs original vision (andsometimes odious to academic investigators), built upon a decade of basicresearch. iptosponsored research had created numerous new ideas thatcould now be tried on a large scale. indeed, ipto had several large,applicationsoriented programs already under way in the early 1970s,including the illiac iv and the arpanet (see chapter 7). the first wasa modular parallel supercomputer being built at the university of illinois.the second project, arpanet, was built to demonstrate principles ofcomputer networking that had been worked out in the previous decade.both of these projects emphasized hardware, and both were built underlarge contracts let to industrial contractors (illiac by burroughs andarpanet by bolt, beranek, and newman). together, illiac iv andarpanet consumed a significant portion of iptoõs budget in 1972.nevertheless, the changes in darpaõs focus generated considerable controversy that continues to this day.one man epitomized the new approach at darpa. robert kahnjoined the agency after a stint on the mit faculty and at bolt, beranek, andnewman, where he worked on the construction of the original arpanet.he joined darpa as a program manager in 1972 and eventually tookover as director of ipto in 1979. kahn embraced the new darpa envifunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.the organization of federal support: a historical review115ronment and turned it to iptoõs benefit. as a program manager and atechnical leader, kahn collaborated with contractors, defining systems forpacket radio, networking, and eventually the internetworking protocolsthat became the transmission control protocol/internet protocol (tcp/ip). on the latter effort, kahn worked closely with vinton cerf, who wasfirst at the university of california, los angeles, then at stanford, andthen assumed kahnõs networking responsibilities as a program managerat darpa.when kahn became director at ipto, his main direction fromheilmeier was to apply a òforcing functionó to artificial intelligence (ai)òto produce something that would be usefuló (kahn, 1989). in addition topushing ai, kahn had two major goals of his own: (1) restoring, and thenincreasing, budgets for basic research (6.1), which had declined duringthe 1970s; and (2) increasing the involvement of industry in darpa programs, creating overt links between universities and companies to transfer technology. òtransfer was all happening . . . by the invisible hand ofthe marketplace, or venture capital, or something. . . . but darpa was nottaking any role,ó kahn recalled.to accomplish his first goal, kahn separated iptoõs applications programs from basic research so they could be managed in different styles.the engineering applications office (eao) split off for applications andòtechnology baseó efforts. the move met with questionable success, and,when saul amarel succeeded kahn as head of ipto, he thought that eaoand ipto were unnecessarily competing for resources. the two officeswere recombined into the information systems technology office (isto).kahn developed two major strategies to achieve his goals: the very largescale integrated circuits program and the strategic computing initiative.very large scale integrated circuits.10efforts to develop very large scaleintegrated circuit (vlsi) technology demonstrate the role darpa playedin the growing computing industry by identifying technological developments of interest to dod and the industry as a whole and helping themreach a state of greater maturity. pioneering work in vlsi was conductedin the mid1970s by carver mead, a professor at the california institute oftechnology (caltech) with interests in semiconductor technology, andlynn conway, an expert in computer architecture at xerox parc. encouraged by bert sutherland, conwayõs laboratory manager at xerox,and bertõs brother, ivan sutherland, chair of computer science at caltech,mead and conway developed a simplified, standardized system designmethodology and layout design rules for vlsi system and circuit design.their design methods allowed integrated circuit (ic) designers to morequickly and easily design new ics. conway also innovated a new form ofnetworkbased, fastturnaround vlsi prototyping service at parc.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.116funding a revolutioncalled the mpc implementation system, the service enabled chips designers at many locations around the country to submit design files overthe arpanet for lowcost, rapid fabrication. the mpc system becamethe basis of what was later called the metal oxide silicon implementationservice, or mosis.mead and conway propagated their new design methods and rulesthrough courses they taught during 1978 and 1979, first conwayõs courseat mit and then additional courses at other universities such as stanford,university of california at berkeley (ucberkeley), and caltech, exploiting prepublication versions of their new textbook about the methods. inthe fall of 1979, conway and her group at xerox parc used the mpcsystem to provide rapid chip prototyping for student design projects atmany universities. the success of the many mpc79 designs validatedtheir methods and quickly led to more widespread use of their designmethodology. their book introduction to vlsi systems was published byaddisonwesley in 1980 (mead and conway, 1980).the meadconway approach also spurred development of a rich variety of computer designs as well as related supporting technologies forchecking and testing designs, for graphics editors, and for simulators.the design methods and rules formed the basis of the specification language used in the mosis program and provided the essential ingredientfor developing computeraided design tools for vlsi layouts. the firstsuch tool, icarus, resulted in 1976 from the work of douglas fairbairnat xerox parc and james rowson at caltech. this tool was used in vlsidesign courses at stanford and adopted by a number of researchers. jamesclark, then an associate professor at stanford university, used vlsi toolsand techniques to develop a geometry engine for producing complexcomputer graphic images. in 1982, clark founded silicon graphics, inc.,which commercialized the technology and subsequently became a leaderin visual computing systems.11darpaõs vlsi program built upon these early efforts. formally initiated by robert kahn in 1978, the darpa program grew out of a study itcommissioned at rand corporation in 1976 to evaluate the scope ofresearch darpa might support in vlsi (sutherland et al., 1976). thefinal report, written by ivan sutherland, carver mead, and thomaseverhardt, concluded that continued attempts to increase computationalpower by packing more devices onto a single integrated circuitñas industry was attemptingñignored the possibility of even greater gainsthrough wholly new computer architectures. as the report noted, theadvancement of vlsi technologies required new paradigms for integratedcircuit designs, because the circuit elements themselves would becomecheap, but the interconnections between them would become more expensive.12 sutherland and mead published a derivative article in scienfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.the organization of federal support: a historical review117tific american in september 1977 to gain an even broader audience fortheir ideas (sutherland and mead, 1977).darpaõs plan for its vlsi program was to foster revolutionary advances by supporting university research and building bridges betweenresearch communities. to promote information sharing, darpa maintained open, nonrestrictive policies on the publication of results, supported research with only indirect connections to military or defense applications, and refrained from classifying results.13 these principles stoodin direct contrast to dodõs other main semiconductor initiative of thetime, the very high speed integrated circuit (vhsic) program, whichtried to advance industrial practices in a more incremental fashion, required direct defense relevance, and had a number of restrictions in placeon publication of results.darpa played a strong role in identifying vlsi as an area for strategic direction but allowed much of the program content to emerge fromthe research community. proposals were supported on the basis of theirindividual persuasiveness and the track record of the proposing institutions and principal investigators. between 1978 and 1979, darpa fundedabout a dozen programs in various aspects of vlsi technology at centerssuch as caltech, carnegie mellon university, the jet propulsion laboratory, mit, mississippi state university, university of north carolina,stanford university, ucberkeley, and the university of utah. darpafavored proposals drawn broadly to cover a range of related areas underthe supervision of a single principal investigator. many, if not most, ofthe participants were early adopters of the meadconway design methods and thus had a common basis on which to build their research explorations.management of darpaõs vlsi program was turned over to duaneadams in 1980 and to paul losleben in 1981 after adams was promotedto deputy director of ipto. losleben came from the national securityagency and brought expertise in semiconductor processing technology.under their direction, the vlsi program evolved into four major lines ofresearch: (1) computer architecture and system design; (2) microelectronic device fabrication process; (3) education and human resource development in microelectronics and computer science; and (4) fastturnaround design fabrication, testing, and evaluation. the program madenumerous contributions in each of these areas (box 4.4 describes someprominent examples) and contributed to the commercialization of severalvlsibased technologies (table 4.2). part of this success resulted from theclose ties between research and educational initiatives, with experimentalclasses leading to technologies such as reduced instruction set computing(risc) processors, and research feeding back into the education and training of students.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.118funding a revolutionon the technical side, the focus of the vlsi program expanded fromattempts to accelerate development of submicron semiconductor devicesto a broader set of improvements in computer capabilities based on submicron devices, with particular attention to computer design and architecture. dod anticipated a range of uses for newgeneration computers,including signal processing and interpretation, aerodynamic simulation,artificial intelligence, image and speech recognition, robotics, and highperformance graphics (van atta et al., 1991a). research it supported ledto a variety of new architectures that found acceptance both in dod andin the commercial marketplace (box 4.4).table 4.2 representative vlsi technologies and resultingcommercial productstechnologyinvestigator/institutionproduct/companyrisc architecturesrisc i and risc iidavid patterson,sparc,ucberkeleysun microsystems, inc.mipsjohn hennessy,mips computers, inc.stanford university(now part of silicongraphics, inc.)parallel processorsconnection machinedanny hillis,thinking machines, inc.mitcosmic cubecharles seitz,ipsc (intel)caltechwarph.t. kung,iwarp (intel)carnegie mellonuniversitycomputer systemsgeometry enginejim clark,silicon graphics, inc.stanford universitysun (networked)forest baskett,sun microsystems, inc.stanford universitydesign toolscaesarjohn ousterhout,public domainucberkeleymagicjohn ousterhout,multiplea ucberkeleyavalid logic, viewlogic, mentor graphics, daisy, and cadence all have productsessentially based on the magic concept.source: van atta et al. (1991a), table 172, pp. 1717 through 1719.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.the organization of federal support: a historical review119box 4.4accomplishments of darpaõs very large scaleintegrated circuit programdarpaõs very large scale integrated circuit (vlsi) program supported researchon a number of innovations that revolutionized computing and computing research.work on computer workstations, reduced instruction set computing, and semiconductor fabrication services for university researchers, in particular, benefited fromdarpa support. in each of these areas, darpa identified ongoing research of interest and provided the support necessary to bring the work to fruition.computer workstationsalthough industry efforts to develop computer workstations were under way atcompanies such as apollo computer, they received a significant boost from darpasponsored research. darpa supported the work of forest baskett, a specialist incomputer architecture at stanford university, who submitted a proposal to darpa tocreate the stanford university network (sun). as part of this effort, he planned tobuild a powerful singleuser workstation, combining a 32bit microprocessor (likemotorolaõs new 68000) and a widescreen display. baskett set andreas bechtolsheimto work on the hardware. he also interacted with james clark, whose work on ahighspeed graphics engine baskett saw as critical to scientific and engineering applications of the system. the prototype sun workstation was successfully demonstrated in 1981.darpa and stanford university encouraged bechtolsheim to commercialize theworkstation, which he originally did through a company called vlsi systems, whichwas to produce the workstation boards for other computer manufacturers. afterreviewing proposals from potential computer manufacturers and seeing apollo announce its own workstation, however, bechtolsheim realized he would have to movequickly and design his own machines. key to his plan was using unix, recentlyexpanded by bill joy at ucberkeley under another darpa vlsi contract to enhance its multitasking, multiuser, and networking capabilities. with help from vinodkhosla and scott mcnealy (both stanford university mbas), bechtolsheim was ableto solicit joyõs participation and attract needed venture capital. the team establishedsun microsystems, inc., in february 1982, and its first product was launched in1983.1 darpa extended funds to a number of academic institutions to allow themto purchase workstations for institutional users and networks. such purchases accounted for 80 percent of sun microsystemsõ sales in its first year of business.2 sincethen, sun has become a major force in the computing industry as both a manufacturerof computer workstations and the developer of the java programming language.riscreduced instruction set computing (risc) computers promised significant gainsin performance by optimizing the flow of instructions through the processing unit.3although pioneering work on risc architectures was conducted by ibm as part of its801 computer, ibm did not move quickly to commercialize the technology for fearthat it would detract from burgeoning sales of its mainframe computers; nor was suchcontinued on next pagefunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.120funding a revolutionwork well publicized, although its existence became known in academic researchcircles (hennessy and patterson, 1990).4 darpa sponsored two universitybasedprograms to develop risc as a workable technology under vlsi: one, led by davidpatterson at ucberkeley, developed the risc i and risc ii architectures; the other,led by john hennessy at stanford university, resulted in the mips architecture. bothwere generalpurpose designs aimed at achieving more efficient interaction betweencomputational, storage, and communications units within a device structure byemploying pipelined architectures and processors closely linked with memory andcommunication circuits.both designs were adopted rapidly by industry. the newly formed sun microsystems, inc., licensed the risc ii architecture from the university of california andhired patterson as a consultant to help develop the scalable processor architecture, ariscbased design that it subsequently incorporated into its workstations. this technology enabled sun to fend off growing competition from companies such as digitalequipment corporation, hewlettpackard, and steve jobsõ next corporation, whichwere planning their own entries into the workstation market. hennessy and hiscolleagues at stanford university founded mips computer systems to commercializetheir risc architecture. the company licensed five major chip producers to producedevices based on the technology and five other companies to use the mips architecture in their own computers. mips computer systems was subsequently purchasedby silicon graphics, inc. (sgi), although sgi is currently spinning off the company.other architecture projectsthe vlsi program supported research on a number of innovative computer architectures other than risc. most of this work centered on designs for parallel computers. a range of projects supported a variety of configurations for linking microprocessors and memory, from the connection machine to the cube machines forgeneralpurpose computing and the warp architectures for specialpurpose applications, such as signal processing. several of these approaches were commercialized through startup companies, such as thinking machines corporation, or established firms, such as intel corporation. although successful technologically, manyof these designs failed to achieve commercial success.mosisdarpa also worked to establish ongoing technical and human infrastructure forvlsi. of note was establishment of the metal oxide silicon implementation service(mosis). based on the innovative multiproject chip (mpc) service created by lynnconway at xerox parc (conway, 1981), mosis provided university researcherswith a means of quickly manufacturing limited numbers of custom or semicustommicroelectronic devices at reasonable cost. new designs could be implemented insilicon within 4 to 10 weeks (less than the duration of an academic term). prior tomosis (and the original mpc service), academic researchers had few economicalways of implementing and testing new semiconductor designs, few universities couldafford their own fabrication lines, and the proliferation of different commercial systems of rules for specifying semiconductor circuit designsñmost of which were keptproprietaryñmade collaboration between universities and industry difficult. withbox 4.4 continuedfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.the organization of federal support: a historical review121mosis, researchers could submit designs for fabrication in a standardized formatthrough the arpanet or, subsequently, email. requests from different researcherswere pooled into common lots and run through the fabrication process, after whichcompleted chips were returned to the researchers. this system obviated the need fordirect access to a fabrication line or for dealing with the complexity of arrangingfabrication time at an industrial facility, by providing access to a qualified group offabrication facilities through a single interface.mosis was widely used by the academic research community and contributed tomany novel systems. access to mosis was originally limited to the vlsi researchcommunity and other department of defense contractors who linked to it throughthe arpanet. after the national science foundation (nsf) assumed responsibilityfor administering mosis in 1982, access was expanded to include nsfsponsoredresearchers and affiliated educational institutions. in 1984, access was expanded toother qualified users as well. altogether, mosis was used by researchers at morethan 360 institutions by 1989. the number of projects run through mosis increasedfrom 258 in 1981 to 1,880 in 1989. riscbased designs, such as risc i, risc ii, andmips, and the geometry engine later commercialized by sgi were all run throughmosis during their early design and testing phases. prominent vlsi researchercharles seitz commented that mosis represented the first period since the pioneering work of eckert and mauchley on the eniac in the late 1940s that universities andsmall companies had access to stateoftheart digital technology.5design toolsdarpa also supported development of tools for designing vlsi devices. in 1978and 1979, darpa funded development of a program for steplevel improvement inthe layout of microelectronic devices. the result was caesar, an interactive vlsilayout editor that was written in c, enabling it to run on vax computers using theberkeley version of unix developed by bill joy. caesar produced caltech intermediate form files for use with the mosis system and was used to develop the risc i,risc ii, and mips designs. further modification made the tool suitable for morewidespread use. a later, more advanced design technology created at ucberkeley,magic, became even more widely used and formed the basis for several computerassisted design systems, including those by vlsi technology, cadence, valid logic,daisy, mentor graphics, and viewlogic.1s. squires, chief scientist, darpa, isto, october 19, 1990, as cited in van atta etal. (1991a).2vinod khosla, as cited in van atta et al. (1991a).3ideally, all risc processor instructions (for example, adding two registers) executein one clock cycle. in actual practice, some instructions (such as multiplication anddivision) require additional clock cycles. depending on the implementation, otherinstructions (such as shifts and register loads from memory) may require more thanone clock cycleñthis makes the distinction between risc and complex instructionset computers (ciscs) somewhat gray. mitchell schnier, dictionary of pc hardwarecontinued on next pagefunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.122funding a revolutiondarpa was by far the largest federal supporter of vlsi research. itsfunding for the vlsi program grew from less than $15 million in 1979 toover $93 million in 1982. but other organizations also played critical rolesin the success of the vlsi program. nsf assumed responsibility formosis. its main objective was to pursue educational applications ofmosis, and it expanded the reach of the program to a wider set of academic institutions than darpa had. onr, too, funded several projectsin vlsi but with much smaller grants than darpa. onr funds wereoften considered a òsandboxó for new ideas that, if successful, wouldmerit subsequent darpa funding.14 similarly, industry contributed touniversity research. the stanford center for integrated systems, for example, attracted funding in small amounts from 11 to 12 companies. thismoney was generally used to support students and to fund faculty whowere starting new research areas and who lacked the long track recordneeded to attract darpa funding. hence, while government researchfunding dwarfed industry contributions, industry funding was key forlaunching areas not mature enough to merit government support.15federal funding for vlsi began to decline in the mid1980s. by 1983,plans for darpaõs strategic computing initiative evolved to the pointthat the most promising ongoing architecture projects in the vlsi program (such as warp, butterfly, and connection machine) shifted to thenew program. the vlsi program became increasingly focused on semiconductor devices. main elements of the program included computeraided design and manufacturing technology, test and evaluation tools,and implementation and testing technologies, including ongoing supportfor mosis.strategic computing initiative.kahnõs second strategy was the strategic computing initiative (sci), which he formulated and proposed withand data communications terms, available online at <http://www.ora.com/reference/dictionary/terms/r/reducedinstructionsetcomputer.htm>.4ibmõs first risc machine, the rtpc, was widely considered to be a failure. itssubsequent offering, the rs/6000, was introduced in 1990 with a processor architecture called power. it later became the basis for the powerpc processors cooperatively developed by ibm, apple computer, and motorola. dictionary of pc hardware and communications terms, available online at <http://www.ora.com/reference/dictionary/terms/r/reducedinstructionsetcomputer.htm>.5charles seitz as cited in van atta et al. (1991a).box 4.4 continuedfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.the organization of federal support: a historical review123the support of heilmeierõs successor, darpa director robert cooper.first presented to congress in 1983, sci aimed to spend $600 million bycombining many of darpaõs computer research projects into an overalleffort, with heavy emphasis on artificial intelligence. sci responded tothe growing unease about the apparent loss of u.s. leadership in thesemiconductor and computer industries to japan, following in the footsteps of the auto industry. japanõs òfifthgenerationó computer program, run by the ministry of international trade and industry, seemed adirect threat. kahn and darpa management argued that a strong, domestic electronics and computer industry was critical to national security.the argument succeeded: the project was budgeted originally at $145million in 1986.kahn proposed four areas for sci: microelectronics (based on thevlsi program), supercomputers, generic applications, and defense applications. the main goals were to create an industrial base for artificialintelligence, to implement multiprocessor technologies that could improve the speed of artificial intelligence programs by three orders of magnitude, and to develop advanced speechunderstanding capabilities.unlike the earlier, universityoriented ipto, kahnõs vision incorporatedindustrial projects, with careful timelines and scheduled breakthroughs.in line with the shift to applications, industry would contribute to theproduction of three major òtestbeds,ó or demonstration projects: the autonomous land vehicle, to navigate hostile terrain based on visual sensors; the pilotõs associate to respond to a fighterpilotõs verbal commands;and the battle management program, a series of expert systems to aidcommanders in naval warfare. òthe sci proposed, for the first time, toplace expert systems and other ai technology into central roles in militaryequipment and commandó (edwards, 1996, p. 295). unlike the earlieruniversitybased research programs, nearly half of sciõs funds went directly to industry, with corresponding emphasis on tangible results andapplications. in the words of kenneth flamm, òeconomic and industrialspinoffs were a conscious objective of the programõs plannersó(flamm,1987, p. 75). in the words of saul amarel, who succeeded kahn as iptoprogram director, òthe whole thing was motivated by developing an aitechnology that would be richer, and more mature, building on what wasdone over the last twenty, twentyfive years, that would have an impacton applications, in particular, military applications . . . that would be usedto help develop an industry of ai in the same way that an aeronauticalindustry was developed in this countryó (amarel, 1989).the new approach at darpa was a radical departure from the visionof its original founders, and it did not go without criticism. some computer scientists were disturbed by what they saw as a shift away fromintellectual research toward demonstrable results. others were uncomfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.124funding a revolutionfortable with the possibility that research products might actually someday pull the trigger in the envisioned autonomous robot weapons. someai researchers saw expert systems as anathema to the fundamental goalof building intelligent machines, and some went so far as to regard the1980s as òyears of distractionó because the emphasis on demonstrationslocked them into overly concrete promises for intelligent machines(norberg and oõneill, 1996). despite these concerns, the sci coincidedwith the early reagan defense buildup and, hence, formed the centerpiece of darpa computer research during that decade.making a science, funding a science: the nsf in the 1970s and 1980sthe 1970s and 1980s saw a number of changes in nsfõs support forcomputing and communications, resulting in a vastly improved budgetfor such activities. in 1970, the nsfõs oca lost its favored position belowthe director and was placed under the directorate for national and international programs, marking the beginning of a decline for computingwithin the nsf hierarchy. soon, two other large changes to the ocafollowed as educational programs (approximately 40 percent of its budget) were spun off to another division. with the passage of the mansfieldamendment, oca actually increased its basic research budget from $4.1million in 1971 (23 percent of its budget) to $9 million dollars in 1973 (90percent of its budget) (nsf, 1971, pp. 96101; 1973, p. vii), but only asincomplete compensation for cuts in basic research within dod. thisincrease in basic research support did not fully offset the loss of the educational programs, however, leaving the oca with a budget of only $10million in 1973, half the size of the 1972 figure. computing funding didnot reach the $20 million mark again until 1981. this distillation of ocaõsobjectives did, however, leave it as the only entity in the federal government whose primary function was to fund basic research in computerscience.subsequent changes recognized nsfõs leading role in computer science, and, between 1973 and 1985, nsfõs computing budgets quadrupled.16 changes included the creation of the computer sciences section of the division of mathematical and computer sciences in 1975 and anew software engineering program created in 1977, which emphasizedsymbolic manipulation, software tools, and programming environments.other divisions also conducted computerrelated research (box 4.5). by1977, the computer sciences section was the largest federal funder ofbasic research in computer science. in its 1979 budget request to congress, nsf stated that it òprovides approximately 80 percent of the support [for theoretical computer science] except in numerical analysis . . . 50percent of the federal support [for software systems science] . . . almostfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.the organization of federal support: a historical review125all of the support for basic research [in software engineering] . . . 60percent of the support for basic research [in computer systems design]ó(nsf, 1979, p. bii3). nsf was also beginning to increase its support ofintelligent systems as darpaõs support for basic ai declined.computer research support at nsf took on its current form in 1986.that year, nsf director erich bloch announced the creation of a newdirectorate entirely for computing, the computer and information sciences and engineering (cise) directorate (cstb, 1992, p. 223). to leadthe new directorate, bloch recruited gordon bell, a pioneering systemarchitect at digital equipment corporation, who had been pushing nsffor several years to increase funding for computer science. bell, like othersin the computer industry, was still concerned that universities were nottraining enough ph.d.s in computer science to continue advancing thefield. he believed that the creation of cise could help alleviate thisproblem.17unlike the more recent organizational changes in computing at nsf,cise was more than a change of name and bureaucratic position. muchlike the creation of oca, cise consolidated all the computer initiatives innsf into one entity. the division of computer research was combinedbox 4.5computer engineering at the national science foundationnot all of the national science foundationõs (nsfõs) support for computingrelated research came through the computer directorate. in 1973, the nsf created theelectrical sciences and analysis section in its engineering division to fund electricalengineering research. over the course of the next 10 years, the sectionõs budgetgrew from $7.4 million dollars to $23.7 million in 1984 as nsf incorporated newprograms. in 1979, the section was renamed the division of electrical, computer,and systems engineering when it began to support computer engineering. the division supported research in very large scale integrated circuit technology, fiberopticcommunications networks, and computeraided drafting.in 1986, many of the divisionõs programs, including the computer engineeringprogram, the instrumentation, sensing, and measurement program, and the automation, instrumentation, and sensing program, were shifted into the new microelectronics information processing system of the computer and information sciences andengineering directorate. the communications programs were left behind, as most oftheir work focused on voice and video communication, rather than data networks.11personal communication from gordon bell, former director of the computer andinformation science and engineering directorate at the national science foundation, july 1998.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.126funding a revolutionwith the computing portions of the electrical, computer, and systemsengineering division. cise also absorbed the office of advanced scientific computing and the division of information science and technology.monetary support for computing exploded immediately. ciseõs 1986budget was over $100 million, almost three times the division of computer scienceõs budget in 1984. cise constituted 7 percent of the entirensf budget as opposed to 3 percent in 1985.18 in addition, attaining thelevel of nsf directorate symbolically marked the end of the uncertainposition of computing within nsf. computer science was formally on apar with the biological sciences, the physical sciences, and the other directorates of nsf.between 1987 and 1996, the cise budget more than doubled from$117 million to $259 million, growing at about the same rate as nsf overall and remaining relatively constant at 7 to 8 percent of nsfõs total budget. while all divisions within cise grew during this period, the divisionof advanced scientific computing and the division of networking andcommunications research received the majority of the absolute dollarincreases, reflecting the growing importance of nsfõs infrastructure programs (table 4.3). the advanced scientific computing divisionõs budgetincreased from $42 million to $87 million between 1987 and 1996, makingit by far the largest division within cise, accounting for 35 percent ofciseõs budget during that time. the networking divisionõs budget increased from approximately 8 percent to almost 20 percent of the entirecise budget, largely as a result of the nsfnet program and relatednetworking infrastructure programs, which grew from $6.5 million in1987 to $41.6 million in 1996 (nsfnet and the advanced scientific computing program are discussed in chapter 3). as a result, infrastructureprograms grew from 42 percent to 50 percent of the cise budget.19starting in 1989, cise also began supporting a number of science andtechnology centers (stcs) whose goal was to promote collaborative, interdisciplinary research related to computer science. they include centers for computer graphics and scientific visualization, discrete mathematics and theoretical computer science, parallel computing, and research incognitive science. these centers are supported not only by nsf but alsoby several other federal agencies, universities, and members of industry.reviews of the stc program in 1995 and 1996 were highly supportive ofthe centers (national academy of public administration, 1995; nationalresearch council, 1996).other federal agencies in the 1970s and 1980sdarpa and nsf, of course, did not represent all federal funding ofcomputer research during the 1970s and 1980s, though they clearly playedfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.127table 4.3 growth in the national science foundationõs computer and information sciences and engineeringdirectorate budget (millions of dollars), 198719961987198819891990199119921993199419951996computer and computation research19222526293428303233crossdisciplinary activities16161618192322232327advanced scientific computing43466171747675828788information, robotics, and intelligent systems17171819222526293031networking and communications research10111620293439505654computer and information engineering6666788899microelectronics and information processing systems67810111313151617total117124152169190210212236254259note: totals may not add because of rounding.source: personal communication from vernon ross, nsf office of budget, finance, and award management, july 1997.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.128funding a revolutiona dominant role. although sci was formulated prior to and independentof the strategic defense initiative (sdi), it could not but be partially absorbed (especially in the minds of the public) by the latter, despite theefforts of darpa management to keep the programs distinct (edwards,1996, pp. 293299). reaganõs $35 billion sdi program pumped tens ofmillions of dollars annually into computing.20 sdi critically relied oncommandandcontrol systems for its effectiveness, and doubts about software testing and reliability proved a major hurdle in implementation.sdi also supported work in parallel architectures, optical computing, andnew semiconductor materials.the vhsic program, launched in 1980, focused on transferring technology from the commercial semiconductor industry into the largely separate military electronics industry. the long procurement cycle of militaryelectronics meant that it was perpetually behind rapidly changing commercial technology. under the vhsic program, dod, through the office of the secretary of defense, spent more than $900 million over thecourse of the decade, but few new chips actually made their way intomilitary systems. as one analyst wrote, òr&d could not solve a procurement problemó (alic et al., 1992, p. 269). the office of the secretary ofdefense (osd) spent significant funds on the development of the adaprogramming language, intended to be standard for all dod computerapplications. while ada displaced a number of other programming languages in dod applications, it did not achieve broad acceptance in thecommercial marketplace as had been hoped.21 osd also made a significant investment in software production and maintenance techniquesaimed at improving productivity and reliability ($60 million in 1984)(flamm, 1987, p. 76).nasa support for computing has varied considerably over the years.overall, nasa has been more of a development than a research agency incomputing: that is, it has focused on hardware and applications ratherthan basic research. in hardware, the agency built highly rugged andreliable machines to run its spacecraft but with conservative rather thancuttingedge technology. although nasa tended to have little effect oncomputer architecture and design (although some significant impact inpackaging), its software work in redundant and faulttolerant computers,simulation, and program verifications made significant contributions toprogramming practice. the saturn v computer pioneered triple redundancy and voter circuits (tomayko, 1985, pp. 718). some of this technology has been transferred to transaction processing in commercial units.funding began to decline rapidly after the peak of the space program, inthe late 1960s, and was virtually halted by 1972, at which point nasaõsonly computing program was the illiac iv. it took off again in the early1980s, focusing on image processing and supercomputers for modeling offunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.the organization of federal support: a historical review129aerostructures. the nastran software package for finite element modeling of physical structures has become the most widely accepted suchprogram in industry (flamm, 1987, p. 85). also during the 1980s, thenational institutes of health (nih) was a small but increasingly important player in developing computer applications for medicine and biology, particularly in innovative applications of expert systems (see chapters 9 and 10 for a description of nihõs support for expert systems andvirtual reality technology). the national library of medicine, along withdarpa and the national institute of standards and technology (nist),also supported work on information retrieval that has influenced the development of internet search engines. similarly, the department of energy invested in highend and parallel machines, at about $7 million peryear (flamm, 1987, p. 93).sematechin 1987, 14 u.s. semiconductor companies joined a notforprofit venture, sematech, to improve domestic semiconductor manufacturing.the joint nature of the effort, combined with member companiesõ willingness to put significant funds into sematech and concerns over thenationõs growing dependence on foreign suppliers for semiconductor devices, helped convince congress to support the effort as well: in 1988, itappropriated $100 million annually for 5 years to match the industrialfunding. the federal dollars for sematech were funneled throughdarpa because semiconductor manufacturing was seen as vital to thedefense technology base. in the words of one analyst, òthe halfbilliondollar federal commitment marks a major shift in u.s. technology policy:a turn toward explicit support for commercially oriented r&d carried outin the private sectoró (alic et al., 1992, p. 277).sematech originally planned to develop new production processesinhouse for manufacturing nextgeneration semiconductor devices, butsoon after decided to concentrate its efforts on strengthening the supplierbase for the semiconductor industry. at the time, japanese semiconductor manufacturing equipment suppliers were gaining market share at arate of 3.1 percentage points a year, and u.s. semiconductor manufacturers planned to purchase the majority of their equipment from japanesesuppliers (sematech, 1991).over the next several years, sematech made several notable advances. it established partnerships with u.s. equipment suppliers to helpthem develop nextgeneration production tools, and it helped semiconductor manufacturers develop consensus regarding their future needs,especially those related to manufacturing equipment. these achievementsallowed equipment manufacturers to meet one set of industry specificafunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.130funding a revolutiontions rather than a variety of company specifications. sematech alsofunded research and development efforts at supplier companies helpingthem improve their equipment and develop systems to make more advanced semiconductor devices. perhaps most important, sematechhelped establish improved communication links between semiconductormanufacturers and their suppliers, allowing freer exchanges of information among users and suppliers of manufacturing equipment.these efforts and others began to show benefits soon thereafter. semiconductor equipment manufacturers regained market share against thejapanese, boasting 53 percent of the world market in 1992 versus 38 percent for japanese suppliers (vlsi research, 1992). production yields foru.s. semiconductor manufacturers improved from 60 percent in 1987 to84 percent in 1992, and u.s. market share in semiconductor devices alsoimproved (gao, 1992, p. 10). clearly, other factors played a role, not theleast of which was the relative rise of the market for microprocessorsñinwhich u.s. firms developed a strong competitive advantageñversusmemory chips. nevertheless, sematech has been cited as a factor inthe resurgence of u.s. semiconductor equipment manufacturers. darpaprogram managers also considered the effort successful, noting that manyof darpaõs objectives were mentioned in sematechõs strategic plan,including efforts to rapidly convert manufacturing technology into practice and to develop technology for more flexible semiconductor production (ota, 1993, p. 128).darpa continued its investment in sematech beyond the originaldeadline, but, in 1995, sematech announced that it would wean itselffrom public assistance. in doing so, it recognized that it had achievedmost of its original objectives and believed it could remain selfsustainingwith industry funds only. doing so would also allow it greater freedomin establishing its research agenda, insulate it from continued uncertaintyover federal funding, and reduce concerns about participating with foreign companies. in 1998, sematech announced the establishment ofsematech international, a division of sematech that would allowparticipation by foreignowned companies.highperformance computingthe late 1980s saw a new theme emerge in government support ofcomputing research: coordination among federal research agencies. themost visible example of this coordination, which also accounts for a significant percentage of todayõs federal support for computing r&d, is thehigh performance computing and communications initiative (hpcci).although this program focused on the highestend computers and applications, it has much broader impact. the pace of microelectronics meansfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.the organization of federal support: a historical review131that the evolution of a given capability (hardware and software) fromsupercomputer to desktop requires about a decade. thus, todayõs highperformance applications are a glimpse into the future of computing.in keeping with its traditional role of providing facilities for computer science in universities, in 1984 the nsf asked congress to set upsupercomputer centers so academic researchers could access stateoftheart supercomputers. the result was the national centers for supercomputing applications. nsf then established a highspeed networkbackbone to connect these centers, which itself became the seed of thehighspeed internet backbone. in 1988, the office of science and technology policy (ostp) and the federal coordinating council for science, engineering, and technology (fccset) created the national research andeducation network, a new system that built on earlier projects withinnsf, doe, nasa, and dod that supported advanced scientific computing and human resource development for computer science. the resultwas the high performance computing program, which also included anemphasis on communications.in 1989, ostp produced a formal program plan for highperformancecomputing. ostp provided a vehicle for interagency coordination amongthe initial players, doe, nasa, and nsf; the national security agency(nsa) has also been an influential player, although not a formal member.thus, economies of scale and scope could be realized by avoiding duplication of effort across research agencies. congress passed the high performance computing act in 1991 as a 5year program. this affirmed theinteragency character of hpcci, which by then had 10 federal agenciesparticipating, including the environmental protection agency, the national library of medicine (a branch of nih), nist, the national oceanicand atmospheric administration, and later, the department of education, nsa, and the veterans administration.originally, hpcci aimed at meeting several grand challenges, including scientific modeling, weather forecasting, aerospace vehicle design, and earth biosphere research. these goals have since been expandedto ònational challenges,ó which include digital libraries, electronic commerce, health care, and improvement of information infrastructure (cstb,1995a). overall, the program achieved a number of notable results. thesuccess of some applications and programming paradigms convincedpeople that parallel computing could be made to work. the programcreated and disseminated technologies to speed the pace of innovation,enhance national security, promote education, and better understand theglobal environment (see chapter 3 for a discussion of some of the resultsof the highperformance computing effort).funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.132funding a revolution1990 and beyondthe 1990s have seen the continued evolution of computing and communications technology and a changing environment for federal support.the technological side has been characterized by an explosion in the useof computers and the internet. personal computers have continued topenetrate businesses and homes. by 1998, approximately 40 percent ofu.s. households had at least one computer, and a growing numberboasted a connection to the internet. building upon decades of federalresearch and development, the internet itself emerged as a major forcewith the number of servers growing exponentially. with the emergenceof the world wide web and browser technologies (also derivatives offederally sponsored researchñsee chapter 7), the internet has become amedium for disseminating information and conducting business. companies such as amazon.com formed solely as virtual entities, and manyestablished firms created a presence on the web to conduct business.development of networking technologies has also created new opportunities for new kinds of computing hardware and software. a number of companies developed and began offering network computers, machines designed specifically for use over the internet and other corporatenetworks. such machines rely on the network for much of their infrastructure, including application programs, rather than storing such fileslocally. although it is not yet clear how well such computers will fare inthe marketplace, especially as pc manufacturers expand their offerings oflowcost, scaleddown computers, network computers demonstrate thekinds of innovation that expansion of the internet can motivate.component software also emerged as a new programming modalityin the 1990s. epitomized by the java programming language, componentsoftware allows programs to be assembled from components that can runon a wide variety of computing platforms. applications can be accessed,downloaded, and run over the network (e.g., the internet) as needed forcomputations.along with these technological changes have come changes in theenvironment for federal research funding. with the fall of the berlin wallin 1989 and the subsequent demise of the soviet union, defense budgetsbegan a slow, steady decline, placing additional pressure on defense research and development spending. at the same time, growing sentimentto reduce the federal deficit further squeezed federal budgets for scienceand technology generally in the first half of the decade. by 1997, theprospect of budget surpluses gave rise to the possibility of expandingbudgets for science and technology spending and renewed attempts todevelop a new framework for federal participation in the innovation process. senator phillip gramm, along with senators joseph lieberman,funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.the organization of federal support: a historical review133peter domenici, and jeffrey bingaman, introduced a bipartisan bill inoctober 1997 to double federal spending for nondefense scientific, medical, and precompetitive engineering research over 10 years (the bill,s.1305, is called the national research investment act of 1998). in early1998, congressman vern ehlers of the house science committee initiateda national science policy study to review the nationõs science policy anddevelop a new, longrange science and technology policy that is òconcise,comprehensive, and coherentó (ehlers, 1998).the structure of federal support for computing and communicationsalso underwent modification in the 1990s. in place of the fccset committee, the clinton administration established a national science andtechnology council in 1993 to coordinate federal programs in science,technology, and space. its committee on computing, information, andcommunications (ccic), through the subcommittee on computing, information, and communications r&d, coordinates computing and communicationsrelated r&d programs conducted by the 12 federal departments and agencies in cooperation with academia and industry. thisgroup has restructured and expanded upon the hpcci to organize programs in five areas: (1) highend computing and computation; (2) largescale networking; (3) highconfidence systems; (4) humancentered systems; and (5) education, training, and human resources. further, infebruary 1997, president clinton established an advisory committee onhigh performance computing and communications, information technology, and the nextgeneration internet. the committeeõs charge is toassist the administration in accelerating the development and adoption ofinformation technology that is vital to the nationõs future (nstc, 1997).federal support for computing and communications infrastructurealso changed in the 1990s. after opening the internet to commercial usein 1992, nsf effectively privatized the network in 1995. nevertheless,nsf and other federal agencies are pursuing development and deployment of the nextgeneration internet (ngi), which will boast data rates100 times those of the internet. the ngi initiative will create an experimental, widearea, scalable testbed for developing networking applications that are critical to national missions, such as defense and health care.further, starting in december 1995, nsf began restructuring its supportof national supercomputing centers, forming a new partnerships for advanced computational infrastructure program. the program will concentrate its resources on two groups of organizations, each with a leadingedge facility and several collaborators. one group, the nationalpartnership for advanced computational infrastructure will have the sandiego supercomputing center in california as its leadingedge site. theother group, the national computational science alliance, will have thenational center for supercomputing applications at urbanachampaign,funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.134funding a revolutionillinois, as its leadingedge site. the objective is to equip these sites withhighend computing systems one to two orders of magnitude more capable than those typically available at major research universities. theywill work in partnership with other organizations that are expected tocontribute to access, to education, outreach, and training, and to softwaredevelopment that will facilitate and enhance both the overall infrastructure and access to that infrastructure (cutter, 1997).funding for research in computer science weathered these changesreasonably well with basic and applied research posting real gains between 1989 and 1995 (see chapter 3). nevertheless, the research community expressed concerns that such funding may not be adequate to support the continuing growth of the field (and the rising number ofresearchers in academia and industry) and that the nature of such research is changing. many researchers claim that federal funding is increasingly focused on nearterm objectives and less radical innovation.calls for greater accountability in the research enterprise, they claim, haveled agencies to favor work that is less risky and that exploits existingknowledge, despite its potentially lesser payback. the implications ofsuch changes are not yet clear, but they will become evident over the nextseveral years and beyond.22notes1.quoted in edwards (1996), p. 122.2.as president eisenhower declared in the 1958 state of the union message,òsome of the important new weapons which technology has produced do not fitinto any existing service pattern. they cut across all services, involve all services,and transcend all services, at every stage from development to operation. insome instances they defy classification according to branch of service.ó3.quoted in barber associates (1975), pp. v51 to v52.4.quoted in norberg (1996), pp. 4053.5.quoted in norberg and oõneill (1996), p. 31.6.figure based on data for 19601968 in the national science foundationõsannual budget request to congress (19601969) and for 19681970 in its annualpublication grants and awards (19681970). both are available from the nationalscience foundation.7.figure based on data from the 1968, 1969, and 1970 editions of the nationalscience foundationõs grants and awards for the fiscal year ended june 30.8.the fundamental discoveries of computability and complexity theoryshow precisely that the details of the computing machine do not matter in analyzing the most important properties of the function to be computed. the scienceof computing is the study of the consequences of certain basic assumptions aboutthe nature of computation (spelled out most clearly in turingõs famous 1936 paper), not the study of particular artifacts. of course, problems arising from theconstruction and use of actual computers are a main source of questions for thefunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.the organization of federal support: a historical review135science of computing, in the same way as problems in the physical sciences andengineering have been a main source of ideas and questions in mathematics.9.blue, quoted in norberg and oõneill (1996), p. 37.10.many of the details contained in this section derive from case studies ofthe vlsi program and mosis contained in van atta et al. (1991a), although theinterpretation here differs in some respects.11.silicon graphics, inc. had sales of $3.1 billion and employed over 9,800workers in 1998.12.charles seitz in a presentation to the study committee, february 28, 1997,stanford, calif.13.in order for the program to benefit u.s. industry more than its foreigncompetitors, there was a general understanding that investigators would delayopen publication of results for roughly 1 year, during which time results wouldbe circulated quickly within the community of darpasponsored vlsi researchers (van atta et al., 1991a, pp. 1710 and 1713, based largely on comments byrobert kahn on august 7, 1990).14.charles seitz in a presentation to the study committee, february 28, 1997,stanford, calif.15.john l. hennessy in a briefing to the study committee, february 28, 1997,stanford, calif.16.data from òcompilation of dataó from the national science foundationõsannual summary of awards between 1973 and 1985.17.personal communication from gordon bell, july 1998.18.personal communication from vernon ross, nsf office of budget, finance, and award management, july 1997.19.personal communication from vernon ross, nsf office of budget, finance, and award management, july 1997.20.sdi budgets for computing are difficult to discern with accuracy, as theywere buried within other types of contracts. one estimate is between $50 millionand $225 million annually from 1985 to 1994 (paul edwards, 1996, p. 292).21.for a discussion of ada and its use in military and civilian applications,see cstb (1997a).22.the computer science and telecommunications board of the nationalresearch council has a project under way to document changes in support forinformation technology research in industry and government and evaluate theirimplications. for more information on this project, òinformation technologyresearch in a competitive world,ó see <http://www4.nas.edu/cp.nsf >.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.136the federal government has made significant contributions to theresearch base for computing technology. as detailed in chapter 3, federalsupport has accounted for a substantial fraction of the total funding forcomputing research in the united states and the vast majority of all university research funds in the field. such funding has supported both thedevelopment of new technologies and the training of students. the federal government has also paid for public research infrastructure, providing most of the funds for research equipment in university departmentsof computer science and electrical engineering, and has sponsored programs to provide access to and infrastructure for highperformance computing and networking. such contributions did not singlehandedly drivesubsequent development of the nationõs computing industry; rather, theyformed part of the larger innovation system that combined the efforts ofgovernment, universities, and industry. they nevertheless played an important role in the industryõs development.what have been the results of federal investments? how can futurefederal programs be designed to enhance their effectiveness? the historydescribed in this report can aid in answering these questions. historydemonstrates by select examples the kinds of effects federal research funding has had on the innovation process in computing, and it illustratessome of the principles of sound project management. this chapter synthesizes the major lessons of this report. it attempts to characterize theeffects of federal investments in computing research and to discuss theprogrammatic considerations that appear to have contributed to the suc5lessons from historyfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.lessons from history137cess of the field. in doing so, the chapter draws on case studies from othersections of the report as needed for examples. readers are referred tochapters 6 through 10 and chapter 4 for a more complete elaboration ofthe case studies.the benefits of federal research investmentsgovernment research funding has had a profound influence on thedevelopment of the computing industry in the united states. federalresearch support has provided a proving ground for testing new concepts, designs, and architectures in computing, and it has helped hastenthe commercialization of technology developed in industry laboratories.this influence has manifested itself in a variety of ways: (1) in the creationof new products, services, companies, and billiondollar industries thatare based on federally funded research; (2) in the expansion of universityresearch capabilities in computer science and electrical engineering; (3) inthe formation of human resources that have driven the computing revolution; and (4) in the ability of federal agencies to better accomplish theirpublic missions.quantifying the benefits of federal research support is a difficult, ifnot impossible, task for several reasons. first, the output of research isoften intangible. most of the benefit takes the form of new knowledgethat subsequently may be instantiated in new hardware, software, or systems, but is itself difficult to measure. at other times, the benefits take theform of educated people who bring new ideas or a fresh perspective to anorganization. second, the delays between the time a research program isconducted and the time the products incorporating the research resultsare sold make measurement even more difficult. often, the delays runinto decades, making it difficult to tell midcourse how effective a particular program has been. third, the benefits of a particular research programmay not become visible until other technological advances are made. forexample, advances in computer graphics did not have widespread effectuntil suitable hardware was more broadly available for producing threedimensional graphical images. finally, projects that are perceived as failures often provide valuable lessons that can guide or improve futureresearch. even if they fail to reach their original objectives, researchprojects can make lasting contributions to the knowledge base.despite these difficulties, several observations can be made that providea qualified understanding of the influence of federal research programson industry, government, and universities. they demonstrate the effectfederal funding has had on computing and, by extension, on society.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.138funding a revolutionproviding the technology base for growing industriesfederal research funding has helped build the technology base onwhich the computing industry has grown. a number of important computerrelated products trace their technological roots to federally sponsored research programs. early mainframe computers were given a significant boost from federally funded computing systems of the 1950s,such as the u.s. air forceõs semiautomatic ground environment (sage)project. although a commandandcontrol system designed to warn ofattacks by soviet bombers, sage pioneered developments in realtimedigital computing and core memory (among other advances) that rapidlyspread throughout the fledgling computer industry. timeshared minicomputers, which dominated the market in the 1970s and early 1980s,exploited timesharing research conducted in the 1960s under the defenseadvanced research projects agencyõs (darpaõs)1 project mac and earlier work sponsored by the national science foundation (nsf) on thecompatible timesharing system at the massachusetts institute of technology (mit) (see chapter 4). the internet, which came of age in the early1990s, was derived from darpaõs arpanet program of the early 1970s,which created a packetswitching system to link research centers acrossthe country, as well as from subsequent programs managed by nsf toexpand and improve its nsfnet (see chapter 7). federal funding forrelational databases helped move that technology out of corporate laboratories to become the basis of a multibilliondollar u.s. database industry.the graphical user interface, which became commonplace on personalcomputers in the 1990s, incorporates research conducted at sri international under a darpa contract some 30 years earlier (chapter 4).the economic impact of federally funded research in computing isevident in the many companies that have successfully commercializedtechnologies developed under federal contracts. examples include sunmicrosystems, inc., silicon graphics, inc., informix corporation, digitalequipment corporation, and netscape communications corporation.established companies, such as international business machines corporation (ibm) and american telephone and telegraph corporation(at&t), also commercialized technologies developed with federal sponsorship, such as core memories and timesharing operating systems.clearly, federally sponsored research was only one element in the successof these companies. private firms had to dedicate tremendous resourcesto bring these technologies successfully to market, investing in their research and development, establishing manufacturing capacity, and setting up marketing and distribution channels. but new technology createdthe seed for continued innovation.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.lessons from history139maintaining university research capabilitiesfederal funding has also maintained university research capabilitiesin computing. universities depend largely on federal support for research programs in computer science and electrical engineering, the twoacademic disciplines most closely aligned with computing and communications. since 1973, federal agencies have provided roughly 70 percent ofall funding for university research in computer science. in electrical engineering, federal funding has declined from its peak of 75 percent of totaluniversity research support in the early 1970s, but still represented 65percent of such funding in 1995.2 additional support has come in theform of research equipment. universities need access to stateoftheartequipment in order to conduct research and train students. althoughindustry contributes some equipment, funding for university researchequipment has come largely from federal sources since the 1960s. between 1981 and 1995, the federal government provided between 59 and 76percent of annual research equipment expenditures in computer scienceand between 64 and 83 percent of annual research equipment expenditures in electrical engineering.3 such investments have helped ensurethat researchers have access to modern computing facilities and haveenabled them to further expand the capabilities of computing and communications systems.universities play an important role in the innovation process. theytend to concentrate on research with broad applicability across companies and product lines and to share new knowledge openly.4 becausethey are not usually subject to commercial pressures, university researchers often have greater ability than their industrial counterparts to exploreideas with uncertain longterm payoffs. although it would be difficult todetermine how much university research contributes directly to industrial innovation, it is telling that each of the case studies and other majorexamples examined in this reportñrelational databases, the internet, theoretical computer science, artificial intelligence, virtual reality, sage, computer timesharing, very large scale integrated circuits, and the personalcomputerñinvolved the participation of university researchers. universities play an especially effective role in disseminating new knowledge bypromoting open publication of research results. they have also served asa training ground for students who have taken new ideas with them toexisting companies or started their own companies. diffusion of knowledge about relational databases, for instance, was accelerated by researchers at the university of california at berkeley who published the sourcecode for their ingres system and made it available free of charge. severalof the lead researchers in this project established companies to commerfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.140funding a revolutioncialize the technology or brought it back to existing firms where theychampioned its use (see chapter 6).creating human resourcesin addition to supporting the creation of new technology, federalfunding for research has also helped create the human resources thathave driven the computer revolution. many industry researchers andresearch managers claim that the most valuable result of university research programs is educated studentsððby and large, an outcome enabledby federal support of university research. federal support for universityresearch in computer science grew from $65 million to $350 million between 1976 and 1995, while federal support for university research inelectrical engineering grew from $74 million to $177 million (in constant1995 dollars).5 much of this funding was used to support graduate students. especially at the nationõs top research universities, the studies of alarge percentage of graduate students have been supported by federalresearch contracts. graduates of these programs, and faculty researcherswho received federal funding, have gone on to form a number of companies, including sun microsystems, inc. (which grew out of research conducted by forest baskett and andy bechtolsheim with sponsorship fromdarpa) and digital equipment corporation (founded by ken olsen,who participated in the sage project). graduates also staff academicfaculties that continue to conduct research and educate future generations of researchers.furthermore, the availability of federal research funding has enabledthe growth and expansion of computer science and computer engineeringdepartments at u.s. universities, which increased in number from 6 in1965 to 56 in 1975 and to 148 in 1995 (andrews, 1997, p. 5). the number ofgraduate students in computer science also grew dramatically, expandingmore than 40fold from 257 in 1966 to 11,500 in 1995, with the number ofph.d. degrees awarded in computer science increasing from 19 in 1966 toover 900 in 1995 (nsf, 1997b, table 46). even with this growth in ph.d.production, demand for computing researchers still outstrips the supplyin both industry and academia (u.s. department of commerce, 1997).beyond supporting student education and training, federal fundinghas also been important in creating networks of researchers in particularfieldsñdeveloping communities of researchers who could share ideasand build on each otherõs strengths. despite its defense orientation,darpa historically encouraged open dissemination of the results of sponsored research, as did other federal agencies. in addition, darpa andother federal agencies funded large projects with multiple participantsfrom different organizations. these projects helped create entire commufunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.lessons from history141nities of researchers who continued to refine, adopt, and diffuse newtechnology throughout the broader computing research community. development of the internet demonstrates the benefits of this approach: byfunding groups of researchers in an open environment, darpa createdan entire community of users who had a common understanding of thetechnology, adopted a common set of standards, and encouraged theiruse broadly. early users of the arpanet created a critical mass ofpeople who helped to disseminate the technology, giving the internetprotocol an important early lead over competing approaches to packetswitching (see chapter 7).scientific societies have also played a significant role in this respect.groups such as the association for computing machinery (acm) and theinstitute of electrical and electronics engineers (ieee) and their subgroups have helped create communities of researchers and facilitatedcommunication among them. the development of virtual reality, forexample, benefited enormously from the creation of siggraph, theacmõs special interest group for computer graphics. this organizationbrought together university and industry researchers, as well as users ofcomputer graphics, from a variety of fields (e.g., arts, entertainment, medicine, and manufacturing). its annual conferences have become a showcase of new technology and a primary forum for exchanging new ideas.accomplishing federal missionsin addition to supporting industrial innovation and the economicbenefits that it brings, federal support for computing research has enabled government agencies to accomplish their missions. investments incomputing research by the department of energy (doe), the nationalaeronautics and space administration (nasa), and the national institutes of health (nih), as well as the department of defense (dod), areultimately based on agency needs. many of the missions these agenciesmust fulfill depend on computing technologies. dod, for example, hasmaintained a policy of achieving military superiority over potential adversaries not through numerical superiority (i.e., having more soldiers)but through better technology. computing has become a central part ofinformation gathering, management, and analysis for commanders andsoldiers alike (high performance computing modernization office, 1995).similarly, doe and its predecessors would have been unable to support their mission of designing nuclear weapons without the simulationcapabilities of large supercomputers. such computers have retained theirvalue to doe as its mission has shifted toward stewardship of the nuclearstockpile in an era of restricted nuclear testing. its accelerated strategiccomputing initiative builds on doeõs earlier success by attempting tofunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.142funding a revolutionsupport development of simulation technologies needed to assess nuclearweapons, analyze their performance, predict their safety and reliability,and certify their functionality without testing them.6 in addition, nasacould not have accomplished its space exploration or its earth observation and monitoring missions without reliable computers for controllingspacecraft and managing data. new computing capabilities, including theworld wide web, have enabled the national library of medicine to expand access to medical information and have provided tools for researchers who are sequencing the human genome.characteristics of effective federal supportthe success of federal funding in computing research derives bothfrom the kind of programs and projects it has supported and the ways ithas structured those programs. by funding a mix of fundamental research and system development activities, for example, government wasable to promote the longterm health of the field and to demonstrate newtechnologies. by funding a mix of work in universities and industry, itwas able to marry longterm objectives to realworld problems. and, bychanneling its funding through a variety of federal agencies, it was able toensure broadbased coverage of many technological approaches and toaddress a range of technical problems. this section examines some of thekey factors that have led to the success of federal research investments inthe past and attempts to provide guidance for structuring future researchprograms.support for longrange, fundamental researcha strength of federal research funding is that it complements, ratherthan competes with, private research investments. successful government research programs have supported research that private industryhas had little incentive or ability to support because the commercial applications of the research were too distant and too uncertain, or because theresearch itself was so fundamental that individual firms could not expectto capture the benefits themselves while preventing others from doing so(see chapter 2). private industry is generally not able to assume the risksinherent in such projects, nor does it continue funding research in a particular field over extended periods if the payback is unclear. in manysuch instances, federally sponsored research has laid the groundwork fornew technologies that ultimately created not only new products, processes, and services, but also entire industries. such investments weretypically made yearsñif not decadesñbefore practical applications became feasible; they helped advance knowledge of the field sufficiently sofunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.lessons from history143that firms could begin to make appropriate investments. this pattern hasbeen repeated in numerous cases:1.in artificial intelligence (ai), early funding came mostly from federal sources, primarily darpa. although large computing and communications companies, such as ibm and at&t, established small programsfor artificial intelligence research in the 1950s, these efforts were scaledback and redirected toward more practical topics (such as speech recognition) when it became evident that more fundamental research might notproduce marketable results for more than a decade. the federal government, too, cut back on some programs that failed to show initial progress(such as machine translation and, for a time, speech recognition), but itcontinued to make strong investments in ai research to explore bothfundamental research questions and applications of ai technology. theseinvestments, combined with industry efforts, enabled sufficient progressfor a number of aibased products to begin entering the market place.based on pioneering efforts such as dendral, an expert (or rulebasedreasoning) system for deducing the likely molecular structure of organiccompounds, a number of firms began creating rulebased reasoning systems for engineering and medical applications in the mid1970s and the1980s. building on work conducted with industry and federal funding,several companies, including ibm, dragon systems, and lucent technologies, introduced in the 1990s robust, continuous speechrecognitionpackages for use with personal computers. a range of other ai technologies began to appear as integral parts of other systems, such as grammarcheckers in word processors, decision aids for troubleshooting software,and software agents for finding information on the world wide web (seechapter 9).2.pioneering work in virtual reality was conducted by ivan sutherland,then at harvard university, with support from several defense agencies.a handful of private firms, such as general electric, established researchprograms to build on this work but soon realized that products incorporating such technology lay many years in the future. subsequent researchððfunded by agencies such as darpa and nsf and conducted atuniversities such as the university of utah, california institute of technology, and the university of north carolina at chapel hillððcreated anumber of advances in hardware and software for rendering two andthreedimensional computer graphics that have since been used widely inmedicine, entertainment, and engineering applications. federally fundedresearch in these areas succeeded in developing the technology to thepoint that private companies could both develop products and invest inproductive research. in virtual reality, for example, the entertainmentindustry has built on early university research to create systems for profunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.144funding a revolutionducing computeranimated films. more recently, microsoft established alarge research group in computer graphics to help improve graphics fordesktop computers. its interest is now driven largely by the video gameindustry and the search for improved user interfaces (see chapter 10 for acase study of the federal role in virtual reality research).in both of these cases, industry had limited incentives to invest inresearch. the time needed for such programs to yield tangible resultswas often measured in decades, far beyond the planning horizons of manycompanies. furthermore, early progress in these fields required fundamental advances that were applicable to a range of potential applicationsand were difficult for any single company fully to appropriate (or control). because few mechanisms exist for companies to collectively fundfundamental research of mutual interest,7 federal funding has often beenthe most appropriate mechanism for supporting research, especially if theresearch is applicable to government missions.this is not to say that industry will not support longterm research.many larger companies conduct fundamental research with broad applicability. ibm and at&t are the most prominent examples. the ability ofsuch companies to support fundamental research is closely linked to theirability to recoup their investments in these areas (see chapter 2) and,hence, to their overall profitability and dominance in the marketplace.at&t, for example, conducted longterm research at bell laboratoriesand for many years had a governmentgranted monopoly on the telephone industry. its research expenditures were in effect a tax on consumers because they were paid for by at&tõs regulated rates for telephoneservice. since divestiture, bell laboratories (now part of lucent technologies) has continued to fund longterm research, but a more consciouseffort has been made to link that research to corporate needs and to capture the benefits of the research investment (buderi, 1998). ibm maintained longterm research at its t.j. watson research center and its otherlaboratories, and, given its market dominance, was able to appropriatemany of the results of that research. however, as the computer industryhas become more competitive and ibmõs market dominance has declined,ibmõs research has been reined in somewhat and redirected to specificstrategic areas (markov, 1996). longterm fundamental research is stillconducted, but it has greater relevance to ibmõs interests. in contrast, asmicrosoft has grown and its dominance has increased in the softwareindustry, it has begun to fund more longterm research. althoughmicrosoft researchers have considerable flexibility in choosing researchtopics, they must demonstrate the relevance of the research to microsoftõsinterests (ziegler, 1997).funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.lessons from history145support for efforts to build large systemsalthough support for fundamental research is an important part ofthe governmentõs research portfolio, many advances in computing havestemmed from projects aimed at building operational systems. systemsdeveloped to meet the governmentõs needs often resulted in pioneeringadvances that were subsequently incorporated into a range of commercialapplications. such systembuilding programs not only created new technology and knowhow but also established networks of people that helpedto rapidly disseminate knowledge broadly throughout the technical community. for example:1. the development of sage stemmed from the needs of dod forimproved early warning capabilities against soviet bomber attacks. itbuilt on project whirlwind, an effort funded by the office of naval research (onr) to develop a generalpurpose aircraft simulator and thatpioneered realtime digital computing. despite the fact that sage wasalmost obsolete when it was finished, it provided invaluable learningexperiences for the engineers and scientists designing and developing thecommunications and computing technology. countless graduate studentsand postdoctoral engineers and scientists, for instance, had their firsthandson experiences with computers while working on these projects(see chapter 4).2. the development of packetswitched networks and internetworking (the interconnection of multiple networks) can be traced to federalfunding from darpa and nsf. packet switching was conceptualized bypaul baran (then at rand corporation) in 1961 and independently bydonald davies at the national physical laboratory in england in 1965.darpa saw the technology as a means of allowing more efficient use ofgeographically separated computing resources and funded developmentof the first packetswitched network. large telecommunications companies, such as at&t, did not participate in darpaõs subsequent programto build a packetswitched network, the arpanet, although they didconduct inhouse research on packetswitched networks (at&tõs workon asynchronous transfer modeñor atm switchingñis an example).instead, darpa contracted with bolt, beranek, and newman, which hadbeen started by three mit professors in 1948 and performed much of thework on the arpanet in association with a handful of universities andprivate companies.8 the arpanet demonstrated the capabilities ofpacket switching and became a source for innovations such as email.the protocols that allowed the flows of information packets through interconnected networks (internets) were developed jointly by vinton cerf,then at darpa, and robert kahn (see chapter 7). continued efforts,funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.146funding a revolutionsponsored by nsf, to develop csnet, and later nsfnet, demonstratedthe value of internetworked communication systems and led to the eventual commercialization of the internet.the value of systembuilding efforts derives from the close linkagesbetween research in computer engineering (as opposed to computer science) and the development of specific artifacts. theory and practice areclosely linked, and innovation tends to proceed in a highly nonlinearfashion, with attempts to build operational systems stimulating identification of new problems for further research. development of new products or services can precede the development of the underlying science,pointing out potentially fruitful avenues of inquiry. for example, development of magnetic core memory for computers did not flow directlyfrom advances in materials research (although it certainly drew uponsuch research), but from the need to develop a memory system with shortenough access times and high enough reliability to support the realtimedigital computing demanded by project whirlwind (see chapter 4). similarly, attempts to develop techniques for virtual surgery (see chapter 10)motivated and accelerated research in areas such as highresolutiongraphics, haptic interfaces, forcefeedback systems, robotics, and controltechniques.building on industrial researcheven in areas in which industry has a welldefined interest, governmentsponsored research has been able to hasten the commercializationof new technology developed in industry laboratories. some technologies, such as relational databases and reduced instruction set computing(risc) computers, were invented by industry researchers but were notcommercialized immediately because they either competed with existingproduct lines or were considered too risky for further development. inthese cases, government funding has supported an independent community of technical experts who validated these technologies and provided apool of talent that helped exploit the idea both in the corporation of originand in competing corporations.early work on relational databases, for example, was conducted byted codd at ibm, but ibm saw the technology as a threat to its established line of database products. codd publicized the results of his work,seeding efforts in relational databases by several university researchers,including michael stonebraker and eugene wong at the university ofcalifornia at berkeley (ucberkeley). with subsequent funding fromnsf, stonebraker and wong were able to develop a relational databasesystem called ingres (interaction graphics and retrieval system). to comfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.lessons from history147mercialize the technology, stonebraker started ingres corporation, whichdemonstrated the viability of the relational approach and helped disseminate knowledge about it, building a community of researchers who further developed the relational database technology. this work, and effortsby other large database vendors, helped stimulate continued development of ibmõs system r, which created the dominant query language forrelational databases.the development of risc processing followed a similar history. johncocke at ibm invented a risc processor for ibmõs stretch computer, butibm did not use the technology more widely because it might detractfrom sales of existing products. darpa funding enabled university researchers to continue working on risc. david patterson at ucberkeleyand john hennessy at stanford university developed risc processordesigns that were commercialized by sun microsystems, inc., and mipscomputer systems, respectively. other designs were offered by competing firms, such as hewlettpackard company and ibm (see chapter 4).to some extent, this phenomenon is not unexpected. large industryresearch groups produce more ideas than they can possibly exploit giventime and financial constraints. these ideas sometimes find their waydirectly into the marketplace through startup companies; at other times,however, the amount of research needed to demonstrate the feasibilityand benefits of a technology is beyond the capabilities of startup companies and direct commercialization is unlikely. in these cases, federal funding of university research can be an effective mechanism for helping bringnew technology to the marketplace.not all pathbreaking research requires government assistance. forexample, the development of the personal computerñwhich representeda significant departure from dominant modes of computing at the timeñtook place mostly in industry, with the xerox palo alto research centerand apple computer playing prominent roles (see chapter 4). this workdemonstrated the viability of personal computers, especially in the business marketplace, and ibm subsequently developed its own personal computer. nevertheless, federal funding was important in supporting someof the early ideas on humancomputer interaction (such as the computermouse) that contributed to developing the personal computer.diverse sources of government supportbetween 1945 and 1995, federal support for computing was providedby a range of organizations, including darpa, nsf, doe, nasa, andnih. this diversity of funding sources has had a salutary effect on computing research. federal funding agencies differ widely in their cultures,goals, resources, and perspectives, and thus in the kinds of researchfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.148funding a revolutionprojects they support. the result has been a federal research establishment that has nurtured diverse approaches to research. darpa, forexample, has tended to award contracts for large programs involvingmultiple researchers and research organizations. it has concentrated itsfunding for computer research on a limited number of centers of excellence, such as mit, stanford university, carnegie mellon university, anducberkeley. program managers have generally been given significantdiscretion in selecting and shaping new research initiatives. nsf, in contrast, has primarily supported individual investigators, with considerablysmaller awards. its funding has been purposely spread among researchers at a wide range of institutions, generally universities, and projectselection has been based largely on peer review. nsf has also fundedprojects intended to support the broad educational and research missionsof universities. other agencies, such as nasa, doe, and nih, mostlyconcentrate their resources on research more directly applicable to theirmissions: space, energy, and health, respectively. as a result, federalfunding agencies complement one another rather than compete in funding research, with each supporting work best suited to its particular needs.in the end, no single approach can support a vibrant research base; all areneeded to play different roles.the most obvious benefit of diverse sources of funding is the opportunity for researchers to seek support from multiple potential sponsors oftheir work. if a particular agency cannot support a worthy research projectfor any of several reasonsððlimited resources, poor match with agencyobjectives, or the judgments of individual program managersððanotheragency may continue to sponsor potentially fruitful lines of inquiry. forexample, darpa and onr declined to fund michael stonebrakerõs workon relational databases because dod was already supporting other database research (see chapter 6). nsf, the air force office of scientificresearch, and the navy electronic systems command, however, viewedthe program as fitting well into their research portfolios and subsequentlyfunded stonebrakerõs ingres project. with this funding, stonebraker wasable to demonstrate the merits of the relational approach, which latergarnered much industry support and became a dominant way to designdatabases. the process of revising and resubmitting proposals for consideration by multiple sponsors also provides an opportunity for morefully exploring the applications of a technology and the different approaches that can be pursued. it is unlikely that any single agency has theexpertise required to understand the varied needs and interests of potential users of new computing and communications technology in government and in industry.diverse modes of support for research (i.e., research funding vs. procurement contracts) have also been valuable in ensuring a balance befunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.lessons from history149tween openended research and research directed toward specific sponsorsõ needs. in the late 1940s, the office of naval research began to doubtthe relevance of the whirlwind computer to its mission of supportingcomputing for scientists and mathematicians. as the project evolvedfrom a programmable flight simulator to a realtime digital computer,onr was not convinced it could continue to support the work. at aboutthe same time, the air force decided that whirlwind was appropriate forits sage commandandcontrol project and maintained support for it.subsequently, the sage project pioneered many advances in computing,from realtime computing to core memories to computer graphics. in theend, it also trained a generation of hardware and software engineers(redmond and smith, 1980).diversity in funding for research also widens the range of applications for new technology and the technological approaches taken. aschapter 9 demonstrates, the majority of federal support for artificial intelligence research, for example, came from darpa, but other agenciessuch as nsf, nasa, and onr funded projects to pursue particular applications of interest to them. nasa supported development of the pioneering expert system dendral, to deduce the likely structure of organiccompounds from known chemical analyses and spectrometry data. thesame is true in virtual reality research (see chapter 10). darpa, nsf,and nih have all sponsored relevant research, but each with specificmission interests to motivate their investments: darpa in helmetmounted displays and applications for training and simulation, nsf inscientific visualization, and nih in molecular design and manipulation ofbiomedical images. such diversity of funding is important in the earlystages of technological development when the uncertainty associated withany particular approach is high. furthermore, some technologies becomereliable or viable only if used in multiple applications, and funding agencies with different needs can help foster the pursuit of diverse, complementary approaches to a problem.in addition, support from different agencies can be effective at different points in the innovation process. for example, work pioneered by oneagency can lead to followup work supported by other agencies that allows the technology to mature. in some cases, smallscale efforts fundedby nsf, onr, or other agencies planted the seeds of larger darpasponsored programs, as occurred in the development of computer timesharing. nsf funded early work at mit on its first timesharing system,ctss (compatible timesharing system), which by 1964 had connected24 terminals across the mit campus. the success of ctss demonstratedthe viability of timesharing and created a nexus of researchers with expertise in developing and using timeshared systems. it also raised additional questions about the ability to scale up such systems to support afunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.150funding a revolutionlarger number of terminals and to provide adequate security to preventusers from corrupting each otherõs programs or data. darpa built onthe ctss effort with project mac, a much larger program that received$25 million between 1963 and 1970. project mac had ambitious goals forexploring interactive computing, including timesharing. by its end, theproject not only had produced the multics system, which eventuallysupported 1,000 users, but also had given impetus to the fledgling timeshared computer industry and helped bring computers out of the laboratory. a program of this scope was beyond the capabilities of nsf at thetime, yet nsf played an important role in demonstrating, on a smallerscale, the viability of timesharing.at other times, darpa has transferred programs to other agenciesonce they reached a certain level of maturity. in the case of the internet,for example, darpa supported development of hardware and software(e.g., network routers and transmission protocols) for the arpanet. by1975ñ7 years after darpa awarded the first contract for work on thesystemððthe project had reached a sufficient level of maturity for darpato transfer management of the network to the defense communicationagency. by the early 1980s, nsf was developing packetswitched networks to link university researchers, first through the csnet (for computer science researchers) and later through the nsfnet. these networks were seen as a means of supporting the research community byproviding a shared medium for exchanging information. in 1989, thearpanet was absorbed by the nsfnet. other disciplinespecific networks that had been constructed by nasa, doe, and other agencieswere also linked to the nsfnet, and nsf became the governmentõs primary supporter of networking infrastructure. it assumed responsibilityfor upgrading and expanding the network, which eventually became thebackbone of the internet.strong program managers and flexible management structuresscientific and technological research explores the unknown; hence, itsoutcomes cannot be predicted at the startñeven if a clear, practical goalmotivates the work. in fact, the outcomes anticipated at the start of aresearch project can differ from those eventually achieved or that prove tobe most important. the internet is a case in point. darpaõs early interest in packetswitched networks (such as the arpanet) grew from adesire to use more efficiently the computing capabilities that were distributed among its many contractor sites. by allowing remote access to thesedisparate computers in a seamless fashion, darpa program managershoped to expand the number of researchers who could use them andincrease their utilization rates. these results were achieved in the end,funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.lessons from history151but, as the arpanet was subsumed into the nsfnet, which laterevolved into the internet, the range of applications for packetswitchednetworks expanded in a number of unanticipated directions. few couldhave predicted the popularity of electronic mail as a means of communicating among computer users; still fewer could have anticipated the emergence of the world wide web as a means for sharing information andconducting business. although visions of expansive computer networksfor public and private use existed, they were not part of darpaõs original plan, nor did they receive much attention then within the researchcommunity.moreover, even research projects that do not achieve their originalobjectives can produce meaningful results or generate valuable knowledge for guiding future research efforts. by some measures, project whirlwind and sage were failures (see chapter 4). planned as a computerdriven aircraft simulator, whirlwind cost far more than expected and didnot produce a simulator; rather, the attempt to develop the simulatorresulted in the development of a realtime digital computer eventuallyused as part of the air forceõs sage commandandcontrol system. bythe time it was deployed in the late 1950s and into the 1960s, sageõsmission was largely obsolete, as intercontinental ballistic missiles wereseen as a greater threat than soviet bombers. yet both these projects madetremendous contributions to computing that have paid back handsomedividends over time, far beyond the costs of research and development.other projects show meaningful returns only after a long time becausetheir applications are not immediately recognized or other technologicaladvances are needed to make their usefulness evident. work on themathematics of oneway functions, for example, was not appreciated fullyuntil it was realized that it provided a basis for publickey cryptography(see chapter 8). twenty years passed before the benefits of work on themathematics of hidden markov models were incorporated into generalpurpose speechrecognition systems for pcs. only after continued increases in processing power and memory capacity did hidden markovmodels become feasible for use in recognizing continuous speech on pcs.such difficulties frustrate attempts to meaningfully measure the performance of research and also highlight the need for ensuring flexibilityin the management and oversight of federally funded research programs.researchers need sufficient intellectual freedom to follow their intuitionand to modify research plans based on preliminary results. constrainingresearch too narrowly can limit their ability and willingness to take risksin choosing new research directions. building such flexibility into federalstructures for managing research requires both skilled program managersñwho understand, articulate, and promote the visions of researchersñandan organizational culture that accepts and promotes exploratory efforts.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.152funding a revolutionthese two elements complement one another: organizations that promote exploration and allow program managers to exercise their own discretion in selecting new directions for research tend to attract individualswho are effective program managers and who earn the respect of theresearch community.darpa and nsf both have incorporated these principles into theirinstitutional structures. especially in the 1960s and 1970s, darpa gaveprogram managers sufficient funds to shape coherent research programs,and program budgets required only two levels of approval: one by theoffice director and one by the darpa director. the organization as awhole aimed to generate orderofmagnitude improvements in computing technology by funding a combination of fundamental research andlarge systembuilding efforts. it was able to attract visionary leaders,such as j.c.r. licklider, ivan sutherland, robert taylor, lawrence roberts, vinton cerf, and robert kahn. many of these leaders were drawnfrom the research community for short tours of duty. they brought todarpa an understanding of current research challenges and a vision ofthe future. they were attracted to darpa by the promise of being able tohelp implement a vision and lead the field. they maintained an interactive relationship with the research community, taking ideas from researchers and turning them into strategic directions, rather than trying to forcetheir own agendas. they managed with a light touch, giving researchersroom to pursue openended projects.clearly, there are limits to the flexibility that researchers and programmanagers can be allowed. in developmentoriented programs, for example, program managers must ensure that specific objectives are met. inexploratory research, program managers must ensure that research fundsare used prudently. but such accountability must be balanced against theunpredictability of research. structures for managing and overseeingfederally funded research need to allow program managers to alter programs midcourse in response to preliminary results and need to recognize that research projects can produce valuable results even if they donot achieve their original objectives. failing to do so risks stifling creativity and innovation. the history of computing demonstrates the benefitsof a flexible approach. by giving program managers greater discretion,federal agencies such as darpa and nsf were able to support the development of the numerous innovations identified in this report.industryuniversity collaborationcollaboration among researchers from academia and industry oftenhas been a successful way of linking practical goals with technical capabilities. although tensions can exist relative to the differing time horizonsfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.lessons from history153between academic research and industry development cycles, collaboration between researchers and product developers has had salutary effectson computing research, helping to ensure the relevance of academic research and helping industry to take advantage of new academic research.such collaboration allows government program managers to better leverage their resources by attracting industry contributions. similarly, government funding can act as a òseal of approvaló that encourages greaterprivate investment. in this way, government funding, on average, spursñrather than displacesñprivate research investments.collaboration between industry and universities builds communitiesof researchers who pursue a particular field and share a common vocabulary. rapid advances in computing technology have resulted from thepace at which information has been exchanged between researchers anddisseminated throughout the research and product development communities. ibmõs ability to commercialize core memories rapidly, for example,was related to its participation in the sage project, which pioneered theinnovation. overall, the computing community has an impressive trackrecord of transferring technology and knowledge successfully betweenthe academic and industrial communities. as a number of researchersnote, however, fruitful collaborations tend to evolve from researchprojects as the necessary skills to conduct a research or development program are assembled and as information about a research topic spreadsthroughout the research community. researchers themselves often serveas the best means of technology transfer, taking knowledge with them asthey move among posts in government, industry, and universities or asthey start new companies to commercialize research results. attempts todeliberately bring together university and industry researchers in collaborative projects can also be successful, but considerable flexibility mustbe allowed in specifying the nature of the collaboration.organizational innovation and adaptationthe history of computing is characterized by frequent modification ofthe structures for federal research support. as discussed in chapter 4,new organizations have been created, and existing ones have been modified to better adapt to changing technology, political influences, and, mostimportant, changing national needs.early work in computing, for example, was driven largely by defenseinterests. the eniac, the nationõs first digital electronic computer, wasdeveloped with funding from the army ballistic research laboratoryand produced its first operational calculations as part of the effort todevelop the hydrogen bomb. subsequently, dod became the largestfederal supporter of research in computer science and electrical engineerfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.154funding a revolutioning. in order to manage defenserelated investments in computing, neworganizations were needed. immediately after world war ii, the individual services established research offices (the army research office,office of naval research, and air force office of scientific research) tomanage their research portfolios. but the desire to prevent another technological surprise like sputnik and to separate defense research frominterservice rivalry and nearterm operational considerations demandedthe establishment of a separate agency, darpa. as the importance ofcomputing became increasingly apparent for defense applications,darpa established the information processing techniques office to manage computing research. this office has changed names and structureover the past 30 years, to better reflect changes in the technology, and hascontinued to invest in an everchanging array of computerrelated technologies.the founding of nsf in 1950 also followed from national imperatives,as policymakers and researchers alike tried to institutionalize and buildon the many successes the nation had in mobilizing the research community during world war ii (marked by the rapid development and introduction of innovations like the atomic bomb and radar). nsf establishedan office of computing activities in 1967 to support research, education,and computing facilities. the components of this office were later dispersed among other nsf directorates. recognizing the emergence ofcomputing as an independent discipline with its own research needs, nsfestablished the computer and information sciences and engineering(cise) directorate in 1986. cise, and its predecessors, carried out multiple missions: funding computing research, supporting educational initiatives, and maintaining computing and communications infrastructurefor the research community.growing concerns over the competitiveness of u.s. industry in the1980s and early 1990s produced a shift in federal policy for computingand a resultant shift in the organization of federal support for computingresearch. greater emphasis was placed on partnerships among government, universities, and industry to facilitate more rapid transfer of technology into the marketplace and to tie research more closely to industrialneeds. as a result, nsf established a number of engineering researchcenters (ercs) to better link academic research to industrial needs, andthe national institute of standards and technology began its advancedtechnology program, which funded consortia working on precompetitiveresearch projects of mutual interest. loss of market share in memorychips and semiconductor manufacturing equipment prompted the government to invest $100 million annually for 7 years in sematech, thesemiconductor manufacturing technology consortium, which brought together 12 of the nationõs largest semiconductor manufacturers to conductfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.lessons from history155precompetitive research that would strengthen the u.s. semiconductorindustry.the end of the cold war and the dominance of u.s. firms in theglobal market for computing in the 1990s significantly altered the politicalenvironment for research funding after 1990. it is likely that computingresearch will be redirected to new missions, whether improving health,providing government benefits (social security, food stamps, and so forth),or supporting economic growth. dod and other federal agencies willcontinue to demand advances in information technology to support theirmissions, but new organizational structures may also be needed to ensurethat the research enterprise is well matched to research needs.concluding remarksgiven the importance of computing to the nationõs economy, security, and health, it is important to ensure that the united states maintainsits leadership in the field. doing so will require the concerted efforts ofindustry, universities, and government. as the lessons above suggest, eachsector has an important role to play in the overall innovation process.while the information technology industry as a whole has evolved considerably over the past 50 years, opportunities for significant innovationcontinue to exist. expanding and exploiting information infrastructurefor a range of social, business, and personal needs will require continuedresearch and development to make computing and communications systems more capable, more useful, and more reliable. the success of suchefforts will depend in large part on resolving ongoing debates about thescope and direction of federal support for science and technology. notes1.darpa was named the advanced research projects agency, or arpa,from the time of its establishment in 1958 until the word òdefenseó was added in1972. it became arpa again between 1993 and 1995. for consistency, this chapter refers to the agency as darpa, its name in 1998, regardless of the time perioddescribed.2.estimates based on data extracted from the national science foundationõsdatabase on r&d expenditures, total and federally financed, in electrical engineering and computer science between fiscal years 1972 and 1996. online accessto the database is available via webcaspar at <http://caspar.qrc.com>.3.estimates based on data extracted from the national science foundationõsdatabase on current fund research equipment expenditures in electrical engineering and computer science between fiscal years 1981 and 1995. online accessto the database is available via webcaspar at <http://caspar.qrc.com>.4.in recent years, a number of concerns have been raised about a reductionfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.156funding a revolutionin the openness of university research, owing to increased links to industryfunded research of a proprietary nature.5.estimates based on data extracted from the national science foundationõsdatabase on federal research obligations to universities and colleges for basicand applied research in electrical engineering and computer science betweenfiscal years 1981 and 1995. online access to the database is available viawebcaspar at <http://caspar.qrc.com>.6.additional information on doeõs accelerated strategic computing initiative is available online at <http://www.llnl.gov/asci/>.7.one notable exception is the semiconductor research corporation, whichfunds university research of interest to its member companiesððmost of the largesemiconductor companies.8. researchers at bolt, beranek, and newman modified honeywell computers for use as switching devices, or routers, on the arpanet.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.part ii, chapters 6 through 10, presents five case studies in computingresearch. these case studies are not meant to be definitive histories of thefields they address; rather, they are intended to illustrate the role thefederal government has played in the innovation process by investing incomputing research. they contain historical material that is importantnot only in indicating the governmentõs role per se, but also in characterizing the larger innovation process, including the movement of researchers between universities and industry, the transfer of research results intopractice, and the interrelationships among people and research in government, universities, and industry.taken together, the cases cover a range of technologies, time periods,and federal investments; individually, they differ considerably in theirscope and emphasis. the case studies of relational databases and theinternet, for example, are relatively narrow in the sense that they trace thedevelopment of a particular technology or system. the innovations described have a fairly well defined beginning and end, although clearly thesystems described in each will continue to evolve over time. the casestudy of theoretical computer science highlights the development oftheory as well as its relationships to computer engineering and the construction of computer systems. it traces the refinement and disseminationof ideas throughout the research community and into educational curricula. the final two case studies, artificial intelligence (ai) and virtualreality (vr), address relatively broad areas of research that are motivatedpart iicase studies in computing research157funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.158funding a revolutionby a longterm vision: development of systems that display intelligentbehavior in the ai case, and development of systems that generate synthetic environments meant to resemble the real world in the vr case.progress in both these fields is marked by a series of research breakthroughsor technological advances that move ever closer to these objectives.each case study concludes with a summary of themes or lessons regarding the innovation process and the governmentõs essential role withinit. because they derive from the individual case studies, these lessonsreflect the particular conditions that prevailed at the time described, suchas the nascent state of the computing field and dominant styles of federalresearch management in past decades. the case studies themselves donot attempt to discuss the relevance of these lessons to the current policyenvironment or to provide guidance regarding future federal support forresearch in computing. that task is taken up instead in chapter 5 of thisreport, which synthesizes the lessons from all the case studies and attempts to consider their more general applicability.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.159largescale computer applications require rapid access to largeamounts of data. a computerized checkout system in a supermarketmust track the entire product line of the market. airline reservationsystems are used at many locations simultaneously to place passengerson numerous flights on different dates. library computers store millionsof entries and access citations from hundreds of publications. transactionprocessing systems in banks and brokerage houses keep the accounts thatgenerate international flows of capital. world wide web search enginesscan thousands of web pages to produce quantitative responses to queries almost instantly. thousands of small businesses and organizationsuse databases to track everything from inventory and personnel to dnasequences and pottery shards from archaeological digs.thus, databases not only represent significant infrastructure for computer applications, but they also process the transactions and exchangesthat drive the u.s. economy. a significant and growing segment of thesoftware industry, known as the database industry, generates about $8billion in annual revenue. u.s. companiesñincluding ibm corporation,oracle corporation, informix corporation, sybase incorporated, teradatacorporation (now owned by ncr corporation), and microsoft corporationñdominate the world market. this dominance stems from a serendipitous combination of industrial research, governmentfunded academic work, and commercial competition.much of todayõs market consists of relational databases based on themodel proposed in the late 1960s and early 1970s. this chapter provides6the rise of relational databasesfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.160funding a revolutionbackground on early data management systems and then examines theemergence of the relational model and its rise to dominance in the database field, and the translation of this model into successful commercialproducts. the final section summarizes the lessons to be learned fromhistory. it highlights the critical role of the government in advancing thistechnology. for instance, although the relational model was originallyproposed and developed at ibm, it was a governmentfunded effort at theuniversity of california at berkeley (ucberkeley) that disseminated theidea widely and gave it the intellectual legitimacy required for broadacceptance and commercialization.this case study does not address the entire database field (it omitstopics such as transaction processing, distributed databases, and multimedia), but rather focuses on events that illustrate the ways in whichsynergistic interactions of government, universities, and industry builtu.s. leadership in a particular subfield, largely through the work of individuals who developed and then transferred technology between firmsand laboratories. as james gray, a senior database researcher, has observed: òa very modest federal research investment, complemented byan alsomodest industrial research investment, led directly to u.s. dominance of this marketó (cstb, 1995a).backgroundemergence of computerized databasesthe u.s. government has always had significant requirements for thecollection, sorting, and reporting of large volumes of data. in 1890, thebureau of the census encouraged a former employee, herman hollerith,to develop the worldõs first automated information processing equipment.the resulting punchedcard machines processed the censuses of 1890 andof 1900. in 1911, hollerithõs company merged with another, also foundedwith census support; the resulting company soon became known as international business machines (anderson, 1988), now ibm.during world war i, the government used new punchedcard technology to process the various data sets required to control industrial production, collect the new income tax, and classify draftees. the socialsecurity act of 1935 made it necessary to keep continuous records on theemployment of 26 million individuals. for this, òthe worldõs biggestbookkeeping job,ó ibm developed special collating equipment. the census bureau purchased the first model of the first digital computer on thecommercial market, the univac i (itself based on the governmentfunded electronic discrete variable automatic computer (edvac)project at the university of pennsylvania). in 1959, the pentagon alonefunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.the rise of relational databases161had more than 200 computers just for its business needs (e.g., trackingexpenses, personnel, spare parts), with annual costs exceeding $70 million. u.s. dominance of the punchedcard data processing industry, initially established with government support, was a major factor in u.s.companiesõ later dominance in electronic computing.by the early 1960s, substantial progress had been made in removinghardwarespecific constraints from the tasks of programmers. the termòdatabaseó emerged to capture the sense that the information storedwithin a computer could be conceptualized, structured, and manipulatedindependently of the specific machine on which it resided. most of theearliest database applications were developed in military command andintelligence environments, but the concept was quickly adopted by commercial users (system development corporation, 1964; fry and sibley,1974).early efforts at standardizationas computing entered the mainstream commercial market, a numberof techniques emerged to facilitate data access, ensure quality, maintainprivacy, and allow for managerial control of data. in 1960, the conferenceon data systems languages (codasyl), set up by the u.s. department ofdefense (dod) to standardize software applications, established the common businessoriented language (cobol) for programming (acm sigplan,1978), incorporating a number of prior datadefinition languages (fry andsibley, 1974). magnetic disk drives, which could access data at random,began to replace magnetic tape drives, which required serial data access,for online storage. in 1961, charles bachman at general electric company introduced the integrated data store (ids) system, a pioneering database management system that took advantage of the new storage technology and included novel schemas and logging, among other features.during these early years, innovations in the practiceoriented fieldtended to be made by user groups and industrial researchers, with littleacademic involvement (cstb, 1982; wiederhold, 1984). in the mid1960s,bachman and others, largely from industry and manufacturing, set up thedatabase task group (dbtg) under codasyl to bring some unity to thevaried field (olle, 1978). the group published a set of specifications forhow computer languages, cobol in particular, might navigate databases. in 1971, it published a formal standard, known colloquially in theindustry as the codasyl approach to database management. a number ofcodasylbased products were introduced for mainframe computers byeckertmauchly computer corporation (the maker of univac), honeywellincorporated, and siemens ag, and, for minicomputers, by digital equipment corporation (dec) and prime computer corporation.1funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.162funding a revolutionnotably missing from the list of vendors that supported codasylproducts was ibm, which had earlier (in 1968) introduced its own product, ims, derived in part from a national aeronautics and space administration (nasa) apollo project. it ran on system/360 equipment.whereas codasyl was based on a network model of data, ibmõs databaseused a hierarchical structure. (cullinet corporation provided a codasylcompatible database for ibm users.) both the ibm and the codasyl products were sometimes called navigational databases because they requiredthe user to program or navigate around a data set. bachmanõs turingaward lecture in 1973, in fact, was entitled òthe programmer as navigatoró (bachman, 1973; cardenas, 1979).emergence of the relational modelcoddõs visionat least one researcher at ibm was dissatisfied with both the codasylproducts and ibmõs database package. edgar f. (ted) codd, an oxfordtrained mathematician, joined ibm in 1949 and later moved to ibm sanjose. codd found existing and new database technologies òtaking theoldline view that the burden of finding information should be placed onusers. . . . [in this view, the database management system] should onlyrecognize simple commands and it would be up to the users to put together appropriate commands for finding what was neededó (codd,1982).2in a series of ibm technical reports and then a landmark paper, òarelational model of data for large shared data banks,ó codd laid out anew way to organize and access data. what codd called the òrelationalmodeló rested on two key points:it provides a means of describing data with its natural structure onlyñthat is, without superimposing any additional structure for machine representation purposes. accordingly, it provides a basis for a high leveldata language which will yield maximal independence between programs on the one hand and machine representation on the other. (codd,1970)in other words, the relational model consisted of (1) data independence from hardware and storage implementation and (2) automatic navigation, or a highlevel, nonprocedural language for accessing data. instead of processing one record at a time, a programmer could use thelanguage to specify single operations that would be performed across theentire data set. coddõs model had an immediate impact on research and,as described below, spawned a number of significant prototyping projects.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.the rise of relational databases163given its eventual commercial success, the relational model mightseem bound to emerge and even dominate the field without any government involvement in research. it was formulated, after all, entirely withinthe walls of an industrial laboratory. but coddõs model was long seen assomething of an intellectual curiosity. to gain legitimacy within the field,it had to survive at least two battlesñone in the technical community atlarge, and one within ibm. the relational model might not have survivedeither battle without government intervention, which, in this case, involved funding of a competing project at another institution.within ibm, the trouble was the existing database product, ims. thecompany had already invested, both financially and organizationally, inthe infrastructure and expertise required to sell and support it. a radicalnew technology had a great deal to prove before it could displace a successful, reliable, revenuegenerating product such as ims. initially, thethreat was minimal; codd published his original paper in the open literature because no one at ibm (himself included) recognized its eventualimpact. the response to this publication from the outside technical community, however, soon showed the company that the idea had great commercial potential. to head off this eventuality, ibm quickly declared imsits sole strategic product, thus setting up codd and his work to be criticized as counter to company goals. internal politics further compoundedthe situation, as ibm was not accustomed to major software innovationscoming from ibm san jose, which until then had worked primarily ondisk storage.in spite of ibmõs reaction, codd spoke out zealously and promotedthe virtues of the relational model to computer scientists. he arranged apublic debate between himself and charles bachman, at that time the keyproponent of the codasylsponsored standard. the debate exposed coddto criticism from within ibm that he was undermining the companyõsexisting products, but it also achieved his intended effect on the technicalcommunity. in the early 1970s, two projects emerged to develop relational technology and prove its utility in practical applications. one,system r, began within ibm, and the other, ingres, began at ucberkeleywith military and national science foundation (nsf) funding. the synergy between the two projects, which were at once mutually reinforcingand competing, demonstrates the subtle but significant effects that governmentsupported research can have on computer technology.system rin the early 1970s, a group of ibm programmers moved fromyorktown to san jose.3 the group designed and built a prototype systemto demonstrate relational ideas. dubbed system r, this prototype wasfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.164funding a revolutionintended to provide a highlevel, nonnavigational, dataindependent interface to many users simultaneously, with high integrity and robustness(astrahan et al., 1976). the first phase of the project, in 19741975, produced a quick prototype to demonstrate feasibility, but its code was eventually abandoned. the next phase produced a fullfunction, multiuserversion, which was evaluated in subsequent trials in 19781979. perhapsthe most lasting development to come out of the project was the structured query language (sql), now a u.s. and international standard fordatabase access (chamberlin et al., 1981; mcjones, 1995).4on its own, system r did not convince ibm management to abandonits existing product and replace it with relational databases. ibm and itscustomers still had strong vested interests in the established ims technology. it took outside efforts, funded by the government, to prove thatrelational databases could become viable commercial products.ingresin 1973, about when system r was getting started at ibm, two scientists at ucberkeley, michael stonebraker and eugene wong, becameinterested in relational databases. initially, they raised money to design ageographic data system for berkeleyõs economics group (the name ingres,which stood for interactive graphics and retrieval system, reflects thislegacy). in search of further support, stonebraker approached the defense advanced research projects agency (darpa), the obvious funding source for computing research and development. both darpa andthe office of naval research (onr) turned ingres down, however; theywere already supporting database research elsewhere.stonebraker then introduced his idea to other agencies, and, withhelp from wong and berkeley colleague lotfi zadeh, he eventually obtained modest support from the nsf and three military agencies: the airforce office of scientific research, the army research office, and thenavy electronic systems command. the experience of acquiring support for ingres illustrates the importance of maintaining diverse fundingsources within the government. when a researcher can propose a newidea to several potential supporters, it not only increases the chances offunding a good idea but also provides a crucial learning process as proposals are rewritten and resubmitted.thus funded, ingres was developed, during the mid1970s, into aprototype relational database system that was similar to ibmõs system rbut based on different hardware and a different operating system. ingreswent through an evolution similar to that of system r, with an earlyphase demonstrating an initial solution in 1974 followed by significantrevisions to make the code maintainable. ingres was then disseminatedfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.the rise of relational databases165to a small user community, both inside and outside academia, whichprovided feedback to the development group. the dissemination processwas advanced by the proliferation of inexpensive dec machines in universities. members of the project team rewrote the ingres prototype repeatedly during these years to incorporate accumulated experience, feedback from users, and new ideas. ingres also included its own querylanguage, quel, which was similar to, but still distinct from, ibmõs sql(stonebraker, 1976, 1980).5diffusion and commercialization ofrelational databasesingres technology diffused into the commercial sector through threemajor channels: code, people, and publications. unlike the technicaldetails of the ibm project, ingres source code was publicly available, andabout 1,000 copies were distributed around the world so that computerscientists and programmers could experiment with the system and adjustit to their own needs. michael stonebraker founded ingres corporation(purchased by computer associates in 1994) to commercialize the berkeley code directly. robert epstein, the chief programmer at ingres in the1970s, went on to cofound brittonlee incorporated and then sybase.both brittonlee and sybase used ideas and experience from the originalingres, and government agencies were early customers of both companies. computer associates released a commercial version of the ingrescode in the 1980s.continued movement of ingres researchers throughout the databasecommunity spread the technology even farther. jerry held and carolyouseffi moved from ucberkeley to tandem computers incorporated,where they built a relational system, the predecessor to nonstop sql.until joining kleiner, perkins, caufield & byers in 1998, held was seniorvicepresident of engineering at oracle, where he headed that companyõsdatabase efforts. paula hawthorn moved from ingres to brittonlee (asdid michael ubell) and eventually became a cofounder of illustra information technologies incorporated, now part of informix. stonebrakerhimself worked with ingres corporation, illustra, and informix. otheringres alumni went to at&t, hewlettpackard company (hp), ibm, andoracle, bringing with them the lessons learned from ingres. as robertepstein observed, òwhat came from ingres was the experience of havingbuilt a prototype . . . to say what parts need to be done differently.ó6the ingres and system r development groups had a complex relationship that fostered a spirit of competition, as both groups worked onsimilar new technology. both groups were relatively small and closeknit. between 1973 and 1979, approximately 30 individuals cycledfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.166funding a revolutionthrough the ingres group, which never contained more than five or sixprogrammers. the system r group included roughly 15 persons whowrote code and papers on system r and later worked with ibmõs productdevelopment groups to commercialize the technology. several membersof the ibm group had berkeley connections, and ucberkeley sent summer students to ibm. timely publication and proper allocation of creditfor new ideas became paramount concerns (mcjones, 1995). ingresõsquel was in competition with ibmõs sql as a query language; the lattereventually won out and became the industry standard.the success of sql transpired almost in spite of ibm, which couldhave taken advantage of its query language several years sooner than itdid. oracle, founded by larry ellison, developed and began selling ansqlcompatible product even before ibm had an sql product in themarket. ellison had learned of sql through publications by the system rproject team. ibm was compelled to develop its sql/ds system by thethreat of competing products from other established database companies,such as software ag, a german company. other fledgling companies,such as informix and ingres, also introduced relational systems, withinformix embracing the sql model. system r programmers influencedthe industry personally as well as by their writing.7system r and ingres were not the only relational database efforts tospring from coddõs work. other research at the university of toronto,ibm in the united kingdom, the university of utah, and the university ofwisconsin made contributions as well. it also became clear that the relational model has limitations, particularly in handling complex data. in1982, the ingres project ended, and in 1985 it was transformed intopostgres at ucberkeley, which sought to extend the relational model toobjects. this change coincided with darpa hiring its first program manager for databases, who funded postgres. this project became a component of the digital library and scientific database efforts within the university of california system.a landmark year for the relational model was 1980, when ibmõs sql/ds product hit the market for mainframes, smaller vendors began sellingsecondgeneration relational systems with great commercial success, andcodd was awarded the association for computing machineryõs (acm)turing award. the relational model had come of age.today, relational databases are but one way of accessing the multipletypes of information computers can handle. related research in information retrieval, multimedia, scientific databases, and digital libraries is under way, supported by darpa, nsf, and the national library of medicine, among others. still, the history of the emergence of relationaldatabase technologies, products, and companies reveals a good deal aboutinnovation in computing and communications.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.the rise of relational databases167lessons from historythe federal government had important effects on the development ofrelational databases. the earliest days of this subfield suggest that government missions can create new markets for technology, providing incentives for innovation. the census bureauõs need to conduct a decadalcensus supported the information processing industry before computerswere created to automate it.later on, government funding hastened commercialization. an example is the case of system r and ingres. the critical issue is not whichone was more successful or influential in the long run, but rather, toparaphrase a system r team member, whether either project would havesucceeded in the absence of the other. the academic interest legitimizedsystem r within ibm, and ingres was bootstrapped off ibmõs commercialinfluence.8 competitive pressure, combined with the legitimacy bestowed on the relational model by government funding and academicinterest, finally convinced ibm to sell relational database products. wereit not for the governmentfunded effort at ucberkeley, such databasesprobably would have been commercialized anyway, but laterñand timetomarket is, of course, a critical factor with new technology.that same example shows that the commercial interests of firms suchas ibm can impede the continued development and commercialization oftechnologies that compete with existing product lines. ibm and its customers had vested interests in the established ims technology and resisted change until external events proved that relational databases couldbecome viable commercial products.this case history also suggests that the large numbers of researcherspassing through university laboratories, their willingness to share dataand code, and their publication imperatives make university researchersideal sources of technology transfer to the broader technical community.industrial laboratories, by comparison, rarely place significant technologies directly into the public domain and have lower rates of personnelturnover, although they often benefit from greater and more stable supplies of resources. especially in the computing industry, employees maytake ideas into the marketplace on their own, but industrial laboratoriesare likely to publish only information that concerns completed projects oris not deemed critical to the companyõs vital interests.academic research is important for other reasons as well. because itcan push the cutting edge of technology and produce results that mayevolve into commercially viable products, existing commercial suppliersnever have a lock on advanced technology and are forced to respond tothe marketplace of ideas.finally, in pursuing new ideas and new areas of technology, acafunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.168funding a revolutiondemic research projects can benefit from access to multiple fundingsources within the government, as any individual sponsor may assess thevalue of a new idea from a limited perspective. although darpa andonr declined to support ingres, for example, the nsf and three othermilitary agencies agreed to do so.notes1.for his work with ids and the codasyl group, bachman was awarded theassociation for computing machineryõs a.m. turing award in 1973 (bachman,1973; king, 1983).2.edgar f. codd, in an interview with a representative of the committee oninnovation in computing and communications, february 7, 1997.3.the group included mike blasgen, ray boyce, donald chamberlain, jamesgray, frank king, leonard liu, raymond lorie, and franco putzolu.4.donald chamberlin, in an interview with a representative of the committee on innovation in computing and communications, february 4, 1997.5.m. stonebraker, in interviews with a representative of the committee oninnovation in computing and communications, december 27, 1996, and february 26, 1997.6.robert epstein, in an interview with a representative of the committee oninnovation in computing and communications, march 19, 1997.7.kapali eswaran left ibm in the late 1970s to form his own company, andits code eventually became part of hp and cullinet products. jim gray movedfrom ibm to tandem, where he worked on nonstop sql, and he is now thesenior database researcher at microsoft. franco putzolu also went from ibm totandem, where he was a principal designer of nonstop sql, and later went tooracle as a senior database architect.8.donald chamberlin, in an interview with a representative of the committee on innovation in computing and communications, february 4, 1997.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.1697development of the internet andthe world wide webthe recent growth of the internet and the world wide web makes itappear that the world is witnessing the arrival of a completely new technology. in fact, the webñnow considered to be a major driver of the waysociety accesses and views informationñis the result of numerous projectsin computer networking, mostly funded by the federal government, carried out over the last 40 years. the projects produced communicationsprotocols that define the format of network messages, prototype networks,and application programs such as browsers. this research capitalized onthe ubiquity of the nationõs telephone network, which provided the underlying physical infrastructure upon which the internet was built.this chapter traces the development of the internet,1 one aspect of thebroader field of data networking. the chapter is not intended to be comprehensive; rather, it focuses on the federal role in both funding researchand supporting the deployment of networking infrastructure. this history is divided into four distinct periods. before 1970, individual researchers developed the underlying technologies, including queuingtheory, packet switching, and routing. during the 1970s, experimentalnetworks, notably the arpanet, were constructed. these networkswere primarily research tools, not service providers. most were federallyfunded, because, with a few exceptions, industry had not yet realized thepotential of the technology. during the 1980s, networks were widelydeployed, initially to support scientific research. as their potential toimprove personal communications and collaboration became apparent,additional academic disciplines and industry began to use the technolfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.170funding a revolutionogy. in this era, the national science foundation (nsf) was the majorsupporter of networking, primarily through the nsfnet, which evolvedinto the internet. most recently, in the early 1990s, the invention of theweb made it much easier for users to publish and access information,thereby setting off the rapid growth of the internet. the final section ofthe chapter summarizes the lessons to be learned from history.by focusing on the internet, this chapter does not address the fullscope of computer networking activities that were under way between1960 and 1995. it specifically ignores other networking activities of amore proprietary nature. in the mid1980s, for example, hundreds ofthousands of workers at ibm were using electronic networks (such as thevnet) for worldwide email and file transfers; banks were performingelectronic funds transfer; compuserve had a worldwide network; digitalequipment corporation (dec) had valueadded networking services; and avnetbased academic network known as bitnet had been established.these were proprietary systems that, for the most part, owed little toacademic research, and indeed were to a large extent invisible to theacademic computer networking community. by the late 1980s, ibmõsproprietary sna data networking business unit already had several billions of dollars of annual revenue for networking hardware, software, andservices. the success of such networks in many ways limited the interestof companies like ibm and compuserve in the internet. the success ofthe internet can therefore, in many ways, be seen as the success of an opensystem and open architecture in the face of proprietary competition.early steps: 19601970approximately 15 years after the first computers became operational,researchers began to realize that an interconnected network of computerscould provide services that transcended the capabilities of a single system. at this time, computers were becoming increasingly powerful, anda number of scientists were beginning to consider applications that wentfar beyond simple numerical calculation. perhaps the most compellingearly description of these opportunities was presented by j.c.r. licklider(1960), who argued that, within a few years, computers would becomesufficiently powerful to cooperate with humans in solving scientific andtechnical problems. licklider, a psychologist at the massachusetts institute of technology (mit), would begin realizing his vision when he became director of the information processing techniques office (ipto) atthe advanced research projects agency (arpa) in 1962. licklider remained at arpa until 1964 (and returned for a second tour in 19741975),and he convinced his successors, ivan sutherland and robert taylor, ofthe importance of attacking difficult, longterm problems.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.development of the internet and the world wide web171taylor, who became ipto director in 1966, worried about the duplication of expensive computing resources at the various sites with arpacontracts. he proposed a networking experiment in which users at onesite accessed computers at another site, and he coauthored, withlicklider, a paper describing both how this might be done and some ofthe potential consequences (licklider and taylor, 1968). taylor was apsychologist, not a computer scientist, and so he recruited larry robertsof mitõs lincoln laboratory to move to arpa and oversee the development of the new network. as a result of these efforts, arpa became theprimary supporter of projects in networking during this period.in contrast to the nsf, which awarded grants to individual researchers, arpa issued research contracts. the ipto program managers, typically recruited from academia for 2year tours, had considerable latitudein defining projects and identifying academic and industrial groups tocarry them out. in many cases, they worked closely with the researchersthey sponsored, providing intellectual leadership as well as financial support. a strength of the arpa style was that it not only produced artifactsthat furthered its missions but also built and trained a community ofresearchers. in addition to holding regular meetings of principal investigators, taylor started the òarpa games,ó meetings that brought togetherthe graduate students involved in programs. this innovation helpedbuild the community that would lead the expansion of the field andgrowth of the internet during the 1980s.during the 1960s, a number of researchers began to investigate thetechnologies that would form the basis for computer networking. most ofthis early networking research concentrated on packet switching, a technique of breaking up a conversation into small, independent units, eachof which carries the address of its destination and is routed through thenetwork independently. specialized computers at the branching pointsin the network can vary the route taken by packets on a momenttomoment basis in response to network congestion or link failure.one of the earliest pioneers of packet switching was paul baran of therand corporation, who was interested in methods of organizing networks to withstand nuclear attack. (his research interest is the likelysource of a widespread myth concerning the arpanetõs original purpose [hafner and lyon, 1996]). baran proposed a richly interconnectedset of network nodes, with no centralized control systemñboth properties of todayõs internet. similar work was under way in the united kingdom, where donald davies and roger scantlebury of the national physical laboratory (npl) coined the term òpacket.óof course, the united states already had an extensive communications network, the public switched telephone network (pstn), in whichdigital switches and transmission lines were deployed as early as 1962.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.172funding a revolutionbut the telephone network did not figure prominently in early computernetworking. computer scientists working to interconnect their systemsspoke a different language than did the engineers and scientists workingin traditional voice telecommunications. they read different journals,attended different conferences, and used different terminology. moreover, data traffic was (and is) substantially different from voice traffic. inthe pstn, a continuous connection, or circuit, is set up at the beginning ofa call and maintained for the duration. computers, on the other hand,communicate in bursts, and unless a number of òcallsó can be combinedon a single transmission path, line and switching capacity is wasted. telecommunications engineers were primarily interested in improving thevoice network and were skeptical of alternative technologies. as a result,although telephone lines were used to provide pointtopoint communication in the arpanet, the switching infrastructure of the pstn was notused. according to taylor, some bell laboratories engineers stated flatlyin 1967 that òpacket switching wouldnõt work.ó2at the first association for computing machinery (acm) symposium on operating system principles in 1967, lawrence roberts, then anipto program manager, presented an initial design for the packetswitched network that was to become the arpanet (davies et al., 1967).in addition, roger scantlebury presented the npl work (roberts, 1967),citing baranõs earlier rand report. the reaction was positive, and roberts issued a request for quotation (rfq) for the construction of a fournode network.from the more than 100 respondents to the rfq, roberts selectedbolt, beranek, and newman (bbn) of cambridge, massachusetts; familiarnames such as ibm corporation and control data corporation chose notto bid. the contract to produce the hardware and software was issued indecember 1968. the bbn group was led by frank heart, and many of thescientists and engineers who would make major contributions to networking in future years participated. robert kahn, who with vinton cerfwould later develop the transmission control protocol/internet protocol(tcp/ip) suite used to control the transmission of packets in the network,helped develop the network architecture. the network hardware consisted of a rugged military version of a honeywell corporation minicomputer that connected a siteõs computers to the communication lines. theseinterface message processors (imps)ñeach the size of a large refrigeratorand painted battleship grayñwere highly sought after by darpasponsored researchers, who viewed possession of an imp as evidence they hadjoined the inner circle of networking research.the first arpanet node was installed in september 1969 at leonardkleinrockõs network measurement center at the university of californiaat los angeles (ucla). kleinrock (1964) had published some of thefunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.development of the internet and the world wide web173earliest theoretical work on packet switching, and so this site was anappropriate choice. the second node was installed a month later atstanford research institute (sri) in menlo park, california, using douglas engelbartõs on line system (known as nls) as the host. sri alsooperated the network information center (nic), which maintained operational and standards information for the network. two more nodeswere soon installed at the university of california at santa barbara, whereglen culler and burton fried had developed an interactive system formathematics education, and the university of utah, which had one of thefirst computer graphics groups.initially, the arpanet was primarily a vehicle for experimentationrather than a service, because the protocols for hosttohost communication were still being developed. the first such protocol, the networkcontrol protocol (ncp), was completed by the network working group(nwg) led by stephen crocker in december 1970 and remained in useuntil 1983, when it was replaced by tcp/ip.expansion of the arpanet: 19701980initially conceived as a means of sharing expensive computing resources among arpa research contractors, the arpanet evolved in anumber of unanticipated directions during the 1970s. although a fewexperiments in resource sharing were carried out, and the telnet protocolwas developed to allow a user on one machine to log onto another machine over the network, other applications became more popular.the first of these applications was enabled by the file transfer protocol (ftp), developed in 1971 by a group led by abhay bhushan of mit(bhushan, 1972). this protocol enabled a user on one system to connect toanother system for the purpose of either sending or retrieving a particularfile. the concept of an anonymous user was quickly added, with constrained access privileges, to allow users to connect to a system andbrowse the available files. using telnet, a user could read the remote filesbut could not do anything with them. with ftp, users could now movefiles to their own machines and work with them as local files. this capability spawned several new areas of activity, including distributed clientserver computing and networkconnected file systems.occasionally in computing, a òkiller applicationó appears that becomes far more popular than its developers expected. when personalcomputers (pcs) became available in the 1980s, the spreadsheet (initiallyvisicalc) was the application that accelerated the adoption of the newhardware by businesses. for the newly minted arpanet, the killerapplication was electronic mail, or email. the first email program wasdeveloped in 1972 by ray tomlinson of bbn. tomlinson had built anfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.174funding a revolutionearlier email system for communication between users on bbnõs tenextimesharing system, and it was a simple exercise to modify this system towork over the network. by combining the immediacy of the telephonewith the precision of written communication, email became an instanthit. tomlinsonõs syntax (user@domain) remains in use today.telnet, ftp, and email were examples of the leverage that researchtypically provided in early network development. as each new capability was added, the efficiency and speed with which knowledge could bedisseminated improved. email and ftp made it possible for geographically distributed researchers to collaborate and share results much moreeffectively. these programs were also among the first networking applications that were valuable not only to computer scientists, but also toscholars in other disciplines.from arpanet to internetalthough the arpanet was arpaõs largest networking effort, itwas by no means the only one. the agency also supported research onterrestrial packet radio and packet satellite networks. in 1973, robertkahn and vinton cerf began to consider ways to interconnect these networks, which had quite different bandwidth, delay, and error propertiesthan did the telephone lines of the arpanet. the result was tcp/ip,first described in 1973 at an international network working group meeting in england. unlike ncp, which enabled the hosts of a single networkto communicate, tcp/ip was designed to interconnect multiple networksto form an internet. this protocol suite defined the packet format and aflowcontrol and errorrecovery mechanism to allow the hosts to recovergracefully from network errors. it also specified an addressing mechanism that could support an internet comprising up to 4 billion hosts.the work necessary to transform tcp/ip from a concept into a usefulsystem was performed under arpa contract by groups at stanford university, bbn, and university college london. although tcp/ip hasevolved over the years, it is still in use today as the internetõs basic packettransport protocol.by 1975, the arpanet had grown from its original four nodes tonearly 100 nodes. around this time, two phenomenañthe development oflocal area networks (lans) and the integration of networking into operating systemsñcontributed to a rapid increase in the size of the network.local area networkswhile arpanet researchers were experimenting with dedicatedtelephone lines for packet transmission, researchers at the university offunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.development of the internet and the world wide web175hawaii, led by norman abramson, were trying a different approach, alsowith arpa funding. like the arpanet group, they wanted to provideremote access to their main computer system, but instead of a network oftelephone lines, they used a shared radio network. it was shared in thesense that all stations used the same channel to reach the central station.this approach had a potential drawback: if two stations attempted totransmit at the same time, then their transmissions would interfere witheach other, and neither one would be received. but such interruptionswere unlikely because the data were typed on keyboards, which sent veryshort pulses to the computer, leaving ample time between pulses duringwhich the channel was clear to receive keystrokes from a different user.abramsonõs system, known as aloha, generated considerable interestin using a shared transmission medium, and several projects were initiated to build on the idea. two of the bestknown projects were the atlantic packet satellite experiment and ethernet. the packet satellite networkdemonstrated that the protocols developed in aloha for handling contention between simultaneous users, combined with more traditional reservation schemes, resulted in efficient use of the available bandwidth. however, the long latency inherent in satellite communications limited theusefulness of this approach.ethernet, developed by a group led by robert metcalfe at xeroxcorporationõs palo alto research center (parc), is one of the few examples of a networking technology that was not directly funded by thegovernment. this experiment demonstrated that using coaxial cable as ashared medium resulted in an efficient network. unlike the aloha system, in which transmitters could not receive any signals, ethernet stationscould detect that collisions had occurred, stop transmitting immediately,and retry a short time later (at random). this approach improved theefficiency of the aloha technique and made it practical for actual use.sharedmedia lans became the dominant form of computertocomputercommunication within a building or local area, although variations fromibm (token ring) and others also captured part of this emerging market.ethernet was initially used to connect a network of approximately 100of parcõs alto pcs, using the centerõs timesharing system as a gatewayto the arpanet. initially, many believed that the small size and limitedperformance of pcs would preclude their use as network hosts, but, withdarpa funding, david clarkõs group at mit, which had received several altos from parc, built an efficient tcp implementation for thatsystem and, later, for the ibm pc. the proliferation of pcs connected bylans in the 1980s dramatically increased the size of the internet.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.176funding a revolutionintegrated networkinguntil the 1970s, academic computer science research groups used avariety of computers and operating systems, many of them constructedby the researchers themselves. most were timesharing systems that supported a number of simultaneous users. by 1970, many groups had settledon the digital equipment corporation (dec) pdp10 computer and thetenex operating system developed at bbn. this standardization enabledresearchers at different sites to share software, including networking software.by the late 1970s, the unix operating system, originally developed atbell labs, had become the system of choice for researchers, because it ranon decõs inexpensive (relative to other systems) vax line of computers.during the late 1970s and early 1980s, an arpafunded project at theuniversity of california at berkeley (ucberkeley) produced a version ofunix (the berkeley system distribution, or bsd) that included tightlyintegrated networking capabilities. the bsd was rapidly adopted by theresearch community because the availability of source code made it auseful experimental tool. in addition, it ran on both vax machines andthe personal workstations provided by the fledgling sun microsystems,inc., several of whose founders came from the berkeley group. the tcp/ip suite was now available on most of the computing platforms used bythe research community.standards and managementunlike the various telecommunications networks, the internet has noowner. it is a federation of commercial service providers, local educational networks, and private corporate networks, exchanging packets using tcp/ip and other, more specialized protocols. to become part of theinternet, a user need only connect a computer to a port on a serviceproviderõs router, obtain an ip address, and begin communicating. toadd an entire network to the internet is a bit trickier, but not extraordinarily so, as demonstrated by the tens of thousands of networks with tens ofmillions of hosts that constitute the internet today.the primary technical problem in the internet is the standardizationof its protocols. today, this is accomplished by the internet engineeringtask force (ietf), a voluntary group interested in maintaining and expanding the scope of the internet. although this group has undergonemany changes in name and makeup over the years, it traces its rootsdirectly to stephen crockerõs nwg, which defined the first arpanetprotocol in 1969. the nwg defined the system of requests for comments(rfcs) that are still used to specify protocols and discuss other engineerfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.development of the internet and the world wide web177ing issues. todayõs rfcs are still formatted as they were in 1969, eschewing the decorative fonts and styles that pervade todayõs web.joining the ietf is a simple matter of asking to be placed on its mailing list, attending thriceyearly meetings, and participating in the work.this grassroots group is far less formal than organizations such as theinternational telecommunications union, which defines telephony standards through the work of members who are essentially representativesof various governments. the open approach to internet standards reflects the academic roots of the network.closing the decadethe 1970s were a time of intensive research in networking. much ofthe technology used today was developed during this period. severalnetworks other than arpanet were assembled, primarily for use bycomputer scientists in support of their own research. most of the workwas funded by arpa, although the nsf provided educational supportfor many researchers and was beginning to consider establishing a largescale academic network.during this period, arpa pursued highrisk research with the potential for high payoffs. its work was largely ignored by at&t, and themajor computer companies, notably ibm and dec, began to offer proprietary networking solutions that competed with, rather than applied, thearpadeveloped technologies.3 yet the technologies developed underarpa contract ultimately resulted in todayõs internet. it is debatablewhether a more riskaverse organization lacking the handson programmanagement style of arpa could have produced the same result.operation of the arpanet was transferred to the defense communication agency in 1975. by the end of the decade, the arpanet hadmatured sufficiently to provide services. it remained in operation until1989, when it was superseded by subsequent networks. the stage wasnow set for the internet, which was first used by scientists, then by academics in many disciplines, and finally by the world at large.the nsfnet years: 19801990during the late 1970s, several networks were constructed to serve theneeds of particular research communities. these networksñtypicallyfunded by the federal agency that was the primary supporter of the research areañincluded mfenet, which the department of energy established to give its magnetic fusion energy researchers access tosupercomputers, and nasaõs space physics analysis network (span).the nsf began supporting network infrastructure with the establishmentfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.178funding a revolutionof csnet, which was intended to link university computer science departments with the arpanet. the csnet had one notable propertythat the arpanet lacked: it was open to all computer science researchers, whereas only arpa contractors could use the arpanet. an nsfgrant to plan the csnet was issued to larry landweber at the universityof wisconsin in 1980.the csnet was used throughout the 1980s, but as it and other regional networks began to demonstrate their usefulness, the nsf launcheda much more ambitious effort, the nsfnet. from the start, the nsfnetwas designed to be a network of networksñan òinternetóñwith a highspeed backbone connecting nsfõs five supercomputer centers and thenational center for atmospheric research. to oversee the new network,the nsf hired dennis jennings from trinity college, dublin. in the early1980s, jennings had been responsible for the irish higher education authority network (heanet), and so he was wellqualified for the task. oneof jenningsõ first decisions was to select tcp/ip as the primary protocolsuite for the nfsnet.because the nsfnet was to be an internet (the beginning of todayõsinternet), specialized computers called routers were needed to pass trafficbetween networks at the points where the networks met. today, routersare the primary products of multibilliondollar companies (e.g., ciscosystems incorporated, bay networks), but in 1985, few commercial products were available. the nsf chose the òfuzzballó router designed bydavid mills at the university of delaware (mills, 1988). working witharpa support, mills improved the protocols used by the routers to communicate the network topology among themselves, a critical function in alargescale network.another technology required for the rapidly growing internet wasthe domain name service (dns). developed by paul mockapetris at theuniversity of southern californiaõs information sciences institute, thedns provides for hierarchical naming of hosts. an administrative entity,such as a university department, can assign host names as it wishes. italso has a domain name, issued by the higherlevel authority of which it isa part. (thus, a host named xyz in the computer science department atucberkeley would be named xyz.cs.berkeley.edu.) servers locatedthroughout the internet provide translation between the host names usedby human users and the ip addresses used by the internet protocols. thenamedistribution scheme has allowed the internet to grow much morerapidly than would be possible with centralized administration.jennings left the nsf in 1986. he was succeeded by stephen wolff,who oversaw the deployment and growth of the nsfnet. during wolffõstenure, the speed of the backbone, originally 56 kilobits per second, wasincreased 1,000fold, and a large number of academic and regional netfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.development of the internet and the world wide web179works were connected to the nsfnet. the nsf also began to expand thereach of the nsfnet beyond its supercomputing centers through its connections program, which targeted the research and education community. in response to the connections solicitation, the nsf received innovative proposals from what would become two of the major regionalnetworks: suranet and nysernet. these groups proposed to develop regional networks with a single connection to the nsfnet, insteadof connecting each institution independently.hence, the nsfnet evolved into a threetiered structure in whichindividual institutions connected to regional networks that were, in turn,connected to the backbone of the nsfnet. the nsf agreed to provideseed funding for connecting regional networks to the nsfnet, with theexpectation that, as a critical mass was reached, the private sector wouldtake over the management and operating costs of the internet. this decision helped guide the internet toward selfsufficiency and eventual commercialization (computer science and telecommunications board, 1994).as the nsfnet expanded, opportunities for privatization grew.wolff saw that commercial interests had to participate and provide financial support if the network were to continue to expand and evolve into alarge, single internet. the nsf had already (in 1987) contracted withmerit computer network incorporated at the university of michigan tomanage the backbone. merit later formed a consortium with ibm andmci communications corporation called advanced network and services (ans) to oversee upgrades to the nsfnet. instead of reworkingthe existing backbone, ans added a new, privately owned backbone forcommercial services in 1991.4emergence of the web: 1990 to the presentby the early 1990s, the internet was international in scope, and itsoperation had largely been transferred from the nsf to commercial providers. public access to the internet expanded rapidly thanks to the ubiquitous nature of the analog telephone network and the availability ofmodems for connecting computers to this network. digital transmissionbecame possible throughout the telephone network with the deploymentof optical fiber, and the telephone companies leased their broadband digital facilities for connecting routers and regional networks to the developers of the computer network. in april 1995, all commercialization restrictions on the internet were lifted. although still primarily used byacademics and businesses, the internet was growing, with the number ofhosts reaching 250,000. then the invention of the web catapulted theinternet to mass popularity almost overnight.the idea for the web was simple: provide a common format forfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.180funding a revolutiondocuments stored on server computers, and give each document a uniquename that can be used by a browser program to locate and retrieve thedocument. because the unique names (called universal resource locators,or urls) are long, including the dns name of the host on which they arestored, urls would be represented as shorter hypertext links in otherdocuments. when the user of a browser clicks a mouse on a link, thebrowser retrieves and displays the document named by the url.this idea was implemented by timothy bernerslee and robertcailliau at cern, the highenergy physics laboratory in geneva, switzerland, funded by the governments of participating european nations.bernerslee and cailliau proposed to develop a system of links betweendifferent sources of information. certain parts of a file would be madeinto nodes, which, when called up, would link the user to other, relatedfiles. the pair devised a document format called hypertext markup language (html), a variant of the standard generalized markup languageused in the publishing industry since the 1950s. it was released at cernin may 1991. in july 1992, a new internet protocol, the hypertext transferprotocol (http), was introduced to improve the efficiency of documentretrieval. although the web was originally intended to improve communications within the physics community at cern, itñlike email 20 yearsearlierñrapidly became the new killer application for the internet.the idea of hypertext was not new. one of the first demonstrations ofa hypertext system, in which a user could click a mouse on a highlightedword in a document and immediately access a different part of the document (or, in fact, another document entirely), occurred at the 1967 falljoint computer conference in san francisco. at this conference, douglasengelbart of sri gave a stunning demonstration of his nls (engelbart,1986), which provided many of the capabilities of todayõs web browsers,albeit limited to a single computer. engelbartõs augment project wassupported by funding from nasa and arpa. engelbart was awardedthe association for computing machineryõs 1997 a. m. turing award forthis work. although it never became commercially successful, the mousedriven user interface inspired researchers at xerox parc, who were developing personal computing technology.widespread use of the web, which now accounts for the largest volume of internet traffic, was accelerated by the development in 1993 of themosaic graphical browser. this innovation, by marc andreessen at thensffunded national center for supercomputer applications, enabledthe use of hyperlinks to video, audio, and graphics, as well as text. moreimportant, it provided an effective interface that allowed users to pointandclick on a menu or fill in a blank to search for information.the development of the internet and the world wide web has had atremendous impact on the u.s. economy and society more broadly. byfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.development of the internet and the world wide web181january 1998, almost 30 million host computers were connected to theinternet (zakon, 1998), and more than 58 million users in the unitedstates and canada were estimated to be online (nielsen media research,1997). numerous companies now sell internet products worth billions ofdollars. cisco systems, a leader in network routing technology, for example, reported sales of $8.5 billion in 1998. netscape communicationscorporation, which commercialized the mosaic browser, had sales exceeding $530 million in 1997.5 microsoft corporation also entered themarket for web browsers and now competes headtohead with netscape.a multitude of other companies offer hardware and software for internetbased systems.the internet has also paved the way for a host of services. companieslike yahoo! and infoseek provide portals to the internet and have attractedconsiderable attention from wall street investors. other companies, likeamazon.com and barnes & noble, have established online stores. amazonhad online sales of almost $150 million for books in 1997.6 electroniccommerce, more broadly, is taking hold in many types of organizations,from pc manufacturers to retailers to travel agencies. although estimatesof the value of these services vary widely, they all reflect a growing sectorof the economy that is wholly dependent on the internet. internet retailing could reach $7 billion by the year 2000, and online sales of travelservices are expected to approach $8 billion around the turn of the century. forrester research estimates that businesses will buy and sell $327billion worth of goods over the internet by the year 2002 (blane, 1997).the web has been likened to the worldõs largest libraryñwith thebooks piled in the middle of the floor. search engines, which are programs that follow the webõs hypertext links and index the material theydiscover, have improved the organization somewhat but are difficult touse, frequently deluging the user with irrelevant information. althoughdevelopments in computing and networking over the last 40 years haverealized some of the potential described by visionaries such as licklider andengelbart, the field continues to offer many opportunities for innovation.lessons from historythe development of the internet demonstrates that federal supportfor research, applied at the right place and right time, can be extremelyeffective. darpaõs support gave visibility to the work of individualresearchers on packet switching and resulted in the development of thefirst largescale packetswitched network. continued support for experimentation led to the development of networking protocols and applications, such as email, that were used on the arpanet and, subsequently,the internet.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.182funding a revolutionby bringing together a diverse mix of researchers from different institutions, such federal programs helped the internet gain widespreadacceptance and established it as a dominant mode of internetworking.government programs such as arpanet and nsfnet created a largeenough base of users to make the internet more attractive in many applications than proprietary networking systems being offered by a numberof vendors. though a number of companies continue to sell proprietarysystems for wide area networking, some of which are based on packetswitched technology, these systems have not achieved the ubiquity of theinternet and are used mainly within private industry.research in packet switching evolved in unexpected directions andhad unanticipated consequences. it was originally pursued to make moreefficient use of limited computing capabilities and later seen as a means oflinking the research and education communities. the most notable result,however, was the internet, which has dramatically improved communication across society, changing the way people work, play, and shop.although darpa and the nsf were successful in creating an expansivepacketswitched network to facilitate communication among researchers,it took the invention of the web and its browsers to make the internetmore broadly accessible and useful to society.the widespread adoption of internet technology has created a number of new companies in industries that did not exist 20 years ago, andmost companies that did exist 20 years ago are incorporating internettechnology into their business operations. companies such as cisco systems, netscape communications, yahoo!, and amazon.com are built oninternet technologies and their applications and generate billions of dollars annually in combined sales revenues. electronic commerce is alsomaturing into an established means of conducting business.the complementary missions and operating styles of federal agenciesare important to the development and implementation of new technologies. whereas darpa supported early research on packet switching anddevelopment of the arpanet, it was not prepared to support an operational network, nor did it expand its network beyond darpasupportedresearch institutions. with its charter to support research and education,the nsf both supported an operational network and greatly expanded itsreach, effectively building the infrastructure for the internet.notes1.several other case studies of the internet have also been written in recentyears. in addition to the references cited in the text, see leiner et al. (1998) andsri international (1997).2.personal communication from robert w. taylor, former director of thefunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.development of the internet and the world wide web183information processing techniques office, defense advanced research projectsagency, august 1988.3.ibm and at&t did support some inhouse research on packet switching,but at the level of individual researchers. this work did not figure prominentlyin at&tõs plans for network deployment, nor did it receive significant attentionat ibm, though researchers in both organizations published important papers.4.ferreiro, mirna. 1996. òthe past and future history of the internet,óresearch paper for international 610. george mason university, fairfax, va.,november.5.sales figures in this paragraph derive from annual reports filed by thecompanies cited.6.sales revenues as reported in amazon.comõs 1997 annual report availableonline at <http://www.amazon.com/exec/obidos/subst/misc/investorrelations/1997annualreport.html/>.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.1848theoretical research: intangiblecornerstone of computer sciencethe theory and vocabulary of computing did not appear readymade.some important concepts, such as operating systems and compilers, hadto be invented de novo. others, such as recursion and invariance, can betraced to earlier work in mathematics. they became part of the evolvingcomputer science lexicon as they helped to stimulate or clarify the designand conceptualization of computing artifacts. many of these theoreticalconcepts from different sources have now become so embedded in computing and communications that they pervade the thinking of all computer scientists. most of these notions, only vaguely perceived in thecomputing community of 1960, have since become ingrained in the practice of computing professionals and even made their way into highschoolcurricula.although developments in computing theory are intangible, theoryunderlies many aspects of the construction, explanation, and understanding of computers, as this chapter demonstrates. for example, the conceptof state machines (described below) contributed to the development ofcompilers and communications protocols, insights into computationalcomplexity have been applied to improve the efficiency of industrial processes and information systems, formal verification methods have provided a tool for improving the reliability of programs, and advances innumber theory resulted in the development of new encryption methods.by serving as practical tools for use in reasoning and description, suchtheoretical notions have informed progress in all corners of computing.although most of these ideas have a basis in mathematics, they havefunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.theoretical research185become so firmly fixed in the instincts of computer scientists and engineers that they are likely to be used as naturally as a cashier uses arithmetic, with little attention to the origins of the process. in this way, theorypervades the daily practice of computer science and lends legitimacy tothe very identity of the field.this chapter reviews the history and the funding sources of four areasof theoretical computer science: state machines, computational complexity, program correctness, and cryptography. a final section summarizesthe lessons to be learned from history. although by no means a comprehensive overview of theoretical computer science, the discussion focuseson topics that are representative of the evolution in the field and can beencapsulated fairly, without favoring any particular thesis. state machines, computational complexity, and verification can be traced to thework of logicians in the late 1800s and early 1900s. cryptography datesback even further. the evolution of these subfields reflects the interplayof mathematics and computer science and the ways in which researchquestions changed as computer hardware placed practical constraints ontheoretical constructs. each of the four areas is now ubiquitous in thebasic conceptual toolkit of computer scientists as well as in undergraduate curricula and textbooks. each area also continues to evolve and poseadditional challenging questions.because it tracks the rise of ideas into the general consciousness of thecomputer science community, this case study is concerned less with issues of ultimate priority than with crystallizing events. in combination,the history of the four topics addressed in this chapter illustrates thecomplex fabric of a dynamic field. ideas flowed in all directions, geographically and organizationally. breakthroughs were achieved in manyplaces, including a variety of north american and european universitiesand a few industrial research laboratories. soviet theoreticians also madea number of important advances, although they are not emphasized inthis chapter. federal funding has been important, mostly from the national science foundation (nsf), which began supporting work in theoretical computer science shortly after its founding in 1950. the low cost oftheoretical research fit the nsf paradigm of singleinvestigator research.originally, such work was funded through the division of mathematicalsciences, but with the establishment of the office of computing activitiesin 1970, the nsf initiated a theoretical computer science program thatcontinues to this day. as thomas keenan, an nsf staffer, put it:computer science had achieved the title òcomputer scienceó withoutmuch science in it, [so we] decided that to be a science you had to havetheory, and not just theory itself as a separate program, but everythinghad to have a theoretical basis. and so, whenever we had a proposal...funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.186funding a revolutionwe encouraged, as much as we could, some kind of theoretical background for this proposal. (aspray et al., 1996)the nsf ended up funding the bulk of theoretical work in the field(by 1980 it had supported nearly 400 projects in computational theory),much of it with great success. although funding for theoretical computerscience has declined as a percentage of the nsf budget for computingresearch (it constituted 7 percent of the budget in 1996, down from 20percent in 1973), it has grown slightly in real dollars.1 missionorientedagencies, such as the national aeronautics and space administration orthe defense advanced research projects agency, tend not to fund theoretical work directly because of their emphasis on advancing computingtechnology, but some advances in theory were made as part of their largerresearch agendas.machine models: state machinesstate machine are ubiquitous models for describing and implementing various aspects of computing. the body of theory and implementation techniques that has grown up around state machines fosters the rapidand accurate construction and analysis of applications, including compilers, textsearch engines, operating systems, communication protocols, andgraphical user interfaces.the idea of a state machine is simple. a system (or subsystem) ischaracterized by a set of states (or conditions) that it may assume. thesystem receives a series of inputs that may cause the machine to producean output or enter a different state, depending on its current state. forexample, a simplified state diagram of a telephone activity might identifystates such as idle, dial tone, dialing, ringing, and talking, as well asevents that cause a shift from one state to another, such as lifting thehandset, touching a digit, answering, or hanging up (see figure 8.1). afinite state machine, such as a telephone, can be in only one of a limitednumber of states. more powerful state machine models admit a larger,theoretically infinite, number of states.the notion of the state machine as a model of all computing wasdescribed in alan turingõs celebrated paper on computability in 1936,before any generalpurpose computers had been built. turing, of cambridge university, proposed a model that comprised an infinitely longtape and a device that could read from or write to that tape (turing, 1936).he demonstrated that such a machine could serve as a generalpurposecomputer. in both academia and industry, related models were proposedand studied during the following two decades, resulting in a definitive1959 paper by michael rabin and dana scott of ibm corporation (rabinfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.theoretical research187figure 8.1 simplified state diagram for supervising a telephone line. states arerepresented by circles, inputs by labels on arrows. actions in placing a call leaddown the left side of the diagram; actions in receiving a call lead down the right.the state labeled òpleaseó at the bottom of the diagram announces òplease hangup.ófunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.188funding a revolutionand scott, 1959). whereas turing elucidated the undecidability2 inherentin the most general model, rabin and scott demonstrated the tractabilityof limited models. this work enabled the finite state machine to reachmaturity as a theoretical model.meanwhile, state machines and their equivalents were investigated inconnection with a variety of applications: neural networks (kleene, 1936;mcculloch and pitts, 1943); language (chomsky, 1956); communicationssystems (shannon, 1948), and digital circuitry (mealey, 1955; moore, 1956).a new level of practicality was demonstrated in a method of derivingefficient sequential circuits from state machines (huffman, 1954).when formal languagesña means of implementing state machines insoftwareñemerged as an academic research area in the 1960s, machinesof intermediate power (i.e., between finitestate and turing machines)became a focus of research. most notable was the òpushdown automata,óor state machine with an auxiliary memory stack, which is central to themechanical parsing performed to interpret sentences (usually programs)in highlevel languages. as researchers came to understand parsing, thework of mechanizing a programming language was formalized into aroutine task. in fact, not only parsing but also the building of parsers wasautomated, facilitating the first of many steps in converting compiler writing from a craft into a science. in this way, state machines were added tothe everyday toolkit of computing. at the same time, the use of statemachines to model communication systemsñas pioneered by claudeshannonñbecame commonplace among electrical and communicationsengineers. these two threads eventually coalesced in the study of communications protocols, which are now almost universally specified interms of cooperating state machines (as discussed below in the sectiondealing with correctness).the development of formal language theory was spurred by the construction of compilers and invention of programming languages. compilers came to the worldõs attention through the fortran project (backus,1979), but they could not become a discipline until the programminglanguage algol 60 was written. in the defining report, the syntax of algol60 was described in a novel formalism that became known as backusnaur form. the crisp, mechanical appearance of the formalism inspirededward irons, a graduate student at yale university, to try to build compilers directly from the formalism. thereafter, compiler automation became commonplace, as noted above. a task that once required a largeteam could now be assigned as homework. not only did parsers becomeeasy to make; they also became more reliable. doing the bulk of theconstruction automatically reduced the chance of bugs in the final product, which might be anything from a compiler for fortran to an interpreter for hypertext markup language (html).funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.theoretical research189state machines were developed by a mix of academic and industrialresearchers. the idea began as a theoretical construct but is now fullynaturalized throughout computer science as an organizing principle andspecification tool, independent of any analytical considerations. introductory texts describe certain programming patterns as state driven (garland, 1986) or state based (clancy and linn, 1995). an archetypal statebased program is a menudriven telephoneinquiry system. based ontheir familiarity with the paradigm, software engineers instinctively knowhow to build such programs. the ubiquity of the paradigm has led to thedevelopment of special tools for describing and building statebased systems, just as for parsers. work continues to devise machine models todescribe different types of systems.computational complexitythe theory of computability preceded the advent of generalpurposecomputers and can be traced to work by turing, kurt godel, alonzochurch, and others (davis, 1965). computability theory concentrated ona single question: do effective procedures exist for deciding mathematical questions? the requirements of computing have raised more detailedquestions about the intrinsic complexity of digital calculation, and thesequestions have raised new issues in mathematics.algorithms devised for manual computing often were characterizedby operation counts. for example, various schemes were proposed forcarrying out gaussian elimination or finite fourier transforms using suchcounts. this approach became more common with the advent of computers, particularly in connection with algorithms for sorting (friend, 1956).however, the inherent degree of difficulty of computing problems didnot become a discrete research topic until the 1960s. by 1970, the analysisof algorithms had become an established aspect of computer science, andknuth (1968) had published the first volume of a treatise on the subjectthat remains an indispensable reference today. over time, work on complexity theory has evolved just as practical considerations have evolved:from concerns regarding the time needed to complete a calculation, toconcerns about the space required to perform it, to issues such as thenumber of random bits needed to encrypt a message so that the codecannot be broken.in the early 1960s, hao wang3 noted distinctions of form that rendered some problems in mathematical logic decidable, whereas logicalproblems as a class are undecidable. there also emerged a robust classification of problems based on the machine capabilities required to attackthem. the classification was dramatically refined by juris hartmanis andrichard stearns at general electric company (ge), who showed thatfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.190funding a revolutionwithin a single machine model, a hierarchy of complexity classes exists,stratified by space or time requirements. hartmanis then left ge to foundthe computer science department at cornell university. with nsf support, hartmanis continued to study computational complexity, a fieldwidely supported by nsf.hartmanis and stearns developed a òspeedupó theorem, which saidessentially that the complexity hierarchy is unaffected by the underlyingspeed of computing. what distinguishes levels of the hierarchy is theway that solution time varies with problem sizeñand not the scale atwhich time is measured. thus, it is useful to talk of complexity in terms oforderofgrowth. to that end, the òbigohó notation, of the form o(n),was imported from algorithm analysis to computing (most notably byknuth [1976]), where it has taken on a life of its own. the notation is usedto describe the rate at which the time needed to generate a solution varieswith the size of the problem. problems in which there is a linear relationship between problem size and time to solution are o(n); those in whichthe time to solution varies as the square of the problem size are o(n2).4bigoh estimates soon pervaded algorithm courses and have since beenincluded in curricula for computer science in high schools.the quantitative approach to complexity pioneered by hartmanis andstearns spread rapidly in the academic community. applying this sharpened viewpoint to decision problems in logic, stephen cook at the university of toronto proposed the most celebrated theoretical notion incomputingñnp completeness. his òp versus npó conjecture is nowcounted among the important open problems of mathematics. it statesthat there is a sharp distinction between problems that can be computeddeterministically or nondeterministically in a tractable amount of time.5cookõs theory, and previous work by hartmanis and stearns, helps categorize problems as either deterministic or nondeterministic. the practical importance of cookõs work was vivified by richard karp, at the university of california at berkeley (ucberkeley), who demonstrated that acollection of nondeterministically tractable problems, including the famous travelingsalesman problem,6 are interchangeable (ònp completeó)in the sense that, if any one of them is deterministically tractable, then allof them are. a torrent of other npcomplete problems followed, unleashed by a seminal book by michael garey and david johnson at belllaboratories (garey and johnson, 1979).cookõs conjecture, if true, implies that there is no hope for preciselysolving any of these problems on a real computer without incurring anexponential time penalty. as a result, software designers, knowing thatparticular applications (e.g., integratedcircuit layout) are intrinsically difficult, can opt for ògood enoughó solutions, rather than seeking òbestpossibleó solutions. this leads to another question: how good a solutionfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.theoretical research191can be obtained for a given amount of effort? a more refined theoryabout approximate solutions to difficult problems has been developed(hochbaum, 1997), but, given that approximations are not widely used bycomputer scientists, this theory is not addressed in detail here. fortunately, good approximation methods do exist for some npcomplete problems. for example, huge òtraveling salesman routesó are routinely usedto minimize the travel of an automated drill over a circuit board in whichthousands of holes must be bored. these approximation methods aregood enough to guarantee that certain easy solutions will come very closeto (i.e., within 1 percent of) the best possible solution.verifying program correctnessalthough the earliest computer algorithms were written largely tosolve mathematical problems, only a tenuous and informal connectionexisted between computer programs and the mathematical ideas theywere intended to implement. the gap between programs and mathematics widened with the rise of system programming, which concentrated onthe mechanics of interacting with a computerõs environment rather thanon mathematics.the possibility of treating the behavior of programs as the subject of amathematical argument was advanced in a compelling way by robertfloyd at ucberkeley and later amplified by anthony hoare at thequeenõs university of belfast. the academic movement toward programverification was paralleled by a movement toward structured programming, christened by edsger dijkstra at technische universiteit eindhovenand vigorously promoted by harlan mills at ibm and many others. abasic tenet of the latter movement was that good program structure fosters the ability to reason about programs and thereby assure their correctness.7 moreover, analogous structuring was to inform the design processitself, leading to higher productivity as well as better products. structured programming became an obligatory slogan in programming textsand a mandated practice in many major software firms.in the full verification approach, a programõs specifications are described mathematically, and a formal proof that the program realizes thespecifications is carried through. to assure the validity of the(exhaustingly long) proof, it would be carried out or checked mechanically. to date, this approach has been too onerous to contemplate forroutine programming. nevertheless, advocates of structured programming promoted some of its key ideas, namely precondition, postcondition,and invariant (see box 8.1). these terms have found their way into everycomputer science curriculum, even at the high school level. whether orfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.192funding a revolutionnot logic is overtly asserted in code written by everyday programmers,these ideas inform their work.the structured programming perspective led to a more advanceddiscipline, promulgated by david gries at cornell university and edsgerdijkstra at eindhoven, which is beginning to enter curricula. in this approach, programs are derived from specifications by algebraic calculation. in the most advanced manifestation, formulated by eric hehner,programming is identified with mathematical logic. although it remainsto be seen whether this degree of mathematicization will eventually bebox 8.1the formal verification processin formal verification, computer programs become objects of mathematical study.a program is seen as affecting the state of the data with which it interacts. thepurpose of the program is to transform a state with known properties (the precondition) into a state with initially unknown, but desired properties (the postcondition). aprogram is composed of elementary operations, such as adding or comparing quantities. the transforming effect of each elementary operation is known. verificationconsists of proving, by logical deduction, that the sequence of program steps startingfrom the precondition must inexorably lead to the desired postcondition.when programs involve many repetitions of the same elementary steps, appliedto many different data elements or many transformational stages starting from someinitial data, verification involves showing once and for all that, no matter what thedata are or how many steps it takes, a program eventually will achieve the postcondition. such an argument takes the form of a mathematical induction, which assertsthat the state after each repetition is a suitable starting state for the next repetition.the assertion that the state remains suitable from repetition to repetition is called anòinvariantó assertion.an invariant assertion is not enough, by itself, to assure a solution. to rule out thepossibility of a program running forever without giving an answer, one must alsoshow that the postcondition will eventually be reached. this can be done by showing that each repetition makes a definite increment of progress toward the postcondition, and that only a finite number of such increments are possible.although notionally straightforward, the formal verification of everyday programsposes a daunting challenge. familiar programs repeat thousands of elementary stepsmillions of times. moreover, it is a forbidding task to define precise preconditionsand postconditions for a program (e.g., spreadsheet or word processor) with an informal manual running into the hundreds of pages. to carry mathematical argumentsthrough on this scale requires automation in the form of verification tools. to date,such tools can handle only problems with short descriptionsña few dozen pages, atmost. nevertheless, it is possible for these few pages to describe complex or subtlebehavior. in these cases, verification tools come in handy.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.theoretical research193come common practice, the history of engineering analysis suggests thatthis outcome is likely.in one area, the design of distributed systems, mathematicization isspreading in the field perhaps faster than in the classroom. the initialimpetus was westõs validation of a proposed international standard protocol. the subject quickly matured, both in practice (holzmann, 1991)and in theory (vardi and wolper, 1986). by now, engineers have harnessed a plethora of algebras (e.g., temporal logic, process algebra) inpractical tools for analyzing protocols used in applications ranging fromhardware buses to internet communications.it is particularly difficult to foresee the effects of abnormal events onthe behavior of communications applications. loss or garbling of messages between computers, or conflicts between concurrent events, such astwo travel agents booking the same airline seat, can cause inconvenienceor even catastrophe, as noted by neumann (1995). these reallife difficulties have encouraged research in protocol analysis, which makes it possible to predict behavior under a full range of conditions and events, notjust a few simple scenarios. a body of theory and practice has emerged inthe past decade to make automatic analysis of protocols a practical reality.cryptographycryptography is now more important than ever. although the military has a long history of supporting research on encryption techniques tomaintain the security of data transmissions, it is only recently that cryptography has come into widespread use in business and personal applications. it is an increasingly important component of systems that secureonline business transactions or maintain the privacy of personal communications.8 cryptography is a field in which theoretical work has clearimplications for practice, and vice versa. the field has also been controversial, in that federal agencies have sometimes opposed, and at othertimes supported, publicly accessible research. here again, the nsf supported work for which no funding could be obtained from other agencies.the scientific study of cryptography matured in conjunction withinformation theory, in which coding and decoding are central concerns,albeit typically in connection with compression and robust transmissionof data as opposed to security or privacy concerns. although claudeshannonõs seminal treatment of cryptography (shannon, 1949) followedhis founding paper on information theory, it was actually written earlierunder conditions of wartime security. undoubtedly, shannonõs involvement with cryptography on government projects helped shape his thinking about information theory.through the 1970s, research in cryptography was pursued mainlyfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.194funding a revolutionunder the aegis of government agencies. although impressive accomplishments, such as great britainõs ultra codebreaking enterprise inworld war ii, were known by reputation, the methods were largely keptsecret. the national security agency (nsa) was for many years theleader in cryptographic work, but few of the results were published orfound their way into the civilian community. however, an independentmovement of cryptographic discovery developed, driven by the availability and needs of computing. ready access to computing power madecryptographic experimentation feasible, just as opportunities for remoteintrusion made it necessary and the mystery surrounding the field madeit intriguing.in 1977, the data encryption standard (des) developed at ibm foruse in the private sector received federal endorsement (national bureauof standards, 1977). the mechanism of des was disclosed, although apivotal aspect of its scientific justification remained classified. speculation about the strength of the system spurred research just as effectivelyas if a formal request for proposals had been issued.on the heels of des came the novel proposal for publickey cryptography by whitfield diffie and martin hellman at stanford university,and, independently, by r.c. merkle. hellman had been interested incryptography since the early 1970s and eventually convinced the nsf tosupport it (diffie and hellman, 1976). the notion of publickey cryptography was soon made fully practical by ronald rivest, adi shamir, andleonard adleman at the massachusetts institute of technology, who,with funding from the nsf and office of naval research (onr), deviseda publickey method based on number theory (rivest et al., 1978) (see box8.2). their method won instant acclaim and catapulted number theoryinto the realm of applied mathematics. each of the cited works has become bedrock for the practice and study of computer security. the nsfsupport was critical, as it allowed the ideas to be developed and published in the open, despite pressure from the nsa to keep them secret.the potential entanglement with international traffic in arms regulations is always apparent in the cryptography arena (computer scienceand telecommunications board, 1996). official and semiofficial attemptsto suppress publication have often drawn extra notice to the field (diffie,1996). this unsolicited attention has evoked a notable level of independence among investigators. most, however, have achieved a satisfactorymodus vivendi with the concerned agencies, as evidenced by the seminalpapers cited in this chapter that report on important cryptographic research performed under unclassified grants.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.theoretical research195box 8.2rivestshamiradleman cryptographybefore publickey cryptography was invented, cipher systems required two communicating parties to agree in advance on a secret key to be used in encrypting anddecrypting messages between them. to assure privacy for every communication, aseparate arrangement had to be made between each pair who might one day wish tocommunicate. parties who did not know each other in advance of the need tocommunicate were out of luck.by contrast, publickey cryptography requires merely that an individual announcea single (public) encryption key that can be used by everyone who wishes to sendthat individual a message. to decode any of the messages, this individual uses adifferent but mathematically related key, which is private. the security of the systemdepends on its being prohibitively difficult for anyone to discover the private key ifonly the public key is known. the practicality of the system depends on there beinga feasible way to produce pairs of public and private keys.the first proposals for publickey cryptography appealed to complexity theory forproblems that are difficult to solve. the practical method proposed by rivest, shamir,and adleman (rsa) depends on a problem believed to be of this type from numbertheory. the problem is factoring. the recipient chooses two huge prime numbersand announces only their product. the product is used in the encryption process,whereas decryption requires knowledge of the primes. to break the code, one mustfactor the product, a task that can be made arbitrarily hard by picking large enoughnumbers; hundreddigit primes are enough to seriously challenge a stable of supercomputers.the rsa method nicely illustrates how theory and practice evolved together.complexity theory was motivated by computation and the desire to understandwhether the difficulty of some problems was inherent or only a symptom of inadequate understanding. when it became clear that inherently difficult problems exist,the stage was set for publickey cryptography. this was not sufficient to advance thestate of practice, however. theory also came to the fore in suggesting problems withstructures that could be adapted to cryptography.it took the combination of computers, complexity theory, and number theory tomake publickey cryptography a reality, or even conceivable. once the idea wasproposed, remarkable advances in practical technique followed quickly. so didadvances in number theory and logic, spurred by cryptographic needs. the generalarea of protection of communication now covers a range of topics, including codebreaking (even the ògood guysó must try to break codes to confirm security); authentication (i.e., preventing imposters in communications); checks and balances (i.e., forestalling rogue actions, such as embezzlement or missile launches, by nominally trustedpeople); and protection of intellectual property (e.g., by making information theftproofor providing evidence that knowledge exists without revealing the knowledge).funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.196funding a revolutionlessons from historyresearch in theoretical computer science has been supported by boththe federal government and industry. almost without exception in thecases discussed, contributions from u.s. academia acknowledge the support of federal agencies, most notably the nsf and onr. nevertheless,many advances in theoretical computer science have emerged from majorindustrial research laboratories, such as ibm, at&t (bell laboratories),and ge. this is partly because some of the areas examined developedbefore the nsf was established, but also because some large corporatelaboratories have provided an environment that allows industry researchers to produce directly relevant results while also carrying on longterm,theoretical investigations in the background. shannon, for example, apparently worked on information theory for a decade before he told theworld about it.theoretical computer science has made important contributions tocomputing practice while, conversely, also being informed by that practice. work on the theory of oneway functions, for example, led to thedevelopment of publickey cryptography, and the development of complexity theory, such as cookõs conjecture, sparked efforts to improvemethods for approximating solutions to nondeterministically tractableproblems. similarly, the theoretical work in complexity and programcorrectness (or verification) has been redirected by the advancing needsof computing systems.academia has played a key role in propagating computing theory.by teaching and writing textbooks, academic researchers naturally influenced the subjects taught, especially during the formative years of computer science departments. however, some important synthesizing bookshave come from industrial research laboratories, where management hasseen fit to support such writing to enhance prestige, attract candidates,and foster the competence on which research depends.foreign nations have contributed to theoretical computer science.although the united states has been the center of systemsrelated research, a considerable share of the mathematical underpinnings for computer science can be attributed to british, canadian, and european academics. (the wider practical implementation of this work in the unitedstates may be explained by a historically greater availability of computers.) the major foreign contributions examined in this case were all supported by governments; none came from foreign industry.personal and personnel dynamics have also played important roles.several of the papers cited in this chapter deal with work that originatedduring the authorsõ visiting or shortterm appointments, when they werefree of the ancillary burdens associated with permanent positions. researchfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.theoretical research197ers in theoretical computer science have often migrated between industryand academia, and researchers in these sectors have often collaborated.such mixing and travel helped infuse computing theory with an understanding of the practical problems faced by computer designers andhelped establish a community of researchers with a common vocabulary.notes1.between 1973 and 1996, nsf funding for theoretical computer sciencegrew from less than $2 million to almost $7 million dollars. in 1996 dollars (i.e.,taking inflation into account), the nsf spent the equivalent of $6.1 million ontheory in 1973, versus $6.9 million in 1996. thus, the real increase in fundingover 23 years was just 13 percent, or about 0.5 percent a year, on average.2.a class of mathematical problems, usually with yes or no answers, iscalled òdecidableó if there is an algorithm that will produce a definite answer forevery problem in the class. otherwise, the class of problems is undecidable.turing demonstrated that no algorithm exists for answering the question ofwhether a turingmachine calculation will terminate. the question might beanswered for many particular machines, even mechanically. but no algorithmwill answer it for all machines: there must be some machine about which thealgorithm will never come to a conclusion.3.hao wang, who began his work at oxford university and later moved tobell laboratories and ibm, elucidated the sources of undecidability.4.as an example of bigoh notation, the number of identical fixedsize solidobjects that can be fit into a cube with sides of length l is o(l3), regardless of thesize or shape of the objects. this means that for l arbitrarily large, at most l3objects will fit (scaled by some constant).5. a class of problems is said to be òtractableó when the time necessary tosolve problems of the class varies at most as a power of problem size.6.this problem involves figuring out the most efficient route for a salesperson to follow in visiting a list of cities. each additional city added to the listcreates a whole series of additional possible routes that must be evaluated toidentify the shortest one. thus, the complexity of the problem grows much fasterthan does the list of cities.7.correctness is defined as the property of being consistent with a specification.8.for a more complete discussion of cryptographyõs growing importance,see computer science and telecommunications board (1996).funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.1989developments in artificial intelligenceartificial intelligence (ai) has been one of the most controversial domains of inquiry in computer science since it was first proposed in the1950s. defined as the part of computer science concerned with designingsystems that exhibit the characteristics associated with human intelligenceñunderstanding language, learning, reasoning, solving problems,and so on (barr and feigenbaum, 1981)ñthe field has attracted researchers because of its ambitious goals and enormous underlying intellectualchallenges. the field has been controversial because of its social, ethical,and philosophical implications. such controversy has affected the funding environment for ai and the objectives of many research programs.ai research is conducted by a range of scientists and technologistswith varying perspectives, interests, and motivations. scientists tend tobe interested in understanding the underlying basis of intelligence andcognition, some with an emphasis on unraveling the mysteries of humanthought and others examining intelligence more broadly. engineeringoriented researchers, by contrast, are interested in building systems thatbehave intelligently. some attempt to build systems using techniquesanalogous to those used by humans, whereas others apply a range oftechniques adopted from fields such as information theory, electrical engineering, statistics, and pattern recognition. those in the latter categoryoften do not necessarily consider themselves ai researchers, but ratherfall into a broader category of researchers interested in machine intelligence.the concept of ai originated in the private sector, but the growth offunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.developments in artificial intelligence199the field, both intellectually and in the size of the research community,has depended largely on public investments. public monies have beeninvested in a range of ai programs, from fundamental, longterm research into cognition to shorterterm efforts to develop operational systems. most of the federal support has come from the defense advancedresearch projects agency (darpa, known during certain periods asarpa) and other units of the department of defense (dod). other funding agencies have included the national institutes of health, nationalscience foundation, and national aeronautics and space administration(nasa), which have pursued ai applications of particular relevance totheir missionsñhealth care, scientific research, and space exploration.this chapter highlights key trends in the development of the field ofai and the important role of federal investments. the sections of thischapter, presented in roughly chronological order, cover the launching ofthe ai field, the governmentõs initial participation, the pivotal role playedby darpa, the success of speech recognition research, the shift frombasic to applied research, and ai in the 1990s. the final section summarizes the lessons to be learned from history. this case study is basedlargely on published accounts, the scientific and technical literature, reports by the major ai research centers, and interviews conducted withseveral leaders of ai research centers. (little information was drawn fromthe records of the participants in the field, funding agencies, editors andpublishers, and other primary sources most valued by professional historians.)1the private sector launches the fieldthe origins of ai research are intimately linked with two landmarkpapers on chess playing by machine.2 they were written in 1950 byclaude e. shannon, a mathematician at bell laboratories who is widelyacknowledged as a principal creator of information theory. in the late1930s, while still a graduate student, he developed a method for symbolicanalysis of switching systems and networks (shannon, 1938), which provided scientists and engineers with muchimproved analytical and conceptual tools. after working at bell labs for half a decade, shannonpublished a paper on information theory (shannon, 1948). shortly thereafter, he published two articles outlining the construction or programming of a computer for playing chess (shannon, 1950a,b).shannonõs work inspired a young mathematician, john mccarthy,who, while a research instructor in mathematics at princeton university,joined shannon in 1952 in organizing a conference on automata studies,largely to promote symbolic modeling and work on the theory of machineintelligence.3 a year later, shannon arranged for mccarthy and anotherfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.200funding a revolutionfuture pioneer in ai, marvin minsky, then a graduate student in mathematics at princeton and a participant in the 1952 conference, to workwith him at bell laboratories during 1953.4by 1955, mccarthy believed that the theory of machine intelligencewas sufficiently advanced, and that related work involved such a criticalmass of researchers, that rapid progress could be promoted by a concentrated summer seminar at dartmouth university, where he was then anassistant professor of mathematics. he approached the rockefellerfoundationõs warren weaver, also a mathematician and a promoter ofcuttingedge science, as well as shannonõs collaborator on informationtheory. weaver and his colleague robert s. morison, director for biological and medical research, were initially skeptical (weaver, 1955). morisonpushed mccarthy and shannon to widen the range of participants andmade other suggestions. mccarthy and shannon responded with a widened proposal that heeded much of morisonõs advice. they brought inminsky and a wellknown industrial researcher, nathaniel rochester5 ofibm, as coprincipal investigators for the proposal, submitted in september 1955.6in the proposal, the four researchers declared that the summer studywas òto proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so preciselydescribed that a machine can be made to simulate it.ó they sought tobring a number of u.s. scholars to dartmouth to create a research agendafor ai and begin actual work on it. in spite of morisonõs skepticism, therockefeller foundation agreed to fund this summer project with a grantof $7,500 (rhind, 1955), primarily to cover summer salaries and expensesof the academic participants. researchers from industry would be compensated by their respective firms.although most accounts of ai history focus on mccarthyõs entrepreneurship, the role of shannonñan intellectual leader from industryñisalso critical. without his participation, mccarthy would not have commanded the attention he received from the rockefeller foundation. shannon also had considerable influence on marvin minsky. the title ofminskyõs 1954 doctoral dissertation was òneural nets and the brainmodel problem.óthe role of ibm is similarly important. nathan rochester was a strongsupporter of the ai concept, and he and his ibm colleagues who attendedthe 1956 dartmouth workshop contributed to the early research in thefield. after the workshop ibm welcomed mccarthy to its research laboratories, in large part because of ibmõs previous work in ai and becauseòibm looked like a good bet to pursue artificial intelligence research vigorouslyó in the future.7 rochester was a visiting professor at the massachusetts institute of technology (mit) during 19581959, and he unquesfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.developments in artificial intelligence201tionably helped mccarthy with the development of lisp, an importantlistprocessing language (see box 9.1).8 rochester also apparently lent hissupport to the creation in 1958 of the mit artificial intelligence project(rochester and gelertner, 1958).9 yet, in spite of the early activity ofrochester and other ibm researchers, the corporationõs interest in aicooled. although work continued on computerbased checkers and chess,an internal report prepared about 1960 took a strong position againstbroad support for ai.thus, the activities surrounding the dartmouth workshop were, atthe outset, linked with the cuttingedge research at a leading private research laboratory (at&t bell laboratories) and a rapidly emerging industrial giant (ibm). researchers at bell laboratories and ibm nurturedthe earliest work in ai and gave young academic researchers likemccarthy and minsky credibility that might otherwise have been lacking. moreover, the dartmouth summer research project in ai was fundedby private philanthropy and by industry, not by government. the sameis true for much of the research that led up to the summer project.the government steps inthe federal governmentõs initial involvement in ai research wasmanifested in the work of herbert simon and allen newell, who attended the 1956 dartmouth workshop to report on òcomplex informationprocessing.ó trained in political science and economics at the universityof chicago, simon had moved to carnegie institute of technology in 1946and was instrumental in the founding and early research of the graduateschool of industrial administration (gsia). funded heavily by the fordfoundation and the office of naval research (onr), and the air force,gsia was the pioneer in bringing quantitative behavioral social sciencesresearch (including operations research) into graduate management education.10 because of his innovative work in human decision making,simon became, in march 1951, a consultant to the rand corporation, thepioneering think tank established by the air force shortly after worldwar ii.11at rand, where he spent several summers carrying out collaborative research, simon encountered newell, a mathematician who helped toconceive and develop the systems research laboratory, which was spunout of rand as the system development corporation in 1957. in 1955,simon and newell began a long collaboration on the simulation of humanthought, which by the summer of 1956 had resulted in their fundamentalwork (with rand computer programmer j.c. shaw) on the logic theorist, a computer program capable of proving theorems found in thefunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.202funding a revolutionbox 9.1the development and influence of lisplisp has been an important programming language in ai research, and its historydemonstrates the more general benefits resulting from the efforts of ai researchers totackle exceptionally difficult problems. as with other developments in ai, lisp demonstrates how, in addressing problems in the representation and computational treatment of knowledge, ai researchers often stretched the limits of computing technology and were forced to invent new techniques that found their way into mainstreamapplication.early ai researchers interested in logical reasoning and problem solving neededtools to represent logical formulas, proofs, plans, and computations on such objects.existing programming techniques were very awkward for this purpose, inspiring thedevelopment of specialized programming languages, such as listprocessing languages. list structures provide a simple and universal encoding of the expressionsthat arise in symbolic logic, formal language theory, and their applications to theformalization of reasoning and natural language understanding. among early listprocessing languages (the name is based on that phrase), lisp was the most effectivetool for representing both symbolic expressions and manipulations of them. it wasalso an object of study in itself. lisp can readily operate on other lisp programs thatare represented as list structures, and it thus can be used for symbolic reasoning onprograms. lisp is also notable because it is based on ideas of mathematical logicthat are of great importance in the study of computability and formal systems (seechapter 8).lisp was successful in niche commercial applications. for instance, lisp is thescripting language in autocad, the widely used computeraided design (cad) program from autodesk. but it had much broader implications for other languages.effective implementation of lisp demanded some form of automatic memory management. thus, lisp had critical influence far beyond ai in the theory and design ofprogramming languages, including all functional programming languages as well asobjectoriented languages such as simula67, smalltalk, and, most notably, java.this is not just a happy accident, but rather a consequence of the conceptual breakthroughs arising from the effort to develop computational models of reasoning. other examples include framebased knowledge representations, which strongly influenced the development of objectoriented programming and object databases;rulebased and logicprogramming language ideas, which found practical applications in expert systems, databases, and optimization techniques; and cad representations for reasoning with uncertainty, which have found their way into manufacturing control, medical and equipment diagnosis, and humancomputer interfaces.principia of bertrand russell and alfred north whitehead (newell andsimon, 1956).12this program is regarded by many as the first successful ai program,and the language it used, ipl2, is recognized as the first significant listprocessing language. as programmed by simon, newell, and shaw, acomputer simulated human intelligence, solving a problem in logic infunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.developments in artificial intelligence203much the same way as would a skilled logician. in this sense, the machinedemonstrated artificial intelligence. the project was funded almost entirely by the air force through project rand, and much of the computerprogramming was done at rand on an air forcefunded computer (thejohnniac, named after rand consultant john von neumann, the creatorof the basic architecture for digital electronic computers).13newellõs collaboration with simon took him to carnegie tech, where,in 1957, he completed the institutionõs first doctoral dissertation in ai,òinformation processing: a new technique for the behavioral sciences.óits thrust was clearly driven by the agenda laid out by the architects ofgsia. as newell later stressed, his work with simon (and that of simonõsseveral other ai students at gsia) reflected the larger agenda of gsia,even though most of this work was funded by the air force and onruntil the early 1960s. all of this work concentrated on the formal modeling of decision making and problem solving.simon and newell developed another wellknown ai program as asequel to logic theoristñthe general problem solver (gps), first run in1957 and developed further in subsequent years. their work on gps, likethat on logic theorist, was characterized by its use of heuristics (i.e.,efficient but fallible rules of thumb) as the means to simulate humancognitive processes (newell et al., 1959). the gps was capable of solvingan array of problems that challenge human intelligence (an importantaccomplishment in and of itself), but, most significantly, it solved theseproblems by simulating the way a human being would solve them. theseoverall research efforts at gsia, including the doctoral research of simonõsstudentsñall funded principally by air force and onr moneyñremained modest in scale compared to those at carnegie tech after 1962.14also modest were the efforts at mit, where mccarthy and minskyestablished the artificial intelligence project in september 1957. thiseffort was funded principally through a wordofmouth agreement withjerome wiesner, then director of mitõs militaryfunded research laboratory in electronics (rle). in exchange for òa room, two programmers, asecretary and a keypunch [machine],ó the two assistant professors ofmathematics agreed, according to mccarthy, to òundertake the supervision of some of the six mathematics graduate students that rle had undertaken to support.ó15the research efforts at carnegie tech (which became carnegie mellonuniversity [cmu] in 1967), rand, and mit, although limited, yieldedoutstanding results in a short time. simon and newell showed that computers could demonstrate humanlike behavior in certain welldefinedtasks.16 substantial progress was also made by mccarthy, with his pioneering development of lisp, and minsky, who formalized heuristicprocesses and other means of reasoning, including pattern recognition.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.204funding a revolutionpreviously, computers had been used principally to crunch numbers, andthe tools for such tasks were primitive. the ai researchers found ways torepresent logical formulas, carry out proofs, conduct plans, and manipulate such objects. buoyed by their successes, researchers at both institutions projected bold visionsñwhich, as the research was communicatedto the public, became magnified into excessive claimsñabout the futureof the new field of ai and what computers might ultimately achieve.17darpaõs pivotal rolethe establishment in 1962 of arpaõs information processing techniques office (ipto) radically changed the scale of research in ai, propelling it from a collection of small projects into a largescale, highprofiledomain. from the 1960s through the 1990s, darpa provided the bulk ofthe nationõs support for ai research and thus helped to legitimize ai as animportant field of inquiry and influence the scope of related research.over time, the nature of darpaõs support changed radicallyñfrom anemphasis on fundamental research at a limited number of centers of excellence to more broadbased support for applied research tied to militaryapplicationsñboth reflecting and motivating changes in the field of aiitself.the early academic centers were mit and carnegie tech. followingjohn mccarthyõs move to stanford in 1963 to create the stanford artificialintelligence laboratory (sail), ipto worked a similar transformation ofai research at stanford by making it the third center of excellence in ai.indeed, the ipto increased stanfordõs allocation in 1965, allowing it toupgrade its computing capabilities and to launch five major team projectsin ai research. commenting in 1984 about how airelated research atcarnegie tech migrated out of gsia into what became an autonomousdepartment (and later a college) of cmu, newell (1984) captured thetransformation wrought by ipto:. . . the darpa support of ai and computer science is a remarkablestory of the nurturing of a new scientific field. not only with mit,stanford and cmu, which are now seen as the main darpasupporteduniversity computerscience research environments, but with other universities as well . . . darpa began to build excellence in informationprocessing in whatever fashion we thought best. . . . the darpa effort,or anything similar, had not been in our wildest imaginings. . . .another center of excellenceñthe stanford research instituteõs (sriõs)artificial intelligence centerñemerged a bit later (in 1966), with charlesrosen at the command. it focused on developing òautomatons capable ofgathering, processing, and transmitting information in a hostile environfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.developments in artificial intelligence205mentó (nilsson, 1984). soon, sri committed itself to the development ofan aidriven robot, shakey, as a means to achieve its objective. shakeyõsdevelopment necessitated extensive basic research in several domains,including planning, naturallanguage processing, and machine vision.sriõs achievements in these areas (e.g., the strips planning system andwork in machine vision) have endured, but changes in the funderõs expectations for this research exposed sriõs ai program to substantial criticismin spite of these real achievements.under j.c.r. licklider, ivan sutherland, and robert taylor, darpacontinued to invest in ai research at cmu, mit, stanford, and sri and, toa lesser extent, other institutions.18 licklider (1964) asserted that ai wascentral to darpaõs mission because it was a key to the development ofadvanced commandandcontrol systems. artificial intelligence was abroad category for licklider (and his immediate successors), who òsupported work in problem solving, natural language processing, patternrecognition, heuristic programming, automatic theorem proving, graphics, and intelligent automata. various problems relating to humanmachine communicationñtablets, graphic systems, handeye coordinationñwere all pursued with ipto supportó (norberg and oõneill, 1996).these categories were sufficiently broad that researchers likemccarthy, minsky, and newell could view their institutionsõ research,during the first 10 to 15 years of darpaõs ai funding, as essentiallyunfettered by immediate applications. moreover, as work in one problemdomain spilled over into others easily and naturally, researchers couldattack problems from multiple perspectives. thus, ai was ideally suitedto graduate education, and enrollments at each of the ai centers grewrapidly during the first decade of darpa funding.darpaõs early support launched a golden age of ai research andrapidly advanced the emergence of a formal discipline. much of darpaõsfunding for ai was contained in larger program initiatives. lickliderconsidered ai a part of his general charter of computers, command, andcontrol. project mac (see box 4.2), a project on timeshared computingat mit, allocated roughly onethird of its $2.3 million annual budget to airesearch, with few specific objectives.success in speech recognitionthe history of speech recognition systems illustrates several themescommon to ai research more generally: the long time periods betweenthe initial research and development of successful products, and the interactions between ai researchers and the broader community of researchersin machine intelligence. many capabilities of todayõs speechrecognitionsystems derive from the early work of statisticians, electrical engineers,funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.206funding a revolutioninformation theorists, and patternrecognition researchers. another keytheme is the complementary nature of government and industry funding.industry supported work in speech recognition at least as far back as the1950s, when researchers at bell laboratories worked on systems for recognizing individual spoken digits òzeroó through ònine.ó research in thearea was boosted tremendously by darpa in the 1970s.darpa established the speech understanding research (sur) program to develop a computer system that could understand continuousspeech. lawrence roberts initiated this project in 1971 while he wasdirector of ipto, against the advice of a national academy of sciencescommittee.19 roberts wanted a system that could handle a vocabulary of10,000 english words spoken by anyone. his advisory board, which included allen newell and j.c.r. licklider, issued a report calling for anobjective of 1,000 words spoken in a quiet room by a limited number ofpeople, using a restricted subject vocabulary (newell et al., 1971).roberts committed $3 million per year for 5 years, with the intentionof pursuing a 5year followon project. major sur project groups wereestablished at cmu, sri, mitõs lincoln laboratory, systems development corporation (sdc), and bolt, beranek, and newman (bbn). smallercontracts were awarded to a few other institutions. five years later, surproducts were demonstrated. cmu researchers demonstrated two systems, harpy and hearsayi, and bbn developed hear what i mean(hwim). the system developed cooperatively by sri and sdc was nevertested (green, 1988). the system that came the closest to satisfying theoriginal project goalsñand may have exceeded the benchmarksñwasharpy, but controversy arose within darpa and the ai communityabout the way the tests were handled. full details regarding the testing ofsystem performance had not been worked out at the outset of the surprogram.20 as a result, some researchersñincluding darpa researchmanagersñbelieved that the sur program had failed to meet its objectives. darpa terminated the program without funding the followon.21nevertheless, industry groups, including those at ibm, continued to invest in this research area and made important contributions to the development of continuous speech recognition methods.22darpa began funding speech recognition research on a large scaleagain in 1984 as part of the strategic computing program (discussed laterin this chapter) and continued funding research in this area well into thelate 1990s. many of the same institutions that had been part of the surprogram, including cmu, bbn, sri, and mit, participated in the newinitiatives. firms such as ibm and dragon systems also participated. asa result of the controversy over sur testing, evaluation methods andcriteria for these programs were carefully prescribed though mutualagreements between darpa managers and the funded researchers. somefunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.developments in artificial intelligence207researchers have hailed this development and praised darpaõs role inbenchmarking speechrecognition technology, not only for research purposes but also for the commercial market.by holding annual system evaluations on carefully designed tasksand test materials, darpa and the national bureau of standards (laterthe national institute of standards and technology) led the standardsdefinition process, drawing the participation of not only government contractors but also industry and university groups from around the world,such as at&t, cambridge university (of the united kingdom), and limsi(of france). the overall effect was the rapid adoption of the most successful techniques by every participant and quick migration of those techniques into products and services. although it resulted in quick diffusionof successful techniques, this approach may also have narrowed the scopeof approaches taken. critics have seen this as symptomatic of a profoundchange in darpaõs philosophy that has reduced the emphasis on basicresearch.darpaõs funding of research on understanding speech has been extremely important. first, it pushed the research frontiers of speech recognition and ai more generally. hearsayii is particularly notable for theway it parsed information into independent knowledge sources, which inturn interacted with each other through a common database that cmuresearchers labeled a òblackboardó (englemore et al., 1988). this blackboard method of information processing proved to be a significant advance in ai. moreover, although early speechrecognition researchersappeared overly ambitious in incorporating syntax and semantics intotheir systems, others have recently begun to adopt this approach to improve statistically based speechrecognition technology.perhaps more important, the results of this research have been incorporated into the products of established companies, such as ibm andbbn, as well as startups such as nuance communications (an sri spinoff)and dragon systems. microsoft corporation, too, is incorporating speechrecognition technology into its operating system (darpa, 1997; mcclain,1998). the leading commercial speechrecognition program on the market today, the dragon systems software, traces its roots directly back tothe work done at cmu between 1971 and 1975 as part of sur (see box9.2). the dragon program developed in cmuõs sur project (the predecessor of the harpy program) pioneered the use of techniques borrowed from mathematics and statistics (hidden markov models) to recognize continuous speech (baker, 1975). according to some scholars, theadoption of hidden markov models by cmuõs research team owes muchto activities outside the ai field, such as research by engineers and statisticians with an interest in machine intelligence.23other examples of commercial success abound. charles schwab andfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.208funding a revolutionbox 9.2dragon systems profits from success in speech recognitiondragon systems was founded in 1982 by james and janet baker to commercialize speech recognition technology. as graduate students at rockefeller university in1970, they became interested in speech recognition while observing waveforms ofspeech on an oscilloscope. at the time, systems were in place for recognizing a fewhundred words of discrete speech, provided the system was trained on the speakerand the speaker paused between words. there were not yet techniques that couldsort through naturally spoken sentences. james baker saw the waveformsñand theproblem of natural speech recognitionñas an interesting patternrecognition problem.rockefeller had neither experts in speech understanding nor suitable computingpower, and so the bakers moved to carnegie mellon university (cmu), a primecontractor for darpaõs speech understanding research program. there they beganto work on natural speech recognition capabilities. their approach differed fromthat of other speech researchers, most of whom were attempting to recognize spokenlanguage by providing contextual information, such as the speakerõs identity, whatthe speaker knew, and what the speaker might be trying to say, in addition to rules ofenglish. the bakersõ approach was based purely on statistical relationships, such asthe probability that any two or three words would appear one after another in spokenenglish. they created a phonetic dictionary with the sounds of different word groupsand then set to work on an algorithm to decipher a string of spoken words based onphonetic sound matches and the probability that someone would speak the words inthat order. their approach soon began outperforming competing systems.after receiving their doctorates from cmu in 1975, the bakers joined ibmõs t.j.watson research center, one of the only organizations at the time working on largevocabulary, continuous speech recognition. the bakers developed a program thatcould recognize speech from a 1,000word vocabulary, but it could not do so in realtime. running on an ibm system 370 computer, it took roughly an hour to decodea single spoken sentence. nevertheless, the bakers grew impatient with what theysaw as ibmõs reluctance to develop simpler systems that could be more rapidly put tocommercial use. they left in 1979 to join verbex voice systems, a subsidiary ofexxon enterprises that had built a system for collecting data over the telephone usingspoken digits. less than 3 years later, however, exxon exited the speech recognitionbusiness.with few alternatives, the bakers decided to start their own company, dragonsystems. the company survived its early years through a mix of custom projects,government research contracts, and new products that relied on the more maturediscrete speech recognition technology. in 1984, they provided apricot computer,a british company, with the first speech recognition capability for a personal computer (pc). it allowed users to open files and run programs using spoken commands.but apricot folded shortly thereafter. in 1986, dragon systems was awarded the firstof a series of contracts from darpa to advance largevocabulary, speakerindependent continuous speech recognition, and by 1988, dragon conducted the first publicdemonstration of a pcbased discrete speech recognition system, boasting an 8,000word vocabulary.in 1990, dragon demonstrated a 5,000word continuous speech system for pcsand introduced dragondictate 30k, the first largevocabulary, speechtotext systemfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.developments in artificial intelligence209company adopted darpa technology to develop its voicebroker system, which provides stock quotes over the telephone. the system canrecognize the names of 13,000 different securities as well as major regional u.s. accents. on the military side, darpa provided translingualcommunication devices for use in bosnia. these devices translated spoken english phrases into corresponding serbocroatian or russianphrases. the total market for these new personaluse voice recognitiontechnologies is expected to reach about $4 billion in 2001 (garfinkel, 1998).shift to applied research increases investmentalthough most founders of the ai field continued to pursue basicquestions of human and machine intelligence, some of their students andother secondgeneration researchers began to seek ways to use ai methfor generalpurpose dictation. it allowed control of a pc using voice commands onlyand found acceptance among the disabled. the system had limited appeal in thebroader marketplace because it required users to pause between words. other federalcontracts enabled dragon to improve its technology. in 1991, dragon received acontract from darpa for work on machineassisted translation systems, and in 1993,dragon received a federal technology reinvestment project award to develop, incollaboration with analog devices corporation, continuous speech recognition systems for desktop and handheld personal digital assistants (pdas). dragon demonstrated pda speech recognition in the apple newton messagepad 2000 in 1997.late in 1993, the bakers realized that improvements in desktop computers wouldsoon allow continuous voice recognition. they quickly began setting up a newdevelopment team to build such a product. to finance the needed expansion of itsengineering, marketing, and sales staff, dragon brokered a deal whereby seagatetechnologies bought 25 percent of dragonõs stock. by july 1997, dragon hadlaunched dragon naturallyspeaking, a continuous speech recognition program forgeneralpurpose use with a vocabulary of 23,000 words. the package won ravereviews and numerous awards. ibm quickly followed suit, offering its own continuous speech recognition program, viavoice, in august after a crash developmentprogram. by the end of the year, the two companies combined had sold more than75,000 copies of their software. other companies, such as microsoft corporationand lucent technologies, are expected to introduce products in the near future, andanalysts expect a $4 billion worldwide market by 2001.source: the primary source for this history is garfinkel (1998). a corporate historyis available on the companyõs web site at <http://www.dragonsys.com>.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.210funding a revolutionods and approaches to tackle realworld problems. their initiatives wereimportant, not only in their own right, but also because they were indicative of a gradual but significant change in the funding environment toward more applied realms of research. the development of expert systems, such as dendral at sail, provides but one example of this trend(see box 9.3).box 9.3pioneering expert systemsthe dendral project was initiated in 1965 by edward feigenbaum (one ofherbert simonõs doctoral students in ai); nobel prizewinning geneticist and biochemist joshua lederberg; and bruce buchanan, a recent recipient of a doctorate inphilosophy from michigan state university.1 dendral began as an effort to explore the mechanization of scientific reasoning and the formalization of scientificknowledge by working within a specific domain of science, organic chemistry.developed in part with an initial research grant from the national aeronautics andspace administration (in anticipation of landing unmanned spacecraft on other planets), but also picked up under darpa funding, dendral used a set of knowledgeor rulebased reasoning commands to deduce the likely molecular structure of organic chemical compounds from known chemical analyses and mass spectrometrydata. the program took almost 10 years to develop, combining the talents of chemists, geneticists, and computer scientists. in addition to rivaling the skill of expertorganic chemists in predicting the structures of molecules in certain classes of compounds, dendral proved to be fundamentally important in demonstrating howrulebased reasoning could be developed into powerful knowledge engineeringtools. its use resulted in a number of papers published in the chemistry literature.although it is no longer a topic of academic research, the most recent version of theinteractive structure generator, genoa, has been licensed by stanford university forcommercial use.dendral led to the development of other rulebased reasoning programs at thestanford artificial intelligence laboratory (sail), the most important of which wasmycin, which helped physicians diagnose a range of infectious blood diseasesbased on sets of clinical symptoms.2 begun in 1972 and completed in 1980, themycin project went further than dendral in that it kept the rules (or embodiedknowledge) separate from the inference engine that applied the rules. this latter partof the mycin project was essentially the first expertsystem shell (buchanan andshortliffe, 1984).3the development of these pioneering expert systems not only constituted majorachievements in ai but also gave both researchers and research funders a glimpse ofthe ultimate power of computers as a tool for reasoning and decision making. moreover, the apparent success of these projects helped to touch off the rapid development of expert systems. promoted by sailõs edward feigenbaum, expert systemsbecame the rage in ai research in the late 1970s and early 1980s and a commercialtool in the 1980s, when corporations were seeking to embody the knowledge of theirfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.developments in artificial intelligence211skilled employees who were facing either retirement or downsizing (feigenbaum etal., 1988). expertsystem shells, based in large part on the òempty mycinó (emycin)shell, moved on to the commercial software market.starting in the mid1980s, numerous startup ai companies began to appear,many with products akin to expert systems. many such companies came and went,but some flourished. for example, gensym corporation, founded in 1986 by analumnus of the massachusetts institute of technologyõs artificial intelligence laboratory, built a substantial business based on its g2 product for development of intelligent systems. more recently, trilogy development group, inc., went public, sellingboth software and services that apply rulebased reasoning and other ai methods tomarketing operations. one of trilogyõs founders (a stanford university graduate)learned about the expert system that carnegie mellon university (cmu) had developed for digital equipment corporation to configure its vax computers (xcon).4basing their work in part on the systems that had emerged from dendral andmycin and what they learned about xcon, trilogyõs founders also used constraintbased equations and objectoriented programming methods, derived in part from airesearch.5 another of trilogyõs founders applied the companyõs methods to the marketing of personal computers (pcs) over the internet. this new firm, pcorder.com.inc.,promises to simplify the configuration of pcs and drastically lower the cost of buying(or selling) one (mchugh, 1996).many corporations committed substantial capital and human resources to thedevelopment of expert systems, and many reported substantial returns on these investments. others found that, as ai pioneer mccarthy (1990) had argued, theseexpert systems were extremely òbrittleó in that a small development in knowledge orchange in practice rendered such programs obsolete or too narrow to use. in onestudy of ai (office of technology assessment, 1985), expert systems were singledout as evidence of òthe first real commercial products of about 25 years of ai researchó but were also criticized for òseveral serious weaknessesó that demandedòfundamental breakthroughsó to overcome. but expert systems represented a failureto meet expectations as much as a failure of technology. they provided valuablehelp for users who understand the limitations of a system that embodied narrowdomains of knowledge. one of the biggest problems with expert systems was theterm itself, which implied a certain level of capability; a number of users startedcalling them knowledgebased systems to refer to the technology instead of the goal.despite these criticisms, work on expert systems continues to be published; somecorporations with strong knowledgeengineering capabilities continue to report substantial savings from expert systems and have demonstrated a continued commitment to expanding their use. expertsystem shell programs continue to be developed, improved, and sold. by 1992, some 11 shell programs were available for themacintosh platform, 29 for ibmdos platforms, 4 for unix platforms, and 12 fordedicated mainframe applications.6 a recent review of expert systems reported thatthe north american market for expert systems is roughly $250 million (representingabout 70 percent of the total commercial ai market). estimates suggest that morethan 12,000 standalone expert systems are in use (liebowitz, 1997). moreover,small expert systems are being incorporated into other types of computer software,most of it proprietary.continued on next pagefunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.212funding a revolutionthe promotion in 1969 of lawrence roberts to director of ipto alsocontributed to a perceived tightening of ai research. under roberts,ipto developed a formal ai program, which in turn was divided intoformal subprograms (norberg and oõneill, 1996). the lineitem budgetof ai research inevitably led to greater scrutiny owing to reporting mechanisms and the need to justify programs to the dod, the administration,and the u.s. congress. consequently, researchers began to believe thatthey were being boxed in by ipto and darpa, and to a certain extentthey were. the flow of darpaõs ai research money to cmu, mit, andstanford university did not cease or even diminish much, but the demandgrew for interim reports and more tangible results.external developments reinforced this shift. the most important wasthe passage of the mansfield amendment in 1969.24 passed during thevietnam war amid growing public concern about the òmilitaryindustrialcomplexó and the domination of u.s. academic science by the military,the mansfield amendment restricted the dod to supporting basic rebox 9.3 continued1the literature on dendral is extensive. for the most recent participantsõ account,see lindsay et al. (1993).2another important program, carried out in the early 1970s, was metadendral.this inductive program automatically formulates new rules for dendral to use inexplaining data about unknown chemical compounds. using the plangeneratetestparadigm, metadendral has successfully formulated rules of mass spectrometry,both by rediscovering existing rules and by proposing entirely new rules. althoughmetadendral is no longer an active program, its contributions to ideas aboutlearning and discovery are being applied to new domains. these ideas suggest, forexample, that induction can be automated as a heuristic search; that, for efficiency,search can be broken into two stepsñapproximate and refined; that learning must beable to cope with noisy and incomplete data; and that learning multiple concepts atthe same time is sometimes inescapable. more information is available online at<http://wwwcamis.stanford.edu/research/history.html#dendral>.3the mycin team also developed an important program known as teiresias, whichmade the basis of mycinõs reasoning transparent to its users and allowed mycinõsknowledge base to be changed or upgraded more easily. the literature on theseprograms is extensive.4xcon (for expert configurer) has been widely hailed as one of the first successfulexpert systems programs. it was the work of john mcdermott and his team at cmu.see crevier (1994).5trilogyõs history is discussed by mchugh (1996).6these data were compiled from r.r. bowker company (1992) and table 3 in pickett and case (1991).funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.developments in artificial intelligence213search that was of òdirect and apparentó utility to specific military functions and operations. it brought about a swift decline in some of themilitaryõs support for basic research, often driving it toward the appliedrealm.25 roberts and his successors now had to justify ai research programs on the basis of immediate utility to the military mission. the movetoward relevance spawned dissatisfaction among both the establishedpioneers of the ai field and its outside skeptics.26another external development provided further impetus for change.in 1973, at the request of the british scientific research council, sir jameslighthill, the lucasian professor of applied mathematics at cambridgeuniversity and a fellow of the royal society of london, produced a survey that expressed considerable skepticism about ai in general and research domains in particular. despite having no expertise in ai himself,lighthill suggested that any particular successes in ai had stemmed frommodeling efforts in more traditional disciplines, not from ai per se. hesingled out robotics research for especially sharp criticism. the lighthillreport raised questions about ai research funding in the united statesand led dod to establish a panel to assess darpaõs ai program.known as the american study group, the panel (which includedsome of aiõs major research figures) raised some of the same questions asdid lighthillõs report and served to inform george heilmeier, a formerresearch manager from rca corporation who was then assistant directorof defense r&d and later became director of darpa. the lighthillreport and its u.s. equivalent led to a shifting of darpa funds out ofrobotics research (hurting institutions such as sri that had committedheavily to the area) and toward òmissionoriented direct research, ratherthan basic undirected researchó (fleck, 1982).27as a result of these forces, darpaõs emphasis on relevance in airesearch grew during the late 1970s and 1980s. despite the disgruntlement among some scientists, the changes led to increased fundingñalthough not directly to widespread commercial successñfor ai research.a magnet for these monies was the strategic computing program (scp),announced in 1983 (darpa, 1983). darpa committed $1 billion overthe planned 10year course of the program. the four main goals of thescp were as follows:1.advance machine intelligence technology and highperformancecomputing, including speech recognition and understanding, naturallanguage computer interfaces, vision comprehension systems, and advancedexpert systems development, and to do so by providing significant increases in computer performance, through parallelcomputer architectures, software, and supporting microelectronics;2.transfer technology from darpasponsored university researchfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.214funding a revolutionefforts to the defense industry through competitive research contracts,with industry and universities jointly participating;3.develop more new scientists in ai and highperformance computingthrough increased funding of graduate student research in these areas; and4.provide the supporting research infrastructure for ai researchthrough advanced networking, new microcircuit fabrication facilities, advanced emulation facilities, and advanced symbolic processors (kahn, 1988).to achieve these goals, darpa established three specific applications as r&d objectives: a pilotõs associate for the air force, an autonomous land vehicle for the army, and an aircraft battle management system for the navy. the applications were intended to spark the militaryservicesõ interest in developing ai technology based on fundamental research. the scp differed from some other largescale national efforts inthat its goals were extremely ambitious, requiring fundamental advancesin the underlying technology. (by contrast, efforts such as the apollospace program were principally engineering projects drawing from anestablished scientific base [office of technology assessment, 1985]). thescp also differed from earlier large ai programs in that some 60 percentof its funds were committed to industry. however, of the 30 prime contractors for the scp involved in software or ai research, more than 20were established defense contractors (goldstein, 1992).the scp significantly boosted overall federal funding for ai researchbut also altered its character. between 1984 and 1988, total federal funding for ai research, excluding the scp, tripled from $57 million to $159million (see table 9.1). with support for the scp included, federal funding increased from $106 million to $274 million. because the scp wasbudgeted as an applied program, it tipped the balance of federal fundingtoward applied research. although darpaõs funding for basic ai research doubled from roughly $20 million to $40 million during this sameperiod, the dodõs overall role in basic ai research declined (see table9.2). meanwhile, it continued to play the dominant role in supportingapplied research in ai (see table 9.3). although budget categorizationsfor programs such as the scp are somewhat arbitrary and subject to political influence, researchers noted a change in darpaõs funding style.the scp also attracted a tremendous amount of industry investmentand venture capital to ai research and development. firms developingand selling expert systems entered the market, often basing their systemson the lisp machines developed by the ai community. several newfirms entered the market to design, make, and sell the very expensivelisp machines. yet the rapid development of engineering workstations,especially those of sun microsystems, inc., soon undermined the lispmachine industry. this segment of the market, which was clearly tied tofunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.developments in artificial intelligence215table 9.1 total federal funding for artificial intelligence research (inmillions of dollars), 1984198819841985198619871988excluding strategic computingbasic44.163.181.585.586applied12.53154.579.573total56.694.1136165159percent applied2233404846including strategic computingbasic44.163.181.585.586applied61.594170.5171.5188total105.6157.1252257274percent applied5860686769source: goldstein (1992).table 9.2 federal funding for basic research in artificial intelligenceby agency (in millions of dollars), 19841988year19841985198619871988darpa21.634.1414436other dod10.512.5171515nondod1216.523.526.535total44.163.181.585.586percent dod7374716959source: goldstein (1992).table 9.3 federal funding for applied research in artificialintelligence by agency (in millions of dollars), 1984198819841985198619871988darpa5678138135.5151other dod0.5821.52627nondod58111010total61.594170.5171.5188percent dod9291949495source: goldstein (1992).funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.216funding a revolutionthe scp, collapsed. even with the development of expertsystem shells torun on lesscostly machines, doubts began to arise about the capabilitiesand flexibility of expert systems; this doubt hampered the commercialization of ai. in addition, commercial contractors had difficulty meeting thehighprofile milestones of the major scp projects because of difficultieswith either the ai technologies themselves or their incorporation intolarger systems. such problems undermined the emergence of a clearlyidentifiable ai industry and contributed to a shift in emphasis in highperformance computing, away from ai and toward other grand challenges,such as weather modeling and prediction and scientific visualization.artificial intelligence in the 1990sdespite the commercial difficulties associated with the strategic computing program, the aidriven advances in rulebased reasoning systems(i.e., expert systems) and their successorsñmany of which were initiatedwith darpa funding in the 1960s and 1970sñproved to be extremelyvaluable for the emerging national information infrastructure and electronic commerce. these advances, including probabilistic reasoning systems and bayesian networks, natural language processing, and knowledge representation, brought ai out of the laboratory and into themarketplace. paradoxically, the major commercial successes of ai research applications are mostly hidden from view today because they areembedded in larger software systems. none of these systems has demonstrated general human intelligence, but many have contributed to commercial and military objectives.an example is the lumiere project initiated at microsoft research in1993. lumiere monitors a computer userõs actions to determine whenassistance may be needed. it continuously follows the userõs goals andtasks as software programs run, using bayesian networks to generate aprobability distribution over topic areas that might pose difficulties andcalculating the probability that the user will not mind being botheredwith assistance. lumiere forms the basis of the òoffice assistantó thatmonitors the behavior of users of microsoftõs office 97 and assists themwith applications. lumiere is based on earlier work on probabilistic models of user goals to support the display of customized information topilots of commercial aircraft, as well as user modeling for display controlfor flight engineers at nasaõs mission control center. these earlierprojects, sponsored by the nasaames research center and nasaõsjohnson space center, were undertaken while some of the lumiere researchers were students at stanford university.28patent trends suggest that ai technology is being incorporated intogrowing numbers of commercial products. the number of patents in ai,funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.developments in artificial intelligence217expert systems, and neural networks jumped from fewer than 20 in 1988to more than 120 in 1996, and the number of patents citing patents in theseareas grew from about 140 to almost 800.29 the number of airelatedpatents (including patents in ai, expert systems, neural networks, intelligent systems, adaptive agents, and adaptive systems) issued annually inthe united states increased exponentially from approximately 100 in 1985to more than 900 in 1996 (see figure 9.1). changes in the u.s. patent andtrademark officeõs rules on the patentability of algorithms have no doubtcontributed to this growth, as has the increased commercial value of aitechnology. the vast majority of these patents are held by private firms,including large manufacturers of electronics and computers, as well asmajor users of information technology (see table 9.4). these data indicatethat ai technology is likely to be embedded in larger systems, from computers to cars to manufacturing lines, rather than used as standalone products.a central problem confronting the wider commercialization of aitoday revolves around integration. both the software and the hardwaredeveloped by the ai research community were so advanced that theirintegration into older, more conservative computer and organizationalfigure 9.1 artificialintelligencerelated patents awarded per year, 19761996.source: compiled from data in the u.s. patent and trademark officeõs u.s.patent bibliographic database, available online at <http://patents.uspto.gov>; andthe ibm patent server, available online at <http://patent.womplex.ibm.com>.01002003004005006007008009001000197619771978197919801981198219831984198519861987198819891990199119921993199419951996patents awardedfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.218funding a revolutionsystems proved to be an enormous challenge. as one observer has noted,òbecause ai was a leadingedge technology, it arrived in this world tooearly. as a consequence, the ai application community had to ride manywaves of technological quick fixes and fads. . . . many of these integrationproblems are now being addressed head on by a broad community ofinformation technologists using internetbased frameworks such ascorba [common object request broker architecture] and the world widewebó (shrobe, 1996).the rapid development of computer hardware and software, the networking of information systems, and the need to make these systemsfunction smoothly and intelligently are leading to wide diffusion of aiknowledge and technology across the infrastructure of the informationage. federal funding reflects these changes (see box 9.4). meanwhile,much of the knowledge acquired through ai research over the years istable 9.4 leading holders of patents related toartificial intelligence, 19761997assigneenumber of patentsibm297hitachi192motorola114mitsubishi 94toshiba 92general electric 91nec corp. 73taligent 67toyota 60u.s. phillips corp. 59fujitsu ltd 58lucent technologies 57ford motor co. 53digital equipment corp. 53westinghouse electric 48eastmankodak 44at&t 44hughes aircraft co. 42matsushita 42texas instruments 42note: the patents included artificial intelligence, expert systems,neural networks, intelligent systems, adaptive agents, and adaptivesystems.sources: u.s. patent and trademark office database, availableonline at <http://patents.uspto.gov>; ibm corp. patent database,available online at <http://patent.womplex.ibm.com>.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.developments in artificial intelligence219now being brought to bear on realworld problems and applications whilealso being deepened and broadened. the economic and social benefitsare enormous. technologies such as expert systems, naturallanguageprocessing, and computer vision are now used in a range of applications,such as decision aids, planning tools, speechrecognition systems, patternrecognition, knowledge representation, and computercontrolled robots.30box 9.4darpaõs current artificial intelligence programat darpa, funding for ai research is spread among a number of program areas,each with a specific application focus. for example, funding for ai is included in theintelligent systems and software program, which received roughly $60 million in1995. this applied research program is intended to leverage work in intelligent systems and software that supports military objectives, enabling information systems toassist in decisionmaking tasks in stressful, timesensitive situations. areas of emphasis include intelligent systems, software development technology, and manufacturing automation and design engineering. intelligent systems encompass autonomoussystems, interactive problem solving, and intelligent integration of information.1additional darpa funding for ai is contained in the intelligent integration ofinformation (i3) program, which is intended to improve commandersõ awareness ofbattlefield conditions by developing and demonstrating technology that integratesdata from heterogeneous sources. specific goals include a 100fold reduction in thetime needed to retrieve information from large, dynamically changing databases, aswell as the development, demonstration, and transition to the services of tools thatwill reduce the time needed to develop, maintain, and evolve largescale integrateddata systems.2 the program supports basic computer sciences, specifically in airelevant to integration, technology development, prototyping, demonstrations, andearly phases of technology transfer.darpa continues to fund some basic research in ai as well. such funding isincluded in its information sciences budget, which declined from $35 million to $22million annually between 1991 and 1996. the ai funding supports work in softwaretechnology development, humancomputer interfaces, microelectronics, and speechrecognition and understanding, in addition to intelligent systems. the work onintelligent systems focuses on advanced techniques for knowledge representation,reasoning, and machine learning, which enable computer understanding of spokenand written language and images. also included are advanced methods for planning, scheduling, and resource allocation.1this definition was obtained from the fy 97 implementation plan on the web site ofthe national science and technology councilõs committee on computing, information, and communications at <http://www.ccic.gov/pubs/imp97/14.html>.2this information was obtained from the project description (òintelligent integrationof informationó) on darpaõs web site at <http://webext2.darpa.mil/iso/i3/about/main.html>.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.220funding a revolutionai technologies help industry diagnose machine failures, design newproducts, and plan, simulate, and schedule production. they help scientists search large databases and decode dna sequences, and they helpdoctors make moreinformed decisions about diagnosis and treatment ofparticular ailments. ai technologies also make the larger systems intowhich they are incorporated easier to use and more productive. thesebenefits are relatively easy to identify, but measuring them is difficult.federal investments in ai have produced a number of notable results,some envisioned by the founders of the field and others probably noteven imagined. without question, darpaõs generous, enduring fundingof various aspects of ai research created a scientific research disciplinethat meets the standard criteria of discipline formation laid out by sociologists of science.31 at least three major academic centers of excellenceand several other significant centers were established, and they produceda large number of graduates with ph.d.s who diffused ai research toother research universities, crosspollinated the major research centers,and moved ai methods into commercial markets. (figure 9.2 shows theproduction of ph.d. degrees in ai and related fields at u.s. universities.010020030040050060070019561958196019621964196619681970197219741976197819801982198419861988199019921994artificial intelligenceexpert systemsneural networksintelligent systemsadaptive systemsadaptive agentsfigure 9.2 ph.d. dissertations submitted annually in artificial intelligence andrelated fields, 19561995.source: data from dissertation abstracts online, which is available throughsubscription to the oclc firstsearch database from umi company.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.developments in artificial intelligence221figure 9.3 compares ph.d. production in ai and related disciplines todegree production in computer science more broadly.) in sum, the returns on the public investment are clearly enormous, both in matters ofnational security (which are beyond the scope of this study)32 and incontributions to the u.s. economy.lessons from historyas this case study demonstrates, federal funding is critical in establishing new disciplines because it can sustain longterm, highrisk research areas and nurture a critical mass of technical and human resources.darpa helped legitimize the ai field and served as the major source ofresearch funds beginning in the 1960s. it created centers of excellence thatevolved into todayõs major computer science research centers. this support was particularly critical given that some objectives took much longerto realize than was originally anticipated.a diversity of approaches to research problems can be critical to thedevelopment of practical tools. a prime example is the field of speechrecognition, in which the most effective products to date have used tech05001000150020002500300019561958196019621964196619681970197219741976197819801982198419861988199019921994artificial intelligenceexpert systemsneural networksintelligent systemsadaptive systemsadaptive agentscomputer sciencefigure 9.3 number of ph.d. dissertations submitted annually in ai and relatedfields and in computer science, 19561995.source: data from dissertation abstracts online, which is available throughsubscription to the oclc firstsearch database from umi company.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.222funding a revolutionniques borrowed from the mathematics and statistics communities ratherthan more traditional ai techniques. this outcome could not have beenpredicted and demonstrates the importance of supporting competing approaches, even those outside the mainstream.federal funding has promoted innovation in commercial productssuch as expert systems, the establishment of new companies, the growthof billiondollar markets for technologies such as speech recognition, andthe development of valuable military applications. ai technologies oftenenhance the performance of the larger systems into which they are increasingly incorporated.there is a creative tension between fundamental research and attempts to create functional devices. original attempts to design intelligent, thinking machines motivated fundamental work that created a baseof knowledge. initial advances achieved through research were not sufficient to produce, by themselves, commercial products, but they could beintegrated with other components and exploited in different applications.efforts to apply ai technology often failed initally because they uncovered technical problems that had not yet been adequately addressed.applications were fed back into the research process, thus motivatinginquiries into new areas.notes1.several histories of ai research have appeared over the last 25 years, somewritten by members of the ai community, some published by those outside thefield, and still others produced by science journalists. these histories have beenbased largely on the published scientific literature, journalistic accounts of workin the field, published accounts of participants in the field, and interviews withparticipants. with some notable exceptions, few of these histories have relied onoriginal source materials, such as manuscript records of participants or their funding agencies or editors.2.the 1997 victory of ibm corporationõs deep blue computer over worldchess champion gary kasparov demonstrates the publicõs interest in ai. in thedays leading up to the match and throughout the match itself, almost every majoru.s. newspaper, news magazine, and television news or magazine program carried news and feature articles about the match and ai research in general.3.papers presented at the conference were published by shannon andmccarthy (1956).4.minsky had an impressive background: a bachelorõs degree in mathematics from harvard university (1950); a doctorate in mathematics from princetonuniversity (1954); and the title of junior fellow, harvard society of fellows (19541957). his 1954 dissertation was entitled òneural nets and the brain modelproblem.ó the paper he presented at the 1952 conference was entitled òsomeuniversal elements for finite automata.ó his early work at lincoln laboratory(minsky, 1956) dealt with ai.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.developments in artificial intelligence2235.rochester was the chief designer of ibmõs 701 computer.6.dated august 31, 1955, òa proposal for the dartmouth summer researchproject on artificial intelligenceó was actually submitted, along with a cover letter to morison, on september 2, 1955, according to the rockefeller foundationarchives grant files. an edited and lightly annotated version of this proposal canbe found online at <http://wwwformal.stanford.edu/jmc/history/dartmouth/dartmouth.html>.7.this quote comes from an article entitled òlisp prehistoryñsummer 1956through summer 1958,ó which can be found online at <http://wwwformal.stanford.edu/jmc/history/lisp/node2.html#section00020000000000000000>.see also mccarthy (1981).8.indeed, mccarthyõs first memorandum on lisp (dated september 16,1958) survives only because rochester saved and annotated it. rochester was theauthor of at least one of the foundational lisp memoranda; his memo 5, november 18, 1958, provides clear evidence of rochesterõs intellectual contributions tolisp. for an interesting treatment of lispõs history, see òearly lisp history(19561959)ó by herbert stoyan, available online at <http://www8.informatik.unierlangen.de/html/lisp/histlit1.html>.9.rochester also published papers on ai (e.g., rochester and gelernter, 1958).10.the founding of gsia and its ònew lookó are described by gleeson andschlossman (1994).11.the early history of rand is described by smith (1966), jardini (1996),hounshell (1997), and collins (1998).12.this report describes the logic theorist and the ipl2 list processing language (developed with j.c. shaw) for the johnniac. see also newell, simon, andshaw (1957).13.the history of the johnniac is recounted by gruenberger (1979).14.another of simonõs doctoral students, edward feigenbaum, as part of his1960 doctoral dissertation developed a theory of human perception, memory,and learning and then modeled these processes successfully in his epam program. this program is still regarded as a major contribution both to theories ofhuman intelligence and to ai research.15.this quote comes from an article entitled òthe implementation of lispóavailable online at <http://wwwformal.stanford.edu/jmc/history/lisp/node3.html#section00030000000000000000>.16.the history and design of both the logic theorist and gps are describedby newell and simon (1972).17.for example, bold predictions about the future of ai were made by simonand newell (1958); simon, newell, and shaw (1958); simon (1965); minsky (1956,1979).18.darpa definitely created a twotier system in ai research, with cmu,mit, stanford university, and sri occupying the top tier. the second tier included the university of massachusetts, university of maryland, brown university, university of pennsylvania, new york university, columbia university,rutgers university, university of texas, and university of illinois.19.this was the socalled pierce committee, named after its chairperson j.r.pierce, who, according to roberts, said it would be impossible to make a comfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.224funding a revolutionputer understand human speech (roberts, òexpanding ai researchó). a researcher at bell laboratories (where important speech recognition research hadbeen done for decades), pierce published an infamous letter to the editor of thejournal of the acoustical society of america in 1969 in which he railed against theòmad scientist and untrustworthy engineersó who believed that the developmentof a flexible, continuous speech recognition system was possible (pierce, 1969).20.one of the characteristics of ai that sets it apart from many other researchdomains of computer science is the degree to which the objectives can be described in general, nontechnical language that makes measuring performancedifficult (e.g., an objective might be to make a machine that thinks or learns).although specific metrics often can be developed, this may require a consciouseffort by program managers and researchers.21.considerable controversy surrounded this decision and its motivations.j.c.r. licklider claimed the project was turned off because òit didnõt really proveitself by coming up with a conspicuously good demonstrable device. there wasa lot of feeling that speech understanding was a bit ahead of its time, and darpadid a good thing by stimulating the field but it wasnõt really time yet to drive fora workable system. in this case, the speech project met its main objectives, butthat wasnõt enough to save itó (licklider, 1988a). some observers suggest that thedemise of the sur program illustrates òthe dangers of prematurely imposingshortterm goals and milestones on an underdeveloped field of researchó such asspeech recognition (stefik, 1985). marvin denicoff, then at onr, believed sostrongly in the speech program and what it could do for future researchers thathe convinced robert kahn, director of ipto from 1979 to 1985, to fund a projectto study surñto òtake a year or two out, visit all the sites and create a documentof everything that had been accomplished and what the issues were, what thefailures were, and what the positives wereó (denicoff, 1988). the results were theonr reports by w. lea and j. shoup (1979) and w. lea (1980).22.here the allusion is to the work of frederick jelinek and the speech research group at ibm, which contributed enormously to the technology throughstatistical language modeling (e.g., ngram models). other firms that not onlypursued speech recognition research but also entered commercial markets withrelated products included verbex voice systems and texas instruments.23.for example, l.e. baum and j.a. eagon of the institute for defense analyses have been credited with introducing hmm theory (makhoul and schwartz,1994). see also baum and eagon (1967). moreover, even within cmu, the decision to use hmms in speech recognition was, according to a recent analysis,ò[w]ay out of step with the mainstream [of ai thought at cmu]. . . . the systemhad no knowledge of english grammar, no knowledge base, no rulebased expertsystem, no intelligence. nothing but numbersó (garfinkel, 1998).24.the mansfield amendment was passed as part of the defense authorization act of 1970 (public law 91121) on november 19, 1969.25.as noted in chapter 4, several of the dodõs basic research programs incomputer science were transferred to the national science foundation as a resultof the mansfield amendment.26.the mansfield amendment and the spirit of the times in which it waspassed also established the conditions under which some members of the confunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.developments in artificial intelligence225gress would raise questions about ai research, particularly with regard to itsfocus on the playing of games such as chess and checkers but extending to theresearch on speech understanding, which is discussed in box 9.2.27.fleck (1982) also said the lighthill report òled to a considerable sharpening of the focus of ai research.ó not all leaders in ai research agree with fleckõsassessment of the impact of the report. amarel (1988), for example, maintainedthat òit didnõt affect this country.ó28.for more information, see microsoftõs home page at <www.research.microsoft.com/research/dtg/horovitz/lum.htm>.29.these data were obtained from the u.s. patent and trademark officedatabase, available online at <http://patents.uspto.gov>, and ibmõs patent database, also available online at <http://patent.womplex.ibm.com>.30.these technologies are the results of the efforts of ai researchers as wellas researchers in other, related fields.31.see, for example, fleck (1982).32.a report by the american association for artificial intelligence (1994)paraphrased a former director of arpa in saying that dart (the intelligentsystem used for troop and materiel deployment for operation desert shield andoperation desert storm in 1990 and 1991) òjustified arpaõs entire investment inartificialintelligence technology.ó (the report is also available on the associationõsweb site at <http://www.aaai.org/policy/papers/arpareport.html>).funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.22610virtual reality comes of agevirtual reality (vr) is a highly multidisciplinary field of computingthat emerged from research on threedimensional interactive graphicsand vehicle simulation in the late 1960s and early 1970s.1 for much of itsearly development, vr often seemed more like science fiction than science, but it is now transforming fields such as military training, entertainment, and medicine. applications range from navigation systems thatenable pilots and air traffic controllers to operate in dense fog2 to fullydigital design environments for creating new car models3 (see box 10.1).this chapter focuses on research and development (r&d) in computer graphics and related technologies that contributed to the emergence of vr as a practical technology. in particular, it examines thediversity of funding agencies, missions, and environments, as well as thestrong interactions between public and private research and personnel,that have promoted advances in the field. the analysis is not intended tobe comprehensive but rather concentrates on selected topics that illuminate the r&d process. it highlights medical and entertainment applications of vr because they demonstrate interesting aspects of the innovation process. the emphasis on headmounted displays is not meant todownplay the significance of other vr technologies that are not addressed, such as the large projection environments at the national centerfor supercomputing applications at the university of illinois at urbanachampaign.4 the research on headmounted displays is but one illustration of the many ways in which federally sponsored research programshave influenced the vr field.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.virtual reality comes of age227this case history demonstrates that federal support has been the singlemost important source of sustained funding for innovative research inboth computer graphics and vr. beginning in the 1960s with its investments in computer modeling, flight simulators, and visualization techniques, and continuing through current developments in virtual worlds,the federal government has made significant investments in military, civilian, and university research that laid the groundwork for one of todayõsmost dynamic technologies. the commercial payoffs have included numerous companies formed around federally funded research in graphicsand vr.the first section of the chapter briefly outlines the origins of vr. thenext seven sections, which are organized in roughly chronological order,discuss early development of the academic talent pool, the private sectorõscautious initial approach, the role of synergy in launching visionary vrbox 10.1 what is virtual reality?virtual reality (vr) refers to a set of techniques for creating synthetic, computergenerated environments in which human operators can become immersed. in vrsystems, human operators are connected to computers that can simulate a widevariety of worlds, both real and imaginary, and can interact with those worldsthrough a variety of sensory channels and manipulators (national research council,1995, pp. 247303). simple vr systems include home video games that producethreedimensional (3d) graphical displays and stereo sound and are controlled by anoperator using a joystick or computer keyboard. more sophisticated systemsñsuchas those used for pilot training and immersive entertainment experiencesñcan include headmounted displays or large projection screens for displaying images, 3dsound, and treadmills that allow operators to walk through the virtual environment.such systems are increasingly being used in a variety of applications, from telecommunications and information visualization to health care, education and training, product design, manufacturing, marketing, and entertainment. among otherthings, they enable operators to explore foreign cities from the comfort of their ownhomes, train for hazardous missions, develop new surgical procedures, and test newproduct designs.vr is the outcome of a complex alignment of research fields that include computergraphics, image processing, computer vision, computeraided design, geometricmodeling, userinterface design, and physiological psychology. it also incorporatesrobotics; haptics and force feedback; computer architectures and systems development; entire new generations of processors, graphics boards, and accelerators; and ahost of software applications converted to firmware in computers for rendering datavisually. finally, vr also involves work on highspeed data transmission and networks.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.228funding a revolutionresearch, a breakthrough that provided initial building blocks for a commercial vr infrastructure, the mixture of research projects that led tobiomedical applications, the role of entertainment applications in expanding use of vr, and the growing role of military r&d in producing commercial spinoffs. the last section of the chapter summarizes the lessonslearned from history.launching the graphics andvirtual reality revolutionthe earliest use of a computergenerated graphical display on a cathode ray tube (crt) was in project whirlwind, a project sponsored by theu.s. navy to develop a generalpurpose flight simulator (see chapter 4).by the late 1940s, robert everett at the massachusetts institute of technology (mit) had developed a light gun that could cause the crt to react.researchers on sage, the successor to whirlwind, made extensive use ofinteractive graphics consoles with displays equipped with a light guncapable of sending signals coordinated with the display. by 1955, u.s. airforce personnel working on sage were using light guns for data manipulation.these and other early projects convinced a number of researchers thatthe capability to interact with a computer in real time through a graphicalrepresentation was a powerful tool for making complex information understandable. in the late 1950s and early 1960s, several government agencies, including the national science foundation (nsf), national institutesof health (nih), national aeronautics and space administration (nasa),and various divisions within the department of defense (dod), beganfunding research to address an array of computer graphics problems,including the development of input/output devices and programming.the total funding for these early programs was comparatively small.for example, the nsf allocated about 8 percent of its annual computingresearch budget to computer graphics between 1966 and 1985. its graphicsrelated expenditures rose from $93,000 to $1.8 million annually duringthis period.5 another source of funding for computer graphics researchduring these years was the information processing techniques office(ipto) of the dodõs defense advanced research projects agency(darpa, known at times as arpa). the ipto support for the development of interactive graphics was concentrated at mit, carnegie mellonuniversity, and especially the university of utah, which received $10million in ipto support for interactive graphics research between 1968and 1975 (stockham and newell, 1975; van atta et al., 1991a,b). university programs were only loosely coupled to deliverable systems but supported visionary ideas and the training of students to pursue them.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.virtual reality comes of age229the eventual payoffs from these small initial investments were enormous. the government support established an infrastructure for the computer graphics field through universitybased research and training infundamental science. these centers identified key research and technicalproblems, developed sample solutions, created tools and methods, and,above all, produced cadres of students, researchers, and teachers whobecame the leading practitioners in the field. the graduates of the federally supported academic programs have made substantial contributionsnot only to many areas of science, technology, and medicine, but also tothe intellectual and artistic culture of the late 20th century. they havealso launched companies that laid the foundations for a worldwide market for computer graphics worth $40 billion in 1997.seeding the academic talent poolamong the greatest contributions of the federal government has beensupport for the development of human resources. (associations alsoplayed a role in building the graphics community, as illustrated in box10.2). an early pioneer, steven coons, benefited from federal support ofresearch at mit that helped realize his vision of interactive computergraphics as a powerful design tool. during world war ii, coons workedon the design of aircraft surfaces, developing the mathematics to describegeneralized surface patches. an early advocate of the use of computers inmechanical engineering, coons taught in the mechanical engineeringdepartment at mit during the 1950s and 1960s, where he inspired hisstudents with the vision of creating interactive computer graphics to assist design (coons, 1967). among the students he inspired were ivansutherland and lawrence roberts, both of whom went on to make numerous contributions to computer graphics and (in robertsõ case) to computer networks. both men also served as directors of ipto.working in the early 1960s on the tx2 at mitõs lincoln laboratory,which was equipped with an interactive display tube, sutherland developed a graphics system called sketchpad as his dissertation in 1963.sketchpad was an interactive design tool for the creation, manipulation,and display of geometric objects in twodimensional (2d) or threedimensional (3d) space. the system could sketch with a light pen on the face ofthe crt, position objects, change their size, square up corners, createmultiple copies of objects, and paste them into an evolving design.sketchpad was the first system to explore the data management techniques required for interactive graphics.roberts, meanwhile, wrote the first algorithm to eliminate hidden orobscured surfaces from a perspective picture (roberts, 1963). in 1965,roberts implemented a homogeneous coordinate scheme for transformafunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.230funding a revolutiontions and perspective. his solutions to these problems prompted attemptsover the next decade to find faster algorithms for generating hidden surfaces (roberts, 1965).sutherland expanded the talent pool everywhere he went. first mit,then harvard university (especially after sutherlandõs return from hisstint as ipto director in 1966), and, following sutherlandõs move there in1968, the university of utah became the major academic centers of earlywork in interactive graphics. in particular, the period from the late 1960sthrough the late 1970s was a golden era of computer graphics at utah.students and faculty in utahõs arpafunded program contributed to thegrowth of a number of exploratory systems in computer graphics and theidentification of key problems for future work (table 10.1).among their notable activities were efforts to develop fast algorithmsfor removing hidden surfaces from 3d graphics images, a problem identified as a key computational bottleneck (sutherland et al., 1974). studentsof the utah program made two important contributions in this field, inbox 10.2community buildingmany researchers credit the group siggraph with helping to build a strongcommunity of graphics researchers that propelled the field forward rapidly. siggraph, which is the association for computing machineryõs special interest groupon graphics, facilitates the exchange of ideas among researchers and technologydevelopers through conferences and publications in an attempt to advance the technology of computer graphics and interactive techniques. it introduces the latesttopics in computer graphics through conference courses and other educational activities, including development and distribution of curriculum materials.siggraph attracts a diverse range of members, from computer scientists specializing in computer graphics and visualization, to business leaders and artists who usegraphics as a means to further their craft. interaction among such diverse memberscan help technology developers better understand the needs of users and promoteadvances in the capabilities of graphics technology. an annual conference has become a central location for the exchange of ideas and demonstration of developmental systems. numerous academic and industry researchers publish papers in siggraph journals and conference proceedings. edwin catmull has called siggrapha òtremendous communityó and credits its collaborative spirit and broadbased constituency with helping to accelerate the development of computer graphics. 11presentation by edwin catmull, chief technical officer, pixar animation studios,at the computer science and telecommunications board workshop, òmodeling andsimulation: competitiveness through collaboration,ó october 19, 1996, irvine, ca.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.virtual reality comes of age231table 10.1 select alumni of the university of utahõs computergraphics programnameaffiliationaccomplishmentsalan kayph.d. 1969developed the notion of a graphical user interfaceat xerox parc, which led to the design of applemacintosh computers. developed smalltalk.fellow at apple computer.john warnockph.d. 1969worked on the illiac 4 project, a spaceflightsimulator, and airplane simulators at evans &sutherland. developed the warnock recursivesubdivision algorithm for hidden surfaceelimination. founder of adobe systems, whichdeveloped the postscript language for desktoppublishing.nolan bushnellb.s. 1969developed the table tennis game pong, which in1972 launched the video game industry. founder ofatari, which became the leading company in videogames by 1982.charles seitzfacultypioneer in asynchronous circuits. codesigner of19701973the first graphics machine, lds1 (line drawingsystem). designed the cosmic cube machine as aresearch prototype that led to the design of the intelipsc. founder of myricom corp.henri gouraudph.d. 1971developed the gouraud shading method forpolygon smoothingña simple rendering methodthat dramatically improved the appearance ofobjects.edwin catmullph.d. 1974pioneer in computer animation. developed the firstcomputer animation course in the world. cofounder of pixar animation studios, a leadingcomputer graphics company that has worked forlucasfilm and was recently involved in theproduction of the movie toy story. received atechnical academy award (with tom porter, tomduff, and alvy ray smith) in 1996 for òpioneeringinventions in digital image compositing.ójames clarkph.d. 1974rebuilt the headmounted display and 3d wand tosee and interact with threedimensional graphicspaces. former faculty member at stanforduniversity. founder of silicon graphicsincorporated and chairman of netscapecommunications corporation.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.232funding a revolutioncluding an area search method by warnock (1969) and a scanline algorithm that was developed by watkins (1970) and constructed into a hardware system. perhaps the most important breakthrough was henrigouraudõs development of a simple scheme for continuous shading(gouraud, 1971). unlike polygonal shading, in which an entire polygon(a standard surface representation) was a single level of gray, gouraudõsscheme involved interpolation between points on a surface to describecontinuous shading across a single polygon, thus achieving a closer approximation of reality. the effect made a surface composed of discretepolygons appear to be continuous.the work of these individuals alone reflects the high level of fundamental research performed under federal sponsorship in a variety ofgraphics fields, including surface rendering, simulations, computer animation, graphical user interface design, and early steps toward vr. noless than 11 commercial firms, several of which ship more than $100 million in products annually, trace their origins to the utah program.6table 10.1 continuednameaffiliationaccomplishmentsbui tuongphongph.d. 1975invented the phong shading method for capturinghighlights in graphical images by modelingspecular reflection. phongõs lighting model is stillone of the most widely used methods forillumination in computer graphics.henry fuchsph.d. 1975federico gil professor, university of north carolinaat chapel hill. research in highperformancegraphics hardware; threedimensional medicalimaging; headmounted display and virtualenvironments. founder of pixel planes.martin newellph.d. 1975;developed procedural modeling for objectfacultyrendering. codeveloped the painterõs algorithm19771979for surface rendering. founder of ashlarincorporated, which develops computerassisteddesign software.james blinnph.d. 1978invented the first method for representing surfacetextures in graphical images. scientist at jetpropulsion laboratory, where he worked oncomputer animation of the voyager flybys.james kajiyaph.d. 1979developed the frame buffer concept for storing anddisplaying singleraster images.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.virtual reality comes of age233virtual reality in the private sector:approach with cautionindustry and private research centers played an important role in theearly development of interactive graphics. but an examination of severalkey playersñbell laboratories, the mathematical applications group incorporated (magi), and general electric company (ge)ñillustrates thatthe private sector, even when it has federal funding for isolated projects,cannot support development of nascent technologies requiring highriskresearch with uncertain payoffs. indeed, even when a company contributes lucrative new technologies to the field, the government is often thekey to sustaining progress over time (see box 10.3).bell laboratories had one group of researchers, including michaelnoll, bela julesz, and c. bosche, working on computeranimated stereobox 10.3the rise and fall of atariatari, founded by university of utah graduate nolan bushnell, was once thefastestgrowing company in the united states. started in 1972 with an initial investment of $500, atari attained sales exceeding $500 million in 1980. during the late1970s and early 1980s, atari was a center for exciting developments in software andchip design for the home entertainment market. a joint venture with lucasfilm in1982, in which atari licensed and manufactured games designed by lucasfilm, established crosspollination between video games and film studios.several pioneering figures in the vr field got their start at atari. for instance,warren robinett, who has directed the headmounted display and nanomanipulatorprojects at the university of north carolina in chapel hill, developed the popularvideo game adventure at atari in the late 1970s. jaron lanier got his start by creatingthe video game moondust. he used the profits to launch vplresearch in 1984, thefirst commercial vr company. in 1980 atari created its own research center, directed by alan kay, who came from xerox parc and assembled a team of the best andbrightest in the field of interface design and vr research.but atari fell on hard times. not long after its banner year in 1980, atari registered $536 million in losses for 1983. the atari research laboratory was a casualtyof the economic crash in the video game industry (and computer industry moregenerally). most of the people working in vr at atari either migrated to work on vrprojects in federal laboratories, or, like jaron lanier, landed government contracts.lanier won a contract to build the dataglove for nasa.industry was clearly not prepared, after sustaining such a big economic blow, tocontinue the development of vr technology on its own. indeed lanierõs failed efforts to market a consumer entertainment version of the dataglove, called powerglove, for nintendo, demonstrated that the 1980s was not the right time for a sustained industry push. federal support was crucial to building the array of hardwareand software necessary for industry to step in and move vr forward.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.234funding a revolutionmovies, and another group, including ken knowlton, leon harmon, andmanfred schroeder, working on pixel graphics methods for digitizing stillimages, grayscale techniques, and ruledirected animation. knowltonalso produced an important animation language, called beflix, whichpermitted the creation and modification of grayscale pixel images.magi, headed by phillip mittleman, was supported by military contracts for projects simulating equipment behavior. magi developed ahiddensurface algorithm along with a user language, synthavision,which sent output to a specially built monitor for microfilming throughcolor filters. the system provided a useroriented syntax for makingcomputer animation, and it was important for creating film footage foradvertising.the ge group built the first realtime, fullcolor, interactive flight simulator, a project funded by a nasa contract for the manned space program(rouselot and schumacker, 1967).7 the simulator, completed in 1967,permitted up to 40 solid objects to be displayed in full color, with hiddensurfaces removed and visible surfaces shaded to approximate reflectedillumination. the entire display was updated in real time, depending ona traineeõs actions on the controls. this ge system was the prototype fora new generation of training simulators that integrated computerdrivensynthetic visual environments with interactive tactile feedback.although ge had a wellendowed inhouse research infrastructure ofvenerable standing, the company took a cautious approach to this newarea of research. ge aerospace did not market its early imagegeneratingsystems to customers other than the federal government, nor did it initiate its own program to develop vr. ge did spin off a commerciallysuccessful system called genigraphics, a fullcolor, interactive, 2d slidegenerating system aimed at the commercial audiovisual market. and ofcourse, ge did continue contract work on image generators for flightsimulators, including its highly rated compuscene iv system, whichòpractically stole the market in highend military flight simulation andtraining in 1984 when [it] introduced photographicquality texturing torealtime graphics.ó8ge also pursued medical imaging. its medical systems laboratoryhas been a major manufacturer of medical imaging systems, from xraymachines to ultrasound, computerized tomography (ct), magnetic resonance imaging (mri), and positron emission tomography (pet) systems.in addition, ge scientists have made distinguished contributions to thepublished literature on scientific and medical visualization. for example,the òmarching cubesó algorithm developed by william e. lorensen andharvey e. cline of the electronic systems laboratory at the ge researchand development center is one of the most fundamental algorithms forhighresolution, 3d surface reconstruction from ct, mri, or spect datafunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.virtual reality comes of age235(lorensen and cline, 1987).9 graphics work of this sort has been regarded by ge as central to the development of new imaging systems.significantly, geõs achievements in this area have benefited from university collaborations and federal support. an example is the recentarrangement between ge medical systems and the university of chicagoinvolving the ge digital detector system, a 10year, $100 million r&deffort that has been the basis of a portfolio of medical imaging andcomputeraided detection systems involving more than 100 scientists andresulting in 80 patents (general electric, 1997). the ge technology will beused by the university of chicago medical center in a longterm projectsupported by the national cancer institute, american cancer society,u.s. army, and whitaker foundation to develop a platform for computeraided diagnosis, which provides the radiologist with guidance for reading a mammographic image.10the ge experience demonstrates the difficulty faced by private firmsin funding longterm research that is not directly related to ongoing product development efforts. industry seldom funds research that is expectedto take more than 5 to 7 years to produce tangible results, although firmscan misjudge how long it will take to develop a marketable product fromnew technology. and, some firms do support limited research with longertime horizons (see chapter 5 for a discussion of longterm research). in itspress releases on the digital detector system, ge emphasizes that this 10year project is the largest development project in company history. commercial vr, by comparison, has taken 30 years to mature.none of the companies discussed in this section (bell laboratories,magi, or ge) pursued commercial applications of vr. magi left thegraphics field completely, failing to sustain a research capability in computer animation and simulation even though it helped launch the field.11both bell laboratories and ge abandoned work on commercial simulation systems in spite of commanding early positions in the field. it is notdifficult to see why. vr is one of those fields that ivan sutherland wouldchristen òholy grailsóñfields involving the synthesis of many separate,expensive, and risky lines of innovation in a future too far distant andwith returns too unpredictable to justify the longterm investment.synergy launches the quest for the òholy grailówork on headmounted displays illustrates the synergy between theapplicationsfocused environments of industry and governmentfunded(both military and civilian) projects and the fundamental research focusof university work that spills across disciplinary boundaries. work onheadmounted displays benefited from extensive interaction and crossfertilization of ideas among federally funded, missionoriented militaryfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.236funding a revolutionprojects and contracts as well as privatesector initiatives. the playersincluded nasa ames, armstrong aerospace medical research laboratory of the air force, wrightpatterson air force base, and, more recently, dod programs on modeling and simulation, such as the synthetictheater of war program. each of these projects generated a stream ofpublished papers, technical reports, software (some of which became commercially available), computeranimated films, and even hardware thatwas accessible to other graphics researchers. other important ideas forthe headmounted display came from knowlton and schroederõs work atbell laboratories, the approach to realtime hiddenline solutions by themagi group, and the ge simulator project (sutherland, 1968).early work on headmounted displays took place at bell helicoptercompany. designed to be worn by pilots, the bell display received inputfrom a servocontrolled infrared camera, which was mounted on the bottom of a helicopter. the camera moved as the pilotõs head moved, and thepilotõs field of view was the same as the cameraõs. this system wasintended to give military helicopter pilots the capability to land at night inrough terrain. the helicopter experiments demonstrated that a humancould become totally immersed in a remote environment through theeyes of a camera.the power of this immersive technology was demonstrated in anexample cited by sutherland (1968). a camera was mounted on the roofof a building, with its field of view focused on two persons playing catch.the headmounted display was worn by a viewer inside the building,who followed the motion of the ball, moving the camera by using headmovements. suddenly, the ball was thrown at the camera (on the roof),and the viewer (inside the building) ducked. when the camera pannedthe horizon, the viewer reported seeing a panoramic skyline. when thecamera looked down to reveal that it was òstandingó on a plank extendedoff the roof of the building, the viewer panicked!in 1966, ivan sutherland moved from arpa to harvard university asan associate professor in applied mathematics. at arpa, sutherland hadhelped implement j.c.r. lickliderõs vision of humancomputer interaction, and he returned to academe to pursue his own efforts to extendhuman capabilities. sutherland and a student, robert sproull, turned theòremote realityó vision systems of the bell helicopter project into vr byreplacing the camera with computergenerated images.12 the first suchcomputer environment was no more than a wireframe room with thecardinal directionsñnorth, south, east, and westñinitialed on the walls.the viewer could òenteró the room by way of the òwestó door and turn tolook out windows in the other three directions. what was then called theheadmounted display later became known as vr.sutherlandõs experiments built on the network of personal and profunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.virtual reality comes of age237fessional contacts he had developed at mit and arpa. funding forsutherlandõs project came from a variety of military, academic, and industry sources. the central intelligence agency provided $80,000, andadditional funding was provided by arpa, the office of naval research,and bell laboratories. equipment was provided by bell helicopter. apdp1 computer was provided by the air force and an ultrasonic headposition acoustic sensor was provided by mit lincoln laboratory, alsounder an arpa contract.sutherland outlined a number of forms of interactive graphics thatlater became popular, including augmented reality, in which synthetic,computergenerated images are superimposed on a realistic image of ascene. he used this form of vr in attempting a practical medical application of the headmounted display. the first published research projectdeploying the 3d display addressed problems of representing hemodynamic flow in models of prosthetic heart valves. the idea was to generate the results of calculations involving physical laws of fluid mechanicsand a variety of numerical analysis techniques to generate a syntheticobject that one could walk toward and move into or around (greenfield etal., 1971).as sutherland later recalled, there was clearly no chance of immediately realizing his initial vision for the headmounted display. still, heviewed the project as an important òattention focuseró that òdefined a setof problems that motivated people for a number of years.ó even thoughvr was impossible at the time, it provided òa reason to go forward andpush the technology as hard as you could. spinoffs from that kind ofpursuit are its greatest value.ó13in sutherlandõs view, the most important spinoffs were the studentsand the personal and professional connections. sociologists of sciencetalk about the importance of òcore setsó of individuals who define theintellectual and technological direction of a domain. certainly the students trained by sutherland and dale evans, who founded utahõs computer science department, constitute one of the best examples of a coreset in the history of computer science.sutherland knew evans from his arpa days, and in 1968 they cofounded evans & sutherland computer corporation, which manufactured graphical display systems and built military flight and tank simulators under government contract. many commercial and military pilotswere trained on evans & sutherland flight simulators. a number of theirstudents worked on an arpasupported project on 3d graphics, andseveral worked at evans & sutherland on simulations. several of theoriginal harvard group also helped form the corporation, includingcharles seitz, who joined the utah faculty in 1970 and remained until1973, when he moved to california institute of technology and foundedfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.238funding a revolutionmyricom with dan cohen, another harvard alumnus who contributed tothe headmounted display. the interaction between research on basicproblems and development of hardware and software for military projectsat evans & sutherland was an important feature of work at utah.graphics hardware: risc technologycentral to advances in computer graphics and vr technology havebeen improvements in the underlying computer hardware that enhancedcapabilities and reduced costs. a significant advance, derived from bothindustrial and academic research, was the development of reduced instruction set computing (risc), starting in the mid1980s. by eliminatingcertain instructions based on careful quantitative analysis and emulatingthose instructions in software, risc processors can increase the performance of some computers. with risc processors, the performance ofgraphics hardware grew at about 55 percent per yearñresulting in a doubling of performance every 18 months.14the roots of risc lie in three research projects: the ibm corporationõs801, the university of california at berkeleyõs risc processor, andstanford universityõs millioninstructionspersecond (mips) processor.these architectures promised two to five times the performance of traditional machines. the berkeley and stanford projects were funded bydarpaõs highly ambitious very large scale integrated circuits (vlsi)program, which envisioned that integrated circuit (or chip) technologycould be made available to system designers, who had an overall view ofthe objectives and constraints of an entire hardware/software system.the vlsi program also developed the concept of the multichip wafer,which dramatically reduced costs. it expanded the availability of themetal oxide silicon implementation service, which created a multichipwafer from designs submitted electronically from multiple sites, allowinguniversity system designers to access stateoftheart silicon fabrication(see chapter 4).begun in the late 1970s, the ibm machine was designed as a minicomputer made from hundreds of chips, whereas the university projects wereboth microprocessors. john cocke, the father of the 801 design, receivedboth the a.m. turing award, the highest award in computer science andengineering, and the presidential medal of technology. the berkeleyproject, headed by david a. patterson, began in 1980. the berkeley groupbuilt two machines, risci and riscii. because the ibm project was notwidely known, the berkeley groupõs role in promoting the risc approachwas critical to the acceptance of the technology. the stanford mipsproject, begun in 1981, was led by john l. hennessy. mips is a highperformance risc, built in vlsi.15funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.virtual reality comes of age239both the stanford and the berkeley groups were interested in designing a simple machine that could be built as a microchip within the university environment. hennessy played a key role in transferring this technology to industry. during a sabbatical from stanford in 19841985, hecofounded mips computer systems (acquired by silicon graphics incorporated, in 1992), which specialized in the production of computers andchips based on these concepts.in 1986 the computer industry began to announce commercial processors based on risc technology. hewlettpackard company (hp) converted its existing minicomputer line to risc architectures. ibm neverturned the 801 into a product but adapted the ideas for a new lowendarchitecture that was incorporated into the ibm rtpc. this machinewas a commercial failure, but subsequent risc processors with whichibm has been involved (e.g., the apple/ibm/motorola powerpc) havebeen highly successful. in 1987 sun microsystems, inc. began deliveringmachines based on the sparc architecture, a derivative of the berkeleyriscii machine. in the view of many, it was sunõs success with riscbased workstations that convinced the remaining skeptics that risc wassignificant commercially. sunõs success sparked renewed interest at ibm,which announced a new risc architecture in 1990, as did digital equipment corporation in 1993. by 1995, risc had become the foundation of a$15 billion industry in computer workstations.risc computers advanced the field of interactive graphics and promoted the development of vr. silicon graphics incorporated (sgi), cofounded by james clark in 1982, was an early adopter of risc processorsand has been a leader in the recent development of highend graphics,including vr. clark joined the stanford engineering faculty in 1979 aftercompleting his doctorate with ivan sutherland on problems related to theheadmounted display. clark worked with hennessy and forest baskettin the stanford vlsi program and was supported by darpa in the geometry engine project, which attempted to harness the custom chip technology of mips to create costeffective, highperformance graphics systems. in 1981, clark received a patent for his geometry engineñthe 3dalgorithms built into the òfirmwareó that enable the unit to serve up realtime, interactive 3d graphics. the patent formed the basis of sgi. clarkalso invented the graphicslibrary, the graphics interface language usedto program sgiõs computers.silicon graphics is part of the commercial infrastructure for interactive graphics and vr that finally took root in the fertile ground laid byearly federal funding initiatives. companies such as sgi, evans &sutherland, hp, sun microsystems, and others have generated productsthat have enabled simulations of all sorts, scientific visualizations, andcomputeraided design programs for engineering. they also helped crefunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.240funding a revolutionate the film and video game industries, which have stimulated advancesin graphics by providing jobs, markets, and substantial research advances.16 in 1997, sgi reported revenues of $3.66 billion (mccracken,1997).17biomedical applicationsthe basic technologies developed through vr research have beenapplied in a variety of ways over the last several decades. one line ofwork led to applications of vr in biochemistry and medicine. this workbegan in the 1960s at the university of north carolina (unc) at chapelhill. the effort was launched by frederick brooks, who was inspired bysutherlandõs vision of the ultimate display as enabling a user to see, hear,and feel in the virtual world. flight simulators had incorporated soundand haptic feedback for some time. brooks selected molecular graphics asthe principal driving problem of his program. the goal of project grope,started by brooks in 1967, was to develop a haptic interface for molecularforces (brooks, 1990). the idea was that, if the force constraints on particular molecular combinations could be òfelt,ó then the designer of molecules could more quickly identify combinations of structures that coulddock with one another.gropei was a 2d system for continuous force fields. grope ii wasexpanded to a full sixdimensional (6d) system with three forces andthree torques. the computer available for grope ii in 1976 could produce forces in real time only for very simple world modelsña table top;seven childõs blocks; and the tongs of the argonne remote manipulator(arm), a large mechanical device. for realtime evaluation of molecularforces, brooks and his team estimated that 100 times more computingpower would be necessary. after building and testing the grope iisystem, the arm was mothballed and the project was put on hold forabout a decade until 1986, when vax computers became available. gropeiii, completed in 1988, was a full 6d system. brooks and his students thenwent on to build a fullmolecularforcefield evaluator and, with 12 experienced biochemists, tested it in grope iiib experiments in 1990. in theseexperiments, the users changed the structure of a drug molecule to get thebest fit to an active site by manipulating up to 12 twistable bonds.the test results on haptic visualization were extremely promising(ouhyoung et al., 1988, 1989; minsky et al., 1990). the subjects saw thehaptic display as a fast way to test many hypotheses in a short time andset up and guide batch computations. the greatest promise of the technique, however, was not in saving time but in improving situationalawareness. chemists using the method reported better comprehension ofthe force fields in the active site and of exactly why each particular candifunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.virtual reality comes of age241date drug docked well or poorly. based on this improved grasp of theproblem, users could form new hypotheses and ideas for new candidatedrugs.the docking station is only one of the projects pursued by brooksõsgroup at the unc graphics laboratory. the virtual world envisioned bysutherland would enable scientists or engineers to become immersed inthe world rather than simply view a mathematical abstraction through awindow from outside. the unc group has pursued this idea through thedevelopment of what brooks calls òintelligenceamplifying systems.óvirtual worlds are a subclass of intelligenceamplifying systems, whichare expert systems that tie the mind in with the computer, rather thansimply substitute a computer for a human.in 1970, brooksõs laboratory was designated as an nih research resource in molecular graphics, with the goal of developing virtual worldsof technology to help biochemists and molecular biologists visualize andunderstand their data and models. however, because of budget cutbacksand a reorientation of the program, support from the nih national center for research resources has declined by more than 50 percent since1979. fortunately, a variety of other federal agencies have continued tosupport the virtual worlds project since the early 1980s. these agenciesinclude nihõs national cancer institute, darpa, and the nsf. collaboration with the air force institute of technology on imagedelivery systems has also been an important part of the work at unc since 1983 (u.s.congress, 1991). during the 1990s, unc has collaborated with industrysponsors such as hp to develop new architectures incorporating 3dgraphics and volumerendering capabilities into desktop computers (hplater decided not to commercialize the technology).18since 1985, nsf funding has enabled unc to pursue the pixelplanesproject, with the goal of constructing an imagegeneration system capableof rendering 1.8 million polygons per second and a headmounted display system with a lagtime under 50 milliseconds. this project is connected with grope and a large software project for mathematical modeling of molecules, human anatomy, and architecture. it is also linked tovistanet, in which unc and several collaborators are testing highspeed network technology for joining a radiologist who is planning cancertherapy with a virtual world system in his clinic, a cray supercomputer atthe north carolina supercomputer center, and the pixelplanes graphicsengine in brooksõs laboratory.with pixelplanes and the new generation of headmounted displays,the unc group has constructed a prototype system that enables the notions explored in grope to be transformed into a wearable virtualworldworkstation. for example, instead of viewing a drug molecule through awindow on a large screen, the chemist wearing a headmounted displayfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.242funding a revolutionsits at a computer workstation with the molecule suspended in front ofhim in space. the chemist can pick it up, examine it from all sides, evenzoom into remote interior dimensions of the molecule. instead of anarm gripper, the chemist wears a forcefeedback exoskeleton that enables the right hand to òfeeló the spring forces of the molecule beingwarped and shaped by the left hand.in a similar use of this technology, a surgeon can work on a simulation of a delicate procedure to be performed remotely. a variation on andmodification of the approach taken in the grope project is being pursued by unc medical researcher james chung, who is designing virtualworld interfaces for radiology. one approach is data fusion, in which aphysician wearing a headmounted display in an examination room could,for example, view a fetus by ultrasound imaging superimposed and projected in 3d by a workstation. the physician would see these data fusedwith the body of the patient. in related experiments with mri and ctscan data fusion, a surgeon has been able to plan localized radiation treatment of a tumor.in the unc case, funding of vr research by several different agencieshas sustained the laboratory through changing federal priorities and enabled it to pursue a complementary mix of alternative approaches, basic andapplied research, and prototype development. although federal agencieshave different mission objectives, a synergy evolved between the variousprojects, and a common base of knowledge and personnel was established. over the years, the governmentõs investment has greatly expandedthe range of tools available to both the research community and industry.virtual reality and entertainment:toward a commercial industryat a 1991 senate hearing, several vr pioneers noted that commercialinterests, with their need for quick returns, could not merge the substantially different technologies needed to create virtual worlds, particularlywhile the technologies remained at precompetitive stages for so manyyears (u.s. congress, 1991). but a sustained mixture of government, industry, and universitybased r&d and the synergistic development ofseveral applications has helped bring vr to the marketplace. in particular, the nexus between public research and privately developed entertainment systems made vr technology more affordable and scaled it up forlarge consumer markets, thereby promoting the rapid adoption and widespread use of imaging technology in science and medicine.an example is renderman, developed by pixar animation studios.edwin catmull, an alumnus of the utah graphics program, joined alvyray smith at lucasfilm in 1979. catmull and smith had worked togetherfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.virtual reality comes of age243at the new york institute of technology (nyit). to realize the dream ofconstructing an entire film from computergenerated material, smith andcatmull recruited a number of young computer graphics talents tolucasfilm. among them was loren carpenter from the boeing company, who had studied the research of mandelbrot and then modified itto create realistic fractal images. in 1981, carpenter wrote the first renderer for lucasfilm, reyes (renders everything you ever saw), whichwas the beginning of renderman.in 1986, the computer graphics division of lucasfilmõs industriallight and magic was spun off as pixar, with catmull as president andsmith as vice president. under their direction, pixar worked on developing a rendering computer. also joining the reyes machine group atpixar in 1986 was patrick hanrahan, who worked with robert drebin andloren carpenter in developing the first volumerendering algorithms forthe pixar image computer (drebin et al., 1988). these algorithms createdimages directly from 3d arrays without the typical intermediate steps ofconverting to standard surface representations, such as polygons. hanrahanwas the principal architect of the interface and was responsible for therendering software and the graphics architecture of renderman.the rendering interface evolved into the renderman standard nowwidely used in the movie industry. this standard describes the information the computer needs to render a 3d sceneñthe objects, light sources,cameras, and atmospheric effects. once a scene is converted to a rendermanfile, it can be rendered on a variety of systems, from macintoshes to personal computers to sgi workstations. this opened up many possibilitiesfor 3d computer graphics software developers. renderman was used increating toy story, the first featurelength computeranimated film; thedinosaurs in jurassic park; and the cyborg in terminator 2.this powerful tool also has contributed to visualization and volumerendering in a number of fields of science, engineering, and medicine. inaddition, the hardware and software components and the individualsinvolved have circulated between industry and academe. pat hanrahan,after moving from nyit to pixar, moved back to an academic laboratory,first as an associate professor at princeton university, and more recentlyas a professor at stanford university, where he has contributed to severalareas of graphics. one was the development of applications for theresponsive workbench, a 3d, interactive virtual environment workspacefor scientific visualization, architecture, and medicine. the workbenchhas been a cooperative project between stanford and the german institutefor information design, supported by grants from interval research corporation, darpa (for visualization of complex systems), and nasaames (for virtual windtunnel). silicon graphics and fakespace incorporated donated equipment.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.244funding a revolutionthe right mix: virtual reality in the 1990scontinued improvements in computer graphics in processors andnew chip architectures have stimulated the growth of commercial markets for vr technology, fueling the revenues of companies such as sgiand cutting the prices of graphics workstations drastically. the resultingimprovements in the priceperformance ratios for computer graphics technologies have, in turn, increased demand for these products. furthermore, potential markets for multimedia products have driven the searchfor new architectures for image caching and compression techniques thatgreatly reduce bandwidth and memory requirements. this convergenceof highend computer architectures, graphicsrendering hardware, andsoftware with lowend commercial markets for computer graphics expands the opportunities for the use of vr technologies in a variety ofcommercial applications. it also motivates further technical advancesthat benefit commercial and military customers alike. as sgi chief executive officer ed mccracken once explained, òour entertainment customersdrive our technological innovation. and technological innovation is thefoundation of silicon graphics.ó19as civilian research has proceeded and the dod has come underincreasing pressure to operate effectively on reduced budgets, the traditional relationship between military and commercial vr research projectshas changed. the dod continues to be a major consumer of vr technology, but now it can draw increasingly on the commercial technologies. anumber of reforms have been enacted to enable the dod to procure products from the commercial industrial base more easily. a number of defense contractors have also diversified into commercial applications ofvr technology (see box 10.4). in 1998, the dod expected to spend morethan $2.5 billion on programs for modeling and simulation (u.s. department of defense, 1997). such considerable resources will likely stimulatefurther development of graphics and vr technologies. directive 5000.1(u.s. department of defense, 1996) mandates that models and simulations be required of all proposed systems, and that òrepresentations ofproposed systems (virtual prototypes) shall be embedded in realistic, synthetic environments to support the various phases of the acquisition process, from requirements determination and initial concept exploration tothe manufacturing and testing of new systems, and related training.ómore interestingly, attempts have been made to better coordinate theefforts of military and commercial research programs in vr technologies.the defense modeling and simulation office, for example, asked thenational research council to examine areas of mutual interest to thedefense modeling and simulation community and the entertainment industry. the resulting report identified five broad areas of common interfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.virtual reality comes of age245box 10.4real3d emerges from militarycommercial linkagereal3d, one of several companies that offers realtime threedimensional (3d)graphics products for commercial systems, traces its origins to the first ge aerospacevisual docking simulator for the apollo lunar landings. in 1991, ge aerospacebegan exploring commercial applications of its realtime 3d graphics technology,which led to a contract with sega enterprises, limited, of japan, which was interested in improving its arcade graphics hardware so that the games would present morerealistic images. ge aerospace adapted a miniaturized version of its realtime 3dgraphics technology specifically for segaõs model 2 and model 3 arcade systems,which incorporated new algorithms that provided a visual experience far exceedingexpectations.1 to date, sega has shipped more than 200,000 systems that includewhat is today real3d technology.in 1993, ge aerospace was acquired by martin marietta, another leader in thefield of visual simulation. martin marietta not only advocated expanding the relationship with sega but also encouraged further research and analysis to look at othercommercial markets, such as personal computers (pcs) and graphics workstations.in 1995, martin marietta merged with lockheed corporation and shortly thereafterlaunched real3d to focus solely on developing and producing 3d graphics productsfor commercial markets. to that end, in november 1996, a strategic alliance wasformed between real3d and chips and technologies incorporated, aimed at sellingreal3d r3d/100 twochip graphics accelerators to the pc industry and bringingworldclass 3d applications to professionals who use the windows nt environment.2 finally, in december 1997, lockheed martin established real3d incorporated as an independent company and announced that intel corporation had purchased a 20 percent minority stake in real3d.real3d thus builds on more than three decades of experience in realtime 3dgraphics hardware and software going back to the apollo visual docking simulator.this experience has led to more than 40 key patents on 3d graphics hardware andsoftware. strategic relationships with various companies provide opportunities totransition highend graphics technology from leadingedge research environments tothe desktops of physicians, engineers, and scientists. conversely, the company mayalso be able to transfer technology developed for video games to developers of military training simulators.1see the discussion by jeffrey potter in cstb (1997b), pp. 163164. additional information is available online at <http://www.real3d.com/sega.html>.2the r3d/100 chipset directly interfaces with microsoftcompliant application programming interfaces, such as opengl.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.246funding a revolutionest: fundamental technologies for immersive environments, networkedsimulation, standards for interoperability across systems, computergenerated characters, and tools for creating simulated environments (computer science and telecommunications board, 1997b). already, the dodhas work under way in many of these areas. it is exploring ways ofimproving representations of human behaviors in synthetic environmentsand has developed a highlevel architecture (hla) to facilitate interoperability of distributed simulation systems.20 commercial entertainment companies are also exploring related areas of research and maybenefit fromñand contribute toñdefenserelated activities.the growing linkages between the commercial and military vr communities are also apparent in the movement of experts between the twosectors. for example, robert jacobs, director and president of illusionincorporated, a company that derives some 80 percent of its revenuesfrom the commercial entertainment industry, is an inventor of darpaõsdefense simulation network (simnet) program and has been a technical contributor to most of the related training programs. eric haseltine,now vicepresident and chief scientist of research and development atwalt disney imagineering, was previously an executive at hughes aircraft company, a defense contractor he joined after completing a postdoctoral fellowship in neuroanatomy and a doctorate in physiologicalpsychology. real3d senior software engineer steven woodcock beganhis career developing game simulations for martin marietta, where he hasbeen responsible for weapons code development, testing, integration, anddocumentation for the advanced realtime gaming universal simulation (argus).21 argus is a realtime, distributed, interactive commandandcontrol simulation focusing on ballistic missile defense and theatermissile defense, running on a network consisting of a cray2 supercomputer and more than 50 sgi workstations. woodcock has noted thathis martin marietta experience in distributed applications, realtime simulations, and artificial intelligence has proven invaluable in the realtime,3d, multiplayer environments of games he has been designing recently.these examples demonstrate the complex and changing relationshipbetween federally funded research and commercial innovation. yet evenas the commercial industry has grown, federal funding has played a critical role in advancing technologies to serve the governmentõs own needsas well as supporting underlying fundamental technologies. indeed,darpa, the nsf, department of energy (doe), and other federal agencies continue to invest in vr and graphicsrelated research. the nsfõsfunding of the science and technology research center in computergraphics and scientific visualization supports collaborative research onfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.virtual reality comes of age247computer graphics among participants from five universities. the doeõsadvanced strategic computing initiative, although aimed at supportingdevelopment of models of nuclear weapons, includes funding for university research on fundamental techniques for computer graphics and scientific visualization. such programs may ultimately help build a selfsustaining technological infrastructure for vr.lessons from historyfederal funding has played a critical role in developing vr technology. it funded early, precompetitive research on topics such as crts thatindustry had few incentives to support. as the technology advanced andpractical applications emerged, federal funding continued to complementindustry support, as illustrated by work in headmounted displays andthe continuing government support of the field after the collapse of atari.federal support has enabled universities to create and maintain leadingedge computer graphics and vr research centers, which have contributedto the information revolution. industry sectors and companies that generate billions of dollars in annual revenues (sgi is but one example) tracetheir roots to federally funded research.a primary benefit of federal funding, particularly of university research, has been the creation of human resources that have carried out,and driven advances in, vr research. a number of graduate students andacademic researchers who received federal support have made significant contributions to the field and have established leading companies(see table 10.1).research in computer graphics and vr has benefited from multiplesources of federal support, which have enabled the simultaneous pursuitof various approaches to technical problems, funded a complementarymix of basic and applied research, developed a range of applications,provided a funding safety net that has sustained emerging technologydespite changes in federal mission priorities, and offered the flexibilityneeded to pursue promising new ideas. the success of this approach isevidenced by the rich selection of vr products now available across theaerospace, military, industrial, medical, education, and entertainment sectors.finally, this case study demonstrates that advances in computing andcommunications seldom proceed along a linear or predictable path.progress in vr technologies has benefited from varied interactions amonggovernment, universities, and industry and from the fusion of ideas fromdifferent areas of research, such as computer graphics, computer architectures, and military simulation.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.248funding a revolutionnotes1.such statements are invariably subject to the òback to the ancientsó process of identifying precursors, such as edwin linkõs work on vehicle simulationin the 1920s. see ellis (1991, 1994).2.this project and others are listed on the advanced displays and spatialperception laboratory page on the nasa ames research center web site at<http://duchamp.arc.nasa.gov:80/adsp.html>.3.see rowell (1998) and an article posted on the silicon graphics web siteat <http://www.sgi.com/features/1998/aug/chrysler/>.4.the contributions of this center to scientific visualization and work in vrare discussed by cruzneira et al. (1992).5.these estimates are based on data compiled from nsfõs annual reportsummary of grants and awards for the years cited.6.another noteworthy graduate of the utah program in the late 1970s wasgary demos, who started several major computer graphics production companies and had a big impact on the introduction of computer graphics technologyin the film industry.7.the equipment was installed at the manned spacecraft center in houston.8.jeffrey potter, intel corporation, as quoted in cstb (1997b). for an evaluation of one of the ge systems, see brown et al. (1994). this document is alsoavailable online at <http://tspg.wpafb.af.mil/programs/documents/asctr94.htm>.9.this algorithm could run on sun, vax, or ibm systems with conventionalgraphics displays, such as the ge graphicon 700. additional information aboutlorensenõs work is available online at <http://www.crd.ge.com/~lorensen/>, asis information about the ge computer graphics systems program at <http://www.crd.ge.com/esl/cgsp/index.html>.10.like the spellcheck program on a word processor, which helps writersavoid typographical errors, the aim of this project is to develop a cad programthat provides òanother set of ôeyesõ in reviewing images, alerting a radiologist tolook closer at specific areas of an image,ó according to dr. martin j. lipton, chairman of the radiology department at the university of chicago medical center,where the cad technology is being developed. òge and eg&g sign collaboration pact to produce digital xray detectors,ó 21 august, 1997, available onlineat <http://www.ge.com/medical/media/msxrldd>.11.along with triple i, magi was involved in making the film tron.12.other headmounted display projects using a television camera systemwere undertaken by philco in the early 1960s, as discussed by ellis (1996).13.ivan e. sutherland in òvirtual reality before it had that name,ó a videotaped lecture before the bay area computer history association.14.see national research council (1995), especially figure 8.4, òthe historyof workstation computation and memory.ó15.hennessy et al. (1981) published a description of the stanford mips machine, also developed under darpa sponsorship.16.scott fisher, òcurrent status of vr and entertainment,ó presentation to thenational research councilõs committee on virtual reality research and development, woods hole, ma, august, 1993, as cited in national research council (1995).funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.virtual reality comes of age24917.also see the comparative financial data reported for 1993 through 1997 at<http://www.sgi.com/companyinfo/investors/annualreport/97/finselinfo.html>.18.this collaboration is described on the web site of pixelfusion at <http://www.pixelfusion.com>.19.see mccracken (1997). mccracken also noted: òwhile there have beenincredible advances across many areas of science and technologyñthe new craylinkarchitecture for supercomputers, new improvements on the space shuttle, sheepcloningñno advance has been more prolific, more ubiquitous, more widereachingthan consumeroriented entertainment developments.ó20.the program description is available online at <http://www.stricom.army.mil/stricom/pmads/adstii/>.21.steven woodcockõs biography is available online at <http://www.cris.com/~swoodcoc/stevegameresume.html>. also see wall street journal interactive edition(may 19, 1997). also see coco (1997), which is available online at <http://www.cgw.com/cgw/archives/1997/07/07story1.html>.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.250bibliographyaho, a.v., and j.d. ullman. 1992. foundations of computer science. w.h. freeman, sanfrancisco, calif.aho, a.v., r. sethi, and j.d. ullman. 1986. compilers: principles, techniques, and tools.addisonwesley, reading, mass.aitken, hugh g.j. 1985. the continuous wave: technology and american radio, 19001932.princeton university press, princeton, n.j.akera, atsushi. 1996. òcomputers and systems analysis: transforming research strategies at the national bureau of standards,ó dibner symposium on the spread of thesystems approach. dibner institute for the history of science and technology, cambridge, mass., april.alic, john, lewis m. branscomb, and gerald l. epstein. 1992. beyond spinoff: military andcommercial technologies in a changing world. harvard business school press, boston,mass.amarel, saul. 1988. òcurrent ai research,ó p. 268 in expert systems and artificial intelligence: applications and management, thomas c. bartee, ed. howard w. sams & co.,indianapolis, ind.amarel, saul. 1989. interview with arthur norberg, new brunswick, n.j., charles babbageinstitute oral history collection, university of minnesota, minneapolis, october 5.american association for artificial intelligence (aaai). 1994. a report to arpa on twentyfirst century intelligent systems, barbara grosz and randall davis, eds. aaai, menlopark, calif. available online at <http://www.aaai.org/policy/papers/arpareport.html>.anderson, margo j. 1988. the american census: a social history. yale university press,new haven, conn.andrews, gregory r. 1997. ò1996 cra taulbee survey: grad, undergrad student enrollments up,ó computing research news, march, pp. 59.arrow, kenneth. 1962. òeconomic welfare and the allocation of resources for invention,óthe rate and direction of inventive activity. princeton university press, princeton, n.j.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.bibliography251aspray, william, and bernard o. williams. 1994. òarming american scientists: nsf andthe provision of scientific computing facilities for universities, 19501973,ó annals ofthe history of computing 16(4):6074.aspray, william, andrew goldstein, and bernard williams. 1996. òthe social and intellectual shaping of a new mathematical discipline: the role of the national sciencefoundation in the rise of theoretical computer science,ó vita mathematica, maanotes no. 40, ronald calinger, ed. mathematical association of america, washington, d.c.association for computing machinery (acm) sigplan. 1978. a history of programminglanguages, proceedings of the acm conference on programming languages. academicpress, los angeles, calif.association for computing machinery (acm) curriculum committee on computer science curriculum. 1979. òcurriculum õ78: recommendations for the undergraduateprogram in computer science,ó communications of the acm 22:147166.association for computing machinery (acm). 1993. òbefore the altair: the history ofpersonal computing,ó communications of the acm 36(9):2733.astrahan, m.m., m.w. blasgen, d.d. chamberlin, k.p. eswaran, j.n. gray, p.p. griffiths,w.f. king, r.a. lorie, p.r. mcjones, j.w. mehl, g.r. putzolu, i.l. traiger, b.w. wade,and v. watson. 1976. òsystem r: relational approach to database management,óacm transactions on database systems 1(2):97137.bachman, c. 1973. òthe programmer as navigator: 1973 acm turing award lecture,ócommunications of the acm 16(11):653658.backus, john. 1979. òthe history of fortran i, ii, and iii,ó annals of the history ofcomputing 1(1):2137.baker, james k. 1975. òthe dragon systemñan overview,ó ieee transactions on acoustics, speech, and signal processing 23(1):2429.barber associates, richard j. 1975. the advanced research projects agency 19581974, reportprepared for the advanced projects research agency under contract mda 90374c0096. defense technical information center, springfield, va., december.barr, avron, and edward a. feigenbaum, eds. 1981. the handbook of artificial intelligence,vol. 1. heuristech press, stanford, calif.bashe, charles j., lyle r. johnson, john h. palmer, and emerson w. pugh. 1986. ibmõsearly computers. mit press, cambridge, mass.baum, claude. 1981. the system builders: the story of sdc. system development corporation, santa monica, calif.baum, l.e., and j.a. eagon. 1967. òan inequality with applications to statistical estimation for probabilistic functions of markov processes and to a model of ecology,ó bulletin of the american mathematical society 73:360362.bhushan, abhay. 1972. file transfer protocol. internet request for comment 354. july.rfc editor, information sciences institute, university of southern california, los angeles. available online at <http://www.rfceditor.org/rfc.html>.blane, erwin, et al. 1997. sizing intercompany commerce. forrester research, cambridge,mass., july.branscomb, lewis, et al. 1997. investing in innovation: toward a consensus strategy forfederal technology policy, april 24. available online at <http://www.ksg.harvard.edu/iip/techproj/invest.html>.braudel, fernand. 1972. the mediterranean and the mediterranean world in the age of philip ii.harper & row, new york.brooks, frederick p. 1990. òproject grope: haptic displays for scientific visualization,óacm computer graphics 24(4):177185.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.252funding a revolutionbrown, james e., timothy j. lincourt, melissa j. leos, et al. 1994. visual system operationalevaluation, asctr945030. available online at <http://tspg.wpafb.af.mil/programs/documents/asctr94.htm>.brynjolfsson, erik, and lorin hitt. 1996. òparadox lost? firmlevel evidence of highreturns to information systems spending,ó management science 42(4):541558.buchanan, bruce g., and edward h. shortliffe. 1984. rulebased expert systems: the mycinexperiments of the stanford heuristic programming project. addisonwesley, reading,mass.buderi, robert. 1998. òbell labs is dead. long live bell labs,ó technology review 101(5):5057.bureau of the census, u.s. department of commerce, economics and statistics administration. 1997. current industrial reports: communication equipment including telephone,telegraph, and other electronic systems and equipmentñ1996, ma36p(96)1. u.s. government printing office, washington, d.c.bush, vannevar. 1945a. science, the endless frontier. u.s. government printing office,washington, d.c.bush, vannevar. 1945b. òas we may think,ó atlantic monthly 176(july):101108.campbellkelly, martin, and william aspray. 1996. computer: a history of the informationmachine. basic books, new york.cardenas, a. 1979. database management systems. allyn and bacon, boston, mass.chamberlin, d., et al. 1981. òa history and evaluation of system r,ó communications of theacm 24(10):632646.chomsky, noam. 1956. òthree models for the description of language,ó ire transactionson information theory 2(3):113124.church, a. 1936. òan unsolvable problem of elementary number theory,ó americanjournal of mathematics 58:345363.clancy, m.j., and m.c. linn. 1995. designing pascal solutions. w.h. freeman, san francisco, calif.coco, donna. 1997. òcreating intelligent creatures: game developers are turning to aito give their characters personalities and to distinguish their titles from the pack,ócomputer graphics world 20(7):2228. available online at <http://www.cgw.com/cgw/archives/1997/07/07story1.html>.codd, edgar f. 1970. òa relational model of data for large shared data banks,ó communications of the acm 13(6):377387.codd, edgar f. 1982. òrelational database: a practical foundation for productivity (1983acm turing award lecture),ó communications of the acm 25(2):109117.cohen, arnold. 1983. interview with james ross, charles babbage institute center for thehistory of information processing, university of minnesota, minneapolis, januarymarch.cohen, arnold a., and erwin tomash. 1979. òthe birth of an era: engineering researchassociates inc.,ó annals of the history of computing 1(october):83100.collins, martin j. 1998. òplanning for modern war: rand and the air force, 19451950,óph.d. dissertation, university of maryland, college park.comer, d. 1984. operating system design, the xinu approach. prenticehall, englewoodcliffs, n.j.committee on science, u.s. house of representatives. 1998. unlocking our future: towarda new national science policy, a report to congress, september 24. available online at<http://www.house.gov/science/sciencepolicyreport.htm>.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.bibliography253computer science and technology board, national research council. 1982. paper 8, òresearch in data processing: the primacy of practice,ó pp. 6770 in roles of industry andthe university in computer research and development. national academy press, washington, d.c.computer science and telecommunications board (cstb), national research council.1992. computing the future: a broader research agenda for computer science and engineering. national academy press, washington, d.c.computer science and telecommunications board (cstb), national research council.1994. realizing the information future: the internet and beyond. national academypress, washington, d.c.computer science and telecommunications board (cstb), national research council.1995a. evolving the highperformance computing and communications initiative to support the nationõs information infrastructure. national academy press, washington, d.c.computer science and telecommunications board (cstb), national research council.1995b. information technology in the service sector: a twentyfirst century lever. national academy press, washington, d.c.computer science and telecommunications board (cstb). national research council.1996. cryptographyõs role in securing the information society. national academy press,washington, d.c.computer science and telecommunications board (cstb), national research council.1997a. ada and beyond: software policies for the department of defense. national academy press, washington, d.c.computer science and telecommunications board (cstb). national research council.1997b. modeling and simulation: linking entertainment and defense. national academypress, washington, d.c.conway, lynn. 1981. the mpc adventures: experiences with the generation of vlsi designand implementation methodologies, technical report vlsi812. xerox palo alto researchcenter, palo alto, calif.cook, s.a. 1971. òthe complexity of theorem proving procedures,ó pp. 151158 in proceedings of the third annual acm symposium on the theory of computing. acm press,new york.coons, steven a. 1967. surfaces for computeraided design of space forms, project macreport mactr41. massachusetts institute of technology, cambridge, mass.crevier, daniel. 1994. ai. basic books, new york.crout, p.d. 1941. òa short method for evaluating determinants and solving systems oflinear equations with real or complex coefficients,ó transactions of the american institute of electrical engineers 60:12351240.cruzneira, c., d.j. sandin, t.a. defanti, r.v. kenyon, and j.c. hart. 1992. òthe cave:audio visual experience automatic virtual environment,ó communications of the acm35(6):6572.cutter, mary e. 1997. òapproval of the ôpartnerships for advanced computational infrastructureõ program awards.ó memorandum to the national science board, nsb 9750. available online at <http://www.cise.nsf.gov/acir/nsbmemo.html>.dahl, o.j., e.w. dijkstra, and c.a.r. hoare. 1972. structured programming. academicpress, new york.dale, n.b., and c. weems. 1992. introduction to pascal and structured design. d.c. heath,lexington, mass.dasgupta, p., and p.a. david. 1987. òinformation disclosure and the economics of scienceand technology,ó arrow and the ascent of modern economic theory, g. feiwel, ed. newyork university press, new york.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.254funding a revolutiondasgupta, p., and p.a. david. 1994. òtoward a new economics of science,ó research policy23:487521.date, c. 1986. relational databases: selected writing. addisonwesley, reading, mass.david, p.a., and d. foray. 1996. òaccessing and expanding the science and technologyknowledge base,ó science, technology, industry review 16. organisation for economiccooperation and development, paris.david, p.a., d.c. mowery, and w.e. steinmueller. 1992. òa framework for evaluatingeconomics payoffs from basic research,ó economics of innovation and new technologies2(1):7390.davies, d.w., k.a. bartlett, r.a. scantlebury, and p. t. wilkinson. 1967. òa digital communication network for computers giving rapid response at remote terminals,óproceedings of the acm symposium on operating system principles. association for computing machinery, new york.davis, m. 1965. the undecidable. raven press, hewlett, n.y.de forest, lee. 1950. father of radio: the autobiography of lee de forest. wilcox & follet co.,chicago.defense advanced research projects agency (darpa). 1983. strategic computingñnewgeneration technology: a strategic plan for its development and application to criticalproblems in defense. darpa, arlington, va., october 28.defense advanced research projects agency (darpa). 1997. darpa technology transition. darpa, arlington, va., january.defense advanced research projects agency (darpa). 1998. fy 1999 darpa descriptivesummaries. darpa, arlington, va. available online at <http://www.darpa.mil/budget.html>.denicoff, marvin. 1988. òai development and the office of naval research,ó p. 280 inexpert systems and artificial intelligence: applications and management, thomas c. bartee,ed. howard w. sams & co., indianapolis, ind.denning, d.e.r. 1982. cryptography and data security. addisonwesley, reading, mass.diffie, w. 1996. foreword in b. schneier, applied cryptography. john wiley & sons, newyork.diffie, w., and m.e. hellman. 1976. ònew directions in cryptography,ó ieee transactionson information theory 22:644654.dijkstra, e.w. 1976. a discipline of programming. prenticehall, englewood cliffs, n.j.drebin, r.a., l. carpenter, and p. hanrahan. 1988. òvolume rendering,ó siggraph 88conference proceedings, computer graphics 22(4):6574.edwards, paul. 1996. the closed world: computers and the politics of discourse in cold waramerica. mit press, cambridge, mass.ehlers, vernon. 1998. u.s. representative vern ehlers as cited in fyi: the american instituteof physics bulletin of science policy news 32 (february 20). available online at <http://www.aip.org/enews/fyi/1998/fyi98.032.htm>.ellis, stephen r. 1991. ònature and origins of virtual environments: a bibliographicalessay,ó computer systems in engineering 2(4):321347.ellis, stephen r. 1994. òwhat are virtual environments?ó ieee computer graphics andapplications 14(january):1722.ellis, stephen r. 1996. òvirtual environments and environmental instruments,ó pp. 1151in simulated and virtual realities, k. carr and r. england, eds. taylor & francis, london.engelbart, douglas c. 1986. òthe augmented knowledge workshop,ó proceedings of theacm conference on the history of personal workstations. acm press, new york.engineering research associates (era). 1950. high speed computing devices, w.w. stifler,jr., ed. mcgrawhill, new york.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.bibliography255englemore, r.s., et al. 1988. òhearsayii,ó p. 25 in blackboard systems, r. englemore and t.morgan, eds. addisonwesley, boston. cited in reed, sidney g., richard h. vanatta, and seymour j. deitchman, 1990, pp. 2128 in darpa technical accomplishments,volume 1, ida paper p2192, institute for defense analyses, alexandria, va., february.fano, robert m. 1979. òproject mac,ó encyclopedia of computer science and technology, vol.12, jack belzer, albert g. holzman, and allen kent, eds. marcel dekker, new york.feigenbaum, edward, pamela mccorduck, and h. penny nii. 1988. the rise of the expertcompany: how visionary companies are using artificial intelligence to achieve higherproductivity and profits. times books, new york.flamm, kenneth. 1987. targeting the computer: government support and international competition. brookings institution, washington, d.c.flamm, kenneth. 1988. creating the computer: government, industry, and high technology.brookings institution, washington d.c.fleck, james. 1982. òdevelopment and establishment in artificial intelligence,ó pp. 169217 in scientific establishments and hierarchies, vol. 6 of sociology of the sciences, norbertelias, ed. d. reidel publishing company, dordrecht, holland.floyd, r.w. 1967. òassigning meanings to programs,ó pp. 1932 in mathematical aspects ofcomputer science, j.t. schwartz, ed., vol. 19 of american mathematics society proceedingsof symposia in applied mathematics. american mathematical society, providence, r.i.freeman, eva c., ed. 1995. mit lincoln laboratory: technology in the national interest. lincoln laboratory, massachusetts institute of technology, lexington, mass.friedel, robert, and paul israel, with bernard s. finn. 1986. edisonõs electric light: biographyof an invention. rutgers university press, new brunswick, n.j.friend, e.h. 1956. òsorting on electronic computer systems,ó journal of the association forcomputing machinery 3:134163.fry, j.p., and e.h. sibley. 1974. òevolution of database management systems,ó computingsurveys 8(1):742. reprinted in l. laplante, ed., 1996, great papers in computer science,ieee press, new york.garey, m.r., and d.s. johnson. 1979. computers and intractability. w.h. freeman, sanfrancisco, calif.garfinkel, simpson l. 1998. òenter the dragon,ó technology review 101(5):5864.garland, s.j. 1986. introduction to computer science with applications in pascal. addisonwesley, reading, mass.general electric. 1997. òge and eg&g sign collaboration pact to produce digital xraydetectors,ó august 21. available online at <http://www.ge.com/medical/media/msxrldd>.gleeson, robert e., and steven schlossman. 1994. òthe many faces of the new look: theuniversity of virginia, carnegie tech, and the reform of american management education in the postwar era,ó the beginnings of graduate management education in theunited states. graduate management admission council, santa monica, calif.godel, k. 1931. òuber formal unentscheidbare satze der principia mathematica undverwander systeme,ó monatshefte fur mathematik und physik 38:173198.goldstein, nance. 1992. òdefense advanced research project agencyõs role in artificialintelligence r&d: case study of the military as the national agent for technologicaland industrial change,ó defense analysis 8(1):6180.goldstine, herman h. 1972. the computer: from pascal to von neumann. princeton university press, princeton, n.j.goldstine, herman. 1980. interview with nancy stern, oh 18, charles babbage institutecenter for the history of information processing, university of minnesota, minneapolis, august 11.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.256funding a revolutiongouraud, h. 1971. òcontinuous shading of curved surfaces,ó ieee transactions on computers c20(6):623629.green c. 1988. òai during iptoõs middle years,ó p. 241 in expert systems and artificialintelligence: applications and management, thomas c. bartee, ed. howard w. sams &co., indianapolis, ind.greenfield, harvey, donald vickers, ivan sutherland, willem kolff, et al. 1971. òmovingcomputer graphic images seen from inside the vascular system,ó transactions of theamerican society of artificial internal organs 17:381385.gries, d. 1987. the science of programming. springerverlag, new york.grossman, gene m., and elhanan helpman. 1991. innovation and growth in the globaleconomy. mit press. cambridge, mass.gruenberger, f.j. 1979. òhistory of the johnniac,ó annals of the history of computing1(1):4964.hafner, katie, and matthew lyon. 1996. where wizards stay up late: the origins of theinternet. simon & schuster, new york.hartmanis, j., and r.e. stearns. 1964. òcomputational complexity of recursive sequences,ópp. 8290 in proceedings of the 5th annual symposium on switching theory and logicaldesign. ieee press, new york.hayesroth, frederick. 1997. òartificial intelligence: what works and what doesnõt?ó aimagazine 18(2):100.hehner, e.c. 1993. a practical theory of programming. springerverlag, new york.hennessy, john l., and d.a. patterson. 1990. computer architecture: a quantitative approach. morgan kaufman publishers, san mateo, calif.hennessy, john l., n. jouppi, f. baskett, and f. gill. 1981. òmips: a vlsi processorarchitecture,ó in proceedings of carnegie mellon university (cmu). computer sciencepress, rockville, md.high performance computing modernization office, u.s. department of defense. 1995.contributions to dod mission success from high performance computingñmarch 1995,dod hpcm pub 95001. available from the defense technical information center,springfield, va.hoare, c.a.r. 1969. òan axiomatic basis for computer programming,ó communications ofthe acm 12:576581.hochbaum, dorit s., ed. 1997. approximation algorithms for nphard problems. pws publishing company, boston, mass.holzmann, g.j. 1991. the design and validation of computer protocols. prenticehall,englewood cliffs, n.j.hounshell, david a. 1997. òthe cold war, rand, and the generation of knowledge,19461962,ó historical studies in the physical sciences 27(2):131.huffman, d.a. 1954. òthe synthesis of sequential switching machines,ó journal of thefranklin institute 257:161190, 275303.hughes, thomas. 1990. american genesis: a century of invention and technological enthusiasm, 18701970. penguin, new york.hughes, thomas. 1998. rescuing prometheus. pantheon books, new york.hurd, cuthbert c. 1994. interview with robert seidel, palo alto, calif., charles babbageinstitute center for the history of information processing, university of minnesota,minneapolis, november 18.huskey, harry d. 1980. òthe standards western automatic computer,ó annals of thehistory of computing 2(2):111121.information technology industry council (iti). 1997. 1996 iti information technology industry databook. iti, washington, d.c.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.bibliography257intel corporation. 1996. as cited in òintel shifts its focus to longterm, original research,ó wall street journal, august 2, p. b3.international business machines corporation (ibm). 1997. 1997 ibm annual report. ibm,armonk, n.y.jardini, david r. 1996. òout of the blue yonder: the rand corporationõs diversificationinto social welfare research, 19461968.ó ph.d. dissertation, carnegie mellon university, pittsburgh, pa.jones, charles i., and john c. williams. 1996. too much of a good thing? the economics ofinvestment in r&d, working paper in economics 96005. stanford university, department of economics, palo alto, calif., february 26.kahn, d. 1967. the codebreakers: the story of secret writing. macmillan, new york.kahn, robert. 1988. òlater years at ipto,ó p. 250 in expert systems and artificial intelligence: applications and management, thomas c. bartee, ed. howard w. sams & co.,indianapolis, ind.kahn, robert. 1989. interview with william aspray, oh 158, charles babbage institutecenter for the history of information processing, university of minnesota, minneapolis, march 22.kahr, a.s., e.f. moore, and hao wang. 1962. òentsscheidungsproblem reduced to theaea case,ó proceedings of the national academy of sciences usa 48:365377.karp, r.m. 1972. òreducibility among combinatorial problems,ó pp. 85103 in complexityof computer computations, r.e. miller and j.w. thatcher, eds. plenum, new york.king, p.j.h., ed. 1983. database management systems: a technical comparison. pergamoninfotech ltd., maidenhead, berkshire, england.kleene, s. 1936. ògeneral recursive functions of natural numbers,ó mathematische annalen112:727742.kleene, s.c. 1956. òrepresentation of events in nerve nets and finite automata,ó pp. 340in automata studies, c.e. shannon and j. mccarthy, eds. princeton university press,princeton, n.j.kleinrock, leonard. 1964. communication nets: stochastic flow and delay. mcgrawhill,new york.knuth, d.e. 1968. the art of computer programming, 4 vols. addisonwesley, reading, mass.knuth, d.e. 1976. òbig omicron, big omega and big theta,ó sigact news 8:1823.laplante, phillip. 1966. great papers in computer science. west publishing, st. paul, minn.lea, wayne, ed. 1980. trends in speech recognition. prenticehall, englewood cliffs, n.j.lea, wayne, and june shoup. 1979. review of the arpa sur project and survey of currenttechnology in speech understanding. office of naval research, arlington, va., january.leiner, barry m., vinton g. cerf, david d. clark, robert e. kahn, leonard kleinrock, danielc. lynch, jon postel, larry g. roberts, and stephen wolff. 1998. a brief history of theinternet. available online at <http://www.isoc.org/internet/history/brief.html>, version 3.1, february 20.lesk, michael. 1995. òthe seven ages of information retrieval.ó available online at<http://community.bellcore.com/lesk>.licklider, j.c.r. 1960. òmancomputer symbiosis,ó ire transactions on human factors inelectronics 1(march):411.licklider, j.c.r. 1964. òartificial intelligence, military intelligence, and command andcontrol,ó military information systems: the design of computeraided systems for command, edward bennett, james degan, and joseph spiegel, eds. f.a. praeger, new york.licklider, j.c.r. 1988a. òthe early years,ó p. 226 in expert systems and artificial intelligence:applications and management, thomas c. bartee, ed. howard w. sams & co., indianapolis, ind.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.258funding a revolutionlicklider, j.c.r. 1988b. interview with william aspray and arthur l. norberg, cambridge, mass., oh 150, charles babbage institute center for the history of informationprocessing, university of minnesota, minneapolis, october 28.licklider, j.c.r., and r.w. taylor. 1968. òthe computer as a communications device.óscience and technology 76(april):2131.liebowitz, jay. 1997. òworldwide perspectives and trends in expert systems: an analysis based on the three world congresses on expert systems,ó ai magazine 18(summer):115119.lindsay, robert k., bruce g. buchanan, edward a. feigenbaum, and joshua lederberg.1993. òdendralña case study of the first expert system for scientific hypothesisinformation,ó artificial intelligence journal 61(2):209261.lorensen, william e., and harvey e. cline. 1987. òmarching cubes: a high resolution 3dsurface construction algorithm,ó computer graphics 21(4):163169.lorie, r.a., p.r. mcjones, j.w. mehl, g.r. putzolu, i.l. traiger, b.w. wade, and v. watson.1976. òsystem r: relational approach to database management,ó acm transactionson database systems 1(2):97137.mackenzie, donald. 1991. òthe influence of the los alamos and livermore national labson supercomputing,ó annals of the history of computing 13(2):179201.makhoul, john, and richard schwartz. 1994. òstate of the art in continuous speech recognition,ó p. 175 in voice communication between humans and machines, national research council, david b. roe and jay g. wilpon, eds. national academy press, washington, d.c.malone, michael s. 1995. the microprocessor: a biography. springerverlag, new york.mansfield, edwin. 1988. òindustrial innovation in japan and the united states,ó science241(september 30):17691775.markov, john. 1996. òmicrosoft plans 300% increase in spending for basic research in1997,ó new york times, december 9, p. d1.marshall, martin, larry waller, and howard wolff. 1981. òthe 1981 achievement award:lynn conway and carver mead,ó electronics 54(21):102105.may, ernest r. 1972. lessons of the past: the use and misuse of history in american foreignpolicy. oxford university press, new york.mccarthy, john. 1981. òhistory of lisp,ó in richard wexelblat, ed., history of programminglanguages. academic press, new york. the paper is also available online at <http://wwwformal.stanford.edu/jmc/history/lisp/lisp.html>.mccarthy, john. 1990. òsome expert systems need commonsense,ó formalizing commonsense: papers by john mccarthy, v. lifschitz, ed. ablex publishing, norwood, n.j.mccarthy, john, et al. 1955. òa proposal for the dartmouth summer research project onartificial intelligence.ó available from the rockefeller foundation archives grantfiles, òdartmouth collegeñartificial intelligence,ó rg 1.2, series 200, box 26, folder219. rockefeller archives center, north tarrytown, n.y.mcclain, d. 1998. òvoice technology appears ready to recognize bottom line,ó newyork times, january 19, pp. c1c2.mccracken, edward. 1997. òinspired by vision: a letter from ed mccracken,ó in nationalassociation of broadcasters õ97 & national association of broadcasters multimedia world.nab office of science and technology, washington, d.c.mcculloch, warren s., and walter pitts. 1943. òa logical calculus of the ideas immanentin nervous activity,ó bulletin of mathematical biophysics 3:115133. reprinted in warren s. mcculloch, 1965, embodiments of mind, mit press, cambridge, mass.mchugh, josh. 1996. òôholy cow, no oneõs done this!õó forbes 57(11):122128.mcjones, paul, ed. 1995. òthe 1995 sql reunion: people, projects, and politics.ó availableonline at <http://www.research.digital.com/src/personal/paulmcjones/systemr>.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.bibliography259mead, carver, and lynn conway. 1980. introduction to vlsi systems, addisonwesley,new york.mealy, g.h. 1955. òa method for synthesizing sequential circuits.ó bell system technicaljournal 34:10451079.merkle, r.c. 1978. òsecure communication over insecure channels,ó communications ofthe acm 21:294299.mills, d.l. 1988. òthe fuzzball,ó proceedings of the acm sigcomm symposium18(4):115122.mills, h.d., r.c. linger, and a.r. hevner. 1986. principles of information systems analysisand design. academic press, new york.minsky, margaret, m. ouhyoung, o. steele, f.p. brooks, jr., et al. 1990. òfeeling andseeing: issues in force display,ó computer graphics 24(2):235244.minsky, marvin. 1956. heuristic aspects of the artificial intelligence problem, mit lincolnlaboratory report 3455, astia doc. no. as236885. massachusetts institute of technology, cambridge, mass., december.minsky, marvin. 1979. òthe society theory of thinking,ó pp. 423450 in artificial intelligence: an mit perspective, patrick henry winston and richard henry brown, eds.mit press, cambridge, mass.misa, thomas j. 1985. òmilitary needs, commercial realities, and the development of thetransistor, 19481958,ó military enterprise and technological change: perspectives on theamerican experience, merritt roe smith, ed. mit press, cambridge, mass.moore, e. 1956. ògedanken experiments on sequential machines,ó automata studies,claude e. shannon and j. mccarthy, eds. princeton university press, princeton, n.j.narin, francis, kimberly s. hamilton, dominic olivastro. 1997. òthe increasing linkagebetween u.s. technology and public science,ó research policy 26(3):317330.national academy of public administration (napa). 1995. national science foundationõsscience and technology centers. napa, washington, d.c., july.national bureau of standards. 1977. data encryption standard, nbs fips pub 46. nationaltechnical information service, springfield, va.national research council. 1995. virtual reality: scientific and technological challenges,nathaniel i. durlach and anne s. mavor, eds. national academy press, washington,d.c.national research council. 1996. an assessment of the national science foundationõs scienceand technology centers program. national academy press, washington, d.c.national science and technology council (nstc), committee on computing, information,and communications. 1997. computing, information, and communications technologiesfor the 21st century: supplement to the presidentõs fy 1998 budget. national coordination office for computing, information, and communications, arlington, va.national science board. 1996. science and engineering indicatorsñ1996, nsb 9621. u.s.government printing office, washington, d.c.national science foundation (nsf). 1956. sixth annual report. nsf, washington, d.c.national science foundation (nsf). 1958. annual budget request to congress. nsf, washington, d.c.national science foundation (nsf). 1967. grants and awards for the fiscal year ended june30, 1967. nsf, washington, d.c.national science foundation (nsf). 1968. grants and awards for the fiscal year ended june30, 1968. nsf, washington, d.c.national science foundation (nsf). 1971. summary of awards: grants and awards for thefiscal year ended june 30, 1971. nsf, washington, d.c.national science foundation (nsf). 1973. summary of awards: grants and awards for thefiscal year ended june 30, 1973. nsf, washington, d.c.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.260funding a revolutionnational science foundation (nsf). 1979. annual budget request to congress 1979. nsf,washington, d.c.national science foundation (nsf). 1981a.  budget request to congress. nsf, washington,d.c.national science foundation (nsf). 1981b. summary of awards. nsf, washington, d.c.national science foundation (nsf). 1988. profilesñcomputer sciences: human resourcesand funding, nsf 88324. nsf, washington, d.c.national science foundation (nsf). 1991. academic research equipment in computer science,central computer facilities, and engineering: 1989, nsf 91304. nsf, washington, d.c.national science foundation (nsf), division of science resources studies. 1995. federalfunds for research and development: fiscal years 1993, 1994, and 1995, vol. 43, detailedstatistical tables, nsf 95334. nsf, arlington, va.national science foundation (nsf), division of science resources studies. 1997a.  federalfunds for research and development: fiscal years 1995, 1996, and 1997, vol. 45, detailedstatistical tables, nsf 97327, by ronald meeks. nsf, arlington, va.national science foundation (nsf), division of science resources studies. 1997b. scienceand engineering degrees: 196695, nsf 97335, susan t. hill, ed. nsf, arlington, va.national science foundation (nsf), division of science resources studies. 1998a. researchand development in industry: 1996 (early release tables), by raymond m. wolfe. nsf,arlington, va.national science foundation (nsf). 1998b. federal funds survey, detailed historical tables,fiscal years 19511998, nsf 98328. nsf, arlington, va.national science foundation (nsf). 1998c. federal funds survey, fields of science and engineering research, historical tables, fiscal years 19701998, nsf 98326. nsf, arlington, va.national science foundation (nsf). 1998d. federal funds survey, fields of science and engineering research to universities and colleges, historical tables, fiscal years 19731998, nsf98327. nsf, arlington, va.naur, p., et al. 1960. òreport on algol 60,ó communications of the acm 3:299314.nelson, richard. 1959. òthe simple economics of basic scientific research,ó journal ofpolitical economy 67(2):297306.nelson, richard, ed. 1982. government and technical progress: a crossindustry analysis.pergamon press, new york.nelson, richard. 1990. òwhy do firms do basic research (with their own money)?óresearch policy 19:165174.neumann, p.g. 1995. computerrelated risks. addisonwesley, reading, mass.newell, allen. 1984. òreports on artificial intelligence from carnegiemellon university:introduction to the comtex microfiche edition,ó ai magazine 5(3):3539.newell, allen, and herbert simon. 1956. current developments in complex information processing, rand p850. rand corporation, santa monica, calif., may 1.newell, allen, and herbert simon. 1972. human problem solving. prenticehall, englewoodcliffs, n.j.newell, allen, herbert simon, and j.c. shaw. 1957. empirical explorations of the logictheory machine: a case study in heuristics, rand p951. rand corporation, santamonica, calif., february 14.newell, allen, j.c. shaw, and herbert a. simon. 1959. report on a general problemsolvingprogram, rand p1584. rand corporation, santa monica, calif., february 9.newell, allen, et al. 1971.  speechunderstanding systems: final report of a study group.northholland, new york, may.nielsen media research. 1997. as cited in christian g. hill, òtechnology: adult users ofnet in us and canada put at 58 million,ó wall street journal, december 11, p. a11.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.bibliography261nilsson, nils j. 1984. òintroduction to the comtex microfiche edition of the sri artificialintelligence center technical notes,ó ai magazine 5(1):4243.norberg, arthur l. 1996. òchanging computing: the computing community anddarpa,ó annals of the history of computing 18(2):4053.norberg, arthur l., and judy e. oõneill. 1996. transforming computer technology: informationprocessing for the pentagon 19621986. johns hopkins university press, baltimore, md.office of technology assessment (ota). 1985. information technology r&d: critical trendsand issues, otacit268. u.s. government printing office, washington, d.c.office of technology assessment (ota). 1991. miniaturization technologies, otatct514.u.s. government printing office, washington, d.c.office of technology assessment (ota). 1993. defense conversion: redirecting r&d, otaite552. u.s. government printing office, washington, d.c.office of technology assessment (ota). 1995. innovation and commercialization of emergingtechnologies, otabpitc 165. u.s. government printing office, washington, d.c.,september.old associates, bruce s. inc. 1981. return on investment in basic researchñexploring amethodology, report to office of naval research, department of the navy, preparedunder contract n0001479c0192, november. available from defense technical information center, springfield, va.olle, t.w. 1978. the codasyl approach to database management. john wiley & sons, new york.ouhyoung, m., d.v. beard, and f.p. brooks, jr. 1989. òforce display performs betterthan visual display in a simple 6d docking task,ó pp. 14621466 in proceedings of theieee international conference on robotics and automation. ieee computer society press,scottsdale, ariz.ouhyoung, m., m. pique, j. hughes, n. srinivasan, et al. 1988. òusing a manipulator forforce display in molecular docking,ó proceedings of the ieee robotics and automationconference 3(april):18241829.parker, john e. 1985, 1986. interview with arthur l. norberg, washington, d.c., oh 99,charles babbage institute center for the history of information processing, universityof minnesota, minneapolis, december 13, 1985, and may 6, 1986.petskajuliussen, karen, and egil juliussen. 1996. eighth annual computer industry almanac. coriolis group, scottsdale, ariz.pickett, john r., and thomas l. case. 1991. òimplementing expert systems in r&d,óresearch, technology, management 34(4):3742.pierce, j.r. 1969. òwhither speech recognition?ó journal of the acoustical society of america46:10491051.presidentõs information technology advisory committee (pitac). 1998. interim report tothe president. national coordinating office for computing, information, and communications, arlington, va.pugh, emerson w. 1995. building ibm: shaping an industry and its technology. mit press,cambridge, mass.rabin, m.o., and d. scott. 1959. òfinite automata and their decision problems,ó ibmjournal of research and development 3:114125.redmond, kent c., and thomas m. smith. 1980. project whirlwind: the history of a pioneercomputer. digital press, bedford, mass.reed, sidney g., richard h. van atta, and seymour j. deitchman. 1990. darpa technicalaccomplishments: an historical review of selected darpa projects, ida paper p2192.institute for defense analyses, alexandria, va., february.rees, mina. 1982. òthe computing program of the office of naval research, 19461953,óannals of the history of computing 4(2):102120.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.262funding a revolutionrhind, flora m. 1955. letter from flora m. rhind, secretary of the rockefeller foundation,to donald h. morrison, provost, dartmouth college, december 23, 1955. available inthe records of the rockefeller foundation, rockefeller archives center, northtarrytown, n.y.rivest, r.l., a. shamir, and l.m. adleman. 1978. òa method for obtaining digital signatures and publickey cryptosystems,ó communications of the acm 21:120136.roberts, lawrence g. 1963. machine perception of three dimensional solids, tr315. lincolnlaboratory, massachussetts institute of technology, lexington, mass., may. cited intipper et al., eds., 1963. optical and electrooptical information processing, mit press,cambridge, mass.roberts, lawrence g. 1965. homogenous matrix representation and manipulation of ndimensional constructs, ms1505. mit lincoln laboratory, lexington, mass.roberts, lawrence g. 1967. òmultiple computer networks and intercomputer communication,ó proceedings of the acm symposium on operating system principles. associationfor computing machinery, new york.roberts, lawrence g. 1988. òexpanding ai research and founding arpanet,ó p. 234 inexpert systems and artificial intelligence: applications and management, thomas c. bartee,ed. howard w. sams & co., indianapolis, ind.rochester, nathaniel. 1959. symbol manipulation language, ai memo no. 5, mit artificialintelligence laboratory, cambridge, mass., november 18.rochester, nathaniel, and herbert gelernter. 1958. òintelligent behavior in problemsolving machines,ó ibm journal of research and development 2(october):336345.rosenberg, nathan. 1987. òcivilian ôspilloversõ from military r&d spending: the u.s.experience since world war ii,ó chapter 9 in strategic defense and western alliance,sanford lakoff and randy willoughby, eds. d.c. heath and co., lexington, mass.rouselot, r.s., and r.a. schumacker. 1967. general electric real time display, nasa reportnas93916. national aeronautics and space administration, washington, d.c.rowell, amy. 1998. òvirtual vehicles set the pace,ó computer graphics world 21(3):6162.r.r. bowker company. 1992. the software encyclopedia 1992, vol. 2. r.r. bowker company,new providence, n.j.salton, g. 1987. òhistorical note: the past thirty years in information retrieval,ó journalof the american society for information science 38(5):375380.seidel, robert. 1996. òfrom mars to minerva: the origins of scientific computing in theaec labs,ó physics today 49(10):3339.sematech. 1991. annual report. sematech, austin, tex.shannon, claude e. 1938. òa symbolic analysis of relay and switching circuits,ó transactions of the american institute of electrical engineers 57:111.shannon, claude e. 1948. òa mathematical theory of communication,ó bell system technical journal 27:379423, 623656.shannon, claude e. 1949. òcommunication theory of secrecy systems,ó bell system technical journal 30:5064.shannon, claude e. 1950a. òautomatic chess player,ó scientific american 182(2):4851.shannon, claude e. 1950b. òprogramming a computer for playing chess,ó philosophicalmagazine 41:256275.shannon, claude e. 1993. collected papers, n.j.a. sloane and a.d. wyner, eds. ieee press,new york.shannon, claude e., and john mccarthy, eds. 1956. òautomata studies,ó annals of mathematics studies, no. 34. princeton university press, princeton, n.j.shrobe, howard. 1996. òthe innovative applications of artificial intelligence conference:past and future,ó ai magazine 8(winter):16.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.bibliography263sichel, d.e. 1997. the computer revolution: an economic perspective. brookings institutionpress, washington, d.c.simon, herbert a. 1965.  the shape of automation for men and management. harper & row,new york.simon, herbert a. 1995. òartificial intelligence: an empirical science,ó artificial intelligence 77(1):95127.simon, herbert a., and allen newell. 1958. òheuristic problem solving: the next advance in operations research,ó operations research 6:78.simon, herbert a., allen newell, and j.c. shaw. 1958. the process of creative thinking,paper1320. rand corporation, santa monica, calif., september 16.smith, bruce. 1966. the rand corporation: case study of a nonprofit advisory corporation.harvard university press, cambridge, mass.smith, d.k., and r.c. alexander. 1988. fumbling the future. william morrow, new york.sri international. 1997. òthe internet,ó chapter iv in the role of nsfõs support of engineering in enabling technological innovation. sri international, arlington, va., february 14.available online at <http://unix.sri.com/policy/stp/technin/inter1.html>.stefik, mark. 1985. òstrategic computing at darpa: overview and assessment,ó communications of the acm 28(7):690704.stockham, thomas g., jr., and martin e. newell. 1975. òsensory information processingand symbolic computation research proposal, 1 october 1975 through 30 september1977,ó research proposal to darpa. available from national archives branch depository, suitland, maryland, rg 330780012, box 3, folder òutah a02477,ó p. 81.cited in norberg, arthur l., and judy e. oõneill, 1996, transforming computer technology: information processing for the pentagon, 19621986, johns hopkins universitypress, baltimore, md.stonebraker, m. 1976. òthe design and implementation of ingres,ó acm transactions ondatabase systems 1(3):189222.stonebraker, m. 1980. òretrospection on a database system,ó acm transactions on database systems 5(2):225240.stonebraker, m., ed. 1994. readings in database systems. morgan kaufmann, san mateo,calif.sutherland, ivan e. 1968. òa headmounted three dimensional display,ó pp. 757764 inproceedings of the fall joint computer conference. afips press, montvale, n.j.sutherland, ivan e., and carver mead. 1977. òmicroelectronics and computer science,óscientific american 237(3):210219.sutherland, ivan e., carver a. mead, and t.e. carver. 1976. basic limitations in microcircuitfabrication technology, rand corporation report no. adao35149, prepared underdarpa contract dahc1573c0181, arpa order 1891, santa monica, calif.,november.sutherland, ivan, robert f. sproull, and robert a. schumacker. 1974. òa characterizationof ten hidden surface algorithms,ó acm computer surveys 6(1):155.system development corporation. 1964. proceedings of the symposium on development andmanagement of a computercentered data base. system development corporation, santamonica, calif. cited in olle, t.w., 1978, the codasyl approach to database management,john wiley & sons, new york.tanenbaum, a.s. 1988. computer networks. prenticehall, englewood cliffs, n.j.taylor, robert. 1989. interview with william aspray, palo alto, calif., oh 154, charlesbabbage institute center for the history of information processing, university of minnesota, minneapolis, february 28.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.264funding a revolutiontomash, irwin. 1973. interview with robina mapstone, woodland hills, calif., oh 53,charles babbage institute center for the history of information processing, universityof minnesota, minneapolis, march 30.tomayko, james. 1985. ònasaõs manned spacecraft computers,ó annals of the history ofcomputing 7(1):718.turing, a.m. 1936. òon computable numbers with an application to theentscheidungsproblem,ó proceedings of the london mathematics society 2:230265.u.s. congress. 1991. ònew developments in computer technology: virtual reality.ó hearing before the subcommittee on science, technology, and space of the senate committee on commerce, science, and transportation, 102nd congress, first session, may 8.u.s. department of commerce. 1967. technological innovation: its environment and management, report of the panel on invention and innovation. u.s. government printing office, washington, d.c., january.u.s. department of commerce. 1997. americaõs new deficit: the shortage of informationtechnology workers. u.s. government printing office, washington, d.c.u.s. department of defense (dod). 1996. dod directive 5000.1, section d: policy, para 2:acquiring quality products, item (f): modeling and simulation. dod, washington,d.c., march 15.u.s. department of defense (dod), office of the inspector general. 1997. requirementsplanning for development, test, evaluation, and impact on readiness of training simulatorsand device, appendix d, draft proposed audit report, project no. 5ab0070.00, january 10. cited in computer science and telecommunications board, national researchcouncil, 1997, modeling and simulation: linking entertainment and defense, nationalacademy press, washington d.c.u.s. department of labor, bureau of labor statistics. 1997. employment and earnings, vol.44, no. 1, table b12. u.s. government printing office, washington, d.c.u.s. general accounting office (gao). 1992. federal research: sematechõs technologicalprogress and proposed r&d program, gao/rced92223br. u.s. government printing office, washington, d.c., july.van atta, richard h., sidney reed, and seymour j. deitchman. 1991a. darpa technicalaccomplishments, volume ii: an historical review of selected darpa projects, ida paperp2429. institute for defense analyses, alexandria, va., april.van atta, richard h., seymour j. deitchman, and sidney reed. 1991b. darpa technicalaccomplishments, volume iii: an overall perspective and assessment of the technical accomplishments of the defense advanced research projects agency: 19581990, ida paperp2538. institute for defense analyses, alexandria, va., july.van dam, andries, et al. 1991. òfrom discipline in crisis to mature science: evolvingneeds for computing research infrastructure,ó report on an nsf workshop. nationalscience foundation, washington, d.c., july.vardi, m.y., and p. wolper. 1986. òan automatatheoretic approach to automatic program verification,ó pp. 322331 in proceedings of the symposium on logic in computerscience. ieee press, new york.vlsi research. 1992. as cited in the washington post, november 18, p. a7.warnock, john e. 1969. a hidden surface algorithm for computer generated halftone pictures, ph.d. dissertation, department of computer science, university of utah.watkins, garry s. 1970. a realtime visible surface algorithm, ph.d. dissertation, department of computer science, university of utah.weaver, warren. 1955. diary of warren weaver, april 4, 1955. available in the records ofthe rockefeller foundation, rockefeller archive center, north tarrytown, n.y.weik, m.h. 1955. a survey of domestic electronic digital computing systems. ballistics research laboratories, aberdeen, md.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.bibliography265west, c.h., and p. zafiropulo. 1978. òautomated validation of a communications protocol:the ccitt x.21 recommendation,ó ibm journal of research and development 22:6071.wiederhold, gio. 1984. òdatabases,ó ieee computer 10(october):211223.woodcock, steven. 1997. interview on the future of ai technology and the impact ofmultiplayer networkcapable games, wall street journal interactive edition, may 19.zakon, robert h. 1998. hobbesõ internet timeline, version 3.3. available online at <http://www.isoc.org/guest/zakon/internet/history/hit.html>.ziegler, bart. 1997. òlab experiment: gerstner slashed r&d by $1 billion; for ibm, it maybe a good thingððlatest breakthrough shows,ó wall street journal, october 6, p. a1.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.267thomas hughes (chair) is professor emeritus at the university ofpennsylvania and is currently distinguished visiting professor at themassachusetts institute of technology. he will be visiting professor atstanford university and at the royal institute of technology (stockholm)in spring 1999. professor hughes is a member of the royal swedishacademy of engineering sciences, a fellow of the american academy ofarts and sciences, and a member of phi beta kappa. in 1985 he wasawarded the leonardo da vinci medal of the society for the history oftechnology. the society for the social studies of science awarded himthe john desmond bernal award in 1990. in 1990 he received the kenanenterprise award. the johns hopkins university named him a memberof the society of fellows in 1984. he has been a fellow of the wissenschaftskolleg (institute of advanced study berlin) and distinguishedvisiting professor, new school for social research. his most recent book,rescuing prometheus (pantheon books, 1998) is about large technologicalsystems. previous publications include networks of power: electrificationof western society, 18801930 and elmer sperry: inventor and engineer, bothof which won the dexter prize for the outstanding book on the history oftechnology (in 1985 and 1972, respectively). with agatha hughes heedited lewis mumford: public intellectual (oxford university press, 1990).dr. hughes completed his graduate work in european history at theuniversity of virginia.appendixcommittee biographiesfunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.268funding a revolutiongwen bell is the founding president of the computer museum. shestarted the first and only computer museum in the world. dr. bell successfully applied for nonprofit status in 1981, moved to downtown bostonand opened a 42,000squarefoot facility in 1984, raised $3.3 million incapital and grew the operating budget from $30,000 to $1 million per year,and achieved a unique joint collecting agreement with the smithsonianinstitutionõs national museum of american history. before that, she wasa social science editor for pergamon press where she was responsible fora 125book product line. dr. bell was visiting associate professor, graduate school of design, harvard university from 1972 to 1973. she was alsoassociate professor of urban affairs, graduate school of public and international affairs, university of pittsburgh. her published books are strategies for human settlements (university press of hutchinson & ross:stroudsburg, pennsylvania) and human identity in the urban environmentwith j. tyrwhitt (a pelican original, london and new york). dr. bellwas a united nations consultant for indonesia, the philippines, and brazilfrom 1970 to 1977. she received her ph.d. in geography from clark university, worcester, massachusetts, in 1967, her m.c.r.p. in city and regional planning from harvard university in 1959, and her b.s. from theuniversity of wisconsin at madison.erich bloch is a distinguished fellow with the counsel on competitiveness. he was previously the director of the national science foundation (19841990) and a corporate vicepresident for technical personneldevelopment at ibm, which he joined in 1952 after receiving a b.s. degreein electrical engineering from the university of buffalo. he is the recipient of many honorary degrees and the national medal of technology forhis part in the development of the ibm/system 360 computer, whichòrevolutionized the computer industry.ó mr. bloch has received theinstitute of electrical and electronics engineers (ieee) computer pioneeraward, the ieee founders medal, and the national academy of engineeringõs bueche award. he is a member of the national academy of engineering, swedenõs academy of engineering sciences, and the japan academy of engineering, a fellow of the ieee, a member of its computersociety, and a fellow of the american association for the advancementof science.robert bressler is chief scientist of networking for sun microsystems,inc. his responsibilities include charting the future directions for networking products and advanced development in networking for sun aswell as working across all the sun companies to guide the direction ofnetworking technologies. prior to joining sun in 1994, mr. bressler spent4 years at network equipment technologies (n.e.t.) as senior vicepresifunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.appendix: committee biographies269dent, technology and corporate development, and chief technical officer.his responsibilities included longterm strategic planning and overallproduct architecture for n.e.t. and adaptive, as well as the investmentstrategy for r&d spending, and advanced r&d activities. before joiningn.e.t. in 1990, mr. bressler spent 4 years at 3com corporation. his mostrecent assignment at 3com was as chief technical officer and vicepresident,corporate development. his responsibilities included overall productstrategy for 3com, and, in particular, the creation, with microsoft, of theos/2 lan manager. prior to joining 3com in 1986, mr. bressler spentmore than 13 years with bolt, beranek, and newman, inc. (bbn), in cambridge, massachusetts, where he held a variety of senior managementpositions, most recently senior vicepresident of development and engineering for bbn communications. at bbn, mr. bressler played a keyrole in the management and development of packet switching for datacommunications including the arpanet and the evolution of protocolsincluding tcp/ip, x.25, satellitebased communications, and data communications security. mr. bressler holds both an msee and a bsee fromthe massachusetts institute of technology. his thesis topic was in thearea of distributed computing, the research for which was all done on thearpanet.paul david is an economist and economic historian who has held thewilliam robertson coe professorship of american economic history atstanford university since 1978. he was educated at harvard universityand the university of cambridge, and joined the stanford economicsdepartment in 1961. the evolution and diffusion of technological systems and the role of technological and organizational innovation in longterm economic growth have been focal points in dr. davidõs research andteaching, which has included such themes as the importance of the systemsapproach to understanding technological change, the economic significance of network externalities and technical standards in system development, and the òpathdependent,ó historical character of these processes.a frequent contributor to books and scholarly journals, dr. david currently coedits economics of innovation and new technology and serves onthe editorial boards of computers, standards and interfaces, the journal ofindustrial and corporate change, and other journals. he has been a consultant to the national research council, the library of congress, the worldbank, the organisation for economic cooperation and development, andother national and international bodies. dr. david is a fellow of theamerican academy of arts and sciences and a fellow of the internationaleconometric society. in may of 1993 he was elected to a senior researchfellowship in economics at all souls college, oxford.funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.270funding a revolutionmarvin denicoff graduated from temple university in 1949 with adegree in liberal arts. he did graduate work in literature and linguistics attemple university and mexico city college. mr. denicoff joined the navydepartment in 1951, and was among the early civil service trainees in theemerging fields of computer science and operations research. beginningin 1954, he was assigned to george washington university as liaison forresearch in logistics and computation. during a 6year period with georgewashington university, he wrote or cowrote more than 20 scientific papers in the topic areas of inventory control, value theory, failure analysis,and business data processing. in 1960, mr. denicoff took a research management position with the office of naval research (onr) and 2 yearslater became director of the information sciences program. in this capacity, he directed, until his retirement in august 1983, a multimilliondollarperyear basic research grant program in such fields as artificial intelligence, robotics, computer graphics, manmachine systems, computerarchitecture, and software. mr. denicoff has served as exofficio memberof the computer science board of the national research council and hasbeen a participant, leader, or advisor to such government groups as thedepartment of defense (dod) triservice software research committee,the science advisory board on supercomputers, and the united statesinformation agency program on artificial intelligence. mr. denicoff hasbeen honored for his government service with a meritorious serviceaward; he is one of the few individuals who have been given two distinguished civilian service awards. in 1983, he was given a special awardby the american association for artificial intelligence for continuing contributions to that field of research. mr. denicoff was a cofounder, in1983, of thinking machines corporation and served with that firm as avicepresident and board member until his retirement in 1996. he hashad an affiliation with the massachusetts institute of technologyõs medialab in the capacity of principal research associate. in addition to his longcareer in computer science, mr. denicoff is a shortstory writer and playwright. his stories have been published in various literary magazinesand anthologies.david hounshell is luce professor of technology and social changeat carnegie mellon university, where he studies innovation in both itstechnological and its organizational dimensions. since 1982, he has addressed the rise of industrial research and development in the unitedstates and the problems of managing scientific and technical research inorganizations. he is also studying the cold war and its influence on thepursuit of science, technology, and enterprise in the united states, andthis work has led to one of his current projects, a history of the randcorporation of santa monica, california, from its creation in 1948 to thefunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.appendix: committee biographies271end of the cold war. another current project is the development of asequel to his first book, which will bring his study of the development ofamerican manufacturing technology to the end of the 20th century. hislongterm writing project is a book tentatively titled the wealth of a nation:the dynamics of science, technology, and business in the united states, 17751990. a shorterterm project is his editorial work on a massive diary keptby the late crawford h. greenewalt in his role as liaison between theuniversity of chicagoõs metallurgical laboratory and the dupont company during the manhattan project (the atomic bomb project); this diarywill be published by the american philosophical society. hounshell received the 1978 browder j. thompson memorial prize award of the ieee,the 1987 dexter prize in the history of technology, the 1992 thomasnewcomen award in business history, and the 1992 williamson medalfrom the business history conference.amos joel is retired after a 43year career at lucent bell laboratories.he is a pioneer in the design, development, and evaluation of electronicswitching and information processing systems. he has lectured and written extensively in the united states and abroad on switching principlesand history. mr. joel received a b.s. (1940) and an m.s. (1942) from themassachusetts institute of technology. he is a life fellow of the ieee anda member of the national academy of engineering, the association forcomputing machinery, the american association for the advancementof science, and the american academy of arts and sciences. mr. joel haswon numerous awards, including the ieee medal of honor (1992) andbell medal (1972), the international telecommunications union (itu)centenary award (1983), the columbian genoa prize (1984), the kyotoprize (1989), and the u.s. national medal of technology (1993).timothy lenoir is professor and cochair of stanford universityõsprogram on the history of science. dr. lenoir received a b.a. (1970) fromsaint maryõs college, morage, california. he also received a ph.d. in thehistory and philosophy of science from indiana university in 1974. hehas received numerous honors and awards, including a nato postdoctoral fellowship in science (19751976), an nsf research grant (19781980), and the provostõs research fund award, stanford university(1994). among his many publications are the strategy of life: teleologyand mechanics in nineteenth century german biology (d. reidel, dordrechtand boston, 1982), politik im tempel der wissenschaft: forschung undmachtausubung im deutschen kaiserreich (campus verlag, frankfurt/main,1992), instituting science: the cultural production of scientific disciplines(stanford university press, 1997), and inscribing science: scientific textsand the materiality of communication (stanford university press, 1998).funding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.272funding a revolutionm. douglas mcilroy, retired from bell laboratories, is an adjunctprofessor of computer science at dartmouth college. his research interests focus on computer programming and systems, especially programming languages and text processing, graphics algorithms, searching andsorting, and computer security. he received a b.e.p. (1954) from cornelluniversity and a ph.d. (mathematics, 1959) from the massachusetts institute of technology. he is a fellow of the american association for theadvancement of science.emerson w. pugh is the author or coauthor of a college physics textand four books on the history of ibm and the information processingindustry. his most recent book is building ibm: shaping an industry and itstechnology (mit press, 1995). after receiving his ph.d. in physics fromthe carnegie institute of technology in 1956, dr. pugh worked for ibmfor 35 years in a variety of capacities, including research scientist, productdevelopment manager, and corporate executive. he is chairman of theieee history committee, a director of the ieee foundation, a trustee ofthe charles babbage foundation, and a trustee of the samuel f.b. morsehistoric site. dr. pugh is a fellow of the ieee, the american physicalsociety, and the american association for the advancement of science,and he served as president of the ieee in 1989.charles l. seitz is president of myricom, inc., a startup companyinvolved in research, development, production, and sales of highspeedcomputers and localarea networks. during the 16 years before foundingmyricom, he was a professor of computer science at the california institute of technology (caltech), where his research and teaching were in theareas of very large scale integrated circuit (vlsi) design, computer architecture and programming, and concurrent computation. he earned s.b.(1965), s.m. (1967), and ph.d. (1971) degrees from the massachusetts institute of technology, where he was also an instructor and the recipient ofthe goodwin medal for òconspicuously effective teaching.ó he was aconsultant and member of the technical staff of the evans & sutherlandcomputer corporation during its initial years (19681972), an assistantprofessor of computer science at the university of utah (19701972), and aconsultant and leader of several research and development projects forburroughs corporation (19711978). his research in vlsi and concurrentcomputing at caltech, including the development of the cosmic cubemulticomputer, was selected by science digest as one of the top 100 innovations in 1985. dr. seitz was elected to the national academy of engineering in 1992 for òpioneering contributions to the design of asynchronous and concurrent computer systems.ófunding a revolution: government support for computing researchcopyright national academy of sciences. all rights reserved.appendix: committee biographies273charles thacker is director of advanced systems for microsoft corporation. he was previously a senior corporate consultant engineer atdigital equipment corporationõs systems research center. during his 13years at digital, mr. thacker led the development of firefly (the firstmultiprocessor workstation), the alpha demonstration unit, the first alpha system, and the an1 and an2 networks, precursors of digitalõsgigaswitch/atm products. before joining digital, he spent 13 years atthe xerox palo alto research center, where he was responsible for thedevelopment of a number of experimental computer systems includingalto, the first personal workstation. he is a coinventor of the ethernetlocal area network and holds over 20 patents in computer architectureand networking. mr. thacker is a fellow of the association for computing machinery and a member of the national academy of engineering.
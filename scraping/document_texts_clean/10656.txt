detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/10656who goes there?: authentication through the lens of privacy232 pages | 6 x 9 | paperbackisbn 9780309088961 | doi 10.17226/10656stephen t. kent and lynette i. millett, editors; committee on authenticationtechnologies and their privacy implications; computer science andtelecommunications board; division on engineering and physical sciences;national research councilwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.who goes there?authentication through the lens of privacycommittee on authentication technologies andtheir privacy implicationscomputer science and telecommunications boarddivision on engineering and physical sciencesstephen t. kent and lynette i. millett, editorsthe national academies presswashington, d.c.www.nap.eduwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.the national academies press 500 fifth street, n.w. washington, dc 20001notice: the project that is the subject of this report was approved by thegoverning board of the national research council, whose members are drawnfrom the councils of the national academy of sciences, the national academy ofengineering, and the institute of medicine. the members of the committee responsible for the report were chosen for their special competences and with regard for appropriate balance.this study was supported by office of naval research grant number n000140010855, national science foundation grant number ani0090219, general servicesadministration purchase order number gs00c00am00228, social security administration purchase order number 04400150677, and federal chief information officers council award number gs00c00am00228. the vadasz familyfoundation gave supplemental funding. any opinions, findings, conclusions, orrecommendations expressed in this publication are those of the author(s) and donot necessarily reflect the views of the organizations or agencies that providedsupport for the project.international standard book number 0309088968 (book)international standard book number 030952654x (pdf)cover designed by jennifer m. bishop.additional copies of this report are available from the national academies press,500 fifth street, n.w., lockbox 285, washington, dc 20055; (800) 6246242 or(202) 3343313 (in the washington metropolitan area); internet, http://www.nap.edu.copyright 2003 by the national academy of sciences. all rights reserved.printed in the united states of americawho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.the national academy of sciences is a private, nonprofit, selfperpetuating society of distinguished scholars engaged in scientific and engineering research, dedicated to the furtherance of science and technology and to their use for the generalwelfare. upon the authority of the charter granted to it by the congress in 1863,the academy has a mandate that requires it to advise the federal government onscientific and technical matters. dr. bruce m. alberts is president of the nationalacademy of sciences.the national academy of engineering was established in 1964, under the charterof the national academy of sciences, as a parallel organization of outstandingengineers. it is autonomous in its administration and in the selection of its members, sharing with the national academy of sciences the responsibility for advising the federal government. the national academy of engineering also sponsorsengineering programs aimed at meeting national needs, encourages educationand research, and recognizes the superior achievements of engineers. dr. wm. a.wulf is president of the national academy of engineering.the institute of medicine was established in 1970 by the national academy ofsciences to secure the services of eminent members of appropriate professions inthe examination of policy matters pertaining to the health of the public. theinstitute acts under the responsibility given to the national academy of sciencesby its congressional charter to be an adviser to the federal government and, uponits own initiative, to identify issues of medical care, research, and education.dr.harvey v. fineberg is president of the institute of medicine.the national research council was organized by the national academy of sciences in 1916 to associate the broad community of science and technology withthe academys purposes of furthering knowledge and advising the federal government. functioning in accordance with general policies determined by theacademy, the council has become the principal operating agency of both thenational academy of sciences and the national academy of engineering in providing services to the government, the public, and the scientific and engineeringcommunities. the council is administered jointly by both academies and theinstitute of medicine. dr. bruce m. alberts and dr. wm. a. wulf are chair andvice chair, respectively, of the national research council.www.nationalacademies.orgwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.vcommittee on authentication technologies andtheir privacy implicationsstephen t. kent, bbn technologies, chairmichael angelo, compaq computer corporationsteven bellovin, at&t labs researchbob blakley, ibm tivoli softwaredrew dean, sri internationalbarbara fox, microsoft corporationstephen h. holden, university of maryland, baltimoredeirdre mulligan, university of california, berkeleyjudith s. olson, university of michiganjoe pato, hp labs cambridgeradia perlman, sun microsystemspriscilla m. regan, george mason universityjeffrey schiller, massachusetts institute of technologysoumitra sengupta, columbia universityjames l. wayman, san jose state universitydaniel j. weitzner, massachusetts institute of technologystafflynette i. millett, study director and program officerjennifer m. bishop, senior project assistant (beginning october2001)suzanne ossa, senior project assistant (through september 2001)who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.viicomputer science and telecommunications boarddavid d. clark, massachusetts institute of technology, chaireric benhamou, 3com corporationelaine cohen, university of utahthomas e. darcie, university of victoriamark e. dean, ibm thomas j. watson research centerjoseph farrell, university of california, berkeleyjoan feigenbaum, yale universityhector garciamolina, stanford universityrandy h. katz, university of california, berkeleywendy a. kellogg, ibm thomas j. watson research centersara kiesler, carnegie mellon universitybutler w. lampson, microsoft corporation, cstb memberemeritusdavid liddle, u.s. venture partnersteresa h. meng, stanford universitytom m. mitchell, carnegie mellon universitydaniel pike, gci cable and entertainmenteric schmidt, google inc.fred b. schneider, cornell universityburton smith, cray inc.william stead, vanderbilt universityandrew j. viterbi, viterbi group, llcjeannette m. wing, carnegie mellon universityalan s. inouye, interim executive directorjon eisenberg, interim assistant directorkristen batch, research associatejennifer m. bishop, senior project assistantjanet briscoe, administrative officerdavid drake, senior project assistantrenee hawkins, financial associatephil hilliard, research associatemargaret marsh huynh, senior project assistantherbert s. lin, senior scientistlynette i. millett, program officerdavid padgham, research associatecynthia a. patterson, program officerjanice sabuda, senior project assistantwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.brandye williams, staff assistantsteven woo, dissemination officerfor more information on cstb, see its web site at <http://www.cstb.org>, write to cstb, national research council, 500 fifthstreet, n.w., washington, dc 20418; call at (202) 3342605; or email thecstb at cstb@nas.edu.viiiwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.prefaceixthe broadening use of the internet implies that, more and more,people are communicating and sharing information with strangers. the result is growth in different kinds of demand to authenticate system users, and the different motivations for requiring authentication imply different tradeoffs in evaluating technical and nontechnicaloptions. motivations range from those related to system security (forexample, the ability to access critical systems or medical records) to thoserelated to business development (for example, the ability to use freewebbased resources or to have access to elements of electronic commerce). the key questions surrounding these issues relate to what dataabout a person are shared, how they are shared (including whether overtlyand cooperatively as well as by what technique), why they are shared(fitting the purpose to the nature and amount of data), and how the dataare protected.concerns that arise about adverse impacts on personal privacy fromparticular approaches to authentication may reflect judgments about therationale (e.g., how much information about a person is really needed toauthorize access to a particular system) as well as concern about thesoundness of the technical and procedural steps taken to protect the personal information gathered in the process of authentication. those concerns are heightened by the growing ease of aggregation of informationcollected from multiple sources (socalled data matching), the observedtendency to collect information without an individuals knowledge, andwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.xprefacethe ease of publicizing or distributing personal information, like any otherinformation, via the internet.the committee and its chargein september 1999, the u.s. governments chief counselor for privacy,peter swire, met with the computer science and telecommunicationsboard (cstb) in washington, d.c., and described his need for studies ofbiometrics and authentication. enthusiastic support by cstb members,given the importance of the topic and the ability to build on past cstbwork, led to further discussion about initiating a project. richard guida,former chair of the federal public key infrastructure (fpki) steeringcommittee and now with johnson and johnson, provided insight intofederal agency thinking about authentication and encouraged fpki members to be interested in and involved with the project. the scope of theproject was broadened to encompass a range of authentication technologies and their privacy implications. funding for the project was obtainedfrom the national science foundation, the office of naval research, thegeneral services administration, the federal chief information officerscouncil, and the social security administration.the task of the committee assembled by cstbthe committee onauthentication technologies and their privacy implicationswas to examine the interaction of authentication and privacy. the committeesought to identify the range of circumstances and the variety of environments in which greater or lesser degrees of identification are needed inorder to carry out governmental or commercial functions. it also addressed ways in which law and policy can come to grips with the flawsthat are likely in the technology or its implementation. it considered howthe federal government can deploy improved authentication technologiesconsistent with the desire to protect privacy. it also examined the broadimplications of alternative approaches to selecting and implementing authentication technologies by the federal government and others interested in their use.consisting of 16 members from industry and academia (see appendix a), the committee was designed to have a range of technical expertiserelating to different kinds of authentication technologies and informationsystem security technologies generally, to applications, and to the privacyimpacts of information technology and related policy. the memberspossess a range of computer science expertise (e.g., information systemsecurity, cryptography, networking and distributed systems, humancomputer interaction) and associated nontechnical expertise (e.g., privacypolicy and law) as well as user perspectives (including organizations seeking to employ authentication and end users with various concerns in suchwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.prefacexisectors as banking/finance and health). one original committee member,david solo of citigroup, was unable to continue his participation in theproject because of unforeseen time constraints.processempanelled during the winter of 2000, the committee met seven timesbetween march 2001 and august 2002 to plan its course of action, receivetestimony from relevant experts, deliberate on its findings, and draft itsfinal report. it continued its work between meetings and into the fall andend of 2002 by electronic communications. during the course of its study,the committee took briefings from information and authentication technology researchers and developers in industry and universities and fromleaders in government agencies involved in the development and deployment of authentication technologies. it also heard from privacy and consumer protection experts and representatives from various sectors of industry that use authentication technologies for business processes andecommerce. the committee also went to verisign in california for a sitevisit. (see appendix b for a complete list of briefers to the committee.)more than half of the committees meetings were held and most ofthis report was written after the events of september 11, 2001. at itsoctober 2001 meeting, the committee decided, with cstbs encouragement, to develop a short report addressing the concept of nationwideidentity systemsa topic that has received much media and policy attention since the terrorist attacks. given that many of the committees discussions and briefings were closely related to issues of identity and identification, the committee was well positioned to comment in a timelyfashion on the topic. supplemental funding for that activity was provided by the vadasz family foundation. that report was released inapril 2002 and is available from the national academies press.1acknowledgmentsas with any project of this magnitude, thanks are due to the manyindividuals who contributed to the work of the committee. the committee thanks those who came to various meetings to provide briefings andwarwick ford for arranging the site visit at verisign in january. thanksare also due to those who sponsored the study: the national science foun1computer science and telecommunications board, national research council. idsñnot that easy: questions about nationwide identity systems. washington, d.c., national academy press, 2002.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.xiiprefacedation (george strawn and aubrey bush), the office of naval research(andre van tilborg), the general services administration (mary mitchell),the federal chief information officers council (keith thurston and rogerbaker), and the social security administration (sara hamer and tonytrenkle). we are grateful to peter swire for commissioning the project, torichard guida and denise silverberg for helping to muster supportthrough the fpki steering committee, and to kathi webb of rand forproviding early access to its biometrics study project.finally, the committee thanks david d. clark, chair of the cstb, andmarjory s. blumenthal, cstbs director when this study was being carried out, for valuable insights. the committee also thanks the followingmembers of the cstb staff for their contributions. janet briscoe providedcrucial administrative support, especially with the october 2001 workshop. suzanne ossa was the initial senior project assistant for this project.jennifer bishop took over as senior project assistant and provided significant help with report preparation and editing; she also designed the covers of both this report and the earlier committee report and developedmany of the diagrams. david padgham provided background researchand descriptions of various pieces of legislation. wendy edwards, anintern with cstb in the summer of 2002, also provided some backgroundresearch. steven j. marcus made an editorial pass through an earlier draftof the report, and dorothy sawicki and liz fikre made significant editorial contributions in preparation for publishing. special thanks are due tolynette i. millett, the study director for this project. she worked veryclosely with the chair and other committee members, transforming theirinputs into a coherent report that attempts to explain a complex topic inan understandable fashion.stephen t. kent, chaircommittee on authenticationtechnologies and their privacyimplicationswho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.acknowledgment of reviewersxiiithis report has been reviewed in draft form by individuals chosenfor their diverse perspectives and technical expertise, in accordancewith procedures approved by the national research councils report review committee. the purpose of this independent review is toprovide candid and critical comments that will assist the institution inmaking its published report as sound as possible and to ensure that thereport meets institutional standards for objectivity, evidence, and responsiveness to the study charge. the review comments and draft manuscriptremain confidential to protect the integrity of the deliberative process.we wish to thank the following individuals for their review of this report:ross anderson, university of cambridge,scott charney, microsoft,carl ellison, intel corporation,joel s. engel, jse consulting,michael froomkin, university of miami school of law,john d. halamka, harvard medical school,jerry kang, university of california, los angeles,sally katzen, independent consultant,deborah j. mayhew, deborah j. mayhew and associates,jeffrey naughton, university of wisconsinmadison,marek rejmangreene, btexact technologies, andbarbara simons, ibm.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.xivacknowledgment of reviewersalthough the reviewers listed above have provided many constructive comments and suggestions, they were not asked to endorse the conclusions or recommendations, nor did they see the final draft of the reportbefore its release. the review of this report was overseen by mildred s.dresselhaus and randall davis, both at the massachusetts institute oftechnology. appointed by the national research council, they wereresponsible for making certain that an independent examination of thisreport was carried out in accordance with institutional procedures andthat all review comments were carefully considered. responsibility forthe final content of this report rests entirely with the authoring committeeand the institution.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.executive summary11introduction and overview16definitions and terminology, 18authentication in daily life, 21current tensions, 28four overarching privacy concerns, 30what this report does and does not do, 312authentication in the abstract33what is authentication and why is it done?, 33three parties to authentication, 36authenticating to authorize, 37authenticating to hold accountable, 38what do we authenticate?, 41identifiers, 42attributes, 43statements, 44how do we authenticate?, 45authenticating physical identity, 47authenticating psychological identity, 47authenticating possession of an artifact, 49xvcontentswho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.xvicontentsidentification, 50the relationship between authentication and identification, 513privacy challenges in authentication systems55privacy impact of the decision to authenticate, 56access control and information systems, 57the legal foundations of privacy, 62constitutional roots of privacy, 63the common law roots of privacy law, 68statutory privacy protections, 69information privacy and fair information practices, 71privacy of communications, 75concluding remarks, 784security and usability80threat models, 81threats, 81dealing with threats, 84authentication and peopleusercentered design, 86lessons from usercentered design, 87lessons from cognitive and social psychology, 90factors behind the technology choice, 95systems and secondary use, 97concluding remarks, 1015authentication technologies104technological flavors of authentication, 104basic types of authentication mechanisms, 106something you know, 107something you have, 110something you are, 120multifactor authentication, 123centralized versus decentralized authentication systems, 125security considerations for individual authenticationtechnologies, 132cost considerations for individual authentication technologies, 135concluding remarks, 136who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.contentsxvii6authentication, privacy, and the roles138of governmentregulator of private sector and public agency behaviors and processes, 140governmentwide law and policy, 141agency or programspecific law and policies, 145regulation of private sector information managementactivity, 149policy activity in the early 2000s, 151summary, 155government as issuer of identity documents, 155the tangled web of governmentissued identity documents, 162threats to foundational documents, 165government as relying party for authentication services, 169access certificates for electronic services, 170the internal revenue serviceelectronic tax filing, 172the social security administration and pebes, 175nationwide identity systems, 176concluding remarks, 1777a toolkit for privacy in the context of179authenticationprivacyimpact toolkit, 181attribute choice, 182identifier selection, 186identity selection, 189the authentication phase, 190concluding remarks, 192appendixesabiographies of committee members and staff197bbriefers to the study committee207csome key concepts209what is cstb?213who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.1executive summaryas communications and computation technologies become increasingly pervasive in our lives, individuals are asked to authenticatethemselvesñto verify their identitiesñin a variety of ways. activities ranging from electronic commerce to physical access to buildings toegovernment have driven the development of increasingly sophisticatedauthentication systems. yet despite the wide variety of authentication technologies and the great range of activities for which some kind of authentication is required, virtually all involve the use of personal information, raisingprivacy concerns. the development, implementation, and broad deployment of authentication systems require that issues surrounding identity andprivacy be thought through carefully. this report explores the interplaybetween authentication and privacy. it provides a framework for thinkingthrough policy choices and decisions related to authentication systems.authenticationõs implications for privacy do not necessarily equate toviolations of privacy, but understanding the distinctions requires beingaware of how privacy can be affected by the process of authentication.such awareness is usually absent, however, because authentication tendsto be thought about more narrowly, in connection with security. in deciding how to design, develop, and deploy authentication systems, it is necessary to weigh privacy, security, cost, user convenience, and other interests. a key point is that all of these factors are subject to choice: whetherany given system violates privacy depends on how it is designed andimplemented. changes in technology and practice make this the time forbroader, more rigorous analyses of options in authentication.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.2who goes there?the complexity of the interplay between authentication and privacybecomes clear when one tries to define authentication, which can takemultiple forms:¥individual authentication is the process of establishing an understood level of confidence that an identifier refers to a specific individual.¥identity authentication is the process of establishing an understoodlevel of confidence that an identifier refers to an identity. the authenticated identity may or may not be linkable to an individual.¥attribute authentication is the process of establishing an understoodlevel of confidence that an attribute applies to a specific individual.a common understanding and consistent use of these and other termsdefined in the report are a prerequisite for informed discussion. the threevariants above illustrate that authentication is not a simple concept: asthe committeeõs first report on nationwide identity systems1 argued, grappling with these issues and their implications is just not that easy (box es.1).this summary of the report includes the findings and recommendations of the authoring committee on authentication technologies andtheir privacy implications. each of these findings and recommendations,which are more fully developed and supported in the body of the report,is followed by the number of the finding or recommendation in parentheses. this number corresponds to the chapter where the finding or recommendation is found and its order of appearance in that chapter.security, authentication, and privacyauthentication is not an end in itself. in general, people are authenticated so that their requests to do something can be authorized and/or sothat information useful in holding them accountable can be captured.authentication systems are deployed when control of access and/or protection of resources, both key functions of security, are necessary.the three generic means of authentication that tend to be used inpractice can be described loosely as òsomething you know,ó òsomethingyou have,ó or òsomething you are.ó the systems discussed in this reportñbased on technologies such as passwords, public key infrastructures (pki), smart cards, and biometrics, among others (see boxes es.2,es.3, and es.4)ñgenerally implement one or a combination of these approaches.1 computer science and telecommunications board, national research council. idsñnot that easy: questions about nationwide identity systems. washington, d.c., national academy press, 2002.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.executive summary3box es.1nationwide identity systemsin the first report of the committee on authentication technologies and theirprivacy implications, idsñnot that easy: questions about nationwide identitysystems, it was noted that many largescale identity systems are in effect nationwide identity systems. in particular, driverõs licenses and even social securitycards qualify as such. such largescale systems pose significant privacy and security challenges, which were elaborated on in that report. a followon discussionis located in chapter 6 that includes the findings and recommendations below.finding: stateissued driverõs licenses are a de facto nationwideidentity system. they are widely accepted for transactions thatrequire a form of governmentissued photo id. (6.5)finding: nationwide identity systems by definition create a widespread and widely used form of identification, which could easilyresult in inappropriate linkages among nominally independent databases. while it may be possible to create a nationwide identitysystem that would address some privacy and security concerns,the challenges of doing so are daunting. (6.6)recommendation: if biometrics are used to uniquely identify license holders and to prevent duplicate issuance, care must be taken to prevent exploitation of the resulting centralized database andany samples gathered. (6.3)recommendation: new proposals for improved driverõs licensesystems should be subject to the analysis presented in this reportby the national research councilõs committee on authenticationtechnologies and their privacy implications and in the earlier(2002) report by the same committee: idsñnot that easy: questions about nationwide identity systems. (6.4)finding: core authentication technologies are generally moreneutral with respect to privacy than is usually believed. howthese technologies are designed, developed, and deployed insystems is what most critically determines their privacy implications. (5.6)but what kind of security is necessary, and is authentication required?when authentication is needed, which types might serve best? for example, when accountability is required, individual authentication may bewho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.4who goes there?box es.2passwordspasswords pose serious security challenges. they are a commonly usedform of authentication and are the quintessential example of òsomething youknow.ó they require no specialized hardware or training and can be distributed,maintained, and updated by telephone, fax, or email. but they do have seriousdisadvantages, among them susceptibility to guessing and to theft. in addition,passwords generally do not change without human intervention, leaving them opento compromise. passwords are also easily shared, either intentionally or inadvertently (when written down near a computer, for example), and a complex, expensive infrastructure is necessary to enable resetting lost (forgotten) passwords.because people have trouble remembering a large number of names and passwords, there is a trend either toward name and password reuse across systems,which undermines privacy (and security), or toward the creation of centralized systems to keep track of these names and passwords, which has the same negativecentralization effect with respect to privacy and linkage.finding: static passwords are the most commonly used form ofuser authentication, but they are also the source of many systemsecurity weaknesses, especially because they are often used inappropriately. (5.1)recommendation: users should be educated with respect to theweaknesses of static passwords. system designers must consider tradeoffs between usability and security when deploying authentication systems that rely on static passwords to ensure thatthe protections provided are commensurate with the risk and harmfrom a potential compromise of such an authentication solution.great care should be taken in the design of systems that rely onstatic passwords. (5.1)necessary; otherwise, attribute authentication (or no authentication) maysuffice.finding: authorization does not always require individual authentication or identification, but most existing authorizationsystems perform one of these functions anyway. similarly, arequirement for authentication does not always imply that accountability is needed, but many authentication systems generate and store information as though it were. (2.1)the use of authentication when it is not needed to achieve an appropriate level of security could threaten privacy. overall, privacy protecwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.executive summary5box es.3public key systemspublic key systems (sometimes implemented as public key infrastructures, orpkis) employ a sophisticated approach to authentication that relies heavily oncryptography. public key cryptography is often touted as a virtual panacea forecommerce and egovernment authentication and confidentiality challenges; however, implementation and deployment details are key to this technologyõs effectiveness, security, usability, and privacy protection. a critical component of somepublic key systems is a certificate authority (ca) that will certify that a particularkey belongs to a particular individual. one way to implement this functionality is touse a public ca (or trusted third party) to certify keys for multiple users and organizations. this practice, however, places much control in a centralized location,raising privacy and security concerns.the complexity of public key systems has made their ease of use and deployment a challenge. getting the underlying cryptography right is only half the battle.users must be educated with respect to how the systems should be used for maximum effectiveness. certificates must be distributed securely and revoked whennecessary. these systems require considerable storage, bandwidth, and computational ability. their privacy implications depend on how they are implementedand used. the scope of the pki (as with any authentication system) will be onedeterminant of how grave the attendant privacy risks are. at one end of the spectrum is a pki designed to operate in a limited context (for example, in a singleorganization or for a single function), and at the other end are pkis that attempt toprovide service to a very large population for a broad set of purposes.finding: many of the problems that appear to be intrinsic to publickey infrastructures (as opposed to specific public key infrastructure products) seem to derive from the scope of the public keyinfrastructure. (5.5)recommendation: public key infrastructures should be limited inscope in order to simplify their deployment and to limit adverseprivacy effects. software such as browsers should provide bettersupport for private (versus public) certificate authorities and forthe use of private keys and certificates among multiple computersassociated with the same user to facilitate the use of private certificate authorities. (5.3)finding: public certificate authorities and trusted third partiescould present significant privacy and security concerns. (5.3)finding: public key infrastructures have a reputation for being difficult to use and hard to deploy. current products do little to dispelthis notion. (5.4)who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.6who goes there?tion, like security, is poor in most systems in large part because systemsbuilders are not motivated to improve it.there is an inherent tension between authentication and privacy, because the act of authentication involves some disclosure and confirmationof personal information. establishing an identifier or attribute for usewithin an authentication system, creating transactional records, and revealing information used in authentication to others with unrelated interests all have implications for privacy. the many possible impacts ofbox es.4biometricsin addition to public key cryptography, biometrics is also often touted as aneffective authentication solution. as with any authentication technology, however,the truth of this claim depends, among other things, on the context in which thebiometric systems are used. òbiometric authenticationó (often called biometrics) isthe automatic identification or authentication of human individuals on the basis ofbehavioral and physiological characteristics. biometrics has the obvious advantage of authenticating the human, not just the presented token or password. common biometrics in use today verify fingerprints, retinas, irises, and faces, amongother things. downsides to biometrics include the fact that not all people can useall systems, making a backup authentication method necessary (and consequentlyincreasing vulnerability); the fact that revocation is not possible for current systems(the saying goes that most individuals òhave only two thumbsó); and that remoteenrollment of a biometric measure (sending oneõs fingerprint or iris scan over theinternet, for example) may defeat the purpose and is easily compromised.finding: biometric authentication technologies hold the promiseof improved user convenience. vendors of these technologies alsopromise reduced system management costs, but this has yet to bedemonstrated in practice. moreover, these technologies can poseserious privacy and security concerns if employed in systems thatmake use of servers to compare biometric samples against storedtemplates (as is the case in many largescale systems). their usein very local contexts (for example, to control access to a laptop orsmart card) generally poses fewer security and privacy concerns.(5.2)recommendation: biometric technologies should not be used toauthenticate users via remote authentication servers because ofthe potential for largescale privacy and security compromises inthe event of a successful attack (either internal or external) againstsuch servers. the use of biometrics for local authenticationñforexample, to control access to a private key on a smart cardñis amore appropriate type of use for biometrics. (5.2)who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.executive summary7authentication may not be considered by system designersñwhosechoices strongly influence how privacy is affectedñand they may not beappreciated by the public. most individuals do not understand the privacy and security aspects of the authentication systems they are requiredto use in interactions with commercial and government organizations. asa result, individuals may behave in ways that compromise their ownprivacy and/or undermine the security of the authentication systems.finding: authentication can affect decisional privacy, information privacy, communications privacy, and bodily integrity privacy interests. the broader the scope of use of an authentication system, the greater its potential impact on privacy. (3.1)the tension between security and privacy does not mean that theymust be viewed as opposites. the relationship between the two is complex: security is needed in order to protect data (among other things),and in many circumstances the data being protected are privacysensitive. at the same time, authentication may require the disclosure of personal information by a user. if many have access to that personal information, the value of the information for authentication is decreased, andthe decreased privacy of the informationñthrough othersõ access to personal information used in authenticationñcan also compromise security.a critical factor in understanding the privacy implications of authentication technologies is the degree to which an authentication system isdecentralized. a centralized password system, a public key system, or abiometric system would be much more likely to pose security and privacyhazards than would decentralized versions of any of these. the scope andscale of an authentication system also bear on these issues.finding: scale is a major factor in the implications of authentication for privacy and identity theft. the bulk compromise ofprivate information (which is more likely to occur when suchinformation is accessible online) or the compromise of a widelyrelied on documentissuing system, can lead to massive issuance or use of fraudulent identity documents. the result wouldadversely affect individual privacy and private and publicsector processes. (6.4)usability is a significant concern when determining how authentication systems should be deployed and used in practice. such systems willfail if they do not incorporate knowledge of human strengths and limitations. users need to be aware when an authentication (and hence possibly privacyaffecting) event is taking place. in addition, user understandwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.8who goes there?ing of the security and privacy implications of certain technologies andcertain modes of use plays a major role in the effectiveness of the technologies. for example, without a clear understanding of the security/privacy threats to the system, users may behave in ways that underminethe protections put in place by the designers.finding: people either do not use systems that are not designedwith human limitations in mind or they make errors in usingthem; these actions can compromise privacy. (4.1)recommendation: usercentered design methods should be integral to the development of authentication schemes and privacy policies. (4.2)there are ways to lessen the impacts on privacy that authenticationsystems have. guidelines include the following:recommendation: when designing an authentication systemor selecting an authentication system for use, one should¥authenticate only for necessary, welldefined purposes;¥minimize the scope of the data collected;¥minimize the retention interval for data collected;¥articulate what entities will have access to the collecteddata;¥articulate what kinds of access to and use of the data willbe allowed;¥minimize the intrusiveness of the process;¥overtly involve the individual to be authenticated in theprocess;¥minimize the intimacy of the data collected;¥ensure that the use of the system is audited and that theaudit record is protected against modification and destruction; and¥provide means for individuals to check on and correct theinformation held about them that is used for authentication. (3.2)more generally, systems should be designed, developed, and deployedwith more attention to reconciling authentication and privacy goals.recommendation: the strength of the authentication systememployed in any system should be commensurate with the valueof the resources (information or material) being protected. (2.1)who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.executive summary9recommendation: in designing or choosing an authenticationsystem, one should begin by articulating a threat model in order to make an intelligent choice among competing technologies, policies, and management strategies. the threat modelshould encompass all of the threats applicable to the system.among the aspects that should be considered are the privacyimplications of the technologies. (4.1)recommendation: individual authentication should not be performed if authorization based on nonidentifying attributes willsuffice. that is, where appropriate, authorization technologiesand systems that use only nonidentifying attributes should beused in lieu of individual authentication technologies. whenindividual authentication is required, the system should be subject to the guidelines in recommendation 3.2 (above). (2.3)recommendation: systems that demand authentication forpurposes other than accountability, and that do not themselvesrequire accountability, should not collect accountability information. (2.2)recommendation: system designers, developers, and vendorsshould improve the usability and manageability of authentication mechanisms, as well as their intrinsic security and privacycharacteristics. (4.5)recommendation: organizations that maintain onlineaccessible databases containing information used to authenticatelarge numbers of users should employ highquality information security measures to protect that information. whereverpossible, authentication servers should employ mechanismsthat do not require the storage of secrets. (6.2)multiple identities, linkage, and secondary usewho do you find when you authenticate someone? there is no singleidentity, identifier, or role associated with each person that is globallyunique and meaningful to all of the organizations and individuals withwhom that person interacts.finding: most individuals maintain multiple identities as social and economic actors in society. (1.1)who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.10who goes there?people invoke these identities under different circumstances. theymay identify themselves as named users of computer systems, employees, frequent fliers, citizens, students, members of professional societies,licensed drivers, holders of credit cards, and so on. these multiple identities allow people to maintain boundaries and protect privacy. thatcapacity diminishes with the number of identifiers used.finding: the use of a single or small number of identifiersacross multiple systems facilitates record linkage. accordingly,if a single identifier is relied on across multiple institutions, itsfraudulent or inappropriate use (and subsequent recovery actions) could have far greater ramifications than if used in only asingle system. (4.3)the networking of information systems makes it easier to link information across different, even unrelated, systems. consequently, manydifferent transactions can be linked to the same individual. systems thatfacilitate linkages among an individualõs different identities, identifiers,and attributes pose challenges to the goal of privacy protection. oncedata have been collected (such as from an authentication event or subsequent transactions), dossiers may be created.finding: the existence of dossiers magnifies the privacy risksof authentication systems that come along later and retroactively link to or use dossiers. even a socalled deidentifieddossier constitutes a privacy risk, in that identities often can bereconstructed from deidentified data. (4.2)secondary use of authentication systems (and the identifiers and/oridentities associated with them) is related to linkage. many systems areused in ways that were not originally intended by the system designers.the obvious example is the driverõs license: its primary function is tocertify that the holder is authorized to operate a motor vehicle. however,individuals are now asked to present their driverõs license as proof of age,proof of address, and proof of name in a variety of circumstances. asdiscussed in idsñnot that easy and in this report, the primary use of anauthentication system may require security and privacy considerationsvery different from those appropriate for subsequent secondary uses. (forexample, a driverõs license that certifies one is capable of driving a motorvehicle is a far cry from certification that one is not a threat to airlinetravel.) given the difficulty of knowing all the ways in which a systemmight be used, care must be taken to prevent secondary use of the systemas such use can easily lead to privacy and security risks.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.executive summary11finding: current authentication technology is not generallydesigned to prevent secondary uses or mitigate their effects. infact, it often facilitates secondary use without the knowledge orconsent of the individual being authenticated. (4.4)finding: secondary uses of authentication systems, that is, usesfor which the systems were not originally intended, often leadto privacy and security problems. they can compromise theunderlying mission of the original system user by fosteringinappropriate usage models, creating security concerns for theissuer, and generating additional costs. (4.5)at the extreme end of the identity spectrum is the concept of anonymity. anonymity continues to play an important role in preserving thesmooth functioning of societyñand it helps to protect privacy. the widespread use of authentication implies less anonymity.finding: preserving the ability of citizens to interact anonymously with other citizens, with business, and with the government is important because it avoids the unnecessary accumulation of identification data that could deter free speech andinhibit legitimate access to public records. (6.7)linkage and secondary uses of information and systems can belessened.recommendation: a guiding principle in the design or selection of authentication technologies should be to minimize thelinking of user information across systems unless the expresspurpose of the system is to provide such linkage. (4.3)recommendation: future authentication systems should bedesigned to make secondary uses difficult, because such usesoften undermine privacy, pose a security risk, create unplannedfor costs, and generate public opposition to the issuer. (4.4)the unique roles of governmentgovernment institutions play multiple roles in the area where authentication and privacy intersect. their approaches to authenticationand privacy protection may differ from those of private sector entities forstructural and legal reasons.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.12who goes there?finding: electronic authentication is qualitatively different forthe public sector and the private sector because of a governmentõs unique relationship with its citizens:a.many of the transactions are mandatory.b.government agencies cannot choose to serve only selected market segments. thus, the user population withwhich they must deal is very heterogeneous and may bedifficult to serve electronically.c.relationships between governments and citizens aresometimes cradle to grave but characterized by intermittent contacts, which creates challenges for technical authentication solutions.d.individuals may have higher expectations for governmentagencies than for other organizations when it comes toprotecting the security and privacy of personal data. (6.2)as a provider of services, the government has been seeking ways tomore easily authenticate users who require such services. in some cases,interagency and intergovernmental solutions may conflict with the fundamental principles espoused in the privacy act of 1974.finding: many agencies at different levels of government havemultiple, and sometimes conflicting, roles in electronic authentication. they can be regulators of private sector behavior, issuers of identity documents or identifiers, and also relying partiesfor service delivery. (6.1)finding: interagency and intergovernmental authentication solutions that rely on a common identifier create a fundamentaltension with the privacy principles enshrined in the privacyact of 1974, given the risks associated with data aggregationand sharing. (6.8)government plays a special role in issuing identity documents(driverõs licenses, birth certificates, passports, social security cards) thatare foundational documents relied upon to establish identity in numerous authentication systems. however, the processes used to producethese foundational documents are not necessarily sufficiently secure toserve their stated function. further, although states issue driverõs licensesand the federal government issues passports, each may depend on theother for reissuance or replacement; no single entity has a complete authoritative database. while on the one hand the lack of easy linkage canwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.executive summary13be seen as a privacy boon, on the other the relative ease with which somefoundational documents can be forged means that fraud is more likelyand security and privacy risks (including identity theft) are great.finding: many of the foundational identification documentsused to establish individual user identity are very poor from asecurity perspective, often as a result of having been generatedby a diverse set of issuers that may lack an ongoing interest inensuring the documentsõ validity and reliability. birth certificates are especially poor as base identity documents, becausethey cannot be readily tied to an individual. (6.3)recommendation: birth certificates should not be relied uponas the sole base identity document. supplemented with supporting evidence, birth certificates can be used when proof ofcitizenship is a requirement. (6.1)moving forwardwhen people express concerns about privacy, they speak about intrusion into personal affairs, disclosure of sensitive personal information, andimproper attribution of actions to individuals. the more personal the information that is collected and circulated, the greater the reason for theseconcernsñand the proliferation of authentication activity implies more collection and circulation of personal information. there are choices to bemade: is authentication necessary? if so, how should it be accomplished?what should happen to the information that is collected? it is time to bemore thoughtful about authentication technologies and their implicationsfor privacy. some of this thinking must happen among technologists, but itis also needed among business and policy decision makers.the tension between authentication and privacyñand the need forgreater care in choosing how to approach authenticationñwill grow inthe information economy. in addition to the management control concerns associated with security, the economic value of understanding thebehavior of customers and others is a strong motivator for capturingpersonal information. it is also a strong motivator for misusing suchinformation, even if it is only captured through authentication systems.the decision about where and when to deploy identity authenticationsystemsñif only where confirmation of identity is already required todayor in a greater range of circumstancesñwill shape society in both obviousand subtle ways. the role of attribute authentication in protecting privacy is underexplored. in addition, establishing practices and technicalmeasures that protect privacy costs money at the outset. many privacywho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.14who goes there?breaches are easy to conceal or are unreported; therefore, failing to protect privacy may cost less than the initial outlay required to establishsound procedural and technical privacy protections. if the individualswhose information has been compromised and the agencies that are responsible for enforcing privacy laws were to become aware of privacybreaches, the incentive for proactive implementation of technologies andpolicies that protect privacy would be greater.finding: privacy protection, like security, is very poor in manysystems, and there are inadequate incentives for system operators and vendors to improve the quality of both. (4.6)finding: effective privacy protection is unlikely to emerge voluntarily unless significant incentives to respect privacy emergeto counterbalance the existing incentives to compromise privacy. the experience to date suggests that market forces aloneare unlikely to sufficiently motivate effective privacy protection. (4.7)even if the choice is made to institute authentication systems onlywhere people today attempt to discern identity, the creation of reliable,inexpensive systems will inevitably invite function creep and unplannedfor secondary uses unless action is taken to avoid these problems. thus,the privacy consequences of both the intended design and deploymentand the unintended uses of authentication systems must be taken intoconsideration by vendors, users, policy makers, and the general public.recommendation: authentication systems should not infringeupon individual autonomy and the legal exercise of expressiveactivities. systems that facilitate the maintenance and assertionof separate identities in separate contexts aid in this endeavor,consistent with existing practices in which individuals assertdistinct identities for the many different roles they assume.designers and implementers of such systems should respectinformational, communications, and other privacy interests asthey seek to support requirements for authentication actions.(3.1)the federal government has passed numerous laws and regulationsthat place constraints on the behavior of private sector parties as well ason government agencies. among them are the family educational rightsand privacy act, the financial services modernization act, the healthinsurance portability and accountability act of 1996, and, in 1974, thewho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.executive summary15privacy act, which regulates the collection, maintenance, use, and dissemination of personal information by federal government agencies.given the plethora of privacyrelated legislation and regulation, makingsense of government requirements can be daunting.toolkitwith a basic understanding of authentication, privacy interests andprotections, and related technologies, it is possible to consider how onemight design an authentication system that limits privacy intrusions whilestill meeting its functional requirements. this report provides a toolkitfor examining the privacy implications of various decisions that must bemade when an authentication system is being contemplated. as mentioned previously, most of these decisions can be made irrespective of theparticular technology under consideration.the kind of authentication to be performed (attribute, identity, orindividual) is an initial choice that will bear on the privacy implications.viewed without regard to the resource that they are designed to protect,attribute authentication systems present the fewest privacy problems andindividual authentication systems the most. despite the fact that it raisesmore privacy concerns, in some instances individual authentication maybe appropriate for privacy, security, or other reasons.in the process of developing an authentication system, several questions must be answered early. decisions will have to be made aboutwhich attributes to use, which identifiers will be needed, which identitywill be associated with the identifier, and how the level of confidenceneeded for authentication will be reached. the answers to each of thesequestions will have implications for privacy. chapter 7 elaborates on fourtypes of privacy (information, decisional, bodily integrity, and communications) and on how they are affected by the answers to each of the preceding questions. the analysis proposed is technologyindependent, forthe most part, and can be applied to almost any proposed authenticationsystem.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.161introduction and overviewthe growth of technologies that ease surveillance, data collection,disclosure, aggregation, and distribution has diminished the obscurity and anonymity that are typical of everyday interactions.from phone systems that block the calling number on outgoing calls andsimultaneously identify all incoming callers,1 to òloyaltyó programs thatcollect data about individualsõ purchasing habits,2 to the governmentõsuse of tracking and identification technologies in an increasingly broadrange of environments, records of individualsõ activities are now routinely made and stored for future use. technologies such as facial recognition and video cameras are being deployed in an attempt to identifyand/or monitor individuals surreptitiously as they go about the mostmundane of activities.3 ubiquitous computing promises to put computa1òpacific bell offers privacy manager,órboc update 12(5) (new offering for percall control over incoming messages); beth whitehouse, òin pursuit of privacy: phone servicesdesigned to protect can also be extremely frustrating,ó newsday, march 26, 2001, p. b03(problems arising from use of caller id and callblocking plans).2see, generally, marion agnew, òcrm plus lots of data equals more sales for bordersñretail convergence aligns webbased marketing and strategies with those of physicalstores,ó informationweek, may 7, 2001 (bordersõ plan to merge online and offline customerdata and loyalty programs); kelly shermach, òcoalition loyalty programs: finding strengthin numbers,ó card marketing 5(3):1 (benefits of shared data from joint marketing card products).3lev grossman, òwelcome to the snooper bowl: big brother came to super sunday,setting off a new debate about privacy and security in the digital age,ó time, februarywho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.introduction and overview17tional power everywhere by embedding it seamlessly and unobtrusivelyinto homes, offices, and public spaces. the fully networked environmentthat ubiquitous computing is making possible raises complicated questions about privacy and identification.4 what does it mean when datacollection, processing, and surveillanceñand perhaps authentication andidentificationñbecome the norm?in applications ranging from electronic commerce to electronic taxfiling, to controlling entry to secured office buildings, to ensuring payment, the need to verify identity and authorize access has driven thedevelopment of increasingly advanced authentication systems. thesesystems vary widely in complexity and scope of use: passwords in combination with electronic cookies are used for many electronic commerceapplications, smart cards coupled with biometrics allow access to securedareas, and sophisticated publickey mechanisms are used to ensure theintegrity of many financial transactions. while there are many authentication technologies, virtually all of them involve the use of personal information and, in many cases, personally identifiable information, raisingnumerous privacy concerns.this report examines authentication technologies through the lens ofprivacy. it is aimed at a broad audience, from users (both end users andorganizations) of authentication systems, to people concerned with privacy broadly, to designers and implementers of authentication technologies and systems, to policy makers.12, 2001, p. 72 (the use of facial recognition technology by the tampa bay police departmentto search the 72,000 people in the crowd at super bowl xxxv); ace atkins, òsurveillancetactic faces off with privacy,ó tampa tribune, february 7, 2001, p. 1 (police might buycontroversial new technology, tried out at the super bowl, that scans faces in public places;surveillance cameras take pictures of people in crowds and a computer compares numericfacial patterns to a databank of criminals); katherine shaver, òarmey protests camerassought on gw parkway; speed deterrent likened to big brother,ó washington post, may 9,2001, p. b01 (the national park service tested a radar camera from august 1999 to february2000 in two areas of the george washington memorial parkway in the washington, d.c.,area, and house majority leader richard armey asked department of the interior secretary gale a. norton to ban the cameras, calling them òa step toward a big brother surveillance stateó); richard morin and claudia deane, òdna databases casting a wider net,washington post, may 8, 2001, p. a21 (the national dna database and the fact that all 50states have passed some version of a dna databanking law); ian hopper, ònew documents disclose extent of fbiõs web surveillance,ó sunday gazette mail, may 6, 2001, p. p6d(the fbiõs use of internet eavesdropping using its controversial carnivore systemña set ofsoftware programs for monitoring internet traffic [emails, web pages, chatroom conversations, and other signals]ñ13 times between october 1999 and august 2000 and a similardevice, etherpeek, another 11 times.)4see cstbõs report embedded, everywhere: a research agenda for networked systems ofembedded computers (washington, d.c., national academy press, 2001), particularly chapter 4, which discusses security and privacy in ubiquitous computing environments.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.18who goes there?notwithstanding considerable literature on privacy, the legal and social meaning of the phrase òthe right to privacyó is in flux. rather thanpresenting an encyclopedic overview of the various technologies or an indepth treatise on privacy, this report explores the intersection of privacyand authentication, which raises issues of identification, authorization,and security.this introductory chapter presents definitions and terminology thatare used throughout the report. it introduces four overarching privacyconcerns that illustrate how privacy and authentication can interact inways that negatively affect privacy. it also provides a òdayinthelifeóscenario to motivate a discussion of authentication and privacy. finally,there is a brief discussion of what this report does not do, along with anoutline of the rest of the report.definitions and terminologythroughout this report, numerous interrelated concepts associatedwith authentication, identity, and privacy are discussed. several of theseconcepts are briefly defined below for clarity. as noted in the committeeõsfirst report, idsñnot that easy, many of these concepts represent complicated, nuanced, and, in some instances, deeply philosophical topics.5note that while the definitions below refer to individuals, they shouldalso be understood to apply, when appropriate, to nonhuman subjectssuch as organizations, identified computers, and other entities. popularbelief to the contrary, authentication does not necessarily prove that aparticular individual is who he or she claims to be; instead, authenticationis about obtaining a level of confidence in a claim. the concepts below areteased apart both to describe how the terms are used in this report and tohighlight how ambiguous many of them remain.¥an identifier points to an individual. an identifier could be aname, a serial number, or some other pointer to the entity being identified. examples of personal identifiers include personal names, socialsecurity numbers (ssns), credit card numbers, and employee identification numbers. it is sometimes necessary to distinguish between identifiers and the things that they identify. in order to refer to an identifier in away that distinguishes it from the thing that it identifies, the identifier iswritten in quotation marks (for example, òjoseph k.ó is an identifierñspecifically, a personal nameñwhereas joseph k. is a person).5indeed, the committee has refined and evolved its core definitions since the publicationof its earlier report idsñnot that easy: questions about nationwide identity systems (washington, d.c., national academy press, 2002).who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.introduction and overview19¥an attribute is a property associated with an individual. examples of attributes include height, eye color, employer, and organizational role.¥identification is the process of using claimed or observed attributes of an individual to infer who the individual is. identificationcan be done without the individualõs having to (or being given the opportunity to) claim any identifier (for example, an unconscious patient in anemergency room might be identified without having to state his or hername).¥authentication is the process of establishing confidence in thetruth of some claim. the claim could be any declarative statementñforexample, òthis individualõs name is ôjoseph k.,õ ó or òthis child is morethan 5 feet tall.ó both identifiers and attributes can be authenticated, asthe examples just cited demonstrate.ñindividual authentication is the process of establishing an understood level of confidence that an identifier refers to a specificindividual. individual authentication happens in two phases:(1)an identification phase, during which an identifier to beauthenticated is selected in some way (often the identifier selectedis the one claimed by the individual), and (2) an authenticationphase, during which the required level of confidence is established(often by challenging the individual to produce one or more authenticators supporting the claim that the selected identifier refers tothe individual). in the information security literature, individualauthentication is sometimes referred to as òuser authentication.óin the biometrics literature, individual authentication of an identifier claimed by the individual is often called òverification.óñidentity authentication is the process of establishing an understood level of confidence that an identifier refers to an identity. itmay or may not be possible to link the authenticated identity to anindividual. for example, verification of the password associatedwith a hotmail account authenticates an identity (foo@example.com)that may not be possible to link to any specific individual. identityauthentication happens in two phases: (1) an identification phase,during which an identifier to be authenticated is selected in someway (often the identifier is selected by a claimant), and (2) anauthentication phase, during which the required level of confidence is established (often by challenging the claimant to produceone or more authenticators supporting the claim that the selectedidentifier refers to the identity).who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.20who goes there?ñattribute authentication is the process of establishing an understood level of confidence that an attribute applies to a specificindividual. attribute authentication happens in two phases: (1) anattribute selection phase, during which an attribute to be authenticated is selected in some way, and (2) an authentication phase,during which the required level of confidence is established, eitherby direct observation of the individual for the purpose of verifyingthe applicability of the attribute or by challenging the individual toproduce one or more authenticators supporting the claim that theselected attribute refers to the individual.¥an authenticator is evidence that is presented to support theauthentication of a claim. it increases confidence in the truth of theclaim. a receipt, for example, can act as an authenticator of a claim thatan item was purchased at a specific store.6 a driverõs license can act as anauthenticator that a particular name (a form of identifier) refers to theindividual who carries the license. knowledge of a secret or the ability todisplay some distinctive physical characteristic such as a fingerprint canalso serve as the authenticators of an individualõs name.¥authorization is the process of deciding what an individualought to be allowed to do. authorization is distinct from authentication(which establishes what an individual òisó rather than what the individual òis allowed.ó) authorization policies determine how authorization decisions are made. authorization policies base decision making ona variety of factors, including subject identifiers (such as names) andsubject attributes other than identifiers (such as employee status, creditrating, and so on).¥the identity of x is the set of information about an individual xthat is associated with that individual in a particular identity system y.however, y is not always named explicitly. an identity is not the sameas an identifierñso òjoseph k.ó is an identifier (specifically, a name), butjoseph k. is a person. it is not always easy to determine which individualan identifier refers to. for example, ògeorge bush, the president of theunited states, who lives in texas and who attended yaleó is an identifierthat refers to two individuals. identities also consist of more than justnamesñso richard nixon was an individual, but his identity also includes other facts, such as that he was president of the united states andthat he resigned that office. furthermore, identities contain statementsthat are not strictly factsña man who was stranded on a desert island in6confusion can arise when the same thing is used as both an authenticator and an identifier, as happens frequently with credit card numbers.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.introduction and overview211971 and who believed in 1975 that richard nixon was still presidentwould have his facts wrong but would not misidentify nixon. finally,people disagree about identities and about which individuals they referto; if one believes newspaperman bob woodward, there was an individual who went by the code name òdeep throató during the watergateinvestigation that led to nixonõs resignation, but different people havedifferent opinions about who that individual is.¥security refers to a collection of safeguards that ensure the confidentiality of information, protect the integrity of information, ensurethe availability of information, account for use of the system, and protect the system(s) and/or network(s) used to process the information.security is intended to ensure that a system resists attacks and toleratesfailures. (see chapter 4 for a more indepth discussion of security andauthentication.)¥privacy is a multifaceted term with many contextually dependent meanings. one aspect of the right to privacy is the right of anindividual to decide for himself or herself when and on what terms hisor her attributes should be revealed. (see chapter 3 for some historicalbackground on privacy and a brief exploration of current privacy law andpolicy in the united states.)authentication in daily lifeindividuals authenticate themselves to others and to information systems in many different contexts. the identifiers and attributes that theyauthenticate vary, depending on the situation. individuals may identifythemselves as named users of computer systems, employees, frequentflyers, citizens, students, members of professional societies, licensed drivers, holders of credit cards, adults over the age of 18, and so on. thereneed not be any single identity associated with each person that is globally unique and meaningful to all of the organizations and individualswith whom that person interacts. thus, people often assert differentidentities under different circumstances.finding 1.1: most individuals maintain multiple identities associal and economic actors in society.to illustrate the myriad ways in which instances of identification andauthentication arise in everyday life and to highlight some of the important issues associated with new systems, the committee hypothesizedscenarios in the life of joseph k. as he goes on a business trip. the italicsentences describe josephõs actions; the indented paragraphs that followwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.22who goes there?point out important associated issues. (specific technologies are discussedin more detail later in the report.)joseph first dials in to his corporate network from home and authenticates himself toa network access server. he does so by claiming to be an employee of compudigicorporation, using a name and a smart card that is read by his computer.successfully completing this authentication procedure authorizesjoseph to access the corporate network. all employees have the samebasic access privileges for the network, so it might seem that there isno need to authenticate each employee independently by name forlogin purposes. however, by assigning each employee a uniquelogin name, compudigi can track josephõs login sessions separatelyfrom those of other employees, enabling audit, and it can more easilyrevoke josephõs access if he leaves the company or if his smart card islost or stolen.joseph now accesses an airline web site to book his flights, probably unaware thatauthentication of another sort is going on.the web site employs secure sockets layer (ssl), a security protocol,to provide confidentiality for data transmitted between josephõspersonal computer (pc) and the site. this prevents eavesdropperson the path between the pc and the web site from observing sensitivedata. it also provides an implicit authentication of the web site tojoseph. this authentication is based on the internet name of the website, as contained in the uniform resource locator (url) that josephimplicitly selected from his list of commonly accessed web sites.joseph is generally unaware of this authentication process unless it fails andgenerates a warning message. the only indication to him that the process hassucceeded is the appearance of a small padlock icon in the browser window (whichhe may not notice). joseph now uses his airline frequentflyer account number toidentify himself and a personal identification number (pin) to authenticate thisidentifier.the airline is not necessarily interested in josephõs identity as anemployee of compudigi but rather in his identity as a customer ofthe airline.based on his frequentflyer status, joseph is able to request a seat with betterlegroom in the front section of the aircraft.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.introduction and overview23joseph is authorized to upgrade his seat based on his frequentflyerstatus (an attribute), which in turn is based on his travel history.(josephõs frequentflyer number may remain constant with the airlinefor many years, but his status and hence his authorization to upgradeto a better seat will vary depending on how often he flies.) thus,josephõs frequentflyer number (an identifier) is used as a key for adatabase that the airline uses to determine his status and hence hisauthorization.to pay for his flight, joseph provides a credit card account number. knowledge ofthe account number and expiration date serves to authenticate him as acardholder.using a credit card number and expiration date as authenticators is arelatively weak form of authentication, since the account numberserves as the primary identifier as well. this credit card data mightbe stored on the web server; or, it might be used only for thetransaction at hand and not be stored on the web server. if therewere a way for joseph to be sure that the web server was not storinghis credit card information, it might increase his trust in the system(assuming that he had been notified of this policy).an electronic ticket is issued for josephõs flights. next, he wishes to connect tothe web site of a hotel chain to book a room.this web site supports a feature known as client certificates, a littleused facet of ssl that can be employed to automate the userauthentication process. when joseph initially registered on the website as a frequent guest of the hotel chain, the site interacted with hisbrowser in order to issue him a public key certificate (an electronic filecontaining information related to josephõs interactions with this site;see chapter 5 for more on public key cryptography, private keys, andcertificates). this certificate contains an identifier that links to josephõsaccount but is otherwise not meaningful. thus, the certificate cannotbe used by joseph to authenticate himself to any other web sites.during the initial certificate generation process, joseph was promptedto provide a password to be used by his browser to protect the privatekey associated with the certificate. this single password could protectall of the private keys stored by josephõs browser for use with all of thecertificates issued by web sites that joseph visits. such use of thepassword would simplify josephõs life if he had many certificates, butfew web sites make use of client certificates, so in practice josephwould gain only a small benefit from this feature. note that in termswho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.24who goes there?of security, the private key becomes a proxy for the password(s) andis thus no more secure than the combination of that password andthe physical means used to protect the encrypted private key.when joseph visits the hotel web site (having registered and received a certificateearlier), his browser is queried by the web site to send josephõs certificate and touse the associated private key to verify josephõs frequentguest account identifier.joseph is prompted by the browser to enter the password to unlock his privatekeys, and he is logged in to the web site.again, it is josephõs identity as a frequent client (rather than his nameor other attributes) that is important. his status as a frequent guestentitles him to a free room upgrade. this is another example ofauthorization based on data associated with josephõs identity in aspecific context. in this context, joseph elected to store credit cardinformation as part of his profile with the hotel chain, so it is usedautomatically to guarantee his reservation in the event of a latearrival. if the web site does not adequately protect the data that itstores, josephõs credit card data may be inappropriately disclosed toothers. the use of encryption to protect josephõs data in transit to thesite does not protect against this sort of security failure in any way.joseph has also elected to store several frequentflyer numbers in his hotel profileso that he can acquire òmileageó credit for his stay.with this action, joseph has voluntarily elected to provide data to thehotel chain, enabling the hotel to link his (otherwise) independenthotel and airline identities. this provides the hotel marketingorganization with an ability to market directly to joseph on the basisof his travel patterns and preferences, as well as to offer amenities ina more customerfriendly fashion when joseph stays at its hotels. italso provides an ability to sell josephõs name, address, and possiblyhis email address to other companies, based on the attributes in hisfrequenttraveler profile.finally, joseph logs in to a rental car web site and arranges for a vehicle for histrip. here, joseph authenticates himself using his name and his frequentrenteraccount number; no explicit password or pin is required.josephõs profile at this web site allows the rental car company toselect automatically his favorite class of vehicle. joseph has alsoprovided a code that identifies him as an employee of compudigi,making him eligible for the special rates negotiated by compudigiwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.introduction and overview25for its employees. this code is an attribute not specific to joseph; it isused as a basis for authorizing all employees to make use of thecorporate discount program. josephõs profile includes credit carddata as well as his driverõs license data, both of which are requiredfor rental car transactions.en route to the airport, joseph makes use of an electronic toll tag lane, whichallows him to avoid longer lines for drivers paying tolls with cash.the toll tag device, mounted on the windshield of josephõs car,engages in an electronic (radio frequency (rf)) challenge/responseauthentication protocol with a responder at each toll plaza,authenticating the toll tag device to the toll collection system. thissystem authenticates the tagõs number, which is linked to josephõsaccount identity in the toll system database. in turn, this number islinked to josephõs bank account, enabling automatic debit of hisaccount for each toll transaction. the toll system may be concernedonly with receiving payment of the toll, so it is the identification ofthe bank account that is of primary interest here.7joseph arrives at the airport and makes use of a kiosk to acquire his boarding pass.to authenticate himself, he swipes the same credit card that he used to purchasethe airline ticket through a magneticstripe reader.in this case, possession of the credit card is viewed as authenticationof identity.at the airport security checkpoint, joseph must present his boarding pass and agovernmentissued photo identification (id) for authentication.the name on the photo id must match (either exactly or òcloselyó)the name on the boarding pass, and the photo on the id must be agood enough likeness to be acceptable to the security screeningpersonnel.upon arrival at his destination airport, joseph proceeds to the rental car area,where his car is waiting in a spot at which his name is displayed. as he exits therental car lot, joseph is required to present his driverõs license.7while it may be possible to link the tag to a cash account that is not linked to the driver,in many cases such systems do make explicit the linkage between the account and the(presumed) driver.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.26who goes there?this procedure is designed to authenticate joseph as the individualwho holds the online reservation and to whose credit card the rentalwill be charged. in principle, the process should also verify thatjoseph holds a valid driverõs license, a prerequisite for car rental. incontrast to the boardingpass check at the airport, the rentalagreement has more information about joseph, including the nameof the state that issued the driverõs license and the license number.such information is nominally part of this authentication process,providing more evidence that the person presenting the license to theelectronic record is connected to a printed receipt. also, note thatwhile a passport would be an acceptable form of photo id for usewith the boarding pass (and would be required for internationalflights), it is not acceptable here, because there is a requirement for acredential that demonstrates authorization to drive and thatestablishes accountability of a particular individual for loss of ordamage to the automobile. a driverõs license accomplishes bothgoals, because it directly asserts authorization to drive and because itcontains or can be used to obtain the driverõs address. the rental caragency (depending on the state in which joseph is renting) may havereserved the right to screen josephõs driving record, which it mayaccess electronically using his driverõs license number.when joseph arrives at his hotel, he presents a credit card at the front desk. thehotel matches the name on the credit card against the roomreservation databaseto identify joseph.since the primary concern of the hotel is that it is compensated forthe room rental, the presentation of a valid credit card (includingverification that the credit card account is in good standing, notreported lost or stolen) is an acceptable form of authentication in thiscontext.8 the credit card is itself authenticated on the basis of theinformation contained on the magnetic stripe on the back of the cardand on the basis of the appearance of the card (for example, theappearance of a standard hologram as part of the card face). if aconflict occursñtwo individuals with the same name claim the samereservation at the same hotel on the same dayñadditionalidentification credentials will be required to resolve the conflict.8note that hotels in countries other than the united states often are required to requestthe presentation of a passport and sometimes even retain the document until the guestchecks out.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.introduction and overview27when joseph arrives at the compudigi meeting site, he uses his employee badgeto gain entrance to the building. joseph presents the card to a reader, whichrequires him to enter a pin, a procedure designed to prevent unauthorized use ofthe card if it is lost or stolen.josephõs badge is a smart card, a creditcardsized device thatcontains a processor, memory, and an input/output (i/o) interface.on this card is stored a public key certificate and correspondingprivate key. the card engages in a cryptographic challenge/responseexchange with the buildingõs physical security computer system toauthenticate joseph as a compudigi employee and to authorize himto enter the building.this scenario illustrates that joseph has many identities, not just one.these different identities represent him in his interactions with differentorganizations, each of which identifies him in a distinct context. in manyinstances, there is no need for these distinct identities to be tightly linkedto one another, although there are exceptions. sometimes joseph makesan explicit choice to create the linkage (for example, for perceived benefits); at other times the linkage is required by the context (for example,the connection of his driverõs license and his driving record). to the extentthat joseph chooses, or is allowed, to maintain separate identities in hisinteractions with organizations, he increases his privacy, because he discloses to each organization only the information required for interactionswith that organization.by maintaining separate and nonlinked identities, joseph has somecontrol over who gets which pieces of information about his activities,preferences, and lifestyle. some of this control might be deliberate onjosephõs part, but some of it may have been the happenstance of a competitive market system in which linkages have not yet been fully unifiedacross corporate and government databases. for joseph to exercise proactive control over the dissemination and use of personal informationabout himself, he must become aware of how and where that informationis being collected, linked, and used. as activities within society becomeincreasingly automated, it becomes harder and harder for anyone to makethese informed decisions.without informed, proactive control on josephõs part, the variousauthentication events described in this scenario pose risks in terms ofboth security and privacy. the rest of this report elaborates on variousauthentication technologies and their relationship to privacy issues.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.28who goes there?current tensionsthe development, implementation, and broad deployment of authentication systems require us to think carefully about the role of identityand privacy in a free, open, and democratic society. privacy, includingcontrol over the disclosure of oneõs identity and the ability to remainanonymous, is an essential ingredient of a functioning democracy. it is aprecondition for the exercise of constitutionally protected freedoms, suchas the freedom of association.9 it supports the robust exercise of thefreedom of expression by, for example, creating psychological space forpolitical dissent.10 it maintains social norms that protect human dignityand autonomy by enabling expressions of respect and intimacy and theestablishment of boundaries between oneself and oneõs community.119see national association for the advancement of colored people v. alabama ex rel. patterson,attorney general, 357 u.s. 449; 78 s. ct. 1163 (1958); 2 l. ed. 2d 1488 (1958) (the court heldthat the immunity from state scrutiny of membership lists was so related to the right of themembers to associate freely with others as to come within the protection of the u.s. constitution); joseph mcintyre, executor of estate of margaret mcintyre, deceased, petitioner v. ohioelections commission, 514 u.s. 334; 115 s. ct. 1511 (1995) (statute prohibiting the distributionof anonymous campaign literature violated the first amendment, as it was not narrowlytailored to serve an overriding state interest; the statute indiscriminately outlawed a category of speech with no relationship to the danger sought to be prevented); buckley v.american constitutional law foundation; taley v. california. also, see the work that the electronic privacy information center (epic) has done on anonymity, including an amicusbrief in the watchtower bible v. stratton case, arguing that òan ordinance requiring doortodoor petitioners to obtain a permit and identify themselves upon demandó implicates privacyas well as rights of anonymity, freedom of expression, and freedom of association. moreinformation is available online at <http://www.epic.org/freespeech/watchtower.html>.10see martin h. redish, òthe value of free speech,ó 130 u. pa. l. rev. 591, pp. 601604(1982) (free expression supports citizensõ participation in decision making); alexandermeiklejohn, political freedom: the constitutional powers of the people, new york, oxford university press, 1965, pp. 389 (free expression provides citizens with access to informationnecessary to formulate opinions and make decisions); rodney a. smolla, smolla and nimmeron freedom of speech: a treatise on the first amendment, clark boardman callaghan, 1994,¤13.01[3] (by allowing disempowered groups to dissent, free expression provides stability);and julie e. cohen, òa right to read anonymously: a closer look at ôcopyright managementõ in cyberspace,ó 28 conn. l. rev. 981 (1996) (arguing that reading is intimately connected with freedom of speech and thought and therefore the right to read anonymouslyshould be an understood guarantee of the first amendment).11robert c. post, òthe social foundations of privacy: community and self in the common law tort,ó 77 calif. l. rev. 957 (1989). post argues that the common law tort ofinvasion of privacy safeguards social normsñòrules of civilityóñis based on the belief thatpersonality and human dignity are injured when these rules of civility are broken. heconcludes with an explanation of the role that the privacy tort plays in enabling individualsto receive and express respect, thereby enabling human dignity; in allowing individuals toreceive and express intimacy, thereby enabling human autonomy; and in establishing obliwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.introduction and overview29if individuals fear unchecked scrutiny, they will be less likely to participate vigorously in the political process and in society in general.12 ifindividuals are denied privacyñby the government, corporations, andother individualsñthey are less able to explore ideas, formulate personalopinions, and express and act on these beliefs. at the same time, òprivacyó is sometimes used as a pretext for hiding illegal activities, andsociety has, at times, a legitimate interest in requiring authentication oridentification, either for validating claims to rights and privileges or forholding individuals responsible for their actions.today, when individual authentication is demanded (such as beforeboarding an airplane), the individual whose identity is to be authenticated is asked to participate in the process of proving who he or she is.13authentication of identity generally (but not always; see chapter 4) requires an affirmative actñthe individual must affirmatively introduceherself or knowingly produce a credential containing identity information. while a third party may at times provide information about anindividualõs identity (such as an adult verifying the identity of a child),such information is more often a tool for confirming the identity presented by the individual being authenticated. because authenticationgenerally requires some affirmative act on the part of the individual, it israre that an individualõs identity is surreptitiously noted and recorded inthe context of an authentication event.the decision about where to deploy authentication systemsñbe itonly where today verification of identity is already required or in a greaterrange of circumstancesñwill shape society in both obvious and subtleways. even if the choice is made to implement authentication systemsonly where people today attempt to discern identity, the creation of reliable, inexpensive systems will invite function creepñthe use of authentication systems for other than their originally intended purposesñunlessaction is taken to prevent this from happening.14 thus, the privacy congations between community members, thereby defining the substance and boundaries ofcommunity life. id. at p. 238; bloustein, òprivacy as an aspect of human dignity: ananswer to dean prosser,ó 39 n.y.u. l. rev. 962, pp. 10001007 (1964) (arguing that theprivacy torts involve the same interest in preserving human dignity and individuality).12see, generally, the numerous privacy statutes that prevent the reuse of information andlimit governmental access because of social interest in promoting or protecting the underlying activities (for example, related to financial information and health care), many of whichare discussed in chapters 3 and 6.13the criminal justice context is an exception in which the individualõs identity may bedetermined without their active participation.14an example of secondary use is that of reliance on the driverõs license for proof of agein establishments that sell alcohol. in at least one establishment in massachusetts, licensesare swiped through a machine and all of the information contained in the magnetic stripewho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.30who goes there?sequences of both the intended design and deployment and the unintended, secondary uses of authentication systems must be taken into consideration by vendors, users, policy makers, and the general public.four overarching privacy concernswhile authentication systems can be used to preserve or enhanceprivacy, there are many ways, as described above, in which an authentication system, or even the act of authentication alone, can affect privacy;that is, privacy is involved as a consequence or corollary of authentication. before discussing the details of authentication technologies andtheir impact on privacy in later chapters, several categories of privacy riskare described below. while not applicable to all authentication systems,these categories broadly characterize the risks to personal privacy thatauthentication systems can create.¥covert identification. some authentication systems make it possibleto identify an individual without the individualõs consent or even knowledge. such systems deny the individual, and society, the opportunity toobject to and to monitor the identification process. these technologies areparticularly vulnerable to misuse because their use is hidden.¥excessive use of authentication technology. cost and public sensitivityhave historically checked the spread of authentication systems. at thesame time that technological progress has reduced the cost of these systems (along with the costs of data collection and processing generally),the public, owing to an increased sense of vulnerability and desire forsecurity or simple familiarity, has become accustomed to demands forauthentication. together, these trends increase the likelihood that authentication systems will become more prevalent. led by a mentality ofòmore is better,ó the public and private sectors have been quick to increase the collection of personal information where this process is supported by cheaper, easier technology.¥excessive aggregation of personal information. the use of a single identifier (such as the social security number) or a small number of identifierscreates the opportunity for more linking of previously separate repositories of personal information. today, different record keepers have different ways of identifying individuals (and in some cases of tying theiridentities to transaction histories). the many cards that people carry intheir wallets reveal some of the multiple identities by which they areon the back is collected. òswipe at your privacy,ó whdh tv, june 4, 2002. availableonline at <http://www.whdh.com/features/articles/specialreport/h37/>.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.introduction and overview31known. the adoption of a single (or small number of) authenticationsystems across the public and private sector would greatly erode privacyby facilitating the linkage of records maintained by many disparate recordkeepers.15¥chilling effects. wherever identity authentication is required, thereis an opportunity for social control. in some instances such control is alaudable goal (such as in contexts that require high security and accountability). but in other areas, there is a risk that new methods of socialexclusion and vehicles for prejudicial social control will be created. forexample, in a world in which a single identifier (for example, a socialsecurity number) is relied on by many public and private institutions, theorganization in charge of issuing this identifier (the government, in thisexample) could interfere with a citizenõs ability to engage in a wide rangeof legitimate private sector transactions by revoking the identifier; or, athief could interfere with the same abilities by stealing the identifier andusing it fraudulently.while there are risks to privacy with some authentication systems, itshould be noted that there are situations in which authentication providesan important method of ensuring accountability and of protecting privacy.for example, when specific individuals are granted access to personal orproprietary information for limited purposes, authentication can play animportant role in monitoring and enforcing adherence to relevant regulations and laws limiting individualsõ access to these purposes.what this report does and does not dothis report explores the concepts of authentication, identity, and privacy. it examines various authentication technologies and describes theirprivacy implications. the report does not recommend specific technologies for specific purposes, nor does it provide an explicit cost analysissuch as might be provided by a consultant. instead, the report discussesthe various technologies and elaborates on the tradeoffs with respect toprivacy that each technology permits. as the remainder of the reportmakes clear, analyses of specific systems or proposed systems can proceed only with an understanding of the context in which a system will beoperating and an understanding of the goals that the system is trying tomeet. this report provides a framework for these issues and the necessary vocabulary within which to consider them.15see this committeeõs first report, idsñnot that easy: questions about nationwide identity systems, washington, d.c., national academy press, 2002, for a discussion of additional questions and issues raised by largescale, widely used identity systems.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.32who goes there?this report seeks to identify ways in which authentication technologies are directly and indirectly affecting privacy. it recognizes that bothgovernment and commercial parties do, under many circumstances, havea legitimate need to determine with whom they are dealing. it exploresways in which current authentication systems operate without adequateheed to personal privacy. the report recommends ways in which privacyinterests might be better served without compromising the legitimateinterests of commercial and government entities that employ authentication technologies.chapters 2 and 3 elaborate on the concepts of authentication andprivacy to establish the framework for the discussion in the remainder ofthe report. given the historical association of authentication with security, chapter 4 describes security concerns that motivate authenticationand then discusses how usability issues matter, both for security andprivacy. chapter 5 examines particular authentication technologies anddescribes some of the technological issues that arise. chapter 6 outlinessome of the unique challenges facing governments and government agencies with respect to authentication and privacy. finally, chapter 7 presents a toolkit for thinking through the implications for privacy of thechoices made with respect to how authentication systems are developedand deployed.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.332authentication in the abstractbefore examining specific authentication technologies and their implications for privacy, a discussion of the terms and concepts themselves is in order. colloquial uses of the term òauthenticationó areoccasionally misleading; for example, authentication is neither authorization nor identification. while this report does not attempt a comprehensive examination of privacy in the context of an informationrich world,1this chapter and the next provide a foundation for thinking about authentication and privacy and a context for the discussions of specific technologies in later chapters.what is authentication and why is it done?authentication is the process of establishing confidence in the truth ofsome claim. while this report focuses primarily on authentication in thecontext of information and computing systems, authentication occursoutside this realm as well, as examples throughout the text illustrate.box2.1, for example, presents a brief discussion of authentication in thecontext of absentee voting. in the context of information security, theunqualified term òauthenticationó is often used as shorthand to meanòverification of a claimed identity,ó although for the purposes of thisreport, a slightly more nuanced meaning is assumed. (see chapter 1 for1another cstb committee is examining the broad topic of privacy in the information age;the status of this project is available online at <http://www.cstb.org/projectprivacy/>.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.34who goes there?definitions of these and related terms.) it is possible to authenticate bothhuman users and entities that are not humans (for example, cellular telephone networks in the united states directly authenticate cell phone handsets rather than handset users2), and it is possible to authenticate claimsthat do not relate to usersõ personal names (for example, an individualmay claim to be tall enough to enjoy a heightrestricted ride at a countyfair; this claim can be verified without knowing the individualõs name).authentication is not usually an end in itself. information systemsusually authenticate users in order to satisfy security or other requirements.3 most commonly, security systems authenticate users in order toauthorize their requests to perform actions and in order to hold themaccountable for the actions that they perform. (see figure 2.1 for a flowchart describing how the policies of the system guide whether authentication, authorization, and/or accountability are needed.) in some instances authentication is unrelated to security; identificationand authentication are sometimes used to create or expand a relationshipbetween parties. for example, cookies4 are sometimes used to identify2often, databases can be used to map from a handset identifier to the name of the individual or organization that pays the bill for the handset.3in some cases, one such requirement is to protect classes of people (usually children)from material deemed inappropriate. in 2002, cstb released a report that looked at thisproblem: youth, pornography, and the internet, washington, d.c., national academy press,2002.4cookies are mechanisms used by web browsers and web servers to track visits and/orprovide continuity of experience.box 2.1absentee votingin many places, absentee voting couples a number of mechanisms in order toachieve authentication, authorization to vote exactly once, and the confidentialityof the ballot itself. the voterõs identity is checked by way of a signature, both atapplication time and on the outer envelope of the ballot itself. a suitable entry insome sort of recordkeeping system is used to record that this person has alreadybeen issued a ballot. but the ballot itself is sealed inside an inner, anonymousenvelope; this envelope is not opened until after it has been separated from theouter, authenticated (and nonanonymous) envelope. the two events are separated temporally and spatially. despite the authentication of the voter (by means ofthe signature), a measure of privacy protection is achieved through a combinationof the two envelopes and rigid procedures.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication in the abstract35yes¥ identify and perform individual authentication.¥ retrieve any attributes necessary for authorization.¥ perform authorization.¥ keep a record of individual and action.nopolicy requiresaccountability?policy requiresauthorizationbased onnonidentifyingattribute?yesno¥ identify and perform individual authentication.¥ retrieve any attributes necessary for authorization.¥ do not keep a record of individual and action.policy requiresauthorizationbased onindividual identityor identifying attribute?yesno¥ do not identify.¥ perform attribute authentication.¥ do not keep a record of attribute and action. ¥do not identify.¥do not authenticate.¥do not keep a record of any kind.policy decisions aremade aboutauthorization andaccountability.figure 2.1authentication, authorization, and accountability. the necessity ofauthenticating the presenterõs identity is based on the needs and policies of thesystem being accessed. if only authorization is sought by the system, the identityof the presenter may not be relevant.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.36who goes there?individuals and track their browsing and purchasing behaviors. such monitoring is undertaken to bolster personalization and marketing efforts.the rest of this chapter describes elements of authentication systemsand protocols at a high level in order to provide a foundation from whichto examine specific authentication technologies. a description of the parties traditionally involved in authentication appears below, followed by amore detailed discussion of authorization and accountability.three parties to authenticationauthentication systems typically involve parties who play three rolesin the authentication process. a òpresenteró presents credentials (an indepth discussion of the nature of credentials in general is provided inchapter 6) issued by a thirdparty òissueró to a òverifieró who wishes todetermine the veracity of those credentials. in some cases, one partymay play two roles. for example, the verifier and issuer roles are oftencombined.the issuer usually5 uses a separate system to perform an initial authentication of prospective credential holders prior to issuing credentials,to ensure that they are issued only to legitimate parties. consider the caseof a department of motor vehicles (dmv), which issues an importantcredential, the driverõs license.6 the dmv often relies on another credential, such as a birth certificate or a passport, to identify applicants forlicenses. this reliance comes with its own risks, in that birth certificates,for example, are not strongly linked to the individuals whom they identify and are so diverse in their format that it is difficult for dmv employees to authenticate them. (the security and integrity problems associatedwith documents such as birth certificates are discussed in chapter 6.)when analyzing different authentication systems, it is important tolook at how a system implements all three roles. one needs to look at thesecurity and privacy issues and the risks for each role and the processesthat the system performs and consider who has a vested interest in thesuccess (or failure!) of the authentication event.5not all credentials require presenting supplementary forms of identification prior toissuance. for example, one may acquire a free email account from one of several services,which then acts as a legitimate email address, anonymously.6the original motivation for driverõs licenses was to document authority to operate amotor vehicle. however, today they are used more often as a primary identity credential inmany unrelated transactions, such as boarding an airliner or cashing a check. (this is anexample of the secondary use of a credential overwhelming the primary use; see chapter 4for more discussion of secondary uses.)who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication in the abstract37authorization can be attained by ascertaining attributes of the user,the system, or external conditions. however, ensuring accountabilityoften requires that a particular individual be linked to a transaction.7authenticating to authorizeauthorization is the process of ensuring that the rules governing whomay do what to which resources are obeyed. authorization works byasking an appropriate authority (referred to herein as a òpolicy decisionpointó) for an authorization decision every time an individual8 submits arequest to the system to access resources. if the policy decision pointdecides to grant the request, the individual is allowed to access the resource; if the policy decision point decides to deny the request, the individual is not allowed access.the international organization for standardization (iso) standard101813 defines a standard model and standard terminology for authorization in an information technology and communications context.9 in theiso model, authorization decisions are based on authorization policy,resource attributes (such as sensitivity of the data), context attributes (suchas time of day), request attributes, and subject attributes. subject attributes might include the requesterõs name and privilege attributes, suchas job role, group memberships, or security clearance.in order to make a determination, the policy decision point needs toknow something about the subject it is dealing with. since subjects mighttry to impersonate one another, the verifier will want to make sure thatthe subject attributes that it uses to make its decision have been authenticated. policy decision points do not necessarily need to know anythingabout a subjectõs name or any other particular claimed identity in order toauthenticate the attributes that are relevant to the policies they enforce.two examples from disney world illustrate this point:7anonymous accountability mechanisms exist. consider, for example, a cash securitydeposit for using a set of billiard balls. if the user steals or damages the balls, he or sheforfeits the deposit (which is then used to finance the purchase of a new set). the user whoreturns the balls in good condition gets the deposit back. the user is held financiallyaccountable for loss or damage to property (the billiard balls) with no record of identitybeing required. of course, this is anonymous only to the extent that anyone participating inor witnessing the transaction may know the userõs identity.8 in this case, the individual is the initiator of an operation that involves an authorizationdecision. the individual is the entity whose privileges are examined to determine if he orshe should be allowed to complete the requested operation.9this definition is adopted because it is the international standard for terminology in thisarea.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.38who goes there?¥disney world restricts access to some rides on the basis of physicalheight of the rider. the policy that enforces this restriction authenticatesthe riderõs height (an attribute) by requiring riders to walk past a sign thathas a picture of a hand positioned at the required height. if the riderõshead is higher than the hand, the riderõs height is authenticated, and therider is allowed onto the ride. this system does not identify individualriders at all, but it still enforces the desired policy. it authenticates therelevant attribute (in this case, height greater than the required minimumvalue) of each rider. it does not collect extraneous information about theindividual.¥disney world also uses a system that is designed to prevent asingleentry pass from being used by multiple users. disney world issueseach passholder a card at the time the pass is purchased. the name of thepassholder is not recorded on the card, and, in fact, the card can be transferred freely from user to user until the first time it is used. at the time offirst use, information about the passholderõs finger geometry (not relatedto the passholderõs fingerprint) is linked to the card. any time after thefirst use of the pass, the person presenting the pass must authenticateownership of the pass using a finger geometry verification check (byholding his or her hand up to a measuring device). if the check fails, theperson presenting the pass is denied access to the park.finger geometry is not distinctive enough to identify the passholderuniquely; therefore, verifying finger geometry does not provide sufficientcertainty for accountability (see below). however, finger geometry variessufficiently from person to person so that a randomly selected individualwho is not the passholder is not likely to match the finger geometrylinked to the card. therefore, this system works well enough to preventmultiple users from using the same pass in most casesñan acceptablelevel of risk, given what the system is protecting. this system uses a looseform of biometric authentication to protect against fraud (here defined asmultiple users) without collecting information that identifies the legitimate owner of the pass. (see chapter 5 for further details on variousbiometrics technologies.)authenticating to hold accountableaccountability is the ability to associate a consequence with a pastaction of an individual. to hold individuals accountable it must be possible retrospectively to tie them to the actions or events for which accountability is desired, or to be able independently to detect and respondto inappropriate behavior. especially for purposes of afterthefact accountability, information from the authentication event must unambiguously identify one and only one individual, who will be held responsiblewho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication in the abstract39for the event.10 an authentication event that identifies a group of peoplebut not a single individual within the group cannot easily support individual accountability.11accountability usually eventually requires personal identification ofindividuals (unlike authorization, which can often be done without identifying individuals). accountability processes are used to generate theevidence needed to punish the guilty when something is done in violation of policy or law. even so, the identifiers used need not always be inthe form of the name of the individual or even something that can be usedto find the individual of interest; they only need to be able to confirm thata suspect is the individual actually responsible. what is needed for thispurpose is something that can be linked to the individual with some levelof assurance. a fingerprint or dna sample, for example, can be used toestablish accountability. neither of these two types of evidence namesthe individualñnor does either provide a mechanism for finding him orherñbut both provide means to verify (after the individual is locatedthrough other means) that the suspect probably is or is not the accountable individual.accountability in information systems usually works by allowinga process for observing and recording usersõ actions in a log. the logcan later be searched (as part of an investigation into wrongdoing, forexample).12while authentication (whether of an individualõs name or of an attribute) is often necessary for accountability, authentication by itself is notalways sufficient to support accountability. authentication is a key toattributing actions and allocating punishments to the correct individuals.10in some cases, it may be possible to impose a certain kind of òaccountabilityó withoutrequiring the authentication of an identity. for example, some technologies are designed toprevent unauthorized duplication of digital media (such as cds). in instances in which thistechnology works, there is no accountability that requires identity authentication; insteadthe cd (or the machine) might just stop functioning.11it is important to note that accountability is not the only mechanism for righting wrongs.legal and other systems sometimes address wrongs without directly holding accountable theindividual responsible for the wrongñfor example, by providing for redress. accountabilityand redress are separate concepts. one can have redress for a harm without holding theperpetrator of the harm accountable. for example, in disputes involving alleged violations ofcopyright, the digital millennium copyright act of 1998 provides for redress by providingcopyright holders with a method of having material removed from web sites. this form ofredress prevents future harm but does not punish previous bad acts.12note that this is conceptually distinct from the notion of auditing for the purposes ofsystem maintenance and/or troubleshooting. auditing may or may not require recordingindividualized user activity. while the mechanisms can be similar, the purposes of thesetwo kinds of logbased monitoring are distinct.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.40who goes there?if identities in the log are not well authenticated, a user who falls undersuspicion of wrongdoing will dispute the validity of the log by claimingthat the actions attributed to him or her were really performed by someone elseñsomeone else who fooled the authentication system and impersonated the suspect.13accountability is not always a requirement in information systems(or elsewhere), and even where it is, collecting and storing informationabout individuals is not always necessary for accountability. in order toestablish accountability, systems often collect and store information aboutwho did what, where, and how. for example, financial markets supportpayment mechanisms with and without accountability. in a cashforgoods transaction, the buyer and seller often divulge no information to oneanother. in contrast, credit card transactions are informationrich: a recordis created that captures both the identity of each party and the details of thetransaction. credit records support accountability by uniquely mapping toan individual (except perhaps in the case of family members who share asingle account and thus share each otherõs accountability or in situationsinvolving powers of attorney) or to an organization.the inability to reconstruct a transaction from records retrospectively(that is, one may not be able to identify the other party or to prove that thetransaction occurred) is the norm in cash transactions. this is at least inpart due to the fact that the exchange is complete when the parties separate (goods have been exchanged for cash), so there is little need to makeprovisions for future accountability for payment. in a credit transaction,by contrast, one party has goods, another has a promise of payment, and athird has a record of the transaction directing the purchaser to makepayment. the incomplete nature of the transfer and the ability of a buyeror seller to deny or fabricate an exchange have resulted in the creation ofidentitybased audit logs that support accountability. these are neededin order to complete the payment and therefore the transaction as a whole.these examples illustrate that anonymous, instantaneous liabilitytransfer mechanisms (such as cash) can reduce or even eliminate the needfor accountability in some transactionbased systems. an example of ananonymous liabilitytransfer mechanism that is not instantaneous is asecurity deposit. a security deposit is an upfront payment equal to orexceeding the value of an item that is rented or borrowed (for example, a13where authentication of individual identities is required, the individual identities inthe log will not necessarily be easily accessible to just anyone who looks into the log. it ispossible to design an audit system that puts a nonidentifying unique identifier (essentiallya pseudonym, sometimes called an audit id) into the log and allows specially authorizedusers to link these audit ids to personal identities only after dueprocess rules have beenobserved.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication in the abstract41cash deposit that is made for the billiard balls used at a bar or for checkingin at a hotel without a credit card).cash is an extremely efficient mode of commerce in part because itdoes not require us to know or seek to know anything about the party onthe other side of the transaction.14 other forms of payment are less efficient, because creditworthiness, identity, or other attributes of one or moreparties to the transaction need to be established before the transaction iscompleted, records of the transaction need to be kept, and settlementprocedures need to be executed. this suggests that where accountability(and therefore authentication) can be dispensed with, transactions can bemade simultaneously more efficient and more protective of privacy.15what do we authenticate?earlier, authentication was defined as the process of establishing confidence in the truth of some claim, often a claimed identity. the nextobvious question is, what kinds of claims are verified? individuals mightmake a variety of claims that would help to support the goals of authorization and accountability.when the goal that motivates authentication is to hold an individualaccountable, it is useful to verify some piece of information that is stronglylinked to or that uniquely identifies the individualñfor example, an identifier (such as a personal name) or a very distinctive attribute (such as adna profile). it may also be useful in these cases to verify some piece ofinformation that will help contact or locate the individualñfor example, aresidence address or email address.when the goal that motivates authentication is to authorize individualsõ actions, it is useful to verify some piece of information that will beuseful to the policy decision point in making its authorization decision.this information may be a property of the individual (such as the fact thatan individual has paid for entrance to an event), or it may be a statementabout the individual by some authoritative party (such as when a credittransaction is authorized by the issuing bank).14if the cash itself is authentic, then we trust in the issuing government to make good thepromise of payment, perhaps in precious metal, and we trust that there will always be anexchange market for precious metal. most currencies now skip the preciousmetal connection; governments back the currency instead with the direct promise of market stability.15of course, providing records of transactions can be important for reasons other thanaccountability. for example, information systems often maintain transaction logs for thepurpose of diagnosing and correcting system failures. in addition, the òanonymityó of cashcreates challenges for law enforcement.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.42who goes there?presentation of a passport to an immigration agent authenticates thepassport holder for the purpose of authorizing entry into a country. theindividual presents the passport, which claims his or her name and country of residence. the immigration agent verifies the claim by examiningthe picture in the passport to verify that the individual is the legitimateholder of the passport and possibly by doing some kind of authenticitycheck on the passport document itself.in summary, authentication is the process of verifying a claim. aclaim asserts that an individual has some attribute. an attribute may bean identifier, a property, or a statement about the individual by a thirdparty. below is a discussion of these various kinds of attributes.identifiersrecall from chapter 1 that an identifier points to an individual. anidentifier could be a name, a serial number, or some other pointer to theentity being identified. an identifier can be strong in the sense that itallows unique mapping to a specific individual in a population, or it canbe weak, in that it could be correctly applied to more than one individualin a population. whether an identifier is strong or weak will dependupon the size of the population and the distinctiveness of the identifyingattribute. however, multiple weak identifiers can, when combined,uniquely identify a specific individual and therefore serve as the functional equivalent of a single strong (unique) identifier. multiple weakidentifiers may lead to unique identification.16 some identifiers, such aspseudonyms, require a list of correspondences between pseudonyms andindividuals (often called a lookup table) for unique mapping back to anindividual, thus allowing only the holder of the list to identify the actionwith the individual. some identifiers, such as common names, allow anyobserver to map an action back to an individual (though not always with100 percent confidence). authorization, in contrast, may require no identifier at all. systems that use multiple weak identifiers can be made just assecure for the verifier as systems that use, for example, personal names,but the former may have the privacy advantage of not as easily identify16interestingly, while the use of multiple weak identifiers may enable a certain level ofsecurity through authentication, these identifiers can also be used to create unforeseenlinkages and therefore pose a risk to privacy (even while, individually, very weak identifiers might pose a minimal risk to privacy). work done by latanya sweeney suggests thatvery little information is needed to uniquely identify a particular individual in even anostensibly anonymized database, suggesting that creating linkages between databasesñeven without biometric data tying individuals to their datañmay not be difficult (see<http://lab.privacy.cs.cmu.edu/people/sweeney/confidentiality.html>).who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication in the abstract43ing individuals to third parties who do not have access to the informationthat links the weak identifiers.a unique identifier refers unambiguously to one and only one individual in the population. legal personal names are commonly used identifiers for human individuals, despite the fact that they are not usuallyunique except in very small populations. in this case, additional identifying information, perhaps limiting the population, is often implicitly required for identification (e.g.,òbob smith of centervilleó). however, individuals can be identified by things other than names. many corporatesystems, for example, use employee id numbers rather than personalnames as identifiers. in large organizations, this is commonly done because id numbers (unlike personal names) are unique.names that are not personal names (such as pseudonyms and emailaddresses) can also be used as identifiers. an entity issuing or relying onsuch an identifier may wish to correlate it to a unique individual; individuals using these identifiers may wish to prevent such correlation. identifiers that are not personal names can be designed to protect individualsõprivacy by limiting or preventing correlation of the identifier to a specificindividual by limiting access to the lookup table (in a way that personalnames cannot).in general, the strength of the identification system is related to howdistinctive the combined identifiers are across the population in question.òbob smithó might be a strong identifier in centerville, but quite weakacross the population of the entire country. identifying a person uniquelyacross the entire population of the united states might require a name andconsiderable additional data, such as a phone or social security number.attributesauthorization policies are often based on attributes that do not inherently identify an individual. an attribute can be inherent (height, forexample) or assigned (vice president, for example); it can be permanent ordynamic. as an example, access to confidential company information ina corporate intranet may be granted to any employee of the corporation.the relevant attribute would be status as an employee.some attributes can be granted to individuals with virtually no claimon the part of the individual being authorized. the most common use ofthis type of attribute is in granting guest access to a system. since bydefinition anyone can be a guest, it is unnecessary to authenticate anindividualõs identity in order to determine that he or she is entitled to theguest attribute. obviously, this attribute is not very discriminating, soone might ask why it is used at all. the answer is that authorizationsystems are normally set up to authorize every access to every resourceñwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.44who goes there?so even publicly available resources need to be governed by a policy thatsays, ògrant access to the public,ó where the public is defined as anyonewho has the guest attribute.some attributes can be observed directly by the information system.for example, an individualõs location can be determined if the system cantell that the individual is using a terminal that is known to be on thebusinessõs premises. gender and race are in many instances observableattributes. authentication of such attributes does not require prior authentication of an identifier for the individual.it is sometimes possible to manifest attributes that ordinarily wouldnot be directly observable. bars do this when they stamp the hands ofpatrons who are older than the minimum legal drinking age; this allowspeople of legal age to enter and leave the bar without having toreauthenticate their age by showing id cards each time they enter.some attributes cannot be observed directly by information systems.for example, an individualõs employer or department number cannot bedetermined by examining the individual or his or her surroundings. systems normally deal with such attributes by creating a process for registering individuals, assigning attributes to registered individuals, storing eachindividualõs assigned properties, and retrieving an individualõs attributeswhen needed for an authorization decision.in order to retrieve a specific individualõs attributes from storage (orfrom a third party), the system must have an identifier for the individual,which it can use to refer to the individual whose attributes it wants tolook up. furthermore, this identifier must be authenticated, so that thesystem can have confidence that it is talking not to an impostor but in factto the individual whose attributes it is going to look up.systems usually require individuals to authenticate themselves (using a unique identifier) and subsequently to be assigned other attributesthat are then stored in an attribute database. then, at some later time, theindividual reauthenticates the same unique identifier to the system, andsome observer function in the system uses the unique identifier to look upthe assigned attributes in the database and to make an access decision.statementsa statement records a belief or claim about an individual by an identifiable party.17 authorization decisions may in some cases be based onattestations or assertions of authorities. for example, a bankõs decision to17that is, it is possible to determine who the party is and hence whether the party isauthoritative for the statement.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication in the abstract45extend credit to an individual may be based on a credit rating asserted bya credit agency. the individualõs credit rating is an attribute that is assigned by the credit agency to the individual. in this case, the partymaking the statement is an authority, and the party that uses the statement to make an authorization decision is a relying party.thirdparty assertions generally provide attribute information thatcannot be directly observed by the relying party. in the example above,an information system cannot observe an individualõs credit ratingñitmust instead query the credit agency to provide the rating. in order toretrieve accurately an individualõs attributes from the authority, the relying party must have an appropriate identifier for the individual, which itcan correlate to the individualõs identity and corresponding statements inits database.how do we authenticate?john locke, in his òessay concerning human understanding,ó18 distinguished two types of identityñphysical identity and psychologicalidentity:[t]he identity of the same man consists . . . in nothing but a participation of the same continued life, by constantly fleeting particles of matter,in succession vitally united to the same organized body.any substance vitally united to the present thinking being is a part ofthat very same self which now is; anything united to it by a consciousness of former actions, makes also a part of the same self, which is thesame both then and now. person, as i take it, is the name for this self.wherever a man finds what he calls himself, there, i think, another maysay is the same person.it is by the consciousness it has of its present thoughts and actions that it[i.e., a person] is self to itself now, and so will be the same self, as far asthe same consciousness can extend to actions past or to come; and wouldbe by distance of time, or change of substance, no more two persons,than a man be two men by wearing other clothes today than he didyesterday.locke also identifies the association of a set of past actions with apresent actor as being critical to the notion of personal identity, and heassociates identity with law and accountability:[a]s to this point of being the same self, it matters not whether thispresent self be made up of the same or other substancesñi being as18john locke. an essay on human understanding, part ii, chapter 27. 1690. availableonline at <http://www.ilt.columbia.edu/publications/ lockeunderstanding.html>.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.46who goes there?much concerned, and as justly accountable for any action that was donea thousand years since, appropriated to me now by this selfconsciousness, as i am for what i did the last moment.in this personal identity is founded all the right and justice of rewardand punishment.the distinction between physical and psychological identity is criticalto the understanding of authentication systems, because authenticationsystems use features of both types of identity, and they differ in theirproperties and application depending on which type of identity they authenticate. password and keybased authentication systems, for example,can be used to make individuals express intent (because such systemsauthenticate psychological identity), but they are prone to the theft ofauthentication secrets. some biometrics, on the other hand, can be usedto identify individuals without those individualsõ active participation andawareness, so care needs to be taken when using biometrics in authentication systems designed to ensure accountability.some authentication systems also authenticate claims not on the basisof physical or psychological identity but instead on the basis of the possession of an artifact.thus there are three generic approaches to authentication. they are oftendescribed as òsomething you know,ó òsomething you have,ó and òsomethingyou are.ó19 the properties of each approach are discussed below.authentication systems often combine two or all three of these approaches. this is what is done at a typical automated teller machine(atm). both òsomething you haveó (the bankcard) and òsomething youknowó the personal identification number or pin are required to accessaccount information or to make changes to the account. a combination ofapproaches (sometimes referred to as multifactor authentication) generally provides more security than do single approaches alone, because thestrengths of one approach can be used to compensate for the weaknessesof another. at the same time, depending on implementation and othersystems choices, multifactor authentication may raise more privacy considerations than singlefactor authentication.19the national institute of standards and technology articulated these principles asrelated to computer security in the 1970s (d.e. raphael and j.r. young, automated personalidentification, sri international, 1974; national bureau of standards, evaluation techniquesfor human identification, fipspub48, april 1977).who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication in the abstract47authenticating physical identityauthentication systems based on physical characteristics (somethingyou are) authenticate individuals by observing physical characteristics ofthe body of the individual; these systems are often called biometric authentication systems. the physical characteristics used differ from system tosystem; some use fingerprints, others use the geometry of the human hand,others use the pattern of tissues in the iris of the eye, and so on.a person wishing to be authenticated by means of a biometric mechanism need not remember anything, nor does he or she need to rememberto carry an object. unlike passwords, properly chosen biometrics cannotbe readily shared in normal use.20authenticating psychological identityauthentication systems based on covert knowledge (something youknow) authenticate users by requiring the individual to recite a secret(sometimes personal information). these systems rely on what lockewould consider the identity of a òpersonóñthat is, they depend on thepsychological continuity of a personõs memory.21 the benefit of this20while a number of effective techniques for attacking biometric systems have been published, the majority of users, who are neither malicious nor technologically sophisticated,will not use these techniques to circumvent protections against sharing for casual reasonsñe.g., to share biometric identifiers with family members, colleagues, and so on(t. matsumoto, h. matsumoto, k. yamada, and s. hoshino, òimpact of artificial gummyfingers on fingerprint systems,ó proceedings of spie 4677 (january 2002), available online at<http://research.nii.ac.jp/kakenjohogaku/reports/h13overview/a04001.pdf>; l.thalheim, j. krissler, and p. ziegler, òbiometric access protection devices and their programs put to the test,ó cõt magazine 11 (may 21, 2002):114, available online at <http://www.heise.de/ct/english/02/11/114>; t. van der putte and j. keuning, òbiometrical fingerprint recognition: donõt get your fingers burned,ó proceeding of the ifip tc8/wg8.8fourth working conference on smart card research and advanced applications, kluwer academic press, 2000, pp. 289303, available online at <http://www.keuning.com/biometry/biometricalfingerprintrecognition.pdf>; and d. blackburn, m. bone, p. grother, andj. phillips, facial recognition vendor test 2000: evaluation report, u.s. department of defense, january 2001, available online at <www.frvt.org>).21another way of thinking about identity continuity is to consider the case where twodifferent names (or instances of the same name) correspond to the same principal (this isknown in the distributed systems literature as an òindirect nameó or òsymbolic linkó). theclassic example comes from the registration of title to real estate. it is very common thatsomeone who wishes to sell a house uses a name different from his or her name at the timethe house was purchased: the person might have changed their name in marrying, or after acriminal conviction. a classic identity problem is knowing that the òmrs. janet rogersówishing to sell property at 1423 constitution avenue is the same person as the òmiss janetfoster smithó who purchased it 11 years ago.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.48who goes there?approach is that it does not require a person to remember to carry aparticular object and is in general one of the simplest forms of authentication. however, what is known can be forgotten, shared, guessed, or justplain stolen by being overheard or from the noting of written reminders.there are at least two types of covert knowledge: synthetic secrets andprivate secrets.synthetic secrets are items of information created specifically for thepurpose of authentication; they typically have no relation to characteristics of the individual or to events in the (human) individualõs life. passwords are a type of synthetic secret (when used properly) and the classicexample of the òsomething you knowó approach to authentication.the principal problem with using a synthetic secret for authenticationis that because it is unrelated to the individualõs life in any meaningfulway, it is often difficult to remember. a joke in the security communityillustrates the point: òthere are two rules for choosing a good password:(1) pick something you canõt remember and (2) donõt write it down.ó thisproblem arises because synthetic secrets that are easy to remember arealso usually easy for others to discover or guess.22private secrets are items of information that are so intimately associated with an individual or with events in the (human) individualõs lifethat no other person (or few others) would be expected to know aboutthem.the use of private secrets for authentication causes several problems.people resist the use of private secrets for authentication on the groundsthat they are private and should not have to be revealed to third parties(even to third parties who wish to authenticate us). private secrets arerarely completely private.23 this leads to another problem: any item ofinformation that is used as a private secret to authenticate an individualwill typically be shared with all the people and organizations that want toauthenticate the individual (technical measures exist that could preventsharing this, but they are not widely used). each party who authenticatesthe individual therefore comes to know the information that is supposedto be a private secret, and thus the information becomes less private andless secret as time goes by.22one approach that makes passwords more difficult to share or guess is to requirepeople to memorize images instead of sequences of words, letters, numbers, and/or characters. see òin a userfriendly world, one pictureõs worth 1,000 passwords: imagedrivenlogons are easier to use and more secure, hightech researchers claim,ó by michael j.kennedy in the los angeles times, june 4, 2002, for a description of this technology andsome of the companies exploring its use.23for example, a dmv and its clerks can find out driverõs license numbers. anindividualõs mother knows her own maiden name, and so do other members of the family.many people know or can find out social security numbers.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication in the abstract49not only do these problems compromise the individualõs privacy, butthey also gradually destroy the usefulness of the information as an authenticator. when the informationõs value as an authenticator has beensubstantially degraded, another piece of (private) information will haveto be chosen as a new basis for the privatesecret authentication process.if this sequence of events is repeated enough times, many of theindividualõs private secrets will have been revealed to large numbers ofother parties. the individualõs privacy will have been put at considerablerisk, as will the ability of other parties to authenticate the individual.social security numbers are an excellent case study in this phenomenon,as a recent incident involving the princeton university admissions officeillustrates. both princeton university and yale university use social security numbers as student identifiers. yaleõs admissions office used thesocial security number as an authentication key to allow students to access the status of their applications for admission via a web browser. amember of the princeton university admissions office staff discoveredthis and apparently used social security numbers obtained from therecords of applicants to princeton in order to access the yale admissionsweb site and learn about yaleõs admissions decisions.24authenticating possession of an artifactanother traditional approach to authentication is the possession of aunique object. a typical house key is an example of the òsomething youhaveó approach. (box 2.2 describes authentication by means of a car keyfob.) the object should be hard to duplicate, at least by simple observation by a third party. in other words, it is possible to duplicate a housekey, but merely observing a key being used does not allow the observer toduplicate the key.the object that is possessed may have a range of functionality. it maybe as simple as a traditional house key, whose shape defines it. it may bea credit card with raised lettering and a magnetic stripe for storing information. it may also be a smart card, with an embedded processor thatmay be used to store information or to act as a cryptographic processor.these different types of objects have different levels of resistance totampering and duplication. a house key is readily duplicated, if desired,by the person in possession of it. a credit card also may be duplicated,provided the person in possession of the card has the appropriate equipment. smart cards, particularly those that perform cryptographic opera24see òivy imbroglio: princeton says it spied on yale,ó wall street journal, july 26, 2002,p. b1.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.50who goes there?tions, are in theory harder to duplicate, because they do not ever discloseduring normal operation the secret information that would be required toduplicate them.the òsomething you haveó approach has the advantage that theholder does not need to remember a password. the object that is used canbe designed to be hard to copy, so it cannot be readily used by more thanone person at a time, although it could be loaned (in which case theoriginal person loses access to it while it is on loan). however, objects caneasily be lost or stolen, and sometimes people are not in a position to carryan item when they want to use a system.identificationthe processes of authentication and identification are related but distinct. while the former may require the verifying party to authenticate anidentifier that refers to the individual, the identification of an individual isdistinct from the authentication of the individualõs claimed identity. someauthentication technologies (particularly biometric technologies) are usedin both processes. identification associates an identifier with an individual without the requirement of a claim on the part of the subject.more specifically, identifying an individual refers to the process ofexamining and/or interrogating an individual (usually an individual whohas been encountered before) with the objective of determining whichbox 2.2car key fobsremote car door key fobs and garage door openers work by way of radiosignals. very early units sent a constant signal, which is clearly insecure. laterversions used an 8bit key to distinguish among different transmitters. again, thiswas inadequate; thieves learned to record and retransmit the signals. many modern units use a socalled rolling code, which is generated from a pseudorandomnumber generator. eavesdropping on a few code transmissions should not provide enough information to predict the next code. to avoid problems from the lossof synchronization, a range of codes is accepted by the receiver. a mechanism isalso provided to resynchronize the transmitter and the receiver. no identificationsignal per se is transmitted by such devices. many new cars use a radio transponder embedded in the keys themselves to unlock the ignition. some of these userolling codes; others use challenge/response technologies. key fobs contain amodest amount of storage and computational ability. specialized equipment isrequired to copy them, but they are easily stolen.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication in the abstract51identifier refers to that individual. in contrast, authenticating an identifier refers to the process of verifying the linkage between an identifier(usually claimed by the individual, but sometimes observed) and theindividual.the relationship betweenauthentication and identificationgiven that authentication and identification are distinct but relatedconcepts, it is important to understand the interplay between them. whileauthentication for accountability almost always eventually requires identifying an individual at some point (as discussed previously), authentication for authorization does not always require this. in the first place,authorization does not always require authentication. when authorization does require authentication, as discussed previously, it does notalways require individual authentication. even when authorizationrequires individual authentication, it often does not require the authenticated identifier to be a personal name. the common use of credit cards isan example. the credit card number is the unique identifier. if the cardhas not been reported lost or stolen, it is assumed that the holder of thecard is authorized to use it. for credit card purchases made by phone orover the internet during which the physical holding of the card cannot beobserved, a secondary piece of information from the card, such as anexpiration date or additional code number, is requested.25it is essential to develop authentication systems whose strength (andoften therefore whose intrusiveness into privacy) is in line with the security needs of and threats to the resources being protected. in some cases itmay be appropriate to require users to forgo some privacy when they areauthenticated for purposes of accessing very sensitive or very valuableresources. note that the information being protected may itself be privacysensitive, and thus may merit strong authentication on that basisalone.recommendation 2.1. the strength of the authentication system employed in any system should be commensurate with thevalue of the resources (information or material) being protected.authorization systems usually do identify individualsõ personalnames, even when it is not necessary to do so to meet the required secu25of course, the second piece of information from the card is valid the first time it is everused and becomes less valuable with each additional use.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.52who goes there?rity goals. the main reason for this is convenience: even for authorization policies that do not use them directly, personal names are familiar,sufficiently differentiable to discriminate among user populations of modest size, and convenient to use as a way to look up other individualattributes. they are therefore often used as the òlabeló on the òfile folderóthat contains the individualõs other attributes. of course, some authorization policies actually do require the use of personal names, and someauthorization systems collect personal names upfrontñthat is, preventivelyñinstead of waiting until it is clear that personal names are necessary before collecting them.finding 2.1. authorization does not always require individualauthentication or identification, but most existing authorization systems perform one of these functions anyway. similarly,a requirement for authentication does not always imply thataccountability is needed, but many authentication systems generate and store information as though it were.recommendation 2.2. systems that demand authentication forpurposes other than accountability, and that do not themselvesrequire accountability, should not collect accountability information.recommendation 2.3. individual authentication should not beperformed if authorization based on nonidentifying attributeswill suffice. where appropriate, authorization technologies andsystems that use only nonidentifying attributes should be usedin lieu of individual authentication technologies. when individual authentication is required, the system should be subjectto the guidelines in recommendation 3.2 (see chapter 3).the cstb report idsñnot that easy raised a number of questionsthat should be addressed before the implementation of any largescaleidentity system. these same questions apply generally to authenticationsystems, given that authentication and identity are often closely connected. while smallerscale authentication systems may imply decreasedurgency (that is, a system to restrict access to a hotel swimming pool, inwhich the attribute necessary for authorization is òcurrent hotel guest,ómay require less rigorous attention to these questions than a system thatwould track the enrollment status of all foreign students in the unitedstates on the basis of their visas or other ids), the principles outlined inidsñnot that easy still hold, especially with regard to understanding thegoals of the system and minimizing unnecessary data collection and rewho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication in the abstract53tention. the questions are reprinted here from idsñnot that easy26 forreference.¥what is the purpose of the system? possible purposes of an identitysystem include expediting and/or tracking travel; prospectively monitoring individualsõ activities in order to detect suspicious acts; retrospectively identifying perpetrators of crimes.¥what is the scope of the population to whom an òidó would be issuedand, presumably, recorded in the system? how would the identities ofthese individuals be authenticated?¥what is the scope of the data that would be gathered about individuals participating in the system and correlated with their system identity?òidentification systems,ó despite the name, often do much more than justidentify individuals; many identity systems use ids as keys to a muchlarger collection of data. are these data identity data only (and what ismeant by identity data)? or are other data collected, stored, and/or analyzed as well? with what confidence would the accuracy and quality ofthis data be established and subsequently determined?¥who would be the user(s) of the system (as opposed to those whowould participate in the system by having an id)? if the public sector orgovernment will be the primary user, what parts of the government willbe users, in what contexts, and with what constraints? in what setting(s)in the public sphere would such a system be used? would state and localgovernments have access to the system? would the private sector beallowed to use the system? what entities in the private sector would beallowed to use the system? who could contribute, view, and/or edit datain the system?¥what types of use would be allowed? who would be able to ask foran id, and under what circumstances? assuming that there are datasetsassociated with an individualõs identity, what types of queries would bepermitted (e.g., òis this person allowed to travel?ó òdoes this person havea criminal record?ó). beyond simple queries, would analysis and datamining of the information collected be permitted? if so, who would beallowed to do such analysis and for what purpose(s)?¥would participation in and/or identification by the system be voluntary or mandatory? in addition, would participants have to be aware ofor consent to having their ids checked (as opposed to, for example, beingsubjected to surreptitious facial recognition)?26computer science and telecommunications board, national research council. idsñnot that easy: questions about nationwide identity systems. washington, d.c., nationalacademy press, 2002, pp. 911.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.54who goes there?¥what legal structures protect the systemõs integrity as well as thedata subjectõs privacy and due process rights, and which structures determine the liability of the government and relying parties for system misuseor failure?the next chapter explores the history and meaning of privacy, concluding with a recommendation for the development of authenticationsystems modeled on these questions.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.553privacy challenges inauthentication systemsin principle, authentication technologies can both advance and undermine privacy interests. in practice, however, a combination of forces,including the followingñ¥the influence of the prevalent security paradigm of fully mediatedaccess,¥the desire of businesses to collect personal information cheaplyand unobtrusively,¥the pressure on governments and businesses to streamline theirinteractions and reduce costs, and¥the resiliency of digital informationñis more likely to lead to authentication systems that¥increase requests for identification,¥increase the collection of personal information,¥decrease the ability of individuals to understand and participate indata collection decisions,¥facilitate record linkage and profiling, and¥decrease the likelihood that individuals will receive notice of orhave the right to object to thirdparty access to personal information.while authentication systems can undermine privacy in these ways,they can also be used in privacyenhancing or privacypreserving ways,who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.56who goes there?primarily by securing personal data and preventing unauthorized access tothe data. the privacyenhancing benefits of authentication systems are derived from the security features of the overall systems in which they aredeployed and are not intrinsic to the authentication components themselves.as with any technology, careful consideration of the privacy risks,benefits, and tradeoffs involved must be considered before authentication systems are designed and deployed. to some extent, tension between authentication and privacy is inherent, because the act of authentication often requires some revelation and confirmation of personalinformation.1privacy impact of the decision to authenticatefirst, let us look in broad terms at what an authentication systemrequires and examine how the collection, retention, reuse, and linkage ofpersonal information might affect privacy interests:¥establishing an initial identifier or attribute for use within the system may require an individual to reveal personal facts or information(such as name, address, fingerprints). a requirement to reveal identifying personal information may inhibit participation in certain activities(such as medical tests).¥the act of authentication itself may cause the creation of records ofindividualsõ actions (such as where they shop, what they read, and whenthey come and go) that are linkable to one of three entities: a specificindividual (individual authentication); a (possibly pseudonymous) identity that may or may not be linked to an individual (identity authentication); or, an attribute that applies to a specific individual (attribute authentication).¥in addition, transactional information revealing details of an event(purchase, building entry) may be created as a result of or subsequent toauthentication and can then be linked back to the identity or individualand be retained in the relevant record.¥the requirements of the authentication or initial identityestablishment process may impose objectionable requirements (for example, theymight conflict with religious beliefs2 or impose on bodily integrity).1in fact, some private sector and public sector policies impose requirements on thosewho collect data related to the protection of those data.2in june 2002, cnn reported òmuslim woman to challenge ban on veil in driverõs licensephoto,ó available online at <http://www.cnn.com/2002/law/06/27/license.veil.ap/index.html>. for religious reasons, a woman wanted to wear a veil for her driverõs licensephoto in spite of objections from the state of florida that allowing it would jeopardizepublic safety.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.privacy challenges in authentication systems57¥personal information or data may be exposed at multiple pointsand to multiple entities during the operation of an authentication system:they may be revealed during the authentication process, created duringthe authentication process, and/or retained as a result of the authentication process, all of which affect privacy. personal information may alsoremain within a device possessed by the individual, reside in a systemrun by a single entity, or enable many entities to observe and/or collectpersonal information.¥authentication may require the use of an identifier that, even if notpersonally identifiable per se, can be used to compile a dossier of facts(records of use of the identifier) that otherwise would be difficult or impossible to correlate. this collection of discrete facts may lead to a revelation of the individualõs identity.¥depending on where the userõs identity and other authenticationrelated data are stored, they may be accessible to a variety of individualswithin one or more institutions, and they may be more or less susceptible toaccess by hostile third parties through technical exploits or legal processes.this general examination of authentication systems and the personalinformation practices that result from such systems harks back to theseveral general privacy risks created or increased by authentication systems, as described in chapter 1 of this report: covert identification, excessive use of authentication technology, excessive aggregation of personalinformation, and chilling effects.given this categorization of privacy risks, an examination of relevantprivacy interests will provide a better understanding of the foundationsand contours of such interests, the values they protect, and the challengesthat authentication technologies pose to privacy interests.access control and information systemsaccess policies are a defining aspect of information systems. in anetworked environment, the mediation of absolutely every user interaction with the system and its resources is a first step in enforcing accesscontrol policies, identifying misuse, and investigating breaches. theinternet, perhaps the canonical example of a large, networked information system and a vast network of networks, while in many respectsòopen,ó is a highly mediated environment. standards and protocols establish the who, what, when, where, and how of information exchanges.33as larry lessig (in code and other laws of cyberspace, new york, basic books, 1999) andjoel reidenberg (in òlex informatica: the formulation of privacy rules through technology,ó texas law review 76(1998):553593) argue, these standards establish the code by whichonline behavior is regulated.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.58who goes there?decisions about whether a given user may communicate with a resource,whether a given computer may communicate with another, whether agiven network may communicate with another, and what extent of information exchange is allowed in each instance dominate the internet. thisis in part because the internet exists at the collective will of individuals,private parties, and government entities to allow information to flowacross their systems. without these agreements to support the exchangeof bits, there would be no internet.4these agreements also conceal the organizational boundaries and institutional rules that users traverse when they access a site. users aregenerally unaware of the intricacies established by their internet serviceprovider (isp) or of the communication requirements for moving aroundon the internet. the reality is that what users experience as a library or apublic space is in fact a mixture of public and private networks. not onlyare users generally ignorant of the jurisdictional boundaries they cross,but they are also usually oblivious of the presence of other users. onecommentator said that being on the internet is òlike being in a movietheater without a view of the other seats. . .[where] masses of silent,shuffling consumers . . . register their presence only by the fact of a turnstilelike ôhitõ upon each web page they visit. . .ó5 these characteristics ofthe online world are in stark contrast with the physical world in threeimportant respects:1.in the physical world there are many clearly defined public spacesand many privately owned spaces in which access control is nonexistentor minimal;2.in physical space, relatively few actions are mediated; and3.in the offline world, if mediation occurs it is typically evidencedby a physical sign.in the offline world, individuals and institutions make decisionsabout whether or not to mediate interactions between individuals andresources. for example, a university may decide not to control who walksacross the campus but to control who enters certain buildings. similarly,libraries and bookstores generally do not exert control over who entersthe premises or what materials they access, but they do exert control over4for a detailed look at the technological underpinnings of the internet, see computerscience and telecommunications board, national research council, the internetõs coming ofage, washington, d.c., national academy press, 2001.5jonathan zittrain. òthe rise and fall of sysopdom.ó harvard journal of law and technology10(1997):495. available online at <http://jolt.law.harvard.edu/low/articles/10hjolt495.html>.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.privacy challenges in authentication systems59the terms on which individuals may remove things from the premises. incontrast, in a networked environmentñthat is, an engineered systemñthe answer to the question should we mediate access? is almost alwaysyes; the inquiry begins with the questions how much do we mediate?with what mechanism?with increasing frequency, authentication systems are being deployedto control access and movement in physical spaces as well as to controlaccess to networked systems themselves. the increase in the scope ofauthentication and identification supported by networked systems is extending the scope of recorded interactions. the systems and the hardware that interacts with them are changing the information that can becollected during interactions and the extent to which it can be reused. asdiscussed below, these changes challenge the privacy of individuals infour significant respects.1.computer technology reduces the costs of record keeping. the reductionin costs has escalated the data collection and retention associated withauthentication events. increased data collection and retention exacerbatethe privacy consequences of authentication events. flashing oneõs driverõslicense in a corner store is qualitatively different from providing a digitalcopy of oneõs driverõs license to an online merchant. in the latter case, thedriverõs license information is provided to the merchant in a format thatencourages capture and allows for retention and reuse. one potentialoutcome of this change is that identity authentication (or the authentication of a relatively unique attribute or set of attributes with the sameeffect) is more likely to result in a personally identifiable stored recordthan was the case in earlier environments. a recent example illustratesthis point. using a scanner that allows him to read and capture data fromthe magnetic stripes on the back of massachusetts driverõs licenses, abarkeep in boston has built a database of personal informationñincluding driverõs license number, height, weight, date of birth, eye and haircolor, address, and, in some instances, social security numberñon hispatrons.6 without the stateissued driverõs license, collecting such dataon individuals would be expensive and cumbersome and would meetwith privacy objections. the introduction of machinereadable cards andthe market availability of readers have increased the chances that personal information would be captured, reused, and potentially sold. theintroduction of technologyñwithout any change in policyñhas led topractices that are more invasive of privacy.6jennifer lee. òfinding pay dirt in scannable driverõs licenses.ó new york times, march21, 2002.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.60who goes there?2.once data are collected, computerized recordkeeping facilitates recordlinkage.7 distributed relational databases allow diverse records with acommon attribute or attributes to be more readily combined. this abilityto link and profile record subjects supports the secondary use of information. to build on the driverõs license example above, stores across thecountry are making similar use of scannable driverõs license data.8 ascustomer records across various sectors of the economy become tied todriverõs license data, it becomes markedly easier to share and merge fordifferent purposes the data collected by different establishments. and it isnot only the private sector that makes use of the scannable licenses tocontrol access. some government buildings are also using these scannable licenses to record information about visitors.3.rules codified for use in computerized systems are generally less flexible(for both good and bad uses) than policies implemented by humans. businessesand other entities often treat longtime customers and firsttime customers differently.9 a longtime customer may not need to provide the samelevel of authentication before engaging in an interaction or transaction.information systems, while they can be programmed to treat differentpeople differently, generally apply authentication rules designed for theworstcase scenario (in this instance, the new customer). in other words,unless otherwise directed, the system will demand the same informationfrom a repeat visitor as from a newcomer and will retain that information.therefore, the baseline data collected in information systems transactionstends to be richer than that collected in manual systems.4.information technology enables covert identification and possibly overtidentity authentication on a large scale. the covert nature of some information systems used for identification and identity authentication (such asthe driverõs license scanners discussed above) denies individuals full information about the transaction and impedes oversight and accountabil7see the 1993 report of the committee on national statistics, private lives and publicpolicies: confidentiality and accessibility of government statistics, washington, d.c., nationalacademy press, 1993, as well as the same committeeõs 2000 workshop report improvingaccess to and confidentiality of research data, washington, d.c., national academy press,2000, for more on issues surrounding data collection, linkage, and confidentiality. availableonline at <http://www7.nationalacademies.org/cnstat/>.8the driverõs privacy protection act of 1994 prohibits states from disclosing this information, except in limited circumstances, without individual consent. while the law doesnot prohibit the creation of such databases by the private sector, it is clear that scannablelicenses undermine congressional policy to limit the use of driverõs license data for nondrivingrelated purposes.9the downside of this practice is discrimination. without accurate data, rules about whois a risky customer are more likely to be influenced by the biases of the business or individual. accurate data can check these tendencies.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.privacy challenges in authentication systems61ity through the political process. while individuals are aware that thelicense is being scanned, they are not necessarily informed that information from it may be retained, reused, exchanged, or used to link withother systems. indeed, individuals are unlikely to know what information can actually be retrieved from scanning the back of the license. evenif people were to learn over time the data collection possibilities inherentin a driverõs license, there will always be circumstances in which nondisclosure of those possibilities can cause problems.there are other systems that, while discussed prior to implementation or debated by the public after the fact, nevertheless provide littlesignal to the individual at the time that identification occurs. for example, many cities have installed cameras to detect drivers running redlights. in locations where such cameras have been proposed or implemented, initial opposition has often generated community discussionabout what information is collected, what decisions can be made on thebasis of it, and what recourse is available to individuals.10 while thispublic debate increases the general awareness of the individuals whoreside in an area (but not necessarily those who pass through), the collecting of information in this way is more covert than the scanning of driverõslicenses described above. an individual gives over a driverõs license.here, an individual drives through an intersectionñhardly an activitythat signals an identification or authentication event. while the camerasare more easily understood by individuals as identification (surveillance)tools than is the driverõs license reader, it is less likely that the presence ofa camera will be noticed.the increasing use of the internet and other networked systems tosupport access to information, deliver services, and communicate raisesquestions about the accesscontrol policies governing these interactionsand their impact on individual privacy. similarly, the use of informationsystems and networking to control access to and movement in physicalspaces and to support attribute and identitybased service and sales decisions offline raises questions about the authentication systems that support these interactions and their privacy implications. ubiquitous computing, sensorequipped buildings, and smart highways are the directionof the future. they raise important questions about what kind of authentication occurs, how the data used and generated during authenticationevents are handled, and how the answers to these questions support or10william matthews. òbattle lines form over redlight cameras.ó federal computer week(september 3, 2001). available online at <http://www.fcw.com/geb/articles/2001/sep/gebcomm20901.asp>.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.62who goes there?undermine individual privacy, access to information, freedom of association, and other democratic values.a highly mediated environment of networked systems requires system owners to choose between attribute authentication and identity authentication. this choice and the decisions about retention, reuse, anddisclosure that flow from it influence the degree of privacy that individuals using the system enjoy. to the extent that individuals are aware of thechosen policies and their implications, the privacy provided by the system will in turn influence individualsõ decisions about how and in whatcircumstances to interact with it.the legal foundations of privacyprivacy is a fundamental tenet of legal systems and political philosophies that value individual freedom, autonomy, and political participation. privacy has many and varied definitions and is evoked in manycontexts to achieve differing results. it has important political, emotional,social, and legal dimensions. it protects against intrusions in physicalplaces, interference with personal decisions, misuse of personal information, and various interests similar to property interests. the underlyingvalues that privacy protects include individuality and autonomy; intimacy; fairness; and limited, tolerant government.early legal definitions of privacy center on the notion of being leftalone. phrases such as òa manõs home is his castleó11 and òthe right to belet aloneó12 capture this notion of privacy, which encompasses the abilityof individuals to retreat to the safety of home, pull the shades, and lockthe doors, freeing themselves from prying neighbors and state surveillance. while a powerful and important element of privacy, this right toseclusion became increasingly incapable of protecting individuals as society became more interdependent and as interactions became more informationrich. social and technological changes in the 1960s and 1970s generated renewed interest on the part of philosophers and lawyers indefining and conceptualizing privacy.13 from their analyses and writings emerged an appreciation for a more complex and multifaceted concept of privacy and its legal foundations.11ò. . . [t]he house of every one is to him as his castle and fortress.ó semayneõs case, 5 c.rep. 91a, 77 eng. rep. 194 (k.b. 1603).12òthey conferred, as against the government, the right to be let aloneñthe most comprehensive of rights, and the right most valued by civilized men.ó justice brandeis dissenting in olmstead v. united states, 277 u.s. 438, 478 (1928).13see, for example, edward j. bloustein, òprivacy as an aspect of human dignity,ó newyork university law review 39 (december 1964): 9621007; charles fried, òprivacy,ó yale lawwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.privacy challenges in authentication systems63privacy law in the united states derives from many sources, including common law, the u.s. constitution and state constitutions, and stateand federal statutes. as the values that it protects suggest, privacy lawcomprises several branches. this report examines the potential privacyimpact of authentication technologies on four areas of privacy, each ofwhich has a constitutional basis in the united states:1.bodily integrity, which protects the individual from intrusivesearches and seizures;2.decisional privacy, which protects the individual from interferencewith decisions about self and family;3.information privacy, which protects the individualõs interest in controlling the flow of information about the self to others; and4.communications privacy, a subset of information privacy that protects the confidentiality of individualsõ communications.as discussed above, authentication technology can intrude on each ofthese privacy interests. authentication methods may require contact withor close proximity to the body, potentially raising concerns under theòbodily integrityó branch of privacy law. authentication may introducenew opportunities to collect and reuse personal information, intruding onòinformation privacy.ó authentication systems may be deployed in amanner that interferes with individualsõ òdecisional privacyó by creatingopportunities for others to monitor and interfere with important expressive or other personal activities. authentication methods may raise newopportunities to intercept or monitor a specific individualõs communications, revealing the personõs thoughts and the identities of the individualswith whom he or she communicates. this section provides some historical context for the privacy interests listed above.constitutional roots of privacythe word òprivacyó is notably absent from the u.s. constitution.however, the values and interests that privacy protects are explicitly expressed in various amendments and have been held by the u.s. supremecourt to be implicit in other amendments. for example, the fourthamendment prohibition against unreasonable searches and seizures andthe fifth amendment prohibition of compelled selfincrimination explicjournal (january 1968): 475493; judith jarvis thompson, òthe right to privacy,ó philosophyand public affairs 4 (summer 1975): 303; james rachels, òwhy privacy is important,ó philosophy and public affairs 4 (summer 1975): 323333; william m. beaney, òthe right to privacyand american law,ó law and contemporary problems 31 (1966): 357.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.64who goes there?itly protect privacy interests in personal papers and effects and in personal thoughts and beliefs, respectively,14 while the first amendmentprohibition against the suppression of speech and assembly has beenfound to implicitly include the right to speak and to assemble anonymously. the supreme court has interpreted the first, third, fourth, fifth,ninth, and fourteenth amendments as providing protection for differentaspects of personal privacy. although it is important to note that constitutional claims arise only in cases in which some state action interfereswith privacy, the values represented by these constitutional claims resonate broadly throughout society.first amendment interest in privacy and anonymitythe first amendment guarantees the freedoms of speech, association, and access to information. numerous supreme court cases document the right of individuals to speak, associate, and receive informationwithout having their identities revealed. the ability to speak anonymously is rooted not only in the constitution but also in the actions forging a consensus for its ratification. the federalist papers were pennedunder several noms de plume. the supreme court has affirmed the rightof anonymity in political speech and the right to solicit door to doorwithout registering or identifying oneself.15 similarly, the court has recognized the chilling effect that the disclosure of membership lists wouldhave on the freedom to associate, and therefore it has shielded such listsfrom government scrutiny.16 the ability to receive information anonymously, the corollary of the right to speak anonymously, while less clearly14òwhen the fourth and fifth amendments were adopted, ôthe form that evil had theretofore takenõ had been necessarily simple. force and violence were then the only meansknown to man by which a government could directly effect selfincrimination. it couldcompel the individual to testifyña compulsion effected, if need be, by torture. it couldsecure possession of his papers and other articles incident to his private lifeña seizureeffected, if need be, by breaking and entry. protection against such invasion of ôthe sanctities of a manõs home and the privacies of lifeõ was provided in the fourth and fifth amendments by specific language.ó justice brandeis dissenting in olmstead v. united states, 277u.s. 473, quoting boyd v. united states, 116 u.s. 616, 630.15see mcintyre v. ohio elections commission, 514 u.s. 334 (striking down a state statuterequiring political leafleteers to identify themselves on their leaflets). recently the supremecourt upheld a similar challenge to a local ordinance requiring all individuals petitioningdoor to door to register and identify themselves (watchtower bible and tract society, inc. v.village of stratton, 001737). also see watchtower bible and tract society of new york, inc., et al.v. village of stratton, et al. (001737) 240 f.3d 553, reversed and remanded; available onlineat <http://supct.law.cornell.edu/supct/html/001737.zs.html>.16naacp v. alabama, 357 u.s. 449 (1958) (striking down a state statute that requiredorganizations to disclose their membership to the state).who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.privacy challenges in authentication systems65articulated by the court, can be found in cases forbidding the governmentfrom requiring individuals to affirmatively register to receive certainkinds of information17 and affirming the right of individuals to possessfor inhome consumption òobsceneó materials that could not legally besold.18 recently the colorado supreme court held that the first amendment to the u.s. constitution and the state constitution òprotect theindividualõs right to purchase books anonymously, free from governmental interference.ó19third amendment privacy protectionthe court has found protection of a right to privacy against unreasonable surveillance and compulsory disclosure in the third amendmentõs protection against quartering soldiers. this protection has generally been viewed as secondary to the broader protection of the fourthamendment.fourth amendment roots of privacy lawthe fourth amendment to the u.s. constitution protects individualsagainst unreasonable searches of their persons and places and againstunreasonable seizures of their property. fourth amendment jurisprudence articulates limits on government searches of individuals, residencesand other private places, and communications. the principle on whichthe fourth amendment is based derives from an even older tradition inbritish common law. as early as the early 17th century, british courtswere placing limits on the power of the crown to enter anyoneõs home.though the power of the monarch was still substantial, semayneõs case20in 1603 says that òthe house of every one is to him as his castle andfortress.ó over time, this basic limitation on entry into the sanctity ofoneõs home has been stated with more precision. the state may not enter17see lamont v. postmaster general, 381 u.s. 301 (1965) (striking down a postal regulationrequiring individuals to register a desire to receive communist propaganda).18see stanley v. georgia, 394 u.s. 557 (1969) (striking down a state statute criminalizinginhome possession of obscene material); denver area educational telecommunications consortium, inc. v. fcc, 518 u.s. 727 (striking down cable statute requiring individuals to request in writing segregated, patently offensive cable programming as overly restrictive inlight of alternatives that protected the anonymity of viewers).19tattered cover, inc. v. city of thorton, colo. sup ct 2002 colo. lexis 269, april 8, 2002;see also julie e. cohen, òa right to read anonymously: a closer look at ôcopyrightmanagementõ in cyberspace,ó 28 conn. l. rev. 981 (1996) (arguing that the right to readanonymously is protected by the first amendment).20see semayneõs case, 5 c. rep. 91a, 77 eng. rep. 194 (k.b. 1603).who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.66who goes there?without a reason and a warrant issued by a court; in addition, the statemust òknock and announceó the search. announcing the search andpresenting the target of the search with a copy of the warrant for inspection is critical to assure that the state does not enter without a warrant andthat the reasons for which the warrant were issued can be challenged, atleast after the fact. these procedural safeguards have been found necessary to guard against abuse of the invasive searching power granted tothe state.searches conducted without simultaneous notice are considered secretsearches and generally prohibited under u.s. constitutional law. for obvious reasons, wiretapping and other types of electronic surveillance are, bydefinition, secret. a telephone wiretap that first announces to the partiesbeing tapped that their voices are being recorded is not likely to yield anyuseful evidence. yet, courts have allowed that wiretapping, though generally violating the rule against secret searches, may be allowed in limitedcircumstances. historically, electronic surveillance was only allowed for alimited class of serious crimes, and only after other investigative means hadfailed.21 in recent years the list of crimes has grown. in addition, thestatutory protections for electronic communications such as email do notdirectly parallel those established for voice communications in the wake ofsupreme court rulings, not to mention that the effects of the usa patriot act of 2001 (public law 10756, uniting and strengthening americaby providing appropriate tools required to intercept and obstruct terrorism act of 2001) on opportunities for surveillance and accountability arestill to be determined. (see the sections below entitled òstatutory privacyprotectionó and òprivacy of communications.ó)fifth amendment protection of privacythe protection against selfincrimination also serves as a basis for atype of privacy protection, including primarily decisional privacy and,somewhat more weakly, bodily integrity. although the principle of thefifth amendmentñthat no person shall be compelled to be a witnessagainst himself or herselfñmay be relevant in many contexts, its application is limited to criminal cases or other government proceedings. thecourt has adopted a rather narrow view of the coverage of the fifthamendment by making a distinction between testimonial evidence, involving communication by the individual and thus falling under the fifthamendment, and physical evidence, entailing the taking of something21recent developments may be changing this baseline, however. for a general discussion of the law, see computer science and telecommunications board, national researchcouncil, cryptographyõs role in securing the information society, washington, d.c., nationalacademy press, 1996.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.privacy challenges in authentication systems67from an individual and thus falling outside the protection of the fifthamendment. this distinction was made most clearly in schmerber v.california,22 in which the court ruled that there was no fifth amendmentprotection against blood tests, viewed as physical evidence, to determineblood alcohol content following a car accident. the court distinguishedbetween situations in which a defendant was forced verbally to incriminate himself or herself and situations in which marks or material weretaken from him or her for identification purposes (fingerprints, photographs) or for purposes of preventing the dissipation of evidence (bloodtest). although the latter situations would not be covered by the fifthamendment, the court indicated that the sixth amendment protection ofcounsel, the fourth amendment protection against unreasonable searchesand seizures, and the due process clause23 would provide protectionagainst the stateõs overreaching in such situations.ninth amendment penumbras, fourteenth amendment due processclause, and decisional and informational privacyas mentioned above, privacy has been invoked to protect theindividualõs right to make decisions about important aspects of life without government interference. a line of supreme court cases starting withgriswold v. connecticut24 in 1965 began to establish such a right, althoughvarious justices viewed the source of the right differently. justice douglas believed the privacy right emanated from the first, third, fourth,fifth, and ninth amendments, which created òpenumbrasó of privacyprotection. other justices preferred to lodge the right in the ninth amendment. in roe v. wade,25 the court held that the right to privacy wasfounded in the fourteenth amendmentõs liberty clause and restrictionson state action. the right to privacy protected in this line of cases hasbeen primarily limited to reproductive and family interests, including theindividualõs right to make choices with respect to childbearing, child rearing, and the use of contraceptives.26 in whalen v. roe,27 the court articu22schmerber v. california, 384 u.s. 757 (1966).23in rochin v. california, 342 u.s. 165 (1952), justice frankfurter, writing for the majority,said that the forced regurgitation of stomach contents was conduct that òshocks the conscienceó and violates the due process clause of the fourteenth amendment.24griswold v. connecticut, 381 u.s. 479 (1965).25roe v. wade, 410 u.s. 113 (1973).26in paul v. davis (424 u.s. 693 (1976)), the supreme court refused to expand the areas ofpersonal privacy considered òfundamentaló to include erroneous information in a flyerlisting active shoplifters. the court limited these fundamental privacy areas to òmattersrelating to marriage, procreation, contraception, family relationships, and child rearing andeducationó (713).27whalen v. roe, 429 u.s. 589 (1977).who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.68who goes there?lated a constitutional basis for a right of information privacy, arguing thatthe constitutionally protected òzone of privacyó protects both an interestin avoiding disclosure of personal matters and an interest in independentdecision making. although recognizing an expanded privacy interest,the court unanimously found that the new york law in question, whichrequired the maintenance of computerized records of prescriptions forcertain drugs, did not pose a significant constitutional threat to eitherprivacy interest, in part because of the security of the computer systemand the restrictions on disclosure. in subsequent cases, the court has notexpanded constitutional protections for information privacy.the common law roots of privacy lawas mentioned above, constitutional privacy protections limit stateaction; they do not protect against intrusion by private individuals orentities. historically, tort law has provided protection for some aspects ofpersonal privacy. english and early american case law provides examplesof the use of tort law to protect against trespass into private spaces, unwanted knowledge of private events, and unwanted publicity of privatematters. in 1890, concerned with tabloid journalistsõ and photographersõintrusion on private matters, samuel d. warren and louis d. brandeis, inòthe right to privacy,ó28 set forth the òright to an inviolate personality.óamerican courts and legislatures adopted various expressions of the newprivacy tort throughout the early 20th century. in 1960, william l. prosserstructured and defined these various tort law privacy protections intofour separate privacy torts:1.intrusion upon seclusion: objectionable intrusion into the private affairs or seclusion of an individual,2.public disclosure of private facts: publication of private informationthat a reasonable person would object to having made public,3.false light: publication of objectionable, false information about anindividual, and4.misappropriation of name or likeness: unauthorized use of anindividualõs picture or name for commercial advantage.29the 1964 restatement of torts (a clarification and compilation of thelaw by the american law institute) adopted the prosser framework.3028samuel d. warren and louis d. brandeis. òthe right to privacy.ó harvard law review4 (december 1890):195.29william l. prosser, òprivacy,ó 48 cal. l. rev. 383 (1960).30restatement of torts (2d) 1964.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.privacy challenges in authentication systems69together, these torts provide a basis for privacy suits against those whopublish embarrassing false information or intimate information about anindividual, peep or spy on an individual, or commercially exploit anindividualõs picture, name, or reputation. today privacy torts providelimited protection for individuals. as torts, they are unlikely to directlyshape the design and use of authentication systems. however, the principles behind the intrusionuponseclusion, publicdisclosureofprivatefacts, falselight, and misappropriationofnameorlikeness torts are useful reminders of some of the things that privacy is designed to protectagainstñintrusion into personal affairs, disclosure of sensitive personalinformation, and improper assignment of actions to individuals. each ofthese is relevant to the discussion of authentication systems.statutory privacy protectionsin recent years, the federal trade commission (ftc) act of 191431has become a tool for enforcing privacy statementsñwhatever they maybeñmade by commercial actors to the public. section 5 of the ftc actgives the ftc jurisdiction over òunfair and deceptive trade practices.óimportantly, while the statute clearly provides an enforcement opportunity where statements about data collection practices are made, it aloneprovides no independent basis for compelling such statements, or fordriving their contents.32 a series of workshops, industrydeveloped selfregulatory guidelines, and enforcement actions by the ftc and offices ofthe states attorneys general have provided some check on objectionableor questionable private sector practices.over the years, congress has enacted a number of privacy statutes.most have come in response to changes in technology, to market failures,or to narrow interpretations of the fourth amendment. market failureshave led, as one would suspect, to statutes that primarily regulate privatesector behavior. narrow rulings on the protections afforded by the fourthamendment have led to statutes regulating government access to information. finally, statutes that address both market failures and narrowconstitutional interpretations have most often resulted from advances intechnology that cause civil libertarians and industry to push for newprivacy protections against the expansion of governmental and privatesector authority to collect and use private information.3115 u.s.c. ¤¤ 4151.32jeff sovern has articulated the position that the ftc actually has the authority to goafter various unsavory data practices under its current legislation and mandate. see jeffsovern, òprotecting privacy with deceptive trade practices legislation,ó fordham law review 69(4):1305.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.70who goes there?the existing federal and state statutory privacy protections are oftendescribed as piecemeal or patchwork.33 personal information containedin òsystems of recordsó held by the federal government are covered bythe privacy act of 1974,34 the freedom of information act of 1967,35 andother federal statutes dealing with particular records or record keepers.36statutes of many states on access to information contain privacy exceptions, and some states have òminió privacy acts. in general, rules governing access to and use of state and local records containing personal information are less stringent. personal information held by the private sectoris afforded the weakest statutory protections. while 11 federal statutescurrently provide some form of privacy protection for records held byspecific private sector entities37 and a set of statutorylike regulatory protections applies to health information,38 much detailed personal information in the hands of businesses is available for reuse and resale to privatethird parties and available to the government with little in the way oflegal standards or procedural protections. (chapter 6 in this report goesinto more detail about some of these statutes and the roles that government plays in the privacy and authentication sense.)business records are subject to few privacy regulations. while recentstatutes have increased the privacy regulations in the private sector, the33see colin j. bennett, regulating privacy: data protection and public policy in europe andthe united states, ithaca, n.y., cornell university press, 1992; david flaherty, protectingprivacy in surveillance societies, chapel hill, university of north carolina press, 1989;priscilla m. regan, legislating privacy: technology, social values, and public policy, chapelhill, university of north carolina press, 1995; and paul schwartz and joel reidenberg, dataprivacy law, charlottesville, va., michie, 1996.345 u.s.c. ¤ 552a.355 u.s.c. ¤ 552.36driverõs privacy protection act of 1994, 18 u.s.c. ¤ 2721 (1994); family educationalrights and privacy act of 1974, 20 u.s.c. ¤ 1232g.37right to financial privacy act of 1978, 12 u.s.c. ¤ 3401; electronic communicationsprivacy act of 1986, 18 u.s.c. ¤ 2510 (1995); communications assistance and law enforcement act of 1994, pl 103414, 108 stat. 4279 (1994) (providing heightened protections fortransactional data); cable communications act of 1984, pl 98549, 98 stat. 2779 (1984)(codified as amended in scattered sections of 47 u.s.c.); video privacy protection act of1988, 18 u.s.c. ¤ 2710 (1994); consumer credit reporting reform act of 1996, 15 u.s.c.1681 ¤ 2 (1997); telemarketing and consumer fraud and abuse prevention act of 1994, 15u.s.c. ¤¤ 6101â 6108; privacy of customer information (customer proprietary networkinformation rules of the telecommunications reform act of 1996), 47 u.s.c. ¤ 222 (c), (d)(1996); fair credit reporting act of 1970, 15 u.s.c. ¤ 1681 et seq.; childrenõs online privacyprotection act (1998), 16 u.s.c. ¤¤ 6501 et seq; financial services modernization act (1999),15 u.s.c. ¤ 6801 et seq.38on april 14, 2001, privacy regulations were issued by the department of health andhuman services by authority granted under the health insurance portability and accountability act of 1996 (see chapter 6 for more information on hipaa).who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.privacy challenges in authentication systems71u.s. legal and regulatory approach continues to be driven by concernsabout a given sector or a narrow class of information (see chapter 6). inaddition to piecemeal rules governing private sector use of personal information, the general rule established in two 1970s cases leaves personalinformation òvoluntarilyó provided to businesses without fourth amendment protection.39 the rationale espoused in these two cases dramaticallyshaped privacy case law and led to statutory protections for privacy. theprinciple that in general individuals have no constitutionally based privacy interest in information about them contained in the routine recordsof a business has specific consequences for individual privacy in authentication systems that routinely collect information about an individualduring the course of an authentication event that precedes a transaction.information privacy andfair information practicesstatutory protections for personal information all rest on the samecore set of òfair information practices,ó which were developed in responseto the move from paper to computerized records. the first òcode of fairinformation practices,ó developed in 1973 by an advisory committee inthe thendepartment of health, education, and welfare (hew), provideda core statement of principles that may be enforced either by statute orvoluntarily.40 these principles set out basic rules designed to minimizethe collection of information, ensure dueprocesslike protections wherepersonal information is relied upon, protect against secret data collection,provide security, and ensure accountability. in general, the principlesemphasized individual knowledge, consent, and correction, as well as theresponsibility of organizations to publicize the existence of a record system, to assure the reliability of data, and to prevent misuse of data. although the practices cited in the hew code have been broadly accepted,slightly different iterations of fair information practices have been offeredby different bodies.41,42 because of the broad recognition accorded the39in 1976, in united states v. miller, the supreme court held that individuals had noconstitutionally protected privacy interest in checks held by a bank. shortly thereafter, in1979, in smith v. maryland, the court ruled that because the numbers dialed by a telephonesubscriber were routinely collected business records of phone companies, subscribers hadno fourth amendment privacy interest in them and therefore no right to receive notice ofor to object to their disclosure to the government.40secretaryõs advisory committee on automated personal data systems. u.s. department of health, education, and welfare. records, computers and the rights of citizens, washington, d.c., 1973. available online at <http://aspe.os.dhhs.gov/datacncl/1973privacy/tocprefacemembers.htm>.41when discussions of online privacy began in the early 1990s, the concept and prinwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.72who goes there?fair information practice principles, they are explained in detail in table 3.1and used later in this report for analyzing the privacy impact of differentauthentication systems. in general, though, the individual principles havenot been implemented with uniform rigor. limitations on the collection ofinformation have not been widely adopted, consent has been largely renounced in favor of choice, and access has been harder to achieve.the concept of notice is in some respects a simple idea: people are tobe informed about how personally identifiable information is collected,used internally, and disclosed or exchanged. an organizationõs information practices should be, in theory, transparent. in practice, there arequestions about how complete notices need to be without either compromising the proprietary interests of the organization or confusing people.additionally, what really constitutes effective notice?43ciples of òfair information practicesó provided the foundation for policy discussions. twoexecutive branch study commissionsñthe information infrastructure task force (iitf) andthe national information infrastructure advisory council (niiac)ñdeveloped privacyprinciples for the national information infrastructure (nii). in both cases, these studycommissions echoed many of the traditional principles developed earlier, often modifying,and in some cases weakening, some of the core principles, such as consent and redress. butboth commissions also struggled with questions about fair information practice that arenew in the online environment. the iitf and the niiac recognized emergent principles,including the need to provide some opportunity for individuals to use technical controls,such as encryption, to protect the confidentiality and integrity of personally identifiableinformation. both acknowledged that individuals should be able to remain anonymous asthey conduct some online activities. the importance of educating the public about theprivacy implications of online activities was highlighted in the codes developed by the iitfand the niiac. although these early online privacy study commissions advocated a fairlydetailed list of fair information practices, by 2000 the various iterations of fair informationpractices for online privacy discussed by the federal trade commission and others largelyfocus on four: notice, choice, access, and security. efforts to articulate more clearly theessence of information privacy were not limited to the united states. indeed, the mostcomprehensive of these codes of fair information practices is the one crafted by the organization for economic cooperation and development (oecd) in 1980. the oecd code emphasized eight principles: collection limitation, data quality, purpose specification, use limitation, security safeguards, openness, individual participation, and accountability.42different countries have adopted these principles to varying extents. canada, for example, has developed a national privacy code, the model code for the protection of personal information. this code was developed through a consensus process that includedrepresentation from canadaõs direct marketing association. more information is availableonline at <http://www.csa.ca/standards/privacy/>.43other problems with the effectiveness of notices are illustrated by experience with thefinancial services modernization act of 1999 (commonly referred to as the grammleachbliley act), discussed in more detail in chapter 6, which requires financial institutions togive notice to customers regarding the sharing of personal information with a third party.financial institutions have complained about the expense incurred in sending notices. consumers have complained that notices are incomprehensible and unhelpful. see markhochhauser, lost in the fine print: readability of financial privacy notices, july 2001. available online at <http://www.privacyrights.org/ar/glbreading.htm>.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.privacy challenges in authentication systems73table 3.1fair information principles and practicesprinciplepractice/meaningcollection limitationcollect the minimum amount ofinformation that is needed for therelationship or transaction at issueññby lawful and fair means.ñwith the knowledge and consent ofthe individual.data qualityinformation should be relevant, accurate,timely, and complete.purpose specificationuse of data should be specified at thetime that data are collected.use limitation (restriction on secondarydata should only be used for the specificuses)purpose for which they are collected andfor which the individual understandsthey will be used, except under twoconditions:ñwith the prior consent of theindividual, andñwith the appropriate legal authority.securitythe integrity of the information and thesystem should be maintained to ensureagainst loss, destruction, unauthorizedaccess, modification, unauthorized use,or disclosure.openness/noticethere should be no secret data systems.people should be able to ascertain theexistence of data systems and theirpurposes and uses.individual participationan individual has rights toñknow if he or she is a subject of asystem,ñaccess information about him orherself,ñchallenge the quality of thatinformation, andñcorrect and amend that information.accountabilitythe organization collecting and usinginformation can be held responsible forabiding by these principles through:ñenforcement and/orñredress.xxxwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.74who goes there?federal agencies comply with notice provisions of the privacy act of1974 by publishing requests for comments in the federal register whenthey plan to create òsystems of recordsóñthose information systems thatcontain personally identifiable information such as a name or social security number. few individuals read the federal register to see whether afederal agency that maintains data on them in a system of record hasannounced in a routine use notice changes in the way that the agencyintends to use those data.the concept of òconsent,ó or the less stringent òchoice,ó is a morecomplex idea. in theory, individuals are to be given some power orcontrol over how personally identifiable information about them is used.in practice, the primary question is whether such control comes fromgiving the individual the opportunity to opt in by giving prior permissionor the opportunity to opt out by allowing them to say no. privacy advocates argue that òopt inó is more consistent with the idea of consent, whileòopt outó erroneously assumes that individuals tacitly give consent tosecondary uses. organizations argue that òopt outó gives individualsadequate opportunity to choose and does not overburden consumers orindustry.recognizing the complexity and importance of access and security inthe online environment, the ftc convened an advisory committee to examine and advise on these subjects.44 with regard to access, the committee addressed four questions: (1) what is the meaning of access (merelyview or view and modify)? (2) access to what? (3) who provides access?and (4) how easy should access be? the advisory committee on onlineaccess and security was unable to agree on a clear recommendation andinstead presented a range of access options. in part, the committee recognized that the dilemmas presented by the need to authenticate for accesspurposes complicated access options and necessitated an evaluation ofthe particular circumstances.the advisory committee on online access and security recognizedthat security likewise is contextual, that costs and inconveniences affectthe level of security that administrators are willing to set and users arewilling to bear, and that the establishment of a security system shouldbegin with a risk assessment. the committee outlined five options forachieving security and recommended a solution including these threeprinciples: (1) every web site should have a security program, (2) the44federal trade commission (ftc). final report of the ftc advisory committee on onlineaccess and security. washington, d.c., may 15, 2000. available online at <http://www.ftc.gov/acoas/papers/finalreport.htm>.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.privacy challenges in authentication systems75elements of the security program should be specified, and (3) the securityprogram should be appropriate to the circumstances.however, in the absence of the comprehensive adoption of statutesbased on fair information practices, much personal information remainsvulnerable to misuse, abuse (including the potential for identity theft),and unfettered government access. this situation poses serious privacythreats to authentication information held by private sector entities.privacy of communicationsconcern about the privacy of individual communication has grownas society comes to depend more and more on electronic and networkedcommunications. while the privacy of communications is only one aspect of the larger privacy policy framework, the evolution of the law ofcommunications privacy provides useful insights for policy makers andsystem designers considering the privacy implications of authenticationsystems. the development of communications privacy law illustratesthat as technology reaches farther and farther into sensitive, protectedareas of human activity, a legal response can guarantee that this sensitiveinformation, which may not have previously been revealed, accessible, oreven in existence, will be protected in accord with basic constitutionalvalues. at the same time, lawmakers and courts have recognized thatalong with protecting the privacy of communications, laws also need toprovide for law enforcement access to confidential information wherenecessary, consistent with basic fourth amendment protections. debatesover the appropriate balance between individual privacy interests andlaw enforcement power revolve around the proposition that increasinglypowerful technologies demand increasingly strong privacy protections.while the privacy issues raised by authentication technologies encompass more than the communications inside or associated with those authentication systems, new privacy protections are indeed needed for theseemerging technologies.communications privacy law has generally governed law enforcement access to interpersonal communications (wiretapping), but it alsocovers access by unauthorized private third parties. the expressive nature of communications has resulted in legislative and judicial recognition of the sensitivity of personal communications through special procedures controlling law enforcement access to communications. in general,advances in communications technology precipitate debates about theappropriate level of fourth amendment protection. these debates revealan evolving notion of what information is sensitive and thus deserving ofprotection from both governmental and commercial intrusion.as advanced communications technologies such as the internetwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.76who goes there?(email, the world wide web, and so on), wireless phones, and otherdevices complement and in some cases replace telephone communications, the united states as a nation has generally recognized the need tocreate privacy protections similar to those established for voice communications by the supreme court.45 from telegraph to telephone, wirelinephone to cell phone, email to the world wide web, users of the majornew communication technologies have acquired privacy protections fortheir communications. thus far in the history of electronic communications, policy makers, commercial providers, and even those in the field oflaw enforcement have come to agree that new technologies demand privacy protections,46 both out of faithfulness to basic constitutional valuesand to assure the commercial viability and acceptance of the latest communications technologies. however, the scope of such protections hasconsistently fallen short of the standards, based on the fourth amendment, that govern realtime voice communications. at the same time, therange of information and communications flowing through these newcommunications technologies has dramatically increased. thus today,many kinds of information are potentially accessible under the secretsearches of wiretap law. in addition, in light of recent events, there is anexpanding sense of what government may legitimately need to access tomeet national security and law enforcement requirements.most recently, congress has struggled with the question of the protection of online transactional records such as logs tracking the web pagesviewed by individual users and records of electronic mail messages sentand received. though law enforcement argued that these logs revealedlittle information and should be easily available for any investigative purpose at all, the legislature found that this information is sufficiently sensitive to warrant extra protection.electronic communications have required the expansion of privacyprotection commensurate with new technology capabilities (see box 3.1).the electronic communications privacy act (ecpa) of 1986 was supported by a coalition of businesses and privacy advocates who understood that protections similar to those for firstclass mail were a necessaryprecursor to business and individual adoption of email as a communica45see katz v. united states, 389 u.s. 347 (1967); available online at <http://laws.findlaw.com/us/389/347.html>.46in kyllo v. united states, justice scalia, writing for the majority, noted òwe think thatobtaining by senseenhancing technology any information regarding the interior of thehome that could not otherwise have been obtained without physical ôintrusion into a constitutionally protected areaõ (silverman, 365 u.s., at 512) constitutes a searchñat least where(as here) the technology in question is not in general public useó; see <http://supct.law.cornell.edu/supct/html/998508.zo.html>.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.privacy challenges in authentication systems77tions tool for sensitive information.47 similarly, the privacy amendmentsto ecpa in 1994 creating a higher level of protection for transactionalinformation generated in webbased interactions recognized that this information was more sensitive than the numbers dialed on a phone, andconsequently that public use of the web would be aided by creating morestringent protections against access.box 3.1expansion of fourth amendment protection andtechnological capabilitiesin their early stages, important new communications technologies such asthe telephone and electronic mail were not accorded the privacy protections thatare now taken for granted. in each case, the application of fourth amendmentprotections was unsettled, so the legislative branch had to step in to provide somelevel of protection. when the telephone first came into use, law enforcement wasable to conduct unfettered surveillance of private conversations because the u.s.supreme court ruled telephone calls to be beyond the protection of the fourthamendment. though telephone callers never invited law enforcement officers tolisten in on their calls, the court held that fourth amendment protections onlyapplied to intrusions on oneõs property (either physical or real). as conversationshad no property interest attached to them, they merited no privacy protection. later, however, the supreme court reversed itself and declared that òthe fourthamendment protects people, not places.ó1early electronic mail systems also lacked clear legal protection. to some, thefact that an email message passed through the hands of third parties (internetservice providers or other operators of electronic mail systems) meant that thesender and the recipient had forfeited their privacy rights by handing over themessage to others. at the urging of the nascent electronic mail industry and privacy advocates, however, congress extended privacy protection to email and setclear rules governing law enforcement access. these rules are now in some fluxowing to the passage of the usa patriot act of 2001 and uncertainty about howit will be applied and enforced.1see katz v. united states, 389 u.s. 347 (1967), which states òbecause the fourth amendment protects people rather than places, its reach cannot turn on the presence or absence ofa physical intrusion into any given enclosure. the ôtrespassõ doctrine of olmstead v. unitedstates, 277 u.s. 438, and goldman v. united states, 316 u.s. 129, is no longer controlling.óavailable online at <http://laws.findlaw.com/us/389/347.html>.47see electronic communications privacy act of 1986 (pl 99508). available online at<http://www.cpsr.org/cpsr/privacy/wiretap/ecpa86.html>.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.78who goes there?concluding remarksauthentication technologies, like other technical advances, renew thedebate about how much privacy protection should be provided to personal information generated in the authentication process. as with otheradvances, in order to speed adoption, policy makers, industry, law enforcement, and privacy advocates should identify the privacysensitivefeatures of these technologies and develop appropriate protections.finding 3.1: authentication can affect decisional privacy, information privacy, communications privacy, and bodily integrityprivacy interests. the broader the scope of an authenticationsystem, the greater its potential impact on privacy.recommendation 3.1: authentication systems should not infringe upon individual autonomy and the legal exercise of expressive activities. systems that facilitate the maintenance andassertion of separate identities in separate contexts aid in thisendeavor, consistent with existing practices in which individuals assert distinct identities for the many different roles theyassume. designers and implementers of such systems shouldrespect informational, communications, and other privacy interests as they seek to support requirements for authenticationactions.in terms of developing an actual system, and considering fair information principles and practices as described in this chapter, as well ashow authentication works in the abstract (as discussed in chapter 2), thefollowing guidelines are offered for the development of authenticationsystems that would protect privacy interests as much as possible.recommendation 3.2: when designing an authentication system or selecting an authentication system for use, one should:¥authenticate only for necessary, welldefined purposes;¥minimize the scope of the data collected;¥minimize the retention interval for data collected;¥articulate what entities will have access to the collected data;¥articulate what kinds of access to and use of the data will beallowed;¥minimize the intrusiveness of the process;¥overtly involve the individual to be authenticated in theprocess;who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.privacy challenges in authentication systems79¥minimize the intimacy of the data collected;¥ensure that the use of the system is audited and that theaudit record is protected against modification and destruction; and¥provide means for individuals to check on and correct theinformation held about them that is used for authentication.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.804security and usabilityprevious chapters describe abstract notions of authentication andprivacy. fully understanding the implications of authenticationfor privacy requires considering authentication systems as a whole.no working authentication technology exists in a vacuum. how is thetechnology deployed? what policies are in place with respect to its use?which resources is the system meant to protect? what are the goals of thesystem? understanding the technology as part of a larger system is key toevaluating its privacy implications. in this chapter, authentication is examined within a broader systems context. two important systemslevelcharacteristics of authentication systems are discussed: security and usability.as noted previously, security is a primary reason for the deploymentof authentication systems. security is also vital to the preservation ofprivacy in that one must make use of security technology in order toprotect privacyrelated information. it is not simply the technical mechanisms of security that matter but also the processes and policies governing who has access (and how) to sensitive data. it is therefore essential tounderstand the security requirements and properties of both the authentication system itself and the resources it is protecting. to that end, adiscussion of threat models and how to think about security risks is presented. the people using these systems are an important component ofthem, and their needs and behaviors must be taken into account. accordingly, the committee develops the notion of usercentered design, withparticular emphasis on the authentication context. finally, it remarks onwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.security and usability81and makes recommendations about secondary and unplannedfor uses ofauthentication systems.threat modelsas noted previously, a significant motivator for authentication technologies is increased system security. ultimately, understanding the context in which the system will be deployed and the threats likely to befaced will enable determining whether authorization, accountability, and/or identification (as well as authentication) will be required. while authentication technologies are generally used to increase system security,security is not a binary property of a system.1 a system is secure, orinsecure, only relative to a perceived threat.2 to understand this concisecharacterization, some definitions are required.threatsthe terms òattackó and òthreató are often used interchangeably insecurity discussions, and in informal discussions this is an acceptablepractice. however, in this report the committee adopts more precisedefinitions for these terms and other, related security terms to facilitate anunderstanding of the security issues related to authentication systems:¥a vulnerability is a securityrelevant flaw in a system. vulnerabilities arise as a result of hardware or software bugs or procedural, personnel, or physical security problems.¥an attack is a means of exploiting a vulnerability. attacks may be1as part of overall system security, the security of the authentication component itself(separate from broader system security issues) is crucial, because without it, a primarypurpose of the authentication process is undermined. for any authentication technology,the possible vulnerabilities, present and future, must be evaluated. apart from flaws particular to a given method, there are several questions that can be asked of any scheme, suchas whether the authentication credentials can be shared (and if so, whether the originalowner still retains the ability to use them), whether the credentials can be forged, andwhich sorts of errors can be due to human limitations (such as forgetting a secret or losing atoken). another question that bears on the security of the authentication system is whethera secret value is transmitted to or stored by the verifier. in some sense, a proper understanding of the vulnerabilities is even more important than the vulnerabilities themselves.do system security administrators understand the failure modes? do users understand theweak points?2for an indepth discussion of computer and information security, see computer scienceand telecommunications board, national research council, trust in cyberspace, washington,d.c., national academy press, 1999. available online at <http://cstb.org/pubtrust/>.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.82who goes there?technical, procedural, physical, and so on, corresponding to the type ofvulnerability being exploited. passive wiretapping, buffer overflows, andsocial engineering (for example, deceiving individuals such that they reveal privileged information) are examples of attacks.¥an adversary is an entity (an individual or an organization) withhostile intent. hackers, criminals, terrorists, and overly aggressive marketers are examples of adversaries.¥a threat is a motivated, capable adversary. the adversary is motivated to violate the security of a target (system) and has the ability tomount attacks that will exploit vulnerabilities of the target.3¥a countermeasure is a security mechanism or procedure designed tocounter one or more types of attack. a countermeasure does not remove avulnerability but instead prevents some types of attack from effectivelyexploiting one or more vulnerabilities. secure sockets layer (ssl), forexample, can be used to encrypt communication between a browser and aserver in order to counter passive wiretapping attacks that could disclosea static password.4in practice, every system contains vulnerabilities of some sort, whenviewed in a broad context. the existence of a vulnerability does not initself make a system insecure. rather, a system is insecure only in thecontext of a perceived threat, because that threat is motivated and capableof exploiting one or more vulnerabilities present in the system. for example, in order to exploit a vulnerability in an implementation of a userauthentication system, an adversary might have to possess a very sophisticated technical capability. if likely adversaries do not possess thatcapability, then the system may be considered adequately secure for anintended application context. of course, the adversary could also bribeone or more insiders. all vulnerabilities must be considered.to understand threats, one usually begins with a list of common adversaries and a discussion of their possible motivations, capabilities, anddegree of aversion to detection. the following examples illustrate thisnotion:¥hackers represent a class of adversaries who tend to be opportunistic. that is, a target often is selected because of its vulnerability ratherthan for its strategic value. hacker capabilities are primarily in the form ofattacks launched via network access, as opposed to the exploitation of3this definition is consistent with the use of the term in political and military contexts,such as references to the òsoviet threató during the cold war.4of course, this protection does not address other fundamental vulnerabilities of staticpasswords, such as ease of guessing.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.security and usability83physical or personnel vulnerabilities. often these attacks are not stealthy,and many hackers do not seem to be especially averse to detection. individual hackers do not tend to possess significant resources, but groups ofthem may collaborate to bring to bear significant computing resourcesagainst a target (some distributed denialofservice attacks are an example,although such attacks may also be carried out by individuals). a smallnumber of hackers are highly skilled, and these individuals create attacktools that are distributed to a much larger, lessskilled hacker community.because of the opportunistic nature of hackers and because the hackercommunity is so large, all systems with network connectivity should consider hackers as threats. many hackers seem to place little value on theirtime, and thus may be willing to expend considerable personal time onwhat might seem a trivial target, perhaps motivated more by the desirefor bragging rights than by the value of the data accessed.¥insiders are authorized users in some organizational context. thus,they have legitimate access to some set of computers and networks withinthat context and usually have physical access to computers employed byother users in that context. the threat from insiders can arise in one oftwo ways. first, benignly intended insiders may behave inappropriatelyout of either curiosity or error, causing damage. second, malicious insiders may intend to cause damage. in both cases, the set of things that couldbe damaged or taken usually is constrained by the organizational contextin which an insider operates. insiders are usually averse to detection,although a disgruntled employee who is being fired may be less so. malicious insiders typically have limited resources, but their intimate knowledge of systems and physical access to them give malicious insiders advantages relative to external attackers.¥industrial spies, in contrast to hackers, select targets on the basis ofthe perceived value of some aspect of the target (for example, content),and they are highly averse to detection. they tend to employ stealthyonline attacks to reduce the risk of detection, but they also may employattacks (for example, bribery) against personnel. because these adversaries are paid to conduct attacks, their methods take personnel and materielcosts into account. industrial spies may also take jobs in order to acquireinsider access (see above).¥criminals often select targets for financial gain and thus often prefer stealthy attacks that minimize the risk of detection. (an exceptionmight be denialofservice attacks used to extort.) they may be willing toexploit personnel or physical security vulnerabilities as well as to engagein technical attacks. they may employ considerable financial resources.¥activists launch attacks whose goal might be to generate publicity(for example, by disrupting services) to serve a more subtle purpose (forexample, to acquire data used to embarrass the target). they are not espewho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.84who goes there?cially averse to detection, nor do they generally possess significant resources, but they may exploit ideological biases in personnel (insiders)and may engage in physical attacks.¥nationstate spies and terrorists typically select targets with greatcare and employ very stealthy techniques to avoid detection and attribution. they may bring significant financial and technical resources to bearagainst the target. they may employ a full range of attacks to exploitphysical, procedural, and personnel vulnerabilities. even statesponsoredespionage budgets are not infinite, so the cost of attacking a target isbalanced against the expected gains.in the context of userauthentication technologies, it is important tounderstand the threats against which the technologies are effective andunder what circumstances the technologies will fail.5 if security is compromised, privacy is likely to be compromised as well. in addition, as therest of the report describes, even with sufficient security there still may bethreats to privacy. choices about which kinds of authentication systemsto deploy need to take these factors into account.recommendation 4.1: in designing or choosing an authentication system, the first step should be to articulate a threat modelin order to make an intelligent choice among competing technologies, policies, and management strategies. the threatmodel should encompass all of the threats applicable to thesystem. among the aspects that should be considered are theprivacy implications of using the technologies.dealing with threatsassuming (perhaps unrealistically) that a serious and explicit evaluation of the security of a system relative to a perceived threat model hasbeen carried out, the next question is what to do in response: how shouldone protect the system? the answer will depend on the potential losses(including the potential damage to reputations) if attacks were to succeed,as well as on the riskmanagement strategies adopted by those makingthe decisions. several fundamental approaches to securing systems havebeen adopted over time, and multiple approaches usually are employedthat complement one another.65for example, an authentication technology that uses a onetime password list may bevery effective against hackers using networkbased attacks but ineffective against an insider who might have ready access to the list taped to a monitor.6the concise mantra adopted by the department of defense in the 1990s, òprevent, dewho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.security and usability85if a vulnerability cannot be eliminated, security countermeasures areoften deployed to mitigate the risk that the vulnerability will be exploited.countermeasures usually thwart some specific, known class of attacksand thus may not offer protection against new attacks that exploit thevulnerability. although deploying countermeasures is not as attractive aseliminating underlying vulnerabilities, it is an essential part of the security technology arsenal, since countermeasures can be added into an existing system to complement the security capabilities of that system.the minimization of vulnerabilities and deployment of countermeasures generally will improve system security, but rarely will they ensuresystem security in the face of a wide range of attacks. thus, it is alsoprudent to engage in monitoring in order to detect traffic patterns orsystem behavior that may be indicative of an attack. this process isreferred to as òintrusion detectionó and is commonly employed in manyenvironments today. of course, it implies being able to differentiate thebehavior of intruders from that of normal authorized users. even if anintrusiondetection system detects an attack only after the attack has beensuccessful, the detection can be useful for evaluating the extent or modeof the attack.responses to attacks usually take the form of applying new softwarereleases or updates, deploying new countermeasures, or adjusting intrusion detection systems to better monitor newly discovered vulnerabilities. rarely is it practical to engage in any form of retaliatory action againstan attacker; this is true for several reasons, including the difficulty oflocating the source of an attack and legal constraints on such activities.once a sufficient threat analysis has been undertaken, the securityrequirements of the system should be more explicit.7 it is at this pointthat decisions can be made about whether authentication is necessary andtect, respond,ó reflects this multifaceted theme. another commonly accepted approach tosecurity is captured by the phrase òdefense in depth.ó the notion here is that multiple,independent security mechanisms provide greatly improved security, since all must becircumvented in order to breach security. defense in depth is compatible with efforts toòprevent, detect, respond,ó though it also can be pursued independently. preventativemeasures attempt to prevent attacks from succeeding. for example, it is obviously desirableto remove vulnerabilities from a system whenever that proves feasible. the security patches(securityrelevant software updates) issued by vendors are an example of this process. applying security patches generally is viewed as a preventative measure since it prevents laterexploitation of a vulnerability. but in many cases a patch is remedialñthat is, the patch isdistributed after an attacker has already exploited a vulnerability.7one of the reasons that ecommerce has blossomed is that banks assume the risk of badcredit card transactions (perhaps passing some portion of this risk on to merchants). riskanalysis and apportionment must therefore be part of a careful threat and security requirement analysis.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.86who goes there?what requirements the authentication subsystem would need to satisfy.the overall security requirements of the system will determine whetheran authentication component is required, and, as indicated previously,the authentication system itself will have its own security needs. finally,the perceptions and actualities of ease of use and usefulness (in this case,the prevention of perceived harm) play important roles in how securesystems of countermeasures are in practice. this is discussed further inthe next section.authentication and peopleñusercentered designauthentication and privacy schemes will fail if they do not incorporate knowledge of human strengths and limits. people cannot rememberall of their passwords, so they write them down (typically under thekeyboard or on their handheld computers) or make them easy to remember (using petsõ names or their own birth dates) and consequently easy toguess. people do not change passwords, because it makes them too hardto remember. furthermore, people do not want to delete a file containingpages and pages of characters such as ò xøøøøøøø ı:i€øøøø h .óits purpose is unclear, and there is a fear of disabling some application onthe computer. similarly, although much private information is stored inthe cookies on computersñinformation about personal identificationnumbers, preferences, and timestamped indicators of which web siteswere visitedñpeople do not remove these after every transaction, even incases where the cookies provide no real benefit to the user.8 two possiblereasons for this are that since there are no visible traces of their existenceand use, people forget that the cookies are there, and if they do remember,it is too hard to find and delete them. similarly, people find it difficult toread the òfine printó of privacy notices that companies are now requiredto send out, both because the print itself may be too small and because thenotices are usually written in obfuscated style.9therefore, in order to work effectively, authentication and privacyschemes need to be designed with the same consideration of humanstrengths and limits as for any other technology, maybe more. to do this,one can borrow heavily from the practice of usercentered design and8rarely are there negative consequences from cookie contents, so people continue totrust the system. this potentially misplaced trust leads to other vulnerabilities and raises aphilosophical problem. like civil liberties, society gains from an atmosphere of trust. is thedesire to preserve that atmosphere enough to accept a few vulnerabilities, or is it so important to prevent the vulnerabilities that the atmosphere of trust will be sacrificed?9the problem of providing effective privacy notice is appreciated by consumer protection experts.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.security and usability87from some additional aspects of psychology. the following section outlines the practice of usercentered design and discusses how it applies tothe design of authentication and privacy schemes. some other facts abouthuman behavior that are relevant to these issues are noted. a suggesteddesign and evaluation process is presented for considering various alternatives for authentication and privacy that are proposed in this report.lessons from usercentered designusercentered design puts the userõs needs and capabilities at theforefront. much of todayõs technology is driven by the pursuit of a technology solution, without regard to whether the technology provides thefull functionality that the user needs in the context of its use or whether itis learnable, usable, and pleasant. after the failure of a number of products and systemsñnotably the apple newton, the mars orbiter, and theflorida 2000 election ballotñmore and more software engineers are putting òuserexperience engineers,ó or people with human factors experience, on their teams.10 userexperience engineers use various methodsto understand the userõs needs and to design an interface that is easy tolearn, usable, and enjoyable. there is evidence that the software is measurably better and the products more successful in the marketplace whenthese methods are used.userexperience engineers rely on a number of design principles thatare based on the known strengths and limits of human cognition. cognitive psychologists have known for years, for example, that human shortterm memory is limited and fragile, that learning takes time, and thatpeople make momentbymoment decisions about cost or effort and perceived benefit. however, people are also remarkably perceptive, in boththe literal and conceptual sense. humans have great powers of understanding visual input, particularly if it follows certain principles of grouping and use of color and dimension. people make errors by misremem10extracted from several articles by userexperience expert donald norman. normanprovides the example of the soviet unionõs phobos 1 satellite in a 1990 article in communications of the acm. the orbiter was lost on its way to mars not long after launch. later, itwas found that the cause of the error was an operator who sent a sequence of digitalcommands to the satellite but mistyped a single character. unfortunately, this error triggered a test sequence stored in the readonly memory that was supposed to be executedonly when the spacecraft was on the ground. the wrong sequence set the satellite in rotation, and it was no longer possible to resume control over it; it was lost in space (d.a.norman, òcommentary: human error and the design of computer systems,ó communications of the acm 33(1990): 47.) norman commented on the 2000 florida ballot in an interview with kenneth chang of the new york times (òfrom ballots to cockpits, questions ofdesign,ó new york times, january 23, 2001).who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.88who goes there?bering, by not attending to signals, and by simple mistaken acts such ashitting a key adjacent to the one intended.on the basis of knowledge accumulated from decades of research incognitive psychology, userexperience engineers have developed a coreset of design principles, a common set of which are shown in table 4.1.how these lessons apply to systems employing authentication and privacy schemes is explored next.table 4.1key design principles in usercentered designprinciplepracticesbuild on what the user knows.do not tax the memory of users byhaving them learn too many new things.use their words. build the systeminteraction with a story or metaphor inmind that the user will easilyunderstand.simplify.do not add features that arenõt useful.package large feature sets in terms ofclusters of things for a particular task,not jumbled together to cover allpossibilities.allow users to do things in the order indo not make users do things in the orderwhich they think of them.that the computer needs if it is differentfrom what users would do naturally.(the computer can keep track of thingsbetter than the user can.)display information in clustered,place things that go togethermeaningful visual displays.conceptually near each other in thedisplay and in an order that fits whatusers are trying to achieve.design for errors.people make errors. design so thaterrors are not costly to the user ordifficult to correct. allow each action tobe undone once the user has seen theconsequence of the error.pace the interaction so that the user is inthe user can be slow, but the systemcontrol.shouldnõt be. slow reaction to a useraction discourages acceptance of atechnology.xxxwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.security and usability89issues of limited memorythe issue of forgetting passwords is clearly an issue of limitedmemory and of the first two principles shown in table 4.1. learningand recall are hard and take time. because of this limitation, peoplewill augment their memories by writing things down. recently, newservices have been offered on the internet and in some operating systems to alleviate the problem that many people have with many different passwordsñfor example, one for ordering tickets online, one foraccess to digital libraries, and others for getting financial informationonline. this type of service will build for a person a single portal towhich he or she signs on with one password. it stores all of thatpersonõs passwords and automatically accesses the services for whichhe or she has signed up. many of the userõs resources are consequently subject to theft with the loss of a single passwordñthe oneused to access such a resource.simplicitythere are systems that have the desired functionality of allowingthe user finegrained control but that tend to be far too complicated touse. some email systems, for example, allow the user to formulaterules to sort incoming email into folders for examination later in someorder of priority. there are two major problems with these systems.first, people find that the rules that they have to write are maddeninglydifficult because they must specify their preferences in very specificterms having to do with searchable fields in email. for example, whena correspondent has more than one email account, the rule writer mustremember not just a particular personõs name, but all of the characterstrings that might appear in that personõs email òfromó line. second, ifthe rules designate things to be automatically deleted, the user mayhave no trace of unintended consequences. similar effects follow whenusers attempt to program their telephones to deny calls fromtelemarketers. they must specify their rules not by naming people fromwhom they want to hear but by designating telephone numbers. thesystem was designed without regard for the difficulty that the user willincur at the interface.technologies that are difficult to learn will exclude people who arenot willing to or cannot expend the effort needed to learn them. of note,a recent study of peopleõs understanding of pretty good privacy (pgp), acommunications encryption technology, showed that many users failedoutright to understand what they were supposed to do and made catastrophic errors, such as sending the private key instead of the publicwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.90who goes there?key.11 however, criticisms of userinterface problems are usually moreaccurately applied to an implementation than to a buildingblock technology per se.making things visibleassociated with peopleõs ability to understand complex visual displays is their inattention to things not visible: òout of sight, out of mind.óthe example of cookies earlier in this report is an instance of invisibility.things invisible are often not attended to. the files in òtempó folderssimilarly fail with regard to visibility and readability. but, since the possible consequences of the loss of privacy are not brought to the userõsattention on a daily basis, it can be easy to forget about their possibleharm. the fact that people ignore things that are not visible may explainwhy they claim to be concerned with privacy and then do not protecttheir privacy when given the opportunity.indirect effects of bad usercentered designpoorly designed systems can also have indirect effects on privacy,accuracy, and control. many of the databases in use at large financialcompanies, motor vehicle bureaus, social service agencies, and so on arelegacy systems with badly designed interfaces and poor checks on theaccuracy of data entered, resulting in errors in database records. many ofthese errors are caused by poorly designed interfaces, poor training forthe highturnover workforce (high turnover caused by the boredom ofsuch a lowlevel job), and low motivation. and, since it costs an organization money to ensure accuracy (for example, verifying by doubly enteringthe data and finding mismatches, or building algorithms that will checkon accuracy and duplication, and so on), errors are accepted. it is thevictim of an error who seems to have to bear the cost of correction.lessons from cognitive and social psychologyother aspects of human behavior, including the following, affect thesuccess of authentication and privacy schemes:¥how people make decisions,¥the basis on which they trust other people and institutions,11a. whitten and j.d. tygar. òusability of security: a case study.ó proceedings of the 9thusenix security symposium. august 1999.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.security and usability91¥their assumption that the physical world does not apply to thevirtual world, and¥how a personõs behavior changes when that person is not visibleto others.these aspects of human behavior are all relevant to the design ofauthentication and privacy schemes.decision makingthroughout daily life, people make small decisions about whether totake actions or not. often such decisions are based on simple calculationsof cost and benefit. in making decisions, people frequently overvaluethings that have immediate value and undervalue actions that may havea longterm payoff. and, sometimes the cost is clear and the benefitunknown. this tendency causes people to not make the effort to dosomething that would protect their longterm interests.for example, knowing that people do not like having their privateinformation accessible to others unknown to them, some entrepreneurshave built software called òcookie cuttersó that automatically preservesthe cookies a person chooses to preserve and flushes the rest when aweb session is over. unfortunately, not only are such programs generally expensive, but they also require a particular level of skill for installation and require users to specify their preferences in much the sameway as email filtering programs and telephone screening devices do.the cost is high, both in time and in the effort to understand and specifythings, and the benefits are really unclear. it is hard to articulate thebenefit of keeping this information private, both now and in an unforeseen future.the fact that people often see immediate benefits but not longtermconsequences also explains why they are willing to divulge private information for shortterm freebies. for example, some people will allow theirweb activity to be monitored if they are given a free email account.trustmuch of our society is based on trust. people take actions that makethem vulnerable, believing that others will do them no harm. theyaccept payment in checks or credit cards assuming that the payer hasthe resources and will pay when asked. they follow traffic rules ingeneral, expecting others to do so as well. people buy things sightunseen on the web, expecting delivery of the goods and/or services asadvertised. people believe, often wrongly, that when they share theirwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.92who goes there?email address or social security number, this information will not besold to third parties who will either solicit them to buy unwanted thingsor defraud them in some way (steal from their accounts). in normal,everyday behavior, people do not authenticate one another. they spendlittle effort and make assumptions based on commonly available data.furthermore, people are usually not proved wrong after doing this, sotheir experience with the system encourages them to behave this way.on what information do people base their judgments with respect totrust and lying? the best information that someone is trustworthy comesfrom direct experience: a person was trusted in the past and all behavioralevidence is that he or she caused the trusting person no harm or, better yet,looked after his or her interests. the second source of information whenmaking such judgments is a set of physical and behavioral attributes thatsuggest someone is similar to the trusting person. the chain of reasoninggoes like this: if i am trustworthy and you are like me, then you must betrustworthy, too. the third source is endorsements, either from someone aperson knows, such as a friend who has had direct experience with theperson or institution in question, or from some famous person who wouldlose reputation if he or she lied about anotherõs trustworthiness. the fourthsource is assessment of reputation directlyñfor example, the reputation ofa person in a powerful position could be lost if he or she were not trustworthy or an organization that has been around for a long time might notcontinue to exist if it was less trustworthy.knowing how people make decisions, some people and institutionshide behind false information. some individuals might buy from a website that has an endorsement symbol from the better business bureau,even though there is not necessarily an immediate way to authenticate thevalid use of the symbol. it could simply have been copied from anothersite. some people might choose to enter credit card information into a sitethat seems to be designed well and not into one that seems to be slappedtogether, making the assumption that a welldesigned site costs moneyand could not have been afforded by a flybynight vendor. becausepeople do not spend the time and effort to investigate authenticity andthe shortcut attributes that they use are well known, they are left open tofraud at many levels.1212the federal trade commission is charged with consumer protection broadly and hasbeen investigating conduct on the web. more information on the ftc, privacy, and fairinformation practices is presented in chapter 3.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.security and usability93assumptions from the real world that do not transferto the virtualin our social interaction in the physicalspatial world, some behaviorsare correlatedñfor example, when i see you, you can see me. when i getcloser to you, you get closer to me; when i hear you, you can hear me. thedigital world has uncorrelated these attributes. with web cameras oroneway videoconferencing, i can see you, but you canõt see me. a wholehost of surveillance devices allows detection without obvious detectability. what used to be reciprocal is now oneway. however, people behaveas if the virtual world had the same reciprocity as the real world. oneday, for example, as a demonstrator was explaining online videoconferencing on a public site, she was startled when someone she couldnot see spoke to her. that person, in turn, could see her, but, because ofthe camera angle, could not see that she had an audience of highlevelexecutives. thinking that she was alone, he made several lewd remarks.the demonstrator and the executives were surprised and offended by theencounter. they had wrongly assumed that the only others online werethose they could see.there are a number of cases today in which the person using a serviceor technology is told the conditions (for example, that others may monitoran interaction or that the numbers a person types in will be used for otherpurposes), but they forget. the many years that humans spend learningphysical reality are hard to dismiss when one is merely told that thingsmight be otherwise.accountability a related issue is accountability. there is mounting evidence that when people are anonymous or hidden, they will behave inways that are very different from normal. for example, when feedbackfor a product or service is anonymous, people are more likely to saynegative things, knowing that they are not held accountable for theirbehavior. they can get away with more. a similar effect comes into playin email flaming. when the recipient is not visible to the writer of the email, the writer tends to say more emotionally charged things, things thathe or she would soften if the recipient were visible. there is evidencefrom experiments on trust that if people cannot be seen by others, theywill behave in a more selfserving, as opposed to a cooperative and trusting way.actions there are at least two approaches to accommodating the knownlimits of human behavior when considering authentication and privacyschemes: designing to fit those known limits and training people to becautious when caution is warranted.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.94who goes there?finding 4.1: people either do not use systems that are not designed with human limitations in mind or they make errors inusing them; these errors can compromise privacy.recommendation 4.2: usercentered design methods should beintegral to the development of authentication schemes and privacy policies.training, public education campaigns, and regulationsit is unlikely that technologies and the policies associated with themwill be transparent to everyone. people have a hard time changing expectations and assumptions in a world that is invisible to them. consequently, some protective measures may have to be put in place to makeup for these human shortcomings. in many cases, it is not apparent to theuser what information is being given to the verifier. most users do notknow what information is on magneticstripe cards, smart cards, and barcoded cards. printed information on the cards is more apparent to theuser, but information learned by way of backend processes (for example,a check of a credit report) can be invasive of privacy, yet invisible to theuser. as for protective measures at a micro level, web sites could havevisible statements of the sort heard in many recorded messages on customer service hotlinesñnamely, that the conversation (site use) may bemonitored to ensure quality. similarly, small explanatory or warningsigns might appear when one moves the mouse pointer over a field thatasks for information from the user. more broadly, a public educationcampaign may be necessary to keep people from unknowingly makingthemselves vulnerable, just as people are warned of new scams. the goalof such education should not be to cause people to mistrust others but totrain them to understand the nature of information systems and how theycan and should protect themselves.ultimately, most individuals do not understand the privacy andsecurity aspects of the authentication systems that they are required touse in interactions with commercial and government organizations. asa result, they may behave in a way that compromises their privacy and/or undermines the security of the authentication systems. to remedythese problems, system interfaces should be developed that reveal theinformation collected and how it is used. research is needed to explorehow to do this effectively. in addition, as part of the deployment of anynew system that includes the collection of privacysensitive data, individuals should be educated about the privacy and security aspects ofthe authentication systems that they use, including the risks associatedwith the systems.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.security and usability95factors behind the technology choicea crucial factor that will encourage or discourage the use of anyauthentication technology is ease of deployment. a scheme that relies onsomething that users already have (or already òareó) is easier to deploythan one that requires shipping (and perhaps installing) new equipment.smart cards are relatively hard to deploy, however, since few people havethe smart card readers and associated software on their computers. inaddition, most card issuers insist on owning their own cards, which iswhy cards that could technically òshareó issuers (for example, a paymentcard that is also an airline affinityprogram card) have not been successfulin the market. with respect to possession of the correct hardware, however, most computers sold in the past several years have universal serialbus (usb) ports; a usbbased token that functioned like a smart cardmight require less effort to deploy, although the software barrier wouldstill exist. this observation is not an endorsement of usb tokens butrather points out how many factors, cost among them, may inhibit orfacilitate the deployment of hardwarebased authentication technologies.the building blocks from which different authentication systems areconstructed may be used to protect or invade privacy. among the privacy issues that arise are what data are revealed to the issuer upon initialinteraction with the system and what data are created and stored at thattime, as well as when authentication events occur. a system in whichpersonal data are retained after an authentication transaction is moreintrusive of privacy than one in which no data are collected or retained.one of the most crucial issues with respect to privacy and authenticationsystems, as discussed in chapter 2, is linkage: can the results of an authentication process be linked to other sets of data? if the same individualuses cash for two activities, those activities are not linkable through thecash that was used; however, two uses of a storedvalue card are linkableeven if the card itself was purchased anonymously.cost is another factor in deciding which authentication technologiesto deploy. authentication can be expensive. there may be hardware,software, and proceduraldevelopment costs for the presenter, the verifier, and the issuer. integrating authentication procedures into existingsystems and procedures may be expensive as well. data collection andverification, both for initial registration and for recovery from lost credentials, can impose additional overhead. after deployment, authenticationsystems have ongoing costs. new users must be added to the system andold users deleted. hardware and software require maintenance and upgrades. procedural updates and user and administrator training andeducation are important (and costly) too; many systems fail not for technical reasons, but because someone did not follow proper procedures.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.96who goes there?that, in turn, raises the issue of skills: what does it cost to obtain theservices of enough people with appropriate abilities to deploy and operate an authentication system? finally, costs have to be apportioned. whopays for all of this? what is the balance of costs between the user, theissuer, the verifier, and any backend systems?balanced against the costs of using an authentication system are theperceived and actual gains from deploying the system. since vulnerabilities are hidden and threats are generally not observable except to thosewho have done ample analysis of the particular system and its context,people using the system may underestimate the benefit of risk mitigation.this can result in inappropriate choices of authentication systems or ininappropriate usage models that defeat the purpose of the system. forexample, a recent study found that many users did not adhere to the rulesabout not making their passwords guessable, or about keeping them secret and changing them often. people had a number of excuses for ignoring such rules: they were not the kind of people who keep secrets, theyshared passwords because doing so showed how they trusted otherpeople, they thought they were not targets because they had nothing ofvalue, and so on.13other factors besides cost and security have a bearing on technologychoices. not all authentication systems work for all population sizes.some are too expensive on a peruser basis to work with large numbers ofusers; others have too high an upfront cost to be suitable for small populations. furthermore, all systems have error rates encompassing falsepositives and/or false negatives. the likelihood and cost of each type oferror, along with a priori estimates of the number of potential imposters,must be assessed to determine what tradeoffs should be made.14 inaddition, the consequences of privacy invasions represent costs that maybe hard to monetize (absent actual damages) and may be incurred bysomeone other than the system owner.finally, an authentication system must be matched to the needs of thecontext in which it is employed, keeping in mind that authentication maybe excessive if simple authorization will suffice. notwithstanding thatstronger authentication is often suggested as an initial step to improvesecurity, the issues are often much subtler. deciding whether and how toemploy authentication systems (or any other security technology) requires13d. weirich and m.a. sasse. òpersuasive password security.ó proceedings of chi 2001acm conference on human factors in computing systems. seattle, wash., april 2001.14creation of exceptionhandling procedures for dealing with incorrect decisions opensup additional vulnerabilities for the system, as imposters might claim to have been falselyrejected and request handling as an exception.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.security and usability97careful thought and decisions about what types of riskmanagement strategies are acceptable. avoiding expedient responses and kneejerk choicesis critical, both for security and for privacy protection. this complexity,the multitude of contexts in which authentication systems could be deployed, and the ultimate need for someone to make policy decisions aboutsecurity and privacy requirements are why simple costbenefit analysesare unlikely to be effective in guiding the needed choices.a final factor that must be considered when deciding how to proceedñnamely, the recognition that authentication systems must be adequate to protect the resources they are guarding against the perceivedthreats, while at the same time remaining simple enough for administrators and others to use. this important point is often overlooked whendeveloping technologies. authentication systems will ultimately be usedby people, soñas described in the previous sectionñtheir ease of use andunderstandability will have a major impact on their effectiveness.systems and secondary useunderstanding only the underlying technologies is insufficient forappreciating the ramifications of authentication systems. it is importantto know how the systems will work in context to determine their securityand privacy implications. as discussed previously, some of the gravestprivacy violations come about because of the inappropriate linking ofdata within or across systems. this can happen because the same identifier is used in multiple systems, because efforts were made to correlatedata that has had the identification information removed, or for otherreasons. once data have been collected about an individual, dossiers(anonymous or identified) can be created. the creation of dossierscoupled with an authentication system may pose a privacy risk. avoiding this risk requires awareness of the broader context in which the authentication systems and the data associated with authentication eventswill be used.finding 4.2: the existence of dossiers magnifies the privacyrisks of authentication systems that come along later and retroactively link to or use dossiers. even a socalled deidentifieddossier constitutes a privacy risk, in that identities often can bereconstructed from deidentified data.finding 4.3: the use of a single or small number of identifiersacross multiple systems facilitates record linkage. accordingly,if a single identifier is relied on across multiple institutions, itsfraudulent or inappropriate use (and subsequent recovery acwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.98who goes there?tions) could have far greater ramifications than if it is used inonly a single system.recommendation 4.3: a guiding principle in the design or selection of authentication technologies should be to minimizethe linking of user information across systems unless the express purpose of the system is to provide such linkage.in addition to dossier creation through plannedfor uses of authentication systems, secondary use and unplannedfor uses increase the risk ofprivacy violations. unintended uses of a given technology or informationsystem can always have inadvertent side effects, and there are numerousexamples in the literature of a system meeting its design specificationsbut failing in the field because it was used in ways not anticipated by itsdesigners. in the realm of authentication technology and individual privacy, unplannedfor secondary uses can have grave consequences forusers. arguably, much identity theft is accomplished through secondaryuses of authentication or identification data. see box 4.1 for a more indepth discussion of identity theft.an authentication system could be designed and deployed in a secure, privacysensitive fashion. however, if some aspect of the systemwere to be used in ways not originally intended, both security and anyprivacy protection available could be at risk. the driverõs license is acanonical example of inappropriate secondary use. its primary functionis to identify those authorized to drive motor vehicles. it is quite unlikelythat the designers of the original processes by which driverõs licenses areissued anticipated they would be used as a security document needed toboard a commercial airliner or drink in bars.most information systems, including authentication systems, do notexplicitly guard against secondary uses, although occasionally there arecontractual relationships that limit secondary use (such as credit cardagreements). in some cases, the credential presented may be used foradditional verification purposes in contexts unrelated to the original purpose (such as with a driverõs license or when an ecommerce web sitegives permission for the cookies that users have allowed it to place ontheir hard drives to be used by other entities). in other instances, the datacollected prior to, during, or subsequent to authentication may be used inways that have little to do with the authentication step itself. a simpleexample would be when information used to register at a web site inorder to access content that is later used for marketing purposes by thatweb site. a more insidious form of unplannedfor usage would involve atechnology designed for a certain security context, user population, andso on that is later (intentionally or unintentionally) used in a new contextwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.security and usability99box 4.1identity theftidentity theft occurs when someone usurps a portion of another personõs personal identifying information in order to pose as that person. the informationusually includes some combination of name, address, social security number,motherõs maiden name, password, credit card number, date of birth, driverõs license number, and employer. with this information the òthiefó can open new accounts, order products, rent apartments, take out a mortgage, and/or borrow money, all under the identity of the first party. identity theft goes beyond the theft of acredit card to the misappropriation of a personõs very identity. it constitutes fraud.from the victimõs perspective, there is no easy way to develop an audit trail thatproves who actually made the transaction. was it the person who owns or isauthorized to use the account, or was it the person who appropriated the account?note that a significant part of the problem is that nonsecret data are used by manyentities for authentication purposes. an additional problem is the difficulty in revoking authenticators and identifiers when they are misused. in essence, the levelof confidence required, in the vocabulary introduced in chapter 1 of this report, isnot high enough. as chapter 7 describes, the choice of identifier and authenticatoris crucial for privacy protection and better security.reports of identity theft have increased over time. in march 2002, the general accounting office (gao), acknowledging that there are no comprehensivestatistics on identity theft, reviewed data from a number of sourcesñincluding consumer reporting agencies, the federal trade commission (ftc), the social security administration (ssa), and federal law enforcementñall of which indicated thatthe prevalence of identity theft was growing.1 victims of identity theft pay dearly;in 1997, the secret service estimated that victims lost an aggregate $745 million.but victims also face nonfinancial hardships, including criminal records, difficultyfinding a job, and inability to get mortgages or credit. sallie twentyman, a victim ofidentity theft, testified before the senate committee on the judiciary that the confusion and frustration that resulted felt like a òfinancial cancer.ó in ms. twentymanõs case, her renewal credit card was stolen before it reached her, and the thiefchanged òheró address and opened more accounts in òheró name.2a survey conducted by privacy rights clearinghouse and the california public interest research group found that the average victim of identity theft did notfind out that he or she was a victim until 14 months after the identity theft occurredand that it took the victim an average of 175 hours to resolve the problems thatoccurred as a result of the identity theft.3 in january 2002, the ftc announcedthat identity theft was the top consumer fraud complaint in 2001, accounting for 42percent of the complaints in the consumer sentinel database.1general accounting office. identity theft: prevalence and cost appear to be growing,gao02363. washington, d.c., government printing office, pp. 35, march 2002.2sallie twentyman (witness), òidentity theft: restoring your good name,ó testimony beforethe senate committee on the judiciary, march 20, 2002.3calpirg (sacramento, calif.) and privacy rights clearinghouse (san diego, calif.), nowhere to turn: victims speak out on identitytheft, may 2000.(continues)who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.100who goes there?box 4.1continuedalthough identity theft existed before the internet, there is concern that it willescalate even more in the digital world. the internet has given identity thieveseasier access to more sources of information. with esignatures and digital certificates, it will be imperative to ensure that the digital representation is authentic, orassociated with the correct individual, and that the transaction can be audited.there is a dual need: to determine who the consumer is and to ensure that thepersonal information of the consumer is protected.in 1998, congress passed the identity theft and assumption deterrence act,which legally recognized that the victims of identity theft were the individuals whoseidentities were stolen and not the financial institution that lost money; made it illegal to steal another personõs personal information (not necessarily documents)with the intent to commit a violation; and increased potential sentencing for violators. the act also required the ftc to establish a national clearinghouse for identity theft complaint data and to educate consumers about how to protect themselves.4 the ftc has held workshops on the subject. other government agencieshave also taken action regarding identity theft. for example, the treasury department sponsored an id theft summit in march 2000. finally, most states havepassed laws that criminalize identity theft.in response to the human and financial costs of identity theft, a variety ofpolicy responses have been proposed. most recognize the necessity of a multipronged effort involving the public and private sectors and employing legal andtechnological tools. the education of consumers is essential to ensure that theytake steps to minimize the possibility of identity theft and to alert them to signs ofpossible theft. public and private organizations can help prevent identity theft byreducing the amount of data that is exposed, limiting the release of information thatis given at the point of service, and enhancing the security of data that are collected. additionally, aggressive criminal investigations, prosecution, and punishmentare seen as critical.in the 107th congress much attention was focused on identity theft. a number of bills were introduced. in the house, these include the identity theft protection act of 2001 (h.r. 220), the social security number protection act of 2002(h.r. 4513), the id theft loophole closure act (h.r. 2077), and the protect victims of identity theft act of 2001 (h.r. 3368). in the senate, bills include therestore your identity act of 2001 (s. 1742), the social security number misuseprevention act of 2001 (s. 848), and the identity theft prevention act of 2001 (s.1399).without a determination as to whether the security and privacy safeguards still hold.abstractly, the difficulties that arise from secondary use are primarilydue to incorrect assumptions. secondary uses are implicitly relying onwhatever assurances, security models, and privacy protections the original designers and implementers were working with. these may not align4see the web site <http://www.consumer.gov/idtheft>.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.security and usability101well with the needs of the secondary user. in addition, the original system was probably designed with a particular threat model in mind. however, that threat model may not be appropriate for secondary uses. thisincongruity can make it difficult to respond to an attack on the primarysystem, since with widespread secondary use, the universe of motivations behind the attack is much larger. another problem is that the datacollected for primary purposes may not be the data that are needed bysecondary uses, or, they may not be of appropriate quality or reliability.in addition, secondary uses can facilitate information leakage from theoriginal system, which can cause both security and privacy problems. asnoted in the committeeõs first report,15 understanding and clearly articulating the goals of the system is crucial and may help mitigate any problems that might arise from secondary uses. nonetheless, the risks ofsecondary use are significant and must be considered.finding 4.4: current authentication technology is not generallydesigned to prevent secondary uses or mitigate their effects. infact, it often facilitates secondary use without the knowledge orconsent of the individual being authenticated.finding 4.5: secondary uses of authentication systems, that is,uses for which the systems were not originally intended, oftenlead to privacy and security problems. they can compromisethe underlying mission of the original system user by fosteringinappropriate usage models, creating security concerns for theissuer, and generating additional costs.recommendation 4.4: future authentication systems should bedesigned to make secondary uses difficult, because such usesoften undermine privacy, pose a security risk, create unplannedfor costs, and may generate public opposition to the issuer.concluding remarksas is evident from this and preceding chapters, neither authentication nor privacy is a simple issue. instead, the issues interact in complexways with the total system. this is seen clearly in the case of knowledgebased authentication, which relies on some prior history of contact and onreasonably private and unguessable knowledge that the authentic client15computer science and telecommunications board, national research council. idsñnot that easy: questions about nationwide identity systems. washington, d.c., national academy press, 2002.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.102who goes there?will nevertheless have access to. seen from this perspective, passwordbased authentication is popular because it demands so little upfrontñinitial passwords can easily be chosen or assigned without any need forhistory, special hardware, custom software on the clientõs computer (ifany), and so on. by the same token, the weaknesses of passwordsñguessability, the cost of recovering forgotten passwords, and so onñstemfrom many of the same roots.privacy has similar attributes. many organizations that rely on identitybased authorization do so not because they wish to, but because it iseasy: they can rely on an infrastructure that someone else has built. thisaccounts for the ubiquity of driverõs licenses as a de facto òofficialó identification and ageauthorization card: the departments of motor vehicles(dmvs) are paying the cost of issuing the cards; everyone else can ridefree. furthermore, it is precisely this overloading of function that givesrise to privacy violations: many different transactions can be linked backto the same individual. switching to betterñand privacyprotectingñtechnologies is thus not an easy task.finally, computer security in many organizations is not as strong as itcould be or needs to be.16 in the federal government, there are numerousefforts to document federal agency computer security plans and practicesand some that find glaring weaknesses in these plans and practices.17many reports over the years have described security issues, concerns, andresearch challenges.18 while security and privacy are often discussed asthough they were in opposition to one another, in many ways adequatesecurity is a prerequisite for privacy. if data are not well protected, theymay compromise the privacy of the individual to whom they pertain.however, achieving information security sometimes requires the disclosure of personal information (for example, by requiring authentication).at the same time, insufficient privacy protection may mean that personalinformation about others is easily discovered, calling into question thereliability of authentication systems that depend on such information.while this report urges that care be taken to avoid unnecessary authentication and identification (and therefore avoid unnecessary privacy risks),16see computer science and telecommunications board, national research council,cybersecurity today and tomorrow: pay now or pay later, washington, d.c., national academy press, 2002.17see representative stephen hornõs report card on federal agency computer securityefforts as one measure of the state of preparedness in this area; available online at <http://www.house.gov/reform/gmit/hearings/2000hearings/000911computersecurity/000911reportcard.htm>. another measure is the office of management and budgetõsscorecards, available at <http://www.whitehouse.gov/omb/budintegration/scorecards/agencyscorecards.html>.18see cstbõs reports on security at the web site <http://cstb.org/topicsecurity/>.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.security and usability103the interplay between achieving privacy protection and security in thedevelopment of information systems should be carefully considered. privacy and security, while often in tension, are complementary as well.security can protect private data, and maintaining privacy can aid inavoiding security breaches. usability is a key component of this mix,since hardtounderstand or hardtouse systems will be prone to errorsand may drive an individual to work around either the security mechanisms or the mechanisms that would protect privacy. lessons learned intrying to create secure, usable systems therefore apply when seeking todevelop systems that protect privacy.finding 4.6: privacy protection, like security, is very poor inmany systems, and there are inadequate incentives for systemoperators and vendors to improve the quality of both.finding 4.7: effective privacy protection is unlikely to emergevoluntarily unless significant incentives to respect privacyemerge to counterbalance the existing incentives to compromise privacy. the experience to date suggests that market forcesalone are unlikely to sufficiently motivate effective privacy protection.recommendation 4.5: system designers, developers, and vendors should improve the usability and manageability of authentication mechanisms, as well as their intrinsic security andprivacy characteristics.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.1045authentication technologiesthis chapter describes the basic technologies used as building blocksfor authentication systems, especially those employed in computerand network environments. first, it describes technological choicesthat determine a dimension of authentication separate from the differentkinds of authentication described in chapter 2. then, technologicalinstantiations of the three main authentication mechanisms (somethingyou know, have, are) are described. multifactor authentication is considered, and decentralized and centralized systems are compared. finally,security and cost considerations for individual authentication technologies are discussed. throughout, this chapter also touches on the privacyimplications of specific technologies in the context of authentication systems, as appropriate.technological flavors of authenticationòindividual authenticationó is defined in chapter 1 as the process ofestablishing an understood level of confidence that an identifier refers toa specific individual. in an information systems context, it often is usefulto distinguish among several types or modes of authentication, both forindividuals (often referred to as òusersó) and for devices (such as computers). this is a dimension distinct from the individual/attribute/identityauthentication types discussed in chapter 2. in the security literature,these modes are often referred to as oneway as opposed to twowaywho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication technologies105authentication, initial versus continuous authentication, and data originversus peerentity authentication.much individual authentication in an information system takes placein a client/server context, in which the individual user is the client (aòpresenteró in the terminology introduced in chapter 2) and some computer is a form of server (the òverifieró). a user is required to authenticate his or her identity to a computer, usually as a prerequisite for gainingaccess to resources (access control or authorization). this is typically anexplicit oneway authentication process; that is, the user authenticateshimself or herself to the computer. if the user is authenticating to acomputer directly (for example, when sitting at a desktop or laptop computer), there is an implicit twoway authentication; the user sees the computer with which he or she is interacting and presumably knows that it isthe one he or she wishes to use.1however, if the user is authenticating to a computer accessed via acommunication network, there is often no way to verify that the computer at the other end of the communication path is the one that the useris trying to contact. the user typically relies on the communication infrastructure operating properly and thus connecting him or her to the intended computer. this assumption may be violated by any of a numberof attacks against the communication path, starting with the computerthat the user is employing locally. this lack of explicit, secure, twowayauthentication can subvert many types of individual authenticationmechanisms. if a presenter provides an identifier and authenticator to thewrong verifier, both security and privacy are adversely affected. thus,twoway authentication is preferred so that a presenter can verify theidentity of the verifier to which a secret may be disclosed.initial authentication takes place when an individual first establishesa connection of some sort to a system. this may be a direct, very localconnection, such as logging in to a desktop or laptop computer, or it maybe a remote connection to a computer via a communication network. ineither case, there is an assumption that future communication, for someperiod of time, is taking place between the two parties who were initiallyauthenticated. for a direct connection, as defined here, this assumptionusually relies on physical and procedural security measures; there is anassumption that the user will log out when leaving the computer unattended and in a place where others might access it. this is a form ofimplicit, continuous authentication. this assumption may not always be1looks can be deceiving, and even visual inspection of a proximate device is not alwayssufficient to authenticate it.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.106who goes there?valid, and sometimes users are required to reauthenticate themselves explicitly to the computer periodically, to verify that they are still present.this periodic reauthentication requirement is an explicit attempt at continuous authentication, although it is not really continuous. periodicreauthentication is also burdensome for the user and thus not commonlyemployed.when the connection between the user and a computer is through anetwork, there are many more opportunities for the connection to beòhijackedóñthat is, for an attacker to inject traffic into the connection orto seize the connection from the legitimate user. in remoteaccess contexts, it is appropriate to employ explicit measures to ensure continuousauthentication. typically, this continuity is effected using cryptographicmeans, based on a secret (a cryptographic key) shared between a localcomputer employed by the user and a remote computer being accessedby the user, for the life of the connection. in this latter context, the technical term for the security service being provided is òdata origin authentication.ó continuous authentication is generally a result of a transition frominitial, individual authentication to data origin authentication. it is thesource (origin) of the data sent between two systemsñfor example, between a userõs desktop and a serverñthat is being authenticated ratherthan the user per se. a further technical distinction is sometimes applied.if the authentication mechanism ensures the timeliness of the communication and thus provides protection against attacks that replay old messages, the service is referred to as òpeerentity authentication.óindividual authentication increasingly takes place in the context ofinformation systems, and thus all of the flavors of authentication described above are relevant to this discussion of individual authenticationtechnologies.basic types of authentication mechanismsby the mid1970s, three basic classes of authentication technologiesfor use with information systems had been identified.2 they are colloquially characterized as òsomething you know, something you have, andsomething you areó and were discussed abstractly in chapter 2. thissection focuses on specific technological examples of each of these basicclasses. in the first class are authentication technologies based on whatan individual can memorize (know). passwords and personal identifica2d.e. raphael and j.r. young. automated personal identification. stanford research institute international, 1974; national bureau of standards. òevaluation techniques for humanidentification.ó fipspub48, april 1977.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication technologies107tion numbers (pins) are the canonical examples of such technology. inthe òsomething you haveó class are physical objects that are (assumed tobe) hard to forge or to alter, such as magneticstripe cards, smart cards,securid cards, and so on. the object is issued to an identified individualand retained by the individual, so that possession of the object serves toidentify the individual. in the last class are biometric authentication technologies, which measure physical and behavioral characteristics of anindividual. each of these classes of authentication technologies has advantages and limitations with regard to security, usability, and cost.something you knowsimple, passwordbased authentication is the most common form ofinitial, oneway authentication used in information systems. a user remembers a short string of characters (typically six to eight) and presentsthe character string to a system for verification when requested. thestring of characters is reused many times in authenticating to the samesystem; hence the passwords are usually referred to as òstatic.ó this sortof system is susceptible to many forms of attack. because users havetrouble choosing and remembering values with a significant number ofòrandomó bits, passwords generally are vulnerable to guessing attacks.unless passwords are protected (usually this means encrypted) for transmission over communication paths, they are subject to interception andsubsequent use by a wiretapper. the lack of twoway authenticationmeans that a user can be tricked into revealing a password if he or sheconnects to an attacker instead of to the desired system.passwordbased authentication is cheap to implement; it may notrequire any explicit software purchases. it is easy for users and developers to understand, so training costs are low. but, on a lifecycle basis,passwords are expensive for an organization to administer, largely because of the costs of helpdesk support for users who forget passwords.users find passwords difficult to manage as they deal with a growingnumber of systems that require them. this leads to password reuse (thatis, using the same password for multiple systems) and insecure storage ofpasswords (for example, in unencrypted files on computers). both ofthese practices undermine the security of passwordbased systems. in theformer case, if one system is compromised and the passwords used in itbecome known, other systems in which the user employs the same passwords could be compromised. that is, compromise of the userõs desktopor laptop or personal digital assistant (pda) compromises all of the passwords employed by that user to access many other systems.many of the common recommendations for improving the security ofpasswords without changing the fundamental mechanisms involved tradewho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.108who goes there?one form of insecurity for another. for example, if users are encouragedto choose passwords that are hard to guess, they will probably have torecord the passwords somewhere (because these passwords are not easilyremembered), making them vulnerable to attacks against the stored passwords. users are encouraged to change passwords periodically, whichalso increases the likelihood of recording the passwords in vulnerablelocations.passwords are easily shared. a user can tell others his or her password; in many situations this is common and even encouraged as a signof trust.3 passwords can also be shared inadvertently, as they are oftenwritten down in semipublic places.4 this is not always a serious problemif the threat model focuses primarily on outsiders, but insiders representthreats in many contexts, and users often do not consider this type ofthreat when sharing passwords or recording them.in principle, passwords can offer a great deal of anonymity. in practice, however, most people cannot remember many different passwords,and they tend to reuse the same passwords for different purposes. moreover, if allowed to select an identifier as well as a password, the user maychoose to use the same values for multiple systems. this makes it potentially easy to link multiple accounts to the same user across system boundaries, even though the base technology does not necessarily impose suchlinkages. additionally, the recovery mechanisms for lost passwords generally require oneõs motherõs maiden name, an email address, or someother form of personal information. thus, the infrastructure for password maintenance often requires sharing other forms of information thatis personal and so presumed to be less likely to be forgotten (see box 5.1).this, too, potentially undermines privacy.finding 5.1: static passwords are the most commonly used formof user authentication, but they are also the source of manysystem security weaknesses, especially because they are oftenused inappropriately.recommendation 5.1: users should be educated with respect tothe weaknesses of static passwords. system designers mustconsider tradeoffs between usability and security when deploying authentication systems that rely on static passwords toensure that the protections provided are commensurate with3d. weirich and m.a. sasse. òpersuasive password security.ó proceedings of chi 2001conference on human factors in computing systems, seattle, wash., april 2001.4j. nielsen, òsecurity and human factors,ó useit.comõs alertbox, november 26, 2000.accessed on march 26, 2002, at <http://www.useit.com/alertbox/20001126.html>.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication technologies109the risk and harm from a potential compromise of such an authentication solution. great care should be taken in the designof systems that rely on static passwords.more secure authentication technologies can be based on passwordtechnology at some levels. for example, schemes such as encrypted keyexchange (eke)5 and kerberos (a network authentication protocol)6 alsobox 5.1knowledgebased authenticationa form of authentication with similarities to both passwords and challenge/response is knowledgebased authentication. in this case, users present dataelements that the verifier can approve on the basis of previous transactions andregistration activity. for example, the internal revenue service (irs) and a number of state revenue agencies have implemented an electronic signature and authentication technique that relies on a taxpayerõs presenting data from the previousyearõs transaction. in addition to checking traditional identifying information presented on a tax return, such as the tax identification number and date of birth,these tax authorities have the taxpayer sign a tax return by entering data from theprevious yearõs tax return. for instance, at the federal level, the irs electronicauthentication program for tax year 2001 allowed the taxpayer to sign his or herreturn with a selfselected pin. the irs verifies the identity of the taxpayer usingthe selfselected pin when the taxpayer provides the adjusted gross income (agi)for tax year 2000. in subsequent years, the taxpayer has the choice of using thesame or a different pin, but must update the agi data for each previous tax year.by using the data from the previous yearõs return, the irs is able to authenticatethe transaction on the basis of knowledge that presumably only the irs, the taxpayer, and whoever might have prepared the return for the taxpayer possesses.this type of authentication relies heavily on the data in question being kept secure.(chapter 6 in this report provides more information on electronic authentication inthe irs efile program and the distinction between electronic signatures and authentication.)similarly, the popular online payment site paypal.com deposits a smallamount into a userõs bank account and asks the user for the amount deposited. ineffect, socalled knowledgebased authentication is a form of password. the crucial difference is that it is communicated to the user via some outofhand mechanism to which an imposter is presumed not to have access. additionally, it isassumed that the legitimate user will look up the authenticator rather than know it;this is no different from a conventional password that is written down. the crucialdistinction is that knowledgebased technology generally relies on a prior history ofcontact between the client and the verifier.5s. bellovin and m. merritt. òencrypted key exchange: passwordbased protocols secureagainst dictionary attacks.ó proceedings of the ieee symposium on security and privacy. oakwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.110who goes there?make use of static passwords. these schemes employ sophisticated cryptographic mechanisms and protocols to counter many of the attacks thatare effective against static passwords. they typically provide oneway,initial authentication, which may transition to twoway, dataorigin andpeerentity authentication for subsequent communication. these are not,per se, passwordbased authentication technologies. the section òmultifactor authenticationó discusses in more detail authentication protocolsof this sort.something you havethe òsomething you haveó class of authentication technologies isbased on the possession of some form of physical token that is presumedto be hard to forge or alter. many forms of physical tokens are used forauthentication and for authorization outside the context of informationsystems, and they exhibit varying degrees of resistance to forgery andalteration. for example, many driverõs licenses and credit cards make useof holograms as a deterrent to forgery, relying on visual verification by ahuman being when they are presented. yet credit cards are now usedextensively for purchases by mail or telephone or over the web. in thesecontexts, there is no visual verification of the credential, so the antitampersecurity mechanisms are ineffective. in information systems, the securityof hardware tokens, to first order, is usually based on the ability of thesedevices to store, and maybe make direct use of, one or more secret values.each of these secrets can be much larger and more random than typicalpasswords, so physical tokens address some of the vulnerabilities, such asguessability, cited above for passwords. nonetheless, the simplest formsof tokens share some of the same vulnerabilities as passwordsñthat is,they both deal with static, secret values.a magneticstripe card is an example of a simple physical authentication token. tokens of this sort offer the primary benefit of storing largersecrets, but they offer almost no protection if the token is lost or stolen,because readers are readily available and can extract all data (secrets)from the magnetic stripe. after a secret is read from the card (even in thecontext of a legitimate authentication process), the secret is vulnerable inthe same ways that a password is (for example, it can be intercepted iftransmitted via an insecure communication channel or compromisedland, calif., may 1992, pp. 7284. available online at <http://citeseer.nj.nec.com/bellovin92encrypted.html>.6more information on kerberos is available online at <http://web.mit.edu/kerberos/www/>.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication technologies111while held in storage in a computer7). when a magneticstripe card isswiped, all the data can be read from the card and become accessible tomalicious software in the system. this possible misuse argues againststoring the secrets used to authenticate multiple, distinct identities on onecard. (the storage space on cards of this sort also is very limited.) conversely, requiring a user to carry multiple physical cards to maintainmultiple secrets is inconvenient for the user and adds to overall costs.although magneticstripe cards and their readers are not very expensive,computer systems (other than in retail sales contexts) generally do notoffer readers as standard equipment, so there are cost barriers to the useof such cards for individual authentication in home or corporate environments. this is a good example of tradeoffs among competing goals ofsecurity, user convenience, and cost. sometimes applications for magnetic cards focus on authorization rather than authentication, as in thecase of transit fare cards (see box 5.2).box 5.2new york city transit metrocardthe new york metropolitan transportation authority uses a magneticstripefare card called a metrocard. the metrocard is accepted on all new york citybuses and subway trains. a magnetic stripe on the card holds the stored valueinformation; however, the same information is recorded on a central computersystem. thus, although the card appears to be an anonymousbearer instrument,transactions charged to a particular card are logged. while this feature is primarilyan antifraud device, it has in fact been used by police agencies to verify suspectsõmovements. furthermore, since cards may be purchased by credit card, the potential exists to track an individual even without possession of the individualõs physical card.the privacy implications of this system have been known for some time. a1997 study1 on multipurpose fare media not only discussed the privacy issue butalso found that it was indeed a major concern of consumers. fare cards have amodest amount of storage and no computational ability. because they are copyable, there is a need for consultation with the central site.1transit cooperative research program research results digest, june 1997, availableonline at < http://gulliver.trb.org/publications/tcrp/tcrprrd16.pdf>.7in a welldesigned system, a secret read from a card would not be retained in storage onthe system for very long, and the vulnerability here would be much less than when passwords are stored in a file.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.112who goes there?a notsoobvious form of òsomething you haveó authentication is aweb cookie. the need for cookies arises because the protocol used forweb access, hypertext transfer protocol (http), is òstatelessó; http doesnot provide a reliable way for a server to know that a particular request iscoming from the same user as a previous request.8 pure stateless operation would make it difficult to provide functions such as browsingthrough an online catalog and collecting a òshopping cartó full of itemsthat the user has decided to purchase. also, if the user is browsingthrough information that needs authentication (such as information aboutthe userõs bank account), it would be inconvenient if the user had to typea name and password each time a different page was viewed.the solution to this problem, designed by netscape, is called aòcookie.ó a cookie is data given by a web server to the client to maintainstate. each time a client makes a request to a server, any cookies providedto the client by that server are sent to the server along with the request.thus, for example, the identifier and password provided by a user forinitial authentication may be transformed into a cookie to facilitate continuous authentication of the http session. sometimes a cookie is abigger secret than an individual could remember. it may be like a secretstored in a token; in this case, the token is the userõs computer, with all theattendant security problems that arise from storing secrets in a file on acomputer and the problems that arise if the secret is transmitted across acommunication network without encryption. sometimes a cookie is usedto track an individualõs authorization in the context of an http session.in such cases, the cookie itself may be a cryptographically protected valuein order to prevent a user from tampering with it and thus fooling theweb server.the use of cookies is often criticized as a mechanism that violatesprivacy, but it depends on how they are used. if they are used solely toeffect session continuity, overcoming the limitations of http, and if theserver does not maintain information about the user, they can be a privacyneutral or even privacyenhancing technology. but cookies aresometimes used to track a userõs movements through multiple sites. manysites that do not require authentication will set a cookie on the first visit.this lets the site track return visits by presumably the same user, eventhough the site operators do not know who that person is in a largercontext. often, this technique is employed by thirdparty advertising8the statelessness of the http protocol implies that each request sent for a page iscompletely independent of any requests that came before. thus preserving informationfrom one click to the next requires additional technology (d. kristol and l. montulli,òhttp state management mechanism,ó request for comments (rfc) 2965).who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication technologies113sites; this use of cookies permits tracking users (and their interests) acrossmultiple web sites. this is a form of covert identification (see chapter 1);the userõs identity as a web site visitor and a dossier of his or her activityare compiled and retained, and an identifier in the form of the cookie isassigned. it is not necessary to use cookies to track user activity, however.even if cookies were banned, it would still be possible to track a userõsweb history through other mechanisms such as log files, browser caches,and browser history files.9smart cards are creditcardsize tokens that contain memory and, often, a processor. smart cards that act only as memory devices are essentially as vulnerable as magneticstripe cards in terms of extracting thesecrets stored on the cards, because readers are widely available, andmalicious software can extract stored values from the card. the costs forthese cards is somewhat higher than those for magneticstripe cards, andsmart card readers are more expensive as well, but smart storage cardsoffer more data storage than magneticstripe cards do, and they resistwear better.10 universal serial bus (usb) storage tokens are anotherhardware storage token format. they have a potential advantage in thatmany pcs offer usb interfaces, thus eliminating reader cost and availability as barriers to deployment.tokens that act only as storage devices may be used to provide initial,oneway authentication analogous to static passwords. however, because these devices can hold larger, òmore randomó secret values (that is,an arbitrary collection of bits as opposed to something meaningful ormnemonic to a person), they can provide somewhat better security. increasingly, tokens of this sort are being used to bootstrap continuousdataorigin authentication schemes that are implemented using the processing capabilities of a computer to which the token is (locally) connected. (recall that the authentication taking place here is authenticatinga local computer to a remote computer, not a person to a remote computer.) these schemes are often challenge/response protocols, as described below. since these protocols are executed in the computer, not the9socalled òweb bugsó are another mechanism used to surreptitiously observe anindividualõs actions online. they are objects, usually onepixelsquare graphic images, embedded within the html source on a web site that cause part of the displayed web page tobe retrieved from another web site, thereby transmitting information about the requester toa third party. web bugs are used on a surprisingly large number of sites, primarily forstatistical purposes and to gauge the effectiveness of advertising. the information transmitted to the òbuggeró includes an ip address and the last site visited and may be linked tocookies to collect individual web surfing profiles. web bugs are also embedded in emailmessages by spammers, who use them to validate live addresses.10the magnetic stripe can abrade, and the data records on it may be degraded by exposure to magnetic fields.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.114who goes there?token, they also can make use of secrets stored in the computer rather thanon separate hardware storage tokens. the term òsoftware tokenó has beencoined to refer to the use of secrets stored on a computer and employed inconjunction with an authentication protocol. software tokens are not assecure as hardware storage tokens, since the secrets used by the softwareare held in files in a computer on a longterm basis. at best, these secretstypically are protected by a password. thus, any attack against the computer that compromises these files allows an attacker to retrieve the storedsecrets through passwordguessing attacks. in contrast, a welldesignedauthentication technology that uses a hardware storage token would readthe secret(s) stored on the token, use them, then erase them from the computer memory as quickly as possible. these actions present a smaller window of opportunity for the compromise of the secret(s), making the use ofhardware storage tokens potentially more secure. the main attraction ofsoftware tokens is the low cost; the software may be free or inexpensive,and there is no need to buy token readers.some of the earliest hardware authentication tokens11 and some ofthe most popular ones employed today, such as securid (see box 5.3), donot interface directly with an authentication system. instead, the user isrequired to act as an interface, relaying information between an information system and the token. tokens of this sort typically implement a typeof authentication known as algorithmic challenge/response, or just challenge/response. challenge/response schemes operate much like humanenacted authentication scenarios. most movie goers would recognize thewords òhalt! who goes there?ó as the beginning of a challenge/responseexchange between a guard and an individual approaching a guardedarea. the password in such a scenario would usually change daily, consistent with human limitations for adapting to new passwords. in anonline authentication technology, the challenge can change every time,making the corresponding response unique in order to thwart eavesdropping attacks.challenge/response schemes are a generic technique to prove knowledge of a secret, sometimes even without disclosing it to the party performing the authentication check.12 challenge/response schemes areanalogous to intruder: friend or foe (iff) systems originally developed11j. herman, s. kent, and p. sevcik. òpersonal authentication system for access controlto the defense data network.ó proceedings of the 15th annual ieee electronics and aerospacesystems conference (eascon), september 1982.12research into a class of algorithms known as òzero knowledge algorithmsó is movingwork forward in this area. as a starting point for what this work involves, see s.goldwasser, s. micali, and c. rackoff, òthe knowledge complexity of interactive proofsystems,ó in proceedings of the seventeenth annual acm symposium on theory of computing,who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication technologies115by the military for automated authentication of aircraft by ground personnel operating antiaircraft batteries. although challenge/response systems for information systems were originally implemented using hardware tokens, software tokens now are employed frequently for thispurpose and there are manual analogs. imagine a large sheet of paperwith many different numbered passwords. the verifier sends the number; the presenter sends back the corresponding password (which illustrates why these systems are sometimes called onetime passwordschemes). in practice, the verifier sends some string of characters to thepresenter (user); the presenter computes a response value based on thatstring and on a secret known to the user. this response value is checkedby the verifier and serves as the òpasswordó for only one transaction orsession. as typically employed in a usertosystem authentication exchange, this is an example of a oneway initial authentication scheme, butbox 5.3securidrsa security (formerly security dynamics) markets a challenge/responsecard. this card, trademarked securid, contains a builtin clock and a liquid crystaldisplay. rather than requiring the user to obtain a challenge from a host computer,the challenge is implicitly the current time. to gain access, the user merely entersa user id and then enters the current number displayed on the card. the numberdisplayed changes periodically, usually every 30 seconds to 1 minute dependingon the card configuration.the securid has become quite popular in some contexts. it is relatively easy(though not especially convenient) to use, it requires no special hardware, and it iseasily integrated with existing, passwordstyle authentication software. some versions of the card require the user to enter a pin into the card (where it is combinedwith the cardresident key), making this into a twofactor authentication system. inthis case, even if an adversary acquires a card, the ability to impersonate theaffected user depends on guessing the pin. if the usual practice of monitoringfailed login attempts is being followed, an adversary guessing pins for use with acaptured card will probably be detected prior to guessing the pin.new york, acm press; o. goldreich and h. krawczyk, 1985, òon the composition ofzeroknowledge proof systems,ó proceedings of 17th international colloquium on automata,languages and programming (icalp), coventry, u.k, july 1620, 1990; u. fiege, a. fiat, anda. shamir, òzero knowledge proofs of identity,ó proceedings of the nineteenth annual acmconference on theory of computing, new york, acm press, 1987; and j. j. quisquater and l.guillou, òhow to explain zeroknowledge protocols to your children,ó advances incryptology (crypto õ89), springerverlag, pp. 628631, 1990.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.116who goes there?it is much more secure, relative to a variety of threats, than are staticpasswords.there are many variations on this scheme: often a shared secretknown to both the presenter and the verifier can be used to generate andverify the response. in schemes based on public key cryptosystems, apresenter may employ a private key to generate a response that the verifier checks using the corresponding public key associated with the presenter. these operations can be carried out using software (such ass/key13), or the user may employ a hardware token to perform the calculation. (see box 5.4, on how such a technology might be used.)hardware tokens that contain processors (for example, cryptographicprocessor smart cards, pc cards, some proximity cards,14 or usb processor tokens) are qualitatively different from all of the previous token types.they can be much more secure than hardware storage tokens or softwaretokens, because they can maintain secret values within the card and neverexport them (i.e., transmit secrets off the card). a smart card typicallyperforms cryptographic operations in the card, using stored secret valuesto execute parts of an authentication protocol, such as a challenge/response protocol, on behalf of the cardholder. with a token capable ofcryptographic operations, the secrets contained in the token are not exposed as a result of inserting the token into a reader, and no secrets arereleased to the computers to which the readers are attached or transmitted across a communication path. typically, a user must enter a pin toenable a smart card token, and the entry of a wrong pin value multipletimes in succession (logically) disables the token. this provides someprotection in case the card is lost or stolen. nonetheless, capable adver13see l. lamport, òpassword authentication with insecure communication,ó communications of the acm 24(11)(november 1981):770772, and phil karnõs reference implementation of s/key described in neil haller, òthe s/key onetime password system,ó rfc1760, february 1995; available online at <http://www.faqs.org/rfcs/rfc1760.html>.14a proximity card contains information stored electronically within the card. the information is transmitted via radio over a short distance (typically less than 10 centimeters)after the card is queried. users like these cards because they require very few steps toperform authentication and therefore are quite fast. these cards are vulnerable to physicalattacks that extract data from them. they also may be susceptible to interception of thetransmitted data (over a short distance) and to spoofing attacks, in which the attackertransmits the same sort of query as a legitimate verifier and records the response. fordisabled users, proximity cards may be attractive alternatives to magneticstripe cards sincecard readers for the former have instructions that are typically visual, they are not alwayslocated in positions accessible to those in wheelchairs, and they are hard to insert for thosewhose manual dexterity is poor. for more, see john gill and j.n. slater, ònightmare onsmart street,ó tiresias: international information on visual disability, 2002 (updated), available online at <http://www.tiresias.org/reports/tidecon2.htm>.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication technologies117box 5.4tokenbased authentication system: a scenariolaura visits bestcare hospital for a chronic illness that requires many laboratory tests and radiological examinations. dr. jones, her physician, inquires whether laura has an internetconnected computer at her home that she can use toconnect to bestcareõs webbased patient information portal. this portal can beused to check results, communicate with various health care providers, requestand check clinic scheduling information, view physicianrecommended literature,and join healthcarerelated chat groups. since laura has been having a difficulttime scheduling her visits by phone and is finding it hard to ask informed questionsof her physician during her visits, she eagerly accepts the offer. dr. jones thenrecords lauraõs acceptance in her computerized record and tells her she will haveto click on some healthrelated popup advertisements to receive free access.bestcareõs webbased patient information center allows patients to checktheir records and communicate with doctors at their convenience, without havingto schedule an appointment. such a utility saves time for both doctor and patientand helps bestcare honor its commitment to providing continuous care to patientswith chronic illnesses. statistics collected from patientsõ visits to the site help dr.jones with her research in chronic disease management, and bestcare receivesadditional revenue from healthcare companies in exchange for posting their advertisements on the site.because bestcare has decided to require additional security measures protecting patient records from unauthorized access through its web site, laura isgiven a token card in addition to a user id and a password. at home, laura asksher son to help; he configures the web browser, enables the token, and changesthe password from the one given to her at the hospital. from now on, laura willenter her new password in her token card, which then displays a different password that she enters with her user id to access the portal. she is required to readand accept the privacy notice, which she only skims in her rush to get to the portal.when the portal displays a message that her test results are pending, her sonñwho is still looking over her shoulderñis curious about the tests. not wanting toupset her son with information about her unconfirmed diagnosis, laura decidesthat she will keep the token in a secure place so that only she can access the portalin the future.dr. jones carries a similar token card to access clinical information about herpatients. the system only requires her to use the token when she is not accessingthe information from a workstation directly connected to the bestcare network.this reduces the humaneffort cost for the care providers but means that the bestcare network must maintain good security with standard firewalls and intrusiondetection.several parties have been involved in setting up and maintaining the patientinformation portal. bestcare has outsourced its information technology to helpfulit, inc., and has set up its internet portal business with its medical records vendor,goodmedrec co., which hosts its web servers and databases with betterasp,inc. bestcare has also contracted with several pharmaceutical, nursing home, and(continues)who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.118who goes there?other health care companies to advertise their services on its site. it arranged thecontracts through an internet advertising management company named fineclick.in this scenario, several sets of information are collected. helpfulit managesuser id/token/password information in an authentication database. goodmedrecmanages user id and user demographic information and clinical information in adatabase. every time laura signs on, betterasp maintains user id/ip addressmappings in its audit systems for performance and billing purposes. on behalf ofbestcare, goodmedrec provides fineclick with an abbreviated set of òclinicalcodes of interestó in order to conduct customized marketing, and fineclick maintains clinical codes and ip address information for billing.authentication/authorization/identificationlaura is authorized to view only her own records, not those of other patients.twofactor authentication, using something she knows (the password) and something she has (the token card), prevents laura from accessing information that shedoes not have permission to see. the system records lauraõs user id and dateand time of access every time she looks at her records online. such identificationprotects bestcare from liability. laura might also need to identify herself if shewants to ask a doctor a question that pertains to her own medical record. she isnot required to identify herself when she searches for general health information,uses the chat groups, or explores advertisements.because it is timeconsuming for bestcare staff to continually update theweb portalõs record of which patient is assigned to which physician, all physiciansusing the system have been given access to all patient files. again, it is importantfor legal reasons to know who has looked at a patientõs records, so when dr. jonesaccesses lauraõs record using the system, she must be identified and logged asthe person who has seen the record.twofactor authentication also protects doctor and patient from unauthorizedaccess by outsiders. even if lauraõs token card is stolen, her medical record issafe as long as the thief does not know lauraõs password. unfortunately, laurahas written down her password because she is afraid she might forget it; in doingso, she has made her information more vulnerable.dr. jones is similarly protected from token card theft. however, dr. jones hasshared her user id and password with her assistant, who is not otherwise authorized to view confidential patient records. although dr. jones has no reason todoubt her assistantõs trustworthiness, such sharing exposes the medical records ofall bestcareõs patients to unauthorized access and tampering, whether maliciousor purely accidental.there is a downside to using tokens as extra security measures to protectagainst unauthorized access to medical records; authorized users might be deniedaccess at inconvenient times because of a lost or malfunctioning token. in thisscenario, if dr. jones is away from the office and her token card is lost or broken,she cannot view lauraõs questions as soon as they are asked and lauraõs healthmight be compromised. however, if it becomes common practice for dr. jonesand her colleagues to circumvent such security measures by calling the office andasking another physician or an administrative assistant to access the record,box 5.4continuedwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication technologies119lauraõs privacy may be compromised. although bestcareõs privacy policies mayexplicitly address this issue, laura may not understand its implications.breachesthe vice president of helpfulit, inc., is told that patients are having problemsaccessing its systems from home, and he suspects these problems are associatedwith fineclickõs advertisementdownload process. an audit log analysis is initiated, and a staff programmer correlates user ids, ip addresses, clinical codes, andadvertisement clicks from different sources in a spreadsheet. a problem is found inspecific codes and advertisements, and it is fixed. the spreadsheet is posted tothe help desk as a solution for a possible problem.now lauraõs clinical data are being used, shared, and cited to resolve anoperational problem. the staff member at helpfulit, inc., who resolved the problem does not have medical records access for patients but is now able to combinethe user id, ip address, and types of advertisements (each collected from differentinteractions) to determine which patient suffers from which illness.another major problem relates to the safekeeping of the databases. in light ofthe increasing number of identified vulnerabilities in systems such as betteraspõsweb servers, the increasing sophistication of attack methods, and the inertia ofsystem administrators when it comes to patching their systems, it is easier thanever to compromise many information systems. sometimes, inadvertent mistakeslead to the publication of private information over the internet without adequateauthentication and encryption. misuse and accidental errors by insiders contributeto serious privacy problems. although such potential breaches of security and compromises to privacyare sobering to consider, lauraõs main concern is that her family remain unawareof her condition until she knows more facts and can share them at the time of herchoosing. back home, lauraõs son uses the computer after she is finished exploring bestcareõs portal. he clicks the òbackó button on the open browser windowand, curious, clicks several more times, eventually reaching a disease management site that laura had been browsing. his level of concern rising, he checks thebrowserõs history file to identify the last sites visited. on the basis of this indirectand incomplete information, he concludes that his mother is seriously ill and, in astate of great emotion, confronts her. laura is forced to explain her disease to herson before she is ready to do so.privacy intrusivenesstoken cards are reasonable authentication methods from a security perspective,and this scenario describes how they might be used in a particular context. potentialprivacy violations, however, are a result of faults in overall system protection. thescenario demonstrates that the choice of information delivery technology, the decisionto allow access by ostensibly uninterested parties for maintenance purposes, andhuman factors such as password sharing for the sake of convenience, may open upunforeseen vulnerabilities. many of these same privacy concerns could, however, arisewith authentication technologies not based on token cards. how systems are implemented and deployed, and what policies are put in place to govern their usage, all bearon what the privacy implications of authentication systems will be.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.120who goes there?saries with access to sophisticated technology (of the sort that might commonly be found in a college physics lab) can extract secret values storedon smart cards.15 processor tokens are noticeably more expensive thanmagneticstripe cards or other storage tokens. the cost of readers varies,depending on which token technology is employed.often a token of this sort is used only for initial one or twowayauthentication,16 based on the execution of the cryptographic operationsof a challenge/response protocol within the token. this provides a securefoundation for twoway, continuous authentication schemes based onthis initial exchange. responsibility for continuous authentication oftenis borne by the computer to which the smart card is attached, becausesmart card interfaces are too slow to deal with all the data transmitted orreceived on a connection. however, the continuous authenticationbootstrapped from a smart card offers the opportunity for better securityoverall.hardware tokens have the desirable security property of not beingable to be readily replicated by users, although the difficulty of unauthorized replication varies widely, as noted above. in principle, if one userchooses to share his or her token with another, the first user relinquishesthe ability to authenticate himself or herself as long as the hardware tokenis loaned. this guarantee is diminished with some forms of hardwarestorage tokens and with software tokens, since one can copy the files thatpersonalize them. the extent to which tokens preclude sharing represents a significant improvement over static passwords. a necessary corollary to this observation is that the loss of any token results in a replacement cost, and the replacement process is more difficult than for apassword. a help desk cannot remotely replace a token in the way that itcan reset a password. hardware cryptographic tokens and software tokens entail costs for integration into applications because they executeauthentication protocols rather than act just as repositories for secrets.something you arethe final class of technologiesñòsomething you areóñ refers to theuse of biometrics to authenticate individuals. biometric authentication,which has received much attention in the media of late, is the automatic15see, for example, r. anderson and m. kuhn, òtamper resistanceña cautionarynote,ó the second usenix workshop on electronic commerce proceedings, oakland, calif.,november 1821, 1996.16typically, twoway authentication relies on the use of public key cryptography andcertificates.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication technologies121identification or identity verification of human individuals on the basis ofbehavioral and physiological characteristics.17biometric authentication is fundamentally different from the othertwo classes because it does not rely on secrets. biometrics themselves arenot secrets; people commonly leave fingerprints on everything they touch.our voice, handwriting, and facial image can be captured without ourknowledge. rather, biometric authentication relies on registering andlater matching what are believed to be distinguishing physical or behavioral characteristics of individuals. there are many different examples ofbiometric authentication: it can be based on fingerprints, iris scanning,voice analysis, handwriting dynamics, keystroke dynamics, and so on.biometric authentication operates by matching measured physicalcharacteristics of a person against a template or a generating model ofthese characteristics that was created when the person was registeredwith an authentication system. the match between a captured biometricand the template is never exact, because of the ònoiseó associated with themeasurement processes, the way the characteristic is presented to thesensor, and changes in the underlying biometric characteristic itself. thus,these technologies require an administrator to set threshold values and adecision policy that controls how close the match must be and how manyattempts to be authenticated the user will be allowed to make.the scoring aspect of biometrics is a major departure from otherclasses of individual authentication technologies, which provide a simple,binary determination of whether an authentication attempt was successful. the scoring aspect of biometric authentication technologies meansthat they exhibit type i (false negative) and type ii (false positive) errors.type i errors run the risk of inconveniencing or even alienating individuals whose authentication attempts are erroneously rejected. type ii errors are security and privacy failures, as they represent authenticationdecisions that might allow unauthorized access. for any specific biometric technology implementation, there is a tradeoff between these twotypes of errors: changing the scoring to reduce type i errors increasestype ii errors, and vice versa. some implementations of biometric authentication technologies exhibit relatively poor tradeoffs between thesetwo error types, forcing an administrator to choose between inconveniencing legitimate users while rejecting (appropriately) almost all imposter attempts, or minimizing inconvenience to legitimate users whileaccepting a higher rate of successful imposters.17j.l. wayman, òfundamentals of biometric authentication technologies,ó internationaljournal of imaging and graphics 1(1)(2001); b. miller, òeverything you need to know aboutbiometrics,ó pin industry sourcebook, warfel and miller, 1989.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.122who goes there?biometric values that are captured for authentication and transmittedto a remote location for verification must be protected in transit. they arevulnerable to interception and replay, just like (static) passwords, unlesssuitably protected during transmission across communication networks.it also is important to ensure that the transmitted value represents a legitimate, digitized sample of a user biometric. otherwise, an attacker mightinject a string of bits that purports to be a biometric sample in an effort tosubvert the system. typically (though not always), the range of possiblevalues for a biometric sample is so large that guessing is not a viablemeans of attack. but since biometric values are not secrets per se, it isconceivable that an attacker has gained access to a userõs fingerprint, forexample, and has digitized it in an effort to masquerade as the user.moreover, if unencrypted (or weakly encrypted) biometric templates arestored in centralized authentication servers, an attack against one of theseservers could result in the disclosure of the templates for all the usersregistered with the compromised server. with access to the templatesand knowledge of the scoring algorithm, an attacker could engage in offline analysis to synthesize bit strings that would pass as legitimate biometric samples for specific users.today biometric authentication systems are not widely deployed, andthere are many implementation variants for the same type of biometric(for example, a plethora of fingerprint systems). however, if biometricauthentication were to become widely deployed and if there were significant consolidation and standardization in the industry (resulting in fewervariants), the compromise of an authentication server could have a verysignificant impact owing to the special characteristics of biometrics:namely, that they are not secret and cannot be easily modified.as with hardware tokens, the deployment of biometric authentication sensors entails hardwareacquisition costs, although the cost here istypically for each access point of a system rather than for each user of thesystem. sensors for biometric authentication have been expensive, andthis has been a barrier to adoption. however, the cost of some biometricsensors, especially fingerprintscanning sensors, has declined, makingthem affordable for use in individual computers and laptops. althoughall biometric measures change over time, an individual cannot forget hisor her biometric values, unlike passwords and pins, nor can they be lost,like hardware tokens. thus, lifecycle costs can, in principle, be lower forbiometric authentication technologies, primarily because of reduced helpdesk costs. however, in practice, most biometric authentication systemsrequire the use of a password or pin to improve security, and this eliminates the cost advantage that would have accrued from fewer helpdeskcalls. in fact, one can improve the performance of biometric authentication systems in some contexts by offering an alternative authenticationwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication technologies123mechanism for some individuals who are not compatible with a specifictechnology. (for example, some percentage of the general population hasfingerprints that are not well recognized by fingerprint scanners.)finding 5.2: biometric userauthentication technologies holdthe promise of improved user convenience. vendors of thesetechnologies also promise reduced system management costs,but this has yet to be demonstrated in practice. moreover, thesetechnologies can pose serious privacy and security concerns ifemployed in systems that make use of servers to compare biometric samples against stored templates (as is the case in manylargescale systems). their use in very local contexts (for example, to control access to a laptop or smart card) generallyposes fewer security and privacy concerns.recommendation 5.2: biometric technologies should not beused to authenticate users via remote authentication serversbecause of the potential for largescale privacy and securitycompromises in the event of a successful attack (either internalor external) against such servers. the use of biometrics forlocal authenticationñfor example, to control access to a privatekey on a smart cardñis a more appropriate type of use forbiometrics.biometric authentication offers only oneway initial authentication.as noted above, biometric authentication does not provide direct protection for secrets, so it does not provide a basis for bootstrapping frominitial to continuous authentication, nor does it support twoway authentication, unlike many of the òsomething you haveó technologies describedabove. thus, biometric authentication is not an appropriate replacementfor other authentication technologies, specifically for cryptographic technologies used to provide twoway initial authentication and continuousauthentication. box 5.5 provides some commonsense guidelines for theuses of biometric authentication systems.multifactor authenticationit is often asserted that individual authentication can be improved byemploying multiple òfactors.ó generally this translates into using authentication technologies from two of the classes described above. examplesinclude a pin plus a hardware token (something you know and something you have) or a pin and a biometric (something you know andsomething you are). there is a reasonable basis for this strategy, but it iswho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.124who goes there?box 5.5items to keep in mind when using biometrics1. never design or use a biometric system that allows either remote enrollment or reenrollment. such systems have no good way of connecting a user withthe enrolled biometric record other than additional authentication, so the advantage of using biometrics is lost.2. biometric measures can reveal your identity if they are linked at enrollmentor at subsequent usage to your name, social security number, or other identifyinginformation.3. remember that biometric measures cannot be reissued if stolen or sold.consequently, your biometric measures will be only as secure as the most insecure site that has them. do not enroll in a system that does not seek to preserveanonymity unless you have complete trust in the system administration.4. all biometric accesscontrol systems must have exceptionhandling mechanisms for those individuals who either cannot enroll or cannot reliably use thesystem for whatever reason. if you are uncomfortable with enrolling in a biometricsystem for positive identification, insist on routinely using the exceptionhandlingmechanism instead.5. the most secure and most privacysensitive biometric systems are thosein which each user controls his or her own template. however, simply controllingyour own biometric template, say by holding it on a token, does not guaranteeeither privacy or security.6. because biometric measures are not perfectly repeatable, are not completely distinctive, and require specialized data collection hardware, biometric systems are not useful for tracking people. anyone who wants to physically track youwill use your credit card purchases, phone records, or cell phone emanations instead. anyone wanting to track your internet transactions will do so with cookies,web logs, or other technologies.not foolproof. the assumption underlying the perceived security benefitsof multifactor authentication is that the failure modes for different factorsare largely independent. so, for example, a hardware token might be lostor stolen, but the pin required for use with the token would not be lost orstolen at the same time. this assumption is not always true, however.for example, a pin attached to a hardware token is compromised at thesame time that the token is lost or stolen. if a fingerprint is used toactivate a hardware token, is it not likely that copies of the fingerprint willappear on the token itself? as noted earlier, one cannot evaluate the relative security of mechanisms without reference to a threat model, andsome threat models undermine the perceived security of multifactor authentication. nonetheless, multifactor authentication can improve thesecurity of authentication under many circumstances.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication technologies125centralized versus decentralizedauthentication systemsa crucial issue for authentication technologies is whether they areinherently centralized or decentralized. this distinction affects both theirdeployability and their privacy implications.some technologies require little or no infrastructure; any system canmake use of such technologies without relying on additional systems forsupport. this is one of the major drivers of the use of static passwords:they are extremely easy to set up in a highly localized fashion. an application can create and maintain its own password database with about thesame effort as that needed for maintaining a database of authorized users.(in fact, doing so properly is rather more complex, but there are numerous poorly implemented password systems.) some public key authentication technologies have similar decentralized properties. some challenge/response protocols are designed for local use and require minimal infrastructure. the onetime password18 and secure shell (ssh)19 protocolsare good examples. the latter makes use of public key cryptography butnot a public key infrastructure (described later in this section). botharguably provide a more secure authentication capability than passwords,but they are still intended for use in local contexts, such as a single computer or at most a single organization.some types of authentication technologies require some degree ofcentralizationñfor example, to help amortize the costs associated withdeployment to gain security benefits. kerberos and public key infrastructure (pki) are good examples of such systems. in kerberos, the key distribution center (kdc) is a centralized infrastructure component that storesthe passwords of all users, preventing them from having to be sharedwith each system to which a user connects. the kdc is aware of all siteswith which the user interacts, because the kdc is invoked the first timethat a user establishes a connection in any given login session. thecontent sent over the connections is not necessarily revealed; nevertheless, the central site operates as the verifier, acting as an intermediarybetween the user (presenter) and the sites that rely on kerberos for authentication. the scope of a kerberos system deployment is typicallylimited, which in practice mitigates some of the privacy concerns. although kerberos systems can be interconnected, most usage of kerberosis within an individual organization. when crossrealm authentication isemployed, only those transactions that involve multiple realms are known18see rfc 1760 at <http://www.faqs.org/rfcs/rfc1760.html>.19see <http://www.ietf.org/internetdrafts/draftietfsecshuserauth16.txt>.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.126who goes there?outside a userõs home realm.20 this limits the adverse privacy aspects ofusing such a system. however, if a single kerberos realm was used toauthenticate individuals to systems across organizational boundaries, theprivacy implications would be much worse. thus, the same technologycan be used in different contexts with vastly different privacy implications. for more information about kerberos, see figure 5.1.the passport and liberty systems, though very different in detail, arecentralized systems designed expressly to authenticate large user populations to a wide range of disparate systems, with attendant privacy implications. while their designs differ slightly, both offer users the same basicfeature: the convenience of single signon to a variety of web services.from a privacy perspective, the obvious drawback to centralized authentication systems is that all web clients cannot be expected to trust thesame authentication service with what could be personally identifyinginformation. passport and liberty both address this fundamental obstacle by allowing what they call a federated topology. òfederated,ó inthis context, means that peer authentication services can interoperate withdifferent subsets of service providers. for example, a car rental companycould rely on four different airlinesõ authentication services. theoretically, a single user could navigate seamlessly between multiply affiliatedsites after authenticating only once. even in their federated form, however, there are two types of privacy risk inherent in these single signonsystems: exposure of what we call identity data (the set of all informationassociated with an individual within this identity system) by the authentication service and the aggregation of an entityõs (or his or her identifierõs)downstream behavior. while the committee did not undertake a detailedanalysis of these two systems (one of which is proprietary and one ofwhich has a specification developed and licensed by a private consortium), as with any authentication system the privacy implications willultimately depend on choices made at the design, implementation, anduse stages.21 detailed analysis of a particular product is beyond the scopeof this report.public key infrastructure has often been touted as a universal authentication technology, one that might have national or even global scope.20a kerberos òrealmó is a local administrative domain. typically an organization willhave its own realm. different realms can be configured to interoperate with each other, butthis requires explicit action from each organization.21recently, the ftc stepped in (at the request of privacy advocates) to assure thatmicrosoftõs security and privacy policy is correctly represented to consumers. many observers commented that this move should be considered a warning that governments internationally will scrutinize how centralized authentication services collect, protect, and useconsumer data.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication technologies127figure 5.1 kerberos:1.user provides a principal (user name) and password to the client system.2.client queries the initial ticket service of the kerberos key distributioncenter (kdc) for a ticketgranting ticket (tgt), which will allow the clientto request tickets for specific services later on. the clientõs request includes a derivative of the userõs password, which the initial ticket serviceverifies.3.the kdcõs initial ticket service provides the client with a dualencryptedinitial tgt containing a login session key. the client system converts theuserõs password into an encryption key and attempts to decrypt the tgt.4.the client uses the tgt and the login session key to request tickets tospecific services from the kdcõs ticketgranting service.5.the ticketgranting service decrypts the tgt with its own key, and thendecrypts the service request using the tgtõs session key. if decryption issuccessful on both counts, the ticketgranting service accepts the userõsauthentication and returns a service ticket and a servicesession key (encrypted with the login session key) for the targeted service. this resultcan be cached and reused by the client.6.the client uses the login session key provided in step 3 to decrypt theservice ticket, gaining access to the servicesession key. this key is thenused to request access to the target service. this request is accompaniedby an encrypted time stamp as an authenticator.7.access to the target service is granted. steps 4 through 7 can be repeatedwhen access to other services is needed; service messages can be encryptedwith the servicesession key. a time limit is built into the login session insteps 3 and 5; the user will need to enter the password again when thelogin session has timed out.kdcserverclient1initial ticket serviceticketgranting servicetarget servicetarget servicetarget service236745who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.128who goes there?certainly a very largescale pki would have very serious privacy implications, as it might provide a single, uniform identifier that an individualwould employ in transactions with many different organizations. (seebox 5.6 for a brief description of public key cryptography.) since eachpublic key certificate carries a clearly visible identifier for the personrepresented by the certificate, it is easy to link different uses of the samecertificate to that personõs identity.the general services administrationõs access certificates for electronic services (aces) program, described more fully in chapter 6 inthis report,22 has this flavor for citizen interactions with the u.s. government. in japan, plans call for the creation of a nationallevel pki thatwould be used not only for individual interactions with the governmentbut also for a wide range of private sector interactions. verisign andother socalled trusted third party (ttp) certificate authorities (cas) inboth the united states and europe promote the notion of using a singlepublic key certificate as the universal personal authenticator for a widerange of transactions.for example, if citizens were issued a single òinteract with the governmentó public key certificate, it might be relatively easy to determine if,say, the individual who had a reservation to visit yosemite national parkwas the same person who had sought treatment in a department of veterans affairs (va) hospital for a sexually transmitted disease. by contrast,if the va and the national park service each issued their own certificates,or if they relied on some other decentralized authentication mechanism,such linkage would be harder to establish. thus, it is not the use of pkiper se (except as it is an authentication system with all of the privacyimplications intrinsic to authentication itselfñsee chapters 2, 3, and 7 inthis report) but rather the scope of the pki that influences the privacy ofthe authentication system.pki technology does not intrinsically require large scale or use acrossmultiple domains in order to be useful or costeffective to deploy. thisreport has already argued that individuals typically have multiple identities and that most identities are meaningful only in limited contexts, whichsuggests that many pkis could arise, each issuing certificates to individuals in a limited context, with an identifier that is meaningful only in thatcontext.23 pkis of this sort can be privacypreserving, in contrast to verylargescale pkis. proposals have been made to use pkis in a highly de22see <http://www.gsa.gov/aces/>.23for another view of pki, digital certificates, and privacy, see stefan brands, rethinkingpublic key infrastructures and digital certificates: building in privacy, cambridge, mass., mitpress, 2000.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication technologies129box 5.6public key cryptographypublic key cryptosystems were first described in the open literature by whitfield diffie and martin hellman at stanford university in 1976.1 in public key systems, each user has two keys. one is kept private while the other, as the nameimplies, is usually made public. these keys are mathematically related in such away that knowledge of the public key does not allow one to determine the corresponding private key. (the reverse may or may not be true, depending on thepublic key algorithm in question.) this property of public key cryptosystems meansthat data encrypted with one userõs public key can be decrypted using his or hercorresponding private key, without sharing the private key with others. conversely, data that are transformed with a userõs private key (digital signing) can be verified with the corresponding public key, again without the need to divulge the keyused to generate the signature. this latter relationship is most relevant to userauthentication systems based on public key technology. the use of public keysystems significantly transforms the problem of key distribution: distribution ofpublic keys requires authentication and integrity of the public keys (we have toknow whose public keys we are using) but not confidentiality (because the publickeys need not be kept secret).there are two basic ways in which public key systems are used:2¥encryption. public key systems are used to provide confidentiality by having the recipient of a confidential message first provide its public key to the sender.this transaction does not have to be held in secret because the key distribution,the public key, does not have to be kept confidential. the sender then encrypts thecommunication in the recipientõs public key and sends it to the recipient. only therecipient can decrypt the message using his or her private key.¥digital signature. public key systems such as the rsa system3 and thedigital signature standard4 can provide what is often referred as a òdigital signature.ó digitally signed data are not encrypted by the process. instead, they areprotected against unauthorized modification, and the identity of the signer of thedata can be determined (data origin authentication) if, for example, a pki has beenestablished (or if the verifier trusts, through some other means, that the public keyof the signer is as described). a message or document is digitally signed by transforming the data5 using the signerõs private key.1see w. diffie and m. hellman, ònew directions in cryptography,ó ieee transactions oninformation theory it22(6)(1976):644654.2a third type of public key cryptosystem, public key agreement algorithms, is not discussedhere, since these systems usually are employed for confidentiality but not for authentication.3rsa was developed by ron rivest, adi shamir, and leonard adelman at mit. see òamethod for obtaining digital signatures and publickey cryptosystems,ó communications ofthe acm 21,2 (february 1978): 120126.4see federal information processing standards publication 186 on the digital signaturestandard, available online at <http://www.itl.nist.gov/fipspubs/fip186.htm>.5in practice, the data to be signed are first compressed in a oneway fashion, using a hashalgorithm, and the resulting hash value is digitally signed. this variant on the basic scheme isemployed because public key signature operations are relatively slow and signing a largeamount of data would be very burdensome.(continues)who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.130who goes there?digital certificatesmost uses of public key systems require that one know that a given public keybelongs to a particular person or organization. one obvious way to obtain thepublic key securely is to obtain it directly from the sender in a secure outofbandchannel (for example, by way of a personal interaction). this approach, whileviable in some circumstances, in general does not scale very well.however, the very nature of a digital signature lends itself to a solution to thisproblem. specifically, if a recipient knows one public key, the issuer of that publickey can òvouchó for the association between a different public key and its owner byissuing a digital document of that assertion. with some additional structure, thissystem becomes the basis for digital certificates, and therefore a pki. the entitythat signs (issues) a certificate usually is referred to as a certificate authority (ca).note that cas collect data from many users as part of the certificate issuance(registration) process and assign a single identifier to each user. this practiceencourages a user to collapse multiple identities into a single identity for presumedease of use in interactions with a diverse set of organizations, heightening the riskof linkage.because a certificate represents a binding between an identifier (presumablyassociated with the owner) and a key, it inherently contains some notion of identity.just how strong this notion is, and the form of identity bound into a certificate,depends on the policies of the ca and on the intended use of the certificate. insome forms, a certificate can contain a name, an email address, or an accountnumber. in others, there may be no meaningful identification, just the public keyitself. the basic notion behind the use of a certificate is to establish a certificationpath between a known public key and the certificate being verified.box 5.6continuedcentralized fashion24,25 that supports this notion of multiple identities foran individual and thus supports privacy. however, multiple pkis mightimpose burdens on users, who would be required to manage the multitude of certificates that would result. in a sense, this is not too differentfrom the common, current situation in which an individual may holdmany physical credentials and has to manage their use. if individuals aregoing to accept and make use of a multitude of pkis, software needs toprovide a user interface that minimizes the burden on users.24s. kent. òhow many certification authorities are enough?ó proceedings of milcom(unclassified papers) 97(1)(november 1997):6168.25s. kent. òsecurity issues in pki and certification authority design.ó advanced securitytechnologies in networking. nato science series. burke, va., ios press, pp. 3352, 2001.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication technologies131finding 5.3: public certificate authorities and trusted third parties present significant potential privacy and security concerns.finding 5.4: public key infrastructures have a reputation forbeing difficult to use and hard to deploy. current products dolittle to dispel this notion.finding 5.5: many of the problems that appear to be intrinsic topublic key infrastructures (as opposed to specific public keyinfrastructure products) seem to derive from the scope of thepublic key infrastructures.recommendation 5.3: public key infrastructures should be limited in scope in order to simplify their deployment and to limitadverse privacy effects. software such as browsers should provide better support for private (versus public) certificate authorities and for the use of private keys and certificates amongmultiple computers associated with the same user to facilitatethe use of private certificate authorities.this analysis suggests that authentication technologies that implysome degree of centralization can be operated over a range of scales withvastly differing privacy implications. thus, neither kerberos nor pkiintrinsically undermines privacy (beyond the fact that they are authentication systems and as such can affect privacy), although each could beused in a way that would do so. in general, decentralized systems tend tobe more preserving of privacy: no single party has access to more than itsown transaction records. an individual may use the same password fortwo different web sites; for a third party to verify this, the party wouldneed at least the cooperation of both sites and (depending on the precisepassword storage technology being used) perhaps specialpurpose monitoring software on both sites. but if users employ the same identifiers ateach site, the potential for privacy violations is significantly increased.this same observation applies to any form of decentralized authentication system.an essential requirement for preserving privacy in authenticationsystems is allowing an individual to employ a different identifier when heor she asserts a different identityñfor example, in different organizational contexts. the use of different identifiers makes it harder to correlate the individualõs activities across systems, which helps preserve privacy. this goal can be achieved with technologies ranging frompasswords to pkis. also, if each system collects less personal informationwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.132who goes there?on its usersñonly what is required to satisfy the requirements of thatsystemñthis, too, is privacypreserving.finding 5.6: core authentication technologies are generallymore neutral with respect to privacy than is usually believed.how these technologies are designed, developed, and deployedin systems is what most critically determines their privacy implications.security considerations for individualauthentication technologiesauthentication technologies are often characterized by the securitythat they offer, specifically in terms of resistance to various types of attack. many authentication technologies rely on the use of secret valuessuch as passwords, pins, cryptographic keys, and so on. secrets may bevulnerable to guessing attacks if they are selected from a set of values thatis too small or predictable. passwords, when selected by individuals andnot subject to screening, often exhibit this vulnerability.26 secrets alsomay be compromised by computational attacks, even when the secretsare chosen from large sets of values. for example, a large, randomlychosen cryptographic key would generally be immune to guessing attacks. but this key could be used in an authentication protocol in a manner that permits an attacker to perform computations that reveal the valueof the key.if secrets are transmitted across a communication network, from presenter to verifier, as part of an authentication process, they are vulnerableto interception unless otherwise protected (e.g., by encryption). an encrypted communication path is often necessary, but it is not sufficient toprotect secrets against being transmitted. an attacker might masqueradeas a system that a user wants to access and thus trick the user into revealing an authentication secret, even though the secret was encrypted enroute.27 a secret need not be transmitted across a network to be subject toattacks of this sort. several years ago, thieves installed a fake atm in a26in this context, the user (presenter) also is acting as the issuer, and as an issuer is doinga poor job.27this is an example of why twoway authentication is important. a form of this attacksometimes takes place when a user employs secure sockets layer to encrypt communication between a browser and a web server. the user may reveal credit card account information (account number, expiration date, shipping address, and so on) to a sham merchant,who then can use this information to carry out unauthorized transactions.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication technologies133shopping mall.28 unsuspecting individuals inserted atm cards and entered pins, which were collected by the thieves and used to make unauthorized withdrawals from the usersõ accounts. thus, even physical proximity and an ability to see a verifier does not ensure that it is the device itappears to be and one to which authentication information should bepresented!secret values that are too big for individuals to remember must bestored. the way in which the secrets are stored may make them vulnerable. for example, passwords written on a note stuck on a monitor in theworkplace may be observed by other employees, custodial staff, or evenvisitors. secret values stored in a file on a computer can be compromisedby a wide variety of attacks against the computer, ranging from physicaltheft to network intrusions. even secret values stored in hardware dedicated to authentication can be extracted illicitly, with varying degrees ofdifficulty, depending on the technology used to store the secrets.often there is a requirement to prevent individuals from sharing authentication data in support of individual accountability. if authentication data are known to individuals or are easily extracted from storage,then individuals may voluntarily make copies and thus circumvent thissystem goal (see chapter 4). even when secrets are stored in physicaltokens, the tokens may be loaned to others, in violation of proceduralaspects of a security policy.sometimes authentication is based not on the possession of a secretvalue but on the possession of a physical item that is presumed to beresistant to tampering and forgery. an authentication system may beattacked successfully if the assumptions about its tamper or forgeryresistance prove to be false. in many cases, the security of the credentialis derived from the integrity of the data associated with the credential,rather than on the physical characteristics of the credential. for example,a physical credential might contain digitally signed data attesting to aname and employee id number. verification of the credential, and thusauthentication of the individual possessing the credential, would be basedon successful validation of the digital signature associated with the data.careful use of public key cryptography can make the digital signaturehighly secure, protecting against modification of the signed data or creation of new, fake signed data. however, it may be quite feasible to copythe data to additional physical credentials. these duplicate credentialsrepresent a form of forgery. unless the signed data are linked directly to28in 1993 in connecticut, a fraudulent atm was installed in a shopping center. see therisks digest for more details; available online at <http://catless.ncl.ac.uk/risks/14.60.html#subj3>.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.134who goes there?the holder of the credential (for example, by means of biometrics), thissort of forgery by duplication is a security concern.biometric authentication also relies on the possession of a physicalitem that is presumed to be resistant to tampering and forgery, namelysome measurable part of an individualõs body or behavior. examplesinclude fingerprints, voiceprints, hand geometry, iris patterns, and so on.biometric values are not secrets; we leave fingerprints on many items thatwe touch, our voices and facial images may be recorded, and so on.29thus, the security of biometric authentication systems relies extensivelyon the integrity of the process used to capture the biometric values and onthe initial, accurate binding of those values to an identifier. it is criticalthat later instances of the biometric capture process ensure that it is a realperson whose biometric features are being capturedñthis may mean requiring biometric sensors to be continuously monitored by humans. biometric authentication systems may be fooled by fake body parts or photographs created to mimic the body parts of real individuals.30 they alsomay be attacked by capturing the digitized representation of a biometricfeature for an individual and injecting it into the system, claiming that thedata are a real scan of some biometric feature.the preceding analysis of the security vulnerabilities of classes ofauthentication technologies, while accurate, does not determine whetherany of these technologies is suitable for use in any specific context.instead, a candidate technology must be evaluated relative to a perceived threat in order to determine whether the technology is adequatelysecure. nonetheless, it is important to understand these vulnerabilities29òthe physical characteristics of a personõs voice, its tone and manner, as opposed tothe content of a specific conversation, are constantly exposed to the public. like a manõsfacial characteristics, or handwriting, his voice is repeatedly produced for others to hear.no person can have a reasonable expectation that others will not know the sound of hisvoice, any more than he can reasonably expect that his face will be a mystery to the world.óñjustice potter stewart for the majority in u.s. v. dionisio, 410 u.s. 1, 1973.30see t. matsumoto, h. matsumoto, k. yamada, and s. hoshino, òimpact of artificialgummy fingers on fingerprint systems,ó proceedings of the international society for opticalengineering (spie) 4677 (january 2002), available online at <http://research.nii.ac.jp/kakenjohogaku/reports/h13overview/a04001.pdf>; l. thalheim, j. krissler, and p. ziegler,òbiometric access protection devices and their programs put to the test,ó cõt magazine 11(may 21, 2002):114, available online at <http://www.heise.de/ct/english/02/11/114>; t.van der putte and j. keuning, òbiometrical fingerprint recognition: donõt get your fingers burned,ó proceedings of the ifip tc8/wg8.8 fourth working conference on smart cardresearch and advanced applications, kluwer academic publishers, dordrecht, the netherlands, 2000, pp. 289303, available online at <http://www.keuning.com/biometry/biometricalfingerprintrecognition.pdf>; and d. blackburn, m. bone, p. grother, andj. phillips, facial recognition vendor test 2000: evaluation report, u.s. department of defense, january 2001, available online at <www.frvt.org>.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication technologies135when evaluating the security characteristics of individual authentication technologies.cost considerations for individual authentication technologiescosts are an important factor in the selection of authentication technologies. these costs take many forms. capital costs are associated withthe acquisition of any hardware or software needed for an authenticationtechnology. the hardware and software costs may be a function of thenumber of individuals being authenticated, or of the number of points atwhich authentication takes place, or both. for example, an authenticationsystem that makes use of hardware tokens has a peruser cost, since eachuser must have his or her own token, and each device that will authenticate the user (for example, each desktop or laptop computer) must beequipped with a reader for the token. a biometric authentication systemmight typically require readers at each point where individuals are authenticated, and there would be a perdevice, not a perperson cost. asoftwarebased authentication system may impose costs only for eachcomputer, not each individual, although licensing terms directed by avendor might translate into peruser costs as well.many authentication systems also make use of some common infrastructure, which also has associated hardware and software acquisitioncosts. the infrastructure may be offline and infrequently used, or it maybe online and require constant availability. in the online case, it may benecessary to acquire replicated components of the infrastructure, to geographically disperse these components, and to arrange for uninterruptiblepower supplies, in order to ensure high availability. the key distributioncenter component of a kerberos system (see box 5.7) and the ace/serverused by the securid system are examples of the latter sort of infrastructure. a certificate authority in a pki is an example of the online type ofinfrastructure component.operation of an authentication system involves labor costs of various types. help desks must be manned to respond to usersõ questionsand problems. if the system relies on secret values that users are required to remember, the help desk will have to interact with users toreset forgotten secret values. if the system makes use of hardware tokens, provisions will have to be made to replace lost or stolen tokens.users and system administrators must be trained to work with an authentication technology and with that technologyõs interaction withvarying operating systems and applications. application developersmust learn how to make use of an authentication technology and tointegrate it into their applications.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.136who goes there?box 5.7kerberosthe kerberos authentication system was developed at the massachusettsinstitute of technology (mit) in about 1985 as part of mitõs project athena.1 akerberos system consists of three parties: (1) a client, which attempts to authenticate itself to (2) a server and (3) a key distribution center (kdc), also known as akerberos server, which acts as an intermediary for the authentication.every kerberos òprincipaló (either client or server) has a secret key knownboth by the principal and the kdc. most kerberos systems emulate a traditionalpasswordbased authentication system inasmuch as the human end user knows asecret password. however, unlike a traditional passwordbased authenticationsystem, this password is never transmitted over a network and is therefore noteasily stolen by eavesdropping. when used, this password is hashed into a secretkey that is used to complete the kerberos protocol.when a client wishes to make use of a server, it queries the kdc for a òticketófor the server. the ticket contains the identity of the client along with a time stampand validity period. this ticket is encrypted by the kdc so that only the server candecrypt it. among the information encrypted into the ticket is a randomly generatedòsessionó key. the ticket and the session key are then provided to the client by wayof a ticketgranting ticket service.the ticketgranting ticket service is a kerberos service in which the kdc isalso the server. in general, a ticket for the ticketgranting ticket service is obtained at login time. additional tickets are then obtained for other services fromthe kdc as needed. these additional tickets, encrypted in the session key of theticketgranting ticket instead of the clientõs password, are sent by the kdc to theclient. this permits the kerberos system to obtain as many tickets as are necessary without the clientõs secret key (password) needing to be kept in memory onthe clientõs workstation (computer).1for more information on project athena, see <http://web.mit.edu/olh/welcome/intro.html>.this brief discussion illustrates how complex it can be to evaluate thecost of an individual authentication system. initial capital outlays aregreater for some types of systems; ongoing costs of other types of systemsmay eventually outweigh these capital outlays. different contexts meritdifferent levels of security and will tolerate different costs for authentication technology. thus, there can be no single right answer to the questionof how much authentication technology should cost.concluding remarksthe preceding chapters describe three different conceptual types ofauthentication (identity, attribute, and individual), and this chapter fowho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication technologies137a clientõs secret key, as mentioned earlier, is typically known to the client asa password, which is hashed into the key when needed. servers store their secretkey (needed to decrypt incoming tickets to obtain the stored session key) somewhere on the server, typically in a file.kerberos provides for clients to authenticate themselves to servers and forservers to be authenticated to clients. at the end of a successful authenticationtransaction, the session key created by the kdc and stored in the ticket is knownto both the client and the server. this key can then be used to provide integrityprotection for information sent between client and server and for the encryption(confidentiality) of information transfer as well.kerberos is an online system. the kdc is required to be involved in mosttransactions.2 a ticket is typically issued with a lifetime measured in hours.3 shortticket lifetime simplifies the revocation problem. if an entityõs key is compromised,all that needs to be done is to change that key in the kdc (and on the server if aserver key is compromised). the use of symmetric algorithms means that kerberos computations are much faster than the equivalent public key operations.kerberos still relies on the limited human storage capacity for passwords. italso relies on the userõs computer for computational ability, which means that itmust be trusted by the user.recent work on the kerberos protocol introduces the use of public key encryption into the initial ticket granting ticket (login) transaction. this means thatonly clientsõ public keys need to be stored by the kdc, reducing the amount ofsecret information that needs to be protected by the kdc.2but this is not true for all transactions. once a ticket is obtained by a client for a particularservice, that ticket can be reused for multiple transactions without further involvement of thekdc.3in version 4 kerberos, the maximum ticket lifetime was 21 hours. version 5 lifts this restriction, but it is still a good idea to limit ticket lifetime.cuses on the technologies that go into building an authentication systemand some of the technologyrelated decisions that must be made. some ofthese decisions will bear on the privacy implications of the overall system. in general, decentralized systems tend to be more preserving ofprivacy, but the core authentication technologies that make up authentication systems tend to be privacyneutral. what matters most in terms ofprivacy are design, implementation, and policy choices, as described elsewhere in this report.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.1386authentication, privacy, and theroles of governmentgovernment institutions play multiple roles in the area where authentication technologies intersect with privacy concerns. notonly do all levels of government (state, federal, and local) useauthentication systems, but the technologies are employed within andacross government institutions at each level as well. furthermore, government plays multiple roles in the authentication process. as a relyingparty, government uses authentication technologies for electronic government applications and for physical and systems security applications.given the size of its workforce and its user base, government is a significant user of these technologies. the governmentõs role in the authentication process (as regulator, issuer, and relying party) is important, since somany forms of authentication or identification rely on some form of governmentissued identity or identifier.it is not surprising, therefore, that government organizations haveconflicting and supporting roles in authentication. as an example, thesocial security administration (ssa) fills all three roles simultaneously.the social security number (ssn) was designed by ssa for its own use inrecording earnings. for its intended purpose, in conjunction with otherssa business processes and controls, the ssnõs security level meets thessaõs requirements. in this case, the ssa is both the issuing and therelying party, so the incentives for risk mitigation are properly aligned.when the parties that issue and rely on an identifier are different, theincentives for risk mitigation are not necessarily aligned. for instance,who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication, privacy, and the roles of government139secondary uses of the ssn have proliferated, beginning with their use bythe internal revenue service (irs) and state taxation agencies, and nowextending to many private sector organizations such as credit reportingagencies. there is an inherent conflict between the higher confidence levelsdesired by the relying party and the extra cost imposed on the issuer tomeet this confidence level. for example, it is probably neither reasonablenor costeffective for the ssa to change its ssn issuance and maintenanceprocesses in order to help the private sector manage business risk aroundcreditworthiness just because most credit bureaus use ssns as unique identifiers for credit history. an examination of the various roles that government fills in authentication processes and privacy protection, anchored byspecific examples, helps to explain this complexity.the issuance of ids illustrates how different levels of governmentinteract with the public through specific programs for sometimes uniquereasons. the principle of federalismñthe division (and in some casesoverlap) of government responsibilities among federal, state, and localgovernment,1 designed into the u.s. constitutional form of governmentñhelps to explain why it is important not to view the government role inany area as monolithic.by design, as protected by public law and policy, government activities are assumed to be fair, impartial, and immune from commercial manipulation.2 this legal and policy context for the government management of information and related technology makes government use ofthese technologies a special case and certainly different from their use bythe private sector. individuals who are citizens of or permanent residentsin the united states also have a unique relationship with governmentagencies. sometimes by choice, and in many instances by compulsion,citizens and residents are both participants in governance and users ofgovernment goods and services. for instance, citizens may choose tocomment on proposed changes in informationreporting requirements1one insightful definition of federalism and its complexity comes from woodrow wilson: òto make town, city, county, state, and federal government live with a like strengthand an equally assured healthfulness, keeping each unquestionably its own master and yetmaking all interdependent and cooperative, combining independence and mutual helpfulness.ó see woodrow wilson, òthe study of administration,ó political science quarterly2(june 1887):197222, quoted in dell s. wright, òa century of intergovernmental administrative state: wilsonõs federalism, new deal intergovernmental relations, and contemporary intergovernmental management,ó a centennial history of the american administrativestate, ralph clark chandler, ed. new york, n.y., the free press, 1987, p. 220.2charles goodsell. the case for bureaucracy, 3rd ed. new york, n.y., seven bridgespress, 1994.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.140who goes there?such as the questions on census forms. alternatively, some relationshipswith government are straightforward and contractual, just as with a business. for example, when it is time to repay student loans, beneficiarieshave a legal obligation to return the money that they borrowed from thegovernment with interest.unlike private sector organizations, though, public agencies cannotchoose their customers. public law and regulation instead of businessplans dictate whether an individual is a beneficiary or a regulated entity.while private sector organizations may only want an individualõs business under certain conditions (for example, one can only get a mortgageor a credit card upon meeting certain eligibility criteria), most citizensinteract with government organizations from cradle to grave. from therecording of birth to the issuance of death certificatesñand annually inbetween for some government programsñcitizensõ interaction with government is virtually inescapable.finding 6.1: many agencies at different levels of governmenthave multiple, and sometimes conflicting, roles in electronicauthentication. they can be regulators of private sector behavior, issuers of identity documents or identifiers, and also relying parties for service delivery.regulator of private sector and public agencybehaviors and processesthe government acts as a regulator of multiple sectors, includinghealth and medical services, financial services, and education. for thisanalysis, these regulatory activities are put in three groups: (1) governmentwide law and policy that are focused internally on the activities offederal agencies in a particular domain (for example, privacy, electronicgovernment, or computer security); (2) program or agencyspecific lawand policy that apply to specific types of transactions but may cut acrossa number of government agency and private sector organization boundaries for transactions such as federally funded health care or higher education; and (3) public law or policy intended to regulate the informationmanagement activities of the private sector broadly or more specificallyin certain areas such as financial services. this section summarizes someof this public law and government policy and concludes by identifyingsome pending legislation that is relevant to privacy and authentication.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication, privacy, and the roles of government141governmentwide law and policyprivacy act and computer matching acta recent government accounting office (gao) report3 refers to theprivacy act of 1974 (5 u.s.c. sec. 552a)4 as the òprimary [u.s.] law regulating the federal governmentõs collection and maintenance of personalinformation.ó generally speaking, the privacy act aimed at balancing thefederal governmentõs need to maintain information about individualswith the rights of individuals to be protected against unwanted invasionsof their privacy. the act attempts to regulate the collection, maintenance,use, and dissemination of personal information by federal governmentagencies. as one source summarizes, the act provides privacy protectionin three ways:1.it sustains some traditional major privacy principles. for example,an agency shall maintain no record describing how any individual exercises rights guaranteed by the first amendment unless expressly authorized by statute or by the individual about whom the record is maintained or unless pertinent to and within the scope of an authorized lawenforcement activity.2.it provides an individual who is a citizen of the united states, or analien lawfully admitted for permanent residence, with access and emendation arrangements for records maintained on him or her by most, butnot all, federal agencies. general exemptions in this regard are providedfor systems of records maintained by the central intelligence agency andfederal criminal law enforcement agencies.3.the act embodies a number of principles of fair information practice. for example, it sets certain conditions concerning the disclosure ofpersonally identifiable information; prescribes requirements for the accounting of certain disclosures of such information; requires agencies toòcollect information to the greatest extent practicable directly from thesubject individual when the information may result in adverse determinations about an individualõs rights, benefits, and privileges under federal programsó; requires agencies to specify their authority and purposesfor collecting personally identifiable information from an individual; requires agencies to òmaintain all records which are used by the agency in3government accounting office (gao). internet privacy: agenciesõ efforts to implementombõs privacy policy [ggd00191]. washington, d.c., gao, september 2000. availableonline at <http://www.gao.gov/archive/2000/gg00191.pdf>.4the full text of the act itself is available online at <http://www.usdoj.gov/04foia/privstat.htm>.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.142who goes there?making any determination about any individual with such accuracy, relevance, timeliness, and completeness as is reasonably necessary to assurefairness to the individual in the determinationó; and provides civil andcriminal enforcement arrangements.5however, passed in great haste during the final week of the 93rdcongress, the òactõs imprecise language, limited legislative history, andsomewhat outdated regulatory guidelines have rendered it a difficult statute to decipher and apply.ó6one major complicating factor in the implementation and regulationof privacy act provisions has been the òlack of specific mechanisms foroversight.ó7 indeed, some have cited the absence of a central agency forthe oversight and coordination of the nationõs privacy matters as a majorreason for the ineffectiveness of american privacy laws in general.8 incomparison, several other nations have dedicated whole departments andappointed highlevel officials to oversee their privacy matters.the computer matching and privacy protection act of 1988 (pl 100503) amended the privacy act of 1974 by adding new provisions regulating the federal governmentõs use of computer matchingñthe computerized comparison of information about individuals, usually for the purposeof determining the eligibility of those individuals for benefits. the mainprovisions of the act include the following:¥give individuals an opportunity to receive notice of computermatching and to contest information before having a benefit denied orterminated;¥require that federal agencies engaged in matching activities establish data protection boards to oversee matching activities;¥require federal agencies to verify the findings of computer match5text for these three items is adapted from harold c. relyea, the privacy act: emergingissues and related legislation, congressional research service (crs) report rl30824, washington, d.c., crs, library of congress, september 2000.6department of justice. òoverview of the privacy act of 1974ó (introduction), 2000.available online at <http://www.usdoj.gov/04foia/1974intro.htm>.7charles r. booz. òelectronic records and the right to privacy: at the core.ó information management journal 35 (3): 18.8see david h. flaherty, òthe need for an american privacy protection commission,ógovernment information quarterly 1(3)(1984):235258. in later work, he also observes thatdesign of the system and policy choices are crucial to privacy protection. see, for example,òprivacy impact assessments: an essential tool for data protection,ó presentation to aplenary session ònew technologies, security and freedomó at the 22nd annual meeting ofprivacy and data protection officials held in venice, italy, september 2730, 2000. availableonline at <http://www.anu.edu.au/people/roger.clarke/dv/piasflaherty.html>.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication, privacy, and the roles of government143ing programs before suspending, denying, or otherwise òadverselyó affecting an individualõs benefits; and¥require agencies to negotiate written agreements with other agencies participating in the matching programs.an amendment to the act that was passed in 1990 somewhat alteredthe original actõs due process provisions. specifically, the amendmentchanged some of the details regarding subject notification of adverse findings and gave data protection boards the ability to waive independentverification of information under certain circumstances.in december 2000, the office of management and budget (omb) issued a memorandum reminding federal agencies of the actõs requirements.9,10 according to the memorandum, as ògovernment increasinglymoves to electronic collection and dissemination of data, under the government paperwork elimination act and other programs, opportunitiesto share data across agencies will likely increase.ó therefore, òagenciesmust pay close attention to handling responsibly their own data and thedata they share with or receive from other agencies.ócomputer security act and recent amendmentsthe computer security act of 1987 (pl 100235) addressed the importance of ensuring and improving the security and privacy of sensitiveinformation in federal computer systems. the act required that the national institute of standards and technology (formerly the national bureau of standards) develop standards and guidelines for computer systems to control loss and unauthorized modification or disclosure ofsensitive information and to prevent computerrelated fraud and misuse.the act also required that operators of federal computer systems, including both federal agencies and their contractors, establish security plans.11additionally, the law stipulated that agency plans for protecting sensitiveinformation and systems be costeffective, and most important, it established a standard for risk mitigation. specifically, the law says that federal agencies must òestablish a plan for the security and privacy of eachfederal computer system identified by that agency pursuant to subsection (a) that is commensurate with the risk and magnitude or the harm9office of management and budget (omb). òguidance on interagency sharing of personal datañprotecting personal privacy,ó m0105, december 20. available online at<http://www.whitehouse.gov/omb/memoranda/m0105.html>.10this and related activities at omb were part of the context that led to this study.11for the full text of the act, see <http://www.epic.org/crypto/csa/csa.html>.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.144who goes there?resulting from the loss, misuse, or unauthorized access to or modificationof the information contained in such system.ógovernment paperwork elimination actpart of the impetus for federal agencies to move quickly toward electronic government (and therefore authentication, to an extent) is publiclaw. enacted in 1998, the government paperwork elimination act(gpea)12 both requires federal agencies to move from paperbased toelectronic transactions with the public and provides some of the enablersnecessary to make such a transition. it also amplifies federal privacyprotections regarding sensitive data collected during the electronic authentication process.following on the tradition of the paperwork reduction act (pra)13of 1995, one of the goals of gpea is to minimize the burden imposed onthe public by federal paperwork requirements. more specifically, though,the goal of both the pra and gpea is for federal agencies to minimize theinformationcollection burden on the public, regardless of whether thecollection instrument is a paper form, an electronic transaction, or a phonesurvey.14 gpea recognizes the benefits to both federal agencies and thepublic of moving from paperbased to electronic transactions, includingreduced error rates, lower processing costs, and improved customer satisfaction. as a result, gpea required agencies by the end of fiscal year2003 to provide for the electronic maintenance, submission, or transactionof information as a substitute for paper where practicable. additionally,the law stipulates that agencies use and accept electronic signatures inthis process.gpea goes so far as to define the term òelectronic signatureó and tolegitimate the legal force of such signatures in the scope of public interactions with federal agencies.15 in doing so, federal law and policy help toclear up what has historically been the subject of some debate amongfederal agencies about what is legally sufficient to òsignó a transactionwith a member of the public. section 1709(1) of gpea reads:the term òelectronic signatureó means a method of signing an electronicmessage thatñ(a) identifies and authenticates a particular person as the12government paperwork elimination act of 1998 (pl 105277, div. c, tit xvii).13paperwork reduction act of 1995 (44 u.s.c. chapter 35).14for background on the goals of gpea, see senate report 105355, and for backgroundon the pra and gpea, see omb, òprocedures and guidance; implementation of thegpea,ó federal register, may 2, 2000.15for more on electronic signatures, see the discussion of the electronic signatures inglobal and national commerce act (esign) of 2000 later in this chapter.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication, privacy, and the roles of government145source of the electronic message; and (b) indicates such personõs approval of the information contained in the electronic message.it is important to note as well what the definition does not do, whichis to specify the technologies or policies that an agency might use tocomply with this definition. the omb implementation guidance to federal agencies cites examples of appropriate technologiesñshared secretssuch as pins and passwords, digitized signatures or biometrics such asfingerprints, and cryptographic digital signatures such as those used inpkis.16 the omb guidance does, though, suggest an analytical framework for an agency to use to help determine the risk inherent in thetransaction it hopes to automate and which authentication technologymight most appropriately mitigate that risk.gpea also cleared up what might otherwise have been a contentiousdebate among federal agency general counsel offices throughout washington, d.c., by addressing directly the enforceability of electronic signatures. for transactions involving electronic records submitted or maintained consistent with the policy enabled by gpea and using electronicsignatures in accordance with the same policy, neither the electronicrecord nor the signature is to be denied legal effect just because it iselectronic instead of paper. both congress and the omb state that theintent is to prevent agencies or the public from reverting to paper insteadof electronic transactions and signatures because of concerns that anysubsequent prosecutionñin a benefits fraud case, for instanceñmight bethrown out of court.one other provision of the law pertinent to the topic of this studyrelates to the protection of information collected in the course of providing electronic signatures services. consistent with the fair informationpractices (described in chapter 3 of this report) and the privacy act,gpea requires that information gathered from the public to facilitateelectronic signatures services be disclosed only for that purpose.agency or programspecific law and policiesfamily educational rights and privacy actthe family educational rights and privacy act (ferpa) of 1974 (20u.s.c. ¤ 1232g; 34 cfr part 99) is a federal law designed to protect theprivacy of a studentõs education records. the law applies to all schoolsthat receive funds under an applicable program of the u.s. department of16see the web site <http://www.whitehouse.gov/omb/fedreg/gpea2.html>.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.146who goes there?education. ferpa gives parents certain rights with respect to theirchildrenõs education records (for example, the right to inspect and reviewall of the studentõs education records maintained by the school and theright to request that a school correct records believed to be inaccurate ormisleading). these rights transfer to the student, or former student, whohas reached the age of 18 or is attending any school beyond the highschool level.under the law, schools must also have written permission from theparent or eligible student before releasing any information from astudentõs record. schools may disclose, with notification, directorytypeinformation, such as a studentõs name, address, telephone number, dateand place of birth, and so on.17 as schools move toward authenticationtechnologies such as pki, issues arise as to how ferpa applies.18health insurance portability and accountability actthe health insurance portability and accountability act (hipaa),19or pl 104191, was passed by congress and became law in 1996. itspurpose was, among other things, to improve the continuity of healthinsurance coverage and the efficiency in health care delivery by mandating standards for electronic data interchanges and to protect the confidentiality and security of health information. title i of hipaa deals withhealth insurance access, portability, and renewability (for example, whena worker loses or changes his or her job), while title ii of the act containswhat are referred to as the actõs administrative simplification provisions.these provisions fall roughly into three categories: transactions and codeset standards,20 privacy standard,21 and security standard.22 the privacystandard, along with the security standard, provides rules for legal controls over patientsõ medical records.17the full text of the act is available online at <http://www.epic.org/privacy/education/ferpa.html>.18educause, an organization that promotes information technology in higher education, has looked at this and related issues in its initiative pki for networked higher education. see the web site <http://www.educause.edu/netatedu/groups/pki/> for more information.19the full text of the act is available online at <http://aspe.os.dhhs.gov/admnsimp/pl104191.htm>.20health insurance reform: standards for electronic transactions. 45 cfr parts 160 and162. federal register: august 17, 2000 (volume 65, number 160), pp. 5031250372.21standards for privacy of individually identifiable health information. 45 cfr parts160 through 164. federal register: december 28, 2000 (volume 65, number 250), pp. 8246182510.22security and electronic signature standards: proposed rule. 45 cfr part 142. federalregister: august 12, 1998 (volume 63, number 155), pp. 4324143280.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication, privacy, and the roles of government147the process for creating these standards, or rules, has been fairlycomplicated. indeed, according to the department of health and humanservices (hhs), the process is a òdeliberate [one] designed to achieveconsensus within hhs and across other federal departments.ó23 however, in general, once a proposed rule has made its way through severalfederal groups (such as the hhs data councilõs committee on healthdata standards, advisers to the secretary of hhs, and the omb), theproposal is published and comments from the public are solicited. thesecomments, which are also open to public view, are then òanalyzed andconsidered in the development of the final rules.ó24hipaa is aimed at all organizations that store, process, or transmitelectronic health care information through which an individual might beidentified. accordingly, the act applies to virtually all health care organizations, includingñamong othersñhealth plans (insurers), health careclearinghouses, health care providers (including health maintenance organizations, hospitals, clinics, physician group practices, and singlephysician offices), billing agencies, and universities.hipaa also provides for serious civil and criminal penalties for failure to comply with the rules. for example, the penalties include fines ofup to $25,000 for multiple violations of the same rule in one year, as wellas fines of up to $250,000 and up to 10 yearsõ imprisonment for knowinglymisusing individually identifiable health information.the privacy rule formally defines òprotected health information,ówhich includes individual patient information such as name, social security number, address, and so on; and clinical information such asdisease, treatment, drugs, test results, and so on. it permits disclosureof this information for necessary treatment, payment, and operations(tpo) functions. for all other uses, especially for fundraising and marketing functions, explicit authorization from the patient is required.(there are exceptions, such as for military patients and for clinical research, which is largely governed by the informed consent rule.) ifinformation is provided to an organization not covered by hipaa fortpo functions (such as a bill collection agency), the rule requires explicit business associate (and possibly chain of trust) agreements thatmake the recipients responsible for hipaaspecified privacy and security rules.the original privacy rule issued in december 2000 required collectingand tracking of a consent form signed by all patients that explained theprivacy practices of the institution providing care. subsequently, a techni23from the web site <http://aspe.os.dhhs.gov/admnsimp/8steps.htm>.24ibid.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.148who goes there?cal correction proposed in january 200125 removed the consent form requirement and replaced it with a privacy notice that does not require apatientõs signature. several basic rights for patients are provided in theprivacy rule: the right to access their records, the right to emend thoserecords, and the right to see what disclosures of their information havebeen made. institutions are required to employ a privacy officer, whoprovides services related to the privacy rights of patients.fundamental to the privacy rule are the òminimum necessaryó andòneed to knowó principles. based on these principles, the rule requiresinstitutions to develop and implement policies and procedures to defineformal rolebased or userbased access authorization rules, and the security rule requires assurance that these policies and procedures are beingfollowed. additionally, a common and uniform sanctions policy is required for addressing privacy and security policy violations. patient privacy has always been important in care institutions; the hipaa privacyrule formalizes the concept in a legal framework with significant penalties for noncompliance.there are several complexities in meeting hipaa regulations. if informationaccess rules are incorrectly defined, the care process could beadversely affected, an obviously unacceptable tradeoff. the roles of careproviders in an organization are fluid: nurses working in shifts or fillingin, oncall consultants rotating on a weekly basis, medical students onmonthly rotations, multiple physicians in consulting or specialty roles,and so on. practically, it is very difficult to assign roles to a fine accessgranularity and to implement such a system in mostly vendorsupportedand heterogeneous clinical application environments without raising therisks to proper health care.managing authorizations and tracking privacy notices are operationalchanges for institutions, but centrally tracking all disclosures for reviewby the patient if requested is a difficult and costly problem in large institutions. in the context of an academic medical center, for example, hipaaremains vague in addressing the matter of information collected for andby clinical trial and other kinds of research. open questions about educational rounds and hipaa were addressed in the latest rule making, andthere are other questions that may be clarified, but only later. the privacyrule was required to be adopted by april 14, 2003, but it is likely that therewill be a gradual culture change in care environments toward better privacy protection.25standards for privacy of individually identifiable health information; proposed rule.45 cfr parts 160 through 164. federal register: march 27, 2002 (volume 67, number 59),pp. 1477514815.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication, privacy, and the roles of government149regulation of private sector information management activityelectronic signatures in global and national commerce actaimed at eliminating òlegal barriers to the use of electronic technology to form and sign contracts, collect and store documents, and send andreceive notices and disclosures,ó26 the electronic signatures in globaland national commerce (esign) act (pl 106229) became law in june2000. the act cleared the way for electronic signatures and contracts tocarry the same legal significance as that of traditional paper contracts andhandwritten signatures.27 indeed, president clinton signed the act intolaw with a smart card.the esign act includes consumer consent provisions that òrequireinformation to be made available electronically only after the recipientaffirmatively consents to receive the information [in that manner].ó28 infact, recipients must give this consent electronically as well, to ensure thatthey possess the necessary technological capability (usually internet access and/or an email account). the act does not specify the use of anyparticular technological solution; rather, it òleav[es] those choices [up] tothe marketplace.ó29 however, some critics view this aspect of the act asbeing a very real disadvantage, fearing a òmishmash of incompatiblesolutionsó30 and a òstandards battle that could take years to resolve.ó31in 2001, the federal trade commission and the department of commerce completed a congressionally mandated study32 on the impact ofthe consumer consent provisions of the esign act. according to the26omb. òguidance on implementing the electronic signatures in global and national commerce actó (m0015), september 25, 2000. available online at <http://www.whitehouse.gov/omb/memoranda/m0015.html>.27electronic signatures, in this context, are not the same as the digital signatures thatwere described in chapter 5. one of the critiques of this law is that electronic signatures donot embody the same security methods and principles as digital signatures.28news bytes news networks. òesign law appears to work fine so farñgovõtstudy.ó june 27, 2001.29john schwartz. òesignatures become valid for business.ó new york times. october 2,2000, p. c1.30abby ellin. òesign on the dotted line.ó business 2.0, november, 2000. availableonline at <http://www.business2.com/articles/mag/0,1640,8542,ff.html>.31jesse berst. òsign of trouble: the problem with esignatures.ó zdnet, july 17,2000. available online at <http://www.zdnet.com/anchordesk/stories/story/0,10738,2604099,00.html>.32federal trade commission (ftc) and department of commerce (doc). electronic signatures in global and national commerce act: the consumer consent provision in section101(c)(1)(c)(ii). washington, d.c., ftc, doc, june 2001. available online at <http://www.ftc.gov/os/2001/06/esign7.htm>.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.150who goes there?report, the actõs consumer consent provisions seem to be òworking satisfactorily.ó the report also suggests that òimplementation issues [such assignature and authentication standards] should be worked out in themarketplace and through state and federal regulationsó rather than bycongressional action to òamend the statute.ófinancial services modernization actthe financial services modernization act of 1999 (commonly referredto as the grammleachbliley act) repealed longstanding legislation thatprohibited banks, securities firms, and insurance companies from venturing into business with each other. under grammleachbliley, these typesof companies may now join to form what the act calls financial holdingscompanies (fhcs). what this means with respect to personal privacy isthat, for instance, a consumerõs bank can now develop a relationship andshare information with that same consumerõs insurance company, brokerage firm, credit union, or other financial institution, creating new opportunities to crossmarket services.33 however, grammleachblileyalso contains provisions for protecting consumersõ personal information:(1) consumers must be given notice of a companyõs intent to share theirinformation with a third party and (2) they must be given the option todecline such information sharing.nevertheless, grammleachbliley is viewed by many privacy advocates as being rife with loopholesñto the point of rendering any privacyprotections that it spells out moot. for instance, dan gillmor, a technology columnist, describes what he views as two major problems with theact:¥consumers must opt out of information sharingñòthat is, [consumers must] explicitly notify the institutions that they [do not] wanttheir data sharedñrather than ôopt in,õ which is to allow data sharingonly after a consumer gives his or her permission.ó¥òaffiliated companies (such as those under the same corporateumbrella or in joint marketing deals), so broadly defined as to be almostmeaningless, are exempt in every respect.ó3433cecelia kempler and robert wood. 2000, òliving with the grammleachbliley act.ówashington, d.c., leboeuf, lamb, greene & macrae l.l.p. available online at <http://www.insurelegal.com/livingwith031500.html#1>.34dan gillmor. ògrammleachõs privacy problem.ó computerworld 35 (30)(2001): 34.available online at <http://www.computerworld.com/cwi/story/0,1199,nav4774sto62385,00.html>.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication, privacy, and the roles of government151business, on the other hand, takes a different view of grammleachbliley. indeed, most financial holdings companies, while taking intoaccount the additional resources and time they must allocate to meetingthe actõs privacy provisions, see the act as beneficial to their business.as federal reserve vice chairman roger ferguson put it in a recentspeech, grammleachbliley offers òopportunities for banking organizations to expand their lines of business and their range of customerservices.ó35nevertheless, there has been recent activity within state legislaturesto strengthen or òenhance the protections of grammleachbliley, including requiring actual consentñor optinñbefore information sharing,ó36breeding significant concern among financial firms at the prospect of having to account for up to 50 different state privacy laws along with grammleachbliley.37another challenge has been the requirement for banks and financialinstitutions to notify customers, in readable and understandable fashion,about their privacy policies. privacy advocates have noted that many ofthe policy notifications were written in hardtounderstand legal languageand/or distributed in a way that did not draw attention to what wasbeing disclosed.policy activity in the early 2000sthe last couple of years have seen a flurry of activity relating to bothauthentication and privacy issues. some of this legislation predated theterrorist attacks of september 11, 2001, but a great deal of the new legislation is a direct result of the perceived inadequacies of government information management practices leading up to the attacks. this and thefollowing section include recently enacted legislation whose implementation continues to be planned.35roger w. ferguson, òumbrella supervision: emerging approaches,ó speech beforethe national association of urban bankers, urban financial services coalition, san francisco, calif., may 26, 2001. available online at <http://www.federalreserve.gov/boarddocs/speeches/2000/20000526.htm>.36m. maureen murphy. privacy protection for customer financial information, congressional research service (crs) report for congress rs20185. washington, d.c., crs, library of congress, august 2001.37indeed, in june 2002, voters in north dakota approved a referendum that would barthe sale of personal data collected by banks, credit unions, and other financial services firmsto third parties. see p. thibodeau, òn.d. voters side overwhelmingly with privacy,ócomputerworld, june 12, 2002.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.152who goes there?usa patriot actpl 10756, uniting and strengthening america by providing appropriate tools required to intercept and obstruct terrorism act of 2001ñthe usa patriot actñgives federal officials broader authority to monitor communications and share information.38 enacted in response to theterrorist attacks of september 11, 2001, the actõs intent is to combat terrorism, and it encompasses among other things criminal and foreign intelligence investigations, money laundering, and alien terrorists. the mostsalient effects on individual privacy result from the increased surveillancerelated empowerment of government agencies.in general, there are four surveillance mechanisms provided by u.s.law: interception orders, search warrants, pen registers and trap and traceorders, and subpoenas. interception orders have given authorities a clearlydelineated process for eavesdropping electronically on telephone and facetoface conversations and electronic communications in serious criminalcases. search warrants allow the search of premises and the seizure oftangible things, including records. pen registers and trapandtrace orderssurreptitiously identify the source and destination of calls to and from aparticular telephone. subpoenas compel the production of evidence, including physical items and testimony. there are also differing standards ofproof for each of these mechanisms, based on whether domestic law enforcement or foreign intelligence agencies are conducting the surveillance.the usa patriot act has changed laws governing all four of themechanisms described above. it permits pen registers and trap and traceorders for electronic communications, as well as for phone conversations,and authorizes nationwide execution of some surveillancerelated courtorders. voice mail is treated like stored email, which gives it less protection than telephone conversations. the act allows interception of communications relevant to the investigation of a suspected computer trespasserand permits sneakandpeek search warrants and delayed (possibly forever) notification of the subject of the search. it also empowers the attorney general to acquire education records relevant to the investigation ofterrorist offenses and to collect dna samples from prisoners convicted ofcertain terrorismrelated offenses.the act reduces restrictions on foreign intelligence gathering (that is,on u.s. agencies gathering intelligence about other countries and theircitizens) within the united states and facilitates information sharing between foreign intelligence and domestic law enforcement agencies. the38see charles doyle, the usa patriot act: a sketch, congressional research servicereport rl21203, and charles doyle, the usa patriot act: a legal analysis, congressional research service report rl31377.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication, privacy, and the roles of government153act eased some of the restrictions on foreign intelligence gathering introduced during the 1970sñit now permits roving surveillance, expands theallowable circumstances for a search or surveillance under the foreignintelligence surveillance act of 1978 (fisa) (pl 95 511, 92 stat. 1783),39and sanctions courtordered access to any tangible item held by lodging,car rental, and storage businesses. roving surveillance means that warrants need not include specific information about the instrument or location of a search. unlike domestic law enforcement, fisa searches do notrequire probable cause of criminality; rather, they require only that thetarget be suspected of being an agent of a foreign government. information obtained from these searches may be shared with the fbi. likewise,domestic law enforcement agencies may also share certain information(e.g., criminal wiretap results) with foreign intelligence agencies.the usa patriot act also addresses the financial aspects of terrorism, particularly money laundering. it requires financial services professionals to file suspicious activity reports (sars) in certain circumstancesand also increases òspecial measuresó and òdue diligence requirementsórelated to foreign money laundering. as a result of the act, standards forcustomer identification and record keeping have become more stringent,and financial institutions are being encouraged to share information withlaw enforcement agencies.egovernment actthe egovernment act of 2002 (pl 107347) provides further impetusfor the government paperwork elimination act of 1998 to enable electronic government at the federal level. the act also authorizes increasedfunding for egovernment projects, creates an administrator for a new egovernment office within the omb, extends provisions of the government information security reform act of 2000 (pl 106398, subtitle g,¤10611065),40 provides uniform safeguards to protect the confidentiality39for more information on fisa, see <http://www.eff.org/censorship/terrorismmilitias/fisafaq.html> and the law itself at <http://www4.law.cornell.edu/uscode/50/ch36.html>.40the government information security reform act (gisra) of 2000 required agenciesto report annually to omb on the security of their information systems and to make information system security part of their regular process of doing business (e.g., in budget requests). under a sunset provision, gisra was originally intended to expire in november2002; however, the federal information security management act (h.r. 3844), which waslater incorporated into the egovernment act, made the gisra provisions mentioned abovepermanent. see judi hasson, òegov agenda takes shape,ó federal computer week, december 2, 2002. available online at <http://www.fcw.com/fcw/articles/2002/1202/newsegov120202.asp>.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.154who goes there?of information collected from the public for statistical purposes, and requires the issuance of governmentwide policies for standards for federalweb site usability and for records management for federal web sites.especially relevant for this report, the law includes provisions on electronic signature technologies and privacy impact analyses.the provision concerning electronic signature technologies is intended to promote the compatibility of agency solutions. executive agencies must ensure that their use and acceptance of electronic signatures tosecure electronic transactions with the federal government are compatible with the pertinent policies issued by the omb. the law further designates the general services administration (gsa) as the lead federalagency to create a framework for the interoperability of electronic signatures solutions, which is to include digital signatures.41the privacy provisions of the act recognize that more citizencenteredegovernment requires an exchange of personally identifiable information between users and federal agencies. in response, the act requires thatagencies conduct privacy impact statements when developing or procuring a new information system. while the act leaves the details of thecontent of privacy impact statements to the omb to develop, it is reasonable to assume that the omb will use the best practice of the chief information officers (cio) council as a starting point for this policy.42as of this writing, there are still many implementation details to beworked out, but this new law clearly provides more tools for agenciesthat seek to implement authentication technologies and to consider theprivacy implications of those decisions.homeland security act of 2002the homeland security act of 2002 (pl 107296) establishes a cabinetlevel department of homeland security, bringing together a myriadof federal agencies currently spread among a number of federallevelcabinet agencies and executive branch organizations. as a consumer ofintelligence data from the fbi and cia as well as a coordinator of thedissemination of such data throughout the federal government and tostate and local governments, the new department of homeland securitywill have significant information management responsibilities that haveboth authentication and privacy implications. much of this informationmanagement would appear to fall under the purview of the undersecretary41 gsaõs federal bridge certification authority is a move in this direction; for moreinformation, see <http://www.cio.gov/fbca/>.42see the cio councilõs web site for information on the irsõs implementation of theprivacy impact statement as a best practice. available online at <http://www.cio.gov/documents/piaforitirsmodel.pdf>.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication, privacy, and the roles of government155for information analysis and infrastructure protection; the delegation ofsuch responsibilities requires more scrutiny.summaryin recent years, the desire for highly integrated electronic government is driving government organizations toward interagency and intergovernmental authentication technology and policy solutions.43 the access certificates for electronic services (aces) system, described later inthis chapter, is an example of such a proposed solution. however, complications arise when this type of solution is undertaken in the publicsector. for example, public sector authentication solutions often involvesignificant roles for private sector trusted third parties, complicating rolesand responsibilities and therefore accountability. in addition, singlesignon approaches allow interaction with multiple government agencies, butmight lead to many of the privacy concerns cited in this report. it is notclear that the current privacy policy framework is sufficiently robust orflexible to provide the privacy protections needed to accommodate theseapproaches (see chapter 3 in this report for more on the current state ofprivacy law and policy in the united states). while there may be a desirefor a simple, secure, privacypreserving means by which citizens can interact with multiple government agencies, it is difficult to satisfy all ofthese criteria simultaneously. indeed, as is clear from the discussion inthis chapter, privacy law and policy in the united states tend to be without overarching or apparent unifying principles. the lack of cohesiveness in the legal and policy framework could lead to gaps, inconsistencies, and overlaps, making compliance difficult and potentially moreexpensive.government as issuer of identity documentsthe preceding sections addressed the first role of government, asregulator, and this section discusses its second role with respect to authenticationñas an issuer of identity documents, often in conjunctionwith the private sector. anyone who has traveled on a commercial airlinesince september 11, 2001, has a sense of the unique role that governmentfills in issuing identification documents. the airlines, enforcing regulations issued by the federal government, are quite clear in their instruc43see computer science and telecommunications board, national research council, information technology research, innovation, and egovernment, washington, d.c., nationalacademy press, 2002, for a broad look at egovernment innovation and approaches that canhelp accelerate innovation in government.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.156who goes there?tions; passengers must present a ògovernmentissued photo id.ó44 aphoto id issued by a travelerõs employer is not sufficient. it must begovernmentissued. the government is able to compel individuals tohold certain ids and follow specified processes in order to help ensure theintegrity of the id issuing process. as a result, governmentissued idsare presumed to be of higher quality than those issued by private organizations (although fraudulent government ids are demonstrably not hardto come by). (see box 6.1 for a general discussion of credentials presentedby those wishing to be authenticated.)the integrity of any authentication system that relies on a governmentissued identifier depends on the integrity of a small number offoundation id documents issued by government organizations. what issomewhat surprising, though, is how circular the process of either obtaining, getting a duplicate, or even retiring a governmentissued id is andhow involved in the process the private sector is. as discussed below,hospital staff play an integral role in recording birth information that isused for the issuance of birth certificates and then social security numbers. it is also possible to get printed copies of birth certificates fromprivate sector companies working on behalf of local governments.45 atthe other end of the continuum of life, private funeral directors often issuedeath records for sharing with a variety of government organizations.the first identity document issued for most nativeborn u.s. citizensis a birth certificate. birth and death records are maintained by municipalgovernments. many births (all of those outside public hospitals) occurwithout the presence of any government employee. many a new parenthas faced the admonition from hospital staff not to leave the hospitalwithout naming the new child, making it possible for the city or countyoffice of vital records to record the birth and issue a birth certificate. inthe case of a small local government, a town clerk may serve this function.it is interesting to note, however, that the hospital staff play a role indocumenting the birth for purposes of establishing the identity of thechild and his or her relationship to its parents. a delivery room nurse andthe physician who delivered the child are likely to sign the paperworknecessary to obtain the birth certificate. the presence of a nonmedicalgovernment employee during childbirth would, in fact, probably be considered to constitute a significant invasion of privacy.44see the web site <http://www.tsa.gov/workingwithtsa/travel.shtm> for federal ruleson governmentissued ids for flying commercially. john gilmore is presently challengingthese regulations in federal court. see <http://cryptome.org/freetotravel.htm> for a chronology of that suit.45see, for example, the web site <http://www.vitalchek.com/provideroverview.asp>for information on obtaining birth certificates and other vital records.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication, privacy, and the roles of government157box 6.1on the nature of credentialsunderstanding the nature of credentials is an important component of understanding authentication technologies, processes, and systems, since the òverifieróin an authentication transaction verifies credentials presented by the òpresenter.ócredentials may be bound in some way to the individual to whom they were issued, or they may be bearer credentials. the former are necessary for identification, while the latter may be acceptable for some forms of authorization. a driverõslicense or a passport is an example of the former, while an admission ticket for anentertainment event or a discount coupon is an example of the latter. cash is atrue bearer credential with very good anticounterfeit protection. a credential intended to be bound to a specific individual should effect the binding in some waythat can be checked by a verifier; otherwise it risks becoming a bearer credential.most driverõs licenses, employee id cards, and all passports include a photo toallow a human verifier to determine if the individual presenting the credential is theone to whom the credential was issued. machineverifiable credentials may bebound to bearers through use of personal identification numbers or biometrics.most credit cards do not include a photo, even though they are credentialsintended for use by a specific individual. instead, most credit cards contain asignature strip, and verifiers (merchants) are supposed to compare the purchaserõs signature with that on the card for what are referred to as òcard presentó transactions. this signatureverification approach to user authentication is generallypoorer than the photo id approach, and it is not always used by merchants, especially in the united states. a growing number of credit card transactions are conducted as mail order or telephone order (moto) transactions or internet transactions, and in these cases neither a photo nor a signature is available to the verifierto be checked. to help address this deficiency, credit card verification generallyentails an online check. this is necessary in part because the credit card, as anauthorization credential, is tied to data that cannot easily be maintained on thecredit card, for example, the outstanding purchase total relative to the cardholderõscredit limit.this example points to another aspect of credential systems: offline versusonline verification. thanks in part to ubiquitous networking, credit cards have effectively become online verification systems, which they were not when they werefirst used. online verification may be effected simply by querying a database usingan identifier (for example, a credit card number), or it may involve a complex interaction between the credential and its issuer (for example, as many smart cardsoperate). online verification also is attractive in that it supports rapid revocation ofcredentials. many credentials are issued with an explicit period of validity. theymust be periodically renewed. the issuer can revoke a credential by refusing torenew it, but this is not a very responsive way to revoke a credential. if failure torenew is the only way to revoke a credential, the issuer must trade off the costs ofmore frequent renewal against accepting the costs imposed by delaying revocation until the next renewal period. if the issuer can physically recall a credential, anintermediate form of revocation is possible; but in a world where an increasingnumber of transactions are not conducted in person, physical revocation is oftennot a viable option. (if one tried to use an invalid credit card in a store, it might be(continues)who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.158who goes there?it is also important to note that not all births occur in hospitals. besides the proverbial delivery in a taxicab, one must consider home birthsand so on. the u.s. constitution defines anyone born in the united statesas a citizen, whether or not he or she is born in a hospital (at the time theconstitution was written, far fewer children were born in hospitals), sothat (hypothetical) controls on birth certificates based on hospital licensing would miss some number of children. in effect, the issuance of a birthcertificate approximates the issuance of an identity certificate at birth.in the past, human witnesses could serve to authenticate someoneõsidentity in the way that the birth certificate is meant to do now. childrentended to be born at home, in the presence of witnesses from the localcommunity. they grew up among family and neighbors, many of whomremained available for later testimony as to the identity of the eventualretained or destroyed by the merchant, but card confiscation is not possible whenthe transaction is conducted via the phone, mail, or internet.)for any physical credential, there is always a concern about how easily thecredential can be forged or altered. if one can readily modify a credential or createa bogus credential and have a verifier accept the credential as valid, then thecredential system has failed. for credit cards, knowledge of a legitimate creditcard number (plus expiration date and billing address) is sufficient to effect mostmoto transactions. credit card account numbers (and expiration dates and addresses) cannot be wellprotected secrets, because they must be transmitted tomerchants to effect transactions. this points out a fundamental deficiency of credit cards as credentials in the moto environment: the information needed to poseas the legitimate cardholder is not a wellprotected secret. ancillary measures areadopted by merchants to counter credit card fraudñfor example, reliance on automatic number identification (ani) for phone orders placed to toll free numbers, andshipping only to the billing address associated with an account.an obvious security concern for physical credentials is the ability of a verifierto detect forgeries. many driverõs licenses and current u.s. passports includeantitamper and anticounterfeiting measuresñfor example, holograms that are designed to make it easy for a human verifier to determine if the credential is legitimate. many credit cards also make use of holograms, to raise the bar againstgeneration of fake physical credit cards. if legitimate credentials come in manyforms, the verifier is less likely to be able to spot fakes. birth certificates exhibit thisproblem (among others), since there are more than 17,000 jurisdictions that mayissue these documents, and the formats vary widely. machineverifiable credentials ameliorate this problem, but they are typically more expensive to create, andthe cost of deploying verification technology also creates barriers to deployment.against this backdrop, one can examine various forms of credentials to seehow they rate. for example, a driverõs license is an identity credential for a namedbox 6.1continuedwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication, privacy, and the roles of government159individual. it carries a photo of the individual to whom it was issued and is designed for offline verification by a human. it can be physically revoked by a lawenforcement officer or officer of the court. because of wide variability in formatsand anticounterfeiting measures, forged licenses may be hard to detect, especiallywhen the license does not purport to be from the state in which it is being verified.a license typically contains datañfor example, home addressñwhich are not wellmaintained on the credential and which are not generally essential to the primaryfunction for which the license was issued.a combination of a user id and a password constitutes a bearer credential inpractice, even when the intention is otherwise. authentication takes place over anetwork, and any binding to an individual is based on the assumption that the userdid not share the password and that the pair was not guessed by an attacker.most credit cards are essentially bearer credentials, although that is not theintention. cards that bear a photo of the cardholder offer added protection in cardpresent transactions but do not improve the security of moto transactions. counterfeit cards, even ones that make use of holograms, have been produced bythieves, demonstrating the limits of current, anticounterfeiting measures.a smart card with a photo id is a hybrid form of individual credential designedfor both human verification and machinebased, typically online, verification. thehuman verification aspect of such cards is vulnerable to tampering attacks, except tothe extent that anticounterfeiting measures are applied. the machine verificationaspect of these cards can be of very high quality: that is, creating a fake public keycertificate that would be accepted by the verifier can be made infeasible from amathematical perspective. however, it may be possible to covertly acquire the private key and certificate of a legitimate individual from his or her card and insert theminto a smart card with another individualõs photo, thus allowing the second individualto pose as the first for both human and machine verification purposes. this illustrates the difficulty of developing very high assurance credential technology, althoughtechnology of this sort does pose significant barriers to counterfeiting.adult. in other words, family and neighbors could testify to continuity ofidentity despite biometric changes in the identified person. the currentbirth certificate is meant to serve as a substitute, but it has several flaws ofits own.4646there are many contexts in which human witnesses serve as verifiers of identity ofeither people or objects. for example, chains of custody in court cases require humanwitnesses to testify to the provenance of something admitted into evidence. human witnesses can, of course, authenticate other individuals; this requires establishing the authorityand veracity of the witness. this report does not investigate the pros and cons of humanwitness testimony, as the committeeõs charge was to examine authentication technologies,but certainly there are situations in which human witness testimony may be the best (oronly) method available for verifying identity.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.160who goes there?there is, of course, a human element in the issuance procedure, sothere are errors both of commission (for example, not accounting properly for stillborn children or creating fraudulent documentation of aòbirthó that never occurred) and omission (childbirths that go unrecorded).47 as in all automated systems, there must be a manual processfor handling these errors, and the impact of the errors must be considered. in addition, in order to ensure that the document refers to thecorrect individual when it is issued, it is important to issue the initialidentity document as close to birth as possible. this begs the question ofwhether biometrics of some sort are needed, which raises some interesting problems with respect to the many biometrics (including footprints,which are standard in some locales) that might be used as part of anidentity document for a small child. photographs of newborn infant facesare difficult to use for anything other than determining the race of thechild, if that. eye color changes over the first year of life, and newbornshave not yet acquired their permanent hair color. while fingerprints areset at birth, there are some important technical limitations: the ridges areso close together that a standard 500dotsperinch scanner does not haveenough resolution to scan them accurately, and todayõs algorithms do nothandle finger growth, which causes the ridges to spread farther apart.the social security administration (ssa) has some nearly uniquetechnical requirements for its system of authenticating people. parentsare given a strong incentive to get an ssn for their child before filing thefirst federal income tax return after the childõs birth: they must do so toclaim the child as a dependent (this is a relatively recent change). hence,the ssn is likely to be issued near the time of birth. while some childrenhave large enough unearned incomes to require paying taxes, many children will have no use for their ssn until they are teenagers getting theirfirst summer jobs. the first technical requirement, therefore, is that although the binding between a person and his or her ssn will sit dormantfor approximately 15 years, it must routinely be verifiable at that latertime. very few commercial systems have this property: if they have not47there are other randomization processes at work in addition to errors in documentation. it is known, for instance, that on rare occasions parents have taken the wrong childhome from a hospital nursery. as discussed already, family mobility is a randomizer forthe early childhood years. a child whose parents move frequently while the child is maturing can lose all witnesses to his or her identity continuity except for his or her parents, andif those parents should die or otherwise not be acceptable as witnesses, that child wouldhave no witnesses to identity. as individuals urbanize and become more mobile, if theonly method of establishing identity is through human witnesses to identity continuity, itmay not be possible to establish identity with close to 100 percent confidence. put anotherway, it may be becoming easier for someone to infiltrate an urbanized nation under anassumed identity.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication, privacy, and the roles of government161interacted with a customer for 15 years, very few, if any, businesses arerequired (or want, for that matter) to keep that customer record. untilrecently, most people had little or no contact with the ssa during theirworking years. now, people receive annual account statements (previously called personal earnings and benefits estimate) from the ssa. ifthe statement is correct, there is no need for followup, so individualsmay still go three to four decades between their first contributions andtheir next interaction with the ssa, when they claim benefits after retirement. hence the second technical requirement: the authentication technology needs to work with absolutely minimal interaction over spans of50 or more years.48 this requirement suggests that authentication systems that must persist for long periods of time will have to support renewal, as systems (presumably) change over time.49a third technical requirement for the ssa system of authenticatingpeople is that the system must work for the entire population, not just asignificant percentage. as interaction with the ssa, the irs, and othergovernment agencies is legally mandatory, not being able to authenticateto them is not an option. either the authentication system must work foreveryone (for example, ruling out biometrics that have significant failurerates for reasons beyond peopleõs control), or there must be a nononerous exception mechanism. declaring that certain sets of people are notworth serving (or going to great lengths to serve) is an option in mostcommercial contexts, but not for the government. if the choice is to include an exception mechanism, it will need to be designed in from thebeginning, and the security implications of the exception mechanism willneed thorough analysis, since experience indicates that security problemsare often found in littleused parts of systems.finding 6.2: electronic authentication is qualitatively differentfor the public sector and the private sector because of agovernmentõs unique relationship with its citizens:a.many of the transactions are mandatory.b.government agencies cannot choose to serve only selectedmarket segments. thus, the user population with which theymust deal is very heterogeneous and possibly difficult toserve electronically.48it should be noted that no cryptographic system other than the onetime pad has remained secure for 50 years in modern history.49note that this may mean more than just changing key sizes, as is done for some authentication algorithms; other cryptographic parts of the system, such as hash functions,may need replacing.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.162who goes there?c.relationships between governments and citizens are sometimes cradle to grave but characterized by intermittent contacts,which creates challenges for technical authentication solutions.d.individuals may have higher expectations for government agencies than for other organizations when it comes toprotecting the security and privacy of personal data.this finding echoes the analysis in cstbõs 2002 report informationtechnology research, innovation, and egovernment,50 which described special considerations in egovernment. ubiquity (access for everyone, anywhere, anytime) and trustworthiness (citizens will not tolerate unauthorized or accidental disclosure of personal information) were described asareas in which government leads in demand for technologies.the tangled web of governmentissued identity documentsit is overly simplistic to view government as monolithic. it may turnout that each agency picks its own authentication technology and deploysit among the user base that the agency serves. some agencies may voluntarily agree to use another agencyõs authenticators. in fact, most modelsof egovernment presume a degree of integration among governmentagencies and between levels of government.51,52 making such electronicinteractions work seamlessly for users will require a degree of interoperability for both policy and technology. to this end, as noted in thediscussion of the egovernment act, the federal government is sponsoring the deployment of a pki that will authenticate transactions across thefederal government and (potentially) with state governments as well. inmany cases, the government will both issue and rely on the authenticator.as discussed below, once an individual has one of the widely recognizedgovernmentissued ids, he or she can obtain other governmentissuedids. in essence, government acts as a certificate authority (ca) of sorts inthe issuance of these documents. (figure 6.1 illustrates the interdependencies of foundational identity documents.)50computer science and telecommunications board, national research council. information technology research, innovation, and egovernment. washington, d.c., national academy press, 2002.51for example, see k. layne and j. lee, òdeveloping fully functional egovernment: afour stage model,ó government information quarterly 18: 122136, and janine s. hiller andfrance b”langer, òprivacy strategies for electronic government,ó egovernment 2001, marka. abramson and grady e. means, eds. new york, rowman and littlefield, 2001.52see also the white paper òfederal electronic government infrastructure: the eauthentication gatewayñconnecting people to services,ó available online at <http://www.cio.gov/eauthentication/presentations/authenticationgatewaywhitepaper.pdf>.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication, privacy, and the roles of government163issuance of an ssn requires proof of age and of citizenship or appropriate noncitizen status and a current proof of identity. for adults, theproof of identity is generally another governmentissued photo identification, although it need not be: nonphotographic government recordssuch as marriage and divorce records and adoption records are alsoaccepted. additionally, some private sector identification cards areaccepted: for example, employer and school id cards (usually with photographs), along with insurance policies and id cards (usually withoutphotographs). for a child, proof of identity is based on a parentõs vouching for the childõs name, possibly indirectly through the hospitalõs registrar and/or county/state officials.a california state id card (for identification purposes, equivalent to adriverõs license), for example, is issued upon presentation of a birth certificate, passport, or expiring id card and proof of ssn. as noted earlier,it is difficult to link the birth certificate to the person applying for the idcard. in the united states, state id cards are the dominant form of photoidentification. most private organizations rely on state id cards for everyfigure 6.1interdependencies of foundational identity documents issuedby both governments and the private sector.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.164who goes there?day transactionsñfor example, paying by check, setting up a video rentalaccount, and so on.acquiring a u.s. passport requires a birth certificate, a prior passportor other proof of citizenship, and current proof of identityñthat is, alocal, state, or federal governmentissued photo identification, includingstate id cards and u.s. passports. an ssn must be stated on the passportapplication. the passport is sufficient to request any of the three otherforms of identification. in addition, a passport is sufficient proof of identity and authorization to satisfy the u.s. department of justice i9 form.53alternatively, the i9 is most commonly satisfied with a state id card anda social security card. other possibilities (for example, other military orgovernment id cards) also exist but are used relatively rarely. most largeemployers will issue their own photo identification, which is occasionallyused outside the place of employmentñfor example, for verifying eligibility for various corporate discounts.the forms of identification described above have several noteworthyfeatures. birth certificates are generally issued by municipal jurisdictions,state id cards by states, and social security cards and passports by differentagencies within the federal government. weaknesses in various documents, particularly the difficulty of binding a birth certificate to a specificperson, are addressed by the business practices of the various agencies. ingeneral, a birth certificate alone is insufficient evidence for the generation ofother identity documents. in the united states, each agency has its ownpolicy for what it accepts, while in australia this concept has been formalized in the ò100 points of proofó model.54 the formalization occurs throughthe assignment of points to different identifying documents. for instance, acurrent australian passport is worth 70 points, a bank statement is worth 40points, and employment records are worth 10 points. different servicesrequire different point totals. the decentralized nature of this arrangementmeans that no single entity has a completely authoritative database. theevidence required for the initial governmentissued identity document, thebirth certificate, is often attested to by private sector employees. it shouldalso be noted that the united states is a nation of immigrantsñdocumentsprepared overseas may introduce even more uncertainty into the system.55all of these factors contribute to the difficulty that the relying party mayhave in verifying the documents.53the i9 form is required to be completed by all new employees so that their employercan verify their employment eligibility.54see the web sites <http://www.centrelink.gov.au/internet/internet.nsf/multifilestores/poi0110t/$file/poi0110en.pdf> and <http://www.whittlesea.vic.gov.au/enquiries/eservuser.asp>.55note the case of danny alimonte, born in the dominican republic. his eligibility toplay little league baseball came into dispute during the 2001 little league world series. itwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication, privacy, and the roles of government165as the authentication environment changes over time, with fewerpeople to attest to the provenance of a person and more and more authentication happening electronically, it will be necessary to revisit the security of foundational identity documents. todayõs systems have workedreasonably well, but it will become increasingly easy for correlated information about an identity to be acquired by someone with malicious intent. for many purposes, birth identity becomes largely irrelevant later inlife. birth certificates, which are bearer credentials, suffer from the problems inherent in all bearer credentials. however, stronger alternatives,such as dna, are very expensive, may be unpopular with large segmentsof society, and raise new privacy and technical challenges.recommendation 6.1: birth certificates should not be reliedupon as the sole base identity document. supplemented withsupporting evidence, birth certificates can be used when proofof citizenship is a requirement.threats to foundational documentsas noted previously, governmentissued identity documents are thefoundational documents relied upon by many authentication systems inboth the public and private sectors. any analysis of such systems musttherefore take into account the threat model faced in the issuance and useof those documents.56 only after the threats against the reliedupon documents are understood can the threat faced by the system under analysisbe considered. as discussed in chapter 4, in order to evaluate the security of any authentication system, one first needs to define the threat facedby the system.57was unclear whether danny was 12 or 14 years old. dominican republic birth records fromthe mid1980s yielded inconsistent answers. he was eventually declared ineligible.56this discussion applies as well to the generation of identity documents that does nottake place under governmental purview. however, given that in practice many authentication systems do ultimately rely on a governmentissued identification document, the discussion is in that context.57it is worth remembering that authentication is often the first step in authorization. themany authorization policies that different government and private sector parties may haveare outside the scope of this report. however, from a privacy perspective, it is often betterto handle authorization directly, rather than as a function of identity (verified throughauthentication). for example, one can anonymously watch a movie: one goes to the ticketwindow, purchases a ticket with cash, gives the ticket to the ticket collector, and enters thetheater. no record is ever made of the identity of the moviegoer. on the other hand, manymoviegoers voluntarily give up some amount of privacy to purchase tickets with a creditcard, possibly beforehand, over the telephone or the internet.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.166who goes there?the attacks against generation systems for traditional foundationaldocument fall into these general categories:¥obtaining a fraudulent identity document,¥passing off someone elseõs valid identity document as oneõs own,¥modifying the contents of a valid identity document,¥compromising private information stored in a backend system,and¥unauthorized modification of information stored in a backendsystem.motivations for these attacks can vary, of course, ranging from thedesire to purchase alcohol when under age to the desire to move easilythrough security checkpoints in order to perpetrate a terrorist act. attacks could be aimed at individuals (as in the case of identity theft) or atundermining public and private institutions. the disparity of possiblemotivations highlights another way (see chapter 4) in which secondaryuse is dangerous: namely, it makes it difficult to determine the dangerposed by a security breach or attack. given the myriad uses of driverõslicenses, for example, it is incredibly difficult to ascertain the danger froma successful attack against a motor vehicles department database. each ofthese attacks is discussed in more detail below.¥fraudulent identity documents. these are a major problem in todayõssystems. the problem includes both external threats (for example, someone getting a driverõs license with someone elseõs birth certificate andssn) and internal threats (for example, a corrupt clerk at the dmv issuingdriverõs licenses without checking supporting documentation). it is worthnoting that this kind of fraud happens only on a small scale (in terms ofthe percentage of identity documents issued), but it is a relatively easyway for a determined person to circumvent the system.¥imposters. one can attempt to pass off someone elseõs identity document as oneõs own. regardless of how the identity document is acquired(for example, a driverõs license stolen in transit or a lost wallet found onthe beach), the technical problem is the binding of the document to itsholder. photo identification represents a primitive biometric aimed atsolving this problem. it has the advantages of being selfcontained andrequiring no infrastructure to support verification. the problem is thatfaces can change dramatically over time, and some identity documentshave long lives (for example, with automatic renewal, the picture on adriverõs license can easily be 8 or more years old).¥document modification. identity documents may be fraudulently altered (for example, the substitution of photographs on a passport orwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication, privacy, and the roles of government167driverõs license). preventing this sort of attack requires technical measures in the identity document (for example, holograms on driverõs licenses or embossed seals on passport photos). document modification isan attack on the integrity of the document itself; in the digital world,cryptographic integrity techniques such as message authentication codes(macs) and digital signatures, when properly used, provide strong protection against such attacks.¥compromising confidential information. many information systemsstore confidential personal information that is not physically present onthe identity document. for example, california and new jersey driverõslicenses show only the mailing address of their holder, not the streetaddress, if two different addresses are on file. however, an attackercompromising the relevant database could learn the street address of anylicense holder.¥modifying computerized records. given that most identity authentication systems have computerized data records and that additional information may be stored in those systems, one also must be concerned aboutmodification of that information. for example, driverõs license suspensions in some states are handled by electronically marking the appropriate record as having suspended driving privileges, while the licenseholder is permitted to retain the physical license (and hence to continue touse it as stateissued photo identificationñfor example, for cashingchecks). if an attacker changed this flag, either someone could drive whoshould not be driving, or someone innocent could be caught driving witha supposedly suspended license. the backend database, not the physicaldocument, is the arbiter of license status.moving to digital credentials will not change these basic categories offraud. depending on the technology chosen for authentication, the distribution of fraud among these categories may change. for example, the useof cryptographic integrity techniques for digital credentials would makedocument modification extremely difficult, if not impossible, assumingthat the technology is properly implemented.a major change between traditional authentication and digital authentication is the scale of likely fraud. with todayõs systems, one of theprimary weaknesses relates to the validity of a specific instance of anidentity document and permeates all of the first three categories above(fraudulent documents, imposters, and document modification). however, controls generally work well enough to prevent the widespreaddissemination of fraudulent identity documents. as we move forwardinto a world of digital identity documents, the issuing process is stillextremely important. all the cryptography in the world cannot overcomeweakness in this step, because cryptographic notions of trust (and validwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.168who goes there?ity) are necessarily relative: they can only be as trustworthy (in the bestcase) as something else. the hope is that this trust chain is firmly anchored by a statement acknowledged as true in the real world. it isunfortunate that all too many people (including those who should knowbetter) have a tendency to trust as accurate anything that the computersays, even in situations where they would not trust a paper document.the ability to generate fraudulent identity documents on a small scaletends to have a minor impact on the overall system. however, in a digitalworld, the compromise of secret informationñfor example, of a signingkey or other methods of accessing the documentissuing systemñcouldopen the way to massive issuance or use of fraudulent identity documents. the compromise of backend systems today is already a problemfor the last two categories of threats (compromising information andmodifying records). one has to consider the difference in speed of propagation of security breaches. electronic issuance of identity certificates cango orders of magnitude faster than the issuance of paperbased identitycertificates.finding 6.3: many of the foundational identification documentsused to establish individual user identity are very poor from asecurity perspective, often as a result of having been generatedby a diverse set of issuers that may lack an ongoing interest inensuring the documentsõ validity and reliability. birth certificates are especially poor as base identity documents, becausethey cannot be readily tied to an individual.finding 6.4: scale is a major factor in the implications of authentication for privacy and identity theft. the bulk compromise of private information (which is more likely to occur whensuch information is accessible online) or the compromise of awidely relied on documentissuing system can lead to massiveissuance or use of fraudulent identity documents. the resultwould adversely affect individual privacy and private and publicsector processes.recommendation 6.2: organizations that maintain onlineaccessible databases containing information used to authenticatelarge numbers of users should employ highquality information security measures to protect that information. whereverpossible, authentication servers should employ mechanismsthat do not require the storage of secrets.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication, privacy, and the roles of government169government as relying party forauthentication servicesgovernment, in addition to issuing identity documents, is also a relying party (that is, it makes payments, allows access to records, and distributes benefits electronically based on claims made by users) for authentication systems to administer public programs at all levels (federal, state,and local). in fact, the government faces some unique challenges as arelying party, owing to its large size and multifaceted nature. it should benoted that government revenues and expenditures are an order of magnitude larger than those of the largest private corporations in the unitedstates. the governmentõs customer base is everything from an individualcitizen to neighborhood nonprofit organizations to large, multinationalcorporations. in some cases, the interactions between government andvaried customers are for the same purposeñfor example, paying incometaxes. in other cases, the interactions are very differentñfor example,military procurements tend to come from government contractors, notfrom private citizens.additionally, the functions of government are spread among a multitude of federal, state, county, and local agencies. government units areorganized by function (health and welfare, defense, education, publicworks, and so on), regardless of how people might interact with them.for example, a small businessperson such as a farmer probably has interactions and transactions with several agencies within the federal governmentñthe department of agriculture, the internal revenue service, thedepartment of labor, and perhaps the small business administration.at the state level, state tax, labor, and agriculture agencies may have theirown set of reporting requirements and benefits but may also pass alongsome programs funded by the federal government.there is a belief, held mainly by information technology and publicadministration professionals, that applications of technology through egovernment could reorient public organizations, causing them to becomemore responsive to the needs of users.58 as discussed earlier, gpea isdriving federal agencies to move information and transactions to theinternet. for many public sector organizations, though, the move to egovernment was already under way before the enactment of gpea gavestatutory impetus to federal agency efforts. through the office of man58for some selected visions of egovernment, see the national association of chief informationofficers, online at <http://www.nascio.org/publications/digitalgovernmentreport2001.pdf>;council for excellence in government, online at <http://www.excelgov.org/usermedia/images/uploads/pdfs/thenextamericanrevolution.pdf>; or omb egovernment strategy, online at <http://www.whitehouse.gov/omb/inforeg/egovstrategy.pdf>.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.170who goes there?agement and budget, the federal government has identified 24 egovernment projects that might offer a variety of electronic services betweengovernment and citizens, government and business, and government andgovernment (that is, between federal, state, and local governments), andseveral administrative efforts that are for internal use in federal agencies.59 for the most part, the task force that recommended this list toomb found that these efforts had been under way for some time andpredated gpea.cutting through the organizational complexity, though, requires adegree of consistency in policy, management, and technology that is rarelyfound in the paperbased world. many government agencies, most notably some leading federal agencies, are investing heavily in pki as themeans to deploy an electronic authentication system that will work universally for users of government programs.the next three subsections describe ways in which the governmenthas tried to authenticate citizens in different contexts. the first is adetailed discussion of a programñaccess certificates for electronic services (aces)ñthat the federal government had endorsed as a way toauthenticate users across a variety of program and organizational lines,the second describes the internal revenue serviceõs electronic tax filingprograms, and the third describes the social security administrationõsattempt at remote authentication for access to earnings and benefits statements. brief concluding remarks follow.access certificates for electronic servicesaces is a program instituted by the gsa. the programõs primarypurpose is to provide a pki to facilitate secure online citizen transactionswith government agencies.60 under aces, a user acquires a public keycertificate by interacting with one of a small number of selected, commercial cas. these cas commit to certification policies and procedures consistent with a model established by gsa for this purpose. this procedureis intended to ensure uniform quality of user authentication and statuschecking for federal agencies that act as relying partiesñthat is, that acceptthese certificates to identify users.the user employs a certificate issued by an acesapproved ca andthe corresponding private key when engaging in transactions with participating government agencies. federal agencies developing pkienabled59see the web site <http://www.egov.gov/egovreport3.htm> for a more detailed description of these 24 initiatives.60the general services administration is a federal management agency that sets policyin areas related to federal procurement, real estate, and information resources.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication, privacy, and the roles of government171applications are encouraged to take advantage of the certificate authentication module (cam)ña gsasupplied and mitretekdeveloped softwareñto verify an aces certificate prior to use. the cam is designed toperform the requisite certificate validation checks, relieving applicationdevelopers of the need to implement this complex pki software. thecam always verifies the revocation status of an aces certificate by contacting an online certificate status protocol (ocsp) server operated bythe ca that issued the certificate. (the rationale for adopting this specificrevocationstatuschecking mechanism is described later.)to acquire a certificate, a user provides a ca with some personalinformation to verify the userõs claimed identity. the standards for thisverification are established by gsa and thus are uniform for all of thecas providing the service (within the procedural security limits that thesecas enforce). the form of identity established through this interaction isa name.the identification provided by this type of interaction generally willnot be sufficient to identify the user uniquely to a government agency,since many users may share the same nameñfor example, john smith.thus, aces certificates generally will be ambiguous relative to the idrequirements for any government agency. an agency may identify a useron the basis of both a name and an ssn or other parameters (for example,home address, age, birth date, and so on.) thus, when the user contacts anagency for the first time with his or her aces certificate, the user willneed to provide this other information to an agency server to establish thecorrespondence with the userõs record in the agency database. if this isthe userõs first contact of any form with the agency, the agency will needto verify the supplied information as, for example, the ssa does by consulting with other government records. this procedure needs to be repeated by the user when he or she initially contacts an agency. eachagency must then find a means for binding the aces certificate to theuser identity in that agencyõs databaseñfor example, on the basis of theca name and serial number of the certificate or a hash of the public keyfrom the certificate. (the ca name and serial number are unique to theuser, but they will change whenever the user renews the certificate, because a new serial number must be assigned to every certificate issued bythe ca, even if the user merely renews the certificate. this proceduresuggests that users may have to reauthenticate themselves periodically toeach agency when the userõs certificate expires, using whatever means theagency employed to effect the initial binding between an aces certificateand its records. if the hash of the public key of the certificate is employed,similar problems arise whenever the user changes his or her key pair.)under the terms of the aces program, neither the user nor the government pays the ca for issuing an aces certificate. instead, every timewho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.172who goes there?an individual uses a certificate in a transaction with a government agency,the agency pays. the government agency pays the issuing ca for therevocation status check (via ocsp, usually invoked by the cam), thusproviding the financial motivation for cas to issue these certificates.aces avoids the need for government agencies to make upfront investments in establishing a pki to support this sort of egovernment service.it also avoids the need for these agencies to act as cas. instead, theagencies pay for the pki on a sort of installment basis, indefinitely. (thisarrangement is analogous in many ways to the government allowing aprivate company to build a toll road and then collect the tolls, forever.)it has been suggested that aces is an appropriate way to enablecitizen egovernment because the technical aspects of ca operation exceed the capabilities of most government agencies. however, since thecertificates issued by the cas are not sufficient to identify individualsuniquely relative to agency database records, each agency ultimately actsas a registration authority (ra) when it establishes the correspondencebetween the certificate holder and the database records. the ra function,while less technical than that of the ca, is usually the most securitycritical procedure of ca operation, so agencies have not avoided the needto participate in pki management as a result of aces. arguably, theagencies have databases that are ideal for identifying their users to thegranularity required to ensure authorized access to records and to effectauthenticated transactions. thus, the use of commercial cas to issue usercertificates does not relieve government agencies of the burden of performing this securitycritical function. it is true that ca operation doesrequire specialized technical capabilities, and the aces program avoidsthe need for agencies to acquire these capabilities. however, it is not clearthat an agency with the it resources needed to create and operate pkienabled applications could not also operate a ca for the users that itserves by means of these applications.the internal revenue serviceñelectronic tax filingthe irs has been working to increase the volume of the electronicfiling of individual tax returns since the program began in the late 1980s.while irs efile has been described as a pioneer program in electronicgovernment, it is interesting to note that for many years the irs requiredthat electronically filed returns be accompanied by paper signature documents. only since 1999 has the irs begun to make the efile program atotally paperless process, including electronic authentication, for someselected taxpayers.fortunately for the irs, there is public law and policy that supportselectronic authentication. the basic requirement in the internal revenuewho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication, privacy, and the roles of government173code is that tax returns be signed. however, the law does not specifywhat constitutes a signing and, in fact, treasury regulations give the irscommissioner broad discretion to determine what constitutes a signing.additionally, the irs reform and restructuring act of 1998 (pl 105206)speaks directly to the issue of electronic signatures and provides that theyare criminally and civilly equivalent to paper signatures.there is only one direct channel for the public to efile with the irs.through the telefile program, the irs òinvitesó taxpayers to participateby getting a specially designed tax package. invitations go to taxpayerson the basis of expected eligibility (as a 1040ez filer, with income lessthan $50,000, or as a single filer with no dependents). the package includes instructions for how to file using a touchtone phone and a customer service number (csn), which is a fourdigit pin. irs relies on thecsn used by the taxpayer to sign the return, but that does not authenticate the transaction, since the csn is not a unique identifier. the irsauthenticates the transaction by comparing data elementsñthe csn, dateof birth, taxpayer identification number (generally an ssn), and a namepresented by the taxpayerñto those same data elements maintained inirs databases.what makes authentication for irs efile somewhat challenging is therole of intermediaries between the irs and the taxpayer. in addition tothe direct provision of service through telefile, the irs relies extensivelyon intermediaries to deliver its electronic filing products to the public.generally, over half the individual tax returns filed with the irs are prepared by tax preparers such as commercial services, certified public accountants, and enrolled agents. tax preparers do an even larger percentage of the returns prepared for efile, a program that emerged out of apartnership between the irs and h&r block. a subset of preparers,authorized efile providers, are authorized to efile individual tax returnsfor their clients. the authorization, in this case, refers to the fact that theirs regulates the preparers that can efile in some detail.prior to 1999, the only way for a taxpayer to sign a return filed througha preparer was to fill out a paper signature document, called a jurat,which the preparer also had to sign and then send to the irs within 48hours of the irs accepting the return electronically. the requirement forthe preparer to file the jurat with the irs is contained in irs revenueprocedures governing the behaviors of authorized efile providers, whichalso require them to exercise due diligence in verifying the identity oftaxpayers by requesting forms of identification to help validate claimedidentity. similarly, the taxpayer who used personal computer or webbased tax preparation software prior to 1999 had to complete a jurat andsend it to the irs after the return was acknowledged as accepted. (as awho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.174who goes there?side note, the return can only be transmitted to the irs through a thirdparty transmitter authorized by the irs.)beginning in 1999, the irs built on experience of the telefile programto issue efile customer numbers (ecns) to those individuals who hadfiled their returns using webbased or personal computer tax preparationsoftware the previous year. much as with telefile, these selected taxpayers got a sealed postcard that explained program eligibility and containedone csn (two for joint filers).as part of a parallel pilot in 1999, those taxpayers using an authorizedefile provider could avoid the use of a jurat. in the presence of thepreparer, taxpayer(s) would select their own pin(s), to be used to sign thereturn. irs revenue procedures required that the taxpayers physicallyenter their selfselected pins on the preparerõs keyboard. as part of thesigning process, the preparer and taxpayers also record the pin(s) andother data from the return on a worksheet that both the preparer andtaxpayer(s) retain.in both of the 1999 electronic signature efforts, much as with telefile,the irs used the pinlike fourdigit number (for example, csn, ecn,selfselected pin) to sign the returns. this procedure meets the legalrequirement for a return to be signed. used in combination with otherdata presented by the taxpayer(s), the irs is able to authenticate the transaction to ensure that the taxpayer is who he or she claims to be. the needfor such authentication results from the use of a fourdigit pin that is nota unique identifier. additionally, authentication beyond the signing ofthe return is necessary because of the business risks associated with refund fraud. the irs refers to the need for òrevenue protection.ó61 sincethe initial offering in 1999 and 2000, the irs has evolved its electronicauthentication efforts for taxpayers filing from home using the web, taxpreparation software and/or the services of a tax preparer. the primarydifference now is that the irs no longer mails out the ecn to home filersand instead allows taxpayers to selfselect a pin for signing purposes.additionally, the signing is bound to the transaction by the taxpayer(s)providing information from the previous yearõs tax return like adjustedgross income (agi) so the irs can validate claimed identity of the taxpayer beyond name, address, and taxpayer identification number.61given that over 70 percent of individual tax returns result in a refund and that there isa history of individuals trying to defraud the government by seeking refunds they are notentitled to, this is a significant business risk. it is interesting to note that the shift frompaperbased to electronic signing altered the irsõs ability to prevent some refund fraud.for instance, the use of the csn in the case of telefile and the ecn in the first 2 years ofthat effort, in conjunction with other identifying data, provides an extra check upfront thatis not possible with paperbased signings.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication, privacy, and the roles of government175the social security administration and pebesone of the more infamous cases of privacy involving authenticationcolliding with egovernment capabilities comes from the ssaõs onlinepebes initiative.62 in 1997, the ssa had to bring down its otherwisesuccessful webbased capability allowing individuals to request and ultimately receive a personal earnings and benefits estimate statement(pebes) over the web. the ssa took this action after a usa today articleand subsequent stories in other national news outlets raised concernsabout the how the ssa authorized the release of a pebes to an individualelectronically (described below). although pebes provided dramaticallyimproved service through reduced cycle times and cost per transaction,ssa yielded to congressional pressure and suspended the service owingto the public outcry resulting from the national media coverage.historically, ssa had provided pebes to those who made a requestover the phone or by mail if the requester produced three identifiers(ssn, date of birth, and name), without validating if the requester wasreally the person related to that record. for instance, it was quite possiblethat a wife could have provided the requested information and obtainedher husbandõs pebes using that business rule. using this process, ssafilled literally millions of requests per year for pebes by mail and overthe phone.to improve service and reduce the workload of the shrinking ssastaff, the organization launched an effort to fulfill requests for pebesthrough a selfservice application over the web. initially the ssa provided partial electronic service, taking the request electronically but reverting to paper by mailing the report to the address provided in therequest. over time, and after considering the results of pilot testing andsome risk analyses, the ssa launched the fully interactive version bywhich the pebes was delivered back to the requester electronically. asan acknowledgment that moving this kind of transaction (even just thepebes request portion) to the web might entail more risk, the ssa addedmore data elements to the identifiers used in the knowledgebased authentication that had been used for the previous 25 years. the webbasedselfservice application would now require the requester to provide placeof birth and motherõs maiden name in addition to the three elementslisted above.62zachary tumin. òsocial security on the web: the case of the online pebes.ó strategiccomputing & telecommunications in the public sector. boston, john f. kennedy school ofgovernment, harvard university, 1998.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.176who goes there?the fully interactive pebes was up for approximately 1 month beforethe press depicted the offering as òsocial insecurity.ó the concern expressed by the press and by several senators who wrote to the commissioner of the ssa soon after the story broke was that the data elementsused in the knowledgebased authentication system might well be knownto people other than the owner of the data (and of the related pebesreport). more than even disgruntled spouses (or exspouses), close friendsor other individuals who might be able to assemble the required dataelements could gain access to the pebes that they were not entitled to.63the three examplesñaces, the irs and electronic tax filing, and thessa and pebesñillustrate the complexity of authentication and securityrequirements and privacy. the irs and the ssa have different threatmodels and different security and privacy requirements, demonstratingonce again that monolithic solutions, even at the federal level, are unlikely to be satisfactory.nationwide identity systemsthe federal government is not the only government body that plays arole in authentication and privacy considerations. it is through localgovernments that most individuals acquire identification documents.state governments also play a key part in individual authenticationñforexample, in the issuance of driverõs licenses by state departments of motor vehicles (dmvs). the committeeõs first report, idsñnot that easy,64examined the concept of a nationwide identity system, raising numerouspolicy and technological questions that would need to be answered ifsuch a system were to be put into place. in such a hypothetical system,government would likely fill all three roles: regulator, issuing party, andrelying party.as noted in idsñnot that easy, state driverõs licenses already constitute a largescale (nationwide and, in some cases, international) system ofidentification and authentication. earlier in this report, it was noted thatsecondary use of state driverõs licenses and state ids raises significantprivacy and security concerns. recognizing the ease with which suchdocuments can be fraudulently reproduced or obtained, there have beenproposals to strengthen driverõs licenses. the american association of63for ssaõs own analysis of pebes, see òprivacy and customer service in the electronicage: report to our customers,ó available online at <http://www.ssa.gov/reports/service/>.64computer science and telecommunications board, national research council. idsñnot that easy: questions about nationwide identity systems. washington, d.c., national academy press, 2002.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.authentication, privacy, and the roles of government177motor vehicle administrators is in the process of developing and proposing standards to do that.65 they include provisions for the use of biometrics to tie a driver to his or her license; some states already require fingerprints. as with any system that uses biometrics, however, care must betaken to mitigate threats against the resulting database.finding 6.5: stateissued driverõs licenses are a de facto nationwide identity system. they are widely accepted for transactions that require a form of governmentissued photo id.finding 6.6: nationwide identity systems by definition create awidespread and widely used form of identification, whichcould easily result in inappropriate linkages among nominallyindependent databases. while it may be possible to create anationwide identity system that would address some privacyand security concerns, the challenges of doing so are daunting.recommendation 6.3: if biometrics are used to uniquely identify license holders and to prevent duplicate issuance, care mustbe taken to prevent exploitation of the resulting centralizeddatabase and any samples gathered.recommendation 6.4: new proposals for improved driverõs license systems should be subject to the analysis presented inthis report by the national research councilõs committee onauthentication technologies and their privacy implicationsand in the earlier (2002) report by the same committee: idsñnot that easy: questions about nationwide identity systems.concluding remarks government organizations, especially federal agencies, must livewith a plethora of legal and policy demands and guidelines in the area ofauthentication and privacy, as well as provide accountability and submitto oversight. while citizens demand ease of use, they also expect securityand privacy protection for the information that in many cases they arerequired to provide to the government. reconciling this tension is acontinuing challenge for any institution, but especially for the government, owing to its unique role and requirements. this report emphasizesthe need to avoid authentication or identification when mere authoriza65more information is available online at <http://www.aamva.org/idsecurity/>.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.178who goes there?tion will suffice. in the case of government, respecting the legitimatefunction of anonymity is even more crucial. given the often obligatoryrelationship between citizen and government, allowing anonymity andtherefore increased privacy protection when possible not only increasesefficiency (by avoiding the need to employ complicated authenticationmachinery before a transaction) but also enables the many advantages ofprivacy protection described in chapter 3. (a simple example in whichauthentication is not required for interaction with the government is thedownloading of tax forms. the identity of the person downloading certain forms does not need to be verified before the forms are made available. the same holds true for many public records.)finding 6.7: preserving the ability of citizens to interact anonymously with other citizens, with business, and with the government is important because it avoids the unnecessary accumulation of identification data that could deter free speech and inhibitlegitimate access to public records.egovernment is a driver for authentication and privacy solutionsthat place greater emphasis on government as a relying party than as anissuer of id documents. systems that depend on a common identifier ingovernment are subject to the privacy risks associated with the potentialfor inappropriate data aggregation and (inadvertent or deliberate) information sharing in ways that the individual providing the information didnot expect. care must be taken to adhere to the principles in the privacyact of 1974 and the privacy principles described in chapter 3 of thisreport.finding 6.8: interagency and intergovernmental authenticationsolutions that rely on a common identifier create a fundamentaltension with the privacy principles enshrined in the privacyact of 1974, given the risks associated with data aggregationand sharing.finally, while this chapter emphasizes many of the unique constraintsunder which government must operate, government is not immune to thechallenges faced by the private sector when developing authenticationsystems, many of which are touched on in the preceding chapters. threatmodels must be understood before proceeding, and the goals of any authentication system should be well articulated.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.1797a toolkit for privacy in thecontext of authenticationthe preceding chapters provide an indepth look at authentication,abstractly and with respect to particular technologies, as well as anoverview of privacy and a look at governmentspecific issues related to authentication and privacy. this concluding chapter provides atoolkit that can aid in designing an authentication system that is sensitiveto privacy concerns. it focuses on the three types of authentication identified in chapter 1:¥individual authentication is the process of establishing an understood level of confidence that an identifier refers to a specific individual.¥identity authentication is the process of establishing an understoodlevel of confidence that an identifier refers to an identity. the authenticated identity may or may not be linkable to an individual.¥attribute authentication is the process of establishing an understoodlevel of confidence that an attribute applies to a specific individual.authentication systems using one or more of these techniques aregenerally deployed to meet one of two goals:¥limiting access. authentication may be used to limit who enters oraccesses a given area/resource and/or to control what they do oncegranted entrance or access.¥monitoring. authentication may be used to enable monitoring ofsystem use. this may occur regardless of whether decisions about accesswho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.180who goes there?to or use of resources are being made on the basis of authentication. suchauthentication is conducted to support realtime and retrospective uses,including audits to assess liability or blame, to mete out rewards andpraise, to provide for accountability, or to make behaviorbased decisionssuch as those made by marketers.privacy issues arise in all systems that exercise control over access ormonitor behavior, regardless of the method of authentication used. asdescribed in chapter 3, the decision to authenticate, whatever the reason,may affect decisional privacy, bodily integrity privacy, information privacy, and communications privacy. as noted earlier, affecting privacy isnot always equivalent to violating privacy. without delving into thenormative decisions surrounding what is an appropriate level of sensitivity to privacy, this chapter describes how choices made at the outset ofsystem design and deployment can have baseline privacy implicationsthat should be taken into account.the choice of attribute, identity, or individual authentication is a substantial determinant of how large an effect on privacy the authenticationsystem will have. however, for cases in which the resource to be protected is itself private information or something else to which access mustbe controlled in order to protect privacy, a privacyinvasive authentication system may be necessary and appropriate. such an authenticationsystem also may be warranted in other contexts for other reasons. thus,examining the privacy consequences of authentication technology is bestdone in tandem with evaluating the nature of the resource that the authentication system is deployed to protect.as mentioned above, access control can be supported by proving thatone is allowed to do something or by proving that one is not on a list ofthose prohibited from doing something. this proof can be provided using attribute, identity, or individual authentication methods. for example,sensitive areas in a workplace are frequently limited to those individualswho can prove that they are on the list of those permitted access. thisproof may come in the form of an attribute authentication system (theemployee has a property that permits access), an identity authenticationsystem (the identification number given the employee permits access), oran individual authentication system (the individuals on this list are permitted access). in contrast, it is common for bars and nightclubs to haverules about those individuals who may not enter. when individualspresent themselves for entry, those who possess certain traits (such asbeing underage or being someone who is not welcome by the owner) maynot enter. the under21 criterion uses an attribute authentication system,and a driverõs license or other ageverification documents are used tomake agebased decisions about entry. an individual authentication syswho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.a toolkit for privacy in the context of authentication181tem, albeit usually a lowtech one, is used to prohibit from entering thosewhom the owner has indicated are not welcome.an attribute authentication system deployed in either of the contextsdescribed above (employment or bar) need not maintain a database. ineach situation, the decision to permit or deny entry is based on an attribute that, as far as the authentication system is concerned, any individual may possess. in contrast, an identity or individual authenticationsystem in these two contexts potentially is implemented quite differently.1in the employment context, the identity or individual authentication system must contain a record on everyone who is allowed to enter. in the barcontext, the identity or individual authentication system will only containinformation on those who cannot enter (such as the list of people theowner has indicated are unwelcome and the fact that those under 21 arenot allowed in). an important consequence flows from this limitation. inthe employment scenario, the system easily allows for additional controlsover the individual once he or she enters the building, and it potentiallysupports monitoring of those within the system even where such monitoring is unrelated to decisions about access to or use of a resource. in thebar scenario, on the other hand, the system put in place generally will notprovide any means for controlling or monitoring those who enter basedon the way they authenticated themselves.an authentication system designed to limit the access of a specificgroup of individuals has no further privacy consequences for those not onthe initial list if the system is designed so as to limit its function to its goal.these examples illustrate that the privacy implications of authenticationsystems stem from implementation and system design choices and notnecessarily from the reasons for which the authentication system is neededor the form of authentication technology employed. in the next section, adetailed toolkit is presented for thinking through how different choices inthe design of an authentication system can have an impact on privacy.privacyimpact toolkitthe choice among an attribute authentication system, an identity authentication system, and an individual authentication system bears sub1in considering the distinction between identity and attribute authentication, note thatidentity authentication, which assumes the existence of an identity and a unique identifier,allows for the creation of longitudinal records. in attribute authentication, if the attributechosen is sufficiently distinctive it is functionally equivalent to an identity authenticationsystem, in which case the attribute may be more accurately labeled an identifier, therebyeroding the protections that might otherwise be provided by an attribute authenticationsystem.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.182who goes there?stantially on the privacy consequences of the system. viewed independently of the resource they are designed to protect, attribute authentication systems present the fewest privacy problems and individual authentication systems the most. nevertheless, in some instances individualauthentication systems may be appropriate for privacy, security, or otherreasons.separate from the type of authentication, the overall scope of thesystem will have obvious implications for user privacy. to limit effectson the privacy of users, systems should collect information on the fewestindividuals possible. not all accesscontrol decisions contemplate or require auditing. while many accesscontrol systems, particularly thosethat control access to sensitive or valuable information or resources, explicitly call for auditing, it is possible to design a system that supportsaccess control but not auditing.2 where auditing is not contemplated ornecessary, the scope of the system should be narrowed. for example, ifauditing is not needed, then once a decision to permit access or action isrendered, there may be no reason to store and maintain data about thedecision and many privacy reasons to destroy it.in general, when developing an authentication system, several questions must be answered that go beyond the scope of the system and whattype of authentication will be used. decisions will need to be made aboutwhich attributes to use, which identifiers will be needed, which identitywill be associated with the identifier, and how the level of confidenceneeded for authentication will be reached. the answers to each of thesequestions will have implications for privacy. below, the four types ofprivacy described in chapter 3 (information, decisional, bodily integrity,and communications) are discussed in the context of each of the abovequestions. the analysis proposed here is technologyindependent, for themost part, and can be applied to almost any proposed authenticationsystem.attribute choiceattribute authentication and, frequently, identity authentication andindividual authentication require the collection or creation of attributesthat the system uses to determine whether to grant an individual accessduring the authentication phase. in an attribute authentication system,the attribute alone will be the thing being authenticated. in an identityauthentication system, the identifier will correlate to some collection of2for example, auditing may not be necessary when controlling access to theaters, amusement parks, or other òonetime pay to enteró locales.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.a toolkit for privacy in the context of authentication183information that the system considers to be an identity. the identity maybe nothing more than an email account that bears no obvious relation tothe underlying individual and the password that accesses it (in otherwords not john.mulligan@example.com, but abracadabra@example.com).in an individual authentication system, however, the identity, which potentially includes attributes as well as personal information, is distinctiveto a given individual. for example, when a driverõs license is initiallyissued, an effort is made to bind the driverõs license number (nothingnecessarily individual about it at this point) to an identity that is distinctenough to be linked, in theory, to the individual who requested the license. part of the identity comprises attributes such as eye and hair color,height, weight, a photographic image of the individual, and so on.information privacyto analyze how the choice of attribute(s) may implicate informationprivacy, it is useful to consider the fair information principles detailed intable 3.1 in chapter 3.several characteristics of an attribute may be related to collecting theminimum amount of information needed for authentication. for example,the more distinctive the attribute is in relation to the individual, the easierit will be to establish the necessary level of confidence that the attributeapplies to a specific individual; conversely, this may increase the potential for privacy problems. when a large group of individuals is allowed toaccess a resource, the selection of a unique attribute may inappropriatelycreate opportunities for revelation of the individual to whom the attributepertains. the selection of an overly distinctive attribute in such a situationwould violate the minimization principle. however, the selection of aunique attribute may be appropriate where attribute authentication is beingused to limit access to an individual or a small set of individuals. forexample, the use of a highly distinctive attribute to control access to personal information about an individual maintained by a third party maymeet the minimization principle and be necessary to protect againstinappropriate access to the personal information in question.regardless of whether the choice of a highly distinctive attribute isappropriate, the more sensitive or revealing the attribute is, the greaterthe information privacy problems raised. thus, greater attention must bepaid to protecting against misuse and disclosure. similarly, the moreattributes collected for authentication (regardless of whether they are appropriate), the greater the information privacy problems raised. clearlythere are tradeoffs between the privacy implications an attribute posesand that attributeõs security value. ideally, attributes should be selectedthat minimize the privacy effect and maximize the security potential.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.184who goes there?in selecting an attribute, the quality of the data represented shouldalso be examined. the attribute should be relevant to the system. forexample, organizational role might be an appropriate attribute in an employment context but not in a retail context; eye color might be appropriate for physical access systems but not online systems. if an attribute issubject to change, then in some circumstances it may not be a good attribute to select because its quality may be compromised. for example,hair color may be a poor choice of attributes if the goal is to limit access toindividuals whose hair is naturally a given shade. in other circumstancesthe changeable nature of the attribute may improve its value as an authentication attribute. for example, the use of lastdeposit or lastwithdrawal information in a financial context as an attribute may meet thedataquality standard despite its variable nature. the fact that the valueof these attributes changes frequently means that ongoing system compromise is less likely if the value is guessed or stolen.the accuracy of an attribute should also be considered. differentsystems may tolerate different levels of accuracy. in general, to enhanceprivacy protection, a system should select an attribute that is relevant,accurate, and fresh. if the levels of accuracy, relevance, and reliability ofan attribute are high, the number of attributes can be minimized.in selecting an attribute, system designers should also consider howwidely it is used in other systems. if an attribute is widely used, it canmore easily facilitate secondary uses and record linkages from and toother systems. a less widely used attribute (ideally an attribute unique tothe system) is less likely to serve as a link between the records of disparatesystems. in addition, if an attribute is unique to a system, and the attribute space is sufficiently large and the attributes are randomly distributed in that space, then the system is less vulnerable to outside attacksbased on attribute guessing. to limit the likelihood that its value will becompromised, an attribute used for authentication purposes should notbe used for other system purposes. for example, any attribute used as anidentifier in a system (perhaps an account number) is likely to be exposedto a wide range of individuals and/or system elements and thus is a poorchoice as an authentication attribute.in order to protect privacy, the security level that the system accordsan authentication attribute should be consistent with the value of theattribute, as well as the value of the data that can be accessed on the basisof knowledge of the attribute. if an attribute is sensitive or unique, itsvalue to the individual may go well beyond its value to the system as anauthenticator. the data subjectõs valuation of the attribute and the consequent security that it should be afforded may not be immediately obviousto system developers or users.to better protect information privacy (and in accordance with fairwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.a toolkit for privacy in the context of authentication185information principles), once an attribute is selected, individuals shouldreceive clear notice about whether information regarding that attributewill be retained in a separate authentication system of records, what theuses of that system are, who has access to it, and what rights the individual has with respect to accessing the system. the system should alsospecify how controls on the attribute authentication system will be enforced and to whom the system is accountable.decisional privacythe choice of attributes may affect decisional privacy. in addition toraising information privacy problems, the choice of a sensitive or revealing attribute(s) may also affect the individualõs willingness to participatein the system for which authentication is sought and to engage in activities that might result in the collection or generation of additional sensitiveor revealing information. examples of attributes that are themselves revealing or sensitive are political party, religion, and weight.bodily integrity privacybodily integrity privacy may also be affected by the choice of attributes. for example, the collection of blood in order to ascertainblood type as an attribute, or of dna in order to screen for a geneticattribute raises two types of privacy issues that have implications forbodily integrity. first, the collection of the attribute may be physicallyintrusive or invasive. second, once collected, the attribute may revealadditional information about an individualõs physiological or psychological condition (such as a predisposition to certain diseases), as wellas information about an individualõs recent activities (such as pregnancy or drug use).communications privacyif identifiers such as network or communication system addresses (oreven phone numbers) are mislabeled and used as authentication attributes, communications privacy can be implicated. these addresses canfacilitate collection and analysis of information about the individual thatcan be correlated with other records.summary of attribute choice discussionthis analysis indicates that an attribute selected for an authenticationsystem that minimizes privacy implications should:who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.186who goes there?¥not be unique to an individual unless tightly controlled access isrequired,¥not be widely used in other systems,¥not be sensitive or revealing,¥be relevant to the system, accurate, and fresh,¥require no physical contact,¥entail obvious (as opposed to covert) collection, and¥not be related to communication activities.identifier selectionidentity authentication and individual authentication systems bothuse identifiers to tie individuals to an identity within the system. bothsystems require the selection or construction of an identifier, such as aname, a random number, or a tax id number. the choice of or creation ofan identifier raises privacy concerns.information privacyto analyze how the choice or creation of an identifier may implicateinformation privacy, consider once again the fair information principlesin table 3.1 in chapter 3.the principle of limiting the collection of information is raised by theselection or construction of an identifier. first, the minimization aspect ofthe collectionlimitation principle requires that efforts be made to limitthe collection of information to what is necessary to support the transaction. the selection of an identifier that in itself has meaning, is important,or is revealing (if unnecessary to the underlying purpose of the transaction) would violate this principle. an effort should be made to use identifiers that are not themselves personal information. thus, randomnessand system exclusivity are valuable traits in an identifier. as discussedabove, these traits are valuable from the perspective of system security aswell. an identifier that is created or constructed for the purpose ofauthentication in that one system will offer more protection for both privacy and security than will an identifier selected from or based uponexisting identifiers.because the identifier is being selected for its capacity to link to theindividual in the context of an individual authentication system, the information privacy concerns are greater than they are in attribute andidentity authentication. to best protect privacy, identifiable informationshould be collected only when critical to the relationship or transactionthat is being authenticated. the individual should consent to the collection, and the minimum amount of identifiable information should bewho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.a toolkit for privacy in the context of authentication187collected and retained. the relevance, accuracy, and timeliness of theidentifier should be maintained and, when necessary, updated. restrictions on secondary uses of the identifier are important in order to safeguard the privacy of the individual and to preserve the security of theauthentication system. the individual should have clear rights to accessinformation about how data are protected and used by the authenticationsystem and the individual should have the right to challenge, correct, andamend any information related to the identifier or its uses.the privacy question related to how involved an individual shouldbe in the selection or creation of an identifier is an interesting one. itwould appear at first that allowing the individual to select an identifierwould maximize the individualõs control and involvement and allow thatperson to establish a desired level of privacy. yet studies on users indicate that individuals are likely to select an identifier that they can easilyremember and, in most cases, an identifier that they use elsewhere or thatis related to some personal information (see chapter 4 for more on usability). a random identifier, created exclusively for use in that system,will provide more protection from an information privacy perspectivebut will be more cumbersome for the individual to use and remember.however, technical mechanisms can be employed to minimize theseinconveniences.3decisional privacyan identifier that is randomly created and used exclusively for aparticular authentication system will pose fewer adverse implications fordecisional privacy than an identifier that reflects or contains personalinformation. the selection of an identifier that can be linked to the individual is likely to pose greater risks to decisional privacy than the selection of an attribute or identifier that cannot be linked. such a systemwould not provide for anonymous or pseudonymous participation. instead, it will be possible to associate a particular individualõs involvementwith the activity. depending on the activity, it is possible that the selection of an identifier linked to the individual will cause some individualsnot to participate.3for example, in web access contexts, a different public key certificate can be created foruse with each web site, and browser software can automatically interact with the site toselect the right certificate when the site is visited. this affords a high degree of privacyrelative to linkage concerns, and it can provide a very convenient individual authenticationinterface.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.188who goes there?bodily integrity privacyidentifiers, unlike attributes, generally do not represent a characteristic of an individual and thus are not likely to affect bodily integrity. theselection of an identifier that could be associated with physical characteristics or physical activities of an individual may affect bodily integrity ifthe collection of the identifier was physically intrusive, invasive, or intimidating.communications privacycommunications privacy is affected if the identifier is the individualõsnetwork or communication system address or number (telephone number, email address, ip address, and so on). if the identifier is related tothe communication activities of an individual, its collection raises questions of communication privacy, because it would enable linking betweenauthentication and communication activities. for example, if the identifier could be linked both to the individual and to the communicationsactivities of the individual (phone number or email address), it couldsignificantly compromise communications privacy. this informationwould be valuable to both the system collecting the information and alsoto those outside the system, especially for law enforcement and otherinvestigative purposes. to minimize privacy implications and enhancesecurity, it would be best if the identifier used in an authentication systemis not related to communications. however, if the system for which access is being authenticated is a communications system, then use of acommunications identifier would be appropriate, as it would be information that is germane to the system.summary of identifier selection discussionthis analysis indicates that an identifier selected for an authenticationsystem that also minimizes privacy implications should:¥be unique to the system (possibly random),¥not be widely used,¥not be sensitive or revealing,¥require little or no physical contact,¥entail obvious (as opposed to covert) collection/assignment, and¥not be related to communication activities.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.a toolkit for privacy in the context of authentication189identity selectionin both identity and individual authentication systems, identifiers areoften associated with other data records. even in an individual authentication system that does not have as a primary goal the creation of anidentity record, these data records constitute a de facto identity of theentity pointed to by the identifier. there are, accordingly, three types ofindividual/identity authentication systems, each with different privacyconcerns:1.purely individual authentication systems. in these systems, the identifier itself is the only information about the entity available to the system;no additional data records are associated with the entityõs identifier. inthis case, the privacy analysis above regarding the selection of an identifier applies directly.2.selfcontained identity authentication systems. in these systems, anentityõs identifier is linked to data records that are held within the systemand contain identity information about the entity; this information mayinclude the history of the entityõs access to the system. in these systems,an entityõs identifier is not linked to information about the entity outsidethe system. for example, a systemspecific number might be assigned toeach entity in the system. in this case, no new privacy issues are introduced.3.nonselfcontained identity authentication systems. in these systems,the identifier used by the system to refer to an entity is also linked to datarecords that are held outside the system and contain identity informationabout the entity. for example, a system of this type might maintain anentityõs credit card number, which is linked by credit agenciesõ externalsystems to the entityõs transaction history and credit rating. in this case,new privacy issues arise; these issues are explored below.information privacyif the system identity is associated with a particular individual, all thefair information principles should be honored in order to best protectprivacy. an authentication system that is organized in such a way that aparticular individualõs privacy may be compromised requires the following: the individualõs knowledge and consent; collection of the minimumamount of information and publication, including specific examples, ofthe level of collection required; that the information be relevant, accurate,timely, and complete; that information collected be used only for thespecific purpose for which it was collected, unless the individual consentswho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.190who goes there?or a valid court order is issued; that the individual have the right toaccess, challenge, correct, and amend information; and that the system bemaintained with the highest level of security. all of the issues related toidentifier selection and information privacy remain present in the contextof identity selection.decisional privacyviewed independently of context, individual authentication systemspose the greatest risk to decisional privacy. the creation of transactionalinformation about authentication events that is closely tied to a givenindividual has the greatest potential to have a chilling effect on individuals who do not want their identity associated with an activity or organization. similarly, individually identified transactional records about authentication events are more likely to be reused by third parties (lawenforcement, private litigants, hackers, and so on). individually identified records are more easily repurposed and reused.bodily integrity and communications privacythe discussions in the òidentifier selectionó section above about issuesrelated to bodily integrity and communications privacy also apply here.the authentication phasethis phase determines whether the attribute, identifier, or identityrefers to the individual being authenticated at the level of confidencerequired by the system. this determination is usually accomplished byobservation of the individual or by challenging the individual to producesomething supporting the claim. (for example, requiring a card and pinat an atm, requiring a badge to be swiped as the bearer enters a building,and requiring a password at an ecommerce web site are all authentication phases within their respective systems.)information privacywhether records of the act of authentication are kept, including, forexample, time and date logs, implicates information privacy most directly. if such transactional records are keptñfor example, to provide anaudit trail for security purposesñthen the system should minimize theamount of information collected. the individual should also be notifiedthat the information is being kept as part of a system, how long it will bekept, who has access to the system, and what other uses may be made ofwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.a toolkit for privacy in the context of authentication191the information. the individual should be able to access the record andcorrect or amend it if necessary. the system should limit the retention ofrecords containing information about authentication acts as well as secondary uses of or access to such records.decisional privacythe intrusiveness and visibility of the authentication phase may alsoaffect decisional privacy. the attribute or identifier itself may not beparticularly revealing or sensitive, but the process for verification may beso revealing as to inhibit someone from participating. if the individual ischallenged to produce something supporting the attribute or identityclaimed, decisional privacy may be affected if the challenge seems to beintimidating or if the supporting evidence is revealing or sensitive. decisional privacy is directly affected by the creation of transactional recordsof authentication events to support auditing.bodily integritythe authentication phase may also affect bodily integrity if the observation of the attribute requires close or direct contact with the individualor observation that appears intrusive. if the authentication phase requires the individual to produce physical evidence of an attribute, theindividualõs bodily integrity may be compromised. for example, a casualobservation that someone is 5 feet 6 inches tall is not likely to affectsomeoneõs sense of bodily integrity, while actually measuring someone islikely to affect that sense.communications privacyif the authentication phase requires the use of a communications system, communications privacy may be implicated. any authenticationthat occurs on the internet, for example, involves communications privacy. the question of who has access to the content of the authenticationand to the transactional information generated during the communication should be addressed before the authentication system is implemented. again, the creation and maintenance of transactional records ofauthentication events (or the authentication events that are unrelated tothe need to control system access) may raise particularly troubling issuesof communications privacy. if the monitoring reveals information aboutwhom an individual associates or communicates with, directly or indirectly, the system will infringe on communications privacy. finally, if theauthentication phase entails use of a communications system that can bewho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.192who goes there?associated with a particular individual, then communications privacy maybe affected because the system will generate content and transactionalinformation linked to the individual.summary of authentication phase discussionthis analysis indicates that in order to minimize privacy consequences, the following goals should be kept in mind when designing theauthentication phase of an authentication system:¥choose the minimum level of confidence in the authentication thatsupports the system needs. these needs could be satisfied in range ofways: from selfreported, to verified by the second party to the transaction, to verified by a third party, to verified by multiple third parties, topolling government sources, and so on.¥establish processes to achieve this level of confidence and makesure that the individual being authenticated is involved in the authentication.¥ensure that the system does only what it sets out to do (e.g., accesscontrol, monitoring, or some combination of these).¥limit the maintenance and storage of transactional records to theminimum amount necessary.¥set destruction policies for those records that need to be kept forlimited periods.¥segregate authentication information from transactional data subsequent to authentication events.¥create technical and procedural strategies that limit the ability toconnect authentication information with specific authentication events.¥understand and consider the security risks of authentication activity data storage, including risks of unauthorized access, unauthorized useby those with authorized access, and legally compelled access.concluding remarksthe development, implementation, and broad deployment of authentication systems require thinking carefully about the role of identity andprivacy in a free, open, and democratic society. privacy, including control over the disclosure of oneõs identity and the ability to remain anonymous, is an essential ingredient of a functioning democracy. it is a precondition for the exercise of constitutionally protected freedoms, such asthe freedom of association. it supports the robust exercise of freedom ofexpression by, for example, creating psychological space for political dissent. it maintains social norms that protect human dignity and autonomywho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.a toolkit for privacy in the context of authentication193by enabling expressions of respect and intimacy and the establishment ofboundaries between oneself and oneõs community.information collected in or generated by authentication systems canbe valuable and revealing. it may document where individuals havebeen, what resources they have sought, what individuals or institutionsthey have chosen to associate with, and so on. it is likely to be sought bylaw enforcement, commercial entities, and private parties. if individualsfear unchecked scrutiny, they will be less likely to vigorously participatein the political process and in society in general. if individuals are deniedphysical and mental privacyñfrom the government, corporations, andother individualsñthey are less able to explore ideas, formulate personalopinions, and express and act on these beliefs. in the context of systemsthat mediate access to political, cultural or artistic information and/orprovide state and private parties with access to personal information,identity and individual authentication mechanisms chill the free flow ofinformation and free association. at the same time, òprivacyó is oftenused as a pretext to hide illegal activities, and society has, at times, alegitimate interest in requiring authentication or identification. this requirement may stem from the need to validate claims to rights and privileges or the need to hold individuals responsible for their activities.the decision about where to deploy identity authentication systemsñbe it only where today confirmation of identity is already required, or in agreater range of circumstancesñwill shape society in both obvious andsubtle ways. because many privacy breaches are easy to conceal and/orare unreported, failing to protect privacy may cost less in the short runthan the initial outlay required to establish sound procedural and technical privacy protections. in addition, establishing practices and technicalmeasures that protect privacy costs money at the outset. if individualswhose information was compromised and agencies responsible for enforcing privacy laws were informed of privacy breaches, there would begreater incentive to proactively implement technologies and policies thatprotect privacy. even if the choice is made to institute authenticationsystems only where people today attempt to discern identity, the creationof reliable, inexpensive systems will inevitably invite function creep andunplannedfor secondary uses unless action is taken to avoid these problems. the role of attribute authentication in protecting privacy isunderexplored and may be a way to mitigate some of these concerns.it is critical that there be analysis of the intended context and usagemodels and thoughtful decision making about what system requirementsare. to best protect privacy, the privacy consequences of both the intended design and deployment and the potential secondary uses of authentication systems must be taken into consideration by vendors, users,policy makers, and the general public.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.appendixeswho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.197appendix abiographies of committeemembers and staffcommittee membersstephen t. kent, chair, is chief scientist in information security atbbn technologies, a part of verizon communications. during the pasttwo decades, dr. kentõs research and development activities have included the design and development of user authentication and accesscontrol systems, network layer encryption and access control systems,secure transport layer protocols, secure email technology, multilevel secure (x.500) directory systems, and public key certification authority systems. his most recent work focuses on security for internet routing, veryhigh speed internet protocol (ip) encryption, and highassurance cryptographic modules. dr. kent served as a member of the internet architecture board (19831994), and he chaired the privacy and security researchgroup of the internet research task force (19851998). he chaired theprivacy enhanced mail working group of the internet engineering taskforce from 1990 to 1995 and has cochaired the public key infrastructureworking group since 1995. he is the primary author of the core ipsecstandards: rfcs 2401, 2402, and 2406. he is a member of the editorialboard of the journal of computer security (1995 to the present), serves onthe board of the security research alliance, and served on the board ofdirectors of the international association for cryptologic research (19821989). dr. kent was a member of the national research councilõs (nrcõs)information systems trustworthiness committee (19961998), which produced trust in cyberspace. his other nrc service includes membershipwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.198appendix aon the committee on rights and responsibilities of participants in networked communities (19931994), the technical assessment panel for thenist computer systems laboratory (19901992), and the secure systemsstudy committee (19881990). the u.s. secretary of commerce appointeddr. kent as chair of the federal advisory committee to develop a federalinformation processing standard for federal key management infrastructure (19961998). the author of two book chapters and numerous technical papers on network security, dr. kent has served as a referee, panelist,and session chair for a number of conferences. since 1977 he has lecturedon network security on behalf of government agencies, universities, andprivate companies throughout the united states, europe, australia, andthe far east. dr. kent received the b.s. degree in mathematics, summacum laude, from loyola university of new orleans and the s.m., e.e.,and ph.d. degrees in computer science from the massachusetts instituteof technology. he is a fellow of the association for computing machinery and a member of the internet society and sigma xi.michael angelo is currently a staff fellow at compaq computercorporation and runs a laboratory at compaq that assesses biometricsand other securityenhancing technologies, such as smart cards. he isconsidered a subjectmatter expert for security and its associated technologies. his job is to provide technical guidance and input into strategicplanning and development of secure solutions. in addition, he is responsible for providing technical assistance to the corporate security team. dr.angelo possesses expertise in both biometric and token access authentication technology, including technical threat model and implementationanalysis, as well as risk reduction enhancement methodology, appliedcomputer system security, computer forensics, advanced dataprotectionmethodologies, and practical encryption techniques. his experience comprises 15 years in designing, implementing, managing, and supportingsecure intra and internets, including gateways, firewalls, and sentinels,and 20 years working at the kernel level of numerous operating systems,including a wide variety of hardware platforms (from personal computers to supercomputers) and software platforms (including unix [severalflavors], msdos/windows/nt, and vms). he holds several patents.dr. angelo has been active in a number of trade standards organizations:the trusted computing platform association, americans for computerprivacy, the bureau of export administration technical advisory committee, the information security exploratory committee, the key recovery alliance, the computer systems policy project, the crossindustryworking team security working group, and the national institute ofstandards and technologyõs industry key escrow working group.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.appendix a199steven bellovin is a fellow at at&t research. he received a b.a.degree from columbia university and m.s. and ph.d. degrees in computer science from the university of north carolina at chapel hill. whilea graduate student, he helped create netnews; for this, he and the othercollaborators were awarded the 1995 usenix lifetime achievementaward. at at&t laboratories, dr. bellovin does research in networks andsecurity and why the two do not get along. he has embraced a number ofpublic interest causes and weighed in (e.g., through his writings) on initiatives (e.g., in the areas of cryptography and law enforcement) that appear tothreaten privacy. he is currently focusing on cryptographic protocols andnetwork management. dr. bellovin is the coauthor of the book firewalls andinternet security: repelling the wily hacker, and he is one of the securityarea directors for the internet engineering task force. he was a member ofthe cstb committee that produced trust in cyberspace (1999) and served onthe information technology subcommittee of the group that produced thenrc report making the nation safer. he has been a member of the nationalacademy of engineering since 2001.bob blakley is chief scientist for security and privacy at ibm tivolisoftware. he is general chair of the 2003 institute for electrical andelectronics engineers security and privacy conference and has served asgeneral chair of the association for computing machineryõs (acmõs) newsecurity paradigms workshop. he was named distinguished securitypractitioner by the 2002 acm computer security and applications conference and serves on the editorial board for the international journal ofinformation security. dr. blakley was the editor of the object managementgroupõs common object request broker architecture (corba) securityspecification and is the author of corba security: an introduction to safecomputing with objects, published by addisonwesley. dr. blakley wasalso the editor of the open groupõs authorization application programming interface specification and the oasis security services technicalcommitteeõs security assertion markup language specification effort.he has been involved in cryptography and data security design worksince 1979 and has authored or coauthored seven papers on cryptography, secretsharing schemes, access control, and other aspects of computer security. he holds nine patents on securityrelated technologies.dr. blakley received an a.b. in classics from princeton university and amasterõs degree and a ph.d. in computer and communications sciencesfrom the university of michigan.drew dean is a computer scientist at sri international. he joined sriin july 2001; prior to that he was a member of the research staff at xeroxparc. he pioneered the systematic study of java security and morewho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.200appendix arecently has worked across a wide range of security issues, includingdenial of service, the theory of access control, and ip traceback. amonghis publications, he has received a best student paper award from theacm computer and communications security conference (1997), anoutstanding paper award from the acm symposium on operating system principles (1997), and a best paper award from the internet societyõsnetwork and distributed systems security symposium (2001). dr. deanis a member of the editorial board of springerverlagõs international journal of information security. dr. dean holds m.a. and ph.d. degrees fromprinceton university and a b.s. degree from carnegie mellon university,all in computer science.barbara fox is a senior software architect in cryptography and digitalrights management at microsoft corporation and is currently a seniorfellow at the kennedy school of government at harvard university. sheserves on the technical advisory board of the creative commons and theboard of directors of the international financial cryptography association. ms. fox joined microsoft in 1993 as director of advanced productdevelopment and led the companyõs electronic commerce technology development group. she has coauthored internet standards in the areas ofpublic key infrastructure and xml security. her research at harvardfocuses on digital copyright law, public policy, and privacy.stephen h. holden is an assistant professor in the department ofinformation systems at the university of maryland, baltimore county.dr. holdenõs research, publications, and teachings leverage his substantial federal government experience in governmentwide policy in information technology management and electronic government. he left theinternal revenue service (irs) in 2000 after a 16year career in the federalcareer service. while at the irs, he served as the program executive forelectronic tax administration (eta) modernization, reporting to the assistant commissioner (eta). he also served on the federal public key infrastructure steering committee during his time at the irs. prior to going tothe irs, dr. holden worked for 10 years at the office of management andbudget, doing a variety of policy, management, and budget analysis work.his federal civil servant career began in 1983 when he was a presidentialmanagement intern at the naval sea systems command. he holds aph.d. in public administration and public affairs from virginia polytechnic and state university, a master of public administration, and a b.a. inpublic management from the university of maine.deirdre mulligan was recently appointed director of the newsamuelson law, technology and public policy clinic at the university ofwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.appendix a201california, berkeley, school of law (boalt hall). while attendinggeorgetown university law center, ms. mulligan worked at the american civil liberties unionõs privacy and technology project, where shehoned her interest in preserving and enhancing civil liberties and democratic values. after law school, she became a founding member of thecenter for democracy and technology, a hightech public interest organization for civil liberties based in washington, d.c. for the past 6 years,mulligan has been staff counsel at the center. she has worked with federal lawmakers, government agencies, the judicial system, public interestorganizations, and the hightech business community, with the goal ofenhancing individual privacy on the internet, thwarting threats to freespeech on the internet, and limiting governmental access to private data.she has testified in several settings and has contributed to technical standards development. ms. mulligan received her j.d., cum laude, fromgeorgetown university law center in 1994 and a b.a. in architecture andart history from smith college in 1988.judith s. olson is the richard w. pew chair in human computerinteraction at the university of michigan. she is also a professor in theschool of information, computer and information systems, the businessschool, and the department of psychology. her research interests include computersupported cooperative work, humancomputer interaction, the design of business information systems for organizational effectiveness, and cognitive psychology. dr. olsonõs recent research focuseson the nature of group work and the design and evaluation of technologyto support it. this field combines cognitive and social psychology withthe design of information systems. she began her career at the universityof michigan in the department of psychology, served as a technical supervisor for human factors in systems engineering at bell laboratories innew jersey, and returned to the university of michigan, first to the business school and then the new school of information. she has more than60 publications in journals and books and has served on a number ofnational committees, including the national research councilõs committee on human factors and the council of the association for computingmachinery (acm). she has recently been appointed to the chi academyof the acmõs special interest group for humancomputer interaction.dr. olson earned a b.a. in mathematics and psychology from northwestern university in 1965 and her ph.d. 4 years later in the same disciplinesfrom the university of michigan.joe pato is the principal scientist for the hp labs trust, security andprivacy research program. he has also served as chief technology officerfor hewlettpackardõs internet security solutions division. mr. patoõswho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.202appendix acurrent research focus is the security needs of collaborative enterprises onthe internet, addressing both interenterprise models and the needs oflightweight instruments and peripherals directly attached to the internet.specifically, he is looking at critical infrastructure protection and theconfluence of trust, eservices, and mobility. these interests have led himto look at the preservation of internet communication in the event ofcyberterrorism, trust frameworks for mobile environments, and how toapply privacy considerations in complex systems. his work in cybercrimeand homeland security recently led him to become one of the foundersand board members of the it sector information sharing and analysiscenter. his past work included the design of delegation protocols forsecure distributed computation, key exchange protocols, interdomaintrust structures, the development of public and secretkeybased infrastructures, and the more general development of distributed enterpriseenvironments. mr. pato has participated on several standards or advisorycommittees for the institute for electrical and electronics engineers,american national standards institute, national institute of standardsand technology, department of commerce, worldwide web consortium,financial services technology consortium, and common open systemenvironment. he is currently the cochair of the oasis security servicestechnical committee, which is developing security assertions markuplanguage.radia perlman is a distinguished engineer at sun microsystemslaboratories. she is the architect for a group that does research in network security issues, recently most focused on public key infrastructuredeployment. some of the groupõs implementation will be distributed aspart of a reference implementation for java. dr. perlman is the author ofmany papers in the field of network security, as well as coauthor of atextbook on network security (and author of a textbook on lowerlayernetworking protocols). she is well known for her work on sabotageproofrouting protocols. her work on lowerlayer protocols, also well known,forms the basis of modern bridging, switching, and routing protocols.this expertise is crucial to understanding the technology behind suchthings as providing internet anonymity. dr. perlman has about 50 issuedpatents, a ph.d. in computer science from the massachusetts institute oftechnology, and s.b. and s.m. degrees in mathematics from mit. she wasrecently awarded an honorary doctorate from the royal institute of technology, sweden.priscilla m. regan is an associate professor in the department ofpublic and international affairs at george mason university. prior tojoining that faculty in 1989, she was a senior analyst in the congressionalwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.appendix a203office of technology assessment (19841989) and an assistant professorof politics and government at the university of puget sound (19791984).since the mid1970s, dr. reganõs primary research interest has been analysis of the social, policy, and legal implications of the organizational use ofnew information and communications technologies. she has publishedmore than 20 articles or book chapters, as well as legislating privacy: technology, social values, and public policy (university of north carolina press,1995). as a recognized researcher in this area, dr. regan has testifiedbefore congress and participated in meetings held by the department ofcommerce, the federal trade commission, the social security administration, and the census bureau. she received her ph.d. in governmentfrom cornell university in 1981 and her b.a. from mount holyoke college in 1972.jeffrey schiller received his s.b. in electrical engineering (1979)from the massachusetts institute of technology (mit). as mit networkmanager, he has managed the mit campus computer network since itsinception in 1984. before that, he maintained mitõs multiplexed information and computing service (multics) timesharing system during thetime of the arpanet tcp/ip conversion. he is an author of mitõskerberos authentication system. mr. schiller is the internet engineeringsteering groupõs area director for security. he is responsible for overseeing securityrelated working groups of the internet engineering taskforce. he was responsible for releasing a u.s. legal freeware version ofthe popular pgp (pretty good privacy) encryption program. mr. schilleris also responsible for the development and deployment of an x.509based public key infrastructure at mit. he is also the technical lead forthe new higher education certifying authority being operated by thecorporation for research and educational networking. mr. schiller isalso a founding member of the steering group of the new england academic and research network (nearnet). nearnet, now part of genuity,inc., is a major nationwide internet service provider.soumitra sengupta is assistant professor in the department ofmedical informatics at columbia university. dr. sengupta has focusedhis work on the challenges of security and privacy in health care, complementing his academic work by service as security officer for the newyork presbyterian healthcare system. his research interests are in theareas of distributed systems, their monitoring, management, and securityaspects, and their application in a health care environment. he is interested in the architectural design and engineering concerns of buildinglarge, functioning systems over heterogeneous platforms and protocols.dr. sengupta holds a b.e. from birla institute of technology and sciencewho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.204appendix a(electrical and electronics engineering), pilani, india, and m.s. and ph.d.degrees from the state university of new york at stony brook, new york,in computer science. he was a member of the association for computingmachinery (19841994), the institute for electrical and electronics engineers (ieee) computer society (19841992) and is currently a member ofthe american medical informatics association.james l. wayman has been the director of the biometrics test centerat san jose state university in california since 1995. the test center isfunded by the u.s. government and other national governments to develop standards and scientific test and analysis methods and to advise onthe use or nonuse of biometric identification technologies. the test centerserved as the u.s. national biometrics test center from 1997 to 2000. dr.wayman received the ph.d. degree in engineering from the university ofcalifornia at santa barbara in 1980 and joined the faculty of the department of mathematics at the u.s. naval postgraduate school in 1981. in1986, he became a fulltime researcher for the department of defense inthe areas of technical security and biometrics. dr. wayman holds threepatents in speech processing and is the author of dozens of articles inbooks, technical journals, and conference proceedings on biometrics,speech compression, acoustics, and network control. he serves on theeditorial boards of two journals and on several national and internationalbiometrics standards committees. he is a senior member of the institutefor electrical and electronic engineers.daniel j. weitzner is the director of the world wide web consortiumõs (w3cõs) technology and society activities. as such, he is responsible for the development of technology standards that enable theweb to address social, legal, and public policy concerns such as privacy,free speech, protection of minors, authentication, intellectual property,and identification. he is also the w3cõs chief liaison to public policycommunities around the world and a member of the internet corporationfor assigned names and numbers protocol supporting organization protocol council. mr. weitzner holds a research appointment at the massachusetts institute of technologyõs (mitõs) laboratory for computer science and teaches internet public policy at mit. before joining the w3c, hewas cofounder and deputy director of the center for democracy andtechnology, an internet civil liberties organization in washington, d.c.he was also deputy policy director of the electronic frontier foundation.as one of the leading figures in the internet public policy community, hewas the first to advocate user control technologies such as content filtering and rating to protect children and avoid government censorship ofwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.appendix a205the internet. these arguments played a critical role in the 1997 u.s. supreme court case, reno v. aclu, awarding the highest free speech protections to the internet. he successfully advocated the adoption of amendments to the electronic communications privacy act creating new privacyprotections for online transactional information such as web site accesslogs. mr. weitzner has a degree in law from buffalo law school and ab.a. in philosophy from swarthmore college. his publications on communications policy have appeared in the yale law review, global networks,computerworld, wired magazine, social research, electronic networking: research, applications and policy, and the whole earth review. he is also acommentator for national public radioõs marketplace radio.stafflynette i. millett is a study director and program officer with thecomputer science and telecommunications board (cstb) of the nationalresearch council. she is currently involved in several cstb projects,including a study examining certification and dependable systems, a comprehensive exploration of privacy in the information age, and a look atthe fundamentals of computer science as a research discipline. she is alsoexploring possible study options for cstb with respect to the issues ofbiometrics and open source software development. she recently completed a cstb study that produced embedded, everywhere: a researchagenda for networked systems of embedded computers. before joining cstb,ms. millett was involved in research on static analysis techniques forconcurrent programming languages as well as research on valuesensitive design and informed consent online. she has an m.sc., is òabdó incomputer science from cornell university, and has a b.a. in mathematicsand computer science from colby college. her graduate work was supported by both a national science foundation graduate fellowship andan intel graduate fellowship.jennifer m. bishop has been a senior project assistant with the computer science and telecommunications board (cstb) since october 2001.she is currently supporting several projects, including digital archivingand the national archives and records administration; computing frontiers: prospects from biology; and telecommunications research and development. she also maintains cstbõs contact database, handles updatesto the cstb web site, and has designed book covers for several reports.prior to her move to washington, d.c., ms. bishop worked for the city ofithaca, new york, coordinating the police departmentõs transition to anew sqlbased time accrual and scheduling application. her other workwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.206appendix aexperience includes designing customized hospitalityindustry performance reports for realtime hotel reports, maintaining the police recordsdatabase for the city of ithaca, and handpainting furniture formackenziechilds, ltd., of aurora, new york. she is an artist working inoil and mixed media. ms. bishop holds a b.f.a (2001) in studio art fromcornell university.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.appendix bbriefers to the study committee207although the briefers listed below provided many useful inputs tothe committee, they were not asked to endorse the conclusions or recommendations, nor did they see the final draft of the report before its release.march 1314, 2001washington, d.c.roger baker, department of commerce and cio councildaniel chenok, office of information and regulatory affairs, office ofmanagement and budgetsara hamer, social security administrationjohn woodward, randmay 3031, 2001washington, d.c.lt. col. robert bollig, executive officer, department of defense, biometrics management officemike green, director, department of defense, pki program managementofficecathy hotka, vice president, information technology for the nationalretail federationmark maccarthy, senior vice president for public policy, visa usawho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.208appendix bdavid temoshok, office of governmentwide policy, general servicesadministrationrichard varn, chief information officer, state of iowaworkshopoctober 34, 2001herndon, virginiamichael aisenberg, verisignkim alexander, california voter foundationbrian arbogast, microsoft corporationpaul barrett, real user corporationstefan brands, mcgill universityroger clarke, xamax consultancy pty ltd and the australian nationaluniversityjohn daugman, university of cambridgemark forman, office of management and budgetchris hoofnagle, electronic privacy information centerpaul van oorschot, entrust, inc.margot saunders, national consumer law centerjudith spencer, general services administrationpeter swire, george washington university law schoolpaul syverson, naval research laboratorybrian tretick, privacy assurance and advisory services, ernst & youngjanuary 9, 2002palo alto, californiachristopher kuner, morrison and foerster llpmarch 1314, 2002washington, d.c.patti gavin, social security administrationfred graf, social security administrationjay maxwell, american association of motor vehicles administratorsjoe sanders, new york state department of motor vehicleswho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.appendix csome key concepts209attribute. an attribute describes a property associated with an individual.attribute authentication. attribute authentication is the process of establishing an understood level of confidence that an attribute applies to aspecific individual.authentication. authentication is the process of establishing confidence inthe truth of some claim.authenticator. an authenticator is evidence that is presented to supportthe authentication of a claim. it increases confidence in the truth of theclaim.authorization. authorization is the process of deciding what an individualought to be allowed to do.biometrics. biometrics is the automatic identification or identity verificationof individuals on the basis of behavioral or physiological characteristics.bodily integrity. bodily integrity in the context of privacy refers to thoseissues involving intrusive or invasive searches and seizures.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.210appendix ccertification authority. a certification authority is the entity that issues adigital certificate in a public key cryptosystem.communications privacy. communications privacy is a subset of information privacy that protects the confidentiality of individualsõ communications.credential. credentials are objects that are verified when presented to theverifier in an authentication transaction. credentials may be bound insome way to the individual to whom they were issued, or they may bebearer credentials. the former are necessary for identification, while thelatter may be acceptable for some forms of authorization.decisional privacy. decisional privacy protects the individual from interference with decisions about self and family.identification. identification is the process of using claimed or observedattributes of an individual to infer who the individual is.identifier. an identifier points to an individual. an identifier can be aname, a serial number, or some other pointer to the entity being identified.identity. the identity of x is the set of information about individual x thatis associated with that individual in a particular identity system y. however, y is not always named explicitly.identity authentication. identity authentication is the process of establishing an understood level of confidence that an identifier refers to an identity. it may or may not be possible to link the authenticated identity to anindividual.individual authentication. individual authentication is the process of establishing an understood level of confidence that an identifier refers to aspecific individual.information privacy. information privacy protects the individualõs interestin controlling the flow of information about the self to others.password. a sequence of characters, presumed to be secret, that is divulged in order to gain access to a system or resource.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.appendix c211privacy. privacy is a multifaceted term, with many contextually dependent meanings. one aspect of the right to privacy is the right of anindividual to decide for himself or herself when and on what terms his orher attributes should be revealed.private key. in public key cryptography systems, a private key is a value(key), presumed to be secret, and typically known only to one party. theparty uses the private key to digitally sign data or to decrypt data (orkeys) encrypted for that party using the partyõs public key.public key. in public key cryptography systems, a public key is a valueused to verify a digital signature generated using a corresponding privatekey, or used to encrypt data that can be decrypted using the corresponding private key.public key certificate. sometimes called a digital certificate, a public keycertificate contains attributes, typically including an identifier, that arebound to a public key via the use of a digital signature.public key infrastructure. a public key infrastructure (pki) consists of a setof technical and procedural measures used to manage public keys embedded in digital certificates. the keys in such certificates may be used toenable secure communication and data exchange over potentially insecure networks.registration authority. a registration authority is the entity in a pki thatestablishes a correspondence between an identifier that will appear in acertificate and an individual.security. security refers to a collection of safeguards that ensure the confidentiality of information, protect the integrity of information, ensure theavailability of information, account for use of the system, and protect thesystem(s) and/or network(s) used to process the information.threat. a threat is a motivated, capable adversary. the adversary is motivated to violate the security of a target (system) and has the capability tomount attacks that will exploit vulnerabilities of the target.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.who goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.213what is cstb?as a part of the national research council, the computer science andtelecommunications board (cstb) was established in 1986 to provideindependent advice to the federal government on technical and publicpolicy issues relating to computing and communications. composed ofleaders from industry and academia, cstb conducts studies of criticalnational issues and makes recommendations to government, industry,and academic researchers. cstb also provides a neutral meeting groundfor consideration of complex issues where resolution and action may bepremature. it convenes invitational discussions that bring together principals from the public and private sectors, assuring consideration of allperspectives. the majority of cstbõs work is requested by federal agencies and congress, consistent with its national academies context.a pioneer in framing and analyzing internet policy issues, cstb isunique in its comprehensive scope and effective, interdisciplinary appraisal of technical, economic, social, and policy issues. beginning withearly work in computer and communications security, cyberassuranceand information systems trustworthiness have been a crosscutting themein cstbõs work. cstb has produced several reports known as classics inthe field, and it continues to address these topics as they grow in importance.to do its work, cstb draws on some of the best minds in the country,inviting experts to participate in its projects as a public service. studiesare conducted by balanced committees without direct financial interestsin the topics they are addressing. those committees meet, confer elecwho goes there?: authentication through the lens of privacycopyright national academy of sciences. all rights reserved.214what is cstb?tronically, and build analyses through their deliberations. additionalexpertise from around the country is tapped in a rigorous process ofreview and critique, further enhancing the quality of cstb reports. byengaging groups of principals, cstb obtains the facts and insights criticalto assessing key issues.the mission of cstb is to¥respond to requests from the government, nonprofit organizations,and private industry for advice on computer and telecommunicationsissues and from the government for advice on computer and telecommunications systems planning, utilization, and modernization;¥monitor and promote the health of the fields of computer scienceand telecommunications, with attention to issues of human resources,information infrastructure, and societal impacts;¥initiate and conduct studies involving computer science, computertechnology, and telecommunications as critical resources; and¥foster interaction among the disciplines underlying computingand telecommunications technologies and other fields, at large and withinthe national academies.as of 2003, cstb activities with security and privacy componentsaddress privacy in the information age, critical information infrastructureprotection, authentication technologies and their privacy implications,information technology for countering terrorism, and geospatial information systems. additional studies examine broadband, digital government, the fundamentals of computer science, limiting childrenõs access topornography on the internet, digital archiving and preservation, andinternet navigation and the domain name system. explorations touchingon security and privacy are under way in the areas of the insider threat,cybersecurity research, cybersecurity principles and practices, dependable/safe software systems, biometrics, wireless communications andspectrum management, open source software, digital democracy, theòdigital divide,ó manageable systems, information technology and journalism, supercomputing, and information technology and education.more information about cstb can be obtained online at <http://www.cstb.org>.
detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/790mental models in humancomputer interaction: research issuesabout what the user of software knows56 pages | 5 x 9 | paperbackisbn 9780309078016 | doi 10.17226/790committee on human factors, national research councilmental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.mental models inhumancomputerinteractionresearch issues about what the user of softwareknowsjohn m. carroll and judith reitman olson, editorsworkshop on software human factors: users' mental modelsnancy anderson, chaircommittee on human factorscommission on behavioral and social sciences and educationnational research councilnational academy presswashington, d.c. 1987imental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.notice: the project that is the subject of this report was approved by the governing board of thenational research council, whose members are drawn from the councils of the national academyof sciences, the national academy of engineering, and the institute of medicine. the members ofthe committee responsible for the report were chosen for their special competences and with regardfor appropriate balance.this report has been reviewed by a group other than the authors according to proceduresapproved by a report review committee consisting of members of the national academy of sciences, the national academy of engineering, and the institute of medicine.the national academy of sciences is a private, nonprofit, selfperpetuating society of distinguished scholars engaged in scientific and engineering research, dedicated to the furtherance ofscience and technology and to their use for the general welfare. upon the authority of the chartergranted to it by the congress in 1863, the academy has a mandate that requires it to advise the federal government on scientific and technical matters. dr. frank press is president of the nationalacademy of sciences.the national academy of engineering was established in 1964, under the charter of thenational academy of sciences, as a parallel organization of outstanding engineers. it is autonomousin its administration and in the selection of its members, sharing with the national academy of sciences the responsibility for advising the federal government. the national academy of engineeringalso sponsors engineering programs aimed at meeting national needs, encourages education andresearch, and recognizes the superior achievements of engineers. dr. robert m. white is presidentof the national academy of engineering.the institute of medicine was established in 1970 by the national academy of sciences tosecure the services of eminent members of appropriate professions in the examination of policy matters pertaining to the health of the public. the institute acts under the responsibility given to thenational academy of sciences by its congressional charter to be an adviser to the federal government and, upon its own initiative, to identify issues of medical care, research, and education. dr.samuel o. thier is president of the institute of medicine.the national research council was organized by the national academy of sciences in 1916 toassociate the broad community of science and technology with the academy's purposes of furthering knowledge and advising the federal government. functioning in accordance with general policies determined by the academy, the council has become the principal operating agency of both thenational academy of sciences and the national academy of engineering in providing services tothe government, the public, and the scientific and engineering communities. the council is administered jointly by both academies and the institute of medicine. dr. frank press and dr. robert m.white are chairman and vice chairman, respectively, of the national research council.the united states government has at least a royaltyfree, nonexclusive and irrevocable licensethroughout the world for government purposes to publish, translate, reproduce, deliver, perform,dispose of, and to authorize others so as to do, all or any portion of this work.available fromcommittee on human factorscommission on behavioral and social sciences and educationnational research council2101 constitution ave., n.w.washington, d.c. 20418printed in the united states of americaiimental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.workshop on software human factors:users' mental modelsnancy anderson (chair), department of psychology, university of marylandelizabeth k. bailey, consultant, falls church, virginiajohn m. carroll, watson research center, ibm corporation, yorktown heights,new yorkrichard j. jagacinski, department of psychology, ohio state universitydavid r. lenorovitz, computer technology associates, inc., englewood,coloradomarilyn mantei, center for machine intelligence, ann arbor, michiganphyllis reisner, almaden research center, ibm research, san jose, californiajudith reitman olson, department of computer and information systems,graduate school of business administration, university of michiganjanet walker, symbolics, inc., cambridge, massachusettsjohn whiteside, digital equipment corporation, nashua, new hampshirestanley deutsch, study directoriiimental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.ivmental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.committee on human factors 1986œ1987thomas b. sheridan (chair), department of mechanical engineering,massachusetts institute of technologynancy s. anderson, department of psychology, university of marylandclyde h. coombs, department of psychology, university of michiganjerome i. elkind, information systems, xerox corporation, palo altobaruch b. fischhoff, decision research (a branch of perceptronics, inc.),eugene, oregonoscar grusky, department of sociology, university of california, los angelesrobert m. guion, department of psychology, bowling green state universitydouglas h. harris, anacapa sciences, santa barbara, californiajulian hochberg, department of psychology, columbia universitythomas k. landauer, information sciences division, bell communicationresearch, morristown, new jerseyjudith reitman olson, department of computer and information systems,graduate school of business administration, university of michiganrichard w. pew (past chair), computer and information sciences division, boltberanek and newman laboratories, cambridge, massachusettsstover h. snook, liberty mutual research center, hopkinton, massachusettsvmental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.robert c. williges, department of industrial engineering and operationsresearch, virginia polytechnic institute and state universitystanley deutsch, study director (19841987)harold van cott, study directorerratum œ p. viithe air force office of scientific research should have been included as one of thesponsors of the committee on human factors.vimental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.forewordthe committee on human factors was established in october 1980 by thecommission on behavioral and social sciences and education of the national researchcouncil. the committee is sponsored by the office of naval research, the armyresearch institute for the behavioral and social sciences, the national aeronautics andspace administration, and the national science foundation. the principal objectives ofthe committee are to provide new perspectives on theoretical and methodological issues,to identify basic research needed to expand and strengthen the scientific basis of humanfactors, and to attract scientists both within and outside the field for interactivecommunication and to perform needed research. the goal of the committee is to providea solid foundation of research as a base on which effective human factors practices canbuild.human factors issues arise in every domain in which humans interact with theproducts of a technological society. in order to perform its role effectively, thecommittee draws on experts from a wide range of scientific and engineering disciplines.members of the committee include specialists in such fields as psychology, engineering,biomechanics, physiology, medicine, cognitive sciences, machine intelligence, computersciences, sociology, education, and human factors engineering. other disciplines arerepresented in the working groups, workshops, and symposia. each of these disciplinescontributes to the basic data, theory and methods required to improve the scientific basisof human factors.forewordviimental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.forewordviiimental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.contents preface xi abstract xv introduction 3 models of what, held by whom? 3 types of representations of users' knowledge 5 simple sequences 6 methods and ways to choose among them 8 mental models 12 surrogates 13 metaphor models 13 glass box models 14 network representations of the system 15 comparisons 17 how users' knowledge affects their performance 19 chaos and misconception in both novices and experts 20 skilled performance 21 applying what we know of the user's knowledge to practical problems 23 designing interfaces 24 user training 26 research recommendations 29 references 34contentsixmental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.forewordxmental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.prefacethere has been a longstanding problem with inferring the causes of complexbehavior. mental events are not directly observable; they must be inferred from overtbehavior. behaviorists reject mental events as legitimate scientific concepts. morerecently, however, developments in cognitive science and artificial intelligence, in whichmental events are specifically modeled and found to have measurable correlates inbehavior, have brought the concepts back into fashion. these mental events, theirdescription and postulated interrelationships, are the subject of this report. we focusspecifically on the mental events that are postulated to occur as someone learns orperforms complex tasks on computer software.from the point of view of cognitive science, users of computer software systemsbase their behavior on stored knowledge about particular sequences of actions, ongeneral rules about how to accomplish certain tasks, or on a mental model (an underlyingunderstanding of how the system works). knowing what the user knows about or expectsfrom a system has implications for both design and training purposes. from a designpoint of view, the system could be designed to fit the user's goals in accomplishing tasksor could display enough of how it works to make accomplishing a task easy tounderstand. from the training point of view,prefaceximental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.users could be given instructions and exercises that clearly present sequences, rules, and/or a model in order to make learning and performing easy.at present, there is no satisfactory way of describing what the user knows. there isno way to characterize the differences among users of various systems as they gothrough the process of developing an awareness and understanding of how the systemworks or how a given task is to be performed. consequently, the committee on humanfactors conducted a twoday workshop on may 15 and 16, 1984, to determine means forachieving a better understanding of what users know and its implications for system andsoftware design as well as user training. this workshop was a continuation of thecommittee's efforts to define research needs in the area of software human factors. tennationally known researchers on software design, cognitive psychology, and humanfactors met to discuss the issues having to do with what a user of software knows.as background for this workshop, john m. carroll wrote an invited paper entitledﬁmental models and software human factors: an overview.ﬂ this was distributed to allparticipants in advance of the meeting. in turn, the workshop members prepared shorttwo to threepage position papers addressing additional topics and issues that theybelieved were important and warranted discussion at the workshop. much of thediscussion at the workshop centered on sifting through the many definitions of the termmental model, gathering ideas from among the variety of methods used to representusers' knowledge about software systems.this report was prepared by merging the ideas generated by the workshop memberswith those in carroll's paper. it includes his central organization and literature review,adds more recent information, and clarifies the distinction between mental models andtask representations. this report was then distributed to workshop participants forchanges and additions.this report is written for the researcher concerned with the psychology ofperformance of complex tasks and for the practitioner who would like to use informationabout how the user thinks about both the task and the system in the design of computersoftware, its documentation, or training for its use. most of the research on thesequestions has used softwarebased textediting tasks as a domain and looked at themental models people are purported to build of only simple devices. the results should beprefacexiimental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.generalized to even more complex tasks, such as process control, tactical decisionmaking, project planning, and graphics design; but their scope has not been tested. theexclusion of these kinds of tasks is not to be taken as an indication that the researchreported cannot cover these more complex tasks. but their scope is an important researchneed.judith reitman olsonprefacexiiimental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.prefacexivmental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.abstractusers of software systems acquire knowledge about the system and how to use itthrough experience, training, and imitation. currently, there is a great deal of debateabout exactly what users know about software. this knowledge may include one or moreof the following: simple rules that prescribe a sequence of actions that apply under certainconditions; certain general situations and goals; andmental models, knowledge of the components of a system, theirinterconnection, and the processes that change the components; knowledge thatforms the basis for users being able to construct reasonable actions; andexplanations about why a set of actions is appropriate.discovering what users know and how these different forms of knowledge fittogether in learning and performance is important. it applies to the problem of designingsystems and training programs so that the systems are easy to use and the learning isefficient. research on the effects of different representations on ultimate performance ismixed. research on exactly what usersabstractxvmental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.know is scattered. analytical methods and techniques for representing what the userknows are sparse but growing.this report reviews current work and through the review, identifies severalimportant research needs: detail what kinds of mental representations people have of systems that allowthem to behave appropriately in using the software. detail what a mental model would consist of and how a person would use it todecide what action to take next. have and use mental models. determine the behaviors that would demonstrate a mental model's form and theoperations used on the model. explore alternative views of goaldirected representations (e.g., socalledsequence/method representations) and detail the behavior predicted from them. expand the types of mental representations that may exist to include those thatmay not be mechanistic, such as algebraic and visual systems. intermix different representations in producing behavior. explore how knowledge about systems is acquired. determine how individual differences have an impact on learning of andperformance on systems. explore the design of training sequences for systems. provide systems designers with tools to help them develop systems that evokeﬁgoodﬂ representations in users.the task domain of this research to include more complex software.abstractxvimental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.mental models in humancomputerinteraction: research issues about whatthe user of software knows1mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.2mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.introductiondiscovering what the users of a computer software system do know and shouldknow are important goals in current research on humancomputer interaction. researchon the kinds of knowledge people have when they use computers, including the conceptof a mental model of the system, is one of the major topics that is bringing the field ofhumancomputer interaction from the tradition of human factors closer to that ofexperimental/cognitive psychology. traditional human factors work has focusedprincipal attention on behavior and performance itself, and has avoided the problem ofdescribing the conceptual causes and effects of that behavior. on the other hand, whileacademic cognitive psychology does concern itself with theoretical interpretations ofmental processes, it has focused on narrowly restricted mental processes, such asparticular aspects of learning, memory, problem solving, or planning, and has studiedthem in the context of highly controlled and contrived laboratory tasks. the study ofknowledge representations of users of computerbased systems affords an opportunity toexplore both the theoretical base of behavior as well as specific behaviors in tasks thatinvolve many different cognitive processes in concert.because a number of researchers are concerned with mental representations, andbecause this topic has an impact on cognitive psychology and software human factors,there is an emerging need to clarify the concepts underlying knowledge representation and mental models as they apply to humancomputer interaction. we intend to fill thisneed by reviewing relevant current research and presenting a preliminary framework ofthe kinds of mental representations of procedures people might have.models of what, held by whom?several key distinctions need to be recognized in discussing mental representationsand mental models in humancomputer interaction. for example, various individuals areconcerned with using or designing a piece of software, and they hold differentconceptions of it. these individuals include the user, the softwaremodels of what, held by whom?3mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.engineer, the human factors analyst, and the cognitive psychologist. furthermore, thereare different aspects of the system to be known: the task, knowing what the goal is and ingeneral what subtasks need to be accomplished to achieve the goal; the system interface, knowing how to accomplish the sequence of subtasks in this system, given the datapresentation and interaction languages of this system; and the system architecture, knowing the way the data are stored, the internal processes the interactions invoke, andin general how the system works.confusion has surrounded the term mental model because different authors havereferred to different owners of the models (the user, the software engineer, etc.) and arenot clear as to what the model actually represents (the task, the architecture, etc.).for example, some researchers and human factors analysts acknowledge that it isimportant to know the way users themselves are built and work, what their memorylimits are, their common strategies in problem solving, their individual differences, andso on, in order to build useful, usable software. a system that requires the user toremember a list of 100 codes that represent areas of the country or the types oftransactions that are required (as in some airline or automobile reservation systems) ispredictably difficult because our model of the user includes a longterm memory that isconfused by similar meaningless items. these researchers have sometimes used the termmental model to refer to the model that they, as researchers, have of the user's mentalarchitecture.similarly, software engineers have ideas about what the user wants to do and howthe system itself is structured that dictate how they will program the system and how itwill operate to serve the users' needs. engineers have mental models of their design.this highlights another distinction, that between descriptive and prescriptive representations. researchers want to be able to analyze what the user currently knows sothey can explain why he or she is having difficulty, which aspects are learned and whichare confused, and so on. in this case, they are using a descriptive model, one that tells uswhat the user knows. designers, however, want to construct a model of what the usershould know. this representation could be used to analyze, for example, whether aproposed system will be too difficult to learn or where the errors might be. and, indesigning commands and screen presentations, designers would like to invoke a model inthe user that fits the dialog; they would like to get the user to build a mental modelmodels of what, held by whom?4mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.of the system that fits what the users have to do to operate the system. descriptivemodels are those held by the researcher to approximate what the user does know;prescriptive models are those held by the designer to approximate what the user shouldknow.the concern of this report, however, is the representation that the user has of how acomputer system works. furthermore, since a mental model may be only one way ofdescribing the knowledge that a user has about a system, this report is broadened toinclude all of what a user knows about using a particular piece of software, includinghow to use it and how it works.what users know differs in several important dimensions. it differs according to thesophistication of the user. for example, a user who is a programmer might have a verydifferent understanding of a piece of software than a person with no programmingexperience. also, multiple mental models or several representations at different levels ofabstraction might coexist within the same individual. for example, a person who bothdesigned and later used a system might develop two somewhat compartmentalizedunderstandings of the system. analogous distinctions arise if we consider different taskenvironments. for example, the representation elicited for routine skilled behavior mightdiffer substantively from that elicited when a person tries to recover from an error orotherwise solve problems (e.g., rasmussen, 1983).because understanding what the user knows has practical importance for designingsoftware and its training, and because it has theoretical importance in understandingpeople as they generally perform complex cognitive tasks, this report considers only therepresentations the users have when using softwarešrepresentations of the task beingperformed, the usersystem interface, and the system architecture.types of representations ofusers' knowledgethere are three basic types of representations that have been formulated tocharacterize what a user of software knows. the most elementary is a simple sequence ofovert actions that fit a particular situation. the second is a more complex and generalcharacterization, the knowledge of methods. this kind of representation of the user'sbehavior incorporates general goals, thetypes of representations of users' knowledge5mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.subgoals associated with it, a set of methods that could be brought to bear to accomplishthe subgoals, and, finally, sequences of operators for those methods. both of theseconceptualizations are taskoriented in that they contain no theory of how the software orsystem works or what the user's actions do internally to produce the results.the third, the mental model,1 is knowledge of how the system works, what itscomponents are, how they are related, what the internal processes are, and how theyaffect the components. it is this conceptualization that allows the user not only toconstruct actions for novel tasks but also to explain why a particular action produces theresults it does.simple sequencesusers often have no knowledge of the underlying system or even general rules forgetting things done. novices, in particular, resort to a learning method that borders onrote memorization. they learn sequences of actions that will get the system to docommon types of tasks. for example, in using the operating system on the michiganterminal system to print the contents of a text file with the laser printer, many usersmerely memorize the nearly nonsense strings:$run *textform scards = pc:fw.macros + file spunch = !x ‚run a programcalled ﬁtextformﬂ with input from a master file of parameters plus the inputfile, send the output to a temporary file called ﬁxﬂ '$run *pagepr scards = !x par = onesided ‚run a program called ﬁpageprﬂwith input from the temporary file ﬁxﬂ so that the output is printed on only oneside of each page'where the only free parameter to be entered is the name of the file after the ﬁ+ﬂ inthe first ﬁscardsﬂ designation. similarly, some word processors require the user tomemorize short, common1this is a subset of the knowledge rouse and morris (1986) call mental models.we would include knowledge that helps the user to explain the function andstates of the system and to predict its future behavior. we would not includedescriptions of its purpose and form, information that seems shallow andunhelpful in a performance context.types of representations of users' knowledge6mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.command sequences to accomplish certain repetitive actions, such as ﬁ<cntl> xmeﬂ toexit, and ﬁ<cntl> xlaﬂ to enact the printing sequence. a good clue as to how oftenusers rely on these simple sequences is to note the cheat sheets that they keep availablewhen they are using software, or the notes made and often stuck to the side of thecathoderay tube to remind the user of some commands that are commonly used butdifficult to remember.young (1983) described one way in which users think about a calculator, as simplesequences or sets of taskaction pairs. a task includes something the user wishes toaccomplish (e.g., an arithmetic calculation or formula evaluation), which is associatedwith an action, or what the user must do in order to accomplish the task (e.g., key presseson a calculator). this knowledge is in the form of paired associates, and like thesequences to print a file described above, it has simple slots that indicate the freeparameters the user must designate to fit the current situation.a second description of simple sequences of actions is the keystroke model (card etal., 1980a,b, 1983; embley et al., 1978). the analyses in the keystroke models containnotations that describe what sequences of actions users make in invoking simplecommands: the keystrokes, mouse movements and so on. in card et al. (1980a,b, 1983)keystroke analysis, the analyst assumes that the user needs time to make each act inproducing the command: a time to make a keystroke, a time to point with a mouse, atime to move the hands from the keyboard to the mouse or back, and a time to mentallyprepare each command and its parameters. the analysis assumes that users must retrieveeach command sequence from their memory, incurring a pause for mental preparation,and then execute the components of the command, pausing for additional mentalpreparation times before each command word, each parameter, and each delimiter (suchas pressing a parenthesis, return, or other type of operator). for example, a commandsequence for using a lineoriented editor to search a file for an error and fix it:s /f ﬁerrorstringﬂ‚search the whole file for an error'a 16 ﬁoldstringﬂ newstringﬂ‚alter line 16 so that the old string is replaced with the new string'types of representations of users' knowledge7mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.would include mental preparations before each line and before each parameter, suchas ﬁ/fﬂ and ﬁ16,ﬂ and the strings to be searched for and replaced. analysis proceeds byattaching a constant time for each keystroke, movement, or mental preparation, affordinga prediction of how long the formulation and execution of each command wouldnormally require.in the same spirit, reisner (1984) assumes that the user needs a fixed amount oftime to make each individual act in producing a command. instead of one mentalpreparation time, however, reisner (1984) posits specific mental acts (e.g., retrievingfrom longterm memory, calculating a number, copying a number), each of which takes adifferent length of time. the analyst assumes (or knows from prior experimentation) howthe various parameters are related (e.g., the time to calculate a number will be greaterthan the time to copy that number from a display) without specifying each time exactly.simple algebra is then used to predict which of various whole design alternatives, orwhich of various user methods, will require the shortest time to perform.these analyses of simple sequences serve to facilitate both comparison of existingsoftware packages for the one that will require the shortest time to perform and thedesign and development of new system languages.methods and ways to choose among themusers not only elicit simple sequences to fit simple situations by rote; theysometimes also choose among various possible general methods that fit a particularsituation.2a number of investigators have studied the organization of more general actions asa function of task goals in the domain of programming. a general finding is that skilledprogrammers recognize aspects of particular situations and select general actionsappropriate to them. for example, individual statements or sets of lines of code in aprogram are ﬁchunkedﬂ into higherorder taskrelevant structures. skilled programmerscan recall at a glance more lines of code than novice programmers (adelson, 1981;mckeithen et al., 1981; shneiderman, 1980). this is consistent2these methods are similar to the procedures remembered and used in the stageof ﬁdeciding and testing actionsﬂ in supervisory control tasks, described bysheridan et al. (1986).types of representations of users' knowledge8mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.with prior studies of expertise and the organization of memory (chase and simon, 1973;egan and schwartz, 1979; reitman, 1976). these studies suggest that in the skilledprogrammer's knowledge base there is a mapping between chunks of actions or methods(that often go together) and general task features, so that the actions will be recalled andused at appropriate times in the future. these chunks reflect a developed, deeperunderstanding of routine programs, which are useful to a programmer writing programs.similarly, ehrlich and soloway (1984) have shown that skilled programmers tend toemploy patterns of actions, called plans, consisting of routinely occurring sequences ofprogramming statements.furthermore, by examining the structure of recall protocols, mckeithen et al.(1981) determined that skilled programmers organize their vocabulary of programmingstatements more stereotypically than do novice programmers. it appears that withexpertise, the users' understanding converges to a similar set of representations ofconcepts in the programming language. data base designers reveal mental organizationsthat become increasingly homogeneous with greater expertise (smelcer, 1986).a more complete theory about what the user knows about how to accomplish aparticular task is the goms model (card et al., 1983). goms is an acronym that standsfor the elements of what the user knows: the goals, the operators, the methods, andselection rules. in the goms model, the user has a certain goal to accomplish (such asediting a manuscript that has been marked up). the user recognizes that this large goalcan be broken into a set of subgoals (such as finding each editing mark and making therequisite changes). subgoals are broken down into smaller and smaller subgoals untilthey match a basic set of methods, that is, sequences of operations that satisfy a smallsubgoal.the goms model states that users have some rules by which they choose themethod that will fit the current situation. for example, users may know that there areseveral methods that can be used to find the first place in the manuscript to be edited:using the search function with a distinguishing string to be found, using the pageforwardkey until the target page is found visually, or using the cursor key to find the specifictarget location visually. people will choose whether to use the search, pageforward, orcursor key method depending on how far away the next editing target is assumed to be.each of these methods is made up oftypes of representations of users' knowledge9mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.certain operators, key presses, and hand motions, as specified in the keystroke modeldescribed above in the discussion of simple sequences.a number of empirical studies have shown that the predictions of goms and thekeystroke model are reasonably accurate, and that sometimes one can even use the sametime parameters across applications. card et al. (1983) showed that their parameters forkeystrokes and mental processing time were similar across text processors, operatingsystems, and graphics packages. olson and nilsen (1987) extended the analysis to showthat the basic parameters applied well to spreadsheet software. however, additional timeparameters were required. one was to account for the time it took users to scan thescreen (for example, to find on the screen the coordinates of a particular value in aspreadsheet). a second time parameter was required to account for the time it takes theuser to choose between methods: the more methods to choose from, the longer the pausebefore executing a simple sequence in a command.command grammars use a different analytic representation, but are analyzing thesame kinds of mental events. the command language grammar (clg) (moran, 1981)and backus normal form (bnf) (reisner, 1981, 1984) have been used to describe theorganization of sequences of actions that fulfill goals. these grammars are sets of rulesthat show the different ways in which an ﬁalphabetﬂ of actions can be formed to produceacceptable ﬁsentencesﬂ that are understandable to a system or a device.for example, reisner (1981, 1984) treats user actions that are acceptable to thesystem as a language. she describes the structure of this language as a bnf grammar.figure 1 shows a sample of what in this formalism are called rewrite rules. at the higherlevels are the user's task goals and the possible methods that can achieve the goal. this ispresumably a representation of the components of plans the user has ready to evoke tofill an overall task goal. below these are the varieties of action sequences that can beelicited in a method. the top several lines of figure 1 are similar to the goals/subgoalsand methods of the goms analysis; the lower levels are similar to the keystroke modelsequences.compared to goms, this representation more compactly shows the alternativeways to accomplish a task or to enact a series of keystrokes; goms requires a newmethod for each alternative. while various methods (represented as sentences from sucha grammar) can be compared to see which takes less time, a grammatical representationis less adequate than goms in that it lacks any way to represent how a user selects themethod appropriate for the current situation.types of representations of users' knowledge10mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.use dn..>identify first line + enter dncommand + press enteridentify first line..>get first line on screen + movecursor to first lineget first line on screen..>use ﬁlocateﬂ strategy use scrollstrategyﬁlocateﬂ strategy..>move cursor to command inputfield + type ﬁlocateﬂ command + pressentermove cursor to command inputfield..>use cursor keys press pfcursornulltype locate command..>type ﬁlocateﬂ keyword + type linenumbertype locate keyword..>l+o+c l l+o+c+a+t+etype line number..>type numberfigure 1 a command grammar representation of actions necessary to edit aline using a word processor. rewrite rules applied to this domain are compactdefinitions of the many acceptable ways to get something done in a particularcommand language. one reads these rules from left to right; the lefthand termsare made up of the elements listed on the righthand side. elements connectedby a ﬁ+ﬂ are executed in sequence, elements connected by a ﬁšﬂ representalternative ways of invoking the same goal. for example, ﬁuse dnﬂ consists ofidentifying the first line, then entering the ﬁdnﬂ command, and then pressingenter. typing the locate keyword, however, includes typing ﬁloc,ﬂ ﬁl,ﬂ orﬁlocate.ﬂ source: reisner (1984:53).the language format of grammars, however, allows the use of standard sentencecomplexity measures to predict some aspects of user behavior: the more rules, the long ittakes a user to learn; the greater the sentence (sequence) complexity, the longer thepauses between keystrokes; the more terminal symbols in the language, the harder thelanguage is to learn. these predictions have not been fully tested, and there is somesuggestion in the literature about language understanding that these measures do notadequately predict how difficult it is to understand sentences (fodor et al., 1974; miller,1962). the formalism, however, allows a number of intriguing predictive possibilities forunderstanding and recalling command languages. see reisner (1983) for a discussion ofthe potential value of such grammars.types of representations of users' knowledge11mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.mental modelsin its most generic application, the term mental model could be applied to any set ofmental events, but few if anyone would claim such meaning for the word model. somewhat narrower in meaning, the term could be used for any thought process in whichthere are defined inputs and outputs to a believable process which operates on the inputsto produce outputs. in this sense, one could have a mental model of one's own behavior(ﬁif i do this, then that will happenﬂ), another person's behavior, the inputoutputcharacteristics of any software process run on a computer, or any information processmediated by people or machines. it could be a series of paired associates by which theuser predicts, through a causal chain, outputs of a process given its inputs.given these general possibilities for the term mental model, it is most commonlyused to refer to a representation (in the head) of a physical system or software being runon a computer, with some plausible cascade of causal associations connecting the inputto the output. accordingly, the user's mental model of a system is here defined as a richand elaborate structure, reflecting the user's understanding of what the system contains,how it works, and why it works that way. it can be conceived as knowledge about thesystem sufficient to permit the user to mentally try out actions before choosing one toexecute. a key feature of a mental model is that it can be ﬁrunﬂ with trial, exploratoryinputs and observed for its resultant behavior (sheridan et al., 1986).mental models are used during learning (such as using an analogy to begin tounderstand how the system works), in problem solving (such as in trying to extricateoneself from an error or performing a novel task), and when the user is reflecting on orattempting to rationalize or explain the system's behavior.users are typically described as using a mechanistic model; that is, the user isassumed to have a conceptual ﬁmachineﬂ whose simulated function matches the actualtarget machine in some way.3 three general kinds of models are called surrogates(young, 1983), metaphors (carroll and thomas, 1982), and glass boxes (duboulay etal., 1981). a fourth kind of model, the network model, is a composite, blending thefeatures of surrogates and glass boxes.3this may be more due to the fact that researchers are good at describingmechanistic models than to the fact that it is the only kind of model peoplehave. in fact, exploration of other representations is an important research need.types of representations of users' knowledge12mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.surrogatesa surrogate is a conceptual analysis that perfectly mimics the target system's input/output behavior and that does not assume that the way in which output is produced in thesurrogate is the same process as that in the target system. it is a system that behaves thesame, but is not assumed to be isomorphic in its internal workings. thus, while thesurrogate always provides the right answer (the one that the target system would havegenerated), it offers no means of illuminating the real underlying causal basis for theanswer. it is a good, complete analogy that may allow the user to construct appropriatebehavior in a novel situation, but it does not help the user explain whey the systembehaves the way it does.young (1983) noted that it is very difficult to construct an adequate surrogate, evenfor a fairly simple system like a handheld calculator. this raises the question of whetherpeople ever hold surrogates in their minds, even for simple devices.metaphor modelsa metaphor model is a direct comparison between the target system and some othersystem already known to the user. a common example, referred to widely in theliterature, is the metaphor that ﬁa text editor is a typewriter.ﬂ many investigators haveobserved that new users spontaneously refer to this typewriter metaphor during earlylearning about text processors (bott, 1979; carroll and thomas, 1982; douglas andmoran, 1983; mack et al., 1983). the explanations people offer for system behavior areoften couched in the vocabulary of the metaphor. furthermore, the extent to whichknowledge in the metaphor source domain matches the target domain correlates withperformance. that is, the taskaction pairs that fit both the metaphor source and thetarget system are easy to learn; those that do not are often learned last or remain constantsources of error. for example, learners have less trouble learning how to use characterkeys than the backspace and carriage return keys; the latter typically operate differentlyin text processors than they do in typewriters.types of representations of users' knowledge13mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.unlike surrogates, metaphor models are easy to construct or learn, and they provideexplanations of why the system behaves as it does. however, metaphors vary greatly inaccuracy. for example, ﬁthe interface is a desktopﬂ seems less accurate than ﬁvalues areput into storage locations.ﬂone difficulty with using metaphors in analyzing users' behavior with computers isit is difficult to find out what the users' metaphors are. as young (1983) put it, ametaphor analysis exchanges the problem of describing what the user knows about thetarget system for the problem of describing what the user knows about the metaphorsource. for example, user have to know enough about pipelines for the metaphor ﬁa flowchart is a pipelineﬂ to be useful. in addition, metaphors that map one domain perfectlyinto another are rare. consequently, metaphors can sometimes be misleading as well ashelpful. the hydrodynamic metaphor for electric current, for example, is only good for alimited subset of phenomena, and is misleading for many others. similarly, thetypewriter metaphor for a word processor helps with some actions (like using thebackspace key), but interferes with the learning of others (like the return key) (douglasand moran, 1983).glass box modelsglass box models lie between metaphors and surrogates. they are surrogates in thatthey are perfect mimics of the target system. but they are metaphors in that they offersome semantic interpretation for the internal components. for example, mayer (1976)discusses a glass box mimic for a basiclike programming language. this glass box isnot simply a surrogate, because its components are presented via metaphors (input as aticket window, storage as a file cabinet). it can be run to perfectly predict outputs frominputs, but it can also be interpreted via these metaphors. yet it is also not a simplemetaphor; it is a composite metaphor (carroll and thomas, 1982; rumelhart andnorman, 1981). it does not merely exchange the target system for a metaphor source intoto; it uses aspects of several metaphors to provide the surrogate behavior.glass boxes have been used primarily in a prescriptive context rather than in adescriptive one. mayer's (1976) glass box is not a mental structure that was discovered; itis a mental structure thattypes of representations of users' knowledge14mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.was taught to the user (e.g., subjects were instructed to think of input as a ticketwindow). studies of prescriptive conceptual models tell us something about what kindsof models are useful, and about models that people could generate. on the other hand,they can validate prescriptive models that help users of complex systems when it is hardfor the user to deduce an adequate representation merely from experience.network representations of the systemnetwork representations contain the states a system can be in and the actions theuser can take that change the system to another state (miller, 1985). one particular typeof network representation, the generalized transition network (gtn), contains detaileddescriptions of what the system does (kieras and polson, 1983). gtn's are statetransition diagrams that represent the visible states of the system (i.e., the display on thescreen) as nodes, and the actions the user can take at each state (the commands or menuchoices) as arcs. the connected nodes and arcs form a network that shows the sequenceof states that follow user actions at each point in the software interaction. gtn's andother network diagrams are often used as tools in system development, to give thedesigner a picture to refer to in order to keep track of what can be done at every state inthe transaction. figure 2 illustrates a portion of one of these networks for the actions thatcan be taken when a user enters a system and loads the word processing application.networks can also be used to describe what the user knows about the system(olson, 1987). olson (1987) suggests that gtns be used to represent users' knowledgeof system states and allowable actions; these can be compared to the gtn of the actualtarget system to measure the user's level of learning or understanding. examination ofthe parts of the real gtn that are missing in the user's representation could indicatedareas in which learning or remembering certain functions is difficult.the gtn is like a surrogate representation in that it does not give an underlyingexplanation about why the elements are related in the way that they are nor how theinternal system components behave. nor is there any indication of the purpose theseactions fill toward a user's goal. it does, however, represent what the user knows abouthow the system works in simple stimulusreponse terms. a gtn displays the simpleresponse that can be expected from the system given each action the user takes. and,importantly to the user, knowledge of these actions and their consequences can be usefulwhen the user must solve problems, either when an error has just occurred or when anovel goal has arisen and the user needs to decide on an appropriate sequence of actions.types of representations of users' knowledge15mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.figure 2 a generalized transition network (gtn) representation of part of thetask of editing a document. circles represent states or tasks, arcs represent theconnections between states, and labels to the arcs represent the actions the usertakes. source: kieras and polson (1983:104).types of representations of users' knowledge16mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.comparisonsit is useful to consider the relation between sequence/method representations andmental models. people undoubtedly have both kinds of knowledge when they usecomputing systems. but research on these two approaches is largely complementary inthat the kinds of questions addressed about one kind of representation have beendifferent from those about the other. briefly, the sequence/method representations aremore analytic in that they can predict behavior (except errors) in some detail. althoughthe sequence/method approach has not typically dealt with predicting user errors,attempts have been made to show how user learning takes place. the mental modelsapproach, on the other hand, accounts for errors as well as accurate behavior in noveland standard situations, but does not predict the details of behavior well nor how themodels are learned.sequence/method representations, because they are composed of goalaction pairs,by their very nature predict how knowledge is used. to date they have represented onlyhow to accomplish routine tasks (in which all the goalsubgoal and subgoalactionrelations have been worked out) but have little or nothing to say about how knowledge isused in nonroutine tasks, such as in recovering from an error or behaving in an entirelyunfamiliar situation. they do not have much generality in their conditions. and, there isno posited mechanism for problem solving when a new situation fits several generalconditionaction pairs. without this mechanism, these analyses cannot account for errors.some attempt has been made to account for how sequence/method representations arelearned. lewis (1986) provides an account of how users might acquire goalactionknowledge after they watch another person use the system. through several simpleheuristics that link actions to probable causes, the user begins to build a reasonable set ofrules. the acquisition of rules is detailed bytypes of representations of users' knowledge17mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.lewis (1986), but the further learning in finetuning those rules is not covered. kierasand bovair (1986) do not explain original learning per se, but have shown that learning anew system is speeded up if the user is familiar with another system that has many of thesame rules in common. neither of these approaches addresses the continued learning thatgoes on as the user acquires or discovers new strategies for efficiency.research on mental models, on the other hand, has not concentrated on the detailsof how a user uses a mental model nor how it is acquired. douglas and moran (1983)have produced the most detailed analysis of the behavior of a user who has a mentalmodel. they examined the analogy of ﬁa text processor is a typewriterﬂ by noting thetypewriter conditionaction sequences that matched and mismatched those in the newsystem. those conditionaction pairs that matched were learned easily and quickly, andthose that did not match produced continued errors and pauses. other researchers haveattempted to make the analysis of the behavior of the user who has a mental model morespecific and revealing (foley and williges, 1982; moran, 1983; payne and green, 1983).what is missing from these analyses, however, is how users use their mental models tocome up with a set of appropriate actions. there are likely to be some very interestingcognitive actions going on in the pause between the presentation of the problem (e.g., thefeedback from the screen after an error) and the choice of the next action.most of the empirical work on the effectiveness of mental models and the predictivepower of sequence/method analyses has been at a gross behavioral level. the studies ofexperts' chunking of information (chase and simon, 1973, for example) are almostcompletely empirical; they focus entirely on the acquisition of the condition part of aconditionaction pair and offer little basis for theory. the grammatical approaches oftenhold a key assumption: that the fewer actions there are per task, the cognitively simplerthe task. recent work has raised questions about the accuracy of this assumption (olsonand nilsen, 1988; rosson, 1983). there are occasions when a task has a few actions, butthe planning and calculating necessary to make those actions is difficult.moran has described a number of connections and contrasts between sequence/method and mental model approaches. the goms analysis (a methods analysis) andclg (a blend of method and mental model) sprang from common theoretical roots.indeed,types of representations of users' knowledge18mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.goms can be viewed as a simplified and more parameterized, compiled clg. moran(1981), however, stresses two contrasts. first, where clg incorporates a limited mentalmodel of the system in its semantic level, goms incorporates no mental modelwhatsoever. goms incorporates only the knowledge required to perform a task. second,where the focus of clg is the functional description of various levels of user knowledgeand the mappings between these levels, the focus of goms is the sequencing ofoperators and the time requirements for each. the bottom line for goms is predictingperformance times.kieras and polson (1983) simulate users' behavior on particular computer systems.they have two representations in their simulations, which with an additional twist can beviewed in much the same spirit as moran's (1981) view of the relation between clg andgoms. in the kieras and polson (1983) model, a jobtask representation describes theperson's understanding of when and how to carry out tasks (very much like goms). thesimulated user's behavior is responded to by a simulation of the system, a devicerepresentation, which is a gtn of the states and transitions between them in the system.some knowledge of this system behavior, a mental gtn, can represent what the userknows about the systemša thin, surrogate mental model. the former gomslikerepresentation is the user's knowledge that produces performance, while the latter, themental gtn, could be the user's theory during learning, problem solving, and explaininghow the system works.how users' knowledge affectstheir performancethe discussion up to this point has treated what the user knows as a static structure.while we have alluded to its underlying role in behavior (learning, problem solving,explanation, skill), we have not focused on these behavioral processes per se.nevertheless, this aspect is critical both to assessing the empirical content of currentanalyses and to determining how these analyses might be applied to practical problemslike the design of user interfaces and training materials.how users' knowledge affects their performance19mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.chaos and misconception in both novices and expertslearning involves internalizing, constructing, or otherwise attaining arepresentation of the system being learned. how does this process proceed and what areits early results? the summary picture is of a halting and often somewhat nonconvergentprocess of problem solving and invention (e.g., bott, 1979; mack et al., 1983; rumelhartand norman, 1981). indeed, the models that learners spontaneously form are incomplete,inconsistent, unstable in time, overly simple, and often rife with superstition.a person may develop an understanding that is adequate for simple cases but thatdoes not extend to more complex cases. for example, mayer and bayman (1981) foundthat users of calculators often believed that evaluation only occurs when the equals key ispressed. scandura et al. (1976) describe a student who concluded that the equals key andplus keys on a calculator had no function because they caused no visible change in thedisplay. norman (1983) describes learners who superstitiously pressed the clear key oncalculators several times, when a single key press would do. people learning to use asimple programmable robot developed wrong analogical models of its behavior that theyaccepted without testing until the models failed to predict the actions the robot took(shrager and klahr, 1983). mantei (1982) found that users performing a task in a menubased retrieval system developed and maintained simplistic sequences of actions thatwere eventually ineffective in accomplishing their search goals.chaotic and misconceived conceptual models are not merely an issue of earlylearning and something that users outgrow. experienced users hold them as well. forexample, mayer and bayman (1981) asked students to predict the outcomes of key presssequences on a calculator. even though all of the students were experienced in the use ofcalculators, their predictions varied considerably. for example, some predicted that anevaluation occurs immediately after a number key is pressed, some predicted thatevaluation occurs immediately after an operation (e.g., plus) key is pressed, and somepredicted that an evaluation occurs immediately after equals is pressed.rosson (1983) found that even experienced users of a text editing system often hadrather limited command repertoires, routinely employing nonoptimal methods (such asmaking repeatedhow users' knowledge affects their performance20mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.local changes instead of a single global change). even in large powerful systems, most ofthe activity involves the use of only a very small portion of the system. in the case ofunix, for example, 20 of the available 400 commands accounted for about 70 percent ofthe usage (kraut et al., 1983). like the mayer and bayman work (1981), this suggeststhat even an extensive amount of experience does not necessarily lead the user to acomplete, consistent, or even correct conceptual model. there are some things about asystem that most users never learn.skilled performancehuman performance analyses have been well developed in vehicular control (e.g.,aircraft, ship, automobile) and target pursuit tasks. many of these analyses explicitlyhypothesize a mental model of the system being operated (e.g., baron and levison,1980; jagacinski and miller, 1978; pew and baron, 1983; veldhuyzen and stassen,1976). in these cases, the mental model is used to anticipate the response of a dynamicsystem and hence to overcome the deleterious effects of time delays either from otherhumans or hardware. these models have produced good descriptions and predictions ofhuman performance.because these models deal with spatiotemporal trajectories, their applicability islimited to continuous detection and movement tasks. in contrast, episodic models ofmovement that incorporate an additional, abstract level of description in terms of discretesituationaction pairs have much in common with goalaction models in humancomputer interaction. discrete representational and data reduction techniques developedfor episodic skilled performance (jagacinski et al., in press; miller, 1985) may proveuseful in the domain of humancomputer interaction. software user tasks do, however,typically involve a larger set of situationaction pairs than is covered in humanperformance analyses, and they probably involve more varied categorization andplanning by the human operator. whether they can be generalized to the greatercognitive complexity of humancomputer interaction tasks is an open question.if we assume that knowledge of simple sequences is in the form of goalactionpairs, then we should be able to apply what we know from traditional verbal learningstudies about the retention of paired associates (e.g., hilgard and bower, 1975; postmanandhow users' knowledge affects their performance21mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.stark, 1969) to predict which systems will be easy to learn and what kinds of errors willoccur. for example, presumably, those systems that have few paired associates to belearned or those that have distinct, nonconfusable goalaction pairs will be easy to learnand remember.landauer et al. (1983), barnard et al. (1981), and others have explored certainaspects of this issue with mixed results. landauer et al. (1984) discuss the difficulties ofconstructing command names that are natural, that is, those that would have existinggoalaction paired associates in memory and ready to transfer easily to a new task. theyargue that if one incorporates command names generated by naive users, these names arenatural but often are not distinctive enough to allow users to keep from getting themconfused among each other. preexisting paired associates can help transfer, but if theyare not distinct paired associates as a set (e.g., ab may be good until it must be learnedalong with ac), the confusion can offset any positive effect from their naturalness.polson and kieras (1984, 1985) embody the goms model in a production systembased simulation of users' behavior while using software. this is a very concreterepresentation of what the user knows when performing welllearned tasks and has anumber of confirmed behavioral correlates. their analyses postulated that the number ofproductions (the number of rules needed to decompose goals into subgoals, to findmethods to fit the subgoals, and to execute the sequence of actions in a method)necessary to perform a task is a good predictor of the time it takes to learn a system, thatthe number of productions that two systems have in common predicted the ease oflearning the second after the first, that the number of productions used in constructingthe next overt action predicted the delay from one overt action to the next, and that thenumber of items held temporarily in a working memory predicted the likelihood of errorsor delays (kieras and bovair, 1985; kieras and polson, 1985; polson and kieras, 1984,1985; polson et al., 1986). some of the predictions afforded by this specific analysishave been successfully tested; others are being tested now.though this approach is to be lauded for its specificity and the accuracy of some ofits predictions, its weakness lies in determining how one counts the number ofproductions required for a task. since production rule formalisms are generalprogramming languages, a single function can be programmed in many ways.consequently, for purposes of replicability, it is importanthow users' knowledge affects their performance22mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.for kieras and polson (1985) to specify further what production language style underliesthese production analyses, and further, whether this style can be argued to be consonantwith a theoretically reasonable model of the architecture in which human informationprocessing operates.the chief limitation of the goms analysis is that it considers only errorfreeperformance. this is a serious limitation since even skilled users spend at least a quarterof their time making and recovering from errors. in goms, goals are very specific totask situations; they are not currently in a general form (card et al., 1983). this is not alimitation in principle, however, since goms is a deliberate simplification of newelland simon's (1972) general problem solver, a model that is general enough to describeany goaldirected behavior. robertson (1983) suggests how error and error recoverycould be incorporated into a gomslike analysis.rumelhart and norman (1982) present a performance analysis of skilled typing thattakes the description of errors as a primary concern. the treatment of errors in theiranalysis raises an important issue. in order to describe the occurrence of some kinds oferrors, they were forced to change the assumption of how information is stored inmemory. the analysis was fundamentally altered in order to qualitatively predict thetypical errors for the task. this raises the question of whether goms, in which onlyerrorfree behavior was analyzed, embodies a representation that can be generalized toreal performance that includes errors.applying what we know of theuser's knowledge to practicalproblemsthe foregoing discussions have reviewed various representations of the user'sknowledge of a system. we have described them in terms of the theoreticalrepresentations posited and some of the cognitive processes included in each type ofanalysis. it seems safe to conclude that while the area of research on users' mentalrepresentations is very active, it is not yet well developed (see the researchrecommendations section below). nevertheless, software human factors is an appliedarea, and there is continual pressure to apply what we do know in this work to the task ofdesign and training.applying what we know about mental representations to practical ends raises manyquestions. for example, if we knew whatapplying what we know of the user's knowledge to practicalproblems23mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.the user knew, how would we use this knowledge in design? do we build the userinterface to reflect a consistent mental model? if so, what does the input and presentationlook like? should we tell the learner what model to build?designing interfacesif the interface suggests or reflects an appropriate model, then the user couldconceivably learn it with less guidance and perform it with fewer errors. the question is:what should the model be?one approach to picking a model is to design user interfaces to accord with naiveuser conceptual models (carroll and thomas, 1982; mayer and bayman, 1981).although this approach is simple and straightforward, its general utility is open toquestion. for example, wright and bason (1982) designed two software packages for acasual user population. one package was designed to be maximally consistent with theusers' prior knowledge; users were asked how they thought about their data and whatthey wanted to be able to do with it, and this formed the basis for the user interface. thesecond package was also designed with input from potential users, but in this case, thedesigner used this information to determine how the users ought to think about their dataand operations on it. the finding was that, in every way, the second package was a betterdesign.in a similar vein, landauer et al. (1983) replaced the verbs in a word processor'scommand names (like append, substitute, and delete) with those that secretariesgenerated most often when describing to another secretary how to change a marked upmanuscript (such as add, change, and omit). paired associate learning theory would havepredicted that these welllearned goalaction pairs from the secretaries' own vocabularywould have been good command names for secretaries learning a new word processor.the goalaction pairs are presumably preexisting paired associates, ones not needing newlearning. learning the word processor with these command names, however, was nobetter than learning the one with the system developers' names or even one with randomnames like allege, cypher, and deliberate. naive users do not necessarily design bettersystems.a variant of the naive model approach is to enter into the design process with apreconceived model, and then to iteratively build a prototype, test it, and refine thedesign (including the user'sapplying what we know of the user's knowledge to practicalproblems24mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.model) until acceptable usability is attained. this technique is the classic empiricalapproach (dreyfus, 1955); it has been employed in recent designs that use the desktopmetaphor in the interface for office systems (bewley et al., 1983; morgan et al., 1983),as well as in other application system designs (gould and boies, 1983; wixon et al.,1983). the theoretical problem with this approach is that in the context of iterative andoften radical redesign of a user interface, it is difficult to clearly separate the effect of themodel on usability from that of other aspects of the redesign.4a second design approach is to reduce the problem of communicating anappropriate conceptual model to the user by simplifying the system and its interface.duboulay et al. (1981) stress this in their characterization of a glass box model thatconsists of only a small number of components and interactions, all obviously reflectedin the feedback that learners get from running the system. carroll and carrithers (1984)implemented this approach by providing new users with only a small but sufficientsubset of commands to learn. this small set fits a relatively simple conceptual model.carroll and carrithers (1984) called this the ﬁtraining wheelsﬂ approach, borrowing theanalogy from learning to ride a bicycle. once the subset of commands was learned, theuser was gradually introduced to more complicated or more rarely used commands. thisapproach led to faster and more successful learning. an important question raised in thiswork, however, is how to decide which subset of commands is sufficient to do the taskand fits a simple model. furthermore, it raises the question of how to embellish theinitially simplified conceptual model so that the change does not disrupt the learning theuser has already accomplished.a third design approach focuses on the method that the user learns rather than onthe mental model. moran (1981), reisner (1981, 1984), and young (1981) all stress thepotential utility of taskoriented knowledge for design. such knowledge can berepresented formally. the suggestion is that these representations can be examined ormanipulated prior to actual construction of the user interface to determine the leastcomplex organization for4however, olson et al. (1984) highlight the importance of running prototypetests with two prototypes that differ in only one variable at a time, so that theeffects of individual design changes can be measured independently.applying what we know of the user's knowledge to practicalproblems25mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.the interface. for example, the designer can calculate values of merit for a system basedon the number of rules in a grammar, the number of different terminal symbols, orvarious other metrics known in computational linguistics.this approach could also makeit possible to define precisely concepts like consistency: similar tasks or goals should beassociated with similar or identical actions (moran, 1983). for example, deleting asentence ought to have similar actions to deleting a paragraph. empirical work hasshown the importance of such concepts (e.g., barnard et al., 1981; black and sebrechts,1981; thomas and carroll, 1981; but see landauer et al., 1984, for a caveat).it should be noted that analysis of these relations, like consistency, may not go veryfar toward describing the interface design fully. for example, two interfaces with exactlythe same grammatical description of a command language may have very different visuallayouts. the visual layouts may lead to performance differences not predicted by acalculated complexity measure that is based only on inconsistencies in the commandlanguage. with the exception of dunsmore (1986), most grammars do not representfeatures of visual layout that are known to be important. dunsmore (1986) predicted andthen experimentally verified that a crowded display would be more difficult for users todeal with than an uncrowded one. furthermore, with the exception of shneiderman(1982), whose multiparty grammars can be used to describe both a user's action and thesystem's response, there has been little attempt to integrate models of the variouscomponents of a system. moreover, optimizing a design with respect to a taskorientedanalysis will not necessarily include any of the design considerations that would beindicated by optimizing the presentation of a good mental model.user trainingif a system has been built to conform to a consistent model or a wellformed set ofmethods, training may simply involve presenting the user with the model or methods.several researchers have been concerned with developing techniques for providing userswith appropriate conceptual models, something that even stateoftheart instructionalmaterials for software often fail to do (bayman and mayer, 1984; duboulay et al., 1981;halasz and moran, 1982). the benefits from presenting a mental model, however, areunclear.applying what we know of the user's knowledge to practicalproblems26mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.schlager and ogden (1986), for example, incorporated both a method representationand a mental model in the training materials for teaching students how to form successfulqueries in a data base. for those specific query types that fit the model or methodspresented, both representations speeded learning, regardless of whether therepresentation was a method representation or a mental model. errors and difficultyoccurred only when queries were different from the method or model taught.mayer (1976, 1981) provided students with a diagrammatic tool which incorporateda variety of concrete metaphors (e.g., input as a ticket window and storage as a filecabinet). students who were exposed to this tool before studying a training manual werelater able to perform better on both programming and recall tasks.kieras and bovair (1984) taught people how a simple device worked either by arote sequence of steps, with a model of the system, or with an analogy. the sequence ofsteps showed them what to do when. one model displayed what part was connected toanother part beneath the surface, as if a flow diagram were painted on the control panel.the analogy described the control panel as being part of a mock spaceship, explainingwhat each control knob did in terms of battlerelated actions. the results showed nobenefit from either of the models over the rote sequence. on closer inspection, kierasand polson (1985) noted that neither ofthe models gave the user any actionorientedhelp; the models merely gave a story about what the connections were, not how theyworked.halasz and moran (1982) taught students how to use a calculator using either a stepbystep action sequence to do standard calculations or instructions which included averbal model of how the internal registers, windows, and stacks worked. they found thatperformance on standard tasks was identical for the two groups, but that the group wholearned the model performed better on novel tasks.foss et al. (1982) provided a file folder metaphor to students learning to use a texteditor. they found that students who were provided with the metaphor learned more inless time. in the same domain, rumelhart and norman (1981) used a composite of threemetaphors: a secretary metaphor, which was used to explain that commands can beinterspersed with text input; a card file metaphor, which was used to describe thedeletion of aapplying what we know of the user's knowledge to practicalproblems27mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.single numbered line from a file; and a tape recorder metaphor, which was used toconvey the need for explicit terminators in files. although performance was goodoverall, the fact that there were several metaphors produced cases in which a subjectwould employ one of the metaphors when another was appropriate.most of this work has focused on the use of mental models narrowly in training,namely, by telling the student the model or by providing simple and explicit advancedorganizers (ausubel, 1960). in another approach, an explicit mental model wasprescribed; a system's training manual had a diagrammatic model of control flow for amenubased system (galambos et al., 1985). the resultant benefits were equivocal,however. even greater integration between model and training appears necessary. thefeasibility of this approach is exemplified in systems that have mental model analyses intheir expert systems to interactively diagnose learner problems and to provide tailoredsupport (e.g., burton, 1981). no systematic behavioral studies have been carried out,however, to evaluate the effectiveness of this approach.a more theoretical issue in the area of training pertains fundamentally to the natureof learning and the implications for designing training programs. one view of humanlearning and memory conceives of learning as an active process of problem solving inwhich concepts are created by the learner (e.g., jenkins, 1974; wittrock, 1974). thisview contrasts with one in which learning is merely the storage of concepts in memory.in the latter view, a learner can be given a conceptual model explicitly (by diagram or averbal explanation). in the active learning view, however, a conceptual model must beinvented by the learner after an appropriate series of experiences.mayer (1980) adopted the active learner view. he asked learners to generate ametaphorical elaboration of programming statement types as they were learned. forexample, after learning a for statement, the student was asked to describe its functionusing a metaphorical desktop vocabulary. he found that learners who had provided theseelaborations were later able to perform better on novel and complex programmingproblems.carroll and mack (1985) suggested that taking a serious active learning view raisesthe possibility that metaphors are useful not only when they provide familiar descriptionsof novel experiences, but also when they provoke thought by failing to accord perfectlywith the target of the metaphor comparison. carroll and mackapplying what we know of the user's knowledge to practicalproblems28mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.(1985) described a learner who was trying to learn a desktop interface and who initiallytried to get a piece of paper from a paper pad icon by sweeping the cursor across the iconin a tearing motion. here the desktop metaphor failed but also served to highlighteffectively a specific fact about icon manipulation for the learner.the active learning view provides a means of reconciling the observation thatmental models are often chaotic and misconceived and the fact that users do oftensucceed in learning and using software. the suggestion is that people develop modelsthat are good enough to suit their current goals. defective conceptual models mayultimately play useful roles in learning and adequately support some user activity. it is anopen question, however, whether they can actually facilitate learning and be used moreeffectively than more explicitly provided and more correct models (mack et al., 1983).research recommendationsthese observations on the state of research and application of the concept of whatthe user knows lead to the following research recommendations.1. detail what a mental model would consist of and how a person would useit to predict a system's behavior. the term mental model has been usedconfusingly in the literature as referring to goaloriented proceduralknowledge, as well as knowledge about the components of the device, theirfunctions, their relations to other components, and their workings. to datethere have been no concrete characterizations of what a mental model isand how a person would run it to try out various simulated inputs. oneattempt at this specification of a working mental model, a device modelthat is used for guiding external actions, resides in davis's (1982) expertsystem for diagnosing electrical circuit failures. this model is used by thesystem to determine where physically a fault might be and, if it were at aparticular location, what the device's expected behavior would be. perhapsdavis's (1982) formalization of an internalized device model might serveas a base from which to build specifications of what a mental model wouldbe and what mental operations would be necessary in order to use themental model to make predictions about a system's behavior.research recommendations29mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.yet, specifications of how a person would use a mental model topredict what a system will do is not sufficient to predict the user'sbehavior. our understanding of mental models (if they exist) needs to beembedded in a model of a fullblown cognitive system, one that hasproblemsolving and decisionmaking processes that are sufficient toinitiate the model runs, collect the results, and decide on an external action.2. investigate whether people have and use mental models of various kinds. probably the most basic question in this area, still far from being answered,is whether people construct and use mental models at all. and, because ofconfusion of terminology in the literature, behavioral evidence is notclearly supportive. even when we confine ourselves to the specificdefinition of mental models used in this report, however, there is littleevidence that people have and use mental models. so far, the majority ofevidence for mental models has come from people's selfreports that theyform and use them (which may be posthoc rationalizations), and fromsome evidence that when taught a system model or analogy, performanceis sometimes better and learning may be faster. specific research is neededto demonstrate whether people have models and that their behavior isclearly distinguishable from that produced by having stored sequence/method representations.3. determine the behaviors that would demonstrate the model's form and theoperations used on it. if a person has a mental model, there may be someobservable behavior that would give an analyst evidence of its form.traditionally, experimental psychologists have made inferences about theexistence of mental events by carefully constructing test situations withsystematically varied features and observing particular overt responsessuch as the time that it takes to make a certain judgment or carry out anaction, or the amount and kinds of errors made. the construction of theappropriate comparative test situations and the inferences that can bedrawn from the responses, times, and errors must be based on a clearernotion of the form that the model might have and the processes that mayact on it.if the analyst can predict behavior knowing that the person has a mentalmodel of a particular sort, then the analyst should be able to discover themental models of other people from systematic examination of theirbehavior. multidimensional scaling (shepard et al., 1972), unfoldingtheory (coombs, 1964), and ordered treeresearch recommendations30mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.analysis (reitman and rueter, 1980) are examples of techniques that allowthe analyst to infer particular mental representations from behavior.perhaps aspects of behavior can reveal the form of a working mentalmodel. this work could follow from a program of research that built on thetheoretical work outlined above.4. explore alternative views of sequence/method representations and thebehavior predicted from them. we currently have a better conception ofwhat it means to have sequence/method representations and what processesmay act on them to produce behavior than we do of mental models. gomsrepresents the structure of goals, methods, and actions in a mentalhierarchy for welllearned cognitive tasks. kieras and polson's (1985)production system formalism and its inference engine (a standard set ofprocedures for keeping track of where one is in a process and choosing thesubsequent actions) is a concrete specification of this kind of knowledgeand the processes that act on it. from that formalism follow concretepredictions of behavior, such as particular responses (key presses), theirtimes, and the errors. a body of empirical data is growing, answeringquestions about which aspects of the representation affect behavior.what is needed is more research in this vein. formalisms of knowledgeand operational mechanisms would be specified and the behavior of otherkinds of sequence/method representations would be predicted. empiricalstudies could then be formed to answer specific questions about theadequacy of the formalism, in detail, replacing the vague generalizationsand contradictions that seem to plague research in this area today.5. explore the types of mental representations that may exist that are notmechanistic. most of the mental models that are conceived in this researchare mechanistic in nature. the sequence/method representations aremechanistic and serial. these consist of components and processes thatmimic physical devices. there may be mental representations of othertypes, however, that drive people's exploratory and explanatory behavior.people claim to make inferences and explorations from stored visual andauditory images; mathematicians experiment mentally with computationalsystems, making inferences before showing any external behavior; peoplelikely reason at different levels of abstraction about a system, makinginferences of a very general nature in planning before exploring details in astepbystep fashion. thereresearch recommendations31mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.may be visual, auditory, computational, or hierarchical systems that formhelpful bases for people's reasoning. these other possible types of mentalrepresentations should be made concrete, and their behavioral correlatesshould be explored.6. determine how people intermix different representations in producingbehavior. this report has reviewed a variety of types of knowledge thatmay be held by a user of a computer system. it is likely that users havesome knowledge stored in several of these representations: some wellknown procedures for executing simple sequences; some wellformedgomslike structures for doing familiar but more complicated tasks; andsome mental models that help the user explore alternative actions to takewhen an error occurs or when a novel task is presented to them. if all ofthese representations exist simultaneously, then we need to know wheneach is used and how the person moves between them and/or combinestheir operations or products. there is likely to be some problemsolving ordecisionmaking apparatus that guides the overall task behavior,sometimes exploring unknown territory with a process like meansendsanalysis or running a mental model, and other times executing welllearnedactions from stored goal structures (see, for example, the extensiveliterature on automaticity; shiffrin and schneider, 1977). an integratedperformance view is called for.7. explore how knowledge about systems is acquired. if we can discover theform of the representation of knowledge that people have about computersystems, we would like also to know how that information was acquired.lewis's work (1986) on how people make inferences about a system fromwatching its behavior is a good example of how to specify concretely howpeople learn complicated tasks on computers. work is also needed on howpeople acquire mental models, simple sequences, and methods. this workwould have an impact not only on the design of systems and their training,but also would give some basic knowledge about the problem of learningcomplex behavior in general.8. determine how individual differences have an impact on learning of andperformance on systems. individuals' cognitive capacities differ, makingdifferent computer users more and less capable. some of these differencesare likely to arise from simply having more knowledge from longerexposure to the system. exposure could provide a user with more taskknowledge as well as moreresearch recommendations32mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.specific and more accurate mental models. some of the differences inperformance, however, may arise from basic individual differences inabilities. for example, gomez et al. (1983) have shown that people whoare not good at visual memory have difficulty with some word processors.further, they found that a system that required less recall of a commandsyntax reduced the performance differences found between those whocould recall locations and those who could not. we need to know moreabout individual cognitive differences and their concomitant effect onpeople's mental representations of and performance on complex tasks. theresults will have implications for both the design of systems and theconstruction of training sequences for a particular system for particularusers.9. explore the design of training sequences for systems. a related trainingissue surrounds the idea of ﬁtraining wheels,ﬂ the notion that a scaleddownsystem is easier to learn initially. specifying and analyzing the mentalmodel or sequence/method representations implied by the scaleddownsystem may lead designers to build more coherent systems and moreeffective training sequences. further, this analysis may indicate howinformation about the full system should be taught as an addon to thetraining wheels system.10. provide system designers with tools to help them develop interfaces thatinvoke good representations in users. there is probably some guidancethat can be provided to systems designers while they design the userinterface to ensure that the sequence/method representation or the mentalmodel will be an effective guide to accurate performance. such tools maycome in the form of user interface management systems; which constrainthe design set. the goal may be to constrain the ways that the designerscan display things or constrain the ways that they can allow the user toinvoke a command so that a coherent, easily understood set is formed, onethat invokes in the user a good mental model or a coherent set of goalactions pairs. designing these guidance tools is an important researchtopic, one that can aid the transfer of technology from the laboratory to thedesign and development arena.11. expand the task domain to more complex software. most of the research inthe area of mental models and sequence/method representations for humancomputer interaction has focused on text processing and simple devicemodels. whatever resultsresearch recommendations33mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.emerge from these areas should be tested for their applicability to morecomplex, nonexclusively textbased tasks, such as graphics design, tacticaldecision making, project planning and tracking, and data base query. it islikely that the complexity of these tasks, in which the user is almost neverdoing a task that is welllearned, requires the user to use mental modelsand to try out actions never used before. these may be ideal domains inwhich to test notions of the use of mental models, the productiveinteraction of sequence/method representations and mental models, and theinvolvement of general problemsolving skills, reasoning, and decisionmaking.referencesadelson, b. (1981) problem solving and the development of abstract categories in programminglanguages. memory and cognition, 9, 422433.ausubel, d. p. (1960) the use of advance organizers in the learning and retention of meaningfulverbal material. journal of educational psychology, 51, 267272.barnard, p. j., hammond, n. v., morton, j., long, j. b., and clark, i. a. (1981) consistency andcompatibility in humancomputer dialogue. international journal of manmachinestudies, 15, 87134.baron, s., and levison, w. h. (1980) the optimal control model: status and future direction.proceedings of ieee conference on cybernetics and society. cambridge, ma.bayman, p., and mayer, r. e. (1984) instructional manipulation of users' mental models forelectronic calculators. international journal of manmachine studies, 20, 189199.bewley, w. l., roberts, t. l., schroit, d., and verplank, w. l. (1983) human factors testing ofxerox's 8010 ﬁstarﬂ office workstation. proceedings of the 1983 chi conference onhuman factors in computing. new york: association of computing machinery.black, j. b., and sebrechts, m. m. (1981) facilitating humancomputer communication. appliedpsycholinguistics, 2, 87134.bott, r. (1979) a study in complex learning: theory and methodology. report 82. center forhuman information processing, university of california at san diego, la jolla, ca.burton, r. b. (1981) debuggy: diagnosing bugs in a simple procedural skill. in d. h. sleemanand j. s. brown (eds.), intelligent tutoring systems. london: academic press.card, s. k., moran, t. p., and newell, a. (1980a) computer text editing: an information processinganalysis of a routine cognitive skill. cognitive psychology, 12, 3274.card, s. k., moran, t. p., and newell, a. (1980b) the keystroke level model for user performancetime with interactive systems. communications of the acm, 23, 396410.references34mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.card, s. k., moran, t. p., and newell, a. (1983) the psychology of humancomputer interaction. hillsdale, nj: erlbaum.carroll, j. m., and carrithers, c. (1984) training wheels in a user interface. communications of theacm, 27, 800806.carroll, j. m., and mack, r. l. (1985) metaphor, computing systems, and active learning.international journal of manmachine studies, 22, 3957.carroll, j. m., and thomas, j. c. (1982) metaphor and the cognitive representation of computingsystems. ieee transactions on systems, man, and cybernetics, smc12, 107116.chase, w. g., and simon, h. a. (1973) perception in chess. cognitive psychology, 4, 5581.coombs, c. h. (1964) a theory of data. new york: john wiley & sons.davis, r. (1982) expert systems: where are we? and, where do we go from here? the aimagazine, spring, 322.douglas, s. a., and moran, t. p. (1983) learning text editor semantics by analogy. proceedings ofthe 1983 chi conference on human factors in computing. new york: association ofcomputing machinery.dreyfus, h. (1955) designing for people. new york: simon & schuster.duboulay, b., o'shea, t., and monk, j. (1981) the black box inside the glass box: presentingcomputing concepts to novices. international journal of manmachine studies, 14,237249.dunsmore, h. e. (1986) a formal grammar approach to human factors research. technical report623, department of computer science, purdue university, west lafayette, in.egan, d. e., and schwartz, b. j. (1979) chunking in recall of symbolic drawings. memory andcognition, 7, 149158.ehrlich, k., and soloway, e. (1984) an empirical investigation of the tacit plan knowledge inprogramming. in j. thomas and m. schneider (eds.), human factors in computingsystems. norwood, nj: ablex.embley, d. w., lan, n. t., leinbaugh, d. w., and nagy, g. (1978) a procedure for predictingprogram editor performance from the users point of view. international journal of manmachine studies, 10, 639650.fodor, j. a., bever, t. g., and garrett, m. f. (1974) the psychology of language.  new york:mcgrawhill.foley, l. j., and williges, r. c. (1982) user models of text editing command languages. humanfactors in computer systems proceedings. washington, dc: national bureau of standards.foss, d. j., rosson, m. b., and smith, p. l. (1982) reducing manual labor: an experimentalanalysis of learning aids for a texteditor. human factors in computer systemsproceedings. washington, dc: national bureau of standards.galambos, j. a., sebrechts, m. m., wikler, e. s., and black, j. b. (1985) a diagrammatic languagefor instruction of a menubased word processing system. in s. williams (ed.), humans andmachines: the interface through language. norwood, nj: ablex.gomez, l. m., egan, d. e., wheeler, e. a., sharma, d. k., and gruchacz, a.m. (1983) howinterface design determines who has difficulty learning to use a text editor. pp. 176179 inproceedings of the 1983 chi conference on human factors in computing. new york:association of computing machinery.references35mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.gould, j. d., and boies, s. j. (1983) human factors challenges in creating a principal support officesystemthe speech filing approach. acm transactions on office information systems, 1,273298.halasz, f., and moran, t. p. (1982) analogy considered harmful. human factors in computersystems proceedings. washington, dc: national bureau of standards.hilgard, e. r., and bower, g. h. (1975) theories of learning. englewood cliffs, nj: prenticehall.jagacinski, r. j., and miller, r. a. (1978) describing the human operator's internal model of adynamic system. human factors, 20, 425433.jagacinski, r. j., plamondon, b. d., and miller, r. a. (in press) describing movement at two levelsof abstraction. in p. a. hancock (ed.), human factors psychology. amsterdam: northholland.jenkins, j. j. (1974) remember that old theory of memory? well, forget it! american psychologist, 29, 785795.kieras, d. e., and bovair, s. (1984) the role of a mental model in learning to operate a device.cognitive science,  8, 255274.kieras, d. e., and bovair, s. (1986) a production system analysis of transfer of training. journal ofmemory and language, 25, 507524.kieras, d. e., and polson, p. g. (1983). a generalized transition network representation forinteractive systems. pp. 103106. proceedings of the 1983 chi conference on humanfactors in computing. new york: association of computing machinery.kieras, d. e., and polson, p. g. (1985) an approach to the formal analysis of user complexity.international journal of manmachine studies, 22, 365394.kraut, r. e., hanson, s. j., and farber, j. m. (1983) command use and interface design.proceedings of the 1983 chi conference on human factors in computing. new york:association of computing machinery.landauer, t. k., galotti, k. m., and hartwell, s. (1983) natural command names and initiallearning: a study of textediting terms. communications of the association of computingmachinery, 26, 495503.landauer, t. k., galotti, k. m., and hartwell, s. (1984) what makes a difference when? commentson grudin and bernard. human factors, 26(4), 423429.lewis, c. (1986) a model of mental model construction. proceedings of the 1986 chi conferenceon human factors in computing. new york: association of computing machinery.mack, r. l., lewis, c. h., and carroll, j. m. (1983) learning to use word processors: problems andprospects. acm transactions on office information systems, 1, 254271.mantei, m. (1982) disorientation behavior in personcomputer interaction. unpublished phddissertation. department of communication, university of southern california.mayer, r. e. (1976) some conditions of meaningful learning for computer programming: advanceorganizers and subject control of frame order. journal of educational psychology, 67,725734.mayer, r. e. (1980) elaboration techniques for technical text: an experimental test of the learningstrategy hypothesis. journal of educational psychology, 72, 770784.mayer, r. e. (1981) the psychology of how novices learn computer programming. computingsurveys, 13, 121141.references36mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.mayer, r. e., and bayman, p. (1981) psychology of calculator languages: a framework fordescribing differences in users' knowledge. communications of the acm, 24, 511520.mckeithen, k. b., reitman, j. s., rueter, h. h., and hirtle, s. c. (1981) knowledge organizationand skill differences in computer programming. cognitive psychology, 13, 307325.miller, g. a. (1962) some psychological studies of grammar. american psychologist, 17, 748762.miller, r. a. (1985) a systems approach to modeling discrete control performance. in w. b. rouse(ed.), advances in manmachine systems research, volume 2 . greenwich, ct: jai press.moran, t. p. (1981) the command language grammar: a representation for the user interface ofinteraction computer systems. international journal of manmachine studies, 15, 350.moran, t. p. (1983) getting into a system: externalinternal task mapping analysis. pp. 4549 inproceedings of the 1983 chi conference on human factors in computing. new york:association of computing machinery.morgan, c., williams, g., and lemmons, p. (1983) an interview with wayne rosing, brucedaniels, and larry tesler. byte, february, 3350.newell, a., and simon, h. a. (1972) human problem solving. englewood cliffs, nj: prenticehall.norman, d. a. (1983) some observations on mental models. in d. gentner and a. stevens (eds.),mental models. hillsdale, nj: erlbaum.olson, j. reitman (1987) cognitive analysis of people's use of software. in j. carroll (ed.),interfacing thought: cognitive aspects of humancomputer interaction. cambridge, ma:bradley books/mit press.olson, j. reitman, and e. nilsen (1988) cognitive analysis of people's use of spreadsheet softwaretechnical report. humancomputer interaction, 1988, in press.olson, j. reitman, whitten, w. b., ii, and gruenenfelder, t. m. (1984) a general user interface forcreating and displaying treestructures, hierarchies, decision trees, and nested menus. in y.vassiliou (ed.), human factors and interactive computer systems. norwood, nj: ablex.payne, s. j., and green, t. r. g. (1983) the user's perception of the interaction language: a twolevel model. proceedings of the 1983 chi conference on human factors in computing. new york: association of computing machinery.pew, r. w., and baron, s. (1983) perspectives on human performance modeling. automatica, 19,663676.polson, p. g., and kieras, d. e. (1984). a formal description of user's knowledge of how to operateat device and user complexity. behavior research methods, instruments, and computers, 16, 249255.polson, p. g., and kieras, d. e. (1985) a quantitative model of the learning and performance of textediting knowledge. proceedings of the 1985 chi conference on human factors incomputing. new york: association of computing machinery.polson, p. g., muncher, e., and engelbeck, g. (1986) a test of a common elements theory oftransfer. proceedings of the 1986 chi conference on human factors in computing. newyork: association of computing machinery.references37mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.postman, l., and stark, k. (1969) role of response availability in transfer and interference. journalof experimental psychology, 79, 168177.rasmussen, j. (1983) skills, rules, and knowledge: signals, signs, and symbols, and otherdistinctions in human performance models. ieee transactions on systems, man, andcybernetics, smc13, 257266.reisner, p. (1981) formal grammar and human factors design of an interactive graphics system.ieee transactions of software engineering, se7, 229240.reisner, p. (1983) analytic tools for human factors software. in a. blaser and m. zoeppritz (eds.),enduser systems and their human factors. proceedings of the scientific symposiumconducted on the occasion of the 15th anniversary of the science center heidelberg ofibm germany, in g. goos and j. hartmanis (eds.), lecture notes in computer science, series no. 150. berlin: springerverlag.reisner, p. (1984) formal grammar as a tool for analyzing ease of use: some fundamental concepts.p. 53 in j. thomas and m. schneider (eds.), human factors in computing systems. norwood, nj: ablex.reitman, j. s. (1976) skilled perception in go: deducing memory structures from interresponsetimes. cognitive psychology, 8, 336377.reitman, j. s., and rueter, h. h. (1980) organization revealed by recall orders and confirmed bypauses. cognitive psychology, 12, 554581.robertson, s. r. (1983) goal, plan, and outcome tracking in computer textediting performance. cognitive science technical report 25. yale university, new haven, ct.rosson, m. b. (1983) patterns of experience in text editing. pp. 171175 in proceedings of the 1983chi conference on human factors in computing. new york: association of computingmachinery.rouse, w. b., and morris, n. m. (1986) on looking into the black box: prospects and limits in thesearch for mental models. psychological bulletin, vol. 100, no. 3, pp. 349363.rumelhart, d. e., and norman, d. a. (1981) analogical processes in learning. in j. r. anderson(ed.), cognitive skills and their acquisition. hillsdale, nj: erlbaum.rumelhart, d. e., and norman, d. a. (1982) simulating a skilled typist: a study of skilledcognitivemotor performance. cognitive science, 6, 136.scandura, a. m., lowerre, g. f., veneski, j., and scandura, j. m. (1976) using electroniccalculators with elementary children. educational technology, 16, 1418.schlager, m. s., and ogden, w. c. (1986) a cognitive model of database querying: a tool fornovice instruction. proceedings of the 1986 chi conference on human factors incomputing. new york: association of computing machinery.shepard, r. n., romney, a. k., and nerlove, s. b. (1972) multidimensional scaling: theory andapplications in the behavioral sciences. new york: seminar press.sheridan, t. b., charny, l., mendel, m. b., and roseborough, j. b. (1986) supervisory control,mental models, and decision aids. mit department of mechanical engineeringtechnical report, july. massachusetts institute of technology.shiffrin, r. m., and schneider, w. (1977) controlled and automatic human information processing.psychological review, 84, 127190.references38mental models in humancomputer interaction: research issues about what the user of software knowscopyright national academy of sciences. all rights reserved.shneiderman, b. (1980) software psychology: human factors of computer and informationsystems. cambridge: winthrop.shneiderman, g. (1982) multiparty grammars and related features for defining interactive systems.ieee transactions on systems, man, and cybernetics, smc12, 2.shrager, j., and klahr, d. (1983) learning in an instructionless environment: observation andanalysis. proceedings of the 1983 chi conference on human factors in computing. newyork: association of computing machinery.smelcer, j. b. (1986) expertise in data modeling or what is inside the head of an expert datamodeler? proceedings of the 1986 chi conference on human factors in computing. newyork: association of computing machinery.thomas, j. c., and carroll, j. m. (1981) human factors in communication. ibm systems journal, 20, 237263.veldhuyzen, w., and stassen, h. g. (1976) the internal models: what does it mean in humancontrol. in t. b. sheridan and g. johannsen (eds.), monitoring behavior and supervisorycontrol. new york: plenum.whiteside, j., and wixon, d. (1984) developmental theory as a framework for studying humancomputer interaction. in h. r. hartson (ed.), advances in humancomputer interaction. norwood, nj: ablex.wittrock, m. c. (1974) learning as a generative process. educational psychology, 11, 8795.wright, p., and bason, g. (1982) detour routes to usability: a comparison of alternative approachesto multipurpose software design. international journal of manmachine studies, 18,391400.young, r. m. (1981) the machine inside the machine: users' models of pocket calculators.international journal of manmachine studies, 15, 5185.young, r. m. (1983) surrogates and mappings: two kinds of conceptual models for interactivedevices. in d. gentner and a. stevens (eds.), mental models. hillsdale, nj: erlbaum.references39
detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/5280computing and communications in the extreme: research forcrisis management and other applications176 pages | 6 x 9 | paperbackisbn 9780309055406 | doi 10.17226/5280steering committee, workshop series on high performance computing andcommunications, commission on physical sciences, mathematics, and applications,national research councilcomputing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.computing andcommunications in theextremeresearch for crisis management and otherapplicationssteering committeeworkshop series on high performance computing andcommunicationscomputer science and telecommunications boardcommission on physical sciences, mathematics, and applicationsnational research councilnational academy presswashington, d.c. 1996icomputing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.notice: the project that is the subject of this report was approved by the governing board of thenational research council, whose members are drawn from the councils of the national academyof sciences, the national academy of engineering, and the institute of medicine. the members ofthe steering committee responsible for the report were chosen for their special competences andwith regard for appropriate balance.this report has been reviewed by a group other than the authors according to proceduresapproved by a report review committee consisting of members of the national academy of sciences, the national academy of engineering, and the institute of medicine .support for this project was provided by the department of the navy, office of the chief ofnaval research, under grant number n000149310166. the project was conducted at the requestof the department of defense, defense advanced research projects agency. the content of thisworkshop report does not necessarily reflect the position or the policy of the federal government,and no official endorsement should be inferred.library of congress catalog card number 9660885international standard book number 0309055407additional copies of this report are available from: national academy press 2101 constitutionavenue, n.w. box 285 washington, dc 20055 8006246242 2023343313 (in the washingtonmetropolitan area)copyright 1996 by the national academy of sciences. all rights reserved.printed in the united states of americaon the cover: a photograph provided by the federal emergency management agency shows urbansearch and rescue workers in action at the alfred p. murrah building in oklahoma city, april 1995.a computer graphic produced by the geophysical fluid dynamics laboratory, national oceanicand atmospheric administration, depicts a simulation of hurricane emily off the north carolinacoast, september 1993.iicomputing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.steering committee, workshop series onhigh performance computing andcommunicationsken kennedy, rice university, chair frances e. allen, ibm t.j. watson research centervinton g. cerf, mci telecommunicationsgeoffrey fox, syracuse universitywilliam l. scherlis, carnegie mellon universityburton smith, tera computer companykaren r. sollins, massachusetts institute of technologystaffmarjory s. blumenthal, directorjames e. mallory, program officer (through april 1995)john m. godfrey, research associategail e. pritchard, project assistantiiicomputing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.computer science and telecommunicationsboardwilliam a. wulf, university of virginia, chair frances e. allen, ibm t.j. watson research centerdavid d. clark, massachusetts institute of technologyjeff dozier, university of california, santa barbarahenry fuchs, university of north carolinacharles geschke, adobe systems incorporatedjames gray, microsoft corporationbarbara grosz, harvard universityjuris hartmanis, cornell universitydeborah a. joseph, university of wisconsinbutler w. lampson, microsoft corporationbarbara liskov, massachusetts institute of technologyjohn major, motorolarobert l. martin, at&t network systemsdavid g. messerschmitt, university of california, berkeleywilliam h. press, harvard universitycharles l. seitz, myricom incorporatededward h. shortliffe, stanford university school of medicinecasimir s. skrzypczak, nynex corporationleslie l. vadasz, intel corporationmarjory s. blumenthal, directorherbert s. lin, senior staff officerpaul semenza, staff officerjerry r. sheehan, staff officerjean e. smith, program associatejohn m. godfrey, research associateleslie m. wade, research assistantgloria p. bemah, administrative assistantgail e. pritchard, project assistantivcomputing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.commission on physical sciences,mathematics, and applicationsrobert j. hermann, united technologies corporation, chair peter m. banks, environmental research institute of michigansylvia t. ceyer, massachusetts institute of technologyl. louis hegedus, w.r. grace and company (retired)john e. hopcroft, cornell universityrhonda j. hughes, bryn mawr collegeshirley a. jackson, u.s. nuclear regulatory commissionkenneth i. kellermann, national radio astronomy observatoryken kennedy, rice universitythomas a. prince, california institute of technologyjerome sacks, national institute of statistical sciencesl.e. scriven, university of coloradoleon t. silver, california institute of technologycharles p. slichter, university of illinois at urbanachampaignalvin w. trivelpiece, oak ridge national laboratoryshmuel winograd, ibm t.j. watson research centercharles a. zraket, mitre corporation (retired)norman metzger, executive directorvcomputing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.the national academy of sciences is a private, nonprofit, selfperpetuating society of distinguished scholars engaged in scientific and engineering research, dedicated to the furtherance of science and technology and to their use for the general welfare. upon the authority of the charter granted to it by the congress in 1863, the academy has a mandate that requires it to advise the federal government on scientific and technical matters. dr. bruce alberts is president of the national academy of sciences. the national academy of engineering was established in 1964, under the charter of the national academy of sciences, as a parallel organization of outstanding engineers. it is autonomous in its administration and in the selection of its members, sharing with the national academy of sciences the responsibility for advising the federal government. the national academy of engineering also sponsors engineering programs aimed at meeting national needs, encourages education and research, and recognizes the superior achievements of engineers. dr. harold liebowitz is president of the national academy of engineering. the institute of medicine was established in 1970 by the national academy of sciences to secure the services of eminent members of appropriate professions in the examination of policy matters pertaining to the health of the public. the institute acts under the responsibility given to the national academy of sciences by its congressional charter to be an adviser to the federal government and, upon its own initiative, to identify issues of medical care, research, and education. dr. kenneth i. shine is president of the institute of medicine. the national research council was organized by the national academy of sciences in 1916 to associate the broad community of science and technology with the academy™s purposes of furthering knowledge and advising the federal government. functioning in accordance with general policies determined by the academy, the council has become the principal operating agency of both the national academy of sciences and the national academy of engineering in providing services to the government, the public, and the scientific and engineering communities. the council is administered jointly by both academies and the institute of medicine. dr. bruce alberts and dr. harold liebowitz are chairman and vice chairman, respectively, of the national research council. www.nationalacademies.orgvicomputing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.prefacefrom august 1994 to august 1995, the national research council's(nrc's) computer science and telecommunications board conducted a seriesof three workshops on research issues in highperformance computing andcommunications. the goal of the series w as to bring together specialists inselected, nationally important application areas and researchers from the highperformance computing and communications (hpcc) research community toexplore unmet technology needs and their implications for research. theworkshops were held at the request of the department of defense, defenseadvanced research projects agency (darpa). they also drew on the interestand input of other agencies that are major supporters of hpcc research, inparticular the national science foundation, the department of energy, and thenational aeronautics and space administration. the agendas and participantlists for workshops i through iii are given in appendix a.the applications discussed in the workshops were selected both for theirimportance to economic and societal goals and for the diversity of challengesthey pose for computing and communications research. the first workshop washeld in august 1994 at the arnold and mabel beckman center of the nationalacademy of sciences and the national academy of engineering in irvine,california. it considered applications in four areas of national importance:manufacturing (e.g., simulation, collaborative design) ; health care (e.g.,computerized patient records, medical information, telemedicine); digitallibraries (e.g., electronic storage, search and retrieval of multiple forms ofinformation); and electronic commerce and banking (e.g., secure, distributedtransactions).although significant insights were gained from examining this broad set ofprefaceviicomputing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.applications, the steering committee decided to explore a single application areain detail to enhance understanding of computing and communicationsrequirements both for that area and for national applications in general. inconsultation with darpa, the steering committee selected crisis managementfor focused study. crisis management incorporates preparation for, response to,and recovery from natural and technological disasters such as hurricanes,earthquakes, and oil spills; politicalmilitary crises; and related emergencies.crisis management seemed an ideal focus because its diverse problems createdemands for a number of different highperformance technologies. these rangefrom highperformance computation to highbandwidth, intelligent, and securecommunications and information systems, as well as tools to support decisionmaking and management of distributed groups of actors in a complex,uncertain, and rapidly changing environment (analogous to command andcontrol in military operations). crisis management also provides a context forevaluating both where specifically highperformance technologies can make asignificant contribution and where knowledge gained from research can lead tovaluable advances in more mainstream (i.e., nonhighperformance) technologies.the second workshop, held at the beckman center in june 1995, examinedthe problems presented by crisis management and the strengths andshortcomings of existing computing and communications technologies foraddressing them. both civil and military crisis management were considered,although civil applications received more attention. the steering committee andworkshop participants found crisis management to be an especially fruitfulsource of research topics that have the potential to advance the state ofcomputing and communications on a broad front, in addition to meeting someof the pressing technology needs of civilian and military crisis managers.the final workshop, held in august 1995 at the national academy ofsciences building in washington, d.c., focused on defining key researchopportunities that should be pursued to meet the needs of application areasaddressed in the first two workshops . that workshop continued the emphasison crisis management but also revisited the other application areas from the firstworkshop as additional sources of input and as a test of the generality ofconclusions about crisis management needs.this report synthesizes and elaborates on what was learned in the threeworkshops. the steering committee emphasizes that it was not the goal of theseries to provide recommendations on how to solve the specific problems ofcrisis management and other application areas in the nation today. solving crisismanagement problems such as slow or incomplete delivery of food, medicine,information, and financial assistance to people affected by a disaster requiresresources, expertise, and effort in many areas in addition to computing andcommunications (e.g., effort to address budget constraints for local and statecrisis management agencies, interagency coordination, personnel training).rather, the workshops' goal was to explore applications to gain insights intoproblems thatprefaceviiicomputing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.computing and communications research could address, thereby helping toalleviate, with more capable or cheaper technologies, problems faced in crisismanagement and other nationally important application areas. in that respect theworkshops proved to be a rich source of ideas for the research community toconsider.the steering committee for the workshop series on high performancecomputing and communications acknowledges the contributions of theworkshop speakers and participants. their insights and creativity were centralto this effort. we especially thank james beauchamp, of the u.s. commanderin chief, pacific command (cincpac); john hwang, federal emergencymanagement agency; robert kehlet, defense nuclear agency; davidkehrlein, office of emergency services, state of california; and lois clarkmccoy, national institute of urban search and rescue, as well as other crisismanagement professionals who educated, stimulated, and challenged a diversegroup of computing and communications researchers. in addition, workshopparticipants joel saltz, of the university of maryland, and clifford lynch, ofthe office of the president, university of california, made valuable writtencontributions to the final report.the steering committee also thanks the nrc staff for their diligentassistance throughout the workshop series and preparation of the final report,including marjory blumenthal, john godfrey, gail pritchard, and jamesmallory. the steering committee and i are especially grateful to john godfreyfor his resourcefulness in identifying experts and information sources and hisconscientious assistance in developing this report. his efforts to attract bothcrisis management and computing experts to join in this collaborative projectand his consistent support in integrating materials and ideas from bothperspectives were key to the successful outcome of this project. gail pritchard'sassistance in ensuring the smooth running of the workshops and providingorganizational support to the steering committee was also essential and muchappreciated. finally, the steering committee is grateful to the anonymousreviewers for helping to sharpen and improve the report through theircomments. responsibility for the report remains with the steering committee.ken kennedy, chairsteering committee, workshop series on high performance computing andcommunicationsprefaceixcomputing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.prefacexcomputing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.contents overview and summary 11 application needs for computing andcommunications 8 introduction 8 crisis management 10 definition and characteristics 10 scenarios 16 crisis management needs for computing and communications 16 other application domains 34 digital libraries 35 electronic commerce 38 manufacturing 42 health care 47 notes 532 technology: research problems motivated by application needs 55 introduction 55 networking: the need for adaptivity 56 selforganization 60 network management 62 security 65contentsxicomputing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved. discovery of resources 68 virtual subnetworks 68 computation: distributed computing 69 modeling and simulation 70 mobility of computation and data 72 storage servers and metadata 73 anomaly detection and inference of missing data 75 sensors and data collection 75 distributed resource management 77 software system development 78 information management: finding and integratingresources 81 integration and location 84 metadata and types 88 production and value 89 distribution and relocation 90 usercentered systems: designing applications towork with people 91 humancentered systems and interfaces 91 collaboration and virtual organizations 93 judgment support 95 notes 973 summary and findings: research fornationalscale applications 99 research challenges of crisis management 99 technology deployment and research progress 107finding 1: crisis management testbeds 109finding 2: studies of existing nationalscale informationinfrastructure 111 support of human activities 112finding 3: usability 113finding 4: collaboration 116 system composability and interoperability 117finding 5: focused standards 118finding 6: interoperability 122finding 7: integration of software components 124finding 8: legacy and longevity 126 adapting to uncertainty and change 130finding 9: adaptivity 130finding 10: reliability 134 performance of distributed systems 136finding 11: performance of distributed systems 136 notes 137contentsxiicomputing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved. bibliography 139 appendixes a workshop series on high performance computing andcommunications 145b backgroundšhpcci and nii 156c acronyms and abbreviations 158contentsxiiicomputing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.contentsxivcomputing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.overview and summarycrises are extreme events. they cause significant disruption and put livesand property at risk. some crises arise from natural disasters such asearthquakes, hurricanes, fires, and floods. manmade crises can be accidental,such as oil spills or the release of toxic substances, or they may be intentional,such as bombings by terrorists. crises require an immediate response and acoordinated application of resources, facilities, and efforts beyond thoseregularly available to handle routine problems.crisis management was the primary application area examined in theworkshop series on high performance computing and communicationsconducted by the computer science and telecommunications board of thenational research council (see box s.1). crisis management was selected notonly because of its critical importance to public safety and wellbeing, but alsobecause building good tools that are useful in meeting the extreme demands ofcrisis management requires significant advances across a combination of manydifferent, broadly applicable computing and communications technologies. thechallenges confronting crisis managers are extreme in several dimensions.crises require an extraordinary quantity of resources, such as search and rescueteams, medical assistance, food, and shelter. the demands are highly diversešimplying a need for cooperation among many different actorsšand largelyunpredictable in terms of location, time, and specific resources needed.moreover, the urgency associated with crises has many implications, such asthe need to rapidly identify, collect, and integrate crucial information about thedeveloping situation; to have access to tools and resources that are notcumbersome or difficult to use, particularly in stressful conditions; and to havethe capability to make projections andoverview and summary1computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.initiate actions in the face of an inevitable degree of uncertainty andincompleteness of information.box s.1 overview and context of the workshopseriesconsultation with people who use or want to use computing andcommunications to accomplish their objectives provides a sometimes soberingperspective on technology design and implementation. the three computerscience and telecommunications board (cstb) workshops on highperformancecomputing and communications were designed to foster discussion amongapplication specialistsincluding technology experts (developers or systemsmanagers) and professionals in crisis management, digital libraries, electroniccommerce, manufacturing, and health careand researchers to explore howcomputing and communications technologies are used in these areas, theproblems or shortcomings associated with current technologies, and potentialimprovements that might both enhance the technology base in these nationalscaleapplications and advance the state of the art in computing and communications.researchers and users discussed not only traditional highperformance concerns,such as speed and scale of computation and networking, but also capabilities ininformation management, collaborative work, decision making, and many otherareas. such capabilities are enabled by advances in the underlying computer andnetwork systems and at the same time make them more useful, thus hastening theevolution of a collection of computers and communications links into an informationinfrastructure.the workshop series fits with the intent of several federal programs to fostergreater interaction among researchers, developers, and users of leadingedgecomputing and communications. the framework for many of these activities hasbeen the high performance computing and communications initiative (hpcci; seeappendix b for a brief discussion), which has stimulated such interactions,beginning with scientific investigation of ''grand challenges" and continuing towardstudy of broader "national challenge" applications. a cstb review concluded thatthe hpcci has demonstrated the value for computing and communicationsworkshop discussions covered a spectrum from research throughdevelopment to deployment and use of technology. the mix of professionalsfostered consideration of how the conditions in which computing andcommunications are used can affect the perceived value of technologies and thedemand for improvementšnontechnological conditions, too, shape perceptionsabout the kinds of features that would be helpful. resource constraints of localand state crisis management agencies, for example, limit the amount of trainingavailable to users of technological tools and require users to tradeoffperformance and other features of new technologies against the lifecycle costof equipment.out of these discussions came ideas about where truly high performancetechnology may be helpful in different application domains, where advances inperformance at the leading edge would yield benefits in more mainstreamoverview and summary2computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.research of interaction between developers and users of technologies (cstb,1995). related activities include a february 1994 forum involving several hundredresearchers and others, "r&d for the nii: technical challenges," that yielded adiscussion of current research topics in communications and computinginfrastructure technologies (vernon et al., 1994). the committee on informationand communications (cic) of the national science and technology councildeveloped a plan for coordinating research and development (r&d) across multiplefederal agencies, identifying strategic r&d focus areas that relate to agencymissions and other user needs (cic, 1995). more narrowly focused efforts haveilluminated research opportunities in specific application areas, such as health careand digital libraries.1 these recent examinations of research needs, however, havedrawn mainly from the research community.by comparison, the cstb workshop series emphasized crisis management asan application domain and featured the substantial participation of end users,including nontechnologists.2 traditionally, crisis management has not been a focusof academic computing and communications researchers, other than in the contextof military system development. but at the cstb workshops, crisis managementinspired fresh discussion of a full range of computing and communications researchissues and provided a realworld perspective for calibrating research needs relatedto other nationalscale applicationsšsome of which have been examined moreextensively through various federal programs and privatesector activitiesagainstone that is particularly demanding in terms of urgency and unpredictability ofneeded resources. crisis management was also appropriate for framing questionsrelating to federal support for research in an application area that is primarily apublicsector responsibility.1 for examples, see davis et al. (1995) and lynch and garciamolina (1995).2 this mix contributed to the evolution, subsequent to workshop i, of an nsf workshopfocused on health care (davis et al., 1995). a participant identified individuals inworkshop i to invite to the nsf event, as he reported in workshop iii.systems, and how the interaction of applications in different areas throughthe evolving information infrastructure, on a national scale (e.g., the use oftelemedicine and digital libraries in crisis response), influences the developmentand use of computing and communications. this changing context highlightsthe need for fundamental research to understand crosscutting problems arisingin nationalscale applications (see box s.2) that, on a smaller scale, mayappear merely to be questions of implementing known technologies. workshopparticipants agreed that the formulation of research questions by researcherswill benefit from an explicit recognition that the technologies arising fromtoday's research will be deployed to meet real needs.chapter 1 discusses unmet demands for computing and communicationstechnologies in crisis management and four other nationalscale applicationareasšdigital libraries, electronic commerce, manufacturing, and health care.computing and communications technologies are increasingly central to managoverview and summary3computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.ing activities in all of these areas. however, current stateoftheart technologyis not always adequate to meet existing and emerging demands. society'sdependence on information technology is not absolute; certainly, fire fighterscan continue to put out fires without computerized maps, and doctors can writeclinical reports with pen and paper. however, continued improvements in thequality, efficiency, accessibility, and dependability of nationally importantindustries and services are realizable through advances in informationtechnology and their integration into the work practices of organizations andindividuals.box s.2 characteristics of nationalscaleapplicationsnationalscale applications such as those discussed in this reportšcrisismanagement, digital libraries, electronic commerce, manufacturing, and health carešuse computing and communications on a nationwide (even global) scale.1 "national scale" therefore implies the potential for large volumes of computationand communications, a large number and diversity of individuals and organizations,and the associated complexity. nationalscale efforts such as crisis managementare distributed across multiple locations, are often linked by networks, and makeuse of a variety of computing and communications resources. the people involvedvary in expertise, ranging from scientists and engineers to citizens who may lackspecialized technological knowledge. because they operate in such a broad anddiverse environment, computing and communications systems for these applicationareas must be able to survive and adapt to variety and rapid change in the needs ofindividuals and organizations for technologies.the nationalscale applications examined in the computer science andtelecommunication board's three workshops have several elements in common: scale. nationalscale applications raise qualitatively new challenges forcomputing and communications technologies because of the geographicdistribution, extent, and diversity of requirements for processing, storage, andcommunication of information, as well as the number of interconnected endpointsusers, computers, and information sources and repositories. demand for dependability. as people come to rely increasingly on the computingand communications systems that serve nationalscale applications, thesesystems begin to become part of the infrastructure society counts on, as thetelephone system did early in the century. consideration must be given tosystems' survivability, security, fault tolerance, and graceful degradation (asopposed to consequently, whether expressed as needs of society or as opportunities forresearchers, unmet demands for improved capabilities in areas of broadnational significance suggest many fruitful problems for research in anddevelopment ofoverview and summary4computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved. catastrophic failure), among other issues. systems must also respond to therapid, continuing evolution of underlying technologies in a competitivemarketplace. architectural diversity. distributed ownership of systems among manyorganizations and individuals makes monolithic, rigidly defined architectureslargely impractical.2 common interests among parties in one applicationdomain such as health care or banking may result in agreement on specificarchitectural elements, but these interests evolve, and so generality andflexibility are required. this requirement for generality and flexibility implies theneed for common interconnection standards, as well as support for people toidentify and integrate the resources available to them across multiple systemarchitectures. heterogeneous interfaces and standards. nationalscale applications aredistinguished by an enormous degree of heterogeneity and decentralization inthe interfaces and models of interaction among the systems that support them.centralized control or widespread agreement on a set of protocols andfunctional interfaces is difficult to achieve.3 in designing systems to supportbroad national activities with many autonomous players, the determination ofwhere, what, and how much to hold in common among system elements mustbe an ongoing process open to diversešand competingšcommercialimplementations.41 although it is not the case that each instance of using these applications is necessarilydistributed across the entire nation, one of their distinguishing characteristics is that a givenuse potentially may draw upon resources anywhere in the nationšin some cases, the world.2 architectures are the underlying models of systems and how they relate to each other.interfaces and standards generally embody a particular architecture; however, a standardmay be used in more than one architecture, and an architecture may have more than oneimplementation.3 this is true even in a relatively centralized institutional context. for example,heterogeneity is quite evident in military systems, particularly when more than one servicebranch or national force is involved. in addition, because the department of defense andother users of largescale systems are relying increasingly on available commercialtechnologies, the choice and definition of standards are becoming increasingly significantfor them.4 the need for openness and evolution raises questions about how to achieve formal andinformal standards and conventions on a national scale. the continuing need for standardsto achieve interconnection and integration suggests the broad value of research that canclarify choices among alternative technologies while those technologies are beingdeveloped, can increase compatibility among technologies, or can generate newtechnologies that diminish the problems associated with heterogeneity.highperformance and other computing and communications technologies.these research opportunities are discussed in detail in chapter 2. not all ofthem are new; however, from the perspective of crisis management, problemsfamiliar from other nationally important application domains take on an addeddimension because of requirements for systems and applications that areflexible across extremes of scale, diversity, and rapid change.chapter 3 presents the steering committee's findings based on inputs fromthe workshop series and a sampling of additional, related sources. box 3.2presents selected examples of compelling, applicationsmotivated computerscience and engineering research topics identified in discussions between crisismanagement experts and technologists at the workshops. these includeoverview and summary5computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.communications resources such as rapidly deployable, selfconfiguring wirelessnetworks for coordinating response teams; "judgment support" tools to assistcrisis managers in making decisions in the absence of complete, reliableinformation; simulations of phenomena such as hurricanes and fires that coulddeliver useful results to crisis managers rapidly; virtual "anchor desks" thatplace networkbased resources such as simulations and information systems atthe disposal of crisis managers; and other specific tools and technologies thatappear promising for crisis management.finding 1 emphasizes the importance of experimental testbeds forcrisis managementrelated research and development. testbeds that providea realistic application setting, such as simulation and fieldbased trainingexercises, can serve as demanding implementation environments for newtechnologies and sources of feedback to identify and refine research objectives.application users, such as federal, state, and local civilian crisis managementpersonnel, should participate in testbed activities. their input is essential toassess the fit among systems, tools, and users' needs and to ensure thattechnology is focused on usable, practical solutions.to secure the full benefits of applicationspecific computing andcommunications technologies, there must also be recognition of the increasinglyinterconnected nature of nationalscale applications. in application areas such ascrisis management, digital libraries, electronic commerce, manufacturing, andhealth care, the widespread interconnection of computing and informationresources and the people who use them over networks has made it feasible, andincreasingly common, for resources to be called on in unforeseen ways. crisismanagement, in particular, illustrates the value of being able to integrate highlydiverse resources whose usefulness in an unusual situation could not have beenanticipated in advance.unfortunately, technologies developed to meet a specific applicationrequirement often do not function well in unforeseen circumstances because ofcomplex, difficult problems of interoperation, performance, and scaling up.therefore, the findings resulting from this workshop series also addressresearch, development, and deployment efforts that can lead to both consistentarchitectural approaches that function on a national scale and generalpurposetools and services that facilitate rapid, ad hoc integration of systems andresources.finding 2 highlights the importance of investigating the features ofexisting nationalscale infrastructures for specific applications to identifywhat features do and do not work. findings 3 through 11 identifytechnological leverage points for computing and communications researchinvestments, based on needs of nationalscale applications.  these findingsemphasize research challenges in four areas: (1) support of human activities(e.g., improved ease of use of technologies for individuals and groups), (2)system composability and interoperability, (3) adapting to uncertainty andchange, and (4) performance of distributed systems. outcomes of testbed andarchitecturestudy activitiesoverview and summary6computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.suggested in findings 1 and 2 should inform future reexamination of theseresearch areas, which represent the best understanding of a range of technologyand application experts in 19951996.the research questions discussed in this workshop report can and shouldmotivate the scientific and engineering research communities in the future.they have the potential to increase the ability of individuals and organizationsto make the most of important applications, to present intellectually stimulatingchallenges for researchers, and to promote significant advances in the state oftechnology.referencescommittee on information and communications (cic). 1995. america  in the age of information:strategic implementation plan . national science and technology council, washington,d.c., march 10.computer science and telecommunications board (cstb), national research council. 1995.evolving the high performance computing and communications  initiative to support thenation's information infrastructure . national academy press, washington, d.c.davis, larry s., joel saltz, and jerry feldman. 1995. "nsf workshop on high performancecomputing and communications and health care." report of a workshop, december810, 1994, washington, d.c. available on line at http://www.umiacs.umd.edu:80/users/lsd/papers/nsfwork.html .lynch, clifford, and hector garciamolina. 1995. "interoperability, scaling, and the digitallaboratories research agenda." report on the information infrastructure technology andapplications (iita) digital libraries workshop, reston, va., may 1819. available online at http://wwwdiglib.stanford.edu/diglib/pub/reports/iitadlw .vernon, mary k., edward d. lazowska, and stewart d. personick (eds.). 1994. r&d for the nii:technical challenges . report of a symposium, february 28 through march 1,gaithersburg, md. interuniversity communications council (educom), washington, d.c.overview and summary7computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.1application needs for computing andcommunicationsintroductionthe requirements of nationalscale applications for computing andcommunications pose both opportunities and challenges that derive, ultimately,from the increasing capabilities of the technologies on which these applicationsdepend. significant increases in computation and communications performancein recent years have made qualitative differences in what can be done withinformation technology. for example, widespread deployment of data networksand the increasing processing and display capabilities of personal computersand workstations have made possible a powerful and highly adaptable newmedium of communication, the world wide web. advances in performancehave raised application users' expectations about what their informationtechnology tools can be counted on to accomplish; as box s.2 notes, computingand communications are becoming part of the essential national infrastructureon which important sectors of the nation's economy and society depend.this chapter identifies opportunities for taking advantage of informationinfrastructure to support the missions of people and organizations in fiveimportant application areasšcrisis management, digital libraries, electroniccommerce, manufacturing, and health care. reflecting the language that often isused by people seeking to apply technology to solve a problem, the chaptersometimes characterizes these opportunities as "needs" for technology.society's dependence on information technology is not absolute; certainly, firefighters can continue to put out fires without computerized maps, and doctorscan write clinical reports with pen and paper. however, continued dramaticimprovements in theapplication needs for computing and communications8computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.quality, efficiency, accessibility, and dependability of nationally importantindustries and services are realizable through advances in informationtechnology and the integration of those advances into the work modes oforganizations and individuals (cstb, 1994a,b). whether the proposed advancesare expressed as needs or as opportunities, research relating to enablingtechnologies remains essential; it is the foundation for progress in informationtechnology generally and for advances in the nature and uses of informationinfrastructure. in addition, actual growth in the use of electronic informationand communications systems in the united states and worldwide creates a needfor research into the complex problems of managing information andintegrating information and communications services into broader humanactivities that involve ordinary citizens, including specialists in areas other thaninformation technology.1to explore needs and opportunities for use of computing andcommunications in crisis management and other selected application areas,workshop participants examined four classes of technologies, loosely reflectinga layered model of information infrastructure, with each set of technologiesproviding capabilities used by the higher layers. the organization of eachsection in this chapter reflects this classification scheme, proceeding from lowerto higher layers. networkingštechnologies related to networked voice, video, and datacommunications, including physical facilities (e.g., circuits, switches,routers), the communications services that make use of them, and thearchitectures, protocols, and management mechanisms that make networksfunction. key aspects include, for example, bandwidth, reliability, security,quality of service, and architectural support for the integration of higherlevel functions across the network. computationštechnologies related to computer processing, particularly ina distributed context. traditional computationintensive functions includemodeling, simulation, and some aspects of visualization, among others.key aspects include, for example, strategies for maximizing the use ofprocessing power (such as parallelism and distribution), programmingmodels, software system composition, and management of processing anddata flows across networks, including representation of time and temporalconstraints in distributed computing. information managementštechnologies contributing to the creation,storage, retrieval, and sharing of information across networks. componentsthat may be integrated within an information management system includetraditional databases, object databases for design applications, multimediaservers, digital libraries, and distributed file systems, as well as softwareapplications that process or manage information. they also include remotesensors attached to networks. key aspects include, for example, balancebetween central and distributed control, exchange of diverse types andformats of information across boundaries,application needs for computing and communications9computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.integration of real and synthetic information (e.g., in virtual environments),and easy construction of new applications from existing components. usercentered systemsštechnologies for maximizing the utility ofcomputerbased systems for the people who use them, including naturalhumancomputer interfaces, alternative modes of informationrepresentation (e.g., speech, hypertext, visualization), artificial intelligencebased decision support (including knowledgebased systems and newertechniques for coping with uncertainty), and workgroup collaborationtechnologies. key aspects include, for example, ease of use for individualsand groups and the ability of applications and systems to adapt to userspecific skills and needs.the technologies for communicating and using information are highlyinterrelated, and this scheme is not intended to be rigid or perfectly consistent inapplying a layered approach. to simplify discussion, the application areademands for computing and communications that are examined in this chapterare distributed somewhat arbitrarily among these four areas. a particularcomputing or communications application (e.g., tool, system) may span all ofthese levelsšfor example, an information system that helps a user answer aquestion. the system would assist by translating a need for information into aformal expression that automated systems can understand, identifying potentialinformation sources (including the vast array of sources available acrossnetworks such as the internet), formulating a search strategy, accessing multiplesources across the network, integrating the retrieved data consistent with theuser's original requirement, displaying the results in a form appropriate to boththe user's needs and the nature of the information, and interacting with the userto refine and repeat the search. this system would incorporate both informationmanagement and usercentered technologies, and these would rely on asupporting infrastructure of networking and computation.crisis managementdefinition and characteristicscrisis management was selected as the focus for workshops ii and iii inthe computer science and telecommunications board's series of threeworkshops on highperformance computing and communications because crisesplace heavy demands on computing, communications, and information systems,and such systems have become crucial to providing necessary support in timesof crisis. crises are extreme events that cause significant disruption and putlives and property at risk. they require an immediate response, as well ascoordinated application of resources, facilities, and efforts beyond thoseregularly available to handle routine problems. they can arise from manysources. natural disasters such as major earthquakes, hurricanes, fires, andfloods clearly can precipitateapplication needs for computing and communications10computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.crises. manmade crises can be accidental, such as oil spills or the release oftoxic substances into the environment, or they may be intentional, such asbombings by terrorists. warfare clearly presents a continuing set of crises, andalthough operational warfare concerns were largely outside the scope of theworkshop series, many of the characteristics and computing andcommunications requirements of crisis management in other contexts overlapwith the needs of warfare.2 the military requirements for command, control,communications, computing, and intelligence (c4i), for example, have much incommon with the nonmilitary crisis management requirements forunderstanding a complex situation and preparing a coordinated response. therelatively more centralized and hierarchical structure of military command incomparison to civilian organizations, however, introduces differences in theneeds for and the available approaches to computing and communications in thetwo contexts. as john hwang, of the federal emergency management agency(fema), observed, "military command and control is becoming a discipline;however, civil crisis management is still in its infancy as a discipline."when does a situation become a crisis? one workshop participantobserved that when he had to call up staff to run the crisis center, it was a crisis.this tautological comment underscores that the human decision to invokeextraordinary resources and management priorities implies a situation distinctfrom "business as usual": standard practices no longer apply. beyond thiscommonsense observation, experts whose careers revolve around crisismanagement sometimes offer differing perspectives on crises and crisismanagement. to simplify the discussion and be consistent with its limited scopefor investigation, the steering committee has framed these issues in somewhatgeneral terms in examining the relationships between the crisisrelatedconditions in which computing and communications may be used and thefeatures or functions of those technologies that are needed.crisis management has several phases or components with different timehorizons. among these are preparedness (including planning and training),crisis avoidance (averting a developing crisis), response, and recovery.3 muchof the discussion at the workshops centered on responserelated activities,which offer particularly severe challenges across a range of technologies.response to a crisis involves an initial reaction with available resources, a rapidassessment to determine the scope of the problem, mobilization of additionalresources (such as personnel, equipment, supplies, communications, andinformation), and integrating resources to create an organization capable ofmanaging and sustaining the required response and recovery. during and afterthe response, the need to disseminate information to the public, including thepress, is an important part of the context within which crisis managers operate.the workshops also addressed questions of preparedness, since preparationsand plans can alleviate difficulties associated with response and recovery from acrisis.requirements at each phase differ. for example, conventional (e.g.,application needs for computing and communications11computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.scheduled) training is needed for the earlier phases, while at crisis time, ''justintime" training is needed to bring people up to speed. recognition of precrisisphases illuminates opportunities for specific preparations, such as thesimulation of possible crises to identify likely needs, which can guide the prepositioning of resources in anticipation of predictable kinds of crises (e.g.,earthquakes, floods, or tornadoes in areas prone to such natural disasters) or theformation of plans to access them when needed. an analogy may be made tothe emergency room of a hospital. statistical expectations may help to preposition equipment, supplies, and trained staff. during holidays, trafficaccidents tend to increase. this situation can be handled with an increase inemergency room staffing and supplies to meet the predicted demand; however,the next emergency that is wheeled in the door is usually not predictable as tospecifics. a major crisis that overwhelms the capacity of the emergency roomin a way that cannot be predicted requires contingency plans and coordinationwith other organizations, in order to locate and bring in additional resources orto divert patients elsewhere.these tasks grow increasingly complex at scales larger than a singleemergency room, where many organizations and kinds of resources becomeinvolved. many such tasks relate directly to or make use of computing andcommunications, since important resources for crisis response and recoveryinclude information repositories, computing capacity, and emergencycommunications links. two sets of broad goals for using information resourcesto support crisis management, one from fema and one from thenongovernmental national institute for urban search and rescue (ni/usr), arepresented in box 1.1.workshop participants identified several distinctive characteristics ofcrises and factors relevant to managing them: magnitude. crises overwhelm available resources. (this is the distinctionmade, at least for the purposes of the workshop series, between crises andemergencies.) in many cases, problems that are manageable at one levelbecome crises as the magnitude of the problem increases beyond normal orexpected bounds, thus overwhelming the resources on hand. anautomobile accident or a fire in a single building requires emergencyservicesšfire engines and ambulances are dispatchedšbut does notoverwhelm those services and so is not a crisis. overload situations maylead to crises. they may arise, for example, in telephone systems, powerplants, weather centers, and hospital emergency rooms. hospitals in aregion may be prepared for a certain number of emergency patients withina 24hour period, but will experience a crisis if ten times as many patientsarrive. urgency. crises have a serious, immediate impact on people and propertyand require an immediate response. lifesaving fire, rescue, and emergencymedical services are clear examples. citizens also want immediate accessto information about obtaining disaster relief, such as emergency loans toreplace lost homes and property, and rapid processing of claims. in somecrises, a fastapplication needs for computing and communications12computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.box 1.1 goals for using information resources incrisis managementat workshop ii, john hwang, of the federal emergency management agency(fema), identified four major areas for applying information technology: situation assessment, both immediately after a crisis begins and updatedthroughout the crisis; emergency lane communications so that emergency managers can communicatedespite structural damage and traffic congestionšincluding broadbandcommunications; public access to emergency information, such as warnings, directions to shelters,and ways to obtain relief afterwards; and claims processing after the crisis. the fema information systems directorate's "strategic plan for informationresources" (september 30, 1994)1 sets the following goals: reduce the effect of potential or impending disasters. improve training and exercising through the use of information technology. enhance the local, state, and federal government's ability to set up responseoperations and provide direct disaster assistance after a presidential disasterdeclaration. improve victim registration and processing. increase the availability and timeliness of emergency management information. better coordination of federal, state, and local emergency management functions.the national institute of urban search and rescue provides the followingvision statement:2"vision 2000": crisis information systemwithout such a [crisis information] system there can be no coordinated, costeffective, efficient response. we have established the following goals for the crisiscommunication architecture:to deliver the right informationto the right peoplewithin the "action cycle"to save the greatest number of livesto protect the largest amount of propertyto contain the event at the lowest possible levelto guarantee a sustainable economy for the united states.1 available from the fema information systems directorate home page at http://femapub1.fema.gov/fema/infosys.html.2 available from ni/usr home page, http://www.silcom.com/~usar.application needs for computing and communications13computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved. response may reduce the need for later countermeasures. for example, in acommunications network overload, a cascading problem may be avoidedby isolating the failure quickly, thereby diminishing the need for greatercorrective measures later. although more slowly developing, broadscaleproblems such as global climate change, disease, or overpopulation arecrises of a longterm nature, workshop discussion generally centered onshorterduration events with severe time pressures. (however, it isimportant to note that longterm effects may influence planning for shortterm crises; for example, steven smith, of the national center foratmospheric research, noted research suggesting that global warming islinked to an increase in the intensity of extreme weatherrelated disasters,such as floods and hurricanes.) infrequency and unpredictability. some highmagnitude events, such asearthquakes, are not necessarily unexpected, but they occur infrequentlyand their location and magnitude are unpredictable. therefore, it is notfeasible for agencies with constrained budgets to keep on hand theextraordinary resources needed to handle crises in every location wherethey might occur. the nature of the warning influences the ability torespond; earthquakes, for example, occur with effectively no warning,whereas approaching hurricanes can be tracked, although their exactlandfall is difficult to predict more than a few hours in advance. crisismanagement thus requires contingency plans for identifying neededresourcesšincluding resources that other agencies or organizations canofferšand deploying them rapidly. uncertainty and incompleteness of information and resources (combinedwith a need to respond in spite of these shortfalls). even with completeinformation, chaotic conditions during a crisis make the prediction offuture conditions uncertain. a strategy of waiting and watching is notgenerally viable in a crisis, and so decision makers must be prepared to actdespite these limitations and to change course as new information becomesavailable. special need for information and access methods. both prior to and duringa crisis, there may be extraordinary needs for more and different sorts ofinformation (both from the crisis scene and from remote sources ofinformation and expertise), as well as for sharing and presentation ofinformation to decision and judgment makers, analysts, workers in thefield, and the public. these parties' needs create demands for informationflows into, within, and out of the crisis area. often, special tools and accessmethods are needed to consolidate information from disparate sources. forexample, in the search and rescue efforts after the oklahoma city bombingin april 1995, information was consolidated from many sourcesšincluding agencies with offices in the alfred p. murrah building andnearby damaged buildings, architectural diagrams, city maps, digitizedphotographs of the scene, and reports from rescue workersšto map thebuildings and determine the highprobability locations of missing people.this allowed the searches to focus on those locations, thereby avoidinguseless and dangerous searches of lowerprobability locations.application needs for computing and communications14computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved. multidimensionality. some events become crises because of theirmultidimensional nature and side effects. a crisis that damages thetransportation system can create crises in systems that depend ontransportation, such as medical services; it may also inhibit a rapidresponse, thus worsening the problem. a power failure in new yorkduring a heat wave may cause not only health and safety risks for peoplecaught in a subway system, but also economic disruption due to theinterruption of computerbased financial transactions (e.g., stock trading).several workshop participants commented on the greater consequencesassociated with physical events that caused economic disruption,especially disruption to the financial system of the country or world. location and social context. where a crisis occurs influences its nature andthe ability to respond. many communities apply a rational costbenefitanalysis that gives planning for highly unlikely events a low priority. thuscalifornia, which expects to have earthquakes, is better prepared for themthan are other states. crises may be international, national, regional, state,or local in scope. international events have the broadest set of issues, butperhaps lower expectations from the u.s. public for speed andcomprehensiveness of response.the political and social context can create resource limitations in localcrisis management. this has obvious implications for communities' preparationfor crises, among which is limited ability to acquire and use informationtechnologies effectively. as nicole dash, of the university of delaware, stated,in addition to technological advancements, we must also look at the humanelements. one of our first priorities is to recognize that emergencymanagement is often not a high priority in many communities. community riskassessment tends to employ a rational choice approach in an attempt to balancecost and benefit. because disaster is seen as rare, emergency planning isassociated with high cost and low benefit. . . . in addition, emergencymanagement personnel often lack the computer skills and hardware toutilize . . . technology oriented toward crisis management needs.4the crisis management budget constraints of communities are outside thesphere of computing and communications research, but their implications arenot. they demonstrate the potential value of research to make technology moreaffordable by reducing its complete lifecycle costsšmaking it not onlycheaper to purchase, but also easier to set up and maintain, easier to integrateinto existing organizational processes, and more usable without extensivetraining. remote access to networkbased resources and rapid deployment tocrisis locations can also reduce costs to communities by making it possible toshare resources. comments in the workshops from crisis managementprofessionals about the impracticality of learning and using complex, featureoverloaded equipment in the time and resourcelimited context of crisismanagement, however, showed that to realize these cost reductions, technologydevelopment must be informed by testing, measurement, and experience gainedthrough deployment.5application needs for computing and communications15computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.scenariosrealistic crisis scenarios provide a context for understanding andanalyzing the needs of crisis managers for computing and communicationscapabilities. the characteristics of crises discussed in the preceding section,"definition and characteristics," can be used in developing typical cases tomotivate and test elements of a research agenda for computing andcommunications. numerous crisis scenarios exist, developed by various civilianand military organizations for training and planning purposes. access to somescenarios is necessarily restricted, in order to avoid spreading knowledge ofvulnerabilities and response plans to potential adversaries. one publiclyavailable scenario, which was used in workshop iii to stimulate and focusdiscussions, is summarized in box 1.2. the scenario illustrates some of therange of demands that crises may raise.the steering committee also developed the fictional scenario presented inbox 1.3, describing a future crisis and some of the means by which reliefofficials might respond, given computing and communications capabilitiesbeyond those currently available or tested in experimental contexts such as thejoint warrior interoperability demonstrations (jwids) discussed in box 1.2.these capabilities are extrapolated from current areas of research. the scenariodraws on workshop discussions with both experienced crisis managementofficials and researchers in computing and communications. the scenario issomewhat fanciful and is not intended as a prediction of future capabilities or arecommendation for particular technical solutions. its purpose is to illustratespecific ways in which breakthroughs and incremental advances in highperformance computing and communications could be motivated by the broadrange of crisis management needs that workshop participants identified.crisis management needs for computing andcommunicationsnetworking and communicationswhen a crisis occurs, the first order of business is to find out whathappenedšto perform a situation assessment. nicole dash observed that asituation assessment poses two requirements related to communications. first,authorities (such as emergency services managers) at the location of the crisismust be able to communicate their community's situation to the world outsidethe crisis area; second, rapid response teams must be able to enter the area,perform an assessment, and communicate back what they find in real time. inmany crises, the normal infrastructure of telephone and data networks will notbe able to support these initial communications requirements, for one or moreof the following reasons: the crisis is in a location with little communicationsinfrastructure in normal times (such as a remote location or a developingcountry with weak infrastructure), the crisis itself has destroyed theinfrastructure (as large naturalapplication needs for computing and communications16computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.disasters often do), or people overload the public networks by trying to call inor out of the area.the u.s. wireline telephone network is designed to maintain or restorebasic voice communications in the event of emergencies, but it may not bepossible to depend on complete restoration of telephone service. waltermcknight, of the national communications system (ncs), reported that areview by ncs found recurring communications shortfalls for national andregionallevel emergency users responding to disasters.6 these included thefollowing: inadequate voice services; congested wireline and wireless services; unknown radio frequencies for various relief organizations; limited access to distributed information resources; limited information sharing among different functional branches("emergency support functions," such as transportation, communications,fire fighting, health and medical, hazardous materials, and food); inability to send and receive electronic mail among users and regionaloffices (including difficulty finding users' addresses); and lack of service provisioning (rapid setup) for telecommunicationsequipment and facilities.commenting on the current state of crisis communications, john hwangobserved,i think one of the misconceptions is that . . . we have a very robustinfrastructure already . . . that automatically, in times of crisis, is ready to dealwith the emergency situation. it turns out that's just not true. . . . [i]nemergencies, there are a lot variables like mobility, survivability, breakdowns,things which just don't work the way you think [they're] supposed to work.now, what happens is instead of depending on healing the entire infrastructureand bringing it back up again, what you have to do is find a way through it,which i always call the emergency lane problem.to respond to concern about congestion, federal agencies and telephonecompanies (both longdistance and local carriers) have worked together todevelop the government emergency telecommunications service (gets; seegovernment issue, 1995).7 this is a program to reserve voicegrade, analogcommunications capacity (suitable for fax and modem as well as voice) forpriority emergency users, such as federal, state, and local governments andindustry personnel. users access gets by dialing a special 710 area code andentering a personal identification number (pin). gets became available in1995 and was used in the jwid '95 exercise and in response to the oklahomacity bombing, louisiana floods, and (through international calling) the kobe,japan, earthquake.application needs for computing and communications17computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.box 1.2 joint warrior interoperabilitydemonstration 1995 crisis scenariothe department of defense (dod) conducts annual exercises for training andplanning purposes and to demonstrate interoperability of the military services'information and communications systems. they are called joint warriorinteroperability demonstrations (jwids). they seek to test and demonstratetechnologies such as distributed collaboration and the use of intelligent decisionaids; improved battle space management and a common tactical picture includingintegrated collateral intelligence information; improved joint, combined, and nondod agency interoperability; expanded use of commercial satellites and newswitching technology; multilevel security; knowledgebased informationpresentation; expanded use of modeling and simulation including enhancedoperations and simulation integration; telemedicine; and improved networkmanagement and planning, among others.in the jwid exercises, scenarios are used to create a framework for evaluatingthe performance of systems and planners in relation to valid, simulated operationalrequirements. consistent with the growing military emphasis on operations otherthan war, recent jwids have addressed crisis management applications and haveinvolved civilian agencies along with the military. the following scenario, related tonatural disasters and subsequent complications, is excerpted from the descriptionof phase 3 of jwid '95 (conducted in september 1995).1an earthquake measuring 7.6 on the richter scale is registered by the u.s.geological survey as having occurred near new madrid, missouri. the epicenter islocated at coordinates 36.5nœ89.6w. the director of the arkansas office ofemergency services initiates response measures for the state. the governor of thestate of arkansas, reacting to these actions, declares a state of emergency andforwards request for federal assistance. in response, elements of the federalresponse plan [the federal coordination plan for responding to crises are activatedand deployed to provide immediate response assistance and collection of datanecessary to determine actual extent of damages. an initial disaster field office isestablished at the state emergency operations center to facilitate emergencyresponse teams.priority reservation systems of this kind do little for crisis response inregions where telephone infrastructure is damaged and not yet restored or hasnever existed. for these situations, wireless alternatives include terrestrial andsatellite services. however, gets does not have a mechanism for securingpriority access to cellular telephone circuits, which typically become jammedduring a crisis; this reduces its utility for users who must be mobile at the sceneof a crisis. ncs has experimented with crisis communications integrating voiceand data service via the t1 (1.5 megabits per second) transponder of thenational aeronautics and space administration's (nasa's) advancedcommunicationsapplication needs for computing and communications18computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.most of the state utilities and thoroughfares in the northeast quarter of the stateare severely damaged or destroyed. communications are limited to wireless in thedamaged area. loss of life and critical injuries are substantial and basic medical,shelter, power, food and water supplies are decimated. some of the initial damagesinclude: seismic oscillation disrupts military communications capabilities in asoutheasterly direction. a truck carrying chemical and/or biological ordnance destined for the pine bluffarsenal is overturned and the payload (undetonated) is dispersed over a widearea. numerous roads, bridges and interstate highways are inaccessible to emergencyvehicles. public utilities are nonexistent in the decimated area. local telephone service isdisrupted and limited. the populace of the affected area is without shelter, food, water and medicalsupplies.the jwid '95 phase 3 exercise linked participants distributed in arkansas andthroughout the nation using the government emergency telecommunicationsservice (gets), which provides crisis managers with priority voice service overfacilities of the public longdistance and local telephone services (hazardtechnology, 1995a). the national aeronautics and space administration (nasa)advanced communications technology satellite (acts) and a commercial mobiledata network were used for mobile communications. as part of the exercise, a statetrooper "discovered" the spilled ordnance, identified it as dangerous using adatabase of chemical and biological hazards previously installed on his portablecomputer, and reported it via wireless email to the emergency operations centerin conway, arkansas. there, an atmospheric dispersion model was run to predictareas in danger and to plan an evacuation and cleanup operation. crisis managersshared maps, situation reports, briefings, weather data, and similar information overan "emergency information network," a secure subnetwork deployed over theinternet using world wide web technology.1 scenario document available from jwid home page, http://www.pacom.mil.technology satellite (acts). the u.s. army set up a transportable actst1 very small aperture terminal (vsat) in a few hours during the haitioperations in 1994 (dixon et al., 1995, p. 27). john hwang explained thatfema can deploy to a field command center a mobile (truckmounted) satelliteterminal capable of digital communications at t1 data rates (1.5 megabits persecond).8 this is sufficient for multiple voice conversations and some datacommunications between a field command center and authorities outside thecrisis area, but it does not solve the problem of communications among mobileworkers at the crisis. there is also a drawback in terms of the delay involved indriving the van to theapplication needs for computing and communications19computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.box 1.3 crisis 2005 scenariothe trip to the opera was the high point for the thousands of internationalvisitors to the conference. they are streaming out of the new center, which hadbeen built in a decaying downtown area. here, old warehouses are mixed with thenew buildings of the city's economic redevelopment zone.luke is on duty at the crisis center when the first images from emergencyvideo911 calls show the horrifying sight. gigantic explosions rock a set of oldchemical warehouses, and fires and fumes of unknown composition ring the newopera complex. the frightened audience panics and scatters into the surroundingalleys and buildings, where some become trapped. television crews covering theopera immediately switch their cameras to this catastrophe. within a few secondsafter the initial alarms, all the digital video channels on the global informationinfrastructure (gii) are presenting the chaos, damage, and injuries live to a worldwhose virtual eyes are trained on luke's and the other crisis officials' every action.luke, unlike many today, is well prepared for this event. his graduate specialtywas computersupported intuitive judgmentšthe science of making difficultdecisions under deadline pressure with unprepared, uncertain, and incompleteinformation. this education has been augmented with specialized simulations in thefederal emergency management agency's training facility, where various disastersand collaborative response exercises were presented using experiences andtechnology developed from distributed interactive simulation activities of theprevious decade. within seconds of the crisis' occurrence, the command centersystem suggestsšand luke and his colleagues confirm and refinešthereservation of key gii resources. these include priority communications linksšsocalled emergency lanes on the information highwayšand a wide array ofcommunications, computational, and information resources carried atop these lanes.academic supercomputers and distributed metacomputers roll out theirsimulations of colliding black holes and other physical phenomena. now they standready to model the movement of the chemical plumes and raging fires. specializedintelligent software agents roam the gii, and key resources are identified andactivated. some of the audio and database streams associated with the crisis arerouted through translation service bureaus on the gii, so that luke, his colleagues,and the many doctors, scientists, and decision makers from different countries whowill become involved in the crisis can have information presented to them in theirnative tongues. advanced distributed metacomputer support on the gii allocatesand links the reserved computing resources with specialized resource centers(anchor desks) for chemical and atmospheric modeling, which apply the necessarydatabases and reaction simulations needed for plume prediction. the softwarecodes were written in a highly scalable language descended from highperformance fortran so that they run efficiently on multiple, heterogeneoushardware platforms and adapt smoothly to the scale of distributed computingresources that can be brought to bear on the crisis. parts of the modeling codewere in fact written years before but had scaled successfully as underlyinghardware and communications technologies advanced in performance by orders ofmagnitude.the distributed models and information systems rely on faulttolerant highperformance networking protocols and recently developed neural networkbasednetwork management strategies to ensure that the gii's highperformancecommunicationsapplication needs for computing and communications20computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.backbone supplies the necessary secure, lowlatency bandwidth on demand.the backbone evolved from a confluence of ideas, such as the finegrainedmultiplexing capabilities of asynchronous transfer mode (atm), the need toaccommodate delays in communications over global distances (imposed by thespeed of light), integrated services using heterogeneous hardware and tunablerequests for network resources, research on microkernel protocol composition, andfunctional abstractionšall areas of research in the 1980s and 1990s.the judgment support environment that luke and his colleagues usešwhichextends the rulebased decision support techniques of knowledgebased systemsfurther into the realm of incomplete and uncertain information, unpredictabledemands, and support for intuitive decision making by peoplešwas adapted fromcommercial products to support military, law enforcement, and civilian crisis needs.focused, minimally restrictive interconnection standards allow the crisismanagement application to incorporate components and build on top of giiservices designed for larger commercial markets such as health care. the thrivingmiddleware industry supplies the necessary integration technologies, includingagents, rapidly configurable wrappers and mediators, and graphical scriptingenvironments.luke benefits from a natural, intuitive user interface, which maximizes hiseffectiveness under stress and fatigue. this capability builds on advanced virtualreality ideas and tailors the computer interface to the problem at hand. luke sees athreedimensional geographic information system (gis) when viewing the spatialconfusion of the catastrophe; a virtual podium when briefing news media; aboardroom when defending his actions to angry politicians; and a summerwildflower meadow in moments of thought. monitors record luke's actions so thatthe system can learn for future events. they note an increase of errors or stressthat is signaled to luke. the information filtering, data fusion, and presentationtools also adapt to luke's condition, reducing the number of inputs to which hemust react.luke shares the virtual environment with others from federal, state, and localagencies and private institutions (such as hospitals and universities). these peopleform a virtual instant response organization customized for the situation at hand.whether supported by supercomputer or handheld personal assistant, all interactover the gii through a common environment with a range of collaboration andproductivity tools. however, the presentation of the bandwidth and computationintensive aspects of the environment variesšfor example, from text to still imagesto videošdepending on the available computing and communication resources. inthis way the gii enables adaptive linking of "comeasyouare" computational,communications, and personnel resources.jane, one of the leaders of tactical operations for the crisis, is on vacationhundreds of miles away in the northern adirondacks, but she is able to collaborateeffectively with other leaders and people at the disaster site. in the area of thecatastrophe, a digital infrastructure installed at the end of the previous century isaugmented by wireless connections and supplies digital video and other data fromthousands of sensors to image processing and highperformance multimedia serverresources on call outside the crisis area. fortunately, basic mathematics researchhas developed adaptive compression algorithms, which are included in the giiprotocol stack, so an order of magnitude more data can be carried over these linksthan would have been possible 10 years earlier. jane observes multiple, threedimensional perspectives of the crisis scene composed of data from hundredsapplication needs for computing and communications21computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.crisis. because of congestion or damage to local cellular telephonenetworks, local communications generally must rely on fire and police radios,which do not support data networking.of separate cameras, global positioning system (gps) detectors, satellites, andother sensors, both fixed and carried by relief workers, as well as views from thenews media. these data are integrated into continuously updated simulations ofthe vapor plume spread. they are also used to verify critical, uncertain information,such as the actual location of bridges and roads that may be misplaced onoutdated or incorrect city maps.the local authorities and institutions in the area of the catastrophe had fullyimplemented the new metadata standards in their public records, so that jane isable quickly to access and integrate the necessary community databases to identifymedical and other crisisrelevant resources. jane issues an alert for hospitals whocan care for the unusual chemical poisonings. medical records are fetched fromdistributed databases throughout the globe so that each patient is given theappropriate care. digitized maps of the area are superimposed on the realtimeimages to optimally plan search and rescue operations. maps of specific streetsand buildings from tax records and architectural plans are downloaded to portableflatscreen devices carried by rescue workers at those buildings, who modify andupdate the maps with information obtained firsthand. within the security perimeterof the crisis management system, proprietary data are made available, with thecrisis priority temporarily overriding normal intellectual property safeguards so thatcrisis managers can use the best multimedia commercial yellow pages to help theirpersonnel in the area find key resources. tracers and trusted information agentsmonitor the cryptographically marked proprietary data to ensure they do not migrateout of the virtual subnet reserved for the crisis management effort.jane superimposes a view of the latest predicted spread of toxic plumes with agis representation of firstaid stations and determines that one of the stations soonmust be moved. when she selects an evacuation route, the judgment supportsystem offers up live video of potential choke points along the proposed route, andjane notices debris blocking the way. she could open a voice link with workers onthe scene to make sure they clear the road, but jane decides that those workers'current relief activities (as displayed by the judgment support system) have higherpriority and selects a different, but still adequate, evacuation route. thus, thejudgment system helps jane and other judgment makers make the best use ofavailable police, medical, and fire fighting personnel.by morning, the crisis is over. authorized relatives and colleagues of injuredpeople are able to discover and remain aware of their status on a momentbymoment basis over the gii, including public information kiosks placed at all sheltersand hospitals to which survivors have been dispersed. information gathered duringthe response is integrated and maintained to enable prompt resolution andsettlement of insurance claims.an example of a crisis in which initial response teams went into the fieldwith portable computers and satellitecapable telephones (which are limited toapplication needs for computing and communications22computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.much lower than t1 rates) was hurricane marilyn, which struck the u.s.virgin islands in september 1995. the u.s. army sent a 12person earlyassessment team, called an ''away team," to st. croix before the hurricanearrived (hazard technology, 1995c). the team carried a 27pound kitconsisting of a laptop computer with commercial crisisoriented databasesoftware and a communications set that linked with the commercial inmarsatsatellite communications service. in the first 24 hours after the storm, theirs wasthe only working communications system on the island, and so all official callspassed through their link. not enough official channels were available over thislink to meet the demand, and in the future, away teams are slated to carrymore communications sets. however, no local networking among the laptops onthe scene is currently supported. this limits team members' ability to shareinformation collaboratively.after the initial situation assessment, a rapidly assembled responsestructure with many people from different agencies and organizations must havethe ability to communicate to coordinate their actions. as the ni/usr's vision2000 statement.9 observes, current capabilities are limited to voice telephony,which is inadequate for crisis information and computing needs; furthermore,the lack of interoperability among equipment of different organizations is aserious problem that adds cost to the overall responsešall of which suggeststhat research investments in solving this difficult problem could have highpayoffs. the ni/usr statement adds,an [interoperable] crisis information system is not available to the civilian sideof [crisis management in] the united states today. a new system must havecertain capabilities to function in the worst of circumstances. . . . the systemmust be interlinking, have open architecture, agreedupon standards andconsistent protocols. a lack of interoperable emergency communications is thegreatest cause of the unreasonable escalation of dollar costs of disasters today.the cost of response to large scale, multiagency, multijurisdictionalemergencies has soared off the top of the charts. the crisis informationstructure must be accessible to the scene of the need.the control structure for emergency response must function in a bottomupmanner. people bleed at the site of the disaster, not in the halls of either thestate capitol nor on the floor of congress. however, the uniformity incommunications must be implemented from a national perspective within aunited framework. we have dozens of layers of overlapping technologies withoverlapping and often inconsistent characteristics. thirty or 40 years ofundirected growth in emergency communications has left us with our onlymeans of crosscommunications being the telephone. telephones alone cannotand do not provide the robust crisis information system necessary today nor forthe future.james beauchamp, of the commander in chief, pacific command(cincpac; a u.s. military command organization), noted that interoperabilitycan be an especially significant problem in overseas disaster relief missions, inwhich military forces, government agencies, and humanitarian relief agenciesapplication needs for computing and communications23computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.from many countries may have to interoperate with each other.10 moreover,solutions that are complex and difficult to implement, however technicallybrilliant, are of no value in the urgent context of a crisis. as beauchamp pointedout,the last [communications equipment] i need in a time of crisis is something ihave never worked with before. if you're going to send me something brandnew, one, don't ask me to transport it for you if it's very big. and two, you hadbetter have somebody that knows how to operate it. i haven't got time . . . inthe first 24 to 48 hours to train somebody new. i'm going to go with what iknow. . . . i have to integrate everything i'm doing across the wide function . . .at my cinc [theater commander in chief] level when i'm out with the jtf[joint task force], . . . the foreign country forces, . . . probably anywhere from30 to 75 nongovernmental or private or volunteer organizations. none of themhave the same communications gear. if they have anything, a lot of them haveam [radio]; some of them have nothing at all. all of them have a differentagenda and about half of them don't trust the military at all.security of the network is an obvious concern in crises where there is anactive adversary seeking to obstruct the response. this is clearly the case inwarfare and may also apply in confronting terrorism and criminal acts. theresponse team must keep its plans secret from hostile parties, and it must protectits communications against denial of service. security needs are not limited toactive, hostile situations. crisis managers may need to communicate sensitiveinformation, such as personal medical records and national securityrelatedsatellite imagery; the threat of disclosure of such information over an insecurecrisis response network could leave the owners of information unwilling toshare it with crisis managers. robert kehlet, of the defense nuclear agency,observed, "when you operate at a federal level, though, you get access todatabases and information that are very sensitive in nature, and you don't wantto pass that out to the world in general and make it totally and completelypublic accessible. you just can't. that is privacy act information." in practice,this restriction has prevented fema from sharing some types of informationoutside a narrow sphere; instead, fema must handle the data itself and sharethe results in sanitized form (such as map images without the underlying data).lifting this limitation might improve the flexibility of responses, but before thatcould happen, security technologies (as well as information security practicesand guidelines) would have to offer greater assurance than they do now.another emerging communications issue is the challenge associated withdistributed sensor networks. the crisis 2005 scenario (see box 1.3 ) illustrateshow realtime data might be applied not only in simulation but also tocomplement other information sources. isolated and experimental examplesdiscussed by workshop participants illustrate the value of fixed sensor networksfor both anticipating and responding to crises, if they survive the crisis itself andcan be managed and integrated effectively with other resources. kelvindroegemeier, of the university of oklahoma, discussed the integration ofweather sensors inapplication needs for computing and communications24computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.oklahoma's mesonet and the national oceanic and atmosphericadministration's (noaa) doppler weather radars in real time with highperformance modeling and simulation for severe storm prediction. mesonetsensor data are communicated over wireless spectrum on loan from oklahomalaw enforcement users (oklahoma state university and university ofoklahoma, 1993). a more demanding load on networks is posed by thecalifornia institute of technology's (caltech's) pilot digital seismographic datanetwork, which uses 56kbps (kilobits per second) frame relay services grantedby pacific bell's california research and education network to carry data inreal time to earthquakemodeling computers. caltech's egill hauksson noted,the goal of realtime earthquake monitoring is to collect data in real time fromsensors in the field and to deliver near realtime information and analysis withhigh reliability to the users in the field. today, continuous data collection ismuch more demanding of bandwidth and speed than eventdriven informationdistribution. . . .hauksson added that sustained longterm maintenance of a realtimeearthquake monitoring system is infeasible: past experience shows that analognetworks yield unacceptably noisy data and have too limited a bandwidth, whilethe alternative of commercial digital network services is too expensive.11sensors that could be deployed rapidly during crisis response couldprovide additional inputs and perhaps increase the resolution of existing sensors'coverage. affordable sensor systems for crisis management, however, may notbe available until larger commercial markets demand their development. davidkehrlein, of the office of emergency services, state of california, speaking ofthe need for spatial data including maps, suggested that "10 years from now,when they have . . . locators in vehicles and chrysler and ford and gm say, 'wewant this country mapped, by golly,'. . . you will get that [mapping] done. buttoday, those databases don't exist at that quality level."sensor networks do not automatically ensure data quality. whereas theglobal positioning system (gps) provides information of known reliability,other sensor networks may decrease in quality during a crisis, in ways that maketheir integration with models or with other databases challenging. egillhauksson noted that the delivery of noisefree data to models can be crucialduring a crisis because "computer algorithms and models designed to deal withnoisy and unpredictable data are inherently unstable and prone to failure underhigh load conditions, when they are most needed. if noisefree digital data areavailablešas opposed to noisecontaminated analog datašthe data processingcan be simplified and made more reliable." his comments underscore theinterdependence of computing and communications technologies.the vision expressed by ni/usr (see box 1.1)šof a wellintegrated,interoperable communications network supporting a mix of voice, data, andvideo communicationsšis beyond the reach of current crisis management, forreasonsapplication needs for computing and communications25computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.including both technology limitations and cost. experimental systems of highcost, unknown reliability, and doubtful ease of use are not appropriate forwidespread operational deployment. however, targeted deployment andexperience with real users in realistic exercises (such as jwid '95) and actualcrises are crucial for testing and developing technologies and research ideas(such as those elaborated in chapter 2) to make communications systemsaffordable and usable in the future.computationcrisis management can benefit from computation at all levelsšfrom theforward scene of action to strategic planning and coordination at state, regional,and national levels. crises place demands on traditional highperformancecomputing applications, such as modeling and simulation. they also underscorethe need for a broader notion of delivering computational performance to userswho require it, wherever they are located, through a balanced, integratedcollection of computers, communications, and data storage spread throughoutthe response organization.traditional highperformance computing has been applied for years tomodeling phenomena that are relevant to crises, such as severe storms,earthquakes, and atmospheric dispersion of toxic substances (ostp, 1993,1994a; nstc, 1995). however, highperformance modeling resources havebeen used primarily for scientific research, rather than realtime crisis response.forecasting by the national weather service, including hurricane trackpredictions, appears to be among a small number of exceptions as a resourcederived from highperformance computation that is operationally available forcrisis response planning in real time. (kelvin droegemeier described a stormprediction model, discussed in chapter 2, that has been tested experimentallyfor realtime applications.) the ability to rapidly requisition computers engagedin scientific research and other activities, as envisioned in the crisis 2005scenario, would make highperformance resources available for other casespecific applications during crises, but will require both new administrativearrangements and further advances in the flexibility, affordability, and ease ofuse of these resources.not all simulation problems require highperformance computation toyield useful results. robert kehlet described some ways that fema usesworkstationbased models, integrated into a tool called the consequencesassessment tool, in actual operations, such as speeding up relief assistancebased on earthquake model outputs. for example, after the northridge,california, earthquake, officials approved checks to homeowners withoutwaiting for a site inspection, if their residences were in areas that simulationsidentified as heavily damaged. kehlet's examples are a proof of the concept thatmodeling also can have practical operational value in predicting how animpending crisis will evolve and in planning a response. given some advancenotice of the path and severity of anapplication needs for computing and communications26computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.approaching hurricane, fema can simulate the likely damage to populationcenters, helping officials plan the assembly and deployment of relief supplies(e.g., food, shelter, and medicine) to an affected area.12 however, the ability togenerate accurate inputs to this analysisšthe hurricane's path and severityšis avery difficult simulation problem in which further advances are necessary. asjohn hwang observed,what we are not very good at is phenomenology modeling; e.g., actuallymodeling a hurricane. what we are good at is [estimating damage]. givensome kind of hurricane, a particular path, and the intensity, we certainly can doa lot of analyses of economic, populace, and infrastructure damage, andestimate what will happen to a particular area.hwang's comment is especially significant in light of the particularimportance of weatherrelated phenomena to crisis management. kelvindroegemeier relayed the statistic that between 1967 and 1991, 67 percent of theworld's major disasters were meteorological or hydrological in nature. modelingmany of these phenomena requires highperformance computation. noaa'shigh performance computing and communications office anticipates thatincreases in computing power are needed to improve understanding of weatherand climate effects, for example, by improving the resolution of weather modelsand more accurately representing key features such as weather fronts and oceaneddies (sawyer, 1995).13 roger ghanem, of the state university of new york,buffalo, noted at workshop ii that many other natural and technologicaldisaster phenomena will also be amenable to highperformance modeling, suchas forest fire and urban fire spreading, detailed structural analysis of damagedbuildings, and chemical and nuclear plant accidents.performance is needed not only to produce more accurate modelingresults, but also to deliver them in a timely manner. as lois clark mccoy, ofni/usr, said, "the greatest hazard with which we deal in crisis management istime." james beauchamp noted that timely results are a function of more thanprocessor speed; fast processing is useless if it cannot be applied to current (realtime or nearrealtime) data, which could be the case if large efforts atpreprocessing or formatting the inputs are necessary before a system can get towork on the actual problem at hand. timeliness also requires communicationsand storage to deliver the results of computations where they are needed. this isreflected in lois clark mccoy's call for making remote resources available:systems are available today off the shelf. . . . they work on pcs [personalcomputers]; therefore they have limited memory and their processing time isslow. to date there has been no attempt within the emergency managementdomain to centralize the needed highperformance computer capability for offsite processing. the product of this offsite processing could then be suitable innear real time for downloading onto the field pcs. this seems to be the nextapplication needs for computing and communications27computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.nearterm solution to increasing the power of the present emergencymanagement software.david kehrlein gave an illustration of the potentially useful combinationof centralized highperformance computing and fieldbased pcs. computeraided design (cad) software proved useful in the search and rescue operationat the murrah building in oklahoma city to map the areas to be searched and tocorrelate estimated locations of victims (based on where their offices werelocated before the blast) with the actual scene. a useful application, but one thatwas beyond the available computational resources, would have involvedtransferring the cad data into a structural model and using finiteelementanalysis to predict the loads on various parts of a damaged building. this wouldindicate where shoring was necessary to prop up damaged structures and reducethe danger to survivors and rescuers from further collapses. remotecomputation is appropriate for this application because relief teams in the fieldhave, at most, personal computers available on the scene.simulation can potentially be useful for testing alternative operationalchoices, for decision support during crises, and as an aid to planning andpersonnel training before crises occur. workshop participants suggested thatsimulation of an ensemble of related options and their outcomes in scenarios orduring actual crises could improve decision making, if the models weresufficiently realistic. before a threatened terrorist act, for example, there may beenough time to simulate a range of tactical approaches and select the one mostlikely to succeed.however, more is required than modeling of physical phenomena.phenomena that depend fundamentally on human individual and organizationalbehavior are complex and difficult to model realistically, making the simulationof human judgments such as the actions of adversaries and the politicalconsequences of decisions particularly challenging. nevertheless, there is aneed for ways to model these phenomena, because decision makers requiretraining to develop good judgment skills. speaking of military involvement ininternational disaster relief operations, james beauchamp observed:all of a sudden every decision [the operational commander makes] not onlyhas a military application to it, it has a political application. . . . you have totrain a guy to do that. . . . i haven't found a good model yet to really train thatguy to change his mindset from a tactical commander today to an operationalcommander tomorrow. we've got to give him models that show him the valueof public affairs, the value of doing news interviews, how to manage the press,how to manage information, how to deal with the customs and courtesies ofanother country, how to deal with coalition warfare when the day before hewasn't doing any of that.modeling and simulation are not the only applications requiringcomputation; all elements of an information infrastructure can be made morecapable by increased computing power. in the information arena, applicationsrelevant toapplication needs for computing and communications28computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.crisis management that demand highperformance computation include datamining to detect anomalous entries (outliers) in federated databases; data fusionto integrate sensor inputs with other information sources; geographicinformation systems (perhaps, given sufficient computing power, with threedimensional terrain rendering); and stereo reconstruction from multiple imagesand video streams.computation applications for information management call for a balanceof performance and accessibility. david kehrlein argued, for example, thatalmost any improvement in placing information technology at the front lines ofa crisis, such as a pc in every relief shelter, would be a valuable improvementin the accessibility of computing resources. even maintaining a roster ofsurvivors at each shelter and handcarrying data diskettes between shelterswould be a first step toward improving the current situation. informing rescueteams that someone they are seeking in a collapsed home is actually alive andwell in a nearby shelter is a major benefit to the search and rescue operation. ingeneral, information management is a crucial need that can be highly complexin crises, as discussed in the next section, and it requires access to computingpower at all levels of the response effort.information managementin a crisis, problems can arise from both a scarcity and an excess ofinformation. scarcity of information about an unfolding situation must beovercome by locating and obtaining information from many different sources.once the response organization begins pulling in information, however, a floodof information can overwhelm decision makers. as donald brown, of theuniversity of virginia, observed, "there is too much information for humandecision makers to use effectively in a crisis response situation. computerbased data fusion systems can aid human decision making by quicklyassimilating and filtering information."computing and communications technologies can help to identify, retrieve,filter, and integrate relevant information into a manageable, coherent picture ofthe crisis. alan mclaughlin, of lincoln laboratory, massachusetts institute oftechnology, noted great similarity between crisis management and militarycommand and control, in that both require improved "situation awareness . . .and a common relevant picture of the area of engagement." lois clark mccoystated:the essence of crisis management is an effective information handlingcapability. commanders must have it; analysts must have it; tactical operatorsmust have it. local emergency managers now realize it is possible to obtain arapid and clear picture of the disaster . . . yet, we still have not applied thesetools and capabilities to the actual command and control of emergencyresponse operations.application needs for computing and communications29computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.the urgency of crises forces an ad hoc responsešpiecing togetheravailable sources by any means available. for example, search and rescueworkers in major floods and earthquakes have been guided to victims by imagesfrom news helicopters (gillies, 1994). urgency can lead to extraordinary effortsto bridge the gaps between data sources, such as printing maps and correlatingdata from different systems by hand. david kehrlein related how, following the1991 fires in the oakland hills, california relief officials obtained local utilitymaps and overlaid them with gps data collected from the field as a way ofidentifying the owners of various pieces of unrecognizably devastated ground,who could claim disaster benefits. manually registering this information againstprinted maps is laborious and slow.data sources maintained by many different federal, state, and localagencies may be relevant in a crisis. walter mcknight listed, for example,geographic data, demographic data, medical files, and realtime weather data.because such data are developed in separate contexts specific to each agency,they often follow different formal and de facto standards, which makestranslation and integration difficult. those who hold data may have littleincentive to make major efforts to accommodate external needs such as crisismanagement. thus, efforts such as a recent initiative by the emergencymanagement and engineering society, to develop and obtain compliance withcommon crisis information standards are likely to progress only slowly(newkirk, 1994, p. 305).geographic information systems (giss) provide a good example of boththe opportunities and current limitations of integration across different datastandards. data fusion from multiple sources, managed and presented within agis, can support current assessments of situations and planning for futureevolution of the crisis. for example, a gis map with building locations (drawnfrom a database of residences and businesses) could be combined with sensordata on wind speed, direction, and chemical composition of a toxic vapor cloudto show where evacuation must take place. integrating additional gisformatteddata about the current location of emergency vehicles, shelters, and reliefsupplies could facilitate evacuation planning. in addition to the technicalchallenges posed by fusion of data from mixed sources, however, variationsamong different vendors' gis standards currently impede such uses. althoughexisting commercial gis standards allow for the import and export of data filesin different formats, the main operational processing of geographic informationoccurs within proprietary internal structures (newkirk, 1994).in addition to integrating across different standards and types of data,computational help is necessary for abstracting, adding value, and therebyturning data into useful knowledge. this problem involves much more than justtranslation between data formats. integration requires recognizing connectionsand patterns among completely different kinds of data, such as video imagesfrom aircraft, map coordinates of structures and roads, and spoken or writtenfield reports from relief workers. it also requires a capability to cope withmissing,application needs for computing and communications30computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.inaccurate, or deliberately falsified data. chapter 2 discusses opportunities todevelop and improve what workshop participants characterized as "judgmentsupport" capabilitiesšinformation technology tools that can support the crisismanager in making judgments in unexpected, urgent situations, in whichinformation is uncertain and incomplete.14in crises, integration and analysis must happen rapidly to be useful. asjoseph stewart ii, of mitre corporation, observed, information managementhas been addressed in the battlefield context, but to solve the problem there is aneed for much better integration of computing that is specifically highperformance:decision makers . . . must be presented with timely intelligence . . . the choreis to turn the data into useful, corroborated, validated information that may bepresented to decision makers with confidence. this accrual, sorting,corroboration, consolidation and dissemination of continuously arriving data isa major task. . . .in the military situation, data arrive by electronic means and generally in aformat that is prescribed. this format contains the essential elements offriendly information in easytoextract form, but there is much additionalinformation that is sent along as plain text. some data may be missing fromearly reports. some information on the same contact may be referenced todiffering coordinate systems if it comes from more than one observer. latencyof the information derives from delays in the communications system, poortime coordination in the field, or the inability of an observer to transmit it untilhe returns to friendly territory. . . . in the civil context, sources of data are "lesstrusted" and more varied, and no single corps has the responsibility forensuring that data get consolidated. moreover data may not be released by theorganization that collected them, or the release may be delayed, thereby addingto the latency problem.the application of [highperformance computing] to this problem must providea realtime solution with an inline system that is capable of parsing standardformatted information from a variety of sources. . . . input data could also beweighted in this system such that data with a high degree of positional andtime accuracy from systems that access gps or a triangulation system wouldcount more heavily than other data. . . . computers could be assigned toprocess data from simultaneously arriving messages, until some timebasedsorting and ordering can be done. contacts could also be compared withdatabases, most of which are countable but large. processed information mustthen be presented to the decision makers for fusion with other sources of datathat are not automated. . . . high performance is required to allow calculationsto be done in real time, so that the means of processing does not add to thelatency problem for later users.data quality is another important issue. the quality of commerciallyavailable gis databases poses obstacles to automated integration, because datain the gis cannot always be trusted, but it is not apparent from the gis whichdataapplication needs for computing and communications31computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.points are likely to be out of date or otherwise incorrect. david kehrlein relatedthat in the response to the northridge earthquake, the commercial gis databasethat was used had a 40 percent error rate in locating and identifying hospitals,primarily because of ownership changes, telephone number changes, and so on.usually, maps must be updated and corrected at the crisis scene against aerialphotographs and field reports, to identify roads and buildings that are not foundwhere the crisis managers' maps say they are. a national effort to improve thecompleteness, quality, and standardization of relevant data would be onesolution to the problem, but this is unlikely to occur in response to the relativelysmall marketplace demand for crisis management tools.access to databases specific to a crisis region can also be inhibited byproprietary and security classification constraints. for example, participantsfrom fema reported that during hurricane andrew, fema was unable toobtain some necessary data from dade county until it paid the county for thedata. data and system protection mechanisms, some potentially developed forsuch applications as electronic commerce, could help implement more rapidtransfer of authority to access data, particularly if there were a way to ensurethat the data's privacy or intellectual property value could not be compromisedby release outside the circle of crisis management.if the need for specific information can be anticipated, certain problemsrelated to location and integration of information from varied sources can beworked out in advance. john hwang described fema's ongoing developmentof a national emergency management information system, in which subjectarea databases related to crisis management activities (e.g., regulations andrequirements for obtaining federal disaster assistance) are accessible by networkto federal, state, and local authorities. among other benefits, this approachmakes resource sharing possible, thus reducing costs. it also hastens responseby enabling "onestop shopping" for key information. it is not always feasible,however, to predict the need for specific kinds of information (e.g., treatmentalternatives for a mass outbreak of a rare or unknown disease). it may beequally infeasible to preassemble information concerning all possible specificinstances whose general usefulness is clear. for example, rescue workers needbuilding plansšideally in a form that can be loaded into computer structuralmodels. however, the need for detailed plans of the murrah building could nothave been anticipated, and it would likely be infeasible to preassemble plans forevery building in the nation that might suffer a bombing attack. rapid responsetherefore calls for an ability to locate, retrieve, and integrate such informationduring a crisis.usercentered systemsa powerful message from the workshops was that crisis managementsystems must be usable by technical nonexperts working under extraordinaryapplication needs for computing and communications32computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.conditions; ease of use is therefore a central goal. perhaps the most visibleaspect is the interface between the user and the machine. the purpose of thisinterface is to enable effective humanmachine communication; technologicalcapabilities such as graphical display and speech recognition ultimately arerelevant only in relation to that goal. simplicity is not necessarily the highestvirtue for such interfaces, but rather, appropriateness to the task at hand and thecapabilities of each user is required. training and familiarity with tools arecrucial if they are to be useful during a crisis, as james beauchamp's commentsabove illustrate. the finite resources available to crisis managementorganizations put a premium on reducing the amount of training time needed. 15the issue of usability arises not only in training for crises, but during themas well. crises put severe stress on people, because of the extreme pressures tosave lives and avert damage, as well as the fatigue that comes with overwork.david kehrlein observed that stress can lead to a measurable decline in thecognitive capabilities of crisis managers. considering users as part of the totalsystem makes it clear that the ability of tools to adapt to user needs andcapabilities is important to overall system performance.the system environment should provide support for communication andcollaboration between people, as well as the interactions of people andcomputersšin the extreme, an instant ''electronic administration" to support anewly created response organization. noting the complexity of theorganizational management tasks involved, lois clark mccoy identified theneed for ways to track and control the constantly changing information flowthroughout the crisis organization as a way of reducing wasted effort andimproving the organization's effectiveness. in addition, the varied backgrounds,procedures, and methods of working that different collaborating groups bring toa crisis response increase the need for clear, complete communications andinformation sharing; a photograph or map, for example, might conveyinformation with a persuasiveness and clarity missing from verbalcommunications between people under stress who are not used to workingtogether.to achieve the goal of what don eddington, of the naval research anddevelopment laboratory, described as a consistent picture of the situationshared by everyone involved in responding to a crisis, there is a need forinformation sharing that involves more than just multiparty voicecommunications and can be done without facetoface meetings in conferencerooms. to coordinate complex response efforts involving many parties, therecould be value in collaboration support systems (e.g., teleconferencing) thatintegrate both persontoperson communications (in multiple modes, such astext, audio, and perhaps video images) and other forms of shared data, includingmultimedia and sensor data, in real time. multiple levels of computing andcommunications performance must be accommodated, however, because crisismanagement necessarily involves cooperation among people with widelyvarying resources. in particular, integrating workers in the fieldšwhose upperlimit of resources may be portable telephonesapplication needs for computing and communications33computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.and laptop computersšinto the collaborative environment involves an ability toscale across different levels of resources and adapt to variable or unstableresources in a crisis. usercontrolled adaptivity may be useful, allowing theuser, for example, to select tradeoffs between video image quality andfrequency of image redrawing and between still and moving images;alternatively, there may be automated ways to optimize these decisions. thesetypes of scalable collaborative applications are relevant not only in crisismanagement but also in other application domains, including distributed"collaboratories" for academic research and enterprise systems for business.other application domainsalthough the workshop series ultimately focused on crisis management asa tool for uncovering valuable research areas in computing andcommunications, the steering committee and workshop participants spent timeconsidering other application areas. these served both as additional input fromwhich to identify research issues and as a means of testing the generality ofconclusions based on crisis management. the following sections, drawnprimarily from input at the workshops, highlight similarities and differencesbetween these domains and crisis management, including specific researchopportunities that, with respect to crisis management, are discussed further inchapter 2. all of these areas have been addressed more thoroughly in other,focused reports. they are reviewed here briefly to provide a context foršand toexamine their interdependence withšcrisis management. citations are providedto more extensive treatments.the first two areas, digital libraries and electronic commerce, representboth enduser applications in themselves (e.g., educational use of libraries,consumer banking, and retail transactions) and infrastructural services thatenable specific capabilities within other application areas. for example, crisismanagers could turn to digital libraries for information discovery and retrievaltools or to electronic commerce for secure authentication and payment servicesin order to obtain proprietary information on an expedited basis.the other two areas, manufacturing and health care, are applications that,like crisis management, may derive significant benefit from broadly distributedcomputing and communications technologies. manufacturing and health careapplications (other than emergency medicine) place less emphasis on urgent, adhoc response than does crisis management, and so integration and othertechnical challenges can, in principle, be addressed in a less ad hoc manner.nevertheless, these areas face many of the same challenges as crisismanagement for coping with complexity and diversity, integrating informationand software resources, and adapting to user capabilities and needs.the interconnected demand for and use of resources among applicationareas illustrate the potential for technological advances in one application areato benefit others. they also indicate the drawbacks in terms of lost flexibility offailingapplication needs for computing and communications34computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.to accommodate the interdependenciesšfor example, to accommodatedemands for service and access across architectures and standards that, as notedin box s.2, are owned and controlled by multiple parties and are inevitablydiverse. indeed, one observer characterized it as a firm requirement thatresearch on computing and communications in each application area take intoaccount the others, noting, "you can't address one or two of them and let theothers slide."digital libraries16digital libraries make more intensive demands for storage and bandwidthto manage and interchange image, audio, video, and numeric information thando activities with traditional highperformance computational requirementssuch as modeling and simulation. digital libraries require substantial advancesin software; information management technology and practices; and the abilityto process, navigate, manage, and classify not only textual data but alsomultimedia, sensor feeds, and numeric data. digital libraries also represent aprimary focus of research in the scaling of very large, autonomously manageddistributed systems. central issues in the successful development of digitallibraries encompass the identification, development, and adoption ofappropriate standards, as well as fundamental questions about the definition ofinteroperability among systems and collections of information at various levelsand the mechanisms that can be used to accomplish such interoperability.finally, it is important to recognize that digital libraries are not purelytechnological constructs; rather, they also encompass complex sociological,legal, and economic issues that include intellectual property rights management,public access to the scholarly and cultural record, preservation, and thecharacteristics of evolving systems of scholarly and mass communications inthe networked information environment. the requirements for reflecting thisbroader context in software and network protocols are poorly understood butmay generate substantial computational and infrastructure demandsšforexample, to examine intellectual property rights and ancillary evaluative orrating information associated with very large numbers of digital objects as partof query processing and result ranking. design of technical approaches tosupport the social, legal, and economic framework of digital libraries that aresufficiently flexible to recognize and support reuse within a new framework is achallenging problem that itself has significant legal and economic dimensions.as resources that comprise digital libraries are reused in the crisis managementenvironment, it may not be feasible, for example, to stop to negotiate a licenseagreement for access to a networked information resource that is neededurgently to respond to a crisis.networkingdigital libraries place extensive and challenging demands on infrastructureapplication needs for computing and communications35computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.services relating to authentication, integrity, and security, including determiningcharacteristics and rights associated with users. needed are both a fullerimplementation of current technologies, such as digital signatures and publickey infrastructure for managing cryptographic key distribution, and aconsideration of tools and services in a broader context related to library use.for example, a digital library system may have to identify whether a user is amember of an organization that has some set of access rights to an informationresource (analogous to the privileges discussed below in the section "electroniccommerce"). use of digital libraries will require both adaptivity to changingbandwidth and computational resource constraints and the ability to reservenetwork resources. as an international enterprise that serves a very large rangeof users, digital libraries must be designed to detect and adapt to the varyingconnectivity of individual resources accessible through networks. digitallibraries will also build on a range of other infrastructure services such aselectronic payments and contracting.the availability or reliability of resources is a less central issue for digitallibraries than for crisis management. if a data source is temporarily unreachableor otherwise unavailable, the user can be told to try later; however, crisismanagers must make use of the best data available at a given time. bothapplication domains require adapting to the capabilities of user workstationsand the bandwidth that is available to these workstations. strategies that arealternatives in the digital library environment in many cases are mandatory forcrisis management. for example, a digital library system can simply rankresults and at some later time present them, but crisis management applicationsmust summarize data and provide immediate overviews.computationdigital libraries require substantial computational and storage resourcesboth in servers and in a distributed computational environment. little is knownabout the precise scope of the necessary resources, and deployment andexperimentation are needed (lynch and garciamolina, 1995; ostp, 1994b).from the 1960s to the 1980s, much of the research and development in theinformation retrieval community was constrained by the limited computationalcapacity of machines available to most users, particularly the inability toperform computations on large databases in near real time. current increases inthe availability of computational power are leading to a reconsideration of muchof this work and may point toward the use of algorithms that are extremelyintensive in both their computational and their inputoutput demands as theyevaluate, structure, and compare large databases that exist within the distributedenvironment. in many areas that are critical to digital libraries, however, such asknowledge representation and resource description, or summarization andnavigation, even the basic algorithms and approaches are not yet well defined,which makes it difficult toapplication needs for computing and communications36computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.project computational requirements. it appears likely that many breakthroughsin digital libraries will be computationally intensivešfor example, distributeddatabase searching, resource discovery, automatic classification andsummarization, and graphical approaches to presenting large amounts ofinformation that range from information visualization through virtualrealitybased modeling.in addition, distributed queries may be computationally intensive. digitallibrary applications call for the aggregation of large numbers of autonomouslymanaged resources and their presentation to the user as a coherent whole.computation can compensate where individual resources are poorly optimizedfor uses that involve aggregation with other resources in ways that go farbeyond their original design goals. the ability of digital libraries to reuseinformation resources could support crisis management applications. in crisismanagement, for example, information in a gis or a digital library repositorymay have to be reused as the basis of a modeling or simulation activity.17 current digital library systems, however, tend to be designed to facilitatespecific classes of use of information stored in the digital library.information managementinformation management is at the core of digital library applications. as incrisis management, the digital library user requires access to collections ofinformation scattered among a range of autonomously managed repositories.this information must be processed through sophisticated user interfaces andviewing applications that may offer simulation, visualization, modeling, andrelated capabilities. major advances are needed in methods for knowledgerepresentation and interchange, database management and federation,navigation, modeling, and datadriven simulation; in effective approaches todescribing large complex networked information resources; and in techniques tosupport networked information discovery and retrieval in extremely largescaledistributed systems. in addition to nearterm operational problems, approachesare also needed to longerterm issues such as the preservation of digitalinformation across generations of storage and processing technology (whichevolves quite rapidly) and even information representation standards.work on information management approaches for digital libraries has tosome extent proceeded on two levels simultaneously, corresponding to differentmodels of how people use the applications. one level deals with what arephilosophically extensions of existing, physical libraries. these arecharacterized by the assumption that a person is the direct consumer ofinformation and is managing the navigation and retrieval processes, usingmethods analogous to a visit to the library. the other level assumes that thehuman user is more distant from the actual mechanics and management of theprocesses of information discovery, retrieval, evaluation, and use; this leveldeals with intelligent agents, knowledge representation and interchange, sharedontologies, mediators, and relatedapplication needs for computing and communications37computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.technologies. information management technologies are central to both lines ofdevelopment, but the technologies and approaches differ substantially betweenthe two lines of development.usercentered systemsto be effective, digital library systems must be usercentered systems.research is necessary to better characterize the needs and requirements ofdifferent classes of (potential) users of digital library systems, and to gaininsight into how to adapt systems to specific user needs and behaviors.although much digital library research has focused on "public" digital libraryservices, public digital libraries form one end of a continuum that alsoencompasses personal information spaces and workgroup or organizationalinformation spaces. linking digital libraries to personal and workgroupinformation management systems is a central research and design issuešforexample, to develop distributed systems for collaborative data exploration.there are also major demands for training and user support, as well as effectivemanagement by librarians of information repositories.as in crisis management, one of the key issues involves informationfiltering, categorization, and ranking in situations where there is likely to be toomuch relevant information with which the user of the system must cope.however, the range of information that must be processed in the crisismanagement context is likely to be more tentative and questionable, and thequalification, authentication, and filtering of information constitute a muchmore difficult issue. in addition, crisis management has a much moredemanding realtime constraint. this largely precludes the benefits of librariansskilled in evaluating and organizing information; digital libraries can allow agreat deal of human or machine preprocessing. in both digital libraries andcrisis management, incoming information may sometimes be incomplete,anomalous, suspect, or even actively falsified. the realtime constraint of thecrisis management application requires adapting to and compensating for theseproblems, whereas a digital library can simply defer the data for later humanreview or confirmation from supplementary input sources.in summary, many aspects of crisis management are functionallyequivalent to digital library applications, but with realtime processingconstraints (to meet urgent deadlines) and a requirement to operate successfullyin an environment of questionable data inputs and high penalties for failures orerrors.electronic commerceelectronic commerce involves both retail and wholesale commercialtransactionsšpurchase of goods and servicesšacross networks. these range,for example, from consumer online banking services to procurement of partsby manufacturers through electronic data interchange (edi). electroniccommerceapplication needs for computing and communications38computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.involves the use of processing and storage resources in multiple locations (bothfixed and mobile), owned and managed by a variety of end users, suppliers ofgoods and services, and gobetweens. because it comprises fundamentaleconomic activities, electronic commerce cuts acrossšand is part of theinfrastructure foršother application domains. thus, electronic commerce canenable the procurement of medical supplies or reimbursement by thirdpartypayers in health care, as well as the acquisition of new holdings and transfer ofroyalties in digital libraries, or the procurement of relief supplies and the filingand processing of insurance claims in crisis management. these examples aremore a promise than a reality today, although the number of relevant pilot andactual (if smallscale) programs is growing. limitations of current electroniccommerce implementations include the inability to automate entire transactionprocesses18 and restrictions of users' choice among payment mechanisms.19 nevertheless, simply listing the future possibilities illustrates theinterrelatedness of nationalscale applications and the potential for technicaladvances in one area to confer broad benefit. moreover, the effectiveapplicability or extension of electronic commerce to embrace virtually everyperson and organization that participates in the economy underscores theimportance of technology (and standards) to ensure the interoperability ofdifferent commercial solutions without stifling technical and service innovation.both the nature of economic commerce, which fundamentally revolvesaround financial transactions, and its interconnection with other activities makesecurity a paramount concern. motivations include protection of personalprivacy (e.g., personal spending records, health status, preferences), protectionagainst theft and fraud (against individuals and businesses), and protection ofthe integrity of the systems and of the organizations that use them. privacyrelates not only to unauthorized access to specific items of data, but also toaggregation of separate pieces of information (greatly facilitated by theirplacement on networks) to yield a sensitive result, such as a marketer's profileof an individual's overall buying habits. the greater exposure of institutions tofinancial risks will change the business model, which is currently oriented tomanaging as opposed to eradicating risk.the importance of system integrity is increasingly seen as national orinternational in scope: the dependence of financial markets on networkbasedsystems and the networkbased interdependence of businesses, industries, andsectors lead many to link economic and national security. for example, a denialofservice attack on a hypothetical internetbased gateway handling a largeshare of u.s. retail credit card transactions would create a crisis; withoutsubstantial improvements in the security of gateways, such an attack would bemuch easier to arrange on the internet than on the current telephonebasedsystem.computing and communications technologies are relevant to bothvulnerabilities and countermeasure mechanisms; electronic commercemotivates considerable activity in the development and application of securitymechanisms,application needs for computing and communications39computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.concepts for security architecture, and implementation infrastructure (e.g.,infrastructure needed to support publickey encryption). for example,commercial transactions require authentication and authorization of users andprotection against repudiation of commitments by both buyer and seller.mechanisms include identification technologies (from passwords to biometrics),digital signatures, and audit trails. current publickey infrastructuredevelopment efforts focus on linking cryptographic keys with specific useridentity; a more robust infrastructure would incorporate the notion of a user'sprivileges, which depend on potentially changeable characteristics such ascredit card membership, rank within a company or organization, membership ina frequentflyer program, u.s. citizenship, and others. this capability is alsorelevant to crisis management, in which privileges such as authority to accesssensitive data may have to be rapidly but securely conferred on specific reliefofficials.construction of large commercial software systems, such as those used bybanks (and in other domains, for example, manufacturing), continues to face thevery difficult, decadesold problem of inefficiency in the programming process.workshop participants identified the need to overcome this "programmingbottleneck" as an area for continued research, through approaches such ashardware platformindependent programming as a source of potential advance.networkingbandwidth and architecture are key issues for networking in electroniccommerce. bandwidth currently constrains the introduction of new services.for example, bandwidth for twoway video links between tellers and customersthrough automated teller machines (atms) could allow banks to improveservice while reducing the number of bank branches. increasing the bandwidthto tetherless systems is important if services that rely on graphics like thoseavailable through the world wide web are to be ubiquitous.20 as theseexamples suggest, there is a tradeoff between using information retrievalmechanisms that scale the types of information presented to fit the availablebandwidth and increasing the available bandwidth to achieve a higher level ofservice for tetherless and other intrinsically limitedbandwidth accessmechanisms. of course, some transactions, such as accountbalance inquiries,require only small amounts of bandwidth, but the concept of "anytime,anywhere" banking and commerce implies a suitably provisioned, broadlydeployed fixed infrastructure and support for tetherless access.21there are two architectural challenges for networks in electroniccommerce: accommodating heterogeneity in the commercial environment,which implies a general and flexible architecture, and achieving security in thefullest sense, which includes ensuring reliable and convenient service in theface of unpredictable conditions (e.g., user errors, malicious attacks, mergersand acquisitions thatapplication needs for computing and communications40computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.change entities and their relationships). although electronic commerce does notface the extremes of demands on and availability of resources that characterizecrisis management, the dynamism and ubiquitous scope of the commercialmarket nevertheless call for adaptive, selfhealing networks. these areparticularly important in light of the threat of economically motivated attackson commercial networks to steal services or assets (such as intellectualproperty, personal information, and electronic funds) or to deny service formalicious ends.computationthe computation required to support electronic commerce is a function ofthe kind of transaction and business process being supportedšor the aggregateof many kinds. broad experimentation has already begun for purposes of testingthe relative merits of micropayment, aggregation of transactions, servicesubscription, and other models for electronic commerce. daniel schutzer, ofcitibank, observed that computational performance in distributed systems(including communications and storage as well as processing cycles) currentlyconstrains the ability to perform commercial transactions at very low cost,which is necessary if a market for microtransactions (goods and servicespurchased for cents or fractions of cents) is to emerge.within the confines of a single institution such as a bank, traditional,highly computationintensive tasks such as transaction processing and frauddetection (through identifying purchasing anomalies) benefit from continuedimprovements in distributed computing. added support is implied by emergingrequirements, such as realtime pattern and anomaly detection for deterringfraud: a key challenge is obtaining useful results despite dealing with massiveamounts of data from unknown sources and of unknown reliability. thewidespread experimentation with software agents, such as brokers that searchand evaluate over a wide range of suppliers, is beginning to raise questionsabout qualitative changes to existing computing system architectures. brokersimply a crossservice lookup problem emerging in other domains as well, andthis is a special emphasis in digital library research. networkdistributedcatalogs, directories, and independent appraisal services (such as those ofconsumers union) could also aid resource discovery, as could scalable,networkwide advertising mechanisms.workshop participants also noted that simulation and modeling of firm anduser behavior in largescale commercial systems, such as banking and retail,may help smooth the deployment of electronic commerce applications, to theextent that important aspects of integrating technology into organizations can besimulated and tested prior to fullscale deployment. this demandšand thedifficulty of fulfilling itšis similar to the call, noted above, for more realisticmodeling of human and organizational behavior in crisis management trainingand operational exercises.application needs for computing and communications41computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.information managementfor end users to benefit from many electronic commerce services, theywill need to be able to locate and find out about them. this requires improvedinformation search and retrieval mechanisms that are usable across differingkinds and capabilities of equipment. as in other application areas, this impliesaddressing complex challenges in management of distributed informationresources, including distributed file and program synchronization andreplication, and tools such as web servers and web searchers.the extreme heterogeneity of electronic commerce implies a great concernfor data standards that support information management tools and facilitateinterfaces among planning and design, provisioning, production, and businesssystems (e.g., inventory, ordering, billing, fulfillment, and customer support).support for multiple media, including images, sound, video, and hypertext,implies the need for continued development not only of standards forinterpreting graphical and nongraphical data formats, but also of mechanismsfor adapting to different quality demands (e.g., image compression) and accesscapabilities (enduser access and storage devices and communications links).because it is unrealistic to expect that all users would shift to any single setof standards, whether a current or a new one, a major challenge in electroniccommerce is incorporating legacy systems, such as databases andcommunications systems in differing or outmoded formats.usercentered systemsthe development of easytouse tools and other methods for locatinginformation and other resources, conducting transactions, and implementingsecurity, among other needs, is as significant for electronic commerce as forcrisis management and other domains because of the expectation of involvingpeople without significant technical training. the history of automation in retailbanking (e.g., atms) attests to the recognition that consumers often need to beconvinced that a new system is an improvement, and convenience ortransparency of user interfaces and processes is a major part of that process. thegrowing need for system security is the area in which this practical reality ismost likely to be challenged: achieving better authentication of users will placea premium on methods that both are effective and do not overly inconveniencecustomers (suggesting possibly greater interest in physical tokens andbiometrics as opposed to personal identification numbers).manufacturingcomputing and communications are enabling significant changes inmanufacturing. these relate to a very broad range of capabilities and functions,fromapplication needs for computing and communications42computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.initial design to delivery of products. many aspects are captured in the conceptof highly collaborative design and manufacturing by distributed ''virtualcorporations." such enterprises use information technology to enable them todesign and manufacture products in rapid response to customer demand.discussions in the workshop series addressed mainly this aspect ofmanufacturing. among the technological requirements that this perspectiveilluminates are networked computing and information resources to supportcollaborative design; virtual reality "test drives" that allow customer input to thedesign process beginning early in product development, when changes areeasier and less costly to implement; and simulating the entire manufacturingprocess so designs can be optimized to make products that are higher in qualityand faster and less costly to produce. (it should be recognized that although thisview of integrated design and manufacturing presents a fairly broad perspectiveon manufacturing applications, there are many other issues that are more closelyoriented toward production per se, such as robotic monitoring and control ofassembly lines, plant capacity management, inventory management, andautomated inspection for quality control, among others.22manufacturing begins with design; high performance in computation,storage, and networking is important to support rapid design, as well as redesignand customization based on past designs. in the past, much effort inmanufacturing complex systems such as automobiles and aircraft has been spenton improving performance parameters (e.g., speed, range, altitude, size). theseare still recognized as critical under extreme conditions, but more generally theyform a design framework that is a minimum requirement. today the key designcriterion for manufacturing is competitiveness, including time to market andtotal affordability (cstb, 1995b). thus, design is far from the entire story;concurrent engineering involves a whole corporate information infrastructure,integrating the different component disciplines such as design, manufacturing,and product lifecycle support. each of these presents its own challenges;manufacturing process optimization, for example, requires complex,multidimensional modeling and analysis. simulation of manufacturing andassembly layout, logistics (material in, finished goods out), production flow,and material and process variability are additional computation and dataintensive activities.it is worth noting that less than 5 percent of the initial development costsof the boeing 777 aircraft were incurred in computational fluid dynamics(cfd) airflow simulationsša classic grand challenge in this field (seeappendix b); more than 50 percent of these development costs could beattributed to overall systems issues. thus, from the perspective of improvingmanufacturing efficiency, it is useful but not sufficient to advance the grandchallenge application of highperformance computing for largescale cfd. ifonly 5 percent of a problem is addressed with highperformance computing, onecan at best influence fundamental goals such as affordability and time to marketby this small amount.23 computing must be fully integrated into the entireengineeringapplication needs for computing and communications43computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.enterprise to be effective. however, the difficulty of integrating across theseengineering functions is far from trivial. as david jack, of the boeingcompany, said,rationally we should be designing [a boeing plane] from the tools [alreadyinstalled] to reduce the manufacturing costs. we have some codes which weuse for simulating the tooling. they tend to be rulebased. i haven't seen anyclever way of handling those rules where the same rule may be used inconfiguring the airplane as is used in building the airplane. and you have gotthat huge logistical gap between the two. if you change one rule, does itchange the other one? how do you manage that information? that's a problemthat we're only starting to scratch up against.simulation for prototyping purposes could yield more useful results ifintegrated with both virtual and actual tools that are to be used in production.randy katz, then of the defense advanced research projects agency,discussed computational prototyping as. . . the ultimate dream of hypersimulation that has been with the computeraided design community for the last 40 years: the idea that you could havespecialized accelerator hardware that could run simulations for you, [located]at special places across the network. you might include in your simulationactual processing equipment (e.g., ovens, furnaces and photolithographyequipment); they will be connected, have a network interface on them. you'lllike to be able to understand whether you can build a particular semiconductorprocess from end to end where some of the equipment exists, some is beingdesigned, the process itself is being designed, combining a capability forsimulation with the actual use of hardware devices that may exist.there are a lot of discovery, linkage, conversion, authentication, paymentkinds of issues that take place in this kind of environment. you have to find theservice providers . . . [and] be able to have assurances about intellectualproperty rights, just as you would with anything else you might decide topublish which could be copied and handed out without your knowledge. and,of course, you would like the use of these specialized pieces of equipment tobe feeforservice.although the design phase is not itself a major cost item, decisions made atthis stage lock in most of the full lifecycle cost of an aircraft, with perhaps 80percent of total cost split roughly equally between maintenance andmanufacturing. thus, computational analysis should be applied in the designphase not only to optimize the product's performance parameters, but also toshorten the design and development cycle itself (reducing time to market) andto lower the later ongoing costs of manufacturing and maintenance.a hypothetical scenario from aircraft design illustrates how the integrated,designformanufacturability approach to engineering demands advances incomputing and communications. the example considers design of a futuremilitary aircraft, perhaps 10 years in the future. this analysis is taken from a setof nasasponsored activities centered on a study of the affordable systemsapplication needs for computing and communications44computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.optimization process (asop), which involved an industrial team includingrockwell international, northrop grumman, mcdonnell douglas, generalelectric, and general motors.24 asop is one of several possible approaches tomultidisciplinary analysis and design (mad) and the results of the study shouldbe generally valid for these other approaches. asop is designed as a softwarebackplane (distributed across the nation) linking eight major services ormodules. these are the design (process controller) engine; visualization toolkit;optimization engine; simulation engine; process (manufacturing, producibility,supportability) modeling toolkit; costing toolkit; analytic modeling toolkit; andgeometry toolkit. these are linked to a set of databases defining both theproduct and the component properties. the hypothetical aircraft design andconstruction project could involve 6 major companies and 20,000 smallersubcontractors. this impressive virtual corporation would be verygeographically dispersed on both a national and, probably, an internationalscale. the project could involve some 50 engineers at the first conceptualdesign phase. the later preliminary and detailed design stages could involve200 and 2,000 engineers, respectively.the design would be fully electronic and would demand major computing,information systems, and networking resources. for example, some 10,000separate programs would be involved in the design. these would range from aparallel cfd airflow simulation around the plane to an expert system to planlocation of an inspection port to optimize maintainability. there are acorrespondingly wide range of computing platforms from personal computers tohighperformance platforms and a range of languages from spreadsheets tohigh performance fortran. the integrated multidisciplinary optimization doesnot involve linking all these programs together blindly, but rather a largenumber of suboptimizations involving a small cluster of base programs at anyone time. however, these clusters could well require linking geographicallyseparated computing and information systems.because an aircraft is a system that must function with very highreliability, a strict coordination and control of the many different components ofthe aircraft design is needed. in the asop model, there will be a master systemsdatabase with which all activities are synchronized at regular intervals, perhapsevery month. the clustered suboptimizations represent a set of limitedexcursions from this base design, which are managed in a loosely synchronousfashion on a monthly basis. the configuration management and databasesystem are both critical and represent a major difference between manufacturingand crisis management, where in the latter case, a realtime "as good as you cando" response is more important than a set of precisely controlled activities.networkingintra and interfirm collaboration among engineers and linked simulationsand databases requires reliable, secure, and interoperable communications. theapplication needs for computing and communications45computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.need for simulations to exchange large proprietary datasets leads to majorrequirements on both security and bandwidth for the communicationsinfrastructure. integrating actual tools together with virtual ones poses a specificresearch challenge for new control protocols that behave in predictable,understood ways across the actualvirtual boundary. more generally,information infrastructure supporting communication both betweencollaborating firms and within firms (e.g., manufacturing process control) iscrucial to enabling the agile, distributed style of manufacturing envisioned inthis section.computationthe computing resource for multidimensional optimization reflected in theasop scenario requires linkage of a wide variety of distributed machinesranging from small to large systems. this area is a severe test formetacomputing systems that support the synchronization and linkage ofheterogeneous computing devices. these distributed simulations must be linkedto the many databases involved in design and to the engineers making designdecisions. availability and performance requirements of distributed resourcesare likely much more predictable and stable than in the crisis managementcontext; nevertheless, ease of setting up operational systems acrossorganizational boundaries is a challenge to the success of distributed,collaborative projects.information managementin manufacturing, there is a very structured set of databases that needs tobe reliably interfaced with work flow, configuration management, and othertools. crisis management, by contrast, emphasizes good interfaces tounanticipated databases. manufacturing databases need to have highperformance capabilities when used to drive or support simulations. critical tothe successful linkage of many corporations with (logically if not physically)central information systems is the use of standards both in system (software)interfaces and in product data definitions. in the latter case, there could be someuseful interactions between information technology standards developmentactivities, such as virtual reality modeling language (vrml) for threedimensional object representation, and industrial production standardsdevelopment such as pdes/step (product data exchange using the standardfor the exchange of product model data; cstb, 1995b).another critical problem in asop is integrating legacy systems. it is noteconomically reasonable to assume that industry will rewrite from scratch thelarge number of existing programs (10,000 in the scenario above), nor willfirms rebuild all their databases to new information infrastructure standards.using these resources across a broadly deployed information infrastructurerequires advances in generalpurpose, easily configurable technology for softwareapplication needs for computing and communications46computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.integration (discussed in chapter 2), for example, to take existing codes inmultiple languages (e.g., fortran, c, lisp, excel) and integrate them into asingle, distributed system.usercentered systemsboth in crisis management and in manufacturing, critical decisions aremade from composite systems involving humans, computers, and informationsystems. in crisis management, the emphasis is on intuitive judgment makingwith incomplete information. manufacturing also requires good judgment fordecision makers, but it represents a more classic decision support context thatsupplies engineers with information targeted very precisely at welldefinedquestions. these decisions need to be made by collaborations of geographicallydistributed engineers. this implies a need for collaboratory systems that linkpeople and the information they need to make decisions.health care25computing and communications increasingly affect health care in manydifferent forms. among those discussed in the workshop series were directpatient care, medical research, development of new medical technologies, andmanagement of financial and other aspects of health services. health care willcontinue to be administered by a diverse collection of providers working in avery large number of geographical settings. the health care system in the futurelikely will be characterized by (1) integration of widespread databases; (2)digitization of most health care data modalities (e.g., xrays, magneticresonance imaging (mri)), allowing their transmission across networks; and (3)increased application of telemedicine. health care providers will need todiscover and access information from many sites in order to be able to puttogether a comprehensive description of a patient's medical history. althoughperhaps to a lesser extent than in crisis management, there are significantvariability and unpredictability in both the types of information that must beobtained (text records, handwritten notes, medical imagery) and their location.for example, an integrated health care information infrastructure will be able togive providers ready access to an accurate and detailed account of a patient'smedical history. networked access could compensate for the current, almostcomplete lack of access to patient medical records in some kinds of crises, suchas large natural disasters. at the same time, however, the infrastructure mustprotect the security and confidentiality of personal information.medical decision support systems are increasingly used to help providersidentify and evaluate different diagnostic workups and treatment plans. theability to easily obtain large sets of longitudinal patient records will greatlyfacilitate the ability to carry out meaningful comparative analysis for clinical careapplication needs for computing and communications47computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.and for health science and clinical research. medical researchers and health caresystem administrators need to link multiple patient databases to one another andto auxiliary databases used to define such items as hospital facilities andprocedures. data must be encoded in a reasonably uniform fashion usingstandard vocabularies being developedšin the face of great challenges inachieving consensus among diverse partiesšby the health care industry andmedical informatics communities with the national library of medicine.delays in formulating and agreeing on these standard vocabularies are part ofthe implementation context for health care computing and communications, andthey are indicative of the challenge of hammering out a consensus on standardsin most nationalscale application areas.networkingthe health carespecific applications of networking revolve aroundtelemedicine. telemedicine will enable remote consultation with individuals intheir homes (an advantage for both mobilityimpaired and rural patients) andwith remote specialists. telemedicine should support not only voice and videocommunications, but also realtime data from a range of medical sensors suchas heart monitors and blood chemistry analyzers. although the bandwidthrequirements associated with textual medical record information are modest,digitization of most health care modalities will lead to increasing bandwidthrequirements. the need to deliver the data to remote computing resources forprocessing and integrating in real time also adds complexity to the managementof the overall applicationšfor example, integrating, on one hand, therequirements of voice communications for low latency even at the expense ofreduced quality with, on the other hand, sensor data that may require lownoisecharacteristics to be useful. integrating realtime sensor datašincluding datafrom fielddeployed sensors, as in telemedicinešinto a continuously updatedpatient record is another potentially valuable application.in addition to bandwidth and service requirements, difficult security issuesarise because of the confidential nature of health care records and thepotentially large number of health care providers who have a need to knowabout particular aspects of a patient's medical record. strong guarantees ofprivacy, protection, and authentication will be required.26 new models ofprivacy and protection are needed to address emergency "needtoknow"circumstances, while providing for secure protection of privacy. the type of defacto protection afforded by the current health care system, which still is basedlargely on paper and disconnected computer systems, will diminish as medicalinformation is placed on networks and powerful information location andretrieval mechanisms become available.there are important similarities between the requirements associated withemergency health care and crisis management. network management must copewith nearrealtime constraints that arise in emergency situations. priorityapplication needs for computing and communications48computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.schemes must be structured to give priority to queries related to caring foremergency and critical care patients. during applications that are critical to life(such as image processing or expert assistance during surgery), uninterrupted,reliable service is vital. if the computational and network resources used forthese applications are being used at the same time for other applications,mechanisms must be in place to prevent the denial of service due to resourcelimits.computationthe ability to generate large databases of longitudinal clinical records,combined with substantial computational resources, will enable statisticallymeaningful comparative analysis for clinical care and health science research.this analysis could enable identification of medically distinct models andtemplates to describe diagnostic workups and care plans, thereby improving theefficiency and effectiveness of health care. secure methods are required,however, to disaggregate the information needed for such analysis from datathat could be used to identify individuals.routine testing is another potentially important computational demand.there are a number of highvolume, computationally intensive image screeningapplications (such as mammograms and pap smears) in which semiautomated,wellimplemented image processing methods could have a strong positiveimpact on efficiency and accuracy. although realtime processing is not criticalin this area, the huge volume of data to be processed imposes seriousrequirements for computational power. in addition, whereas some routinetesting examples would simply involve the analysis of individual acquisitions,more robust methods would also include database acquisition and manipulation.one potentially valuable example is the use of change detection algorithms inmammography, in which a current scan is normalized and registered to apreviously acquired scan of the patient; then the two are compared to highlightpotential differences. such an application would be enhanced further by theability to register a new scan automatically to a canonical (standard healthy)reference or atlas, including estimating the deformation of the scan to accountfor patient variability. by registering to an atlas, any detected anatomicalchanges could be interpreted further based on knowledge of the tissue typeassociated with the matched portion of the atlas. image processing is of coursejust one of many potential data inputs about patients that could benefit from thistype of semiautomation. computerbased patient status tracking, automaticrecord updating, and detection of changes and anomalies could be appliedacross a wide range of medical sensor inputs as well as clinical observations byhealth practitioners.significant computational challenges arise in the context of areasassociated with integrating robotics and image processing. the medicalcommunity increasingly seeks minimally invasive surgical procedures, with theexpected benefits of reduced complications, reduced trauma for the patients,and reduced length ofapplication needs for computing and communications49computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.hospital stays, leading to reduced costs and an increased quality of life forpatients. more effective use of minimally invasive procedures requiresimprovements in automatic or semiautomatic methods to localize anatomicalstructures for the surgeon and to facilitate presurgical planning. these methodsshould also support navigation of devices (by robot or surgeon) within the bodyand delivery of treatment and procedures in minimally invasive ways.one example of a significant computational challenge is enhanced realityvisualizations, in which segmented and labeled anatomical models, acquiredthrough threedimensional medical sensors (such as mri and computerizedtomography (ct)) are automatically registered with the patient and displayed tothe surgeon in a superimposed visualization showing internal structures directlyoverlaid on top of the patient, from the correct viewpoint. ideally, suchstructures would be tracked and their registration refined over time, to maintaina consistent visualization as the surgeon changes view, the patient moves, andthe patient's tissues deform. this problem is particularly relevant in endoscopicapplications, where the surgeon has a limited field of view and navigation andlocalization become critically important.a second challenge is the use of robotic devices to assist a surgeon.27 such devices include remote manipulation and tactile feedback devices forpalpation of internal tissue, systems to deliver surgical tools and procedures toinaccessible locations (e.g., in sinus surgery), and tools to improve the accuracyand reliability of surgical procedures. key computing requirements in theseapplications are realtime processing, highbandwidth data storage and retrieval,and computational and data reliability.the creation of new medical devices can benefit from more extensive useof computer simulation. simulations can reduce the time required to complete adesign as well as the time needed for testing. with good threedimensionalmodels, the designer can evaluate the effect of various device parameters in itsfuture physiological environment. for example, the ability to perform accuratesimulations of blood flow through the heart with an artificial valve would helpin the design of such devices. highperformance computing could allow theimplementation of a more accurate model of the heart and greatly reduce thetime it takes to perform such a complex simulation. computational chemistryand molecular modeling are being applied to drug design, with scope forcontinued improvement as greater computing resources become available.there are potentially important overlaps between the types ofcomputations that need to be carried out in the contexts of health care and crisismanagement. both application areas make significant use of sensor data, andboth will potentially benefit from different forms of data fusion. both areas canbenefit from increased use of simulation. because medical care is an importantfacet of crisis management, the ability to access patient records would also be ofpotential use to crisis managers in providing postdisaster medical care. if crisismanagers have information about the individuals affected by a disaster, anability to access theirapplication needs for computing and communications50computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.longitudinal medical records could be used to help prioritize relief efforts bydetermining which individuals might have preexisting conditions requiringspecial attention.information managementto coordinate patient care, it is necessary to be able to integrate inputsreliably from a subset of a very large number of heterogeneous databases. itshould be possible to construct longitudinal medical records recording the careand health of each individual, by discovering and integrating distributedinformation obtained from multiple health care providers. resource discovery isan important need because, in many cases, neither patients or providers are ableto recall or locate key past health care providers. there is also a need to locaterepresentative case histories for comparative purposes. although some of thesetasks can be performed in advance of emergencies, this is not always possible.in addition, integrating medical sensor data to update patient status adds furthercomplexity and realtime constraints. the realtime character of medicalemergencies (particularly if they occur in the largescale context of a disaster orother crisis) highlights the importance of efficiency of these resource discoveryand retrieval mechanisms.currently only a small fraction of electronically stored medical data is in aform that is readily usable in automated clinical analyses, such as studies oftreatment effectiveness. this situation will change as current practice improvesand the health care community moves from computer databases that are largelyoriented toward billing to databases aimed at recording information relevant toobserving and improving individuals' care and health. the ability to obtain andprocess large sets of longitudinal patient records would greatly facilitate theability to carry out meaningful comparative analysis both for clinical care andfor health science and clinical research. there is a range of architecturalapproaches available for aggregating data for use in health systems research andin epidemiological studies. at one extreme is world wide web technology withknowledge agents accessing the database, which itself is in distributed form.the other extreme involves the occasional collection of needed information to acentral aggregated database, which is then mined. (a centralized databaseincorporating medical records of everyone in the united states would beinfeasible with current technology,28 and so this should be understood as anextreme example, beyond current capabilities.) intermediate solutionscorrespond to generalizations of datacaching strategies familiar in parallel anddistributed computing (e.g., dividing the data and storing each part closest towhere it will be needed for access or processing).aggregating patient records for health research raises problems ofmaintaining the privacy of personal information, because it is difficult tosanitize patient records by removing all data that could disclose a patient'sidentity (includingapplication needs for computing and communications51computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.telephone numbers, addresses, birthdates, and others). these problems are mademore complex if the later identification of individuals by aggregating these datawith other information sources such as financial records is also to be deterred.these threats indicate opportunities for technological advances to help preventsuch compromises of privacy while facilitating legitimate research (iom, 1994).both health care and crisis management share a need to search aheterogeneous collection of databases. in the health care context, it usually isnot necessary to access databases that have unanticipated qualitative features.both emergency health care and crisis management share analogous securityand policy issues associated with the need to access crucial information rapidlywithout incurring significant securityrelated delays.usercentered systemsan integrated health care information infrastructure would be capable ofgiving providers ready access to an accurate and detailed account of a patient'smedical history. however, this information is useful only if the caregiver canreadily obtain and understand critical information, especially duringemergencies. significant, continued research efforts are needed to improve boththe caregiver's ease of using medical information systems and the ease withwhich caregivers may insert new clinical information electronically into patientrecords. these embody issues both within and outside computing andcommunications technology. examples of the former include user interfaces,natural language processing, and handwriting recognition, whereas broaderimplementation contexts might include incorporating informatics into medicalschool curricula.even with access to all available information, health care providers areoften faced withšand are trained foršmaking intuitive decisions whenavailable information is not complete. economic pressures in the health careindustry, however, have created a need for providers to justify the medicaltreatment they provide. this pressure is spurring research into the developmentof health care decision support. one important area that may underlie thedevelopment of decision support systems is the need for standard encodingprocesses to represent care plans and diseases. (this is not only a problem offinding technically optimal encoding schemes; as noted above, there are alsochallenges in reaching consensus among diverse parties about what names touse to distinguish various diseases, conditions, treatments, and the like.) thesetechniques should support the development of process representations, theautomatic detection of processes from database records, and identification ofsimilar process representations. this is analogous to crisis managers' need forsupport in making judgments, but with less unpredictability about the types ofdecisions that must be made, and therefore the ability to tailor rulebaseddecision support systems toward specific questions.health care would also benefit from increased deployment of remoteapplication needs for computing and communications52computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.collaboration technologies optimized for telemedicine, teleradiology, andperhaps telesurgery, along with remote sensing mechanisms to facilitate remotephysical examinations. effective use of these tools requires not only bandwidthand security, but also effective shared environments for communicating andworking collaboratively with information about patients and resources. there isa strong overlap between this application need and crisis management, wherethe expertise and equipment for health care delivery may be damaged or remotefrom the crisis location.notes1. the information infrastructure technology and applications component of the federal highperformance computing and communications initiative was formed in 1994 to promoteresearch and development of technologies for a broadly accessible national informationinfrastructure. the digital libraries initiative discussed in this chapter funds a range of projectsrelated to information storage, discovery, integration, and retrieval.2. the distinction between military and civilian crises does not necessarily extend to the mix ofparticipants in a crisis response. for example, military personnel are frequently called upon toprovide relief from natural disasters, and civilian relief organizations may be present in lowintensity military conflicts.3. sometimes mitigation is included by crisis managers as another stage of crisis management.mitigation involves efforts to lessen the impact disasters have on people and property.examples of mitigation include using zoning to keep homes away from floodplains, engineeringbridges to withstand earthquakes, and enforcing effective building codes to protect propertyfrom hurricanes. successful mitigation has the effect of reducing the impact of a crisis andperhaps keeping a situation from becoming a crisis. for a detailed case study, see fema (1993).4. for further discussion of information technology costs, training needs, and usage patterns incivilian crisis management organizations, see drabek (1991).5. for detailed discussions of the importance of deployment and feedback from actual users inthe design and development of information technologies, see landauer (1995) and cstb(1994a, pp. 181184).6. ncs is the primary agency responsible for communications functions in the federalresponse plan for disasters. the study was conducted as part of a review of needs for a newservice, the emergency response link (erlink), that the ncs is developing.7. see also the ncs's gets home page, http://164.117.147.223/~ncpp/html/gets.htm.8. civilian relief agencies sometimes call upon u.s. military units to deploy similar capabilities.9. available from ni/usr home page, http://niusr.org/vision.html.10. whereas in a single organization it might be possible to dictate standards forinteroperability, achieving agreement on standards is much more difficult when resources areowned and controlled by different organizations. this circumstance is increasingly common inmany nationalscale applications.11. an interesting note on the impact of the regulatory policy environment on scientificexperimentation is illustrated by the fact, reported by egill hauksson at workshop ii, that thecalifornia institute of technology (caltech) was unable to deploy an experimental earthquakenetwork for all of california, rather than just southern california, because the network servicedonor, pacific bell, was unable to carry communications between the two halves of the state onits own networks. it would have to hand off the communications to a longdistance carrier,which as hauksson noted, could have less direct incentive to support a public need in californiathan would a local firm such as pacific bell.application needs for computing and communications53computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.12. the consequences assessment tool uses a model to predict damage from high winds that isadapted from a nuclear blast effects model developed by the defense nuclear agency. theassessment tool is described in detail in linz and bryant (1994).13. for additional details, see noaa hpcc home page, http://hpcc1.hpcc.noaa.gov/hpcc.14. workshop participants observed that good judgments require not only access toinformation, but also a good general education on the part of judgment makers.15. see drabek (1991) for results of a detailed investigation of the relationship between trainingand information technology use in crisis management organizations.16. workshop series participant clifford lynch, office of the president, university ofcalifornia, made valuable contributions to this section. for a discussion of these research issuesin greater depth and breadth, see lynch and garciamolina (1995).17. the unpredictable timing of such demands highlights the potential benefit of continuousupdate of information in both gis and digital libraries, or at least the incorporation ofassociated information (metadata) about the currency and expected reliability of information.18. ordering and distribution of informationbased (intangible) products can be nearlysimultaneous, but the supporting accounting and inventory information, payment, and actualfunds transfer may lag. the resulting decoupling of the accounting and payment informationfrom the ordering and delivery of goods and services increases the credit risks associated with atransaction.19. each payment mechanism tends to work in a manner analogous to a physical mechanismsuch as credit cards, checks, or cash. developing and deploying interoperable, seamless supportfor multiple payment mechanisms at an economically feasible cost is a challenge with bothinstitutional aspects (e.g., negotiating contractual frameworks) and requirements for research.20. incorporation of video and sound into web pages increases the richness of the contentprovided, but also increases the bandwidth required for access.21. networks among atms involve links with known and stable locations and relativelypredictable load patterns (unlike the networks needed for crisis management).22. for a more complete overview, see cstb (1995b).23. this illustrates what might be called ''amdahl's law for practical hpcc." for a classicdiscussion of key principles, see amdahl (1967).24. for a detailed description, see syracuse university and multidisciplinary analysis anddesign industrial consortium team 2 (1995).25. workshop series participant joel saltz, of the university of maryland, made valuablecontributions to this section. for a discussion of these research issues in greater depth andbreadth, see davis et al. (1995).26. for a discussion of medical record privacy issues in a networked environment, see iom(1994).27. robots may find application in other elements of health care, such as handling andinspection of clinical or research specimens.28. the current research frontier is petabytesized databases. an estimate from the nsfworkshop on high performance computing and communications and health care (davis etal., 1995) postulated that nationwide adoption of computerized patient records over the nextdecade will yield a full database size of 10 terabytes, which is well beyond current databasemanagement capabilities. this corresponds to the equivalent of 100 text pages for each of 100million patients.application needs for computing and communications54computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.2technology: research problems motivatedby application needsintroductionchapter 1 identifies opportunities to meet significant needs of crisismanagement and other nationalscale application areas through advances incomputing and communications technology. this chapter examines thefundamental research and development challenges those opportunities imply.few of these challenges are entirely new; researchers and technologists havebeen working for years to advance computing and communications theory andtechnology, investigating problems ranging from maximizing the power ofcomputation and communications capabilities to designing informationapplications that use those capabilities. what this discussion offers is acontemporary calibration, with implications for possible focusing of ongoing orfuture efforts, based on the inputs of technologists at the three workshops aswell as a diverse sampling of other resources.this chapter surveys the range of research directions motivated byopportunities for more effective use of technology in crisis management andother domains, following the same framework of technology areasšnetworking, computation, information management, and usercentered systemsšdeveloped in chapter 1. some of the directions address relatively targetedapproaches toward making immediate progress in overcoming barriers toeffective use of computing and communications, such as technologies to displayinformation more naturally to people or to translate information more easilyfrom one format to another. others aim at gaining an understanding of coherentarchitectures and services that, when broadly deployed, could lead eventually toeliminating these barrierstechnology: research problems motivated by application needs55computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.in a less ad hoc, more comprehensive fashion. research on modeling thebehavior of software systems composed from heterogeneous parts, for example,fits this category.networking: the need for adaptivitybecause of inherently unpredictable conditions, the communicationssupport needed in a crisis must be adaptable; the steering committeecharacterizes the required capability as "adaptivity." adaptivity involvesmaking the best use of the available network capacity (including settingpriorities for traffic according to needs and blocking out lowerpriority traffic),as well as adding capacity by deploying and integrating new facilities. it alsomust support different kinds of services with fundamentally different technicaldemands, and to do so efficiently requires adaptivity. this section addressesspecific areas for research in adaptive networks and describes the implicationsof a requirement for adaptivity; the importance of adaptivity at levels ofinformation infrastructure above the network is discussed in other sections ofthis chapter.box 2.1 provides a sampling of networking research priorities discussed inthe workshops. although problems of networking that arise in nationalscaleapplications are not entirely new, they require rethinking and redefinitionbecause the boundaries of the problem domains are changing. three issues thatinfluence the scope of networking research problems are (1) scale, (2)interoperability, and (3) usability. scale. highperformance networking is often thought of in terms of speedand bandwidth. speed is limited, of course, by the speed of light in thetransmission medium (copper, fiber, or air), and individual data bits cannotmove over networks any faster. however, the overall speed of networkscan be increased by raising the bandwidth (making the pipes wider and/orusing more pipes in parallel) and reducing delays at bottlenecks in thenetwork. highspeed networks (which include both highbandwidthconduits or "pipes" and highspeed switching and routing) allow largerstreams of data to traverse the network from point a to point b in a givenamount of time. this makes possible the transmission of longer individualmessages such as data files, wider signals (such as fullmotion video), andgreater numbers of messages (such as data integrated from large numbersof distributed sensors) over a given path at the same time. researchchallenges related to the operation of highspeed networks include highspeed switching, buffering, error control, and similar needs; these wereinvestigated with significant progress in the defense advanced researchproject agency's (darpa's) gigabit network testbeds.speed and bandwidth are not the only performance challenges related toscale; nationalscale applications must also scale in size. the number ofinformation sources involved in applications may meet or even far exceedthe size of thetechnology: research problems motivated by application needs56computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.nation's or world's population. in theory, every information producer maybe an information consumer and vice versa. consequently, there is theneed not only to reduce the amount of time needed for quantities of bits tobe moved but, even at the limits of technology in increasing that speed, totransport more bits to more places. the set of people, workstations,databases, and computation platforms on networks is growing rapidly.sensors are a potential source of even faster growth in the number of endpoints; as crisis management applications illustrate, networks may have toroute bits to and from environmental sensors, seismometers, structuralsensors on buildings and bridges, security cameras in stores and automatedteller machines, and perhaps relief workers wearing cameras and othersensors on their clothes, rendering them what vinton cerf, of mcitelecommunications corporation, called "mobile multimodal sensor nets."medical sensors distributed at people's homes, doctor's offices, crisis aidstations, and other locations may enable health care delivery in a new,more physically distributed fashion, but only if networks can manage theincreased number of end points. inbox 2.1 selected networking researchpriorities suggested by workshopparticipants daniel duchamp, columbia university: priority highbandwidth and/or frequent upstream communication bandwidth allocation that permits sudden, very large reallocations selforganizing routing structures end point identification by attributes rather than just by name ease of use elimination of the concept of administratorrajeev jain, university of california, los angeles: portable highbandwidth radio modems that interface with portablecomputers and can be powered off the computer batteryšthese should beadaptable and interoperable with different frequency bands, channelconditions, and capacity requirements. peertopeer distributed network protocols for setting up networks in theabsence of wireline backbones. bandwidthefficient transmissions that allow increased capacityšforexample, in many crises like the northridge earthquake or hurricaneandrew, even people in the same neighborhood were cut off due to thebreakdown of telephone service. a portable bandwidthefficient batteryoperated peertopeer network technology would allow informationsystems to be set up to provide important support to communities in acrisis.technology: research problems motivated by application needs57computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.response, the communications infrastructure must be prepared to transportorders of magnitude more data and information and to handle orders ofmagnitude more separate addresses.a particular case, such as a response to a single disaster, may notinvolve linking simultaneously to millions or billions of end points, butbecause the specific points that will be linked are not known in advance,the networking infrastructure must be able to accommodate the fullnumber of names and addresses. the numbering plan of the publicswitched telecommunications network provides for this capability forpointtopoint (voice circuit) calling under normal circumstances. in thebroader context of all data, voice, and video communications, the internet'sdistributed domain name servers manage the numerical addresses thatidentify end points and names associated with those addresses. theexplosive growth in internet usage has motivated a change in the standard,internet protocol version 6, to accommodate more addresses.1 interoperability. the need for successfully communicating acrossboundaries in heterogeneous, longlived, and evolving environmentscannot be ignored. in crisis management, voice communications arenecessary but not sufficient; response managers and field workers must beable to mobilize data inputs and more fully developed information(knowledge) from an enormous breadth of existing sourcesšsome of themyears oldšin many forms. telemedicine similarly requires a mix ofcommunications modes, although not always over as unpredictable aninfrastructure as crises present. interoperation is more than merely passingwaveforms and bits successfully; interoperation among the supportingservices for communications, such as security and access priority, is highlycomplex when heterogeneous networks interconnect. usability. the information and communications infrastructure is there toprovide support to people, not just computers. in nationalscaleapplications, nonexperts are increasingly important users ofcommunications, making usability a crucial issue. what is needed are waysfor people to use technology more effectively to communicate, not onlywith computers and other information sources and tools, but also withother people. collaboration between people includes many modes oftelecommunication: speech, video, passing data files to one another,sharing a consensus view of a document or a map. in crises, for example,the ability to manage the flow of communications among the people andmachines involved is central to the enterprise and cannot be reservedsolely to highly specialized technicians. users of networks must be able toconfigure their communications to fit their organizational demands, not thereverse. this requirement implies far more than easytouse humancomputer interfaces for network management software; the network itselfmust be able to adapt actively to its users and whatever information orother resources they need to draw upon.for networks to be adaptive, they must be able to function during orrecover quickly from unusual and challenging circumstances. the unpredictabledamagetechnology: research problems motivated by application needs58computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.and disruption caused by a crisis constitute challenging circumstances for whichno specific preparations can be made. unpredicted changes in a financial ormedical network, such as movement of customers or a changing businessalliance among insurers and hospitals that exchange clinical records, may alsorequire adaptive response. mobilityšof users, devices, information, and otherobjects in a networkšis a particular kind of challenge that is relevant not onlyto crisis response, but also to electronic commerce with portable devices,telemedicine, and wireless inventory systems in manufacturing, among others.whenever the nodes, links, inputs, and outputs on a network move, that networkmust be able to adapt to change.randy katz, of the university of california, berkeley, has illustrated thedemands for adaptivity of wireless (or, more generally, tetherless) networks formobile computing in the face of highly diverse requirements with the exampleof a multimedia terminal for a firefighter (katz, 1995). the device might beused in many ways: to access maps and plan routes to a fire; examine buildingblueprints for tactical planning; access databases locating local fire hydrants andnearby fire hazards such as chemical plants; communicate with and display thelocations of other fire and rescue teams; and provide a location signal to acentral headquarters so the firefighting team can be tracked for broaderoperational planning. all of the data cannot be stored on the device (especiallybecause some data may have to be updated during the operation), so realtimeaccess to centrally located data is necessary. the applications require differentdata rates and different tradeoffs between low delay (latency) and freedomfrom transmission errors. voice communications, for example, must be realtime but can tolerate noisy signals; users can wait a few seconds to receive amap or blueprint, but errors may make it unusable. some applications, such asvoice conversation, require symmetrical bandwidth; others, such as data accessand location signaling, are primarily one way (the former toward the mobiledevice, the latter away from it).research issues in network adaptivity fall into a number of categories,discussed in this section: selforganizing networks, network management,security, resource discovery, and virtual subnetworks. for networks to beadaptive, they must be easily reconfigurable either to meet differentrequirements from those for which they were originally deployed or to workaround partial failures. in many cases of partial failures, selfconfiguringnetworks might discover, analyze, work around, and perhaps report failures,thereby achieving some degree of fault tolerance in the network. over shortperiods, such as the hours after a disaster strikes, an adaptive network shouldrestore services in a way that best utilizes the surviving infrastructure, enablesadditional resources to be integrated as they become available, and givespriority to the most pressing emergency needs. daniel duchamp, of columbiauniversity, observed, "especially if the crisis is some form of disaster, theremay be little or no infrastructure (e.g., electrical and telephone lines, cellularbase stations) for twoway communication in the vicinity of an action site. thatwhich exists may be overloaded. there are twotechnology: research problems motivated by application needs59computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.approaches to such a problem: add capacity and/or shed load. adding capacityis desirable but may be difficult; therefore, a mechanism for load shedding isdesirable. some notion of priority is typically a prerequisite for load shedding."networks can be adaptive not only to sharp discontinuities such as crises,but also to rapid, continuous evolution over a longer time scale, one appropriateto the pattern of growth of new services and industries in electronic commerceor digital libraries. the internet's ability to adapt to and integrate newtechnologies, such as frame relay, asynchronous transfer mode (atm), and newwireless data services, among many others, is one example.selforganisationselforganizing networks facilitate adaptation when the physicalconfiguration or the requirements for network resources have changed. danielduchamp cast the problem in terms of an alternative to static operation:most industry efforts are targeted to the commercial market and so are focusedon providing a communications infrastructure whose underlying organizationis static (e.g., certain sites are routers and certain sites are hosts, always).statically organized systems ease the tasks of providing security and handlingaccounting/billing. most communication systems are also preoptimized toaccommodate certain traffic patterns; the patterns are in large part predictableas a function of intra and interbusiness organization. it may be difficult orimpossible to establish and maintain a static routing and/or connectionestablishment structure, because (1) hosts may move relative to each other, and(2) hosts, communication links, or the propagation environment may beinherently unstable. therefore, a dynamically "selforganizing" routing and/orconnection establishment structure is desirable.crisis management provides a compelling case for the need of networks tobe selforganizing in order to create rapidly an infrastructure that supportscommunication and information sharing among workers and managersoperating in the field. police, fire, citizen's band, and amateur radiocommunications are commonly available in crises and could be used to set up abroadcast network, but they provide little support to manage peertopeercommunications and make efficient use of the available spectrum. portable,bandwidthefficient peertopeer network technologies would allow informationsystems to be set up to support communications for relief workers. the issues ofhardware development, peertopeer networking, and multimedia support arenot limited to crisis management; they may be equally important to such fieldsas medicine and manufacturing (e.g., in networking of people, computers, andmachine tools within a factory). thus, research and development on selforganizing networks may be useful in the latter fields as well.rajeev jain, of the university of california, los angeles, suggested twomain deficiencies in terms of communications or networking technologies in atechnology: research problems motivated by application needs60computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.situation where relief officials arrive carrying laptop computers: (1) portablecomputing technology is not as well integrated with wireless communicationstechnology as it should be, and (2) wireless communications systems still oftenrely on a wireline backbone for networking.2 these factors imply that portablecomputers cannot currently be used to set up a peertopeer network if thebackbone fails; radio modem technology has not yet advanced to a point whereit can provide an alternative.3 in mobile situations, people using portablecomputers need access to a wireline infrastructure to set up data links withanother computer even if they are in close proximity. in addition, portablecellular phones cannot communicate with each other if the infrastructure breaksdown. jain concluded that both of these problems must be solved by developingtechnologies that better integrate portable computers with radio modems andallow peertopeer networks to be set up without wireline backbones, by usingbandwidthefficient transmission technologies.peertopeer networking techniques involve network configuration,multiple access protocols, and bandwidth management protocols. betterprotocols need to be developed in conjunction with an understanding of thewireless communications technology so that bandwidth is utilized efficientlyand the overhead of selforganization does not reduce the usable bandwidthdrastically (the current situation in packet radio networks). bandwidth is at apremium because of the large volume of information required in a crisis andbecause, although data and voice networks can be deployed using portablewireless technology, higher and/or more flexibly usable bandwidths are neededto support video communication. for example, images can convey vitalinformation much more quickly than words, which can be important in crises orremote telemedicine. if paramedics need to communicate a diagnostic image ofa patient (such as an electrocardiogram or xray) to a physician at a remote siteand receive medical instructions, the amount of data that must be sent exceedsthe capabilities of most wireless data communications technologies for portablecomputers. technologies are now emerging that support data transmission ratesin the tens of kilobits per second, which is sufficient for still pictures but not forfullmotion video of more than minimal quality. a somewhat higher bandwidthcapability could support a choice between moderatequality fullmotion videoand highquality images at a relatively low image or frame rate (resulting injerky apparent motion). another example relates to the usefulness ofbroadcasting certain kinds of data, such as fullmotion video images of disasterconditions from a helicopter to workers in the field; traffic helicopters of localtelevision stations often serve this function. however, if terrestrial broadcastcapabilities are disabled, it could be valuable to use a deployable peertopeernetwork capability to disseminate such pictures to many recipients, potentiallyby using multicast technology.the statement of james beauchamp, of the u.s. commander in chief,pacific command, quoted in chapter 1 underscored the low probability that allindividuals or organizations involved in a crisis response will have interoperabletechnology: research problems motivated by application needs61computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.radios (voice or data), especially in an international operation or one in whichgroups are brought together who have not trained or planned together before.selforganizing networks that allowed smooth interoperation would be veryuseful in civilian and military crisis management and thus could have a highpayoff for research. the lack of such technologies may be due partly to theabsence of commercial applications requiring rapid configuration of wirelesscommunications among many diverse technologies.one purpose of the department of defense's (dod's) joint warriorinteroperability demonstrations (jwids; discussed in chapter 1) is to test newtechnologies for bridging gaps in interoperability of communicationsequipment. the speakeasy technology developed at rome laboratory, forexample, is scheduled to be tested in an operational exercise in the summer of1996 during jwid '96.4 speakeasy is an effort sponsored by darpa and thenational security agency to produce a radio that can emulate a multitude ofexisting commercial and military radios by implementing previously hardwarebased waveformgeneration technologies in software. such a device should beable to act as if it were a highfrequency (hf) longrange radio, a very highfrequency (vhf) airtoground radio, or a civilian police radio. managing apeertopeer network of radios that use different protocols, some of which canemulate more than one protocol, is a complex problem for network research thatcould yield valuable results in the relatively near term.network managementnetwork management helps deliver communications capacity to whoevermay need it when it is needed. this may range from more effective sharing ofnetwork resources to priority overrides (blocking all other users) as needed.network management schemes must support making decisions and settingpriorities; it is possible that not all needs will be met if there simply are notenough resources, but allocations must be made on some basis of priority andneed. experimentation is necessary to understand better the architecturalrequirements with respect to such aspects as reliability, availability, security,throughput, connectivity, and configurability.a network manager responding to a crisis must determine the state of thecommunications infrastructure. this means identifying what is working, what isnot, and what is needed and can be provided, by taking into accountcharacteristics of the network that can and should be maintained. for example,the existing infrastructure may provide some level of security. then it must bedetermined whether it is both feasible and reasonable to continue to provide thatlevel of security. fault tolerance and priorities for activities are othercharacteristics of the network that must similarly be resolved.in addition to network management tools to assess an existing situation,tools are needed to incorporate new requirements into the existing structure. fortechnology: research problems motivated by application needs62computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.example, there may be great variability in the direction of data flow into and outof an area in which a crisis has occurredšfor example, between commandposts and field units. during some phases, remote units may be used for datacollection to be transmitted to centralized or command facilities that in turn willneed only lower communication bandwidth to the mobile units.adaptive network management can help increase the capability of thenetwork elements, for example, by making the communications andcomputation able to run efficiently with respect to power consumption. randykatz has observed that wireless communication removes only one of the tetherson mobile computing; the other tether is electrical power (katz, 1995).advances in lightweight, longlived battery technology and hardwaretechnologies, such as lowpower circuits, displays, and storage devices, wouldimprove the performance of portable computers in a mobile setting. apossibility that is related directly to network management is the development ofschemes that adapt to specific kinds of communications needs and incorporatebroadcast and asymmetric communications to reduce the number and length ofpowerconsuming transmissions by portable devices. for example, katzobserves that if a mobile device's request for a particular piece of informationneed not be satisfied immediately, the request can be transmitted at low powerand low bandwidth. the response can be combined with those to other mobiledevices, which are broadcast periodically to all of the units together at highpower and bandwidth from the base stations. if a particular piece of informationsuch as weather data is requested repeatedly by many users, it can berebroadcast frequently to eliminate the need for remote units to transmit requests.priority policy is a critical issue in many applications; the need for rapiddeployment and change in crisis management illustrates the issue especiallyclearly. priority policy is the set of procedures and management principlesimplemented in a network to allocate resources (e.g., access to scarcecommunications bandwidth) according to the priority of various demands forthose resources. priority policy may be a function of the situation, the role ofeach participant, their locations, the content being transmitted, and many otherfactors. the dynamic nature of some crises may be reflected in the need fordynamic reassignment of such priorities. the problem is that one may have tochange the determination of which applications (such as lifecritical medicalsensor data streams) or users (such as search and rescue workers) have priorityin using the communications facilities. borrowing resources in a crisis mayrequire reconfiguring communications facilities designed for another use, suchas local police radio. a collection of priority management issues must beaddressed: who has the authority to make a determination about priorities? how are priorities determined? how are priorities configured? configuration needs to be secure, but alsotechnology: research problems motivated by application needs63computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.user friendly, because the people performing it may not be network orcommunications experts. how are such priorities provided by the network and related resources? how will the network perform under the priority conditions assigned?the last is a particularly difficult problem for network management.michael zyda, of the naval postgraduate school, identified predictive modelingof network latency as a difficult research challenge for distributed virtualenvironments, for which realistic simulation experiences set relatively strictlimits on the latency that can be tolerated, implying a need for giving priority tothose data streams.one suggestion arising in the workshops was a priority server within aclientserver architecture to centralize and manage evolving priorities. thisapproach might allow for the development of a multilevel availability policyanalogous to a multilevel security policy. a dynamically configurablemechanism for allocating scarce bandwidth on a priority basis could enablecreation of the ''emergency lane" over the communications infrastructure thatcrisis managers at the workshops identified as a highpriority need. if suchmechanisms were available they could be of great use in managing priorityallocation in other domains such as medicine, manufacturing, and banking. insituations that are not crises, however, one might be able to plan ahead forchanges in priority, and it is likely that network and communications expertisemight be more readily available.victor frost, of the university of kansas, discussed the challenges ofmeeting diverse priority configuration within a network that integrates voicewith other services:some current networks use multilevel precedence (mlp) to ensure thatimportant users have priority access to communications services. the generalidea for mlplike capabilities is that during normal operations the networksatisfies the performance requirements of all users, but when the network isstressed, higherpriority users get preferential treatment. for voice networks,mlp decisions are straightforward: accept, deny, or cut off connections.however, as crisis management starts to use integrated services (i.e., voice,data, video, and multimedia), mlp decisions become more complex. forexample, in today's systems an option is to drop lowprecedence calls. in amultimedia network, not all calls are created equal. for example, dropping alowprecedence voice call would not necessarily allow for the connection of ahighprecedence data call. mlplike services should be available in futureintegrated networks. open issues include initially allocating and thenreallocating network resources in response to rapidly changing conditions in anmlp context. in addition, the infrastructure must be capable of transmittingmlplike control information (signaling) that can be processed along withother network signaling messages. there is a need to develop mlplikeservices that match the characteristics of integrated networks.technology: research problems motivated by application needs64computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.an ability to configure priorities, however, will require a much betterunderstanding of what users actually need. victor frost also observed,unfortunately, defining applicationlevel performance objectives may beelusive. for example, users would always want to download a map or imageinstantaneously, but would they accept a [slower] response? a 10minuteresponse time would clearly be unacceptable for users directly connected to ahighspeed network; but is this still true for users connected via performancedisadvantaged wireless links? . . . performancerelated deficiencies of currentlyavailable computing and communications capabilities are difficult to definewithout userlevel performance specifications.securitysecurity is essential to nationalscale applications such as health care,manufacturing, and electronic commerce. it also is important to crisismanagement, particularly in situations where an active adversary is involved orsensitive information must be communicated. many traditional ideas of networksecurity must be reconsidered for these applications in light of the greater scaleand diversity of the infrastructure and the increased role of nonexperts.to begin with, the nature of security policies may evolve. longertermresearch on new models of composability of policies will be needed as peoplebegin to communicate more frequently with other people whom they do notknow and may not fully trust. on a more shortterm basis, new security modelsare needed to handle the new degree of mobility of users and possiblyorganizations. the usability or user acceptability of security mechanisms willassume new importance, especially those that inconvenience legitimate use tooseverely. new perspectives may be required on setting the boundaries ofsecurity policies not based on physical location.composability of security policiesas organizations and individuals form and reform themselves into newand different groupings, their security policies must also be adapted to thechanges. three reorganization modelsšpartitioning, subsumption, andfederationšmay be used, and each may engender changes in security policies.the following are simplistic descriptions, but they capture the general nature ofchanges that may occur. partitioning involves a divergence of activity whereunanimity or cooperation previously existed. in terms of security, partitioningdoes not appear to introduce a new paradigm or new problems. in contrast,subsumption and federation both involve some form of merging or aligning ofactivities and policies. subsumption implies that one entity plays a primary role,while at least one other assumes a secondary role. federation, on the other hand,implies an equal partnering or relationship. both subsumption and federationmay require thattechnology: research problems motivated by application needs65computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.security policies be realigned, while perhaps seeking ways to continue tosupport previous policies and mechanisms. both models of joining may befound in crisis management, as local emergency services agencies provide radionetworks that other organizations brought in from outside must interact withand/or assume control over.if policies and mechanisms are to be subsumed, the problems for securitybecome significantly more difficult to address than in the past. in this case, if aunified toplevel policy is to be enforced that is a composite of several others,interfaces among themšor, more abstractly, definitions of the policies,abstraction, and modularityšwill be necessary to allow for exchange incontrolled and wellknown ways. it is only through such formal definitions thatthe composition of such activities can be sufficiently trustable to allow for theprovision of a toplevel composite of security policies and mechanisms.a perhaps even more difficult problem is peerlevel interaction within afederated model, in which neither domain's security policy takes clearprecedence over the other. such interaction will become more common asalliances are formed among organizations and individuals who are widelydistributed. as virtual networks are set up in conjunction with temporaryrelationships, there is a continued need for security during any coordinatedactivities within the affiliation. thus, the security mechanisms required by eachparticipant must collaborate in ways that do not impede the coordination of theiractivities. since there is no domination model in this case, coordination andcompromise may be necessary. again, these problems will be helped byresearch that provides better modularity and abstraction in order to formalizethe relationships and interactions.mobility of access rightsin many, perhaps all, of the nationalscale applications, users can beexpected to move from one security policy domain or sphere to another andhave a need to continue to function (e.g., carrying a portable computer from thewireless network environment of one's employer into that of a customer,supplier, or competitor). in some cases, the mobile user's primary objective willbe to interact with the new local environment; in others, it will be to continueactivities within the original or home domain. most likely, the activities willinvolve some of both. in the first case, the user can be given a completely newidentity with accruing security privileges in the new environment; alternatively,an agreement can be reached between the two domains, such that the new onetrusts the old one to some degree, using that as the basis for any policyconstraints on the user. this requires reciprocal agreements of trust between anyrelevant security domains. it is even possible to envisage cascading such trust,in either a hierarchical trust model or something less structured in which a meshof trust develops with time,technology: research problems motivated by application needs66computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.supporting transitive trust among domains. there is significant work to be donein such an area.mobile users who want to connect back to their home domain from aforeign one also have several alternatives. it is likely that the local domain willrequire some form of authentication and authorization of users. the remotedomain might either accept that authentication, based on some form of mutualtrust between the domains, or require separate, direct authentication andauthorization from the user. in addition, such remote access may raise problemsof exposure of activities, such as lack of privacy, greater potential formasquerading and spoofing, or denial of service, because all communicationmust now be transported through environments that may not be trusted.if the user is trying to merge activities in the two environments, it is likelythat a merged authentication and authorization policy will be the only rationalsolution. it is certainly imaginable that such a merged or federated policy mightstill be implemented using different security mechanisms in each domain, aslong as the interfaces to the domains are explicit so that a composite can becreated.usability of security mechanismsusability in a security context means not only that both system andnetwork security must be easy for the end users (such as rescue workers or bankcustomers and officers) to use, but also that the exercise of translation frompolicy into configuration must be achievable by people in the field who aredefining the policies and who may not be security experts. if security systemscannot be used and configured easily by people whose main objectives arecompleting other tasks, the mechanisms will be subverted. according to danielduchamp, "two obvious points . . . need considerable work. first, for disastersespecially, technology should intrude as little as possible on the consciousnessof field workers. second, all goals should be achieved without the need forsystem administrators." users often do not place a high priority on the securityof the resources they are using, because the threats do not weigh heavily againstthe objective of achieving some other goal. thus, the cost (includinginconvenience) to these users must be commensurate with the perceived level ofutility. as richard entlich, of the institute for defense analyses, observed,"creating a realistic way of providing security at each node involves not onlytechnical issues, but a change in operational procedures and user attitudes."ideally, technological designs and approaches should reinforce those neededchanges on the part of users.unfortunately, the problems of formulating security policy are even moredifficult to address with computational and communications facilities. policyformation, especially when it involves merging several different securitydomains, is extremely complex. it must be based on the tasks to be achieved,the probability of subversion if security policy constraints are too obstructive, andtechnology: research problems motivated by application needs67computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.the capabilities of the mechanisms available, especially when merging ofseparate resources is necessary.discovery of resourcescrisis management highlights the need for rapid resource discovery.resources may be electronic, such as information or services, or they may bemore tangible, such as computers, printers, and wires used in networks. first,one must determine what resources are needed. then, perhaps with help frominformation networks, one might be able to discover which resources are localand, if those are inadequate, whether some remote resources may be able toaddress an otherwise insoluble problem. an example of this latter situationwould be finding an expert on an unusual bacterial infection that appears tohave broken out in a given location.in crises, some of the tools mentioned above for network management andreorganization in the face of partial failures may also help to identify whichlocal computing, communications, and networking resources are functional. ifhighperformance computing is necessary for a given task, such as additional ormore detailed weather forecasting or geological (earthquake) modeling,discovering computing and network facilities that are remote and accessible viaadequately capable network connections might be invaluable.virtual subnetworksanother architectural requirement common to several of the applicationareas is the ability to create virtual subnetworks. the virtual "subnet" featureallows communities to be created for special purposes. for example, inmanufacturing, the creation of a virtual subnet for a company and itssubcontractors might simplify the building of applications by providing a sharedengineering design tool. it would allow a global or national corporation tooperate as though it had a private subnet. it might provide similar features forany community, such as a network of hospitals that has a need to exchangepatient records.a virtual subnet will appear to applications and supporting software as ifcommunications are happening on a separate network that actually is configuredwithin a larger one. in essence, the virtual subnet capability allows a policy oractivity boundary concept to be made evident in the network model as a subnet.at present, virtual subnets are generally used to reflect administrative domainsin which a single consistent set of usage and access policies is enforced.the possibility of defining a subnet for crisis management in terms ofsecurity and priority access has already been suggested. another potentiallyuseful way to define a boundary around a subnet would be to control the flow ofinformation passing into that subnet by using prioritybased filteringtechnology: research problems motivated by application needs68computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.mechanisms. this would be done to reserve scarce bandwidth and storagewithin the subnet for only the most valuable information.in order to make virtual subnets useful, there must be automated ways ofcreating them within the internet or the broader national or global informationinfrastructure. this implies understanding the policies to be enforced on such asubnet with respect to, for example, usage and security, and being able to bothrecognize and requisition resources to create and manage subnets. it may meanprovision of various services within the network in such a way that thoseservices can be provided effectively to subnets. examples of these might betrusted encryption services, firewalls, protocol conversion gateways, and others.a virtual subnet must have all the characteristics of a physical subnet, whileallowing its members to be widely distributed physically.5by providing application or userlevel community boundary models downinto the network, one might create a more robust, survivable environment inwhich to build applications. both advances in technology development andmore fundamental research on architectural models for subnets are needed toautomate support for creating such subnets in real time and on a significantlylarger scale than is currently supported.computation: distributed computingthe networked computational and mass storage resources needed fornationalscale application areas are necessarily heterogeneous andgeographically distributed. a geographically remote, accessible metacomputingresource, as envisioned in the crisis 2005 scenario in chapter 1, impliesnetworkbased adaptive links among available people (using portable computersand communications, such as personal digital assistants) to largescalecomputation on highperformance computing platforms. the networkconnecting these computing and storage resources is the enabling technologyfor what might be termed a networkintensive style of computation. allensears, of darpa, summarized this idea as "the network is the computer"; thatis, computation to address a user's problem may routinely take place out on anetwork, somewhere other than the user's location.crisis management is a good example of a networkintensive application.people responding to crises could benefit from largerscale mass storage andhigher computation rates than are typically available in the field, for example, togain the benefits of highperformance simulation performed away from thecrisis location.6 the technical implication of networkintensive computing forcrisis management is not merely a massive computational capability, but ratheran appropriately balanced computing and communications hierarchy. thiswould integrate computing, storage, and data communications across a scalefrom lightweight portable computers in the field to remote, geographicallydistributed highperformance computation and mass storage systems fordatabase and simulationtechnology: research problems motivated by application needs69computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.support. research in many areas, such as mobility and coordination of resourcesand management of distributed computing, is needed to achieve this balancedhierarchy.modeling and simulationhighperformance computation may be used to simulate complex systems,both natural and manmade, for many applications. networks can make highperformance computation resources remotely accessible, enabling sharing ofexpensive resources among users throughout the nation. applications ofmodeling and simulation to crisis management include the prediction of severestorms, flooding, wildfire evolution, toxic liquid and gas dispersion, structuraldamage, and other phenomena. as discussed in chapter 1, higherqualitypredictions than are available today could save lives and reduce the cost ofresponse significantly.grand challenge activities under the high performance computing andcommunications initiative (hpcci) have been a factor in advancing the state ofthe art of modeling and simulation (cstb, 1995a; ostp, 1993, 1994a; nstc,1995). the speed of current highperformance simulation for many differentapplications, however, continues to need improvement. lee holcomb, of thenational aeronautics and space administration (nasa), observed, forexample, that it is currently infeasible for longterm climate change models toinvolve the coupling of ocean and atmospheric effects, because of inadequatespeed of the models for simulating atmospheric effects (which change muchmore rapidly than ocean effects and therefore must be modeled accordingly). inaddition, whereas fluid dynamics models are able to produce very nice picturesof airflow around aircraft wings and to calculate lift, they are not able to modeldrag accurately, which is the other basic flight characteristic required in aircraftdesign. holcomb summarized, "we have requirements that go well beyond thecurrent goals of the high performance computing program."the urgency of crises imposes a requirement that may pertain more strictlyin crisis management than in other applications such as computational science:the ability to run simulations at varying scales of resolution is crucial to beingable to make appropriate tradeoffs between the accuracy of the prediction andthe affordability and speed of the response. kelvin droegemeier, of theuniversity of oklahoma, described work on severe thunderstorm modeling atthe university's center for the analysis and prediction of storms (caps),including field trials in 1995 that demonstrated the ability to generate anddeliver highperformance modeling results within a time frame useful to crisismanagers. for areas within 30 km of a doppler radar station, microscalepredictions have been made at a 1km scale and can predict rapidly developingevents, such as microbursts, heavy rain, hail, and electrical buildup, on 10 to30minute time scales. at scales of 1 to more than 10 km, the emergence andintensity of new thunderstorms, cloud ceiling, and visibility have been predictedup to two hours in advance, andtechnology: research problems motivated by application needs70computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.the evolution (e.g., movement, change in intensity) of existing storms has beenforecast three to six hours in advance. rescaling the model thus allows greaterdetail to be generated where it is most needed, in response to demands from thefield.7as droegemeier noted, time is critical for results to be of operational value:these forecasts are only good for about six hours. this means you have tocollect the observational data, primarily from doppler radars; retrieve fromthese data various quantities that cannot be observed directly; generate aninitial state for the model; run the model; generate the forecast products; andmake forecast decisions in a time frame of 30 to 60 minutes because otherwise,you have eaten up a good portion of your forecast period. it is a very timelyproblem that absolutely requires highperformance computing andcommunications. if you can't predict the weather significantly faster than itevolves, then the prediction is obviously useless.when high performance is required, adding complexity at various scales ofprediction may not be worth the cost in time or computer resource usage. forexample, the caps storm model could predict not only the presence of hail, butthe average size of the hailstones; however, the cost is probably beyond whatone would be willing to pay computationally to have that detail in real time.because the model's performance scales with added computing capacity, moredetailed predictions can in principle be made if enough computational resourcescan be coordinated to perform them.8crisis managers also require a sense of the reliability of data they workwiththe "error bars" around simulation results. to achieve this, an ensemble ofsimulations may be run using slightly different initial conditions. ensemblesimulation is especially important for chaotic phenomena, where points of greatdivergence from similar input conditions may not be readily apparent.ensemble simulation is ideally suited for running in parallel, because theprocesses are essentially identical and do not communicate with or influenceeach other. the difficult problem is identifying how to alter the initialconditions. as droegemeier noted, monte carlo simulation optimizes thesevariations to give the best results, but depends on a knowledge of the naturalvariability of the modeled phenomena that is not always available (e.g., in thecase of severe thunderstorm phenomena at the particular scales caps ismodeling). the infrequency of large crises makes it difficult to gain thisunderstanding of natural variability in some cases. more broadly, it impedesverifying models of extraordinary events. as robert kehlet, of the defensenuclear agency, said, "we are in the awkward position of not wanting to haveto deal with a disaster, but needing a disaster to be able to verify and validateour models."besides resources to perform the computations, remote modeling andsimulation also implies the need for adequate network capability to transportinput data to the model and distribute results to the scene. input data collectionrequirements may be most demanding if large amounts of realtime sensor dataaretechnology: research problems motivated by application needs71computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.involved (see the section "sensors and data collection" below). sensors willideally send compressed digitized data in packets that are compatible withexisting highspeed networks. however, the observation by egill hauksson, ofthe california institute of technology, that highspeed network costs remain toohigh for nonexperimental applications suggests that additional network researchand deployment could be necessary to make this a practical reality for crisismanagement.don eddington, of the naval research and development center, outlineda model, tested in the jwid exercises, for performing and/or integrating theresults of simulations at "anchor desks." anchor desks, located away from thefront line of crisis, could be staffed with people expert at running andinterpreting simulations, who could disseminate results to the field whenconditions warrant (e.g., a major change in the situation is detected). this modelreduces the amount of network traffic below that required by fulltimeconnection from the field to the remote highperformance computing platform.distributing results can be done by simply distributing a map or picture of thesimulation result.however, if information is to be integrated with other data available toworkers at the scene, or if complex threedimensional visualizations of theresults are called for, a picture or map may not suffice and a complete data filemust be sent. (needs for information integration and display are discussed inthe next two sections.) this implies higherbandwidth connections and greaterdisplay capabilities on the front line user's platform. ultimately, finding theoptimal balance of resources for various kinds of crises will requireexperimentation in training exercises and actual deployments. it should also beinfluenced by social science research on how crisis managers actually useinformation provided to them.mobility of computation and dataefficiency and performance typically demand that a computation becarried out near its input and output data. although the traditional solution is tomove the data to the computation, sometimes the computation requires so muchdata so quickly that it is better to move the computation to the data. since theappropriate software may not already reside on the target system, an executableor interpretable program may have to be transmitted across the network andexecuted remotely. this extends the meaning of the term relocatable beyond theability of programmers to port code easily from one platform to another to theability of code to operate in a truly platformindependent manner in response tourgent demands.in some circumstances, achieving high performance requires that theapplication software be optimized specifically for the machine on which it is tooperate, which usually requires recompilation of the application. for thisapproach to have the desired effect, the compilation environment must be ableto tailor the application to the specific target machine. this tailoring will notwork unless thetechnology: research problems motivated by application needs72computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.application is written in a machineindependent implementation language and itcan be compiled on each target machine to achieve performance comparable tothe best possible on that machine using the same algorithm.this problemšcompiler and language support for machineindependentprogrammingšis one of the key challenges in highperformance computation.although languages such as high performance fortran (hpf) and standardinterfaces like the message passing interface (mpi) are excellent first steps forparallel computing, the machineindependent programming problem remains animportant subject of continuing research. comments from lee holcombindicate that although progress has been made, research on machineindependent programming remains crucial to highperformance computing inall areas, not just crisis management:i think [programming for highperformance computing] is getting better. ithink many of the machines coming out today, as opposed to the ones thatwere produced, say, a year and a half to two years ago, provide a much betterenvironment. but when you ask a lot of computational scientists, who havespent their whole life porting the current [code] over to one machine and thenon to the next, when you give them the third machine to port it over to andhave to retune it, they lose a lot of interest and enthusiasm.an ability to relocate computation rapidly will require dynamic binding ofcode at run time to common software and system services (e.g., inputoutput,storage access). this implies a need for further development andstandardization of those services (e.g., through common applicationprogramming interfaces; apis) such that software can be written to takeadvantage of them.however, software applications that were not originally written to berelocatable may require a wrapper to translate their interfaces for the remotesystem. in manufacturing applications, such wrappers are prewritten, which isoften a costly, laborintensive process. research on generic methods enablingmore rapid construction of wrappers for software applicationsšultimately,producing them "on the fly," as might be required in a crisisšwas identified byworkshop participants as potentially valuable but currently quite challenging.advances in wrapper generation for software applications would enable morereuse of software and would benefit many areas in addition to crisismanagement. however, such advances will require basic research leading to anability to model, predict, and reason about software systems composed ofheterogeneous parts that is far beyond current capabilities. these advancescould be more generally relevant to many aspects of software systems, asdiscussed below in the section "software system development."storage servers and metadatacrisis management applications employ databases of substantial size. forexample, workshop participants estimated that a database of the relevanttechnology: research problems motivated by application needs73computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.infrastructure (e.g., utilities, building plans) of los angeles requires about 2terabytes. not all of it must be handled by any one computer at one time;however, all of it may potentially have to be available through the responseorganization's distributed communications. in addition, a wide variety of dataformats and representations occur and must be handled; this may always be thecase because of the unpredictability of some needs for data in a crisis.reformatting data rapidly through services such as those discussed in thesection ''information management" can be computationally intensive andrequire fast storage media.comprehensive provisions must also be made for storing not only data, butcollateral information (metadata) needed to interpret the data. besides concernsappropriate to all distributed file systems (authentication, authorization, naming,and the like), these involve issues of data validity, quality, and timeliness, all ofwhich are needed for reliable use of the data, and semantic selfdescription tosupport integration and interoperability.to customize information handling for particular applications, storageserver software should be able to interpret and respond to the metadata.workshop participants suggested that in crisis management, for example, ascheme could be developed to use metadata to limit the use of scarcebandwidth and to minimize storage media access time while accommodatingincoherence of data distributed throughout the response organization. toconserve bandwidth, a central database system located outside the immediatecrisis area could maintain a copy of the information stored in each computer atthe scene of the crisis. instead of replicating whole databases across the networkwhen new information alters, contradicts, or extends the information in eithercopy, a more limited communication could take place to restore coherencebetween copies or at least provide a more consistent depiction of the situation.a "smart" coherence protocol could relay only changes in the data, or perhapsan executable program to accomplish them. relevant metadata for makingthese determinations might include, for example, time of last update for eachdata point, so that new data can be identified, and an estimate of quality, toavoid replacing older but "good" data with newer "less good" data.besides resource conservation, a beneficial side effect of this coherencescheme would be the creation of a fairly accurate and uptodate representationof the entire crisis situation, valuable for coordination and decision making.modeling the coherence and flow of information into, within, and out of thecrisis zone could be incorporated into a system that would continuously searchfor (and perhaps correct) anomalies and inconsistencies in the data. it could alsosupport collaboration and coordination among the people working on a responseoperation by helping crisis managers know what information other participantshave available to them.technology: research problems motivated by application needs74computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.anomaly detection and inference of missing datahighperformance computing can be used for filling in missing dataelements (through machine inference that they are part of a computerrecognizable pattern), information validation, and data fusion in many nationalscale applications. for example, crisis data are often incomplete or simplycontradictory. simulation could be used to identify outlier data, flaggingpotential errors that should be verified. higher computational performance isrequired to correct or reconstruct missing data from complex dynamic systems,interpolating information such as wind speeds and directions or floodwaterlevels through machine inference. incorrect datašperhaps derived from faultysensors, taken from outofdate or incorrect databases, or deliberatelyintroduced by an active adversaryšcould be detected and corrected bycomputers in situations where the complexity or volume of the data patternswould make it difficult for a human to notice the error. ordinarily, the absenceof key information requires users to make intuitive judgments; tools that helpcope with gaps in information are one element of what workshop participantscalled "judgment support" (see the section "usercentered systems" below inthis chapter).the widespread presence of semantic metadata could enhance datamining and inference for detecting errors in databases. data mining in highperformance systems has been effective in other applications, for example, infinding anomalous credit card and medical claims; new applications such asclinical research are also anticipated (see box 2.2). however, the nature ofcrises is such that data being examined for anomalies may be of anunanticipated nature and may not be fully understood. there is a challenge forresearch in identifying the right types of metadata that could make data miningand inference over those unanticipated data possible.sensors and data collectionmore widespread use of networked sensors could generate valuable inputsfor crisis management, as well as remote health care and manufacturing processautomation. the variety of potentially useful sensors is particularly broad incrisis management, including environmental monitors such as those deployed inthe oklahoma mesonet or the nexrad (next generation weather radar)doppler radar system; video cameras that have been installed to enhancesecurity or monitor vehicle traffic; and structural sensors (as in "smart"buildings, bridges, and other structures networked with stress and strain sensors).some imagery, such as photographs of a building before it collapsed orsatellite photographs showing the current extent of a wildfire, are potential inputdata for simulation. timely access to and sharing of these data require highperformance communication, including network management, both to and fromtechnology: research problems motivated by application needs75computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.the crisis scene. moreover, models could be designed to take realtime sensorinputs and modify their parameters accordingly to accomplish a more powerfulcapability to predict phenomena. as donald brown, of the university ofvirginia, noted, the nonlinearity of many realworld phenomena poseschallenges for modeling; learning how to incorporate these nonlinearities intomodels directly from sensors could improve the performance of modelssignificantly.box 2.2 clinical research applications of datamininghistorically, challenges posed by medical problems have motivated manyadvances in the fields of statistics and artificial intelligence. traditionally,researchers in both fields have had to make do with relatively small medicaldatasets that typically consisted of no more than a few thousand patient records.this situation will change dramatically over the next decade, by which time weanticipate that most health care organizations will have adopted computerizedpatient record systems. a decade from now, we can expect that there will be some100 million and eventually many more patient records with, for example, a fulldatabase size of 10 terabytes, corresponding to 100 text pages of information foreach of 100 million patients. functionalities needed in the use and analysis ofdistributed medical databases will include segmentation of medical data into typicalmodels or templates (e.g., characterization of disease states) and comparison ofindividual patients with templates (to aid diagnosis and to establish canonical caremaps). the need to explore these large datasets will drive research projects instatistics, optimization, and artificial intelligence. . . .care providers and managers will want to be able to rapidly analyze dataextracted from large distributed and parallel databases that contain both text andimage data. we anticipate that . . . significant performance issues . . . will arisebecause of the demand to interactively analyze large (multiterabyte) datasets.users will want to minimize waste of time and funds due to searches that reveallittle or no relevant information in response to a query, or retrieval of irrelevant,incorrect or corrupted datasets.source: davis et al. (1995), as summarized at workshop iii by joel saltz, ofthe university of maryland.sometimes a sensor designed for one purpose can be usedopportunistically for another. for example, an addressable network of electricutility power usage monitors could be used to determine which buildings stillhave power after an earthquake, and which of those buildings with power arelikely to have occupants. a similar approach could be taken using the resourcesof a residential network service provider. workshop participants suggested thatsecurity cameras also provide opportunities for unusual use; with ingenuity itmay be possible to estimate the amplitude and frequency of an earth tremor orthe rate at which rain falls by processing video images. given the high cost ofdedicated sensortechnology: research problems motivated by application needs76computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.networks and the infrequency of crises, technology to better exploit existingsensors opportunistically could facilitate their use.people carrying sensors might be another effective mode of sensor networkdeployment. robert kehlet noted that field workers could wear digital camerason their helmets; personal geographic position monitors could be used tocorrelate the video data with position on a map. physical condition monitors onworkers in dangerous situations could hasten response if someone is injured.research is needed to optimize architectures for processing realtimeinformation from large, highly scalable numbers of inputs.9 the problem islikely amenable to parallel processing, as demonstrated on a smaller scale inresearch described by jon webb, of carnegie mellon university, on machinevision synthesized from large numbers of relatively inexpensive cameras. ahighly decentralized architecture, perhaps using processors built into the sensorsthemselves (sometimes characterized as "intelligence within the network"),might be a highly effective way to conserve bandwidth and processing; sensorscould detect from their neighbors whether a significant change in overall state isoccurring and could communicate that fact to a central location, otherwiseremaining silent. there could be value in research and development toward anetwork designed such that, in response to bandwidth or storage constraints inthe network, discrete groups of sensors perform some data fusion before passingtheir data forward; an adaptive architecture could permit this feature to adjust tochanging constraints and priorities.distributed resource managementnetworkintensive computing places unusual stress on conventionalcomputer system management and operation practice. describing the generalresearch challenge, randy katz said,we tend to forget about the fact that [the information infrastructure] won't bejust servers and clients, information servers or data servers. there are going tobe computeservers or specialized equipment out there that can do certainfunctions for us. it will be interesting to understand what it takes to buildapplications that can discover that such special highperformance engines thatexist out there can split off a piece of themselves, execute on it, and recombinewhen the computation is done.because significant remote computing and storage resources may benecessary, standardized services for resource allocation and usage accountingare important. other important issues are enforcing the proper use of networkresources, determining the scale and quality of service available, andestablishing priorities among the users and uses. mechanisms are needed toaddress these issues automatically and dynamically. operating system resourcemanagement is weak in this area because it treats tasks more or less identically.for example, many currenttechnology: research problems motivated by application needs77computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.networkaware batch systems are configured and administered manually andsupport no rational networkwide basis for cost determination.dennis gannon, of indiana university, suggested the value of continueddevelopment of network resource management tools as follows: "highperformance computing . . . should be part of the fabric of the tools we use. itshould be possible for a desktop applications at one site to invoke the resourcesof a supercomputer or a specialized computing instrument based on therequirements of the problem. a network resource request broker should providecosteffective solutions based on the capabilities of compute servers." hepointed to the information widearea year (iway) experimental network as auseful early demonstration of such capabilities.10software system developmentto the extent that it improves capabilities for integrating softwarecomponents as they relocate and interact throughout networks, researchenabling a networkintensive style of computing may be helpful in addressing alongstanding, fundamental problem for many application areas, that of largesoftware system development. speaking about electronic commerce systems,daniel schutzer, of citibank, said succinctly, "the programming bottleneck isstill there." darpa's duane adams described the problem as follows:many of our application programs [at darpa] are developing complex,softwareintensive systems. for example, we are developing control systemsfor unmanned aerial vehicles (uavs) that can fly an airplane for 24 or 36hours at ranges of 3,000 miles from home; we are developing simulationbaseddesign systems to aid in the design of a new generation of ships; and we aredeveloping complex command and control systems. these projects are usingvery few of the advanced information technologies that are being developedelsewhere in [d]arpašnew languages, software development methodologiesand tools, reusable components. so we still face many of the same problemsthat we have had for years.this raises some interesting technology problems. are we working on the rightset of problems, and are we making progress? how do we take this technologyand actually insert it into projects that are building real systems? i think one ofthe biggest challenges we face is building complex systems. we have talkedabout some of the problems. one of them is clearly a scaling problem . . .scaling to the number of processors in some of the massively parallel systems[and] . . . scaling software so you can build very large systems andincrementally build them, evolve them over time.software reuse through integration of existing components with new onesis necessary to avoid the cost of reproducing functionality for new applicationsfrom scratch. building large systems often needs to be done rapidly, andbecause most large systems have a long, evolutionary lifetime, they must bedesigned totechnology: research problems motivated by application needs78computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.change. however, these are not easy challenges. distributed object librariessuch as those facilitated by the common object request broker architecture(corba; discussed in the next section, "information management") may beuseful, but more developed frameworks and infrastructure are needed to makethem fully usable in the building of applications by large distributed teams ofpeople. basic tools to support scalable reuse, to catalogue and locate them, andto manage versioning are still primitive.it is clear that getting even currently available systembuildingtechnologies and methods into actual use in the software developmententerprise is a major challenge. changing the work practices of organizationstakes time; however, there may be ways in which collaboration technology canmake it easier to incorporate the available techniques into work practices moresmoothly. a collaboration environment that allows software development teamsto manage the complex interactions among their activities could reap benefitsacross the spectrum of applications. dennis gannon identified the need todesign a "problemsolving environment" technology that provides aninfrastructure of tools to allow a distributed set of experts and users to build, forexample, large and complex computer applications. participants in workshop ideveloped a subjective report card rating the current state of the art incomputing environments as follows: application construction (mixed expression, visual programming, groupware,portable and reusable partsšincluding how to finance them)d execution control (management of scheduling and resources, data, performance)d performance and data visualizationb debuggingc+ testingc application management (applications, databases, versioning)c rapid prototyping (e.g., to evaluate look and feel)cœin the absence of a deeper understanding of large, distributed softwaresystems, however, new tools are not likely to improve the situation. decades ofexperience with software engineering indicate that the problems are difficultšthey are not solvable purely by putting larger teams of engineers to work or bymaking new tools and techniques available (cstb, 1992, pp. 103107; cstb,1989). barbara liskov, of the massachusetts institute of technology, cited theneed for a good model of distributed computation on which to develop systemsand reason about their characteristicsša software infrastructure, not just aprogramming language or a collection of tools, that would support a way ofthinking about programs, how they communicate, and their underlying memorymodeltechnology: research problems motivated by application needs79computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.(see box 2.3). a consistent software infrastructure model of computation couldform the basis not only for building systems using that model but for reasoningabout their correctness and performance as they are being built. it would beextremely useful for system developers to be able to predict the performance,fault tolerance, or other specified features of a system composed from partswhose properties are known.box 2.3 challenges of programmingat workshop iii, barbara liskov, of the massachusetts institute of technology,observed:"people have to write programs that run on these [largescale, distributed]systems. applications need to be distributed, and they have got to work, and theymust do their job with the right kind of performance. . . . these applications aredifficult to build. one of the things i was struck by in the conversations today wasthe very ad hoc way that people were thinking of building systems. it was just abunch of stuff that you connect togetheršthis component meshes with thatcomponent. you know, we can't build systems that way. and the truth is we hardlyknow how to build systems that worked on the old kind of distributed network. . . ."we have a real software problem. if i want to build an application where i canreason about its correctness and its performance under a number of adverseconditions, what i need is a good model of computation on which to base thatsystem, so that i have some basis for thinking about what it is doing. i think whatwe need is a software infrastructure, and i don't mean by this a programminglanguage and i also don't mean some bag of tools that some manufacturers makeavailable. i mean some way of thinking about what programs are, what theircomponents are, where these components live, how they communicate, what kindof access they have to shared memory, what kind of model of memory it is, whetherthere is a uniform model, whether it is a model where different pieces of thememory have different fault tolerance characteristics, what is the fault tolerancemodel of the system as a whole, what kinds of atomicity guarantees are provided,and so on. we don't have anything approaching this kind of model for people tobuild their applications on today."this problem of composability of software components is very difficultand requires fundamental research. increased understanding, however, couldsupport a valuable increase in the ability to build systems driven by applicationneeds. dennis gannon said, "we should be able to have software protocols thatwould allow us to request a computing capability based on the problemspecification, not based on machine architectures." this will be especiallycrucial as the stability of discrete machine architectures becomes less fixed withnetworkcentered computing. for example, vinton cerf observed that innetworkintensive computing, the buses of the traditional computer architectureare replaced in sometechnology: research problems motivated by application needs80computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.respects with network links of a reliability that is unpredictable and often lessthan perfect. there must also be a way of representing the cost, reliability, andbandwidth tradeoffs of various network links in a way that software canunderstand and act upon, so they can be optimized according to the needs of theproblem at hand. these fundamental issues of computation represent a difficultbut potentially very valuable avenue for investigation.information management: finding andintegrating resourcesin the past decade, there have been important transitions in informationmanagement technologies used in large organizations. this is usuallycharacterized as a shift from centralized to more distributed resources, butperhaps a more accurate characterization is that it is a better balancing betweencentralized and distributed control of information production, location, andaccess. technologies such as clientserver architectures and distributed onlinetransaction processing systems have enabled this more effective balancing. it isan ongoing activity at all levels of organization structure, from central databasesto individual and groupspecific resources.this situation, difficult as it is within a single organization, becomes muchmore complex with the scale up to national, multiorganizational applications.this section considers the information management challenges posed bynationalscale applications, with particular emphasis on crisis management. itexamines several important issues and trends in information management andsuggests additional challenges.information management involves a broad range of resources withdifferent purposes, such as traditional databases (typically relational), digitallibraries, multimedia databases (sometimes used in video servers), objectrequest brokers (such as those in corba), widearea file systems (such as thenetwork file system and andrew file system), corporate information websbased on groupware and/or the world wide web, and others. besides relationaltables, conventional types of information objects can include multimediaobjects (images, video, hypermedia), structured documents (possiblyincorporating networkmobile, executable software components, or "applets"),geographical coordinate systems, and application or taskspecific data types. itis useful to classify these information management resources into fourorganizational categories: (1) central institutional resources, (2) individualdesktop resources, (3) group resources, and (4) ubiquitous resources such as thecommunications network and email service.central resources include institutional databases, digital libraries, and othercentrally managed information stores. these typically have regular schemas;extensive support for concurrency and robust access; and supporting policyframeworks to maintain (or at least monitor) quality, consistency, security,technology: research problems motivated by application needs81computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.completeness, and other attributes. data models for institutional resources areevolving in several ways (such as the evolution from relational to objectrelational databases), but these models are meant to support largescale anduniform data management applications.individual resources consist of ad hoc structures. these resources may bein a process of evolving into more regular structures of broader value to anorganization (a process often called upscaling). alternatively, they may beindividual resources that differentiate and provide a competitive edge to theindividual and so are unlikely to be shared.group resources can include scaleddown and specialized institutionalresources as well as ad hoc shared resources. this suggests a continuum fromad hoc ephemeral individual resources through group resources to robustmanaged institutional resources. examples of group resources are engineeringdesign repositories, software libraries, and partially formulated logistics plans.the final class of resources, which may be called ubiquitous resources,consists of shared communications and information services on acommunications network, including services such as electronic mail,newsgroups, and the world wide web. these services exist uniformlythroughout an organization and, unlike the other classes of resources, generallydo not reflect organizational hierarchy.this classification of resources provides a useful framework for examiningbroad trends in information management and considering, particularly, thespecial problems associated with nationalscale applications, such as thefollowing:1. information integration. in many of these applications, information mustbe integrated with other information in diverse formats. this mayinclude integration of diverse access control regimes to enableappropriate sharing of information while simultaneously maintainingconfidentiality and integrity. it can also include integration ofinstitutional, group, and personal information. related to the integrationproblem is the issue of information locationšhow can information beindexed and searched to support nationalscale applications?2. metadata and types. shared objects in very largescale applications canhave a rich variety of types, and these types can be very complex. anexample of a family of complex types is the diversity of representationsand formats for image information. how can objects be shared and usedwhen their types are evolving, perhaps not at the same pace as theapplications software that uses them? also, there is an evolving view ofinformation objects as aggregations of information, computation, andstructure. how will this new view affect information management morebroadly? related to this is the more general issue of metadata:descriptive information about data, including context (origin, ownership,etc.) as well as syntactic and semantic information. metadataapproaches are needed that support modeling of quality and otherattributes through antechnology: research problems motivated by application needs82computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.integration process. this could include integration of information thatmay appear to be inconsistent due to quality problems.3. production and value. a final information product can be derivedthrough a series of steps involving multiple information producers andorganizations. this involves addressing the development of models forthe kinds of steps that add value to a product beyond the informationintegration problem mentioned above.4. distribution and relocation. the linking of information resources at alllevels into nationalscale applications places great stress on a variety ofdistributed computing issues such as robustness and survivability, namemanagement, and flexible networking. in addition, there is the issue ofadaptivityšthe interplay of network capability and applications behavior.before examining these four issues in greater detail, it is useful to point outsome general trends in information management that are part of the evolutionalready under way to nationalscale applications.first, the ongoing shift over the past decade from central mainframeresources to more distributed clientserver configurations is giving way to asteady migration of both resources and control over resources withinorganizations. this suggests that the main challenge is to better enable this shiftas an ongoing process, rather than as a onetime effort. this steady flux issustained by the emergence of ad hoc groups that establish and manage theirown resources (which must later be integrated with others), by a continualchange and improvement in information management technologies, and bystructural change within organizations. a military joint task force and a civiliancrisis action team are examples of ad hoc groups that both establish their ownresources and rely on a broad range of institutional resources. in other words,we are just beginning to explore the interplay among institutional informationresources, individual ad hoc information resources, and communications andinformation services such as electronic mail and the world wide web.second, the complexity and quantity of information, the range anddiversity of sources, and the range of types and structures for information are allincreasing rapidly, as is the need to assimilate and exploit information rapidly.the problem is not an overload of information, but rather a greater challenge tomanage it effectively. also, as noted above, the nature of the information itemsis changing: they have more explicit structure, more information about theirtype, more semantic information, and more computational content. there arealso increasingly stringent requirements to manage intellectual propertyprotection and support commerce in information items.finally, there is greater interconnectivity and heterogeneity both withinand among organizations. this enables more complex information pathways,but it also creates greater challenges to the effective management ofinformation. related to this trend is the rapidly increased extent to whichinformation users aretechnology: research problems motivated by application needs83computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.becoming information producers. the world wide web presents the mostcompelling evidence of this; when barriers are reduced sufficiently, greaternumbers of people will make information available on the network. whenelectronic commerce technologies become widely used, in the relatively nearfuture, this will create a rich and complex marketplace for information products.integration and locationnationalscale applications involve large numbers of participatingorganizations with multiple focal points of organizational control and multipleneeds for information. they often involve solving information managementproblems that rely on multiple sources of data, possibly including legacydatabases that are difficult to reengineer. this creates a problem of informationintegration in which multiple information resources, with different schemas,data representations, access management schemes, locations, and othercharacteristics, may have to be combined to solve queries. as discussed inchapter 1, sometimes this information can be preassembled and integrated inresponse to mutually agreedupon, anticipated needs; however, this is notalways feasible. strategies that make integration feasible are needed to meet theshortterm press of crises, and they may well have utility in reducing costs andotherwise facilitating information integration in other, less timesensitiveapplications, which chapter 1 discusses with respect to digital libraries.information integration is an area of active research aimed at introducingadvances over traditional concepts of wrappers and mediators. a ''wrapper" fora database provides it with a new virtual interface, enabling the database toappear to have a particular data model that conforms to a user's requirement forwhich the database may not have been designed. a "mediator" provides acommon presentation for a schema element that is managed differently in a setof related database. a mediator can thus translate different users' requests intothe common presentation, which multiple wrappers sharing that presentationcan then translate into forms understood by the resources they interact with (i.e.,"wrap"). thus, mediators and wrappers give users a uniform way to access a setof databases integrated into a system, so that they appear as a single virtualaggregate database. in the past, much of this work has been performed on alaborious, ad hoc basis; more generalpurpose approaches, such as the stanfordibm manager of multiple information sources (tsimmis; see box 2.4) aim atproducing mediation architectures of more general use.most research now under way focuses on how a virtual aggregate databasecan be engineered for a set of existing databases. this involves developing datamodels and schemas suitable for the virtual aggregate, and mappings among themodels and schemas for the component databases and the common data modeland schema elements. when this is to be done on more than an ad hoc basis,methods are needed to represent the aggregate schemas. when legacy databasestechnology: research problems motivated by application needs84computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.are involved, reverse engineering of those databases may be necessary todetermine their schemas. this can be risky, because there are often hiddenassumptions and invariants that must be respected if a legacy database is toremain useful. as yigal arens, of the university of southern california,discussed, the information integration problem becomes more difficult whenqueries to the aggregate database need to be carried out efficiently (subject to atime constraint), creating research challenges for query optimization at theaggregate level.new approaches in research on information integration are beginning toyield results, but scaling up to national or global scale will significantlycomplicate the information integration problem. for example, when multipleorganizations are involved, access control issues become more important andalso more difficult. just as new schemas are required for the aggregate toreconcile multiple schemas, aggregate access control and security models mayalso have to be developed. also, information integration may be complicated bydistributed computing issuesšfor example, a set of databases may beinterconnected intermittently or over a lowcapacity link, which would affectthe way query processing is carried out. this is a familiar issue in distributeddatabases that becomes more difficult in a heterogeneous setting.richer data models have been developed for specialized uses, such asobject databases for design applications or information retrieval databases fordigital libraries. when these kinds of information assets must be integrated withmore traditional databases, the information integration problem can becomemuch more complicated. one way to address this problem is to developcommon reusable wrapper and mediator elements that can be adapted easily toapply in a wide range of circumstances.applications such as crisis management increase the difficulty ofinformation integration by introducing the need to integrate rapidly a set ofdatabases whose integration was previously not contemplated. the accounts ofinformation management in crisis situations that were presented in theworkshops focused on ad hoc information integration solutions designed tomeet very specific needs. for example, geographic databases, land use andutility databases, real estate tax databases, and other databases from a variety ofsources are necessary to gather information to rapidly process damage claimsrelated to natural disasters such as storms and earthquakes. this suggests thatthere is value in anticipating this kind of integration, and developing, inadvance, a repertoire of taskspecific common schemas and associatedmediators for legacy databases. this hybrid approach to integration has appeal,in that it supports incremental progress toward common schemas when they canbe agreedupon, and when common schemas cannot be arrived at, mediatorscan be developed to support interoperability. this also suggests that informationintegration provides techniques that may be applicable to more general (and lessapproachable) information fusion problems.11in addition to integration, there is the related issue of information location.searching within a database of a specific digital library depends upon finding thetechnology: research problems motivated by application needs85computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.appropriate database or digital library. as eliot christian, of the u.s.geological survey, observed:box 2.4 information integration technologieswrappers and mediators are not new technologies; they have beenimplemented in an ad hoc fashion for many years. one of the original motivationsfor work on wrappers was the desire to make legacy programs and informationsources (such as databases) accessible to diverse requesting applications acrossnetworks. this required laborious, ad hoc production of wrappers that translaterequests from users' applications into queries and other commands that thewrapped resources can interpret and will respond to correctly.disagreement among workshop participants and additional inputs solicited forthis report illustrate the perhaps inevitable breadth of perspectives about what doesor does not constitute a new research idea. some contributors were pessimisticabout the likelihood of solving complex integration problems through wrappers andmediators. they suggested, for example, that years of experience have shown thatfor integration to work well, applications must be written in the expectation that theiroutput will be used as another application's input, or vice versašleavingunaddressed the problem of integrating legacy programs and information sourcesthat were not written with reuse in mind.1 others accepted the truth of thisobservation, but interpreted it as an opportunity for fundamental research, pointingto recent research aimed at developing architectures within which generictechniques may be found for more rapidly and reliably building softwarecomponents to integrate diverse resources, including legacy resources. giowiederhold has described one example in this vein, a threelayer mediationarchitecture consisting of the basic information sources and their wrappers; amediation layer that adds value by merging, indexing, abstracting, etc.; and theusers and their applications that need the information (wiederhold, 1992).there is a range of research challenges to make such an architecture broadlyuseful. for example, models for representing diverse information sources andlanguages for interacting with them must accommodate not only sources with awelldefined schema (e.g., the relational model used in many databases), butothers such as text files, spreadsheets, and multimedia.2 automatic orsemiautomaticone of the fundamental issues in information discovery is that one cannotafford to digest all available information and so must rely on abstractions. yet,the user of information may be working in a context quite different from whatthe information provider anticipated. while cataloging techniques cancharacterize a bibliographic information resource statically, i would like to seea "feature extraction" approach that would support abstraction of informationresources based more closely on the user's needs at the moment of searching.natural language processing may help in the direction of search based onknowledge representations, but the more general problem is to support a fullrange of pattern matching to include imagery and numeric models as well ashuman language. . . .technology: research problems motivated by application needs86computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.generation of wrappers would be a significant contribution; this a seriouschallenge that requires identifying and representing not only the syntactic interfacesbut also the semantic content and assumptions of information sources. someresearch has focused on rulebased tools for generating wrappers.complementary to research on representing characteristics of sources is theformal representation of domainspecific knowledge that users may need to accessand explore. this representation could facilitate generation of mediators optimizedfor understanding requests and translating them into searches that draw upon andintegrate multiple information sources, interacting with each source through awrapper. yigal arens, of the university of southern california, discussed currentresearch on applying a variety of artificial intelligence techniques to partiallyautomate the creation of mediators for specific applications.3 in this approach, amodel is constructed to describe not only the structure and content of a set ofinformation sources, but also the knowledge domain about which the sources haveinformation. the mediator translates user queries related to that domain into searchstrategies, which it then implements. changes in the range of information sourcesavailable (e.g., addition of new sources) can be accommodated by changing thedomain model, rather than rebuilding the mediator.1 one contributor noted the similarity between wrappers and unix pipes. the unix pipeoperator provides a software connection between programs, making the output of oneprogram into the input to another. this allows for plugging together applications in novelways. successful integration, however, requires more than just passing inputs and outputsback and forth; the two programs must also sharešand therefore might have to have beenwritten with explicit recognition ofša semantic agreement about what those elementsmean; otherwise, unpredictable, incorrect results may arise.2 the stanfordibm manager of multiple information sources (tsimmis) is one approachthat offers a data model and a common query language, as well as techniques for generatingmediators and networks that integrate multiple mediators. see garciamolina et al., "thetsimmis approach to mediation: data models and languages (extended abstract),"available on line from http://wwwdb.stanford.edu/tsimmis.3 see the sims project home page for more information, at http://www.isi.edu/sims.to me, the most immediate problem is that it is very difficult to find andretrieve information from disparate information sources. although someprogress has been made in building consensus on presentation issues throughthe likes of web browsers, tools for clientbased network search areconspicuously absent. with serverbased searching, one can only search forinformation in fairly narrow and predetermined domains, and then only withthe particular user interface that the information source thought to provide.for critical nationalscale applications, approaches to this informationresource location problem must go beyond the opportunistic searching andbrowsing characteristic of the internet. even when information resources arediverse, if they may have to be used in critical applicationsšparticularly thosewith urgent deadlinesšthere would be benefit from registering them and theircharacteristicstechnology: research problems motivated by application needs87computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.in an organized manner. with improvements, for example, in schemadescription techniques, this could make the information integration problemmore approachable as well.information location also relates to the distributed computing issues raisedabove, since one approach involves dispatching not just passive queries toinformation sources, but active information "agents" that monitor and interactwith information stores on an ongoing basis. information agents may alsodeploy other information agents, increasing the challenges (both to the initialdispatcher of the agents and to the various willing hosts) of monitoring andmanaging large numbers of deployed agents.metadata and typesinformation is becoming more complex, is interpreted to a greater extent,and supports a much wider range of issues. evidence of the increase incomplexity is found in (1) the growing demand for enriched data models, suchas enhancements to the relational model for objects and types; (2) the adoptionof various schemes for networkbased sharing and integration of objects, suchas corba; (3) the development of databases that more fully interpret objects,such as deductive databases; (4) the rapid growth in commercial standards andrepository technology for structured and multimedia objects; and (5) theintegration of small software components, such as applets, into structureddocuments.one important approach to managing this increased complexity is the useof explicit metadata and type information. william arms, of the corporationfor national research initiatives, observed, "very simple, basic informationabout information is, first of all, a wonderfully important building block and[second,] . . . a much more difficult question than anybody really likes to admit."multimedia databases, for example, typically maintain separate stores forthe encoded multimedia material and the supporting metadata. metadataprovide additional information about an object, beyond the content that is theobject itself. any attribute can be managed as metadata. for example, in amultimedia database, metadata could include index tags, information about thebeginnings and endings of scenes, and so on. metadata can also include qualityinformation. in crisis management applications, this is crucial, since there aresome cases where many of the raw data (40 percent, in david kehrlein'scommercial gis example discussed in chapter 1) are inaccurate in somerespect. as david austin, of edgewater, maryland, noted, "often, data aremerged and summarized to such an extent that differences attributable tosources of varying validity are lost." separately distinguishable metadata aboutthe reliability of sources can help users identify and manage around poorquality data.types are a kind of metadata that provide information on how objects canbe interpreted. in this regard, type information is like the more usual databaseschema. types, however, can be task specific and ad hoc. task specificitytechnology: research problems motivated by application needs88computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.means, for example, that the particular consensus types in the multipurposeinternet mail extension (mime) hierarchy are a small subset of the types thatcould be developed for a particular application.because of this task specificity, the evolution of types presents majorchallenges. for example, the type a user may adopt for a structured documenttypically evolves over a period of months or years as a result of migration fromone desktop publishing system to the next. either the user resists migration andfalls behind technology developments, or the user must somehow manage a setof objects with similar, but not identical types. one approach to this problem isto create a separate set of servers for types that serve up type information andrelated capabilities (e.g., conversion mechanisms that allow objects to betransformed from one type to another).a related issue is the evolution of structured objects to contain softwarecomponents. the distinction between structured documents and assemblies ofsoftware components has been blurring for some time, and this trend willcomplicate further the effective management of structured objects. for example,because a structured object can contain computation, it is no longer benign fromthe standpoint of security. an information object could threaten confidentialityby embodying a communications channel back to another host, or it couldthreaten integrity or service access due to computations it makes while within aprotected environment. many concepts are being developed to address theseproblems, but their interplay with broader information management issuesremains to be worked out. this issue also reinforces the increasing convergencebetween concepts of information management and concepts of software andcomputation.production and valuenationalscale applications provide many more opportunities forinformation producers to participate in an increasingly rich and complexinformation marketplace. every educator, health care professional, and crisismanagement decision maker creates information, and that information has aparticular audience. technology to support the efficient production ofinformation and, more generally, the creation of value in an information valuechain is becoming increasingly important in many application areas and on theinternet in general.the world wide web, even in its present early state of development,provides evidence of the wide range of kinds of value that can be providedbeyond what are normally thought of as original content. for example, amongthe most popular web services are sites that catalog and index other sites. manysites are popular because they assess and evaluate other sites. there are servicesemerging for brokering of information, either locating sites in response toqueries or locating likely consumers of produced specialty information. becauseof the speed of the electronic network, many steps can be made very efficientlyalong the way from initial producer to end consumer of information.technology: research problems motivated by application needs89computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.related to these concepts of information value are new informationservices. for example, there are several candidate services that supportcommerce in information objects. because information objects can be deliveredrapidly and reliably, they can support commerce models that are very differentfrom models for physical objects. in addition, services are emerging to supportinformation retrieval, serving of complex multimedia objects, and the like. theprofusion of information producers on the web also creates a need for atechnology that enables successful smallscale services to scale up to largerscale and possibly institutionallevel services. nationalscale applications suchas crisis management complicate this picture because they demand attention toquality and timeliness. thus the capability of an information retrieval system,for example, may be measured in terms of functions ranging from resourceavailability (for meeting a deadline) to precision and recall.distribution and relocationas noted above, distributed information resources may have to be applied,in the aggregate, to support nationalscale applications. in these applications,there can be considerable diversity that must be managed. the distributedinformation resources can be public or private, with varying access control,security, and payment provisions. they can include traditional databases, widearea file systems, digital libraries, object databases, multimedia databases, andmiscellaneous ad hoc information resources. they can be available on a majornetwork, on storage media, or in some other form. they also can include abroad range of kinds of data, such as structured text, images, audio, video,multimedia, and applicationspecific structured types.for many applications, these issues can interact in numerous ways. forexample, when network links are of low capacity or are intermittent, in manycases it may be acceptable to degrade quality. alternatively, relativeavailability, distribution, and quality of communications and computingresources may determine the extent to which data and computation migrate overthe distributed network. for example, lowcapacity links and limited computingresources at the user's location may suggest that query processing is best done atthe server; but when clients have significant computing resources and networkcapacity is adequate, then query processing, if it is complex, could be done atthe client site. when multiple distributed databases cooperate in responding toqueries, producing aggregated responses, this resourcebalancing problem canbecome more complex; when atomicity and replication issues are taken intoaccount, it can become even more difficult.in crisis management, resource management and availability issues take onnew dimensions. in a crisis, complex information integration problems mayyield results that go into public information kiosks. when communications areintermittent or resource constrained, caching and replication techniques musttechnology: research problems motivated by application needs90computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.respond to levels of demand that are unanticipated or are changing rapidly. candata replicate and migrate effectively without direct manual guidance andintervention? this is more difficult when there are data quality problems orwhen kiosks support direct interaction and creation of new information.usercentered systems: designing applicationsto work with peopleresearch on natural, intuitive user interface technologies has been underway for many years. although significant progress has been made, workshopparticipants indicated that a more comprehensive view of the humancomputerinterface as part of larger systems must be developed in order for thesetechnologies to yield the greatest benefit. allen sears observed, "the fact thathumans make . . . errors, the fact that humans are impatient, the fact thathumans forgetšthese are the kinds of issues that we need to deal with inintegrating humans into the process. the flip side of that . . . is that humans,compared to computers, have ordersofmagnitude more domainspecificknowledge, general knowledge, common sense, and ability to deal withuncertainty."system designs should focus on integrating humans into the system, notjust on providing convenient humancomputer interfaces. the term "system"today commonly refers to the distributed, heterogeneous networks, computers,and information that users interact with to build and run applications and toaccomplish other tasks. a more useful and accurate view of the usersystemrelationship is of users as an integral part of the total system and solution space.among other advantages, this view highlights the need for research integratingcomputing and communications science and engineering with advances in theunderstanding of user and organizational characteristics from the social sciences.humancentered systems and interfacestraditional humancomputer interface research embraces a wide array oftechnologies, such as speech synthesis, visualization and virtual reality,recognition of multiple input modes (e.g., speech, gesture, handwriting),language understanding, and many others.12 all applications can benefit fromeasy and natural interfaces, but these are relative characteristics that vary fordifferent users and settings. a basic principle is that the presentation should beas natural to use as possible, to minimize demands on those with no time orattention to spare for learning how to use an application. this does notnecessarily imply simplicity; an interface that is too simple may not providesome capabilities the user needs and lead to frustration.in addition, designers of interfaces in largescale applications with diverseusers cannot depend on the presence of a particular set of computing andcommunications resources, so the interfaces must be adaptable to what isavailable. thetechnology: research problems motivated by application needs91computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.networkdistributed nature of many applications requires attention to the scalingof user interfaces across a range of available platforms, with constraints that arediverse andšespecially in crisesšunpredictable. constraints include powerconsumption in portable computers and communications bandwidth. forexample, it is important that user interfaces and similar services for accessing aremote computing resource be usable, given the fidelity and quality of serviceavailable to the user. an additional focus for research in making interfacetechnologies usable in nationalscale applications is reducing their cost.crisis management, however, highlights the need to adapt not only toavailable hardware and software, but also to the user. variations in training andskills affect what users can do with applications and how they can best interactwith them. as david austin observed:training is also critical; people with the proper skill mix are often in shortsupply. we have not leveraged the technology sufficiently to deliver shortbursts of training to help a person gain sufficient proficiency to perform thetask of the moment. . . .[what is needed is] a system that optimizes both the human element and theinformation technology element using ideas from the object technology world.in such a system, a person's skills would be considered an object; as the persongained and lost skill proficiency over his career, he would be trained and givendifferent jobs [so that he could be part of] a highperformance work force ableto match any in the world. the approach involves matching a person with a joband at the same time understanding the skill shortfalls, training in short bursts,and/or tutoring to obtain greater proficiency. as shortfalls are understood bythe person, he or she can task the infrastructure to provide justintime, justenough training at the time and place the learner wants and needs it.in addition, because conditions such as stress and information overload canvary rapidly during a crisis, there would also be value in an ability to monitorthe user's performance (e.g., through changes in response time or dexterity) andadapt in real time to the changing capabilities of users under stress. by usingthis information, applications such as a "crisis manager's electronic aide" couldadjust filtering and prioritization to reduce the flood of information given to theuser. improvements in techniques for data fusion in real time among sensorsand other inputs would enhance the quality of this filtering. applications couldalso be designed to alter their presentation to provide assistance, such aswarnings, reminders, or stepbystep menus, if the user appears to be makingincreasing numbers of errors.the focus of these opportunities is inherently multidisciplinary. to achievesignificant advances in the usability of applications, improvements in particularinterface techniques can be augmented by integrating multiple, complementarytechnologies. recent research in multimodal interfaces has proceeded from therecognition that no single technique is always the best for even a single user,much less for all users, all the time, and that a combination of techniques can betechnology: research problems motivated by application needs92computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.more effective than any single one. learning how to optimize the interfacemode for any given situation requires experimentation, as well as building onsocial science research in areas such as human factors and organizationalbehavior.recognizing that the ideal for presentation of information to the user is in aform and context that is understandable, workshop participants noted that insome applications a visual presentation is called for. given adequateperformance, an immersive virtual reality environment could benefitapplications such as crisis management training, telemedicine, andmanufacturing design. in crisis management training especially, a realisticrecreation of operational conditions (such as the appearance of damagedstructures, the noise and smoke of fires and storms, the sound of explosions)can help reproducešand therefore train foršthe stressinducing sensations thatprevail in the field. because response to a crisis is inherently a collaborativeactivity, simulations should synthesize a single, consistent, evolving situationthat can be observed from many distinct points of view by the team members.13don eddington identified a common perception of the crisis situation as afeature that is essential to effective collaboration. a depiction of the geographicneighborhood of a crisis can provide an organizing frame of reference.photographs and locations of important or damaged facilities, visual renderingsof simulation results, logs of team activity, locations of other team members,notesšall can attach to points on a map. given adequate bandwidth andcomputing capacity, another way to provide this common perception might bethrough synthetic virtual environments, displaying a visualization of thesituation that could be shared among many crisis managers. (the crisis 2005scenario presented in box 1.3 suggests a longrange goal for implementing thisconcept such that a crisis manager could be projected into a virtual worldoptimized to represent the problem at hand in a way that enhances the user'sintuition.) research challenges underlying such visualizations include ways tointegrate and display information from diverse sources, including realobservations (e.g., from field reports or sensors) and simulations. the variationin performance among both equipment and skills of different users may preventdisplaying precisely the same information to all users; presumably, someminimal common elements are necessary to enable collaboration. determiningprecisely what information and display features should be common to allcollaborators is an example of the need for technology design to becomplemented with multidisciplinary research in areas such as cognition andorganizational behavior.collaboration and virtual organizationsbecause people work in groups, collaboration support that helps themcommunicate and share information and resources can be of great benefit. crisismanagement has a particularly challenging need: an instant bureaucracy torespond effectively to a crisis. in a crisis, there is little prior knowledge of whowilltechnology: research problems motivated by application needs93computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.be involved or what resources will be available; nevertheless, a way must befound to enable them to work together to get their jobs done. this impliesassembling resources and groups of people into organized systems that no onecould know ahead of time would have to work together. multiple existingbureaucracies, infrastructures, and individuals must be assembled and formedinto an effective virtual organization. the instant bureaucracy of a crisisresponse organization is an even more unpredictable, horizontal, andheterogeneous structure than is implied by traditional command and controlmodels of military organizations in warfarešthemselves a complexcollaboration challenge. crisis management collaboration must accommodatethis sort of team building rapidly; thus, it provides requirements for developingand opportunities for testing collaboration technologies that are rapidlyconfigurable and support complex interactions.one relatively nearterm opportunity is to develop and use the concept ofanchor desks (discussed above, in the section ''distributed computing"). theconcept has been tested in technology demonstrations such as jwid (seechapter 1); field deployment in civilian crises could be used to stress theunderlying concepts and identify research needs. anchor desks can provide aresource for efficient, collaborative use of information, particularly wheremultiple organizations must be coordinated. they represent a hybrid betweendecentralized and centralized information management. each anchor desk couldsupport a particular functional need, such as logistics or weather forecasting. acrisis management anchor desk would presumably be located outside the crisiszone, for readier access to worldwide information sources and expertise;however, it would require sufficient communication with people working at thescene of the crisis to be useful to them, as well as the ability to deliverinformation in scalable forms appropriate to the recipient's available storage anddisplay capabilities (e.g., a geographic information system data file representingthe disaster scene for one, a static map image for another, a text file for a third).an anchor desk could not only integrate data from multiple sources, butalso link it with planning aides, such as optimized allocation of beds andmedicines and prediction of optimal evacuation routes implemented aselectronic overlays on geographic information systems, with tools involving arange of artificial intelligence, information retrieval, integration, and simulationtechnologies. an anchor desk could also house a concentration of informationanalysts and subject matter experts (e.g., chemists, as envisioned in the crisis2005 scenario); computing resources for modeling, simulation, data fusion, anddecision support; information repositories; and others.anchor desks could provide services to support crossorganizationalcollaboration, such as tools for rapidly translating data files, images, andperhaps even human languages into forms usable by different groups of people.furthermore, the anchor desk might not be physically at one place; a logicallycombined, but physically separated, collection of networked resources couldperform thetechnology: research problems motivated by application needs94computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.same function, opening the possibility for multiple ways of incorporating thecapability into the architecture of the crisis response organization. the set oftechnologies implied by this sort of anchor desk could serve to push researchnot only in each technology, but also in tools and architectures for integratingthese capabilities, such as whiteboards and videoconferencing systems thatscale for different users' capacities and can correctly integrate multiple securitylevels in one system.nevertheless, information must be integrated not only at remote locationssuch as command centers and anchor desks, but also at field sites. davidkehrlein, of the office of emergency services, state of california, noted,"solutions require development of onsite information systems and anintegration of those with the central systems. if you don't have onsiteintelligence, you don't know a lot."judgment supportthe most powerful component of any system for making decisions in acrisis is a person with knowledge and training. however, crisis decision makingis marked by underuse of information and overreliance on personal expertise inan environment that is turbulent and rich in information flows. the expert,under conditions of information overload, acts as if he or she has no informationat all. providing access to information is not enough. the ability to evaluate,filter, and integrate information is the key to its being used.filtering and integrating could be done separately for each person on thatperson's individual workstation. however, a more useful approach for anycollaborative activity would be to integrate and allocate information withingroups of users. (in fact, information filtering at the boundary of a linked groupof users could be one of the most important services performed by the virtualsubnets discussed above in the section "networking"; filters could helpindividuals and groups avoid informationpoor decision making in aninformationrich environment.) information integration techniques such as thosediscussed in the section "information management" are generally presented interms of finding the best information from diverse sources to meet the user'sneeds. the flip side of this coin is the advantage of being able to cull the secondbest and thirdbest information, reducing the unmanageable flood.a set of special needs of crisis management, which may have significantutility in other application areas as well, can be captured in the concept ofjudgment support. a crisis manager often makes intuitive judgments in realtime that correspond to previously undefined problems without completecontingency plans. this should be contrasted with traditional notions ofdecision support, which are associated with a more methodical, rulebasedapproach to previously defined and studied problems. judgment support forcrisis management couldtechnology: research problems motivated by application needs95computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.rely on rulebased expert systems to some extent, but the previously definedproblems used to train these systems will necessarily be somewhat differentfrom any given crisis. workshop participants suggested a need for automatedsupport comparing current situations with known past cases. to achieve thisautomation, however, much better techniques are required for abstractlyrepresenting problems, possible solutions, and the sensitivity of predictedoutcomes to variations, gaps, and uncertain quality in available information.the last point is particularly important for crises, because it is inevitablethat some of the information the judgment maker relies upon will be of lowquality. two examples are the poor quality of maps that crisis managementexperts remarked on in the workshops and the rapid rate of change in somecrises that continually renders knowledge about the situation obsolete. thetechnology for representing problem spaces and running computations on themmust therefore be able to account for the degree of uncertainty aboutinformation. moreover, data may not always vary in a statistically predictableway (e.g., gaussian distribution). in some kinds of crises, data points may beskewed unpredictably by an active adversary (e.g., a terrorist or criminal), bysomeone attempting to hide negligence after an accident, or by unexpectedfailure modes in a sensor network.another reason the challenge of representing problems may be particularlydifficult in crisis management is that the judgments needed are oftenmultidimensional in ways that are inherently difficult to represent. jamesbeauchamp's call for tools to help optimize not only the operational andlogistical dimensions of a foreign disaster relief operation, but also the politicalconsequences of various courses of action, illustrates the complexity of theproblem. even presenting the variables in a way that represents and could allowbalancing among all dimensions of the problem is not possible with currenttechniques. by contrast, the multidimensional problem discussed in chapter 1(see the section "manufacturing")šsimulating and optimizing tradeoffs amongsuch facets as product performance parameters, material costs,manufacturability, and full product lifecycle costsšalthough extremelycomplex computationally, is perhaps more feasible to define in terms withwhich computer models can work.if a problem can be represented adequately, a judgment support systemshould be able to assist the judgment maker by giving context and consequencesfrom a multidimensional exploration of the undefined problem represented bythe current crisis. this context construction requires automated detection andclassification of issues and anomalies, identifying outlier data points (whichcould represent errors, but could also indicate emerging new developments),and recognizing relationships between the current situation and previouslyknown cases that may have been missed by or unknown to the crisis manager.because judgments are ultimately made by people, not computers,technologies intended to support making judgments must be designed for easeof use and with an ability to understand and take into account the capabilitiesand needs of the user. to a great extent, of course, it is up to the user to ask forthe informationtechnology: research problems motivated by application needs96computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.he or she needs, but a model of what knowledge that individual already hascould be used to alter the system's information integration and presentationapproaches dynamically. another special application for crisis management ismonitoring the decision maker, because of the stress and fatigue factors thatcome into play. performance monitors could detect when the user's performanceis slipping, by detecting slowed reaction time and onset of errors. thisinformation could guide a dynamic alteration in the degree of informationfiltering, along with variations in the user interface (such as simpler menuoptions). these capabilities could be of more general value. for example, theycould assist in assessing the effectiveness of multimedia training and educationtools in schools and continuingeducation applications.of course, to be useful, a monitoring capability would have to beintegrated properly with the way users actually use systems. for example, userswill ignore a system that instructs them to get some rest when rest is not anoption. instead, it might be valuable for a system to switch to a standardoperating proceduresoriented, stepbystep interface when the user shows signsof tiring. human factors research provides useful insights, including some thatare of generic usefulness. however, needs will always vary with the context ofspecific applications, implying the strong necessity for researchers andapplication users to interact during testing and deployment of systems anddesign of new research programs (drabek, 1991).notes1. partridge, craig, and frank kastenholz, "technical criteria for choosing ip the nextgeneration (ipng)," internet request for comments 1726, december 1994. available on linefrom http://www.cis.ohiostate.edu/hypertext/information/rfc.html.2. services and technologies are now emerging that may meet this need, such as cellular digitalpacket data and digital spreadspectrum. portable terminals that can be used to communicate viasatellite uplink are an additional exception; however, such systems are not yet portable oraffordable enough that many relief workers in a crisis could carry one for general use.3. noncommercial, amateur packet radio is a counterexample; however, commercial serviceofferings are lacking. part of the problem is the lack of methods of accounting for use of thespectrum in peertopeer packet radio networks, without which there is a potential problem ofoveruse of the spectrumša tragedy of the commons.4. a description of the proposed demonstration is available on line at the jwid '96 home page,http://www.spawar.navy.mil.5. many telephone carriers now provide framerelay virtual subnets that are intended to supportthe isolation discussed here. one serious drawback at present is that their establishment is on acustom basis and is both labor intensive and timeconsuming. telephone carriers are likely toadopt a more automated order fulfillment process as demand grows, but it remains technicallyinfeasible to requisition and establish these services in the heat of a crisis to solve an immediateproblem.6. given the current costliness of access to highperformance computation and highspeednetwork services, achieving this gain will require political and economic decisions aboutmaking resources available, perhaps based on building a case that this investment could yield apositive payoff by lowering the eventual cost of responding to crises.7. in addition, the coarsergrained simulation can be used to provide dynamically consistenttechnology: research problems motivated by application needs97computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.boundary conditions around the areas examined in finer detail. the model, called the advancedregional prediction system, is written in fortran and designed for scalability. see droegemeier(1993) and xue et al. (1996). see also "the advanced regional prediction system," availableon line at http://wwwcaps.uoknor.edu/arps.8. a caps technical paper explains that "although no meteorological prediction or simulationcodes we know of today were designed with massive parallelism in mind, we believe it is nowpossible to construct models that take full advantage of such architecture." see "the advancedregional prediction system: model design philosophy and rationale," available on line athttp://wwwcaps.uoknor.edu/arps.9. the ability to effectively handle time as a resource is an issue not only for integrating realtime data, but for distributed computing systems in general. formal representation of temporalevents and temporal constraints, and scheduling and monitoring distributed computingprocesses with hard realtime requirements, are fundamental research challenges. someresearch progress has been made in verifying limited classes of realtime computableapplications and implementing prototype distributed realtime operating systems.10. details about iway are available on line at http://www.iway.org.11. one key data fusion challenge involves data alignment and registration, where data fromdifferent sources are aligned to different norms.12. some key challenges underlying communication between people and machines relate toinformation representation and understanding. these are addressed primarily in the section,"information management," but it should be understood that without semantic understandingof, for example, a user's requests, no interface technology will produce a good result.13. this concept is currently used for military training in instances when highperformancecomputation is available; trainees' computers are linked to the highperformance systems thatgenerate the simulation, and the trainees see a more or less realistic virtual crisis (ota, 1995).nonmilitary access to such simulations likely requires lowercost computing resources.technology: research problems motivated by application needs98computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.3summary and findings: research fornationalscale applicationsresearch challenges of crisis managementcrises can make enormous demands on the widely distributed informationresources of a nation (see summary in box 3.1). responding to the oklahomacity bombing disaster required a national call for search and rescue experts andtheir tools to help find survivors and to reinforce unstable areas of the damagedalfred p. murrah building so that rescuers could enter safely, as well asmassive coordination to focus a diverse set of teams on a common goal.hurricane andrew and the northridge, california, earthquake causedwidespread devastation and placed pressure on relief authorities to distributefood, water, shelter, and medicine and to begin receiving and approvingapplications for disaster assistance without delay.crises often bring together many different organizations that do notnormally work together, and these groups may require resources that they havenot used before. to mount an organized response in this environment, crisismanagers can benefit from the use of information technology to catalog,coordinate, analyze, and predict needs; to report status; and to track progress.this kind of coordinated management requires communications networks, fromhandheld radios and the public telephone network to highspeed digitalnetworks for voice, video, and data. rapidly deployable communicationstechnologies can help relief teams communicate and coordinate their actionsand pool their resources. crisis managers also need computers to help themretrieve, organize, process, and share information, and they rely on computermodels to help them analyze and predict complex phenomena such as weatherand damage to buildings or other structures.summary and findings: research for nationalscale applications99computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.when disasters occur, the public deserves and demands a rapid response,and so the ability to anticipate events is at a premium. for example, when ahurricane approaches, relief agencies deploy mobile communications centers toplaces where sophisticated computer models predict the storm will strike land.damage simulations help planners decide where to send food, medicine,shelters, blankets, and other basic necessities even before the damage hasoccurred. as the response to california's northridge earthquake demonstrated,relief agencies cansummary and findings: research for nationalscale applications100box 3.1 summary of crisis managementcharacteristics and needs 1. crises make largescale demands, are unpredictable, and require animmediate response. largescale demands. crises require resources beyond those on handšpeople, equipment, communications, information, and computing must bemoved rapidly to the scene physically and/or virtually (over networks). unpredictable. it cannot be known in advance what resources will beneeded or where, and what the specific needs will be (although there canbe some degree of generalizing and prepositioning). urgent. the response must be rapid, because lives and property are at stake. 2. crises require planning and coordination. crisis managers must develop and implement a response plan rapidly,despite information shortfalls (gaps, uncertainty, errors, deliberatefalsification by a foe) and the lack of correspondence to any previoussituations (i.e., standard operating procedures are not sufficient). diverse organizations and people respond to crises, including those thathave not worked together before and did not know that they would have todo so. this creates challenges for collaboration, information sharing, andcommunication. crises are complex and multifaceted, and so decision makers must weighmultifaceted consequences. tradeoffs require not just tacticaloptimization, but judgmentšthe best tactical option may not be the bestpolitical option (e.g., in an international context where u.s. military andcivilian agencies are operating in another country, perhaps in a tensesituation). 3. operational needs include communications and networking. a rapid initial assessment of the situation is necessary, requiring reportsfrom the scene, augmented by sending assessment teams with tools andcommunications to report back quickly. remote sensing may also beinvolved (e.g., satellite and aircraft imagery, groundbased weathermonitors, and strain gauges predeployed within bridges and buildings).computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.use computer simulation to speed the approval of disaster relief (e.g., homerebuilding loans) in areas that the model estimates are hard hit, even beforeagents have visited the site. rapid deployment of communications capabilities is requiredšto expand theinitial situation assessment, coordinate the response teams, and disseminateinformation to the public. it is necessary to (a) assess what is available (remoteregions, less developed countries, and badly damaged areas may all havelimited infrastructure) and (b) obtain needed capabilities by commandeeringwhat is there (priority access), restoring networks, and augmenting withdeployable capabilities (cellular telephones, wireless networks, sensors) asneeded. required communications parameters must be defined and implemented rapidly.these include (a) reliability (crucial for lifecritical communications, e.g., fire andrescue, telemedicine); and (b) securityšto maintain confidentiality (especially ifan active adversary is involved, but also to protect any private information that iscommunicated), maintain the integrity of information, and authenticate certainusers to allow them priority access. crises require more than voice communicationsštext, all types of sensoroutputs, images, fullmotion video, and data files must also be communicated;all involve different technological requirements and tradeoffs (e.g., latency,quality, bandwidth). crises demand integration across a wide, unpredictable scale of resources; thus,there must be flexibility about centralization versus distribution: (a) computingand communications that are available at or accessible to the crisis teaminclude laptops and wireless at the scene, workstations and t1 (1.5 megabitsper second) data links at the command center, and fully distributed computingand communications (e.g., world wide web, remote supercomputers) outsidethe crisis zone; (b) flows of information throughout (into, out of, within) thisarchitecture are unpredictable and may change during the crisis itself. at the scene, computers and communications platforms must be mobile anduntethered. 4. operational needs also include information resources and computation. there is a need for multifaceted informationšmultiple modes (voice, video,images, text, sensors, geographic information system (gis) data, relationaldatabases, and so on). it cannot be predicted in advance which multiple sources will be required.sources (a) cannot be used if crisis managers cannot find them or do not knowabout them (discovery); (b) cannot be used unless they can be accessed andpreparing for and responding to crises place demands on informationtechnology that cannot be satisfied readily with existing tools, products, andservices. these unmet demands point to many promising research directions,which this chapter summarizes. they encompass most aspects of computing andsummary and findings: research for nationalscale applications101computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.communications technology, including networks and the architectures andconventions that organize and manage them; the services and standards thatunite a network of communications devices, sensors, computers, and databasesinto a useful information infrastructure; and the applications that rely on thatinfrastructure. integrated into a crisis manager's information system (interoperability,composability, access rightsšintellectual property, privacy); and (c) cannot beused if the crisis managers are flooded with information. some kind ofautomated help is needed to sort informationšnot just filtering it, but alsointegrating the information to reduce the flow and detecting patterns that canhelp with interpretation. information systems must continually check and integrate new informationcoming in from the field. there is a realtime demand: for example, simulation or model data (e.g.,weather forecast) will not be useful if they arrive late. 5. crisis management in a broader context involves other needs as well. crisis management draws on other application domains, for example: (a) secureelectronic commerce for locating, purchasing, and distributing relief suppliesand services; (b) digital libraries as means for information discovery, integration,and presentation for crisis managers; (c) secure telemedicine and distributedmedical records to facilitate the delivery of emergency care in the field; and (d)manufacturing and distributed design, which, although not applicable in realtime (during a crisis), reflect common interests such as distributed modeling,simulation, shared databases, and virtual environments for collaboration. application needs for technology exist in a broader context as well: (a) solvingcrisis management problems is not just a computing or communications issue,given that it also involves political, managerial, social, and educational issues;(b) the political, economic, and marketplace context affects the availability oftechnology resources (hardware, software, information) for crisis managementšthus, affordability is essential; and (c) the sociology of organizations affects howthey use these technologies. complex systems must be tested in operational contexts to validate research anddetermine new research needs.note: see chapter 1 for a detailed discussion of the crisis managementcharacteristics and needs that create demands for computing and communications.one common thread among the steering committee's findings is that someof the most severe technical challenges stem from the sheer scale ofrequirements that must be met. scale in this context has many dimensionsšthelarge number of people and devices (e.g., computers, databases, sensors)involved; the diversity of information resources and software applications thatmust be accessible; the amount of computing power needed to run models andprocess information quickly enough to meet the urgent demands of crises, alongwith the ability tosummary and findings: research for nationalscale applications102computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.access and use that power rapidly and easily; and the complexity of theinteractions among systems (both human and technical) that must interwork todeal with crises.another theme is that technologies must be easy enough to use that theycomplement people, rather than distract them from their mission. technologydoes nothing by itself; people use technology, and designers and developers oftechnical systems must consider people and their methods of working asintegral to the systems and their successful performance. for example, a secure,distributed information system may fail to remain secure in practice if it is socumbersome that users ignore necessary procedures in favor of convenience.too often, unfortunately, users are given too little consideration in the design ofcomplex systems, and the systems consequently fail to be as useful as theycould or should be. in the extreme case of a crisis, a system that is difficult touse will not be used at all.research on and development of computing and communicationstechnologies that help crisis managers cope with extreme time pressures and theunpredictability of crises will likely be useful in other application areasdomains.1 for example, breakthroughs in meeting the timecritical informationdiscovery and integration requirements of crisis management would benefitbroader digital library applications as well. distributed simulation and the needto compose existing, legacy information sources and software smoothly intonew, casespecific systems are among the overlaps with manufacturing. secure,mobile communication in a crisis is also valuable for emergency medicine,particularly as confidential medical records begin to be communicated overnetworks. tools that are easy to use in a crisis will probably also be usable forelectronic commerce, which similarly must span a wide range of personal skills,computer platforms, and network access mechanisms.although many of the research issues identified throughout the workshopseries are not new to the computing and communications research community,placing them in the context of crisis management and other nationalscaleapplications suggests priorities and sharpens the research focus. the prioritiesfall across a spectrum. research projects tied relatively closely to specific crisismanagement application needs are valuable both because of the potentialbenefit to the applications and for the advances they may produce in technologyusable in other areas. box 3.2 presents promising examples from the workshops.to secure the full benefits of this applicationspecific research, there mustalso be recognition of the broader, increasingly interconnected context in whichnationalscale applications operate. these interconnections allow components tobe called on in unforeseen ways. this presents powerful opportunities forcreative new uses of resources, but only if technical challenges to these noveluses can be overcome. during hurricane andrew, for example, it was not onlythe difficulty of translating between different standards that delayed dadecounty authorities from making data available to federal relief officials, butalso theirsummary and findings: research for nationalscale applications103computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.box 3.2 selected crisis management applicationspecific researchthe discussions between crisis management experts and technologists at thethree computer science and telecommunications board workshops led toidentification of a variety of compelling, applicationmotivated computer science andengineering research topics. a selection of these topics is presented here. it is notan exhaustive list of the technologies needed to solve problems in crisismanagement, nor does it imply that technological advances are crisismanagement's most dire needs. however, these topics do appear promising interms of advancing the state of technology and testing broader architecturalconcepts.1. a selfconfiguring wireless data network for linking crisis field stationscould create a capability that does not exist today for crisis managers tocoordinate information. it could produce advances useful in otherdomains, such as hospital or school networking, and could provideuseful tests for new communications protocols and networkmanagement methods based on, for example, selfidentification bycomponents and resources on networks to other components that callupon them.2. adaptive networks could be developed that discover and react tochanges in available communications links and changing patterns oftraffic. they could, for example, route traffic around points of networkfailure and take advantage of links that become available after a crisisbegins, such as the selfconfiguring wireless network described aboveor parts of the public switched telecommunications network. thisresearch could stimulate more general work on adaptivity incommunications hardware and network management software incontexts other than crises.3. in crisis management (as well as in military command and control),there is a need for research on ''judgment support" tools that can assistcrisis managers in making decisions based on incomplete knowledge ofconditions, capabilities, and needs, by drawing on information fromdiverse and unanticipated sources. such tools would interpretinformation about the quality and reliability of varied inputs and assistthe user in taking these variations into account. they would differ from,but could build upon, traditional decision support tools such asknowledgebased systems, which operate using rules associated withpreviously examined problems. because many of the problems raisedby crisis management are not known ahead of time, more generaltechniques are needed. these might include the development of newrepresentations of the quality of inputs (such as metadata about thoseinputs), data fusion, models of information representation andintegration, rapidly reconfigurable knowledgebased systems, and newtechniques for filtering and presenting information to users.4. a metacomputer resource would provide rapid allocation of distributedcomputing resources outside a crisis zone to meet crisis managementneeds for modeling and simulation. because crises are intermittent andunpredictable, but require realtime urgent response, this researchwould highlight ways to bring computers, storage, and communicationson line rapidly. it would also require new ways to coordinate resourcesacross infrastructure that was not previously set aside for that purpose.summary and findings: research for nationalscale applications104computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.5. crisis management can motivate simulation research in hurricanelandfall prediction, severe storm prediction, atmospheric dispersion oftoxic substances, urban fire spread, and many other modeling andsimulation problems. to be useful in actual crisis response, thesesimulations must be linked with inputs such as environmental sensordata in real time and must be able to produce outputs in readily usableform. they should also support a capability to focus the simulation onspecific locations and scales in response to requests from crisismanagers. in addition, better simulation of human behavior and socialphenomena could provide more realistic training and decision supportfor crisis managers by indicating consequences of decisions on publicopinion, international tensions, financial markets, and other areas.6. multimedia fusion of data coming from varied, unexpected sources,including imagery (e.g., still photographs and video from amateurcitizens with video cameras, news helicopters, automated teller machinesecurity cameras), sensor data (e.g., weather, seismology, structuralstress gauges in bridges and buildings); and information from databases(e.g., locations of buildings and roads from a geographic informationsystem (gis), names of residents and businesses from telephonedirectories) will be necessary. the ability to integrate information inresponse to unanticipated queries could be facilitated by automatedtagging of data with relevant metadata (e.g., source, time, imageresolution).7. there is a need for virtual anchor desks in crisis management, with amixture of people and machines and adaptive augmentation asnecessary. anchor desks for specific functional or subject areas (e.g.,logistics, weather forecasting, handling of toxic substances) wouldprovide a resource of computational power, information, and expertise(both human and machine) to crisis officials. the anchor desk capabilitycould be distributed throughout the nation and assembled on call vianetworks in response to crises. research issues related to this effortmight include new models of information needs within organizations andoptimizing the balance of computation, communications, and storageinside and outside the crisis zone.8. adaptive interfaces for crisis managers could respond to changing userperformance under stress by observing usage patterns and testing foruser errors. crisis management tools could adapt to the performance ofthe useršfor example, by presenting information more slowly andclearly and, if necessary, warning the user that he or she needs rest.such tools ideally would operate on platforms across the wide scale ofcomputing and display capabilities (e.g., processing, storage, andbandwidth) available in crisis management.9. geographic information systems that are more capable than currentones could integrate many data formats (which would promotecompetition among information providers) and many types ofinformation (such as pictures and realtime links to sensors). integrationshould include registering and incorporating these data types fully intothe coordinate representation and relational model of the gis, not justappending markers to points on maps. the gis should more naturallydisplay crucial factors that are not currently shown clearly, such asuncertainty of data points. finally, it should be affordable and usable forfield workers with laptop computers.summary and findings: research for nationalscale applications105computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.hesitancy to share private data, which relates to the lack of reliable, inplace mechanisms for ensuring privacy and payment for those data. therefore,applications require both efforts focused on specific needs and a broadlydeployed information infrastructure, including services that help people andtheir tools to achieve integration, and standards and architectures that provideconsistent interactions between elements at all levels.information infrastructure, of course, does not spring into existence from avacuum. the workshops reinforced the observation that in crisis managementand other nationalscale applications, diversityšof people, organizations,methods of working, and technologies (e.g., databases, computers, software)šimpedes creating national architectures from scratch. (see box s.2 in thechapter "summary and overview" for further discussion.) although it might bepossible to imagine a single, uniform architecture that met crisis managers'needs for communications interoperability, data interchange, remote access tocomputation, and others, deploying it would not be practicable. the technicalchallenge of incorporating legacy systems into the new architecture would slowsuch an effort. in addition, many public and private organizations would have toagree to invest in new technologies in concert, but no single architecture couldconform to all organizations' needs and work modes. retraining andreorganizing organizations' processes to accommodate new systems would taketime. finally, crisis management illustrates that even a coherent architecturecreated for one domain would be called upon to integrate in unexpected wayswith other domains.therefore, there is a need foršand the steering committee's findingsaddressšresearch, development, and deployment efforts leading both toconsistent architectural approaches that work on national scales and to generalpurpose tools and services that make ad hoc integration of existing and newsystems and resources easier. specific applications, such as those listed inbox 3.2, should serve to test these approaches, to advance key technologies, andto meet important application needs. the organization of the findings reflectsthis view that both applicationtargeted and broader infrastructural research isneeded. finding 1 emphasizes the importance of experimental testbeds as acontext for coordinating the crucial interplay among research, development, anddeployment in one important and challenging application area, crisismanagement. finding 2 highlights the value of investigating the features ofexisting nationalscale architectures to identify principles underlying theirsuccesses and failures. these findings are discussed in the section "technologydeployment and research progress."the remaining findings identify architectural concerns that representtechnological leverage points for computing and communications researchinvestments, the outcomes of which could benefit many nationalscaleapplications. the research underlying these findings is discussed in greaterdetail in chapter 2. the findings abstract the common threads among thenetworking, computation, information, and usercentered technologies ofchapter 2 to indicate highprioritysummary and findings: research for nationalscale applications106computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.applicationmotivated research needs that cross multiple levels of computingand communications. there is necessarily some overlap in the research issuesdiscussed in these areas, because some technological approaches can contributeto meeting more than one architectural objective. these findings are presentedin four subsequent sections: support of human activitiesfinding 3: usabilityfinding 4: collaboration system composability and interoperabilityfinding 5: focused standardsfinding 6: interoperabilityfinding 7: integration of software componentsfinding 8: legacy and longevity adapting to uncertainty and changefinding 9: adaptivityfinding 10: reliability performance of distributed systemsfinding 11: performance of distributed systemsoutcomes of testbed and architecture study activities (see findings 1 and2) can and should inform future reexamination of findings in these architecturalareas, which represent the best understanding of a range of technology andapplication experts in 19951996.the findings frame research derived primarily from addressing therequirements of crisis management. however, the steering committee believesthat such research would have much broader utility, because of the extremenature of the demands that crises place on technology. in addition, many of theresearch directions relate to increasing the capabilities of informationinfrastructure to meet extreme demands for ease of use, integration, flexibility,and distributed performance, which will benefit any application using it. thefindings are illustrated by practical needs identified in the workshops andexamples of specific directions that researchers could pursue. these suggestionsare not intended to be exhaustive, nor are they presented in priority order;deployment and experimentation are required to determine which approacheswork best. however, they are promising starting directions, and they illustratethe value of studying applications as a source of research goals.technology deployment and research progressthe workshop series focused on applications partly in the recognition thatcomputing and communications research and development processes depend onthe deployment and use of the technology they create. this is true not only in thesummary and findings: research for nationalscale applications107computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.sense that efficient allocation of research investments should lead ideally toproducts and services that people want, but also in the sense that it is ultimatelythrough deployment and use that technologists can test the validity of theirtheories and technical approaches. this is not a unique recognition; it fits withina stream of recent analyses, including a strategic implementation plan of thecommittee on information and communications, america in the age ofinformation  (cic, 1995a) and the computer science and telecommunicationsboard review of the high performance computing and communicationsinitiative (hpcci), evolving the high performance computing andcommunications initiative to support the nation's information infrastructure (cstb, 1995a). the opportunities that the steering committee's first twofindings identify for learning from study of deployed technologies, however,have not received extensive attention to date.box 3.3 crisis management testbeds: relationshipto previous visionsthe committee on information and communications (cic) has called for "pilotimplementations, applications testbeds, and demonstrations, presentingopportunities to test and improve new underlying information and communicationstechnologies, including services for information infrastructures." (cic, 1995a, p. 8)the cic plan anticipates these testbeds and demonstrations as fitting into threebroad classes of userdriven applications related to longterm national science andtechnology council goals: highperformance applications for science and engineering (modeling andsimulation, focused on the grand challenges); highconfidence applications for dynamic enterprises (security, reliability, andsystems integration issues); and highcapability applications for the individual (education, digital libraries, inhomemedical information).the crisis management application area bridges all three of these classes. atestbed or technology demonstration that supported distributed training andplanning exercises in crisis response, for example, could incorporate modeling ofnatural and manmade phenomena in real time; secure and reliablecommunications; interoperability and integration of existing information resources,such as public and commercial databases; and adaptive, usercentered interfaces.evolving the high performance computing and communications initiative tosupport the nation's information infrastructure, a comprehensive review of the highperformance computing and communications initiative conducted by thecomputer science and telecommunications board (cstb, 1995a), also supportedthe notion of emphasizing nationally important applications (what the initiativecalled national challenges) to test and guide the development of basicinfrastructure:summary and findings: research for nationalscale applications108computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.recommendation 8: ensure that research programs focusing on the nationalchallenges contribute to the development of information infrastructure technologiesas well as to the development of new applications and paradigms. the nationalchallenges incorporate socially significant problems of national importance that canalso drive the development of information infrastructure. hardware and softwareresearchers should play a major role in these projects to facilitate progress and toimprove the communication with researchers developing basic technologies for theinformation infrastructure. awards to address the national challenges shouldreflect the importance of the area as well as the research team's strength in boththe applications and the underlying technologies. the dual emphasisrecommended by the steering committee contrasts with the narrower focus onscientific results that has driven many of the grand challenge projects.1testbeds for computing and communications technologies to aid crisismanagement would support the dual focus on applications and infrastructure byemphasizing the participation of crisis managers and technology experts in limitedscale deployments for training, planning, and to the extent practical, operationalmissions.1 the 1995 cstb report concluded that the national challenges defined in the highperformance computing and communications initiative were too broad to offer specifictargets for largescale research, and therefore, "the notion of establishing testbeds for acomplete national challenge is premature. instead, research funding agencies should regardthe national challenges as general areas from which to select specific projects for limitedscale testbeds. . . ." (cstb, 1995a, p. 59).the crisis management testbed described by finding 1 of this current report should beunderstood as an intermediate stepšsomething larger than a single research project as the1995 cstb report implied, but not a complete, nationwide crisis management system.finding 1: crisis management testbedstestbeds and other experimental technology deployments can enableresearchers and technologists to develop and test technologies cooperatively ina realistic application context (see box 3.3). they can serve as a demandingimplementation environment for new technologies and sources of feedback toidentify and refine research objectives. such testing is particularly important inprogressing toward deploying nationalscale applications, in order to verifytheoretical concepts about the scalability of system characteristics,interoperability with other systems, and usability by people in realistic situationsšall of which are difficult or impossible to predict in the laboratory.test projects and technology demonstrations are under way in mostnationalsummary and findings: research for nationalscale applications109computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.scale application areas (nstc, 1995). however, in civilian crisis management,additional government funding and leadership are required if the research anddevelopment benefits of these activities are to be realized. crisis management isprimarily a public service function, led and funded by government agencies.the relatively small operational budgets of federal, state, and local emergencymanagement agencies do not include significant research and developmentfunding. the small size of the commercial marketplace for computing andcommunications in civilian crisis management, in comparison to areas such ashealth care, medicine, commerce, and manufacturing, may limit the likelihoodof largescale industry investment in a testbed effort. greater public investmenthas been made in the military context, but much of this is related to commandand control in warfare, which overlaps with nonmilitary applications onlypartially. the jwid '95 (joint warrior interoperability demonstration of 1995,discussed in chapter 1) military exercise was an exceptional case inincorporating civilian participation in a nonwar crisis response. jwid '96 willnot repeat this activity, and no analogous largescale exercise exists for civiliancrisis management organizations to test and experiment with technologies on asimilar scale.testbeds should provide a context for the participation of crisismanagement practitioners (such as the federal emergency managementagency (fema) and state, local, and nongovernment emergency servicesorganizations) in system testing and development. application users' input isessential to assess the fit among systems, tools, and the needs of users andorganizations and to ensure that technology is focused on usable, practicalsolutions. in addition, the workshops suggested that testbeds could make adirect contribution to crisis management by supporting realistic training andsimulation. training is crucial to effective crisis management. workshopparticipants from the crisis management community made it clear that to beuseful, training exercises must be realistic. highfidelity simulations (includingdistributed simulation across networks) and field exercises in conjunction withtechnology demonstrations could help provide this realistic training. realisticscenariobased "whatif" simulations could also be a useful tool to testoperational plans and choices.1. establishment of one or more technology testbeds for computingand communications systems in crisis management would be valuable. inthese testbeds, government, academic, and industrial researchers shouldwork with application usersšcrisis managersšto test and validatetechnologies under research and development by subjecting them todemands in realistic applications, such as training and planning missionsfor civilian and military crisis management personnel through simulationsand field exercises and, potentially, actual field operations.summary and findings: research for nationalscale applications110computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.finding 2: studies of existing nationalscale informationinfrastructurecrises can occur anywhere, at any time, and their management andresolution may require expertise, information, and resources from around theworld. the unpredictability of crisis locations and sources of aid leads quicklyto the idea that what is needed is an infrastructure that increases the chance ofharnessing farflung information and computing resources on short notice toaddress a problem, anytime and anywhere they are needed. what are thetechnical characteristics that make it possible to create or leverage largescaleinformation infrastructure? how and where should these characteristics bedeployed within the information infrastructure to enable ubiquity in the servicesit supports?2examples of narrowly focused but successful information infrastructurethat functions on a large scale include bank electronic funds transfer systems(e.g., check clearinghouses), automated teller machine networks, airlinereservations systems, and airline communications systems such as the sociétéinternationale de télécommunications aéronautiques(sita). more general systems include the internet and applications itsupports, including electronic mail and the world wide web. what aspects ofthese systems have led to their ability to scalešin terms of their complexity andthe quantity and geographic distribution of their end pointsšand toaccommodate diverse components and evolve over time? these and relatedquestions should form the basis for an exploration of architectural imperativesfor nationalscale systems.3as the workshops unfolded, it became apparent that many existing, largescale computing and communications systems could be studied to understandbetter the general architecture of information infrastructure that can scalesuccessfully to a national or global level. in this finding, it is proposed that ananalytical research effort to study the design and operation of these systemscould produce valuable results by exposing architectural and design featuresthat contribute to successful operation and might be generalized to apply tomany, if not all, information and communications systems.many benefits for development of new systems could arise from betterunderstanding of existing systems.4 for example, in crisis management, accessto specialty databases is often crucial; examples include hazardous substancedatabases, the general databases of the national library of medicine, databasesat the centers for disease control and prevention, census databases, corporatehuman resources records, patient treatment databases, and the like. how arethese databases organized and do methods exist that would permit them tointeroperate more smoothly in a crisis? replacing lost local communicationsresources is another common factor in crisis situations. are there designprinciples for largescale infrastructures that would support integration ofrapidly deployedsummary and findings: research for nationalscale applications111computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.emergency communications facilities with existing resources? could anelectronic "emergency lane" be made to work in some fashion? how cannational and regional sensor networks (such as the oklahoma mesonetwork andnexrad (next generation weather radar) be integrated easily into otherinformation processing systems? how can this be done on short notice in crisissituations? relevant research should draw not only on developers ormaintainers of major existing systems, but also on communities that typicallyuse the systems.2. existing systems that provide widely accessible computing,communications, and information resources for applications such aselectronic commerce, health care support, manufacturing, air trafficcontrol, electronic reservations, and public information disseminationshould be examined to identify and understand the critical features thatmake systems scalable, reliable, and broadly usable. the object is toimprove understanding of what information infrastructures are, whatcomponents they should include, how they should be structured, whatservices they should provide, and how they serve the needs of particularapplications.support of human activitiesin nationalscale applications, people are a critical componentšhumansare "in the loop." discussions with experts in a number of application domainsrevealed that although support of peopleboth as users and as integral parts ofthe system designšis of primary importance, this need often gets too littleemphasis in system designs. to the extent that user roles do receive attention, itis frequently in terms of traditional views of information technology usage in atooluser relationship, stressing easytouse humanmachine interfacetechnologies such as speech recognition and graphical displays. these areimportant and appropriate subjects for continued development; however,independent development of individual technologies will not extend the utilityof information technology to the extent that nationalscale applications require.instead, these applications require an integrated approach to system design thatrecognizes that, increasingly, people and technologies work together as parts ofa larger system; both sides provide inputs and add value. the challenge ofintegration complicates research design and adds to the difficulty of achievinguseful results.an example is an information system that integrates information fromsensors, databases, and field workers and supports processing of the informationby both computer models and human analysts. people affect the performance ofsystems both positively and negatively, and system designs should seek toimprove and extend human performance. as components of systems, peoplemay provide information; they may direct the activities of computerapplications; they may check or verify that decisions or conclusions made byapplications are reasonable; they may provide decisions when the applicationsreach an impassesummary and findings: research for nationalscale applications112computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.and cannot make decisions. people may make errors, particularly when understress. the activities and needs of humans who are not computer specialists, butrather, specialists in their own application areas, must be considered as acomponent of any system architecture.5finding 3: usabilitycrisis management requires systems that people can use well without agreat deal of specialized expertise. improving the usability of systems for crisismanagement, as well as other nationalscale applications, faces manychallenges. users are often geographically dispersed and have widely differingskills. humancomputer interaction technologies that will be deployed innationalscale applications cannot depend on highperformance computing andcommunications being available to all users; technologies must scale acrosswidely varying systems and infrastructures. they must also adapt to a variety ofneeds. usability is more than a technical issue. people use technologies that aredeployed in the context of a social organization, and usability depends onmeeting the user's needs and adapting to the user's capabilities in the context oforganizational and social structures and objectives.basic preferences of users and their styles of interacting vary greatlydepending on domainspecific practices, training, skill level, and the situation.a doctor on rounds may be accustomed to dictating observations, readingcharts, and writing prescriptions. speech recognition, ready access to sensorsupplied patient information and xrays, and penbased devices are examples oftechnologies and devices that, when integrated with patient and health careinformation systems, not only can enhance the effectiveness of patient care andthe efficiency of the caregivers, but also can aid emergency personnel in timesof crisis to obtain immediate and current information.progress is being made on many of the component technologies andmechanisms, but they remain largely fragmented among domainspecific andsituationspecific modes of interaction. there is a need for interactive systemsthat can be tailored easily and quickly to fit the specific requirements andpreferences of users. research on modeling and understanding user needs invarious situations should guide the development of techniques for tailoringinterfaces.in crisis management, situations arise in which transportationinfrastructure has been damaged (e.g., during earthquakes). mobile emergencypersonnel need very specific information about where to go and how to getthere, taking into account the fact that status of local roadways and bridges maybe in flux or unknown. they must be able to understand the informationquickly; this is where research and experimentation with actual users understressful conditions can help. in some situations, a brief ext message may be thebest way to convey information quickly without leading to confusion; in others,a visual display suchsummary and findings: research for nationalscale applications113computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.as a map or aerial photograph may be appropriate, with the crucial locationshighlighted automatically. moreover, the information flow must be twoway. ifa road is not where the database says it should be or a bridge has been knockedout, the mobile user should be able to mark that information on a handhelddisplay and disseminate it to update other users about the situation. appendinga spoken annotation to the data, for example, could be an effective way toenrich the information value added. implementing such technologies willrequire new presentation and format standards that link multiple data typeseasily, while conserving scarce radio bandwidth. the development of standardsshould be based on clear understanding of what factors matter most to users.deployment in realistic crisis exercises and actual crises should serve as ameans of exposing these factors and quantifying their value.the granularity and specificity of information needed by different users atdifferent times can vary greatly, as can the capability of users' equipment (e.g.,display, storage, processing). thus, technologies and capabilities to deploymultilayered data views derived from many sources to users with limitedresources are needed. how this can be accomplished in one context is illustratedby the university of oklahoma center for the analysis and prediction ofstorms' weather model discussed in chapter 2: by zoomingin on one area of alarger picturešthe state weather mapšdetailed, very specific information canbe developed (e.g., local, timespecific thunderstorms). mobile users obviouslydo not have the computation capacity available for this type of weathermodeling or even to display the complete results of a regional simulation.therefore, it would be valuable to be able to focus weather models on the scaleand location of greatest interest to the crisis manager and rapidly present theinformation he or she most needs.people are error prone, particularly if they are tired and under stress as iscommon in crises. depending on the application and instance, human errorsmay be acceptable or disastrous; in either case, they often are not consideredadequately in system designs. applications should be able to cope withreliability problems caused by errors or failures in any component, including thehuman component. for example, adaptive mechanisms are needed to elide andcompensate for errors and, ideally, to associate and propagate confidencefactors with information that other users and applications can interpret.systems should also account for the obvious strengths that humans bring toapplicationsšfor example, unparalleled inferencing skills and vast arrays ofexperience and knowledge. because crises overload people with demands ontheir time and attention, their strengths should be applied where they are mosteffective. formally defining complex problems in a way that identifies whereuniquely human abilities are most effective would be a significant advance.security is another usability consideration that is particularly important insummary and findings: research for nationalscale applications114computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.crisis management. for example, if crisis team workers are setting up a networkfrom the local police radio communications network and a local hospitalnetwork, the security mechanisms associated with the combined network mustreflect the existing security policies of both parts, yet allow authenticatedemergency workers access to appropriate information. this must happen rapidlyand may have to be implemented by nonexperts in security, which implies astrong need for highly usable network security management tools.3. research is necessary to gain a better understanding of users' needs,capabilities, and limitations in working with computing andcommunications technologies in diverse contexts. this understandingshould be used to develop principles for optimizing the effectiveness ofpeople in the overall performance of applications in which humans andmachines work together.suggested research topics: enabling of natural, interactive communication across a variety of devicesand mechanisms used by individuals with a wide range of skills, needs,and financial constraints (which directly affect available computing power,displays, etc.) should be pursued. in building on streams of researchalready under way in many areas, such as speech and languagerecognition, graphics and visualization, humancomputer interaction,human factors, organizational behavior, and other subdisciplines, aprincipal multidisciplinary research challenge is to integrate approaches(e.g., multimodal interfaces combining graphics, text, speech, gesture) tomeet specific needs and conditions. information management tools are necessary to fuse data from multiplesources, filter information from a potentially overwhelming flood ofinputs, integrate it, and present the most crucial information to users undersevere time pressures, distractions, and other stresses. these shouldinclude a capability to adapt the information management processes in realtime in response to user feedback about relevance, timeliness,understandability, and other factors. visually representing the quality of information would be of value to users.it is difficult to represent the quality of threedimensionalplustimeinformation (e.g., fullmotion video). error bars may be adequate forshowing statistical variance of a data point (such as a sensor input or acasualty estimate) but are less so for indicating the probability that a datapoint has been misread accidentally or falsified deliberately. visualization of complex data and/or large quantities of information isneeded, along with interactive virtual environments that enable users toexplore the effects of various assumptions about uncertain or missinginputs and possible future courses of events.6summary and findings: research for nationalscale applications115computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.finding 4: collaborationcrisis management is collaborative. crisis managers interactsynchronously (through facetoface meetings, by telephone, and so on) andasynchronously (through email, voice mail, and information services). theyshare and organize information, contributing to information stores shared byothers, and aggregating and otherwise adding value to diverse pieces ofinformation to enable that information to be a basis for planning, decision, andaction. they share work flow, with deadlines for decisions and plans, schedulesfor actions, and precisely timed operational activities.in crisis management, collaboration is currently dependent on humantransportationšbringing together the key players so they can interact facetofacešand on the telephone. the most common vehicle for collaboration is thesituation room, serving as a physical locus for incoming information andoutgoing decisions and plans. this creates delays and inefficiencies, particularlywhen key players are geographically dispersed or involved in multiplesimultaneous efforts. telephone and occasional video teleconferencingaccommodate physical remoteness, but at the cost of reducing the efficiency ofsharing and accessing information.collaboration, in this context, does not merely include crisis managerssitting in a situation room. it also includes a wide range of other kinds ofinteractions, from shortduration actions focused on specific decisions to longerterm efforts at gathering and collaboratively integrating information over longperiods of time. an open collaboration technology should support the full range.research in distributed computing, humancomputer interaction,information management, and other areas is beginning to create a foundation fora technology of collaboration, enabling effective interaction across space andtime. to support collaboration, research on communications models shouldaddress how users communicate with each other.7 the models must reflect notjust speech, but also many other modes of communication. they should includenot just the people collaborating, but also the information resources theyinteract with. for example, extensions to the collaborative perceptual spacecould include not just videoconferencing, but also a rich synthetic spaceincluding visualizations of terrain, buildings, and participants in the crisisresponse. this kind of shared perceptual space would enable rapid planningiteration, including simulation.the information space shared by collaborators could include databases,documents, working notes, and other material that crisis managers need toshare. commercial groupware, world wide web technology, and some toolsassociated with the internetbased multicast backbone (mbone) constituteimportant initial steps, but there is an unmet need for an internetbased openprotocol collaboration technology that can support a very broad range ofcollaborations.summary and findings: research for nationalscale applications116computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.4. research is needed to develop concepts for new, open, networkbased collaboration tools. collaboration is essential in crisis response andmany other nationalscale applications. although networks are used tosupport collaboration, currently available collaboration tools are notadequate to suit application needs. specific tools, technologies, and openprotocols to support collaborative efforts could yield significant benefits.suggested research topics: common perceptual spaces should be developed that mix video, synthetic(virtual) environments, and information visualizations, to facilitatecollaboration among geographically dispersed participants. common information spaces should be developed that support sharing,organizing, and evolving a jointly viewed collection of complexinformation interactions. a virtual situation room, combining common information and perceptualspaces in a collaborative virtual environment, would assist users to gatherinformation, make plans and decisions, initiate actions, and monitorexecution. work flow management and judgment support tools (discussed in box 3.2)are necessary to augment and enhance the capabilities of decision makersby exploiting collaborative problemsolving infrastructures. a collaborative problemsolving environment for software development bydistributed sets of designers, programmers, and application users could aidmore rapid, higherquality development of software applications.8system composability and interoperabilitycrisis management is a firstrate illustration of the need to rapidlyincorporate and utilize a potentially globally distributed collection ofcommunications, information resources, and system components into anapplication solution. the first requirement is to gain rapid access to informationresources of many different types; the second, to integrate these resources intothe information that each user needs. computers and networks can help achievethese goals, but there is a strong need for improved ways to use existingresources (including both old, legacy resources such as city maps and new onessuch as the latest satellite imagery from the world wide web) and tocommunicate with existing services. research aimed at generic tools andservices for integrating such systems is discussed in this section. however,scaling to national and global levels requires more than ad hoc approaches, sothis section also addresses research toward more forwardlooking architecturalsolutions.because crisis management places a premium on information access andintegration, workshop discussions focused on composing information systemsfrom multiple sources. however, other types of networked computing systems insummary and findings: research for nationalscale applications117computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.nationalscale applicationsšdistributed transaction processing and largescalecollaboration, for examplešare increasingly interconnected and composedfrom many parts. the world wide web, for example, is a platform acrosswhich information, communications, and distributed software applications allwork together.crisis management requires that information cross heterogeneous interfacesša translation problem. for example, data files must be translated from oneformat to another; handheld radio signals must be translated between standards.these translation problems are what is usually meant by ''interoperability."interoperability between systems is crucial for crisis management, but it is notenough. truly collaborative solutions require the ability to compose or integratesystems. for example, an information system that must perform searches acrossdifferent databases with different structures requires a closer integration withdeeper understanding of semantics than the term "interoperability" often implies.semantic composition implies that the user and resource or service havesome deeper level of understanding and agreement about an invocation orcommunication. semantic composition requires agreement about the meaningsof functionality across interfaces. for example, there are different syntacticformats for geographic information systems made by different vendors, such asarcinfo and mapinfo. files made for one must be translated to be used by theother, but both share the semantic idea of latitudinal and longitudinal coordinatereference and ideas such as "x is adjacent to y" and "x surrounds y."finding 5: focused standardsin a crisis, many diverse information and communications resources haveto be brought together. because the diversity of resources available overnetworks is increasing rapidly, they must be linked together by standardprotocols and other elements. without widespread agreement on criticalcommon elements, systems cannot scale up, and nationalscale applications willbe unworkable.at the network level, the internet suite of standards is a particularlycompelling example of how wellchosen technical standards create a powerfulavenue for interconnection and interoperation. the internet protocols servemany functions, such as domain name management, addressing, packettransmission, reassembly, and fault tolerance. they are organized into relativelysmall component elements, enabling users to select particular elements of theinternet suite without having to use the entire set. one example of how thispromotes flexibility is that many firewall systems transmit only certain kinds ofpackets and respond to certain kinds of protocols (e.g., for electronic mail, butnot for file transfer). this flexibility has enabled the internet standards to serveas a de facto basis for many emerging nationalscale applications.in the geographic information systems (giss) widely used in crisissummary and findings: research for nationalscale applications118computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.management applications, a small set of critical commonalitiesšsuch as thelatitude and longitude coordinate system and depiction of features such as roadsand rivers as arcs between pointsšare accepted widely by a broad communityof users and tool vendors. although translation between different vendors'proprietary data syntaxes is still necessary, these commonalities makeinteroperation and sharing through gis possible. for example, theconsequences assessment tool developed by fema and the defense nuclearagency (discussed in chapter 1) integrated the outputs of weather models,buildingstress analysis models, and nuclear explosion blastpressure modelswith databases from the census bureau and other sources to project the impactsof hurricane emily on neighborhoods in its path. these components had notbeen designed to interoperate nor were they developed to be part of a crisismanagement solution. the interoperation was accomplished by accepting asingle geographic coordinate system representation.there are, however, many areas in which multiple standards exist thatoverlap in function, yet whose service models may not be entirely consistent.for example, a variety of standards apply to different ways of querying andaccessing information. examples include structured query language (sql; foraccessing relational databases), common object request broker architecture(corba; for accessing structured objects), object linking and embedding(ole; for accessing distributed structured documents), and american nationalstandards institute (ansi) standard z39.50 (for digital library informationretrieval). although these services have different functions, there is nonethelesssignificant overlap among them. interconnecting these services (e.g., to providea single client interface for retrieval and presentation of information objectsrelated to a particular application) presents formidable interoperability problemsat multiple levels. existing interoperability solutions are often ad hoc and do notscale well. although finding 6 suggests there may be ways to address theseproblems for current technologies, it is also important to consider newapproaches to protocol design in the future that will be more resistant toproliferation and divergence of protocols.the world wide web provides good examples. while there is a widediversity of clients (browsers) and servers, these diverse components are unifiedby a set of common protocols and formats, such as hypertext transfer protocol(http) for connection management, uniform resource locators (urls) toname internet information resources, and hypertext markup language(html) to describe web pages. these protocols enable interchangeability andinteroperation among the increasingly diverse set of web tools andtechnologies. however, the complexity of the html language has increasedrapidly, and there are now multiple versions in simultaneous use and increasingnumbers of browserspecific web pages.these examples highlight the need for a "focused standards" approach, inwhich individual standards are narrowly focused on specific requirements ratherthan locking in a large, interrelated set of requirements. they should be modularsummary and findings: research for nationalscale applications119computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.and composable in the same way system components are composable. thisfacilitates adoption and offers more flexibility for vendors to compete and forintegrators to evolve systems. for example, in the internet, this approach hasalready led to significant success in achieving interoperation among datacommunications networks atop which many applications run. workshopparticipants and others have argued that the internet standards are successfulbecause they follow a focused, minimal approach (cstb, 1994b), with nosingle standard incorporating all the functions of internet protocols andservices, but instead a large suite (transmission control protocol (tcp),internet protocol (ip), file transfer protocol (ftp), http, and many others)allowing more flexible interaction with other services and standards than a largeset of interlocked "allornothing" standards would have done.technology that supports this kind of plugandplay approach betweenapplications could be called "component middleware."9 componentmiddleware technology is just beginning to emerge, and research should aim toaccelerate its development. for example, although various developers haveproduced application programming interfaces (apis) and libraries of enablingsoftware (such as enabling software for databases distributed across a variety ofnetwork substrates), it is not yet apparent that they can interact in a modularplugandplay fashion on a large scale. apis introduce constraints on thecommunications between computing components, and collections of apisdrawn from various sources may not be compatible: even if they allow syntacticinteroperation, the semantic meaning across the apis may be incompatible.development of general principles for middleware design and implementationthat enable the creation of focused, modular standards could contribute greatlyto the likelihood of successful interoperation of two components that had notinterworked before. it is not yet clear what those general principles might be,but such an approach is clearly more likely than trialanderror efforts to workwell on a national scale.two kinds of research are needed: service protocol design and protocoldesign principles. service protocol design research focuses on creation ofprotocols through which new kinds of services can be delivered. serviceprotocols effectively insulate users of the service from details ofimplementations and their continued evolution in capability, and they insulateservice implementors from details of how the services are used in particularapplications. this lessens dependence, facilitates service evolution, and therebystimulates greater competition for supply of services and consequent growth incapability. protocol design principles are needed to guide designers of theprotocols through which these services are delivered and provide them withsome confidence that a new protocol supporting a specific service will interacteffectively with other service protocols already in use or emerging. this notionof composition of information services is analogous to concepts related to thecomposition of software components.summary and findings: research for nationalscale applications120computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.once services become broadly commercialized, the dependence ofmultiple organizations on specific protocols and standards makes evolution ofthose standards difficult. therefore, early and active involvement by researchersin developing prototype protocols for new service concepts can significantlyhasten the acceptance and growth of these new services. any research programthat involves creation of new service concepts should also involve creation,promotion, and evaluation of associated protocols. researchers developing newconcepts for information services should consider protocol design issues fromthe outset, since the protocol design will be the most likely instrument forscaling up the concepts and moving the technology into broader practice. whennew protocol concepts are unencumbered by dependence on specificimplementation or platform details (i.e., keeping them focused and minimal),they are more likely to be accepted and to serve as a basis for growth.5. research is needed to identify design principles that can yield openstandards (such as communications protocols and applicationprogramming interfaces) that interact well with other related standardsand allow for diversity and evolution. individual research and developmentefforts aimed at setting standards should focus on more narrow componentfunctionalities or services, rather than promote aggregation into largermultifunction standards.suggested research topics: service protocol designs must be developed with the participation ofresearchers. areas in which new service protocols are being developedinclude multimedia (e.g., representation of various multimedia and virtualreality objects, compression, metadata for indexing and search), databasedistribution (e.g., distributed queries, object management, agents),collaboration technologies (e.g., videoconferencing, shared virtual spaces,information sharing), and distributed or "migratory" software systemsconstructed from components available over a network (e.g., the javaprogramming language). many of these protocol development efforts arenow at the point of commercial standardization, but many others are still innascent stages, and it is in these cases that participation from the researchcommunity can have significant benefit. protocol design principles should be identified. results of research onprotocol design principles would be in the form of design principles thatservice developers can apply, along with reasoning tools that could be usedto assess critical characteristics of protocol design, for example, freedomfrom deadlock and avoidance of emergent phenomena as use scales up.10summary and findings: research for nationalscale applications121computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.finding 6: interoperabilitythe assembly of resources in a crisis includes formation of teams,configuration of communications systems, and interlinking of data resources. itcan also include interconnection of tools for planning, decision, visualizationand presentation, and work flow management. the diverse players in crisisresponse are reaping significant benefits from their growing reliance oninformation technology, and that reliance creates opportunities for effective andrapid integration of resources. however, it also presents significant technicalchallenges related to interoperability.for example, in hurricane andrew relief operations, enormous effortswere required to incorporate data from county tax assessor registers into thegis coordinate system being used by relief officials. in the aftermath of theoakland hills fires, integration of location data from portable global positioningsystem (gps) sensors with paper maps from the local utility proved valuable,but it was accomplished manually.david kehrlein, of the office of emergency services, state of california,noted that many of the data gathered at relief stations following the northridgeearthquake, such as names and locations of injured survivors, were verydifficult to integrate into a coherent, overall picture. had the relief stations beenusing any of a variety of commercial personal computer databases, there wouldhave been the problem of integrating data from those formats into the mapbased gis databases used for command and decision making at higher levels. infact, many of the data were not even in databases, but in word processors; toofficials at the station, that was a "database." these data had to be integratedmanually by crisis managers even to yield an accurate overall listing of thelocations of survivors, much less to correlate the data with a gis representationof the disaster area or a logistical information system for ordering and allocatingsupplies.workshop participants identified multimedia data fusion as a valuableresearch area. this could provide capabilities for integration based on taggingof amateur and professional video with location and time of the images, alongwith realtime sensor (e.g., atmospheric, seismological) data, keyed to a gisrepresentation of the crisis area. these data sources are more diverse than in thecase of multiple formats for essentially similar database or word processingapplications. significant research challenges relate to the difficulty of fusingdata with different formats and access protocols, some with fundamentallydifferent kinds of metadata (time, location, image resolution, sensor networkscale).the interoperability challenge is particularly acute in crisis managementbecause of stringent deadlines and the inability to anticipate fully the range ofresources that must be made to work together. of course, planning andcoordination before crises occur can mitigate problems of interoperation tosome degree. however, the planning and coordination have two roles. the firstand moresummary and findings: research for nationalscale applications122computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.obvious role is to reduce the relative proportion of unanticipated cases. thesecond role, which has potentially far greater leverage, is to create generalmechanisms for interoperability and rapid integration that can lower the cost(and uncertainty) of dealing with those unanticipated cases. these factorsshould certainly be incorporated into the design of any nationally accessibleinformation repository for crisis management, such as the national emergencymanagement information system described by john hwang, of fema. arepository should include not only information for which the need can beanticipated, but also mechanisms for locating, obtaining, interoperating with,and integrating other, unanticipated information sources.it must also be recognized that information systems and softwareapplications are rarely developed from scratch. legacy software and databasespersist and are the key assets of many enterprises, even in cases where littlerecord remains of the details of their implementation or design rationale. theincreasing use of these resources in an interconnected environment raises thestakes for interoperability.the result of the above factors is that interoperability challenges areincreasing, and any technology specifically focused on the problem could havea significant impact. solutions to interoperability problems need not always bead hoc, and research focused on this problem may have great impact. forexample, wrapper and mediator concepts (see box 2.4) were initially usedentirely in ad hoc ways, crafted by hand to solve specific problems as theyarose. researchers are starting to develop more generic methods (e.g., usingdata mining and other artificial intelligence techniques) that could lead to toolsfor semiautomatic generation of intermediate components such as wrappers andmediators. the problem is exacerbated by the increasing diversity of datamodels and new ways to manage (e.g., name and share) complex object types.however, even very simple approaches to allow sender and receiver tocommunicate a shared identification of types, such as the multipurpose internetmail extension (mime) hierarchy used for structured email documents, havebeen seen to have considerable benefit. as this report was being prepared,designers of languages for networkcentered computation such as java were stillstruggling with how to manage the name space for metainformation (types),and groups involved with webrelated technologies were still working out howthe name space for web objects is to be organized.6. specific research efforts should be undertaken to develop generictechnology that can facilitate interlinking of diverse information resources.nationalscale applications present new challenges to interoperability andintegration of information resources. emerging ideas could yield advancessuch that interoperability problems need not always be met with ad hocsolutions.summary and findings: research for nationalscale applications123computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.suggested research topics: while ad hoc approaches to wrappers and mediators have long been in use,generic approaches should be developed that rely on consistent knowledgerepresentation models. research efforts should be undertaken to assess forwhat kinds of information resources such generic technologies are feasible. methods are needed to better support the explicit identification andmanagement of metainformation about information resources such asinformation about types, data models, schemas, and metadata in networks.metainformation provides a basis for identifying how information objectsare to be interpretedšfor example, what coordinate systems are used?what is the quality of the data? without this basis, reverse engineering isrequired before information resources can be integrated successfully intocomposite systems. advances in data fusion of multimedia information from sources such assensors, relief officials, and amateur citizens are necessary. the usefulnessof automatic tagging of inputs (e.g., tagging video images with locationand time, as they are generated) should be evaluated.finding 7: integration of software componentsrapid response to a crisis may involve integration of applications that werenot originally intended to be used together. crisis management most ofteninvolves the need to compose data from different information systems, althoughintegration of software applications such as fema's consequences assessmenttool (which incorporates federated simulation models and databases) and mesoscale weather models can also be involved. however, the need forcomposability generalizes to almost every nationalscale application area asnew systems must be assembled or modified in response to new requirements.for example, design for manufacturing typically involves composing manyprograms (sometimes numbering in the thousands) that are used for parts of acomplex design. these programs are typically legacy systems whose code willnever be rewritten.11 compounding this problem is the unwillingness ofcollaborating competitors to share source code.the ability to compose new solutions out of existing parts is needed tocontrol costs, reuse legacy code, meet competitive timetomarket requirements,solve crisis problems, manage complexity, and reduce programming effort.these depend in part on standards to support interoperability between softwaresubsystems, but more fundamentally on an ability to predict or understand theproperties of large software systems.currently, composability is often implemented in an ad hoc way. researchon more broadly applicable methods is needed. solutions are likelymultifaceted, encompassing a variety of technologies, including applicationprogramsummary and findings: research for nationalscale applications124computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.interfaces, standards, wrappers, data fusion, cataloging, registering, andcommon object models. a framework for composability might include acommunications model, a distributed computation model, an interface definitionlanguage, interface generation tools, protocol or interface translation, anegotiation protocol, and communications facilities among layers of abstractionwithin and between applications, the infrastructure, and the networks.the java language and programming environment serve as a clear exampleof an environment that was designed from the ground up with these sorts ofmodels, in an effort to ensure composability. to incorporate legacy systems innew architectures is more difficult, and there is a need for an infrastructure withenough standardization to allow interoperation, but not so much as to stiflegrowth. the ability to address this need likely varies among application areas.for example, civilian crisis management applications are likely to pull togethercomponents developed for other purposes. other industries such asmanufacturing are developing applicationlevel architectures for interfacespecifications, collaboration software, and metacomputing support systems.however, even in this application there are challenges: lee holcomb, of thenational aeronautics and space administration, noted, "most companies aren'twilling to change their computational infrastructure to work in partnership. soone of the difficulties is . . . trying to get . . . tools to work across differentcomputational platforms through firewalls in each company that are different."software composability techniques have the potential to improve the stateof the art in a relatively old and intractable problem area, largescale softwaresystem development. historically, the cost and complexity of large softwareprojects have led to delays in application deployment. more fundamentally,they have also produced systems that do not performšthey do not provide thefunctionality or reliability required. one avenue toward improving the ability toproduce software involves methods for composing systems out of components,including some that may not have been designed to work together, such aslegacy systems. this is a very complex problem, requiring advances in manyareas.12a very difficult challenge lies in modeling the behavior of composedsoftware systems. it is currently not possible to predict or reason about thefunctionality, performance, and correctness of most software systems; inpractice, the ability of most large software systems to meet requirements isoften not determined until the system is built, tested, and tuned. a direction forresearch that can perhaps address a subset of this problem is the identificationand design of composability propertiesšproperties of software components thatalso characterize systems composed from them. unless these properties arechosen carefully, it is typically impossible to predict whether a composedsystem will have the properties of its components. this is why, for example,security or fault tolerance of software systems must virtually always bereevaluated when they are combined into larger systems, even if all thecomponents are individually securesummary and findings: research for nationalscale applications125computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.or fault tolerant. better understanding of composability properties mighteliminate this need in some circumstances.7. to reduce both cost and time, nationalscale applications oftenrequire construction of software systems from components that alreadyexist or are provided by different suppliers. research is needed oncomposability of software systems, including ways of predictingperformance, reliability, and other features of composed systems.suggested research topics: programming models must be developed that facilitate interoperable,composable system construction, as well as prediction and reasoning aboutthe scalability, performance, and correctness (conformance to specifiedoperating parameters) of the resulting system throughout its life cycle. research should address creating a capability for virtual secure groupsacross different computational platforms. active software objects that users access across networks can providecomputing and communications functions across networks, provided theyare constructed according to a model that enables them to integrate witheach other and with existing applications. for example, the java languageprovides a framework of assumptions within which new functionalities canbe provided as small, relocatable software components called applets. theability of these assumptions to remain true when deployed on a nationalscale is yet to be determined and is worthy of research attention. tools, frameworks, and infrastructure mechanisms are necessary tocomplement current work on composable, reusable objects. examples oftools are registries and locators; examples of infrastructure mechanisms aredynamic, distributed linkers and loaders.finding 8: legacy and longevitycrisis management places a premium on using known, stable resourcesand avoiding surprises, because there is no time for training or learning newtools during a crisis. as james beauchamp, of the u.s. commander in chief,pacific command, observed (see chapter 1), "the last [communicationequipment] i need in a time of crisis is something i have never worked withbefore." this statement underscores the premium organizations place onmaintaining the usefulness of resources that represent a significant investmentof time, understanding, and money over their life cycle and cannot beabandoned lightly. these resources (e.g., radios, maps, databases, wordprocessors) and the infrastructure they rely upon should be designed so they canremain accessible and, ideally, cansummary and findings: research for nationalscale applications126computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.evolve and incorporate new technologies and services throughout their lifetime.the capacity to adapt and evolve is a necessary feature of the longlived bodiesof information that are central to many nationalscale applications.for example, important text documents containing information needed in acrisis may outlive the word processing or desktop publishing software throughwhich they were created. these documents can always be preserved as imagesor printercoded files (such as postscript), but use of these representationssacrifices access to the content (e.g., for indexing and searching) as well asmutability (to make revisions or to take advantage of new features madeavailable in newer versions of word processing software, for example).compounding this further is the increasing proliferation of compression andencoding formats that make even basic textual information such as asciicharacters unreadable if knowledge about the formats is lost.a consistent, lasting way of associating documenttype information withlonglived documents would enable, for example, servers to be made availableon networks that can interpret documents of various types and, depending onthe present task, translate them into other formats, search and index them, orcarry out other operations. network services may help with problems such asthe following, as well: how can data from legacy databases be preservedwithout preserving the actual database system that hosted the data? how can oldsoftware be used today, for example, through emulation or translation? cancomputeraided design (cad) data be managed over the lifetime of a majorsystem without requiring all the cad tools and their platforms to be preservedas well? clearly, the more complex the object type, the more challenging thisproblem becomes. in addition, other legacy assets may create more difficultproblems that networkbased format translators alone cannot solve. forexample, features other than functional interfaces to resources may be longlived, such as locations and access control lists associated with resources.the need to support longevity is also paramount in other applicationdomains reviewed in the workshop series. medical records must be able tofollow people as they move through life and must remain useful as technologyfor creating, organizing, and managing medical information changes.13 libraries and other repositories for human expression have similar problems ofevolving representations and abstractions of objects (e.g., books, paintings,indices). at workshop i, david jack, of the boeing company, expressed thesame concern in reporting that boeing must keep available in an accessibleform the engineering plans for every airplane they make for the lifetime of thatplane, which may be 40 years or more.the problem in all these cases is that there is a feature of hard copy thatmust be duplicated in the networked computer environment; a hard copy of apiece of information or expression continues to be usable for the lifetime of themedium. in computer systems, however, the evolution of storage technologiesmeans thatsummary and findings: research for nationalscale applications127computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.the medium may outlast the hardware or software for accessing the informationon it, leaving the information inaccessible.this problem is particularly critical for nationalscale applications becausethese applications and the data supporting them should not be bound toparticular software components, computer platforms, data formats, and othertechnological artifacts that will be outlived by the specific information beingmanaged. otherwise, application users in the future will be unable to optimizespecific technology decisions to meet their needs because they will be shackledby a legacy of old information objects and software. the constraints placed oncurrent technical options by the need to maintain access to technologiesdeveloped in the past are the essence of the technological handfromthegraveinfluence that currently restrains the evolution of many large, complex systems,such as the nation's air traffic control systems. an approach to the managementof information objects and systems architectures that is based on sound generalprinciples can prevent such constraints in the future.there are three directions in which further research is needed to addressproblems of longevityš(1) naming and addressing, (2) resource discovery, and(3) support for evolution. with respect to naming and addressing, a key problemis that information and other resources and services are mobile, and over longperiods of time anything that survives is extremely likely to move. for example,network hosts disappear or move to different locations, file systems arereorganized, and whole institutions split, merge, or move. as a result, thesituation with respect to urls, which identify the location of resources in theworld wide web, is unstable. urls contain not only the location (includingboth host name and path name within a host), but also the access method orprotocol. although the widespread deployment of the web is only a few yearsold, many urls have already become obsolete, often providing no recourse todiscover whether the information sought has moved elsewhere or is simplyunavailable.one significant direction for improvement, whose requirements wererecently defined within the internet engineering task force, is to separatenaming from addressing.14 this would involve the definition of uniformresource names (urns), a new type of name intended to be longlived,globally unique, and independent of either location or access method. these, inturn, are translated (resolved) into urls as necessary, but it is the urns thatshould be embedded in objects for longterm storage, enabling futureidentification and use. there is still significant work to be done in this domain,because the problems of how to do nametolocation resolution have not beensolved. this undertaking is larger in scale by orders of magnitude than the hostname resolution provided by the internet's domain name service, which isprobably inadequate to handle the degree of volatility and mobility needed forurns because information probably can move much more frequently thanhosts. a followon problem is that even if a service arises that scales andhandles the rate of updates more effectively, in the long run it may well fail orbe replaced. research at the massachusetts institutesummary and findings: research for nationalscale applications128computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.of technology (mit; the information mesh project) is attempting to addressproblems of allowing for both a multiplicity of resolution services and anarchitecture that provides fallback mechanisms, so that if one path to findingresolution fails, another may succeed; this is all very preliminary, however, andmore research is needed.the second part of the solution is to help users find resources. urns arenot intended to be user friendly, but rather computer friendly. because theyshould be globally unique, they are unlikely to be mnemonic or to fit into thevarious naming schemes that suit human preferences. for this, additionalresource discovery services are needed, such as keyword searching andsimilarity checking. there are some significant early efforts in this direction,15 but there continues to be a need for more sophisticated searching tools,especially as people with less computer savviness become frequent users. it isdifficult to build a local naming and search tool that is tuned to particularapplication domains or to private use. all too frequently these services point todead ends, such as outdated urls; the services should be better able to weedout bad data. in a crisis, if a search engine overwhelms the user with anindistinguishable mix of good and bad information, the overall result may beuseless.a third area, discussed further in finding 9 (''adaptivity"), relates to theability of information and other resources to evolve. although it is desirable fornew capabilities and technologies to be employed within equipment andservices (e.g., to use new, enhanced interfaces), the evolution must be smoothand easy for people and their applications to adapt to or else the newcapabilities may not be used. application designers cannot know in advance allpossible directions for evolution of useful resources, and so to supportevolution, applications and infrastructures should be designed to enableapplications to learn about and utilize new and evolving resources. the specificresearch directions implied by this need are discussed in finding 9.8. technological and architectural methods should be developed forreconciling the need to maintain access to longlived information andsoftware assets with the need to enable application users to exploit newtechnologies as they become available. such methods should be appliedboth at the middleware level and in the architectural design of nationalscale applications.suggested research topics: research is necessary to specify the minimal component services in aninformation infrastructure that allow for identifying, finding, and accessingresources, and to develop protocols for service definitions that are bothminimal in terms of needs and extensible to allow for improved service.some specific examples following the library analogy are services to helppeople determinesummary and findings: research for nationalscale applications129computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.which objects and resources they want (a service like that of a librarianwho suggests books), the registration of individual resources (e.g., libraryof congress catalog numbers), the location service (e.g., a catalog), andmechanisms for user authentication and access control policies (e.g.,placing books on reserve for students registered in a particular class).mechanisms to implement these services require, in particular, ways tomanage information about how to interpret typed information objects(ranging from documents to data in databases and software components) atthe network level.adapting to uncertainty and changea crucial problem faced by all nationalscale application areas, butparticularly crisis management, is that of dealing effectively with uncertainty inthree areas: infrastructure (e.g, networks and networkbased services such asnaming and addressing), components of integrated solutions, and the nature andbehavior of potentially useful resources. uncertainty and change are involved inall of these areas. in a crisis, changes can produce uncertainty on a scale ofminutes: are the telephone lines in the disaster area down? how soon will theybe restored? change on a longer time scale can also produce uncertainty: can afirm adapt its new computer system to work with its old databases? theseproblems highlight the need for systematic, architectural solutions to theproblems of adaptivity and reliability. progress in these specific areas willbenefit any application domain that is sensitive to factors such as human errors,overloading of resources, and other unpredictable situations. indeed, as allapplication domains grow in scale, these conditions will become more common.finding 9: adaptivityduring and after a crisis, it is critically important that network services andresources be available. this need implies an adaptivity to unusual orextenuating circumstances beyond traditional network operational criteria.other nationalscale application areas could also benefit from increasedadaptivity, for several reasons: sharing of networkbased resources impliessignificant fluctuations in demand for and availability of those resources;human errors and system failures are inevitable; and new applications andunusual uses of existing applications can generate entirely unanticipatedcircumstances. networkbased systems (e.g., communications systems,computer networks, and sensor networks) should be prepared not only to routearound points of congestion or failure, but also to adapt to changing availabilityof resources. methods for achieving this adaptivity in a crisis are likely to bebroadly useful in many domains.crisis management demonstrates a number of specific ways in whichadaptivity is critical to system design. at the network level, for example, if thelocal,summary and findings: research for nationalscale applications130computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.preexisting network infrastructure is at least partially operational, it may bevaluable to integrate it with components brought by the crisis managementteam. this could involve attaching portable computers preloaded with crisisrelated data and software into existing local area networks or connectingpredeployed sensors, such as security cameras, into a network deployed for thecrisis response.in practice, identifying and making use of the existing infrastructure aredifficult; consequently, relief workers frequently arrive with an entirely separatesystem whose parts and operation they understand. yet this approach does noteliminate their problems, because in many cases, multiple organizations arrive,each with its own equipment, networks, and policies for using them (such asaccess priority and security), making effective integration of all availableresources difficult. adaptivity in this case may reflect the ability to rapidlyimplement compromise positions where resources owned or controlled bydifferent parties are integrated with agreements about policies for shared use.applications that run in uncertain environments also should be designedfor adaptivity. for example, if network service is available only intermittently,applications such as shared information repositories and collaboration toolsshould be prepared to adapt to varying network resources. they should also bereconfigurable or able to configure themselves to take advantage of new orevolving resources. for example, information sent from a crisis commandcenter might be sent to field workers as maps and diagrams when sufficientbandwidth is available, but as text when the bandwidth is reduced. multiple,distributed copies of databases could be designed to replicate updates to eachother (maintaining overall coherence) only when bandwidth is available or torestrict updates only to the highestpriority information such as locations ofpeople needing medical attention. during a videoconference, if congestionoccurs, a shift to a lower image resolution could enable the conference tocontinue. an attractive feature in such circumstances would be support forchoice by the users between reduced resolution and fewer frames per second asappropriate to their needs.16a different kind of example is the application that can adapt to changes inthe availability of information inputs. crisis managers must make judgments inthe absence of complete data. judgment support applications (e.g., buildingdamage simulations, logistics planners to estimate emergency supplyrequirements, mapbased evacuation route planners) must adapt not only tostatistical uncertainty, but also to gaps, mistakes, and deliberate falsifications intheir input data. this requires much more than simplistic interpolation ofmissing datašit demands an ability to make inferences about what the correctdata are likely to be.applications also need to evolve and adapt to changes on a longer scale.for example, if simulationbased training programs are designed to train peopleby providing accurate maps and images of possible crisis locations, adaptivityshould enable incorporation of new, better sources for that information overtime. originally, the simulation may use line drawings with altitudedesignations, latersummary and findings: research for nationalscale applications131computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.incorporating information from aerial photographs and weather predictionsystems.in crises, it would be especially valuable for applications to discover andexploit automatically, without the need for time and attentionconsuminghuman intervention, the capabilities of resources whose usefulness could nothave been anticipated when the application was written. these might includenew objects or services with enhanced functionalities that did not exist when theapplication was written (e.g., new kinds of environmental sensors); legacyresources that have existed for a long time, but have a structure or form that theapplication designer did not anticipate having to access (e.g., records ofearthquake damage patterns from past years); and resources created for use in adifferent application area (e.g., architectural designs used to plan evacuationroutes during a crisis).to enable successful use of unanticipated resources in all these cases,continued research should address the question of how applications might learnabout and make use of such objects. this problem has two parts. first, theapplication must be able to learn about the functionality of the new resource,which can be expressed in its type. to find the type of the new resource, theapplication must be able to ask the resource itself or some other service toidentify the type of the resource. both corba and the information meshproject at mit make a first cut at this by requiring that all objects (resources)support a function to answer such a query, if asked. second, the application mayhave to import code to access the new type of resource. the importation of codeat run time generally is possible only in programming environments thatsupport interpreters, such as the lisp programming language and its derivativesor java; importing code at run time to interface into other languages such as cor c++ generally is not feasible. thus, the problem of utilizing resources ofunanticipated types can be split into two research directions, one directedtoward protocols for querying objects and the services to support that activity,and the other advancing work in language, compiler, and runtime technologies.9. research is needed to increase the adaptivity of networks andapplications by providing them with the tools, resources, and facility tofunction during and after unusual or extenuating circumstances. nationalscale applications, especially those supporting crisis management, must beable to function in an environment marked by variability of availableresources and of requirements for resources.suggested research topics: selforganizing networks are those in which the components and resourcesof the network can discover and learn about each other without the needfor a centralized management structure. selforganizing networks willhave less need for human intervention than is otherwise required. there isboth theoretical and practical research to be done, ranging from whethersuch networks can stabilizesummary and findings: research for nationalscale applications132computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.themselves, to the protocols by which components learn about each otherand the specific kinds of information that components must share to enableselforganization. supporting mobile users and resources is a particular challenge, since thenetwork must be able to reorganize continually. mobile ip is one way ofaccounting for mobility by forwarding packets to the user's current location(similar to roaming in cellular telephone systems). however, it introduceslatency that is often unacceptable for realtime communications such asvoice and video (katz, 1995). improvements in network management are needed, including tools fordiscovering the state of existing infrastructure17 and extensions to currentmodels of network capabilities to reflect such aspects as reliability,availability, security, throughput, connectivity, and configurability. thesecould enable management tools with new paradigms of merging theaccess, priority, and security parameters of networks that interconnect witheach other during crises in unanticipated ways. one approach might be todevelop a priority server that could administer access rights flexibly withina network as users and their needs change during a crisis. methods are needed for reconciling network adaptivity with minimizingvulnerabilities to intruders and other threats. legitimate actions taken byadaptive selforganizing networks to conform to changes in the availableinfrastructure may in some cases be difficult for network managers todistinguish from hostile infiltration by an intruder. significant challengesexist in making secure, adaptive networks that recognize self and do notlaunch "autoimmune" attacks. artificial intelligence methods in networkmanagement may be a fruitful area for research to meet this need. security should adapt to the mobility of people and changingconfigurations of networks. for example, how can federal officials arrivein california after an earthquake and provide valid identificationrecognized by the network without requiring that the infrastructure assigneveryone new identities and passwords? how do those officials accessuseful files from their home offices while in some other security domain?how do the secure domains decide they can trust each other? research isneeded to support composition of security policies across administrativedomains and mobility of access rights. crisis managers have a clear need for better tools for discovering whatnetworkaccessible resources are available to them in time of crisis. morepowerful search and retrieval mechanisms than keyword matching arenecessary, as are solutions that allow searching within an unanticipatedapplication domain. rapidly configurable virtual subnets are required that span multipleunderlying network resources but provide services such as privacy andaccess control, as though users were isolated on a private network.research is needed both to develop the actual protocols necessary to createfunctional virtual subnets and to provide a clearer understanding of howwell virtual subnets can be isolated from broader network environments tosummary and findings: research for nationalscale applications133computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.support features such as security, access control, reliability, and bandwidthon demand. application component interface specification and exploration protocolsare needed to enable applications to interact with evolving or newresources. there has been some research into interface specifications, butuniformity is lacking. to provide application adaptivity that works at anational scale, either one architecture must be selected (which is unlikely)or protocols must be written to allow negotiation between applications andservices of the interface specification language and support tools to beused in any particular case. for example, new protocols would be neededto allow an application that accesses both corba objects and oleobjects to discover from objects which kind they are and then use theappropriate model to query the object or resource about its capabilities.finding 10: reliabilitythe utility of an application or application component often depends on anassessment of its reliability. maximum reliability is not always necessary; whatthe user requires is to understand the degree of reliability, to determine whetheror not it is within acceptable tolerances, and to decide appropriate actions. inmanaging a crisis, for example, decision makers must constantly judge theaccuracy of the information they are using in making decisions. (they do notnecessarily ignore questionable information, but they weigh it differently thanmore certain information.) aircraft manufacturers assess the reliability of asubcontractor's part design before incorporating it into an airplane design.health care workers assess the probable correctness of each item of data about apatient before making a diagnosis or taking action. the quality of inputs, thepredictability of events, the validity of simulations, the correct functioning oflargescale applications, and similar factors underlie the quality of informationyielded by computer and network applications. these must be understood forpeople to rely on information and computation technologies in nationalscaleapplications.to facilitate these assessments for computing and communications systemson which the nation increasingly depends, reliability attributes of systemcomponents need to be formalized and exposed whenever possible. this willrequire research. for example, a crisis response application constructeddynamically from disparate parts must continually predict and assess thereliability of each of its parts. some of the parts, such as remote computingfacilities running a welltested modeling program, may be assumed by the crisisapplication to be highly reliable with known probabilities of correctness andmeasures of precision. more typically, however, many of the componentscontributing to a crisis management solution do not have such known attributes.this is particularly true if people are part of the system or if untested,previously unintegrated subsystems are used. furthermore, the nature of thecrisis may change a reliable system into an unreliable one through unanticipatedscaling problems. therefore, an important unmetsummary and findings: research for nationalscale applications134computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.application need is the ability to develop confidence factors based on thereliability of parts of a system.assessment of confidence factors can complement other approaches toimproving reliability. many application areas, such as manufacturing, usedesign and testing processes and redundant subsystems to achieve reliabilitygoals. adaptive systems, such as those discussed in finding 9, represent anotherset of approaches to achieving reliability. some components of an applicationsolution, howeveršparticularly those involving peoplešdo not have welldefined ways of developing reliability factors. new insights and approaches areneeded to improve the reliability of the weak links in a system and, as a separatetopic, to capture, quantify, and communicate the reliability status (whetherstrong or weak) of each component.the latter topic is particularly important in nationalscale applications,which have high public visibility and must provide the public with a high levelof confidence that they function correctly and, when they do not, that theproblem can be identified and corrected quickly. when an airplane crashes,investigators retrieve the "black box" and analyze recorded data to determinewhat may have caused the crash, so that steps can be taken to avoid futureproblems and reestablish public confidence. it would be valuable in nationalscale applications to develop a blackbox analog (perhaps a set of requiredprocedures) for identifying and correcting errors.10. research is needed to enable accurate assessments of the reliabilityof systems composed of potentially unreliable hardware, software, andpeople. consistent methods for evaluating reliability should lead not onlyto more reliable systems, but also to better ways of using systems inapplications, such as crisis management, where absolute reliability isunattainable but reliability factors might be assessable. the ultimate goalof these efforts is to develop measures of confidence in the behavior ofsystems that support nationalscale applications.suggested research topics: a black box technology should be developed for nationalscaleapplications, analogous to that in aircraft, that enables the rapididentification and correction of errors, coupled with procedures forresponding to problems that ensure continuing confidence in the viabilityof the application. basic and applied research in chaotic processes is needed to betterunderstand the reliability of applications in the presence of poorqualityinformation (e.g., errors, incompleteness, internal inconsistencies).research might examine the tradeoffs between urgency and fidelity ofinformation collection in crises and methods for validating and reconcilingpoorquality information. to adapt to errors, whatever the source, applications must be robust.summary and findings: research for nationalscale applications135computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.applications should be selfadapting and have selfdescribing, selfpropagating metrics of component and information reliability. thesemetrics should reflect the implications of having people as an integral partof applications. reliability attributes should be developed and propagated as metadataassociated with system components.performance of distributed systemsas the scale of applications grows, not only in the geographical distancebetween components but also in the complexity of the interrelationships amongthe components and the utilization of lowerlevel resources (e.g., networks,processors, memory, storage), the performance of systems that supportapplications must increase if they are to achieve results rapidly enough to beusable. in addition, the performance of the various infrastructural resourcesmust be balanced to produce effective results.finding 11: performance of distributed systemscrisis management presents an especially challenging set of requirementsfor balanced performance in both computer systems and networks. becausetimeliness is nearly always paramount, extraordinary computing power andnetwork bandwidth are required to ensure that results can be delivered soonenough to be relevant. moreover, there is rarely time in a crisis to tune softwareperformance, and the easier a computer program is to use effectively, the morelikely it is to be used in the stressladen working environment of a crisis.since crises are infrequent and seldom predictable as to place and time,establishing dedicated computing and communications resources iseconomically impractical. whatever largescale, highperformance computingand communications capabilities are made available for responding to a crisiswill need to be preempted from less urgent work. the potpourri of data neededto help answer queries and supply input for simulations must be marshaled fromits many resident locations as quickly as possible, and highbandwidthnetworking must be delivered to the scene for transmission of imagery,including simulation results.achieving computer system interoperability, adaptivity, and reliability,especially in connection with a crisis, calls for exceptional computing powerand storage capacity. for crisis management, capabilities even beyond thoseappropriate to ordinary circumstances are required to manage a largely ad hocand unreliable interconnection of computer systems that were never designed towork together in the first place. the software that makes these deficienciestolerable adds to the computing burden.the deployment of computations across networks and the use ofdistributed and possibly heterogeneous computer systems to address singleproblems are attractive for crisis management and other nationalscaleapplications.summary and findings: research for nationalscale applications136computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.increasing the size of many candidate computations to national scale maybe impractical because of poor performance. for example, storm and wildfiresimulations may perform more poorly as distributed computations than dataacquisition and reformatting do. as mit's barbara liskov said, "everyoneknows scalability is important. but no one knows how to show [that] you haveit, short of running experiments with huge numbers of machines, which isusually not practical. we need a way to reason about scalability." at every pointin the parallel and distributed software design and development cycle,scalability in performance should be treated as a firstclass problem.11. research is needed to better understand how to reason about,measure, predict, and improve the performance of distributed systems.crisis management and other nationalscale applications demand highperformance systems and tools that balance processing speed,communications bandwidth, and information storage and retrieval.suggested research topics: current capability to model the performance of systems that are distributedacross heterogeneous networks and computing platforms is very limited.18 predicting the performance of large, distributed software systems isparticularly difficult but would be quite valuable in addressing nationalscale application needs. research is needed to identify what parameters ofnetwork, processing, and storage components are critical to systems' abilityto meet specified performance criteria, such as capacity andresponsiveness, and to develop appropriate metrics for these parameters.research should include a measurement program to evaluate the ability ofmodels to predict how systems will perform under normal conditions andin crises. these models could be tested, for example, in the context of thecrisis management testbeds discussed in finding 1.notes1. the reverse, however, is not necessarily true; technologies that have been developed for otherdomains may not meet the needs of crisis management for coping with urgency andunpredictability.2. the "where" of deployment includes physical as well as conceptual locations, such as a layeror layers in the technical architecture.3. the cstb report observed that the gigabit testbedsšexperimental research networkssupported under the high performance computing and communications initiativešsupportedthe concept of largescale networks offering higher performance than current networks. theexamination of existing, apparently successful network architectures advocated in the steeringcommittee's finding 2 should be seen as complementary to work recommended in the 1995report's conclusion that "ongoing research in several areas is still needed before a ubiquitoushighperformance information infrastructure can be developed and deployed nationwide. . . . [s]uccessful evolution of the nation's communications capability rests on continued investment inbasic hardware, networking, and software technologies research" (cstb, 1995a, p. 54).summary and findings: research for nationalscale applications137computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.4. for a discussion of the relationship between the study of deployed systems and thedevelopment of new research directions, see cstb (1989) and cstb (1992).5. a different kind of negative effect that people may have on systems occurs when, in hostilesituations such as crime or warfare, they attack systems to harm their performance.6. research in scientific visualization aims at permitting computational scientists to observe andunderstand intuitively the effects of variations in models of phenomena they are studying; see,for example, hibbard et al. (1994). extending this sort of visualization into the crisismanagement context not only requires better models of uncertain phenomena such as masssocial behavior, but also challenges the ability to display results meaningfully on equipment ofa performance that crisis management agencies are likely to be able to afford.7. the diversity of organizations with different structures and patterns of working makes itnecessary for these communications models to accommodate different modes, whencollaboration crosses organizational boundaries as it frequently does.8. an important feature of the problemsolving environment would be the ability to abstractapplication requirements and translate those requirements into specifications for softwaresystem functionality. developing such an ability will require considerable research.9. middleware provides services within an information infrastructure that are used in commonamong multiple applications. for a discussion, see cstb, 1994b, p. 49.10. the arpanet, precursor to the internet, exhibited emergent phenomena related tonetwork control functions that unpredictably produced massive slowdowns in the network.fundamental design principles to predict and avoid such phenomena in largescale systemsremain lacking.11. revisions to code are no guarantee of improvement; managing the proliferation of differentversions of the same code is another formidable challenge.12. alternatively, java and similar networkcentered models of computing illustrate anemerging, distributed approach to software development. in this approach, developers acrossthe internet are participating in group development projects using the models of consortia anddistributed applications based on multiple interactive web services. these projects do not looklike software development projects in the traditional sense, but they may yield workable, largescale solutions.13. in fact, because genetic influences on medical conditions may be understood increasingly,maintaining medical histories longer than a lifetime may become more and more valuable todescendants.14. kunze, john a, "functional recommendations for internet resource locators," internetrequest for comments (rfc) 1736, february 1995; and sollins, karen, and larry masinter,"functional requirements for uniform resource names," internet rfc 1737, december 1994.both are available on line at http://www.cis.ohiostate.edu/hypertext/information/rfc.html.15. examples include the wide area information service (wais) and harvest, a project at theuniversity of colorado. there are also a number of searching tools designed specifically for theworld wide web, such as altavista from digital equipment corporation, lycos, and others.16. for example, doctors might decide that only the full level of performance is acceptable,whereas medical insurers might opt for lower resolution and professors showing chalkboarddiagrams might opt for fewer frames per second.17. such tools exist, but are difficult to use and require a higher level of technical expertise thanis readily available in a crisis response.18. research has achieved some success in one aspect of this problem, that of producing realtime systems. these are systems that can vary their algorithmic approach to a problem in orderto converge on a solution by a specified deadline, perhaps sacrificing some accuracy to meet thetime constraint. however, much more work is required to generalize this understanding toaspects of performance other than converging before deadlines and to the less welldefinedproblems characteristic of crises.summary and findings: research for nationalscale applications138computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.bibliographyamdahl, gene m. 1967. ''validity of the single processor approach to achieving largescalecomputing capabilities." american federation of information processing societiesconference proceedings: spring joint computing conference , vol. 30 . thompsonbooks, washington, d.c.andersen, henning boje, henrik garde, and verner andersen. 1994. "mms: an electroniccommunication system for emergency management organizations." pp. 3439 ininternational emergency management and  engineering conference: bridging the gapbetween theory and practice: research and applications , james d. sullivan andsuleyman tufekci (eds.). the international emergency management and engineeringsociety (tiemes), dallas, tex.beroggi, giampiero e.g., laurie waisel, and william a. wallace. 1994. "the role of virtualreality technology in emergency management." pp. 212217 in international emergencymanagement and engineering  conference: bridging the gap between t heory andpractice: research and applications , james d. sullivan and suleyman tufekci (eds.).tiemes, dallas, tex.brutzman, donald p., michael r. macedonia, and michael j. zyda. 1996. "internetworkinfrastructure requirements for virtual environments." in white papers: theunpredictable certainty . national academy press, washington, d.c., forthcoming.committee on information and communications (cic). 1995a. america in the age of information:strategic implementation plan . national science and technology council (nstc),washington, d.c., march 10 .committee on information and communications (cic). 1995b. america  in the age of information:a forum on federal information and communications r&d . compendium of draft whitepapers for forum, washington, d.c., july 67. nstc, washington, d.c.computer science and technology board (cstb), national research council. 1989. scaling up: aresearch agenda for software engineering . national academy press, washington, d.c.computer science and telecommunications board (cstb), national research council. 1992.computing the future: a broader agenda for computer  science and engineering .national academy press, washington, d.c.bibliography139computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.computer science and telecommunications board (cstb), national research council. 1994a.information technology in the service society: a  twentyfirst century lever . nationalacademy press, washington, d.c.computer science and telecommunications board (cstb), national research council. 1994b.realizing the information future: the internet and  beyond . national academy press,washington, d.c.computer science and telecommunications board (cstb), national research council. 1995a.evolving the high performance computing and communications initiative to support thenation's information infrastructure. national academy press, washington, d.c.computer science and telecommunications board (cstb), national research council. 1995b.information technology for manufacturing: a research agenda. national academy press,washington, d.c.computer systems policy project (cspp). 1991. expanding the vision of high performancecomputing and communications: linking america for the future. cspp, washington,d.c., december 3.davis, larry s., joel saltz, and jerry feldman. 1995. "nsf workshop on high performancecomputing and communications and health care," summary of workshop, december810, 1994, washington, d.c. available on line at http://www.umiacs.umd.edu/users/lsd/papers/nsfwork.html.dixon, frank, frank gargione, and richard t. gedney. 1995. "acts to the rescue." satellitecommunications 19(8):2729.drabek, thomas e. 1991. microcomputers in emergency management. institute of behavioralscience, university of colorado, boulder, colo.droegemeier, kelvin k. 1993. "storm warning: field program to validate a thunderstormprediction model." pp. 2829 in projects in scientific computing. pittsburghsupercomputing center, pittsburgh, pa.federal emergency management agency (fema). 1993. lessons of hurricane andrew. excerptsfrom 15th annual national hurricane conference, orlando, fla., april 1316. fema,washington, d.c.fox, geoffrey c., and wojtek furmanski. 1995. "the use of the national information infrastructureand high performance computers in industry," npac technical report sccs732.available on line at http://www.npac.syr.edu/techreports/html.garciamolina, hector, yannis papakonstantinou, dallan quass, anand rajaraman, yehoshuasagiv, jeffrey ullman, and jennifer widom. 1995. "the tsimmis approach tomediation: data models and languages (extended abstract)." available on line at http://wwwdb.stanford.edu/tsimmis/publications.html.gillies, douglas. 1994. "can we talk? reviewing communications systems after suddencataclysmic disasters." hazard technology xiv(4):17.government issue. 1995. "gets is going: at&t assembles emergency communications servicefor ncs." september/october:3.hazard technology. 1995a. "jwid '95: military, civilian experts see top systems in action." xv(2):1, 4. hazard technology. 1995b. "record hurricane season spotlights newvulnerabilities and solutions." xv(2):1, 10. hazard technology. 1995c. "fast actionfrom fema speeds hurricane relief to virgin islands." xv(2):11.hibbard, william l., charles r. dyer, andré l. battaiola, and mariefrançoise voidrotmartinez.1994. "interactive visualization of earth and space science computations." ieeecomputer 27(july):6572.bibliography140computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.houmb, ole gunnar, and ulf roar aakenes. 1994. "a general environment information system fordecision support during major emergencies." pp. 154160 in international emergencymanagement and engineering conference: bridging the gap between theory andpractice: research and applications, james d. sullivan and suleyman tufekci (eds.).tiemes, dallas, tex.institute of medicine (iom). 1994. health data in the information age. national academy press,washington, d.c.international emergency management and engineering conference: bridging the gap betweentheory and practice: research and applications, james d. sullivan and suleymantufekci (eds.). 1994. tiemes, dallas, tex.jensen, steven j. 1994. "automated emergency management: a proposal for a standardizedsystem." pp. 309313 in international emergency management and engineeringconference: bridging the gap between theory and practice: research and applications,james d. sullivan and suleyman tufekci (eds.). tiemes, dallas, tex.katz, randy h. 1995. "adaptation and mobility in wireless information systems." unpublishedpaper available from http://daedalus.cs.berkeley.edu. august 18.kvalem, john, and egil stokke. 1994. "integrated user interface for mem decision support." pp.113119 in international emergency management and engineering conference: bridgingthe gap between theory and practice: research and applications, james d. sullivan andsuleyman tufekci (eds.). tiemes, dallas, tex.landauer, thomas k. 1995. the trouble with computers: usefulness, usability, and productivity.mit press, cambridge, mass.lax, peter d. (chairman). 1982. report of the panel on largescale computing in science andengineering. sponsored by the u.s. department of defense and the national sciencefoundation, in cooperation with the department of energy and the national aeronauticsand space administration, washington, d.c., december 26.linz, adrian, and paul bryant. 1994. "the allhazard situation assessment prototype." pp.175179 in international emergency management and engineering conference: bridgingthe gap between theory and practice: research and applications, james d. sullivan andsuleyman tufekci (eds.). tiemes, dallas, tex.lynch, clifford, and hector garciamolina. 1995. "interoperability, scaling, and the digitallaboratories research agenda." report on the information infrastructure technology andapplications (iita) digital libraries workshop, reston, va., may 18150;19. available online at http://wwwdiglib.stanford.edu/diglib/pub/reports/iitadlw.macedonia, michael r., michael j. zyda, david r. pratt, donald p. brutzman, and paul t. barham.1995. "exploiting reality with multicast groups: a network architecture for largescalevirtual environments." proceedings of institute for electrical and electronics engineers(ieee) virtual reality annual international symposium (vrais), march 1115, researchtriangle park, n.c.moore, timothy j., michael f. vetter, and melanie s. ziegler. 1994. "simulated images foremergency management training." pp. 197204 in international emergency managementand engineering conference: bridging the gap between theory and practice: researchand applications , james d. sullivan and suleyman tufekci (eds.). tiemes, dallas, tex.morentz, james w., and david a. griffith. 1994. "improving emergency planning, preparedness,and response with gis." pp. 913 in international emergency management andengineering conference: bridging the gap between theory and practice: research andapplications, james d. sullivan and suleyman tufekci (eds.). tiemes, dallas, tex.national science and technology council (nstc). 1995. high performance computing andcommunications: foundation for america's information future . nstc, washington, d.c.bibliography141computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.newkirk, ross t. 1994. "information integration for emergency management and engineering."pp. 303308 in international emergency management and engineering conference:bridging the gap between theory and practice: research and application s, james d.sullivan and suleyman tufekci (eds.). tiemes, dallas, tex.office of science and technology policy (ostp). 1993. grand challenges 1993: highperformance computing and communications. ostp, washington, d.c.office of science and technology policy (ostp). 1994a. high performance computing andcommunications: toward a national information infrastructure . ostp, washington, d.c.office of science and technology policy (ostp). 1994b. high performance computing andcommunications: information infrastructure technology and applications. ostp,washington, d.c., february.office of technology assessment (ota). 1995. distributed interactive simulation of combat. u.s.government printing office, washington, d.c., september.oklahoma state university and university of oklahoma. 1993. oklahoma mesonet. the oklahomamesonetwork, norman, okla., summer.perry, walter l., john y. schrader, and barry m. wilson. 1992. a variable resolution approach tomodeling command and control in disaster relief operations. rand corporation, santamonica, calif.proceedings of the 1993 simulation multiconference on the international emergency managementand engineering conference: tenth anniversary: research and applications, james d.sullivan (ed.). 1993. society for computer simulation, san diego, calif.proceedings of the 1995 simulation multiconference, maurice ades and ariel sharon (eds.). 1995.society for computer simulation, san diego, calif.ramsay, stephen, and matthew hilbert. 1994. "monty: a monte carlo method for quantitativerisk assessment." pp. 9196 in international emergency management and engineeringconference: bridging the gap between theory and practice: research and applications,james d. sullivan and suleyman tufekci (eds.). tiemes, dallas, tex."research priorities in networking and communications: report to the nsf division ofnetworking and communications research and infrastructure." 1995. report of aworkshop, airlie house, va., may 1214, 1994. available on line at http://www.cise.nsf.gov/cise/ncri.sawyer, kathy. 1995. "increasing the accuracy of foretelling hurricanes' deadly turns."washington post, july 31, p. a3.shavit, yaron, valerie lavigne, and marc firmignac. 1994. "integration of emergencymanagement information systems: towards a common reference model." pp. 295302 ininternational emergency management and engineering conference: bridging the gapbetween theory and practice: research and applications, james d. sullivan andsuleyman tufekci (eds.). tiemes, dallas, tex.syracuse university and multidisciplinary analysis and design industrial consortium (madic)team 2. 1995. "definition of requirements for an aeronautics affordable systemsoptimization process (revision 1)." draft submitted to nasa langley research center.syracuse university and madic team 2, december 15.vernon, mary k., edward d. lazowska, and stewart d. personick (eds.). 1994. r&d for the nii:technical challenges. report of a symposium, february 28 through march 1,gaithersburg, md. interuniversity communications council, educom, washington, d.c.wiederhold, gio. 1992. "mediators in the architecture of future information systems." ieeecomputer 25(3):3849.xue, ming, keith brewster, kelvin k. droegemeier, fred carr, vince wong, yuhe liu, adwaitsathye, g. bassett, paul yanish, jason levit, and phillip bothwell. 1996. "real timeprediction of stormscale weather during vortex95, part ii: operation summary andexample cases" (preprint). american meteorological society 18th conference on severelocal storms, february. san francisco, calif. available on line at http://wwwcaps.uoknor.edubibliography142computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.appendixes143computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.144computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.aworkshop series on high performancecomputing and communicationsworkshop i: an assessment of hpcc andapplication needsagendamonday, august 15, 19947:308:30 a.m.breakfast8:308:45welcoming remarks and introductionsł ken kennedy, rice university, steering committee chair8:459:45relationships: highperformance computing, nationalinformation infrastructure, and the internet kalil, national economic council (via video)9:4510:30technology status and directions: internetšthe once and futurenetwork farber, university of pennsylvania10:3010:45break and informal discussionsworkshop series on high performance computing andcommunications145computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.10:4512:15 p.m.applications and needs in information delivery and services daniel schutzer, citibank clifford lynch, university of california12:151:15lunch1:152:45applications and needs in medical care james ostell, national library of medicine joel saltz, university of maryland2:453:00break and informal discussion3:004:30applications and needs in design and manufacturing david jack, the boeing company peter will, information sciences institute4:305:30assessment and discussion of common needs ken kennedy, rice university5:30reception and dinnertuesday, august 16, 19947:308:30 a.m.breakfast8:309:00technology status and directions: the emerging globaldigital library (including mosaic demonstration) larry smarr, national center for supercomputingapplications9:009:30technology status and directions: computer architectures andhardware john hennessy, stanford university burton smith, tera computer company9:309:45break and informal discussions9:4510:45technology status and directions: operating systems brian bershad, university of washington distributedand heterogeneous systems clifford neuman, university of southern california karen sollins, massachusetts institute of technologyworkshop series on high performance computing andcommunications146computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.10:4511:00break and informal discussions11:0011:45technology status and directions: computing environments frances allen, ibm t.j. watson research centerlanguages, compilers, and application developmenttools geoffrey fox, syracuse university ken kennedy, rice university11:451:00 p.m.lunch1:002:15middle layer services and application needs william scherlis, carnegie mellon university randy katz, defense advanced research projects agency2:152:30break and informal discussions2:303:45privacy, authentication, and security in network commerce clifford neuman, university of southern california karen sollins, massachusetts institute of technology3:454:00break and informal discussions4:005:00application needs and research directions ken kennedy, rice university5:007:30dinnerwednesday, august 17, 19947:308:30 a.m.breakfast8:309:30building the needed infrastructure leonard kleinrock, university of california, los angelesworkshop series on high performance computing andcommunications147computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.9:3010:15government applications of the developing infrastructure eliot christian, u.s. geological survey10:1510:30break and informal discussions10:3011:45overall application critique of technical status and directions geoffrey fox, syracuse university11:4512:30 p.m.summation, future activities, and workshop closing ken kennedy, rice university12:30lunch and adjournparticipantsrobert aiken, department of energyfrances allen, ibm t.j. watson research centerbrian bershad, university of washingtoneliot christian, u.s. geological surveydavid farber, university of pennsylvaniageoffrey fox, syracuse universitydennis gannon, indiana universityrobert haar, general motors research laboratoriesstephen haynes, west publishing companyjohn hennessy, stanford universitydavid jack, the boeing companyrichard johnson, boozallen & hamiltonthomas kalil, national economic councilrandy katz, defense advanced research projects agencyken kennedy, rice universityleonard kleinrock, university of california, los angelesclifford lynch, office of the president, university of californiarobert neches, defense advanced research projects agencyclifford neuman, university of southern californiajames ostell, national library of medicinejudith ozbolt, university of virginiamerrell patrick, national science foundationfriedrich prinz, carnegie mellon universityal rosenheck, congressional fellowpaul rubbert, boeing commercial airplane groupjoel saltz, university of marylandwilliam scherlis, carnegie mellon universityworkshop series on high performance computing andcommunications148computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.daniel schutzer, citibankhenry d. shay, lawrence livermore national laboratorylarry smarr, university of illinois at urbanachampaignburton smith, tera computer companykaren sollins, massachusetts institute of technologypaul tang, northwestern memorial hospitaljohn toole, defense advanced research projects agencyjan tulinius, rockwell internationalj. douglas tygar, carnegie mellon universitypeter will, university of southern californiawilliam wulf, university of virginiapaul young, national science foundationworkshop ii: hpcc and crisis managementagendamonday, june 12, 19957:308:30 a.m.breakfast8:309:00opening remarks ken kennedy, rice university, steering committeechair john hwang, federal emergency management agency9:0010:00damage assessment and response modeling robert kehlet, defense nuclear agency10:0010:15break10:1511:15sensors, data, and modeling kelvin droegemeier, center for analysis and predictionof storms, university of oklahoma11:1512:15 p.m.interoperability, command and control don eddington, naval research and developmentlaboratory12:151:15lunchworkshop series on high performance computing andcommunications149computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.1:152:45field applications: emergency response and instantadministration david kehrlein, office of emergency services, state ofcalifornia james beauchamp, deployable joint task forceaugmentation cell, cincpac2:453:00discussion and charge to working groups steering committee3:003:15break3:155:30working group sessions 1. data and communications: sensors,networks, interconnectivity, security moderators: vinton cerf, mci telecommunicationskaren sollins, massachusetts institute of technology 2.analysis and computation: modeling and simulation,validation, integration moderators: ken kennedy, rice university burton smith,tera computer company 3. interpretation and action:decision support, human interaction, informationmanagement moderators: frances allen, ibm t.j. watson researchcenter geoffrey fox, syracuse university5:30reception and dinnertuesday, june 13, 19957:308:30 a.m.breakfast8:309:30preliminary reports from working groups nature of the crisis management information andcommunications problem unmet needs of the crisis management community incomputing and communications emerging opportunities for new technology developmentworkshop series on high performance computing andcommunications150computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.9:3012:00 p.m.working group sessions12:001:00lunch1:002:30working group sessions2:302:45break2:454:00final reports by working groups refined statement of unmet computing andcommunications needs possibilities for (a) incremental improvements and (b)"breakthrough" applications enabled by advances incomputing and communications power, integration,ease of use, and wide availability highpotential hpcc research opportunities to meetcrisis management needs4:005:15discussion, synthesis, and assignment of research priorities5:155:30next steps and adjournparticipantsw. richards adrion, university of massachusettsfrances allen, ibm t.j. watson research centerdavid austin, edgewater, md.madeleine bates, bbn systems and technologiesjames beauchamp, u.s. cincpac (u.s. commander in chief, pacificcommand)donald brown, university of virginiabernd bruegge, carnegie mellon universitywilliam buzbee, national center for atmospheric researchvinton g. cerf, mci telecommunicationseliot christian, u.s. geological surveynicole dash, university of delawarelarry davis, university of marylandkelvin droegemeier, university of oklahomadaniel duchamp, columbia universitydon eddington, naval research and development laboratoryrichard entlich, institute for defense analysesrobert fleming, naval research and development laboratoryworkshop series on high performance computing andcommunications151computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.geoffrey fox, syracuse universitypeter freeman, georgia institute of technologyvictor frost, university of kansasroger ghanem, state university of new york, buffaloegill hauksson, california institute of technologycharles j. henkin, cna corporationwilliam hibbard, university of wisconsin, madisonjohn hwang, federal emergency management agencyrajeev jain, university of california, los angelesrobert kehlet, defense nuclear agencydavid kehrlein, office of emergency services, state of californiarichard a. kemmerer, university of california, santa barbaraken kennedy, rice universitypaul kowalski, unisys corporationdavid maier, oregon graduate institutewilliam mark, lockheed martin palo alto research laboratorieslois clark mccoy, national institute of urban search and rescuewalter mcknight, national communications systemalan mclaughlin, massachusetts institute of technologymary fran myers, university of coloradorobert neches, defense advanced research projects agencypeter g. neumann, sri internationaljudith ozbolt, university of virginiaira richer, mitre corporationsteven roa, naval research and development laboratoryjoel saltz, university of marylandcharles a. slocomb, los alamos national laboratoryburton smith, tera computer companyjonathan smith, university of pennsylvaniasteven j. smith, national center for atmospheric researchkaren sollins, massachusetts institute of technologyjoseph stewart ii, mitre corporationstuart thorson, syracuse universityjonathan turner, washington universityrichard watson, lawrence livermore national laboratoryjon webb, carnegie mellon universityworkshop series on high performance computing andcommunications152computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.workshop iii: hpcc research andapplication needsagendathursday, august 24, 19957:308:30 a.m.breakfast8:308:45opening remarks ken kennedy, rice university, steering committeechair8:459:45application needs for advances in hpcc banking and commerce, daniel schutzer, citibank health care, joel saltz, university of maryland manufacturing, geoffrey fox, syracuse universitypresenters will draw from workshop i and other effortsto establish a baseline for further discussion, focusingon what appears most essential today.9:4510:45crisis management application needs overview of workshop ii findings, ken kennedy crisis scenarios, geoffrey fox; vinton cerf, mci telecommunications presentation of one or twocrisis scenarios to help guide subsequent discussions.focus is on identifying problems that stretch thecapabilities of hpcc technologies.10:4511:00break11:0012:30 p.m.research issues: computation and analysis discussion leaders: burton smith, tera computercompany; ken kennedy issues may include, amongothers: modeling complex phenomena;multidisciplinary simulation and optimization;distributed information systems with prioritized datacaching based on metadata; database verificationthrough simulation and parameter identification;scalable systems; simulation applications in training andplanning.workshop series on high performance computing andcommunications153computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.12:301:30lunch1:303:00research issues: communications and sensors discussion leaders: karen sollins, massachusetts institute oftechnology; vinton cerf issues may include, among others:selfconfiguring, rapidly deployable networks; architecturesand network management (e.g., distributed systems withsmart sensors and portable workstations; centralized systemswith highbandwidth links); security; reliability; protectingintellectual property; data compression; and other efficiencytechniques.3:003:15break3:154:45research issues: interpretation and action discussion leaders: frances allen, ibm t.j. watsonresearch center; geoffrey fox; william scherlis, carnegiemellon university issues may include, among others:adaptive user interfaces (flexible, easy to use by nonexpertsduring crises); multisource and multimedia data fusion; datamining; knowledge extraction and judgment; computersupported collaborative work.4:455:30synthesis and next steps ken kennedy hpcc agency representatives generalizability of workshopfindings across domains and sectors; ordering of findings;research and other possible action items for darpa andother agencies; relationship of hpccbased activity tobroader initiatives in the federal government and inapplication domains.5:30reception and dinnerparticipantsduane adams, defense advanced research projects agencyrobert aiken, u.s. department of energyfrances allen, ibm t.j. watson research centerworkshop series on high performance computing andcommunications154computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.yigal arens, university of southern californiawilliam arms, corporation for national research initiativesdonald brown, university of virginiavinton g. cerf, mci telecommunicationseliot christian, u.s. geological surveydavid d. clark, massachusetts institute of technologystephen cross, carnegie mellon universitywalter c. ermler, u.s. department of energydavid farber, university of pennsylvaniageoffrey fox, syracuse universitydennis gannon, indiana universitydavid gunning, defense advanced research projects agencylee holcomb, national aeronautics and space administrationjohn hwang, federal emergency management agencydavid kehrlein, office of emergency services, state of californiaken kennedy, rice universitythomas kraay, boozallen & hamilton inc.barbara liskov, massachusetts institute of technologyteresa f. lunt, defense advanced research projects agencyclifford lynch, office of the president, university of californialois clark mccoy, national institute for urban search and rescueclifford neuman, university of southern californiajoel saltz, university of marylandwilliam scherlis, carnegie mellon universitydaniel schutzer, citibank, n.a.allen sears, defense advanced research projects agencyburton smith, tera computer companykaren sollins, massachusetts institute of technologymary vernon, university of wisconsin, madisonjohn wroclawski, massachusetts institute of technologywilliam wulf, university of virginiamichael j. zyda, naval postgraduate schoolworkshop series on high performance computing andcommunications155computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.bbackgroundhpcci and niithe federal high performance computing and communications initiative(hpcci) was the culmination of a decade of activity focused on building trulyhigh performance computing and communications tools and putting them in thehands of science and engineering users within the united states. this activitydates back to the lax report (lax, 1982) and included the founding of thenational science foundation supercomputer centers in the mid1980s and theestablishment of highperformance computing centers by other agencies as well.the hpcci developed out of discussions within federal agencies in the late1980s, leading to the publication of a program strategy in 1987 and a programplan in 1989.the hpcci was formalized in the fiscal year (fy) 1992 president's budgetand by the high performance computing act of 1991 (p.l. 10294) authorizinga 5year program in highperformance computing and communications. thislegislation affirmed the interagency character of the hpcci, assigning broadresearch and development emphases to the 10 federal agencies that were thenparticipating in the program, without precluding the future participation of otheragencies.one major goal of the hpcci was to provide the computational andcommunications infrastructure needed to attack truly difficult problems inscience and engineering, known as grand challenges. the grand challengesidentified in the first annual program plan, known as the blue book (ostp,1993), were the following:backgroundhpcci and nii156computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved. forecasting severe weather events, cancer gene research, predicting new superconductors, simulating and visualizing air pollution, aerospace vehicle design, energy conservation and turbulent combustion, microelectronics design and packaging, and earth biosphere research.in subsequent years, other grand challenge areas were added. by focusingon problem solving, the hpcci greatly improved the interaction betweentechnology developers and end users of the technology. that interactionaccelerated progress in the development and deployment of highperformancesystems, networks, software, and associated usability technologies.however, one criticism of the hpcci was that although the focus ongrand challenges was very important to science and engineering and providedvaluable balance to the program, its impact on the average citizen was veryindirect (cspp, 1991). in response to this criticism, the initiative was extendedin the fy 1995 blue book (ostp, 1994a) to include research on thedevelopment and application of a national information infrastructure (nii) thatwould leverage technologies and applications associated with elements of thehpcci. in addition, it extended the application focus to include severalnational challenges, which address critical needs of our society and can benefitfrom highperformance computing and communications and nii research. thenational challenges can be viewed as nationalscale applications. the nationalchallenges listed in the fy 1996 blue book (nstc, 1995) included educationand lifelong learning, digital libraries, health care, manufacturing, electroniccommerce, environmental monitoring, energy management, civil infrastructuremanagement, and public access to government information. a 1994 report oninformation infrastructure technology and applications (ostp, 1994b)šanhpcci componentšnotes crisis management as an additional nationalchallenge.backgroundhpcci and nii157computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.cacronyms and abbreviationsactsadvanced communications technology satelliteansiamerican national standards instituteapiapplication programming interfaceasciiamerican standard code for information interchangeasopaffordable systems optimization processatmasynchronous transfer mode; automated teller machinec4icommand, control, communications, computing, andintelligencecadcomputeraided designcapscenter for the analysis and prediction of stormscfdcomputational fluid dynamicsciccommittee on information and communicationscincpaccommander in chief, pacific commandcorbacommon object request broker architecturecstbcomputer science and telecommunications boardctcomputerized tomographydarpadefense advanced research projects agencydoddepartment of defenseecgelectrocardiogramedielectronic data interchangeerlinkemergency response linkacronyms and abbreviations158computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.fccfederal communications commissionfemafederal emergency management agencyftpfile transfer protocolgetsgovernment emergency telecommunications servicegiiglobal information infrastructuregisgeographic information systemgpsglobal positioning systemhfhigh frequencyhpccihigh performance computing and communications initiativehpfhigh performance fortranhtmlhypertext markup languagehttphypertext transfer protocolietfinternet engineering task forceipinternet protocoliwayinformation widearea yearjtfjoint task forcejwidjoint warrior interoperability demonstrationlanlocal area networkmadmultidisciplinary analysis and designmbonemulticast backbonemimemultipurpose internet mail extensionmitmassachusetts institute of technologymlpmultilevel precedencempimessage passing interfacemrimagnetic resonance imagingnasanational aeronautics and space administrationncsnational communications systemnexradnext generation weather radarniinational information infrastructureni/usrnational institute for urban search and rescuenoaanational oceanic and atmospheric administrationnsfnational science foundationoleobject linking and embeddingacronyms and abbreviations159computing and communications in the extreme: research for crisis management and other applicationscopyright national academy of sciences. all rights reserved.pcpersonal computerpdes/stepproduct data exchange using the standard for the exchangeof product model datapinpersonal identification numberr&dresearch and developmentrfcrequest for comments (internet)sitasociété internationale de télécommunications aéronautiquessqlstructured query languagetcptransmission control protocoltsimmisthe stanfordibm manager of multiple information sourcesuavunmanned aerial vehicleurluniform resource locatorurnuniform resource namevhfvery high frequencyvrmlvirtual reality modeling languagevsatvery small aperture terminalwaiswide area information servicewwwworld wide webacronyms and abbreviations160
detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/9591impact of advances in computing and communicationstechnologies on chemical science and technology: report ofa workshop236 pages | 8.5 x 11 | paperbackisbn 9780309065771 | doi 10.17226/9591chemical sciences roundtable, national research councilimpact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.impact of advances in computingand communications technologieson chemical science andtechnologyreport of a workshopchemical sciences roundtableboard on chemical sciences and technologycommission on physical sciences, mathematics, and applicationsnational research councilnational academy presswashington, d.c.iimpact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.notice: the project that is the subject of this report was approved by the governing board of the national research council, whose members are drawn from the councils of the national academy of sciences, the national academy of engineering, and the institute of medicine.the members of the workshop organizing committee responsible for the report were chosen for their special competences and with regard forappropriate balance.the national academy of sciences is a private, nonprofit, selfperpetuating society of distinguished scholars engaged in scientific andengineering research, dedicated to the furtherance of science and technology and to their use for the general welfare. upon the authority ofthe charter granted to it by the congress in 1863, the academy has a mandate that requires it to advise the federal government on scientificand technical matters. dr. bruce alberts is president of the national academy of sciences.the national academy of engineering was established in 1964, under the charter of the national academy of sciences, as a parallelorganization of outstanding engineers. it is autonomous in its administration and in the selection of its members, sharing with the nationalacademy of sciences the responsibility for advising the federal government. the national academy of engineering also sponsors engineering programs aimed at meeting national needs, encourages education and research, and recognizes the superior achievements of engineers.dr. william a. wulf is president of the national academy of engineering.the institute of medicine was established in 1970 by the national academy of sciences to secure the services of eminent members ofappropriate professions in the examination of policy matters pertaining to the health of the public. the institute acts under the responsibilitygiven to the national academy of sciences by its congressional charter to be an adviser to the federal government and, upon its own initiative, to identify issues of medical care, research, and education. dr. kenneth i. shine is president of the institute of medicine.the national research council was organized by the national academy of sciences in 1916 to associate the broad community of science and technology with the academy's purposes of furthering knowledge and advising the federal government. functioning in accordancewith general policies determined by the academy, the council has become the principal operating agency of both the national academy ofsciences and the national academy of engineering in providing services to the government, the public, and the scientific and engineeringcommunities. the council is administered jointly by both academies and the institute of medicine. dr. bruce alberts and dr. william a.wulf are chairman and vice chairman, respectively, of the national research council.support for this project was provided by the national science foundation under grant no. che9630106, the national institutes ofhealth under contract no. n01od42139, and the u.s. department of energy under grant no. defg0295er14556. any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of thenational science foundation, the national institutes of health, or the u.s. department of energy.international standard book number: 0309065771additional copies of this report are available from: national academy press 2101 constitution avenue, nw box 285 washington, dc20055 8006246242 2023343313 (in the washington metropolitan area) http://www.nap.educopyright 1999 by the national academy of sciences. all rights reserved.printed in the united states of americaiiimpact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.iiichemical sciences roundtablerichard c. alkire, university of illinois at urbanachampaign, chairthom h. dunning,jr., pacific northwest national laboratory, vice chairpaul s. anderson,dupont pharmaceuticalsalexis t. bell,university of california, berkeleydaryle h. busch,university of kansasmarcetta y. darensbourg,texas a&m universitythomas f. edgar,university of texas, austinrichard gross,the dow chemical companyl. louis hegedus,elf atochem north america, inc.andrew kaldor,exxon r&d laboratoriesrobert l. lichter ,the camille & henry dreyfus foundation, inc.robert s. marianelli,office of science and technology policyjoe j. mayhew,chemical manufacturers associationwilliam s. millman,u.s. department of energykaren morse,western washington universitynorine e. noonan,u.s. environmental protection agencyjanet g. osteryoung,national science foundationgary w. poehlein,national science foundationmichael e. rogers,national institute of general medical scienceshratch g. semerjian,national institute of standards and technologykathleen c. taylor,general motors corp.marion c. thurnauer,argonne national laboratorymatthew v. tirrell,university of minnesotadiane a. trainor,zeneca pharmaceuticalsfrancis a. via,general electric companyisiah m. warner,louisiana state universitypatrick h. windham,windham consulting, atherton, californiastaffdouglas j. raber, director,board on chemical sciences and technologydavid a. grannis, research assistantruth mcdiarmid, senior program officersybil a. paige, administrative associateimpact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.ivboard on chemical sciences and technologyjohn l. anderson, carnegie mellon university, cochairlarry overman,university of california, irvine,cochairgregory r. choppin,florida state universitybarbara j. garrison,pennsylvania state universityalice p. gast,stanford universitylouis c. glasgow,e.i. du pont de nemours & companyjoseph g. gordon ii,ibmrobert h. grubbs,california institute of technologykeith e. gubbins,north carolina state universityjiri jonas,university of illinois at urbanachampaigngeorge e. keller,union carbide corporationrichard a. lerner,scripps research institutegregory a. petsko,brandeis universitywayne h. pitcher, jr.,genencor corporationkenneth n. raymond,university of california, berkeleypaul j. reider,merck research laboratoriesmartin b. sherwin,chemven group, inc.christine scheid sloane,general motors research laboratoriespeter j. stang,university of utahwilliam j. ward iii,general electric companyjohn t. yates, jr.,university of pittsburghstaffdouglas j. raber, directordavid a. grannis, research assistantmaria p. jones, senior project assistantruth mcdiarmid, senior staff officerchristopher k. murphy, program officersybil a. paige, administrative associateimpact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.viimpact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.prefacethe chemical sciences roundtable (csr) was established in 1997 by the national research council (nrc).it provides a scienceoriented, apolitical forum for leaders in the chemical sciences to discuss chemically relatedissues affecting government, industry, and universities. organized by the nrc's board on chemical sciences andtechnology, the csr aims to strengthen the chemical sciences by fostering communication among the people andorganizationsšspanning industry, government, universities, and professional associationsšinvolved with thechemical enterprise. the csr does this primarily by organizing workshops that address issues in chemical scienceand technology that require national attention.at its second meeting in december 1997, the csr identified the topic of information technology as an issueof increasing importance to all sectors of the chemical enterprise. as we rely increasingly on computers forobtaining, recording, communicating, and publishing the scientific data that enables progress in our discipline, it iscorrespondingly important that we consider the new and developing ways that this essential technology can beused effectively. at the same time, we must also consider the impact of the evolving technology on all sectors ofour discipline and on the ways that these sectors interact.to provide a forum for exploring this topic, an organizing committee was formed, and a workshop wasplanned for november 1998. the workshop, "the impact of advances in computing and communicationstechnologies on chemical science and technology," brought together research scientists and managers fromgovernment, industry, and academia to review and discuss the rapid changes in computer technology that areinfluencing activities in the chemical sciences.the papers in this volume are the authors' own versions of their presentations, and the discussion commentswere taken from a transcript of the workshop. the workshop did not attempt to establish any conclusions orrecommendations about needs and future directions, focusing instead on individual problems and challengesidentified by the speakers. by providing an opportunity for leaders in each of the areas to share their experienceand vision, we intended that the other workshop participantsšas well as readers of this proceedings volumešwould be able to identify new and useful ways of using theprefaceviiimpact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.tremendous power of computing and information technology in their own endeavors. we believe that theworkshop was successful in meeting this goal.workshop organizing commiteethom h. dunning, jr., chairthomas f. edgarjean h. futrellrichard m. grossbeverly k. hartlinerobert l. lichterthomas a. manuelrobert s. marianellijanet g. osteryoungmichael e. rogersprefaceviiiallen j bard.impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.acknowledgment of reviewersthis report has been reviewed by individuals chosen for their diverse perspectives and technical expertise, inaccordance with procedures approved by the national research council's (nrc's) report review committee. thepurpose of this independent review is to provide candid and critical comments that will assist the authors and thenrc in making the published report as sound as possible and to ensure that the report meets institutional standardsfor objectivity, evidence, and responsiveness to the study charge. the contents of the review comments and draftmanuscript remain confidential to protect the integrity of the deliberative process. we wish to thank the followingindividuals for their participation in the review of this report:judith hempel, university of california, san francisco,margaret g. kivelson, university of california, los angeles,david mclaughlin, kodak research laboratories,l. eugene mcneese, oak ridge national laboratory, andstanley i. sandler, university of delaware.although the individuals listed above have provided many constructive comments and suggestions,responsibility for the final content of this report rests solely with the authoring group and the nrc.acknowledgment of reviewersiximpact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.acknowledgment of reviewersximpact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.contents summary 11 the accelerated strategic computing initiativepaul messina (california institute of technology and department of energy) 82 software development for computational chemistry: does anything remain to be done?peter r. taylor (san diego supercomputer center and university of california, sandiego) 203 recent advances in computational thermochemistry and challenges for the futurelarry a. curtiss (argonnenational laboratory) and john a. pople (northwestern university) 26session 1 panel discussion 354 the role of computational biology in the genomics revolutionjeffrey skolnick, jacqueline fetrow, angel r. ortiz, and andrzej kolinski (scrippsresearch institute) 445 needs and new directions in computing for the chemical process industriesw. david smith, jr. (e.i. dupont) 626 vision 2020: computational needs of the chemical industryt.f. edgar (university of texas), d.a. dixon (pacific northwest national laboratory),and g.v. reklaitis (purdue university) 74contentsxiimpact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.session 2 panel discussion 917 collaboratory life: challenges of interactmediated science for chemiststhomas a. finholt (university of michigan) 978 a computer science perspective on computing for the chemical sciencessusan l. graham (university of california, berkeley) 1099 collaboratories: building electronic scientific communitiesraymond a. bair (pacific northwest national laboratory) 12510 the world wide laboratory: remote and automated access to imaging instrumentationbridget carragher and clinton s. potter (university of illinois at urbanachampaign) 14111 the wired laboratorydavid r. mclaughlin (eastman kodak company) 154session 3 panel discussion 17112 chemical data in the ﬁinternet ageﬂw. gary mallard (national institute of standards and technology) 17813 the digital library: an integrated system for scholarly communicationrichard e. lucier (university of california) 19014 electronic journal publishing at the american chemical societylorrin r. garson (american chemical society) 199session 4 panel discussion 210 appendixes a list of workshop participants 217b origin of and information on the chemical sciences roundtable 220contentsxiiimpact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.impact of advances in computing and communicationstechnologies on chemical science and technologyxiiiimpact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.xivimpact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.summarythe second workshop of the chemical sciences roundtable (csr), "impact of advances in computing andcommunications technologies on chemical science and technology," was held in washington, d.c., onnovember 12, 1998. the presentations and discussion at the workshop considered benefits and opportunities forchemical science and technology stemming from ongoing dramatic advances in computing and communications,as well as challenges to be met in using these technologies effectively for research and in applications addressingpressing national problems. this volume presents the results of that workshop.whither computing and communications technologies in chemicalscience and technology?paul messina of the california institute of technology emphasized the importance of ensuring thecontinuing evolution of computing power, unprecedented levels of which are required to meet such daunting needsas ensuring the safety and reliability of the nation's nuclear arsenal. in particular, he described the department ofenergy's accelerated strategic computing initiative (asci), whose goal is to simulate the effects of aging on theu.s. nuclear stockpile and to assess new weapon designs without further underground nuclear testing. to achieve acomputational simulationbased approach to testing will require computer systems capable of trillions toquadrillions of arithmetic operations per second. asci is working closely with the computing industry to ensurethat this highend computing capability is fielded in the next 5 to 10 years.in addition to investing in accelerated development of a new generation of massively parallel computersystems, asci is making major investments in computer systems software and scientific simulation software.rapid progress in both computing and simulation capability is required to make these systems usable foraddressing the targeted problems. this massive undertakingšwhich involves the academic community and theu.s. computer industry in addition to applications scientists and engineers at los alamos, lawrence livermore,and sandia national laboratoriesšis attempting to buildsummary1impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.a balanced system in which computing speed and memory, archival storage and capacity, and network speed andthroughput are combined to dramatically increase the performance simulations. the approach of usingcommercially available components to the extent possible will facilitate transfer of the new technologies for use in anumber of scientific and engineering pursuits without duplicating asci's costs.messina asserted that the advances in computing power that will become available in the next 5 to 10 yearswill be so great that they will change the very manner in which we pursue advances in science and technology.peter r. taylor, san diego supercomputer center and university of california, san diego, spoke on thestate of the art in computational chemistry and the extent to which it moots the requirements of the chemicalscience community. computational chemistryšwhose major activities can be classified as molecular electronicstructure (often referred to as quantum chemistry), reaction and molecular dynamics, and statistical mechanicsšisone of the great scientific success stories of the past three decades, as evidenced by the award of the 1998 nobelprize in chemistry to john pople and walter kohn. taylor described computational chemistry as a mature and verysuccessful field that nevertheless requires continuing effort to improve theories, methods, algorithms, andimplementation. he also pointed to a need for training students in these areas.more powerful computers will allow current methods to be extended to over larger molecules, but newmethodologies will be needed to address many of the problems of interest to chemists. taylor stated that thechemical sciences community needs to encourage the implementation of existing methods on now hardware, aswell as the development and implementation of new methods. as new methods are developed, possible advantagesoffered by new computer architectures can be considered; e.g., approaches previously precluded because ofrequirements for enormous memory might be perfectly feasible on asciclass machines. use of modern softwareengineering practices and modern computer languages in implementations can increase ease of maintenance. newmethods and implementations can also take advantage of modern storage, retrieval, and data managementtechnologies as well as interactive environments in which users can steer simulations and visualize their data.susan l. graham, university of california, berkeley, started by noting that highperformance computing isdifficult. she elaborated on the technical issues that must be addressed if we are to take advantage of the excitingopportunities offered by the ongoing revolutionary increases in computing power. she indicated that one way toget more out of computing is by using parallelismšit reduces the elapsed time required for the most demandingcomputations, keeps the calculation moving along when delays arise in sequential computation, and overcomesfundamental limitations bounding the speed of sequential computation, such as the speed of light. however,advances from parallelism won't come for free. issues that must be addressed in improving endtoendperformance of a calculation include identifying the work that can be done in parallel, correctly partitioning thatwork across the processors, and arranging the data so that it resides close to where it is needed (because ofcommunication delays). even then, at a lower level in the system, the system software (or a programmer) has todescribe the details of how the work is actually done. graham also mentioned issues in addition to performancethat are going to become increasingly problematic, such as security and fault tolerance.among the nontechnical issues mentioned were concerns about having enough people with the deepknowledge of both chemistry and information technology required for developing workable problemsolvingstrategies. in addition, graham pointed out that the scientific community will have to becomesummary2impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.involved in developing software for which vendors do not see a large market and in dealing with issues of accessto very high performance systems.graham closed by mentioning how implementation of the recommendations from the president's informationtechnology advisory committee (pitac), on which she serves, could help address such important issues as theneed for investment in longterm information technology r&d.computational modeling and simulation in chemical science andtechnologyjohn a. pople of northwestern university discussed the importance of having reliable data on thethermochemistry of moleculesšknowledge of which is vital in the chemical sciences and essential to manytechnologies. because experimental measurements yielding thermochemical data are difficult and timeconsuming, it is highly desirable to have computational methods that can make reliable predictions. since the early1970s when ab initio molecular orbital calculations became routine, one of the major goals of modern quantumchemistry has been the prediction of molecular thermochemical data to chemical accuracy (1 kcal/mol). thegaussiann series, with its latest version, gaussian3 (g3) theory, achieves that accuracy on average and iscomputationally feasible for molecules containing up to about eight nonhydrogen atoms; pople asserted that itrepresents a great success of quantum chemistry.ideally, a method for computation of thermochemical data should be applicable to any molecular system in anunambiguous manner. the method needs to be computationally efficient so that it can be widely applied, shouldreproduce known experimental data to a prescribed accuracy, and should be similarly accurate when applied tospecies having larger uncertainty or for which data are not available. the gaussiann methods were developedwith these objectives in mind. despite the successes, pople argued that much remains to be done. among thechallenges will be extension of the methods to larger molecules, increased accuracy in predictions, and extensionto heavier elements. the increased computing power obtainable from new generations of computers, such as thosewith massively parallel architectures, will play an important role in meeting these challenges.jeffrey skolnick of the scripps research institute discussed the role of computational molecular biology inthe genomics revolution. various genomesequencing projects are providing a plethora of protein sequenceinformation, but with no information about protein structure or function. making the results of the genomerevolution applicable to understanding biological processes requires knowledge of protein structure and functionas encoded in the genome. one means of sifting useful proteins out of the genomic databases is the computerprediction of protein function. to extend the level of molecular function annotation to a broader class of proteinsequences, a novel method for identification of protein function based directly on the sequencetostructuretofunction paradigm has been developed. the idea is to predict the native structure first and then to identify themolecular or biochemical function by matching the active site in the predicted structure to that in a protein ofknown function. skolnick believes that the next 5 to 10 years are likely to see the development of improvedcomputational tools for genomic screening.w. david smith, jr. of e.i. dupont described needs and new directions in computing for the chemicalprocess industries. changing needs for process and enterprise modeling and the capability of computers andsoftware have reached a critical point where vendors, academia, and industry must cooperate to develop the nextgeneration of tools for the process engineer. the potential for the european cape open project to bring thetechnologies and the players together to provide this set ofsummary3impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.tools was discussed. smith also explored how this technology may change the ways in which companies likedupont perform engineering and process development in the future.it is not enough to develop computational methods for modeling chemical processes; one must also facilitatethe use of these techniques by scientists. this was the underlying theme of the presentation by gregory j. mcrae1of the massachusetts institute of technology, who provided an overview of a problemsolving environment calledthe chemical engineering workbench. the workbench is currently being developed by a research team associatedwith the national computational science alliance. when completed, it will provide an integrated softwareenvironment supporting a broad range of computational tools for modeling chemical and engineering processes,extending from the molecular level to that of full chemical plants. quantum chemistry and other tools at themolecular level are being coupled with higherlevel chemical process modeling, chemical process reactionmodeling, plant process design, and process control. the team's initial effort is to develop an advanced reactordesign model that can be incorporated into the workbench. reactors are the focal points of chemical plants, andmathematically modeling these complex systems places great demands on highperformance computing. designconsiderations from the plant level will feed down to the quantum level. treating chemistry as a design parametermay allow development of innovative reaction systems that minimize environmental problems.thomas f. edgar, university of texas, david a. dixon, pacific northwest national laboratory, andgintaris v. reklaitis, purdue university, gave a multiauthor perspective on the computational needs of thechemical industry. the current forces driving the u.s. chemical industry, such as globalization, requirements forminimization of environmental impact, and the need for improved return on investment, require the expanded useand application of new computational technologies. forecasted future improvements in process modeling, control,instrumentation, and operations are a major component in the recently completed report technology vision 2020:report of the u.s. chemical industry,2 which presents a road map for the next 25 years for the chemical and alliedindustries. detailed r&d road maps on specific areas of chemical technology summarized in this presentationwere prepared in 1997 and 1998. the areas covered were instrumentation, control, operations, and computationalchemistry.remote collaboration and instruments onlineraymond a. bair, pacific northwest national laboratory, emphasized that new technologies for computingand communications offer opportunities to revolutionize not only the scope but also the process of scientificinvestigation. bair predicted that a significant contribution of collaboratories will be support for creating andsustaining scientific communities that can interact and share information rapidly to address challenging researchproblems. moreover, as the chemical applications and capabilities provided by collaboratories become morefamiliar, researchers will move significantly beyond current practice to exciting new paradigms for scientificwork.1 a written contribution for this presentation was not available for inclusion in the workshop proceedings.2 american chemical society, american institute of chemical engineers, chemical manufacturers association, council forchemical research, and synthetic organic chemical manufacturers association. 1996. technology vision 2020: report of theu.s. chemical industry. washington, d.c.: american chemical society.summary4impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.bair detailed some of the requirements for future success, including development of interdisciplinarypartnerships of chemists and computer scientists; flexible and extensible frameworks for collaboratories; means todeploy, support, and evaluate collaboratories in the field; input from the nation's research community todevelopment of the next generation of interact standards; and higherperformance networks, more scalable andcapable network standards, and better network management capabilities.in concluding, he pointed out the opportunity for competitive advantage provided by collaboratories that giveaccess to expertise, data, experiments, or computations that would not otherwise be available to explore a researchquestion or solve a problem. he also noted the positive impact that collaboratories can have on the complexity andscale of chemical problems considered, as well as the crucial part collaboratories can play in projects that dependon having access to large user facilities and/or multidisciplinary research teams. by removing many barriers oftime and distance, collaboratories can enhance the exchange of information in the sciences and can also contributeto the management of costs for travel and equipment use.bridget carragher and clinton s. potter, beckman institute for advanced science and technology,university of illinois at urbanachampaign, discussed their experience with the development of remote andautomated access to imaging instrumentation in the world wide laboratory (wwl) project. they proposedseveral ways of using remoteaccess technology in practice, including for service, collaboration, education andtraining, remote research, and automated and intelligent control of functions usually performed manually by alocal operator. among the advantages they described for remoteaccess technologyšwhich is one component of acollaboratoryšwere opportunities for consultation with experts located anywhere, access to a network ofdistributed expertise, and unprecedented opportunities for education, training, and access for users at institutionslacking the means to support expensive and unique instruments.specific examples they reported on involved remote work with a transmission electron microscope, nuclearmagnetic resonance imaging spectrometers, and a video light microscopešall of which are accessible in the wwlthrough webbrowserbased user interfaces. one k12 education project, chickscope, showed that very complexremoteaccess technology could be used effectively by students at all grade levels and also demonstrated all of thecomponents defined for a working collaboratory.they concluded by noting that wider acceptance of collaboratories in the general scientific community wouldrequire demonstration of their impact in the scientific research environment as well as a systematic evaluation oftheir contribution to productivity.thomas a. finholt, university of michigan, started by noting that despite the tremendous growth in theknowledge and practical application of chemical principles, the practice of chemistry research and teaching hasremained relatively unchanged. the use of the internet as a worldwide mechanism for scientific communicationchallenges this status quo. innovations such as collaboratories that remove constraints of distance and time onscientific collaboration and increase access to scarce instruments will accelerate the flow of information and placenew demands on senior scientists as mentors. finholt pointed out the need to anticipate and influence thedevelopment of emerging internet technologies that will affect how research is done. he discussed the challengesposed by new ways of conducting research in chemistry in terms of our transition from the past, through thepresent, and into an uncertain future.in reviewing "where we come from," finholt discussed three particularly important innovations: the creationand elaboration of the research laboratory, the use of laboratory classes in chemical education, and the use oflecture demonstrations to illuminate and clarify chemical principles. briefly describing "what we are," heconsidered opportunities that are now available to chemists through the expansion ofsummary5impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.computer and internet technologies and that can be thought of in terms of the raw performance of computerprocessors, the capacity of communication networks, the scope of networks, and the evolution of software.finally, in exploring "where we are going," he used examples from the present to project possible new modes ofteaching and doing research, including the collaboratory, that would enable researchers to perform their workwithout regard to geographical locationšinteracting with colleagues, using instruments remotely, sharing data andcomputational resources, and accessing information in digital libraries. he concluded by cautioning that whilewebbased tools may alter the landscape of practice and pedagogy, simply "surfing" for information will notreplace learning. to master and understand key concepts, students and researchers must absorb and reflect onideas and avoid the temptation to browse endlessly among an everwidening array of online resources.david r. mclaughlin, eastman kodak company, described how kodak has used computers andinformation technology to enhance operations in its research laboratories. the effort has focused on creating anelectronic or computerized laboratory and delivering information to the scientist's desktop. to illustrate the impactof kodak's ﬁwired laboratory," he described four ways that advances in computing technology have helped toincrease the efficiency of the analytical chemistry laboratory: the first through automation and simplification ofsome of the tasks associated with analysis and synthesis, the second in management of information andknowledge, the third in the generation and maintenance of data in electronic (digital) form, and the fourth throughdata analysis and chemometrics. examples given of components of the wired laboratory included quantum, anintegrated spectroscopy information system; a walkup spectroscopy laboratory with instruments online; increasedcapabilities for electronic access to information and analytical data; and wims, a webbased informationmanagement system. he also provided information on an electronic laboratory notebook that has been developedto assist with the management of experiments, projects, and programs.mclaughlin characterized the wired laboratory of the future as one in which all scientists would use anintelligent electronic laboratory notebook linked to all of the datagenerating equipment, and in which evolvinganalytical technology in combination with data analysis techniques would reduce the time required for samplepreparation and data interpretation. all of these capabilities would be provided through a common web interface.he indicated that a challenge to the analytical community is to devise realtime measurements that, when displayedin virtual reality systems, will enable researchers to "see" results and thus better understand them. he also notedthat highquality, reliable software available at reasonable cost is one of the most critical needs for the future.chemical information onlinegary mallard, national institute of standards and technology, discussed a variety of issues in themanagement of chemical data. he suggested that the large growth in publication of information on the internet isbeing driven by a reduction in traditional data resources, demand for faster access to data, and increased needs fordata for modeling and simulation. the last factor has a particularly strong influence because it has also changedthe nature of the data needed. he stressed the importance of quality assurance in data generation and management,including basic data storage and archiving. so that these data will be useful and reliable, critical evaluation mustaim to detect errors and inconsistencies in the information that can originate from incomplete data sets, uncertaintyin the data, and errors introduced during the compilation process. mallard stressed the importance of preventingand correcting errors as the scientific community's demands for data continue to increase.summary6impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.richard e. lucier, university of california, described work directed at developing a "digital library" toserve the entire university system in california. he argued that the conventional libraryšwith its archives oftraditional research journalsšis evolving toward new forms of scholarly communication. in his view traditionallibraries, with the services we have come to expect from them, will not continue to be sustainable. given theinformation explosion, they simply cost too much. costs will be controlled by forming large consortia of librariesto leverage buying power. he proposed that comprehensive access to information will replace comprehensiveownership of information, and that worldclass libraries will consist of complementary paper and digital holdings.in addressing possible new approaches to scholarly communication, he pointed out the conflicting goals of thecurrent system, which uses publications both as a means to disseminate knowledge and as a mechanism forevaluating the performance of research scholars. new approaches to electronic publication may allow these to bedecoupled. pursuing new approaches will require examination of current copyright policies and practices such asthe assignment of rights to publishers.lorrin r. garson, american chemical society, presented an analysis of issues related to the emergence ofelectronic publishing, using the perspective of a scientific society that is already a major publisher in the printmedium. he characterized scientific publishing as a field with high costs, diminishing resources for thosepurchasing publications, competition among publishers, and increasing pressure to publish more material. henoted that both commercial and notforprofit publishers must strive to operate on a "notforloss" basis. hepresented arguments that "firstcopy" costs account for approximately 80 percent of all publishing costs,regardless of whether paper or electronic distribution is the final result. consequently, the financial challenges areunlikely to disappear with a move toward electronic publishing.garson identified a range of important problems and challenges associated with electronic publishing,including the need for improvements in technology and funding of that investment, assumption of responsibilityand costs for archiving of electronic information, terms for and constraints on use of electronic information, andcosts of individual subscriptions. he was optimistic about progress in overcoming the technical barriers butindicated that the financial and sociological obstacles are formidable.summary7impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.1the accelerated strategic computing initiativepaul messinacalifornia institute of technology and department of energywhile the increase in computing power over the past 50 years has been staggering, the scientific communitywill require unprecedented computer speeds as well as massive memory and disk storage to address the pressingproblems that the nation will face in the 21st century. one such problem is ensuring the safety and reliability of thenation's nuclear arsenal while fully adhering to the comprehensive test ban treaty. to address this problem, theu.s. department of energy (doe) established the accelerated strategic computing initiative (asci) in 1996.1the goal of asci is to simulate the results of new weapons designs as well as the effects of aging on existing andnew designs, all in the absence of additional data from underground nuclear tests. this is a daunting challenge andrequires simulation capabilities that far surpass those available today.the goal of asci, however, is not a pipe dream. with funding from asci, the computer industry has alreadyinstalled three computer systems, one at sandia national laboratories (built by intel), one at los alamos nationallaboratory (lanl) (an sgicray computer), and another at lawrence livermore national laboratory (llnl)(an ibm computer), that can sustain more than 1 teraflops on real applications. at the time they were installed,each of these computers was as much as 20 times more powerful than those at the national science foundation(nsf) supercomputer centers (the partnerships for advanced computational infrastructure), the national energyresearch supercomputing center, and other laboratories. and this is only the beginning. by 2002, the computerindustry will deliver a system 10 times more powerful than these two systems and, in between, another computerwill be delivered that has three times the power of the lanl/llnl computers. by the year 2004šonly 5 yearsfrom nowšcomputers capable of 100 trillion operations per second will be available.who needs this much computing power? asci clearly does, but the other presentations at this workshopindicate that many chemical applications could also make use of this capability. similar1 based on the new capabilities being developed by asci, the department of energy, in its fy2000 budget submission,proposed to extend this concept to its civilian research programs. it requested $70 million for the scientific simulationinitiative, doe's contribution to the president's initiative on information technology for the twentyfirst century.the accelerated strategic computing initiative8impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.computational needs can be put forward by climate and weather modelers, computational materials scientists, andbiologists, for example. in the summer of 1998, nsf and doe sponsored the "national workshop on advancedscientific computing," which described the opportunities that this level of computing power would create for theresearch programs funded by nsf and doe.2 the good news is that, as a result of the enormous investment thatasci is making in these machines, it is likely that computers of this power will become available to the generalscientific community, and at substantially reduced cost.asci's need for advanced simulation capabilitythe asci program is a direct result of president clinton's vision, a vision shared by congress as well, thatthe united states can ensure the safety and reliability of its nuclear stockpile without additional nuclear testing.doe's asci program has been designed to create the leadingedge computational modeling and simulationcapabilities that are needed to shift from a nuclear testbased approach to a computational simulationbasedapproach. there is some urgency to putting this simulation capability in place as the nuclear arsenal is gettingolder day by dayšthe last nuclear test was carried out in 1992, so by the year 2004, 12 years will have passedsince the last test. in addition, nuclear weapons are designed for a given lifetime. over 50 percent of the weaponsin the u.s. arsenal will be beyond their design lifetime by the year 2004, and there is very little experience in agingbeyond the expected design life of nuclear weapons. finally, the individuals who have expertise in nuclear testingare getting older, and, by the year 2004, about 50 percent of the personnel with firsthand test experience will haveleft the laboratories. the year 2004 is a watershed year for doe's defense programs.the challengesthe accelerated strategic computing initiative is an applicationsdriven effort with a goal to develop reliablecomputational models of the physical and chemical processes involved in the design, manufacture, anddegradation of nuclear weapons. based on detailed discussions with scientists and engineers with expertise inweapons design, manufacturing, and aging and in computational physics and chemistry, a goal of simulating fullsystem, threedimensional nuclear burn and safety simulation processes by the year 2004 was established. anumber of intermediate, applicationsbased milestones were identified to mark the progress from our currentsimulation capabilities to fullsystem simulation capabilities. before developing the threedimensional burn code,codes must be developed to simulate casting, microstructures, aging of materials, crash fire safety, forging,welding of microstructures, and so on.we cannot meet the above simulation needs unless computing capability progresses along with the simulationcapability. to this end, the first asci computing system, an intel system ("option red"), was installed at sandianational laboratories in albuquerque in 1997. sandia and the university of new mexico wrote the operatingsystem for this machine and, by 1996, it had achieved more than 1 trillion arithmetic operations per second(teraflops) while still at the factory. it also had over onehalf terabyte of memory, the largest of any computingsystem to date. next, "option blue" resulted in the acquisition of two machines: an ibm system at llnl ("bluepacific") and an sgi/cray system at lanl ("blue mountain"). the ibm system achieved its milestone of over 1teraflops on an application2 department of energy and national science foundation. report from the "national workshop on advanced scientificcomputing" held july 3031, 1998, in washington, d.c., j.s. langer, ed.the accelerated strategic computing initiative9impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.on september 22, 1998, an event that was announced by vice president gore just one week before this workshop.the ﬁblue mountainﬂ machine was delivered in october 1998 and became operational in midnovember.this is the current status of the asci computing systems, but the story does not stop here. as followon tothe livermore acquisition, the same ibm contract that led to the 3teraflops system will be used to procure a 10teraflops/5terabyte (memory) system in mid2000. a 30+ teraflops machine is scheduled for delivery in june ofthe year 2001, and a 100teraflops machine is to be delivered in 2004. the above computer schedule is tied directlyto the scientific and engineering application needs through detailed estimates of the computing power, memory,and disk storage that the applications will need at any given time.to follow the pace outlined above, it is necessary to substantially accelerate what the u.s. computer industrywould otherwise do. this requires a partnership between asci, the applications scientists and engineers, and theu.s. computer industry. in order to produce a computer capable of 100 teraflops by the year 2004, asci settled onthe strategy of using the products that industry was already concentrating onšrelatively small, commoditypricedbuilding blocksšand assembling those components into big systems with the needed computing power. thisprocess seems more efficient because smaller building blocks present fewer problems from an innovationstandpoint, there is a much larger commercial market for them (which results in cheaper unit costs), and otherapplications would be able to benefit from the knowledge created in combining smaller elements. designing andbuilding a single computer from the ground up might benefit the asci applications, but would not serve the restof the scientific community except by replication of the entire system.the disadvantage of building a computer system from many smaller units is that the interconnections betweenthese units can become overwhelmingly complicated. scientists nowadays use 32 or 64 processors, but a differentmethod is needed to connect 6,400 processors. not only does the network have to have different characteristics,but the software does also. the software must have fast (lowlatency) access to memory, even on systems such asthese, which have a very complicated memory structure. this is necessary not only for largescale simulations, butalso for the transfer of data and the visualization of results. there is another problem associated with the use ofcomputer systems of this power and complexity: the burden on the user. it will be a challenge to keep the overheadon the user at a reasonable level, especially if the user is at a remote site.where are we now?there has been an inteldesigned and intelbuilt asci computer at sandia national laboratories inalbuquerque for 2 years. it has 4,500 nodes, 9,000 processors (two per node), and a peak performance of 1.8teraflops. during that 2year period, it was the fastest computer on the planet for scientific simulation. it has a fairamount of memoryšhalf a terabytešand a very high speed network interconnecting the 4,500 nodes. it isphysically a very big system, built from the highestvolume manufactured building blocks availablešthe intelpentium pro microprocessor. this machine has been used for a number of breakthrough simulations and has beeninvaluable for the scientific and engineering application teams in their efforts to develop simulation software thatscales to large numbers of processors.the nodes on the two socalled blue systems, blue mountain at los alamos (sgi/cray) and blue pacific atlivermore (ibm), are symmetric multiprocessors (smps), which are interconnected by a scalable, highspeedcommunications network. the ibm terascale computer system, which was delivered to llnl in january 1999, is athreemachine aggregate system where each machine comprises 512 fourprocessor nodes, 488 of which are usedfor computing. the remaining nodes connect to a routerthe accelerated strategic computing initiative10impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.that links the three separate machines. all together, the blue pacific system contains 5,856 processors and has atheoretical peak performance of 3.9 teraflops. the sgi/cray computer at los alamos has only 48 nodes, but eachnode has 128 processors (sgi origin 2000s). the peak performance of the sgi/cray blue mountain system isaround 3.1 teraflops. these two computer systems, although they use the same fundamental architecture, clearlyrepresent two extremes of that architecture. there is n tradeoff between a more complex interconnection of fairlysimple nodes in the one case and a simpler connection of more complex building blocks, smps with 128processors sharing memory and providing cache coherence, in the other case. although the two architectures arethe same, tuning a scientific application to be equally efficient on both computers is difficult because theinterconnect network is different, as is the amount of shared memory for each processor.table 1.1 characteristics of asci computer systemsmachinecharacteristicsintel (sandia)blue pacific(livermore)blue mountain(los alamos)projected 10teraflopsprojected 30teraflopsperformance (peakteraflops)1.83.931030dates operationalmay 1997september 1998november 1998june 2000june 2001processorpentium propower pcmips r10000power pc?number of nodes4,5681,46448512?processors per node2412816?number ofprocessors9,0005,8566,1448,192>8,000 (?)the architecture of the asci blue pacific and blue mountain computersšinterconnected smpsšwill be thedominant architecture over the next few years. moore's law will lead to faster processors,3 of course, and therewill be different sizes of shared memory processor building blocks and different flavors of the interconnect, butthis will be the fundamental architecture by which we will attain the 30and 100teraflops performance levels. it isjust not feasible to create a superfast processor in the next 5 years that could attain such performance levels withonly 100 or so processors. so, we have to deal with the level of complexity that smpbased architectures imply.the specifications for these computer systems are summarized in table 1.1. figure 1.1 shows the asci platformsroad map.it is worth noting, however, that these levels of complexity are not overly different from what we have beendealing with in some laboratories and universities for over 15 years. in 1986 there were systems containing 512processors (as separate nodes) that presented many of the challenges that asci is facing. soon after those efforts, afew thousand processors were successfully connected. however, this does not diminish the challenge that asci isundertaking. it is one thing to get a few computational kernels running on a parallel computer and another thing toget the massive codes used to simulate physical and chemical systems running efficiently on the complex ascimachines.as an example of the applications now possible on the asci machines, a threedimensional calculation usedthe ares code to simulate rayleightaylor turbulent mixing and hydrodynamic instabilities found in supernovaeand contained over 35 million zones and used 1,920 processors on the3 moore's law, actually a "rule of thumb," states that the computing power of microprocessors doubles every 18 months orso.the accelerated strategic computing initiative11impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.blue pacific (figure 1.2). the ardra code was used to simulate the neutron flux emitted by a nova laserexperiment in a calculation using 160 million zones in 33 hours using 3,840 processors. efficient and accurateneutron and photon transport calculations have been conducted on the blue mountain machine, demonstrating alram grid resolution using 100 million particles. on a singleprocessor machine like those typical of most researchenvironments, these calculations would take 10 to 100 years to run. perhaps more important, some ascisimulations have already led to insights about what caused some previously notunderstood historical nuclear testresults.figure 1.1asci computing systems.accelerating technology developmentsasci has an effort it calls "pathforward" whose goal is to enable u.s. companies to develop technologiesneeded to produce the nextgeneration ultrascale computing systems for asci. pathforward draws on thecapabilities, availability, expertise, and products currently being produced by leading computer companies,focusing on interconnect technologies, data storage technologies, systems software, and tools for largescalecomputing systems. these technologies, while critical to asci's platform needs, are areas in which private sectordevelopment would not otherwise take place, at least not in the time frame required by the stockpile stewardshipprogram. at the same time, they arethe accelerated strategic computing initiative12impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.investments in which industry sees value for future products and markets: essential scaling and integratingtechnologies that enable ultrascale computing systems to be engineered and developed out of commoditycomputing building blocks.turbulent mixing, as it occurs in supernovae, represents the combined effects of nuclear reactions and hydrodynamicmixing that are relevant to stockpile stewardshipthe combination of an efficient hydrodynamic algorithm and the performance capability of the blue pacific makessuch a highly resolved threedimensional simulation possiblefigure 1.2areas supernova simulation. this calculation included a modest 1098 × 180 × 180 spatial mesh with more than 35million zones. it was completed over a 72hour period on 1,920 processors.simulation development environmentas we all know, software and algorithms are at least as important as hardware capabilities. here we havesome promising indicators. new methodologies for developing portable and modular application programs arestarting to prove themselves. some asci codes run on all three asci teraflops systems, and enhancements ofthose codes can be implemented in a fraction of the time that was required with traditional methods.algorithms that scale to 6,000 processors have been developed and implemented for a number of asciapplications. future asci machines are expected to have no more than 10,000 processors.an objective is to provide a usable and scalable applicationdevelopment environment for asci computingsystems, enabling code developers to quickly meet the computational needs of scientists. the development staffworks with code developers to identify user requirements and evaluate tool effectiveness, and it develops directties with platform partners and other software suppliers to ensure an effective development environment.an essential part of the asci program is the challenge of solving threedimensional computational problemsthat are far larger than we have ever solved before. this challenge translates to more data, more computationalcycles, and a succession of more powerful computing platforms. from the computhe accelerated strategic computing initiative13impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.tational viewpoint, the issue is not simply bigger and faster, but rather a fundamental shift in the way problems aresolved. codes, tools, algorithms, and systems that fail to scale properly as the data and hardware resources groware useless to asci developers.an evolving effort is a simulation development environment that promotes as much commonality acrossasci platforms as possible. scalability of tool and library interfaces to address hundreds and thousands of centralprocessing units is under investigation. infrastructure frameworks and math software needed by the asci codeteams are in progress, and tools to support code verification and validation are being evaluated. this includescurrent and emerging standards for languages and programming models, such as mpi and open mp; commonapplication programming interfaces for parallel input/output (i/o) and linear solvers; common parallel tools; andcommon ways of accessing documentation.the challenges aheadthere are many challenges in addition to those associated with processing power and memory. a simplifiedway to look at the asci program is as a combination of largescale computing resources, massive scientificdatabases, and telelaboratories, all of which must be put into place to support scientific collaborations, performlargescale simulations, explore complex data sets, and achieve scientific understanding. asci is attempting tobuild a simulation capability that provides a balanced systemšone in which computing speed and memory,archival storage speed and capacity, and network speed and throughput are combined to provide a dramaticincrease in the performance of scientific simulations. one really does not have a new simulation tool if all onedoes is collect a bunch of computers in a warehousešit would be no more than the collections of pcs found inmost universities or laboratories today. one must design and build a wellintegrated computing system thateliminates the many possible bottlenecks to achieving the needed simulation performance levels (a concept veryfamiliar to chemists).archival storage systems and data managementas computer systems such as those described in table 1.1 become operational, they will produce a staggeringamount of output datašhundreds of terabytes to tens of petabytesšper simulation run. we must be able to storethat data and, later, recall it for analysis. this requires the development of efficient datamanagement techniquesas well as fast input/output (i/o) mechanisms. data management is becoming a very complex issue. there will beso much data that arbitrary file names will no longer be a reasonable format. also, with a system consisting ofthousands of processors and a complex of online disks and archival storage systems, these resources will have tobe managed as a single system to provide a rational distributed computing environment.input/outputchemists are familiar with the importance of efficiently performing i/ošprograms like gaussian do atremendous amount of i/o. i/o is critical for many scientific simulations in addition to quantum chemistryapplications. i/o has been a problem for massively parallel computers, for few systems in the past had trulyscalable i/o subsystems. the new machines described in table 1.1 provide muchenhanced i/o capabilities and,for the first time, we are beginning to find truly scalable i/o subsystems in massively parallel computer systems.the accelerated strategic computing initiative14impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.networksthere will be a need for highspeed networks to connect users to the archival storage systems so that they canaccess and analyze the results of past simulations. much of the analysis of the output of the simulations will bedone using visualization techniques, but these will not be the simple visualization techniques of today. because ofthe tremendous amount of data that will be produced, the user must be able to switch easily from coarseresolution, providing a broad overview of the results, to fine resolution, providing a detailed view of a small regionof the data very efficiently. otherwise even the highestspeed networks available will be overwhelmed.data analysis and visualizationwe will have to develop new techniques to allow users to analyze the data. if the needs of the applications areto be met on schedule, we must develop a comparable schedule for the development of the infrastructure for"seeing and understanding" the results. to do this will require not just developing bigger display screens, but alsoinnovation in how we display and interact with graphical data. to develop plans to address this problem, asci hascollaborated with the national science foundation on a series of workshops. a report describing the output ofthese workshops was recently published that lays out a 10year research and development agenda for visualizationand data manipulation.4 asci will be tackling this problem collaboratively with the academic community as wellas other scientific communities.the scientists and engineers involved in asci will be linked together by "data and visualization" corridors, sothat users can access and interact with the needed resources. the capabilities provided by the "data andvisualization corridors" are crucial for impedance matching between the computing systems, the largescalesimulations, and the users because the associated time scales are very different. people work in minutes and hours,and computer results in some cases can take days, and in some cases milliseconds, for generation. in the end, thesystem must operate at human time scales, for it is humans who are seeking knowledge. the development of the"data and visualizationﬂ corridors is also an effort that will be performed in collaboration with the researchcommunity in general.other challenges abound. for example, how do you represent the data? isosurfaces and streamlines arepossible methods; movies or images can be used to analyze many billions of numbers. the data sources may berealtime or the results of simulation, while other sources may be historical. the scientists and engineers involvedin these activities are separated geographically, and there is often temporal separation because they need to refer toprevious results. to guide this effort, a road map has been developed for the datavisualization efforts in the asciprogram. the road map links applications, computers, data management, and visualization corridors to ensure thatall of the needed capabilities are developed in a timely fashion.i/o is also important in analyzing the results of the simulations, to "see and understand" the informationburied in the data. we estimate that scientists and engineers will need to be able to interact in real time with about75 billion bytes of data by early 2000. along with this is a requirement to be able to perform queries on a dataarchive containing roughly 30 terabytes with transfer rates of perhaps 6 gigabytes a second. this is much fasterthan anyone can do right now, and yet it is critical to achieving the goals of asci. by the time we reach the end ofthe year 2000, we estimate that asci's scientists and4 national science foundation, information and data management program. proceedings from 1998 information and datamanagement workshop: research agenda for the 21st century. nsf, washington, d.c.the accelerated strategic computing initiative15impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.engineers will need to interact with a quarter of a terabyte extracted from an archive of at least 100 terabytes with a20gigabytesper second transfer rate. and, the growth will not stop there; the root cause of these highrequirements is the detail with which asci must be able to simulate weapons systems.scientific simulation softwaredeveloping scientific simulation software for smpbased parallel computer systems presents a number ofchallenges because it entails use of a "hybrid programming model." the hybridprogramming model incorporatesboth of the standard programming models for parallel computers: the sharedmemory and messagepassingprogramming models. the sharedmemory programming model relies on the ability of the programmer to accessmemory by its location or name. this approach provides reasonable performance with as many as 128 nodes, allsharing memory. however, although the memory within an smp is shared, to attain reasonable performance theuser must explicitly make the program parallel. current compiler technology is not yet at the point that it can berelied on to produce efficient parallel code on more than 8 to 16 processors. to pass data between the smp nodes,the programmer must send an explicit message that communicates the need for a piece of data. this message ispassed to the node on which the data resides, which then transmits the requested data to the original node. this isthe messagepassing programming model. both the sharedmemory and the messagepassing models have been inuse for some years but with very, very few exceptions, not together. although it is possible to use the messagepassing model within the smp node as well as between the smp nodes, the hybrid programming model willprovide the best performance for the overall system since it can take full advantage of the fast sharedmemoryaccess possible within a node. so, we have some new things to learn!computer systems softwarebecause the systems being acquired by asci are all firstofakind, perhaps even oneofakind systems, thecomputer manufacturers typically cannot provide all the software and tools that are needed to efficiently operateand develop software for the computer. asci must provide the missing pieces. one task that asci has undertakento accelerate the development of application software for these machines is the construction of a "problemsolvingenvironment." the pse provides an integrated set of tools, like compilers and debuggers, for producing code forthese massively parallel computers. problem generation and verification will need another set of such tools, aswill highspeed storage and i/o. finally, the distributed computing environment as well as the associated networkswill have to be enhanced as well.the asci allianceslos alamos, livermore, and sandia are not alone in this process. the academic community is contributing tounderstanding how to create the advanced simulations and how to run these simulations on terascale platforms.asci is funding five large university teams, each of which is attacking a very complex, multidisciplinarysimulation problem. these teams are in the first year of their initial 5year funding and have already contributed anumber of ideas both in computer science and in science and engineering. there are also a number of otheruniversity groups, about 30 in all, that are working on more focused aspects of the problem.the accelerated strategic computing initiative16impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.caltech is one of the five university groups receiving asci funding. the caltech team is tackling the reactionof various materials to shock waves. in the caltech program a detonation is used to create a shock wave in a shocktube, which then impacts the test material. the goal is to develop techniques to simulate the effect of the shockwave on the test material and to understand how that effect is related to the composition of the test material. tosimulate this very complex phenomenon, we must be able to simulate fluid flows, explosions, chemical reactions,shock waves, and materials properties. the effort requires simulations at the atomic level all of the way up to themacroscopic level. this type of academic research will contribute to a general understanding of how to performcomplex, multidisciplinary simulations and so will help general science as well as asci. there are a largenumber of scientific and engineering applications that need to be able to simulate such complex processes.asci's contributions to the greater science communityasci will contribute to science in several ways. it will provide the foundations for building a "science ofsimulation." through asci we will gain a detailed understanding of all the topics and methods that are needed tocreate multiphysics simulation, multisystem engineeringdesign codes, and use them to carry out predictivesimulations of complex phenomena. science and industry alike will benefit from asci's development of vastlymore powerful computational methods, algorithms, software tools, methodologies for program development,validation, and verification, code frameworks, and computing systems with ever greater computing power andmassive memories.asci projects are devoting considerable attention and resources to the development of physics models thatare grounded on theory and experiment and lead to practical numerical computations. these models are thenincorporated into the simulation programs and validated against experimental data and predictions based onaccepted theory. while this approach is not unique, the difference here is the computational scale that is possibleand use of multiple, intertwined physics models in a single simulation code.asci is investing in better algorithms and computational methods. even asci's extremely powerfulplatforms will not be able to tackle the weapons stockpile computations by applying bruteforce algorithms.software design and development for such applications on such machines are particularly challenging andcontroversial aspects of multiphysics scientific simulations. the experience base is currently limited to a fewisolated projects from which it is difficult to draw general conclusions. instead, asci will carry out multiplesubstantial simulation projects that will provide proofs of principle, at scale, of software developmentmethodologies and frameworks that provide portability, extensibility, modularity, and maintainability whiledelivering acceptable performance.asci will have shown, through actual accomplishments, the potential of terascale simulations for a broadrange of scientific and engineering applications. by doing so, and by demonstrating the feasibility of puttingtogether and using computers of much greater scale than would otherwise be available, asci is also reenergizingboth the scientific computing research community and the u.s. commercial computer industry. several federalagencies are already planning to invest in very highend computing, and others are sure to follow. computermanufacturers are seeing renewed customer interest in much higher performance, and there is growing realizationthat there may be a financially viable approach to getting into the highestend markets. the strategy consists ofdesigning the standard midtolargescale products that sell in large quantities so that they are also suited to serveas building blocks for truly large systems. in addition to stimulating the highend computer industry, industry willinherit computational tools that will enable it to design better, more competitive products. computer simulation isalreadythe accelerated strategic computing initiative17impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.used extensively in product design and optimization. the much greater simulation capabilities developed byasci, hardware and software, will enable industries of all kinds to produce even better products.perhaps the most important product of asci will be a cadre of thousands of people with solid experience andexpertise in predictive, multiphysics simulation. this alone will have a major impact on the future of scientificand engineering computing.summaryover the next few years, we will see remarkable increases in the power of highend computing capabilities,from 1 teraflops today to 100 teraflops in 2004. doe's accelerated strategic computing initiative is driving thisincrease and is using the resulting computer systems immediatelyšas soon as they are manufacturedšto address abroad range of scientific and engineering simulations. these simulations are critical to ensuring the safety andreliability of the nation's nuclear arsenal. but because the approach is applicationsdriven, the computer systemsbeing developed will lead to computers that are suitable for most, if not all, highend technical computingapplications.to the extent possible, asci is using commercial building blocks to leverage the cost efficiency of the highvolume computing industry. this approach will make the resulting computer systems readily replicable for othersšand, undoubtedly, at substantially reduced cost, because asci is funding much of the needed development efforts.the recent report from the nsf/doesponsored "national workshop on advanced scientific computing"outlined a number of scientific and engineering areas that would be dramatically affected by access to this level ofcomputing capability.5taking advantage of this new level of computing capability presents a large number of challenges. we haveto consider the computing infrastructure as a wholešprocessing speeds and memory, data storage andmanagement, data analysis and visualization, and networking speed and capacityšif we are to realize thepromised dramatic increase in computing capability. we will also have to discover new computational algorithms,programming methods, and even problem formulations in some cases. despite these challenges, the payoff will besubstantial. the advances in computing power that will become available in the next 5 to 10 years will be so greatthat they will change the very manner in which we conduct the pursuit of science and technology.discussiondavid rothman, dow chemical company: let me say you have succeeded in convincing me how punymy computing project is right now. but one thing i think we have in common is that in any computing project youcan identify a number of potential ratelimiting steps in getting to the goal, and you can generally identify one ortwo things that are very high risk items. can you tell me what you see as the one or two of the highestrisk parts ofthis overall program you have to reach your goal?paul messina: i would consider that the highest risk is the operating system and then, software, and tools.also risky are general issues such as whether the compilers will reach the point of having a respectable fraction ofthe potential speed of the system. the next highest risk would be getting the algorithms in the time scales wherethey use fairly efficiently the thousands of processors.5 department of energy and national science foundation. report from the "national workshop on advanced scientificcomputing" held july 3031, 1998, in washington, d.c., j.s. langer, ed.the accelerated strategic computing initiative18impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.i have no doubt whatsoever about the ability to do that for scientific algorithms given enough time, so it is theaccelerated schedule that is a source of high risk; people are not used to having to program that way. so, it is agood combination of the system's software, which tends to lag pretty badly in terms of its ability to exploit thesevery complicated processors, and then the applications.the one thing i did not emphasize at all is that the way we have obtained this wonderful moore's curve forthe processor increase in speed is not so much by having faster clocks on our cycle speeds, but by making the gutsof the processor much more complicated. and so, theoretically, if everything works you get this wonderful speed.but it means keeping several arithmetic units busy, loading and storing data simultaneously, and that is a hardthing to do. so, to actually get the benefits of these faster processors, even one processor, is beyond what manycompilers can do.andrew white, los alamos national laboratory: thom mentioned the science simulation plan and thepitac response as a new program for fy2000. how would this new initiative and asci/stockpile stewardshipplay together?paul messina: one of the things that i hope to do in part during my 2 years in asci is to work out a waythat the two efforts can get mutual benefit. so, specific things that i can imagine are that perhaps the ssx ssstarprogram will select machines that are similar to the ones that asci has selected. then, these two things that i justidentified as the highestrisk items, the system software and tools and the algorithms, could be developed jointly sothat we would be able to actually use the machines. i would say that is a real target of opportunity, to have verysimilar machines instead of deciding to diverge here and therefore have to develop all new and differentalgorithms.so, at the level of helping to mature the system, i would first think of software, tools, and the algorithms forthe applications. one could imagine sharing facilities. that is practical, but often politically unpalatable, so i donot know that there is much hope of doing that. but i think that in figuring out how to use these new systems,making them robust earlier and sharing the results, and maybe even having joint teams doing applications, wouldhelp both tremendously.thom dunning: i think one of the real benefits of asci is that it has made computational scientists start toask such questions as, what would we do if we had 100 times more power on the desktop or 10,000 times morepower on the very highend systems? these are very good questions to be thinking about because that is thedirection that the computer industry is taking us, whether it succeeds on the accelerated time scale as outlined withasci or whether it comes on a little more slowly.the accelerated strategic computing initiative19impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.2software development for computational chemistry: doesanything remain to be done?peter r. taylorsan diego supercomputer center and university of california, san diegowe consider the state of the art of computational chemistry and then discuss to what extent this state of theart meets the requirements of the chemistry community. our overview is necessarily broad and somewhatsuperficial, but it supports the view that while computational chemistry is a mature and very successful field,considerable effort is still needed at the level of fundamental research into methods, algorithms, andimplementation, and in training students in these areas.for the purposes of this paper, we consider computational chemistry to comprise the study of the structure,properties, and dynamics of chemical systems. we recognize that this somewhat narrow definition does notproperly incorporate areas such as process modeling that are also of importance to the chemical industry, but thediscussions at the workshop strongly suggest that the successes and, not ﬁfailures," but let us say "unmetexpectations," in these other areas are not different in cause or in possible remedies from those in our narrowerdefinition of computational chemistry. with this definition, then, we can argue with some justification thatcomputational chemistry is one of the great scientific success stories of the past decades. twentyfive years agoquantumchemical calculations were performed by quantum chemistry specialists, and the results of suchcalculations were rarely and with difficult acceptance published in general chemistry journals such as the journalof the american chemical society. at that time calculations on molecules of more than a dozen atoms were ararity, and the accuracy of the predictions was often overstated and seldom convincing to experimentalists. weneed not labor the point here: suffice it to say that the situation today is exactly the reverse. indeed, the field ofcomputational chemistry has just been recognized by the award of the 1998 nobel prize in chemistry to pople andkohn. might it not, therefore, be time to declare success in this endeavor, and that the field is essentiallycomplete? in this essay, we argue not only that this would be a grievous mistake, but also that considerableadditional effort is required in a number of areas for computational chemistry to reach its full potential. our fieldis mature, not complete; ripe with opportunities, not sterile.the major activities in computational chemistry can be classified as molecular electronic structure (quantumchemistry), reaction dynamics, and molecular dynamics. these are used to calculate, respectively, the propertiesof individual molecules or groups of molecules, kinetic information and reactionsoftware development for computational chemistry: does anything remain to be done?20impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.pathways for chemical reactions, and the properties and dynamics of large molecules or large assemblies ofmolecules. all of these activities have been extraordinarily successful over the past decades and have firmlyestablished computational chemistry as a third methodology alongside experiment and theory. computationalchemists have also been among the foremost users of computer hardware, with substantial requirements forcomputer time, memory, and disk space, and with a history of exploiting both early access to new architectures andnovel solutions to reduce the cost of performing their research. they have successfully exploited new vector andparallel computer architectures as they have become available, and at the same time have developed newalgorithms to efficiently use first minicomputers and later risc workstations, and most recently clusters ofcommodity pcs. the advent of asciclass computing resources presents a new opportunityša vast increase incomputational capability to be exploited. however, it is not obvious that the community is ready to use suchmassively parallel machines, not the least because even our scalable algorithms have generally been tested insituations only up to a few hundred processors. to use the full power of an asciclass machine will requiresuccessfully harnessing at least an order of magnitude more processors. will our current algorithms, and at an evenhigher level, the computational chemistry methods they implement, be suitable for such architectures? it is thesequestions we concentrate on here, not the detail issues of whether messagepassing implementations are more orless appropriate than sharedmemory implementations of our methods. we demonstrate, for example, that much ofour current methodology is incapable of extending the accuracy of our description of molecular systems beyondwhat we can currently achieve, and that new methods must be sought.we take quantum chemistry, as defined above, as an example. the information for nonempirical dynamicscalculations comes from quantumchemical calculations, so in this sense quantum chemistry is fundamental tononempirical computational chemistry. typical quantum chemistry calculations, in 1999, treat a single moleculeor perhaps a group of molecules, in vacuo, at a temperature of 0 k. the accuracy achievable varies with the size ofthe molecule (see figure 2.1), but the highestaccuracy work is comparable to experimental accuracy for manyproperties. a simple way to represent the relationship between molecular size and accuracy of results is a graphgenerally referred to as a pople diagram.figure 2.1size/accuracy relationship in ab initio calculations.software development for computational chemistry: does anything remain to be done?21impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.figure 2.1 shows the accuracy achievable for different sizes of molecule (or assembly of molecules) and themost common quantumchemical methods used to achieve that accuracy. one important aspect of figure 2.1 is theintersection of the curve with the ordinate axis at around 0.1 kcal/mol. this is not a resolution artifact or a mistakein the figurešit reflects a fundamental limitation in our ability to describe the quantummechanical motion of theelectrons in a molecule, specifically, the cusp behavior as two electrons approach one another. the methods listedin the figure are all based on expansions in oneelectron functions, and such expansions are inherently incapableof describing the cusp in a finite number of terms. increasing the number of terms cannot proceed indefinitely, notonly because the calculations become impossibly large, but also because the resulting expansion function setsbecome linearly dependent. here is one example of a fundamental problem that must be addressed in order toincrease our computational chemistry capabilitiesšwe must develop new methods that better approximate themolecular wave function.an immediate question might be, is such increased accuracy really necessary? a flippant response would bethat everyone would like more accuracy: like money, or network bandwidth, it is something one cannot have toomuch of. a more realistic answer would be that an organic chemist planning a synthesis might find herself in asituation where a difference in barrier heights of a few tenths of a kcal/ mol determines whether the ratio of desiredproduct to useless alternative is 9:1 or 1:9. barrier heights known to 1 kcal/mol or so would be quite useless in thissituation. and, as noted above, the accuracy of computed quantumchemical energies determines the reliability ofnonempirical dynamics calculations, so if more accurate dynamics calculations are desired, the accuracy of thequantumchemical calculations must be improved.the other axis in figure 2.1 is molecular size. again, the crucial question is how to increase the size of thesystem we can treat. this is not merely in order to treat larger molecules per se, although this is one consideration.another important issue is the realistic treatment of environment. most chemistry takes place in the condensedphase or in a gas phase of some density, not in a vacuum. although some progress has been made in describingsolvent effects by embedding the system in a dielectric medium, there is good evidence that detailed solutesolvent interactions are so important in many situations that some solvent molecules must be treated explicitly. wemust therefore increase the size of our systems to include perhaps tens of solvent molecules, and thus must developmethods to handle (much) larger systems.we reiterate that we are not suggesting here that access to much more powerful computers alone is anadequate solution to the accuracy and size problems (indeed, in the case of increased accuracy, it is manifestly notadequate). what is needed is to improve or to develop new methodologies so that they can handle the problems ofinterest in the way we wish to treat them, taking advantage of the increased computing power of asciclassmachines to achieve that end. the first steps must be in methodology, not in porting existing approaches to newarchitectures.what of other components of computational chemistry, such as reaction dynamics or molecular dynamicssimulations? a chat with practitioners in these areas is likely to indicate a need to perform more accuratedynamics studies, on larger systems, including the effects of bulk environment, etc., plus an additional concernabout studying phenomena at longer time scales (that is, how best to simulate phenomena that take place in natureon a time scale of microseconds to milliseconds when normal simulation time steps are at the femtosecond level).in the case of increased accuracy and larger systems, the issues are of course similar in spirit to those discussedabove in the context of quantum chemistry. indeed, increased accuracy in reaction dynamics and simulationswould probably mandate increased accuracy (for larger systems) in the quantumchemical calculations used toprovide potentialssoftware development for computational chemistry: does anything remain to be done?22impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.for the dynamics. our attention is again focused on the need to develop new methods rather than to simply look atways to implement existing methods or to port existing programs to new architectures.development of new computational methods will lead to new implementations for those methods, whichbrings additional advantages. first, there is the opportunity to consider explicitly the nature of new computerarchitectures when developing the methods. for example, methods that might have seemed inappropriate in thepast because their implementation would have enormous memory requirements may be perfectly feasible in thelarge memory environment provided by asciclass machines. second, new implementations can take advantageof modern software engineering practices and modern computer languages, leading to increased ease ofmaintenance compared to the traditional "dusty decks." third, new methods and implementations can takeadvantage of modern technologies, such as the ability to store, retrieve, and manipulate large data sets, orinteraction environments via which users can not only visualize their data but also steer simulations.as we noted at the beginning of this essay, computational chemistry has been a remarkably successfulsubdiscipline. nevertheless, we repeat also that success and maturity should not be confused with completion; ourefforts have been very successful, but much remains to be done. a primary need is to encourage the developmentof new methods, and their implementation, rather than just seeking to reimplement existing methods on newhardware. efforts are needed to increase the accuracy of our results and to achieve that accuracy for larger andlarger systems, to model experiments more realistically, and to develop software appropriate to new hardware soas to fulfill the opportunities offered by asciclass machines. these efforts, in turn, require strong support fromagencies and institutions, not only to support research and development activities, but also to provide a cadre oftrained computational scientists through support of education and training.acknowledgments: the author was supported by the national science foundation through cooperativeagreement daci9619020 and by grant no. che9700627.discussionsam kounaves, tufts university: considering the amount of investment that has been put into these typesof supercomputing centers and initiatives, one of the final outcomes is, of course, the ability to transfer some ofthis technology to other areas. in my own particular specialty, for example, we would love to have more in termsof modeling electrode position nucleation for the fusion phenomena. microsoft last year spent $3 billion just todevelop software, and that may not be significant. but are there any plans for transferring? what type of systemsdo you foresee, for example, to make it more generalized, to make it accessible to other communities in thechemistry world?peter taylor: there are a couple of efforts, i think. some of the work done at emsl (environmentalmolecular sciences laboratory) at pnnl, in northwest chemistry codes, and some of the activities done underour own support in san diego and our larger partnership, national partnership for advanced computationalinfrastructure, attempt to do just this. and i think there has been some success with some of these things already.both of these projects really are just getting rolling in terms of dealing with the outside community. but we haveplans to do this and i would expect to see the fruits of this over the next couple of years.that, i think, is something we can do in cases where we already have the methods that we need. if newmethods have to be developed, then, of course, one has to be less specific about the lead time. we simply do notknow how long it will take to develop some of the new methodology, and so forth. but insoftware development for computational chemistry: does anything remain to be done?23impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.a number of communities, certainly those supported by the department of energy and in the nsf program, thereis a desire to harden these things and move them out so that use can be made of them in a much larger community.peter cummings, university of tennessee and oak ridge national laboratory : i just wanted tocomment, peter, about the things that you said were coming from the molecular dynamics community, which iwill just generalize as the classical simulation community. i think it is not simply an issue of larger systems. it isalso longer simulation times; more complex systems, like proteins and polymers, all have very much longerrelaxation times. and i think that one of the challenges facing parallel computing is how to get beyond these muchlonger time scales, because parallelization does not solve that problem. and, in fact, i think there may be a lot ofpotential in nondynamicalšthat is, monte carlotypeštechniques that subvert the timescale problem.rethinking those on parallel architectures, i think, can lead to a lot of progress.peter taylor: i think that is a very important point. i certainly would not want anybody to go away with theassumption that i felt i had listed all of the issues here. the one of time scale is important enough that i shouldhave said something about it. it is also another useful illustration of one of the points i was making, because that isan area, clearly, where effort needs to be invested in developing new methods. getting beyond longer time scalesis not something we know how to do today. we need to try to find novel ways, as you say, to subvert, in essence,the timescale problem faced otherwise in traditional dynamics.judith hempel, university of california, san francisco: i would like to raise a question that i cannotanswer. what is the impact, really, on computational chemistry of the increase in the computing power of theasci initiative, and what if that were only the beginning? so, in a sort of visionary sense, if computing powerwere to increase at this same rate or even at an accelerated rate, what would be the impact of computationalchemistry on the world as we know it?peter taylor: well, i think the issue is once again related to some of these questions about accuracy andabout the size of the calculations. if you look at, for example, smallmolecule chemistry, then typically theaccurate methods we have scale roughly on the order of the fifth or sixth power of the size of the molecule. sowith a computer 100,000 times faster than what you have today, you would still only be able to study a system 10times larger. now, we would all like to be able to do systems 10 times larger, but that capability does notnecessarily seem that much of a reward for a fiveordersofmagnitude gain in computing power. if we really wantto see dramatic qualitative changes in the type of work we can do with computational chemistry, we need to put alot more effort into figuring out how to make the calculations scale better with the size of the system.judith hempel: do you see a time in the future when quantum mechanics will take over for the empiricalmethods, like the forcefield methods that now use quantum mechanics to extract parameters?peter taylor: what i see is an increasing use of, in essence, hybrid methods. at some level, even withsomething very large like an enzyme, i do not think it should be necessaryši would like to think we are moreingenious than thisšto treat the entire enzyme and the medium in which it is found quantum mechanically. but isuspect that in order to be able to do a good job in predicting the catalysis, the actualsoftware development for computational chemistry: does anything remain to be done?24impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.details of what happens at the bottom of the active site in the enzyme, we will need to use quantum mechanics.the question is, if you start using quantum mechanics there, where do you switch over as you move out inspace to a point where you can go over to a lowerlevel treatment, a semiempirical or empirical method? therehas been a lot of effort in that recently. some efforts have been more successful than others. i think this issomething we will work out over the next few years, and i think this is the real future. ideally there should be noneed to treat a system like that entirely quantum mechanically. if we could do it cheaply enough, well, then wecould do it that way. but in practice i do not think that should be necessary.but i think we do not completely understand yet how to splice these different levels of treatment together.how do we go, for example, from a very accurate quantum chemistry calculation for an area where we are reallyinterested in the details to a lowerlevel quantum chemistry calculation, say a density functional calculation orsomething in a larger region, and then eventually to some completely empirical type of molecular mechanicsapproach in the next layer out?thom dunning: let me make one addition to that statement. one of the surprises that came out of the asciprogram was that the increased computing power makes it possibleševen though, as peter pointed out, thetechniques that we use right now do not scale very well with the size of the systemšto actually compute all of thethermodynamics for all of the species and the reactions involved in combusting both gasoline and diesel fuel.and, in gasoline there are some 1,000 species that are thought to be involved, and some 2,000 reactions that themodelers say they need to have kinetic data for.for diesel fuel you are talking about 2,000 species and 4,000 reactions. and some of those species have noteven been seen or characterized in the laboratory yet, so even with the techniques as they are right now, one can,without increasing computing power, actually make substantial contributions to very practical problems that thecountry faces. but, i would agree entirely with peter in that we need techniques that scale better with the size ofthe system.software development for computational chemistry: does anything remain to be done?25impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.3recent advances in computational thermochemistry andchallenges for the futurelarry a. curtiss,argonne national laboratoryandjohn a. pople,northwestern universityintroductionknowledge of the thermochemistry of molecules is of major importance in the chemical sciences and isessential to many technologies. thermochemical data provide information on stabilities and reactivities ofmolecules that are used, for example, in modeling reactions occurring in combustion, the atmosphere, andchemical vapor deposition. thermochemical data are a key factor in the safe and successful scaleup of chemicalprocesses in the chemical industry. despite compilations of experimental thermochemical data for manymolecules, there are numerous species for which there are no data. in addition, the data in the compilations aresometimes incorrect. experimental measurements of thermochemical processes are often expensive and difficult,so it is highly desirable to have computational methods that can make reliable predictions.since the early 1970s when ab initio molecular orbital calculations became routine, one of the major goals ofmodern quantum chemistry has been the prediction of molecular thermochemical data to chemical accuracy (±1kcal/mol). one of the problems was that the hartreefock calculations done in the 1970s gave large errors (up to100 kcal/mol) in bond energies. prediction of accurate thermochemical data required going beyond hartreefocktheory to include a sophisticated treatment of electron correlation, which made the calculations very difficult.after several decades of work, considerable progress has been made in attaining the goal of a ±1 kcal/molaccuracy through advances in theoretical methodology, development of computer algorithms, and increases incomputer power. it is now possible to calculate reliable thermochemical properties for a fairly wide variety ofmolecules.at the ab initio molecular orbital level, the methods currently used for computing thermochemical data rangefrom very high levels of theory to those that combine moderate levels of theory with some form of empiricalinput. the former is limited to smaller molecules and can attain accuracies of ±0.5 kcal/ mol, while the lattermethods can be applied to larger molecules with somewhat less accuracy. the gaussiann methods1 that we havedeveloped fall in the latter category and have been widely used for1 for recent reviews see: l.a. curtiss and k. raghavachari, in computational thermochemistry, k.k. irikura and d.j.frurip, eds., acs symposium series 677, american chemical society, washington d.c. (1998), pp. 176197; l.a. cuss andk. raghavachari, in encyclopedia of computational chemistry, p.v.r. schleyer, ed., john wiley, new york (1998).recent advances in computational thermochemistry and challenges for the future26impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.predicting thermochemical data of molecules. in the second section of this paper we summarize current stateoftheart methods in computational thermochemistry. in the third section we describe in more detail the gaussiannapproach and then discuss a recent development in this area, gaussian3 (g3) theory, which achieves a new levelof accuracy. finally, in the fourth section we assess prospects for the future of computational thermochemistry.current state of the art in computational thermochemistrythe available methods for computing thermochemical data range from empirical schemes to ab initiomolecular orbital theory.2 in this paper we restrict our discussion to the ab initio based methods. they can beroughly divided into three types: (1) very high level quantum chemical calculations with no experimental input;(2) composite techniques that combine moderate level ab initio quantum chemical calculations with some form ofmoleculeindependent empirical parameters; and (3) techniques that use moleculedependent empirical parametersobtained from accurate experimental data in combination with moderate level ab initio quantum chemicalcalculations. in this section we discuss some examples of these different approaches.in principle it is known how to compute the thermochemical properties of most molecules to very highaccuracy (0.5 kcal/mol). this can be achieved by using very high levels of correlation, such as are obtained withcoupled clustered [ccsd(t)] or quadratic configuration [qcisd(t)] methods, and very large basis sets. theresults of these calculations are then extrapolated to the complete basis set limit and corrected for some smallereffects such as corevalence effects and atomic spinorbit effects. unfortunately, this approach is limited to smallmolecules because of the ÿn7 scaling (with respect to the number of basis functions) of the correlation methods andthe large basis sets used. this methodology has been used by dunning, feller, and coworkers3,4 at pacificnorthwest national laboratory to systematically study a large number of small molecules having one and twononhydrogen atoms. they have applied these very high level calculations to a diverse enough set of molecules toshow that the methodology does perform to a very high level of accuracy.other groups have also used this type of approach to computational thermochemistry. grey, janssen, andschaefer5 have used ccsd(t) with large basis sets to study the thermochemistry of chn and sihn hydrides, andsome of their cations. they achieved bond energies accurate to 0.5 kcal/mol without any empirical corrections forthese small molecules. petersson and coworkers6 have used qcisd(t) with very large basis sets and haveobtained a mean absolute deviation of 0.53 for a subset of the g2 test set of reaction energies. bauschlicher,langhoff, taylor, and coworkers7 have used an approach based on converging to the oneparticle limit through theuse of atomic natural orbitals at a moderate level of correlation treatment. the correlation treatment is calibratedagainst full configuration interaction calculations on smaller systems or against accurate experimental data, insome cases. they have achieved accuracies of 1 kcal/mol or better using these methods.since the very high level calculations are difficult to extend to larger molecules, an alternative2 for a recent review see: computational thermochemistry, k.k. irikura and d.j. frurip, eds., acs symposium series 677,american chemical society, washington d.c. (1998).3 k.a. peterson and t.h. dunning, jr., j. chem. phys. 106, 4119 (1997).4 d. feller and k.a. peterson, j. chem. phys. 108, 154 (1998).5 r.s. grev, c.l. janssen, and h.f. schaefer iii, j. chem. phys. 97, 8389 (1992).6 j.a. montgomery, jr., j.w. ochterski, and g.a. petersson, j. chem. phys. 101, 5900 (1994).7 c.w. bauschlicher, jr., and s.r. langhoff, science 254, 394 (1991).recent advances in computational thermochemistry and challenges for the future27impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.approach is to use a series of highlevel correlation calculations [e.g., qcisd(t), mp4, ccsd(t)] with moderatesized basis sets to estimate the result of a more expensive calculation. the gaussiann8 series described in the nextsection exploits this idea to predict thermochemical data. in addition, moleculeindependent empirical parametersare used in these methods to estimate the remaining deficiencies in the calculation. this will work if the remainingdeficiencies are systematic and scale as the number of pairs of electrons. such an approach has been quitesuccessful in the gaussiann series and the latest version, gaussian3 theory, achieves an overall accuracy of 1kcal/mol and is computationally feasible for molecules containing up to about eight nonhydrogen atoms.petersson et al.9 have developed a related series of methods, referred to as complete basis set (cbs) methods,for calculating energies of molecular systems. the central idea in the cbs methods is an extrapolation procedureto determine the projected secondorder (mp2) energy in the limit of a complete basis set. this extrapolation isperformed pair by pair for all the valence electrons, and is based on the asymptotic convergence properties of paircorrelation energies for twoelectron systems in a natural orbital expansion. as in the gaussiann methods, thehigherorder correlation contributions are evaluated by a sequence of calculations with a variety of basis sets.several empirical corrections, similar in spirit to the higherlevel correction used in g2 theory, are added to theresulting energies in the cbs methods to remove systematic errors in the calculationsthe third approach is the use of moleculedependent empirical parameters in combination with a moderatelevel ab initio molecular orbital method. the use of isodesmic reactions is a primary example of this approach,which can be quite accurate for molecules having no unusual bonding. in the isodesmic approach, a reaction ischosen with the same number of chemical bonds of each formal type (e.g., cc, c=c, cn, ch) on both sides ofthe reaction, and all of the species have accurate experimental thermochemical data available except the species ofinterest.10 a moderate level of theory is used to calculate the reaction energy, and the enthalpy of formation of theunknown species is then extracted. use of specially chosen reactions can give quite accurate enthalpies offormation because of cancellation of correlation effects and also because they make use of accurate experimentaldata. however, suitable reference molecules are often not available, making this method inapplicable in manycases.an extension of the isodesmic reaction scheme is the use of bond additivity corrections. an example of thismethod is the bacmp4 method of melius and coworkers11 that uses a computationally inexpensive molecularorbital method and combines it with a bond additivity correction. this procedure uses a set of accurateexperimental data to obtain a correction for different types of bonds that is then used to adjust calculatedthermochemical data such as enthalpies of formation. quite accurate results can be obtained if suitable referencemolecules are available and if the errors in the calculation are systematic. the bond additivity approach can also beused in combination with computationally more demanding methods such as g2 or g3 theory. in these cases anaccuracy of 0.5 kcal/mol can be achieved for large molecules.8 for recent reviews see: l.a. curtiss and k. raghavachari, in computational thermochemistry, k.k. irikura and d.j.frurip, eds., acs symposium series 677, american chemical society, washington d.c. (1998), pp. 176197; l.a. curtiss andk. raghavachari, in encyclopedia of computational chemistry, p.v.r. schleyer, ed., john wiley, new york (1998).9 j.w. ochterski, g.a. petersson, k. wiberg, j. am. chem. soc. 117 , 11299 (1995); j.w. ochterski, g.a. petersson, andj.a. montgomery, jr., j. chem. phys. 104, 2598 (1996).10 k. raghavachari, b.b. stefanov, and l.a. curtiss, j. chem. phys . 106, 67646767 (1997).11 p. ho and c.f. melius, j. phys. chem. 94, 5120 (1990); m.d. allenedorf, c.f. melius, p. ho, and m.r. zachariah, j.phys. chem. 99, 15285 (1995).recent advances in computational thermochemistry and challenges for the future28impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.gaussiann theoryideally, a method for computation of thermochemical data has several requirements to be successful. (1) itshould be applicable to any molecular system in an unambiguous manner. (2) the method needs to becomputationally efficient so that it can be widely applied. (3) it should be able to reproduce known experimentaldata to a prescribed accuracy and be applied with similar accuracy to species having larger uncertainty or forwhich data are not available. the gaussiann methods12 were developed with these objectives in mind.gaussian2 (g2) theory13 was the second in this series of methods. g2 theory is a composite technique inwhich a sequence of welldefined calculations is performed to arrive at a total energy of a given molecularspecies. it was developed for predicting molecular systems containing the elements h and c1, and extendedsubsequently to thirdrow nontransition metal elements. the goal was an accuracy of ±2 kcal/mol. there areseveral steps to g2 theory:1. geometries are determined using secondorder møllerplesset perturbation theory (mp2) with the631g(d) basis set.2. zeropoint energies are determined at the hartreefock level and scaled to account for knowndifferences between experiment and theory.3. a series of correlationlevel calculations are done using perturbation theory up to fourthorder andquadratic configuration interaction. large basis sets including polarization and diffuse functions with6311g(d,p) as the starting point are used in the correlation calculations. the energies are addedtogether to obtain a total energy.4. a higherlevel correction is added to the total energy in step 3 to account for systematic errors in theenergy calculation that scale as the number of pairs of electrons. this is a single moleculeindependent  empirical parameter that is chosen by fitting to a set of accurate experimental data.5. the zeropoint energy is added to the energy in step 4 to obtain a final total energy that is used tocalculate thermochemical properties.a test set of 125 reaction energies having wellestablished experimental values, referred to as the g2 test set,was used to assess the reliability of g2 theory. the test set was limited to molecules containing one or two nonhydrogen atoms. since its publication in 1991, g2 theory has been widely used for the prediction ofthermochemical data such as enthalpies of formation, bond energies, ionization potentials, electron affinities, andproton affinities.evaluation of g2 theory indicates that it meets the first requirement listed at the beginning of this section, butonly partially meets the second and third ones. since g2 theory needs no empirical input that depends on the typeof molecule, it can be unambiguously applied to any molecular system. due to its computationally intensivecorrelation treatment, g2 theory can handle systems with up to five or six nonhydrogen atoms, but becomesprohibitive beyond about benzene because of the ÿn7 scaling of these methods. on the g2 test set of smallmolecules, g2 theory is able to reproduce adequately (average absolute deviation of 1.2 kcal/mol) theexperimental data; however, on certain larger molecules it fails.12 for recent reviews see: l.a. curtiss and k. raghavachari, in computational thermochemistry, k.k. irikura and d.j.frurip, eds., acs symposium series 677, american chemical society, washington d.c. (1998), pp. 176197; l.a. curtiss andk. raghavachari, in encyclopedia of computational chemistry, p.v.r. schleyer, ed., john wiley, new york (1998).13 l.a. curtiss, k. raghavachari, g.w. trucks, and j.a. pople, j. chem. phys. 94, 7221 (1991).recent advances in computational thermochemistry and challenges for the future29impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.the g2/97 test set of larger molecules (302 reaction energies) was recently developed14 to provide a morestringent test of g2 theory and other methods. an assessment of g2 theory on this test set indicated that it failedfor certain types of molecules such as halogencontaining species and systems with unsaturated rings. forexample, the g2 enthalpy of formation for sif4 is off by 7.1 kcal/mol, and for benzene it is off by 3.9 kcal/mol.hence, although the average errors in g2 theory are less than 2 kcal/mol, the maximum errors are too large to allowfor full confidence in the method.we recently developed gaussian3 (g3) theory,15 which is significantly more accurate than g2 theory andeliminates most of its deficiencies. g3 theory is a composite technique similar in spirit to g2 theory, but withsome new features. steps 1 and 2 in g2 theory remain the same. step 3 is modified with different basis sets thatare more uniform, yet are smaller so that the calculations require fewer computational resources (although theystill scale as ÿn7). the higherlevel correction in step 4 is reformulated in terms of four parameters, but remainsmoleculeindependent and is obtained by fitting to a larger set of accurate experimental data. two new steps areadded: a correction for core correlation at the secondorder perturbation level using a new basis set, g3 large, and acorrection for spinorbit effects in atoms.g3 theory was assessed on a total of 299 energies (enthalpies of formation, ionization energies, electronaffinities, and proton affinities) in the g2/97 test set.16 the average absolute deviation from experiment of g3theory for the 299 energies is 1.01 kcal/mol. for the subset of 148 neutral enthalpies of formation the averageabsolute deviation is 0.93 kcal/mol. the corresponding deviations for g2 theory are 1.49 and 1.56 kcal/mol,respectively. the improvement over g2 theory is shown in figure 3.1 for different types of molecules in the g2/97test set. many of the deficiencies in g2 theory for the g2/97 test set have been eliminated. of particularimportance is the improvement for 35 nonhydrogen systems, such as sif4 and cf4, for which the average absolutedeviation decreases from 2.54 kcal/mol (g2 theory) to 1.72 kcal/mol (g3 theory). another significantimprovement is found for the 47 substituted hydrocarbons in the test set, for which the average absolute deviationdecreases from 1.48 to 0.56 kcal/mol. the deviations from experiment for a series of hydrocarbons are given infigure 3.2 and show excellent agreement with experiment.in summary, the gn approach uses a moderate level of ab initio molecular orbital calculation, combined withmoleculeindependent parameters to account for systematic deficiencies in the calculations. the latest method inthis series, g3 theory, gives thermochemical data accurate to 1 kcal/mol with small maximum deviations formolecules containing up to about eight nonhydrogen atoms. the applicability to larger molecules remains to beinvestigated.future outlookall three approaches to computational thermochemistry discussed above will play important roles in thefuture. it should be stressed that, despite the successes, much remains to be clone in the development ofcomputational thermochemistry methods.one of the major challenges for these methods is the ~n7 scaling and how it limits the size of molecules forwhich thermochemical data can be computed. the increase in computer time with size for some typicalhydrocarbons is shown in figure 3.2. current limitations and capabilities of the various14 l.a. curtiss, k. raghavachari, p.c. redfern, and j.a. pople, j. chem. phys. 106, 1063 (1997); l.a. curtiss, p.c.redfern, k. raghavachari, and j.a. pople, j. chem. phys. 109, 42 (1998).15 l.a. curtiss, k. raghavachari, p.c. redfern, v. rassolov, and j.a. pople, j. chem. phys. 109, 7764 (1998).16 l.a. curtiss, k. raghavachari, p.c. redfern, and j.a. pople, j. chem. phys. 106, 1063 (1997), l.a. curtiss, p.c.redfern, k. raghavachari, and j.a. pople, j. chem. phys. 109, 42 (1998).recent advances in computational thermochemistry and challenges for the future30impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.figure 3.1deviation of calculated enthalpies of formation from experiment for the g2/97 test set.figure 3.2relative cpu times (for a single processor workstation) and accuracy for g3 energy calculations on test moleculescontaining up to ten carbons.recent advances in computational thermochemistry and challenges for the future31impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.approaches are summarized in table 3.1. in the future, thermochemical data for very large molecules will beneeded for modeling studies. an example is the simulation of combustion in diesel engines that involveshydrocarbons containing up to 24 carbon atoms. thus, the scaling problem in computational thermochemistry willhave to be addressed in order to make predictions of thermochemical data needed in this area of research. thedevelopment of massively parallel computers in the teraflopsspeed range, and the software to run on them, willprovide computational chemists the opportunity to greatly extend the size of molecules to which the currentmethods of computational thermochemistry can be applied. however, even this new generation of computers willnot be able to handle the very large molecules, so methods to reduce the n7 scaling will be needed. there are anumber of ways by which this might be accomplished: (1) application of scale reduction techniques to coupledcluster and other highorder correlation methods; (2) development of correlation methods that exploit the localityof molecular interactions; (3) modifications to composite methods such as g3 to reduce computational time; and(4) development of density functional methods with improved accuracy.table 3.1 accuracy and applicability of different approaches to computational thermochemistryaapproachexampleaccuracy, kcal/moltypes of moleculessize limitbdirect calculationccsd(t) with very largebasis sets0.5all2composite techniques withmoleculeindependentempirical parametersg3 theory1all8composite techniques withmoleculedependentempirical parametersg3 theory combined withisodesmic reactions or bondadditivity parameters0.5limited8a for molecules containing first and secondrow elements. the accuracy and size limits are approximate and are based on currentworkstation capabilities.b the size limits refer to the number of nonhydrogen atoms and are somewhat dependent on the symmetry of the molecule and number ofhydrogen atoms.another challenge for the current computational thermochemical methods is remaining problem areas, i.e.,failures for certain molecules and lack of the required accuracy for others. work in the future will focus ondetermining causes for the failures and how to correct the problems. an important aspect of this work will be thedevelopment of expanded test sets of accurate experimental data such as the g2/97 test set. the existence of thesetest sets will be very important to finding weaknesses of current methodologies as well as developing new andmore accurate methods. in addition, contributions to molecular energies that have been neglected as small in thepast will be considered in the future in order to attain the required accuracy in larger molecules. these includerelativistic effects, nuclear motion, etc. methods will be developed to account for these effects.most of the computational thermochemistry methods that have been developed so far have focused onmolecules containing first and secondrow elements. thermochemical data on compounds containing elementsbeyond the second row are important but are more difficult to predict accurately because of the increasedimportance of relativistic and correlation effects; lack of adequate basis sets; and lack of accurate experimentaldata for assessments. in the future, new developments should provide methods that can handle these more difficultspecies.recent advances in computational thermochemistry and challenges for the future32impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.summarythe thermochemistry of molecules is of major importance in the chemical sciences and is essential to manytechnologies. it is now possible to compute reliable thermochemical data for a fairly wide variety of moleculesthrough recent advances in theoretical methodology, computer algorithms, and computer power. this paperreviews the current state of the art in computational thermochemistry and describes the gaussiann series ofmethods that have been widely used for thermochemical predictions. the latest in this series, gaussian3 theory,gives thermochemical data accurate to 1 kcal/mol with small maximum deviations. despite the successes, muchremains to be done in the future to further develop capabilities for accurate prediction of thermochemical data.among the challenges will be extension of the methods to larger molecules, increased accuracy in predictions, andextension to heavier elements. the increase in computing power obtainable from new generations of computers,such as those with massively parallel architectures, will play an important role in meeting these challenges.discussionjack kay, drexel university: could you indicate for which types of molecules the gaussian methods do thepoorest job, and for which types it would do the best job?john pople: well, the poorest ones i showed. they were molecules such as so2 and pf3, generally secondrow molecules where we are probably not doing as well as for the firstrow ones. the best job is certainly done onthe simplest hydrocarbons, which are well known from an empirical point of view, to have great regularities. andif you get a theory that does well on ethane and propane it is going to do well on butane and so forth. that is notsurprising. so those are the better examples. generally, moving toward the right of the periodic table and down inthe periodic table makes things more and more difficult.jack kay: how much luck have you had applying that to the metallic elements?john pople: well, the theory is tested against all the known facts, and that includes the molecules like li2 andna2. it includes completely ionic molecules like lithium chloride and sodium chloride, and completely homoproticones like the hydrocarbons. we have, without bias, taken everything that we could find in the literature.jack kay: what about transitionmetal compounds?john pople: transitionmetal compounds analysis is really just starting. we have developed now the samebasis sets for transition metals so we can begin to examine transitionmetal compounds. a difficulty there is thatthe good experimental data is quite limited. part of the trouble is that doing thermochemistry experimentally wentout of fashion some time ago and there is less new material appearing than used to be there. so there is a need formore.jack kay: i was wondering about relativistic corrections and things of that sort.john pople: that is another very pertinent question. the theory as i have described it does not include anyrelativistic corrections apart from the spinorbit corrections applied to the separated atoms. you canrecent advances in computational thermochemistry and challenges for the future33impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.ask how this changes if relativity is included. and very recently we have generalized the theory to the leading termin the relativistic expansion. you can treat relativity as a perturbation on nonrelativity by using the inverse of thespeed of light as an expansion parameter and just work out the leading term.the results are not big but they are not insignificant. the biggest relativistic correction that we have found isfor silicon tetrafluoride, which is about 3 kcal/mole. so, if you include relativity you would have to reparameterizethese small parameters. but those corrections will get bigger without doubt when we get into transitionmetalcompounds.recent advances in computational thermochemistry and challenges for the future34impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.session 1panel discussionrichard hirsh, national science foundation: peter, you discussed the need for new methods, and newmethods can take almost any form. they can be new chemistry investigations, new numerical methods, newimplementations of the chemistry and the numerical methods to make codes, and so on. i would like to know whatyou meant by new methods, and beyond that, i would also like to know, given what paul said about the capabilityof machines, what new problems those new methods should be or could be applied to.peter taylor: i guess my immediate answer to the list you gave is all of the above. i do not think we shouldrestrict ourselves at this point. but what i was particularly thinking about was not so much reimplementing theexisting methodology we have, which i think has a number of disadvantages, some that i discussed and some thatwere, i think, clear by example in what john presented, but going back to, say, the schrödinger equation andlooking at other strategies for developing approximate solutions of it quite different from what we have now. so iwould like to see an effort across the board on that, starting from looking again at the mathematics and looking atdifferent methods for constructing approximate solutions all the way through. we should not exclude looking atthe methods we have now and reimplementing them, but that certainly should not be the only thing we do.and i think one of the key issues here is that most of the effort we have all put in so far toward developingparallel implementations of quantum chemistry has really addressed taking the methodology we have used for anumber of years and reformulating this so it runs on parallel hardware. and the scaling of this, as one criterion ofhow well you do, is good for some methods and good for some implementations but not so good for others. it isnot at all clear that the domain of methods we use to try to construct approximate solutions to the schrödingerequation necessarily includes the most desirable scaling or methods that might perform very well on a machine,say, with 5,000 or 6,000 or even 10,000 processors. i am glad to see, based on paul's talk, that we are notnecessarily thinking of tens of thousands of processors, but it is clear that if we want to use these machineseffectively, we have to be able to run on thousands of processors, and that is a lot tougher than getting somethinggoing on, say, a 64way symmetric multiprocessor. if we are going to do it effectively we need to look at thelargestrecent advances in computational thermochemistry and challenges for the future35impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.possible variety of methods for constructing approximate solutions to the schrödinger equation, not focus simplyon developing, or trying to develop, scalable implementations of what we have now.paul messina: peter, i remember from a few years ago working with a colleague at caltech, the chemistaron kupperman, that when we were able to provide him a substantially bigger computing environment, hequickly found that the basis set that he was using was, in fact, not adequate, and he had to come up with a morenearly orthogonal basis set to be able to get any type of accuracy. so, would you include in the new methodsexamination the need to look also at those aspects where now you have a much bigger system and consequentlywould have to worry more about the capability of the methods?peter taylor: oh, yes. i think there is no question about that. this is just another aspect of the sorts ofimprovements we need to try.thom dunning, pacific northwest national laboratory: i think just in general, to make very concrete thekind of suggestion peter is making, that it has become painfully clear over the past decade how slowly basissetexpansions converge. yes, we can approach the full solution of the schrödinger equation with the basissettechnology that we have, but it is a very painful process and it gives rise to some of this very horrible scaling thatpeter is talking about. plus, it gives rise in schemes like what john is talking about, g2 and g3, to some of thoselarge deviations that are observed just because in the basis sets that are used, the convergence is not there for thattype of molecule.john pople: yes, there are worries all the time that all the technology we are using, which is based verymuch on gaussian functions, may not be the best approach when we come to very large systems and very newtechnology.one such possibility to be looked at by chemists is the matter of plane wave expansions that solid statephysicists use quite a lot. it is more appropriate for them, perhaps, because a crystal is a periodic system. butnonetheless, one can ask whether you can do better with all the modern techniques of fast fourier transforms andso forth by taking the molecule in the middle of an empty box and proceed to expand everything in plane waves.that technology could conceivably be a new approach that would possibly eliminate some of the difficulties wehave at present.evelyn goldfield, wayne state university: one of the things that i would like to address was in petertaylor's talk when he divided the field into three: electronic structure, reaction dynamics, and moleculardynamics. it seems that one of the hurdles to actually using these codes effectively is that there is a step betweenelectronic structure, when you calculate points, and having a usable potential energy surface. someone has tolaboriously fit a potential, which may or may not be an accurate reflection of the ab initio surface. and thenanother community, the dynamics community, uses that potential and makes some predictions that may or maynot compare to experiment. among the most challenging things that could be produced are codes that actuallyintegrate these three parts of the problem so that the fitting steps are bypassed. such efforts are actually a hot topicright now. to deal with realistic and interesting systems is really going to be incredibly computationally intensiveand i think could effectively make use of any number of processors.peter taylor: i agree. i think this is a defect in what we currently havešthat we do not have enoughintegration between these steps. while there are, for example, dynamics programs that basically call for theelectronic energy or something like the gradient of the potential surface on the fly, we do not haverecent advances in computational thermochemistry and challenges for the future36impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.enough of this. people are put off a bit, even given the sort of hardware we have been talking about, by thedimensionality of it: the fact that if you are doing something with three or four atoms or something similar, this isalready fairly expensive; if you have a larger system, where you are treating explicit solvent molecules, say, theidea that you have to evaluate the energy millions of times starts to become a problem, even if you are talkingabout trying to do this on thousands of processors.but, to give you an example of what i think of as new methods in this regard, one way, of course, tocharacterize a potential energy surface is indeed to have millions of energies that you might fit to a functionalform, where you might calculate on the fly. another way to characterize a potential energy surface, though, wouldbe to expand it around a single point. now, potential surfaces are very complicated functions and so if you were todo, say, a taylor expansion, the order of derivatives you would need at that point to adequately describe thesurface would be very, very high. i would submit, though, that it would be nothing like a millionthorderderivative. it would probably be tens of orders, say. now, at present we do not know how to do better than fourthorder derivatives of the energy in a way that is more efficient than evaluating individual energies. if we put effortinto understanding how to do higher derivatives of the energy efficiently (because we already know that evaluatingthe derivatives of the energy, at least up through fourth order, is something we can do a lot more efficiently thanevaluating lots of individual energies)šif say, we need derivatives through 25th order to characterize an entirepotential energy surface and if we could figure out how to calculate the 25th derivative of the energy in anefficient wayšthen we could do basically one calculation to characterize the whole thing. and that could then beused for the dynamics. that is another example where we need to think about new methods.john pople: i think there is an important point here about the interfacing of programs from slightly differentareas of chemistry. this is clearly something that is very desirable, and it does bring up the question of thedesirability of having source codes public. it is very important, i think, that people should know what the codes doand should be able to carry out some kind of useful interface with other codes. one has to have protection to stoppeople from getting a code, introducing some bug, and then giving it to somebody else. that clearly would bedisastrous. but nonetheless, i think particularly with the development of objectoriented codes we should pay moreattention to producing software that can effectively be interfaced with software from some related field so that wecan begin to build integrated programs to handle these difficult situations.peter taylor: i absolutely agree.paul messina: although i certainly did not dwell on this in my presentation, when i was at caltech andworking on the applications that we are trying to tackle, we identified as the biggest challenge the integration ofcodes from the different disciplines early on. and i think one of the subtexts of asci is to be able to do theseintegrated, multidisciplinary simulations. there are issues of program difficulties that include the code, andcertainly of methodsšdifferent grading, different approximation techniquesšthat, if one is not careful, willintroduce instabilities in the numerical computation, for example. so that is, indeed, a very important problem thatscience in general needs to be able to deal with to do the very large simulations.judith hempel, university of california, san francisco: i am really struck by the fact that we have newchallenges in the computing engines and the codes and in the experiments that are needed to validate the codes,but it does seem to me that for a mature fieldši think we agree that in some ways therecent advances in computational thermochemistry and challenges for the future37impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.field is matureša lot of the impact is in the applications of these theories, and there are new applications that arecoming out. so i would like your input.for example, in the fields of docking and scoring, if you have a biological molecule you can dock many,many different molecules into the binding cleft and score it. and that is a kind of theory in and of itself. so, do yousee any particular challenges for the field as a whole, to drop back, as it were, and to simplify in some ways thetheory to approach these very large and actually very important problems?john pople: i think the main contribution at the present time to that sort of simulation is using these morefundamental methods to work out the energies of interaction of appropriate modeled pieces. we clearly cannothandle the whole systems by these reliable methods but can work at the intermolecular potentials of the pieces,find out something about how configurations may change in the enzymatic environment, and so forth. so, it isreally a twostage step. one level of theory should provide the underlying potentials. others will then simulate thevery large molecular systems using those potentials. so, again, this is an example of integrating the softwarebetween different branches of the field.judith hempel: do you feel that a 1kilocalorie target is the correct target also for predicting a bindingagent?john pople: the 1kilocalorie target is the target that we have for calculating the energy of a bond fromscratch, when we do not know the energy and do not know anything about it beforehand. you can do much betterthan that if you look at the same bond in related circumstances. even hartreefoch theory can work quite well andcan be applied, of course, to very large systems, if you do a comparison between the same set of bonds in onemolecule and the same set of bonds arranged in a different fashion in another molecule, what we call isodesmicreactions or isodesmic comparisons. an elementary theory can do well there, much better than 1 kilocalorie.judith hempel: right. these numbers, of course, are affected very much by solvation issues, as peterpointed out, so there is this integration across many theories.peter taylor: i would also like to address a general point that you raised there. there is a story, i believe toldabout wigner, who said that rather than have a very accurate solution to the schrödinger equation for behavior ofelectrons in a particular metal, which would necessarily be a very complicated thing, he would sooner have a vividpicture of what the electrons were doing in the metal. i think the two aspects of that statement representundesirable extremes. a vivid picture is of no use whatsoever if it is a vivid picture of an answer that would bewrong, and having a simple method that lets you get a very nice picture up on the screen that has binding energiesthat are wrong by 10 kilocalories per mole is really no use to you either. you are not learning anything aboutnature from that.this is one of the things paul emphasized: the need for us to be able to visualize whatever type of calculationwe are doing in some way that lets us learn something more about nature from those numbers. i think we learnedthat the ideal situation would be to have a vivid picture of exactly the right answer. but i would sooner have avivid picture of nearly the right answer, or actually nearly the right answer and no vivid picture, than i would have avivid picture of the wrong answer.douglas doren, university of delaware: it strikes me that the field is, in some ways in terms of ourcomputational needs, at a dividing point. the advances over the last 10 years have made it feasible to get prettygood answers on pretty big systems so that it is now feasible, for example, to calculate therecent advances in computational thermochemistry and challenges for the future38impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.structure and energetics of some large, inorganic molecules that are actually of a size that a real synthetic chemistwould make and be interested in. it is something that we do all the time.making somethingša method or a machinešthat gives us the answer faster is simply going to make thesecalculations more trivial. there will certainly be some systems where it is important to be able to do very largescale simulations, but the market for those, i think, is getting smaller and smaller. i think for the last 10 years wehave been expanding the market for quantum chemistry in general and making it feasible to actually docomputational inorganic and organic chemistry in a reasonably useful and reliable way. the market driving thedevelopment of methods for extremely large scale solutions, i think, is going to be somewhat smaller. i mean, mysynthetic organic chemist friends are not going to start making bigger molecules simply because we are able tocalculate them.there is certainly an important role for these methods. i am interested in them and working on them, but inmany ways, finegrain parallelism and massively parallel systems are going to solve only a subset of all ourproblems. i will give two examples that i can think of. one, a simple case, is that i have a group of five students.each student is working on a couple of different problems. they really need separate resources: they need to beable to get all those problems solved all together. being able to solve a single problem quickly is not their goal. if icould solve each of those problems separately on a workstation, that would be great. being able to do themsequentially on a massively parallel computer does not necessarily bring my group's set of problems to a solutionmore quickly.as another example, i do not think that calculating the 25th derivative of a potential surface at a single pointis going to be enough to characterize the whole surface. we are going to need to calculate the potentials andderivatives at lots and lots of points and still have some way of fitting these together. that would work just finewith loosely coupled parallelism.what are your thoughts? any sense of where the balance is?peter taylor: what i hope that machines like the asciclass machines will do is catalyze development ofnew methods, even if the immediate application of these to new areas or larger systems or more accuratecalculations seems limited. the technology that is developed for handling those problems, the higher accuracy, thelarger systems, the greater efficiency, will inevitably fit into the programs that are run on desktop machines and letpeople do chemistry that almost by definition they are not considering today.an example of this is that if you go back to the earliest days of gaussian programs in the early 1970s, thecapabilities, in essence, were limited to hartreefock calculations. this is partly, i guess, because of what wasfeasible at the time, but partly because i think up until the end of the 1960s there was not a very clearunderstanding of the essential role that electron correlation would play in more accurate predictions. in themid1970s, due to the work of various people (some here, like john, the group that i was part of in australia, rodbartlett who is here at the back), more efficient methods of treating electron correlation were developed. most ofthe chemists i knew who did calculations at all at that time felt that by and large this was unnecessary and couldnot see why we were mucking around with something that was only relevant to water or methane or hydrogenfluoride. and yet today no selfrespecting chemist, not only quantum chemists, not only theoretical chemists, noselfrespecting chemist would restrict his or her investigations of something to just the hartreefock level. so,because now we do not see an immediate need for daily use of 5,000 processors and another orderofmagnitudeaccuracy in our application calculations, i do not think we should conclude from that that the market for thesesorts of things, necessarily, is dwindling. i would say it is the technology that comes out of doing thosecalculations and the way it feeds into what people will use on the desktop in 5 or 10 years time that is the reallycritical thing.recent advances in computational thermochemistry and challenges for the future39impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.thom dunning: i think also there are applications, as i think paul could point out in the asci programitself, where if you tackle a particular problem it turns out that many of these problems are very data hungry. thecombustion problem needs a lot of information on a lot of molecules in a lot of reactions so that you are faithfullyrepresenting the chemistry that is going on in a combustion process.you could say we are going to wait until all those 1,000 species and all those 2,000 reactions arecharacterized in the laboratory. but, in fact, if you do that you are going to be sitting forever. and the only way toget at some of that data, in fact, is to do it computationally, and you may have to be doing a lot of calculations at arelatively high accuracy that will actually require the kinds of machinery being requested in asci. and that is thereal driver. that is why those machines are necessary to solve their problem; they have such a complicated systemthat to describe the various components of that complicated system really represents computational grandchallenges that require generating a lot of data that is just not available from experiment. of course, validatingagainst the experiment, as john said, is done whenever possible, but there are species like radicals and ions thatcan be very difficult to study in the laboratory, where information on them is absolutely critical to the fidelity ofrepresenting that particular physical or chemical process in the total system itself.david dixon, pacific northwest national laboratory: we have really not talked much about nuclearmotion and its problem, one of the things i think that does argue for looking at much larger architectures. it ismuch more straightforward to do things in zero kelvin. if you want to look at chemistry at 298 or 500 degrees, youare after nuclear motion. if you want to treat the water dimer correctly you have to put quantum nuclear motion in,and i think that as we start to look at what we can do with the large architectures, we will actually start thinkingabout how we treat the nuclear motion problem, how we treat coupled routers, how we treat weakly boundsystems. this gets back to the enzyme interaction. that is going to require much larger computer resources thanwe have today where we are working on single, accurate points.if you do not get zero point energy at zero kelvin in methane right, if you just take the zero point energy andcap it with what is known experimentally, you are off by 0.6 or 0.7 kcal, which is almost all of john's error. so thenuclear motion part of it will be critical to really solve, and i think this is going to be one of the arguments we needto have to go to much larger architectures to really understand what is going on. i would appreciate yourcomments on that.john pople: yes, i agree that is a major feature. we are certainly well aware that anharmonicity is one of thethings that is not well treated in the present level of theory. it is swallowed up by one of the empirical parameters.and, indeed, one knows when you come to molecules that are floppyšand most molecules, for example inbiological circumstances, are very floppyšwhere all sorts of rigging around is going on, this is a very importantfeature. so i fully agree with you that one has somehow to merge the methods that are used for electronic structurewith those that are used for handling nuclear motions, and this comes back to the dynamics problem again. thereis a lot to be done in interfacing these areas and developing composite programs to handle the whole problem.andrew white, los alamos national laboratory: what paul has talked about, what quantum chemistryhas pointed out, are predictive models, whether it is thermochemistry or the safety of an aging stockpile. it seemsto me that in your fivestep plan you need a sixth that somehow quantifies the uncertainty in the systems if you arereally looking into the futurešmaybe some place you cannot do experiments, like stockpile stewardship orclimate or nuclear winter or weather or whatever. can you talk about what the state of the art is with gaussian orwith any other code relative to how you quantifyrecent advances in computational thermochemistry and challenges for the future40impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.this uncertainty? maybe there some lessons for the rest of us in how to put all this together for predictive models?john pople: well, the uncertainty question is one that we tried to address by validating the method againstall the known facts. now, if you try to do it purely ab initio, then it is rather hard to give an uncertainty. so inprinciple, but not in practice, you can get upper and lower bounds to the energy to which you could eventuallyclose. bright wilson used to be an advocate of that, but there are no signs of that becoming practical.so, the best that we have been able to do, and i do not think anybody else has come up with an alternativesuggestion, is to test a theory fully against everything that is known really well from experimental chemistry, andto do statistics on that. those are the numbers that we have come up with. i think it is going to be very useful tolook at the bad cases and say there is something wrong with our theory and it is showing up because these resultsare bad; that is a useful form of investigation.but, that is the best best way that i can think of to describe an uncertainty.peter taylor: yes, one can perhaps quantify this a bit. the typical total energies of the sorts of molecules injohn and larry's schemes are of the order of hundreds of thousands of kilocalories per mole and one is looking for1kilocaloriepermole accuracy there. we typically do not do calculations of total energies that are accurate evento tens of kilocalories per mole, and i would claim that we have really no methodology for which it is practicalcurrently to do total energies accurate to 1 kilocalorie per mole.there is a very large compensation for error in what we do. we have always known about this. weunderstand where it comes from in our casešthat is, formation of a molecule is a relatively small perturbation on agroup of atoms and the systematic errors in the atoms cancel out to a significant degree when you form themolecule. but the field is aware of this, and if we really wanted to have hard, small uncertainties, whether it isgoing the route of calculating upper and lower bounds or whatever, we would have to work very much harder inthe calculation of the total energy itself than we are currently set up to do.richard kayser, national institute of standards and technology: i wanted to point out that we have aneffort under way to put together what we call a computational chemistry benchmark database. right now thedatabase contains about 600 compounds for which we believe we have a good handle on the thermochemistry. inaddition to that information, the database will contain the results of many different welldefined calculations basedon different levels of theory and different basis sets, with the goal of trying to get a handle on the systematic errorsthat are inherent in different approaches.edwin wilson, university of akron: one of the things going on at our institution is calculation ofconformations and interactions of polymers. what kind of efforts should be happening in that field and how willthat interact with the asci program?john pople: well, the problems of conformations on molecular geometries were already fairly well handledmany years ago at the hartreefock level. it was found that for organic molecules, even if you used a moderatebasis like 631, hartreefock theory normally gave you the right conformation of individual molecules. so i thinkthere has been fair success with quite elementary theory in dealing with that.there needs to be refinement along the same lines, but i think the point to make is that conformarecent advances in computational thermochemistry and challenges for the future41impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.tional problems are somewhat simpler than getting the total energy of a bond. but undoubtedly further work isneeded.peter taylor: i would have to be a little less sanguine than my colleague is, i think. if you look at largemolecules like polymers, one of the things they can do that the sorts of halfdozen carbon chain molecules insmallmolecule organic chemistry cannot is that they can fold around on themselves. and one of the things that iskey in some of this folding around on itself is relatively weak interactions, say dispersiontype interactionsbetween different parts of the chain. those dispersiontype interactions are not treated at the hartreefock level atall, and so i think something significantly better than hartreefock may ultimately be required to do reliablepredictions of conformations of longer chains where there is substantial folding or coiling or things like this,assuming you want to do something that goes beyond the level of some parametrized model, some empirical typeof force field.john pople: yes, i completely agree with that. my points were referring to things like rotation about singlebonds and boatchair conformations and cyclohexane, which were handled fairly well some years ago.edwin wilson: one also finds that the interaction that occurs at interfaces between different polymers is afairly important aspect of that area of chemistry.john pople: yes, it is true that there is somewhat of a division among quantum chemists, those who work onindividual molecules and those who work on intermolecular forces, and they sometimes do not quite meet. peopleinterested in intermolecular forces tend to look at the longrange limits and they do not know how to join with thepeople who deal with the strong interactions at closer ranges.peter taylor: another integration issue!jean futrell, pacific northwest national laboratory: i am not sure whether this is a showstopper or not,but i would like to inquire about the accuracy of present methods for defining transition states and their energeticproperties, frequencies, and so on, and what one can expect from this leap forward in technology.john pople: one difficulty is that my suggestion that we test theories by comparing with results knownexperimentally to great accuracy does not really hold for transition structures. we would, indeed, like to do that,but only one or two energies of activation in the literature are really well known, and those energies are verydifficult to reproduce theoretically anyway. so, i can only say that is a more difficult problem and we have fewermeans of testing the reliability of our results. this is the best that we can do and we hope this is accurate to thislevel, but we cannot at present completely test it.peter taylor: i would say this is an excellent example of an area where theory needs to do more to meet theexperimentalists on their own turf rather than stopping halfway. ﬁexperimental estimatesﬂ of barrier heights arederived from all sorts of assumptions made in the interpretation of experiments. such assumptions may or may notbe warranted and may or may not mean what theoreticians think they mean when they are arriving at their ownbarrier heights. a far more satisfactory way to deal with the issue of reaction mechanisms, and ultimately kinetics,is for the computational chemist to calculatešthis follows, really, from evie goldfield's point earlieršthe rates ofthe reaction and compare the resultsrecent advances in computational thermochemistry and challenges for the future42impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.directly with some kinetics from experiments that is not subject to all the varied assumptions made in order toobtain some sort of barrier height.i think we should not get hung up on the issue of how to calibrate methods for calculating transition states andhow to get error bars on the heights of barriers. we should go the next step further, integrate the dynamics into thecalculation and then compute what the experimentalists actually measure and compare that with the experiment.that is the way to get reliable calibrations.thom dunning: while there have not been many calculations, there have been some that would certainlyindicate that the techniques that we currently have available to us can achieve very high accuracy. it does turn outthat calculations of transition states are significantly more challenging than calculations of stable species. thebasis sets you have to use are larger and you have to go much closer to convergence to get reliable numbers. but iwould say that the best techniques can get errors on the order of tenths of a kilocalorie per mole. but for the fewsystems that have been checked, the problem there is, as john says, that we really do not have any goodinformation from experiment that pertains directly to what we are calculating. we have to go to the step that peteris talking about to be able to compare with experiments.recent advances in computational thermochemistry and challenges for the future43impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.4the role of computational biology in the genomics revolutionjeffrey skolnick,jacqueline fetrow,angel r. ortiz,andandrzej kolinskiscripps research instituteabstractthe various genome sequencing projects are providing a plethora of protein sequence information, but withno information about protein structure or function. the most effective method for sifting out useful proteins fromthese genomic databases is the computer prediction of protein function. however, current methods, which aremainly sequencebased, are limited by the extent of similarity between sequences of unknown and knownfunction; they increasingly fail as the sequence identity diverges into and beyond the twilight zone of sequenceidentity. in practice, between 30 and 60 percent of all proteins can be functionally identified using currentsequencebased software. to extend the level of molecular function annotation to a broader class of proteinsequences, methods for identification of protein function based directly on the sequencetostructuretofunctionparadigm will need to be developed. one such approach is presented. the idea is to predict the native structurefirst by using ab initio folding or threading techniques and then to identify its molecular or biochemical functionby matching the active site in the predicted protein structure to that in a protein of known function. application ofthis approach to genomic screening is then described. based on these preliminary results, the next 5 to 10 years arelikely to see the development of computational tools that will allow for the mediumresolution prediction of thetertiary structure of single domain proteins, the more robust identification of protein ligands, techniques to predictproteins having specific quaternary interactions, and the beginnings of a bottomup approach to identify importantproteins in metabolic and signal transduction pathways.introductionthe various genome sequencing projects are providing a vast quantity of protein sequence data,1 but what isneeded is information about protein function (rastan and beeley, 1997). to enhance the efficiency of the drugdesign process, one must identify the sequences of functionally important proteins1 see the genbank index at <http://www.ncbi.nlm.nih.gov/web/genbank/index.html>.the role of computational biology in the genomics revolution44impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.that are hidden in these large databases. for example, microbial genomes contain potential protein targets that canbe utilized to kill pathogens or that can be developed into commercially useful enzymes to produce or degradevarious substances. by far the most effective method for sifting out useful proteins from these genomic databasesrelies on the computerbased prediction of protein function (rastan and beeley, 1997). however, most currentmethods, being mainly sequencebased, are limited by the extent of sequence similarity between sequences ofunknown and known function (pearson and lipman, 1988; henikoff and henikoff, 1991; attwood and beck,1994; bairoch, bucher et al., 1995; altschul, madden et al., 1997; attwood, beck et al., 1997). they increasinglyfail as the sequence identity between two proteins crosses into and beyond the twilight zone of sequence identity,which is about 30 percent (fetrow and skolnick, 1998). in practice, current sequencebased software can identifythe molecular or biochemical function of roughly 30 to 60 percent of all proteins in a given genome (bult, whiteet al., 1996; casari, ouzounis et al., 1996). the full annotation of entire genomes is likely to be a majorcomputational and experimental challenge over the next 5 to 10 years, but one which, when successfullyaddressed, will provide a revolution in disease diagnosis and treatment as well as in our conceptual understandingof biology. to be fully successful, this will require a multidisciplinary approach involving biology, chemistry,physics, and computer science.here, we describe one promising means of extending the ability to annotate the remaining orphan sequencesbased on the sequencetostructuretofunction paradigm (fetrow, godzik et al., 1998; fetrow and skolnick,1998). logically, this process can be divided into two parts. first, one employs techniques to determine proteinstructure from sequence (godzik, skolnick et al., 1992; ortiz, kolinski et al., 1998 a,b,c). secondly, one employstools for function prediction based on the identification of active sites in the predicted or experimental structure.the ability to determine function from structure will be very important given the emerging structural genomicsinitiatives where the goal is to determine all possible protein folds. this reverses the more traditional approachwhere one first identifies the function of the protein of interest and then subsequently determines its structure.prediction of protein structure from sequencecurrently, there exist two basic theoretical approaches for the prediction of protein structure from sequencewhen homology modeling (which requires significant sequence identity between the probe sequence and itstemplate structure) (sali and blundell, 1993) cannot be applied: threading (bryant and lawrence, 1993; miller,jones et al., 1996), and ab initio folding (skolnick, kolinski et al., 1997; ortiz, kolinski et al., 1998 a,b,c). inthreading, the idea is to match the sequence of interest to a template structure in a library of known structures(godzik, kolinski et al., 1993); thus, this approach is conceptually similar to standard homology modeling, exceptthat now the goal is to match probe sequences to template structures when there is no apparent sequencerelationship between the two. in ab initio folding, one attempts to fold a protein starting from a randomconformation (kolinski and skolnick, 1996). the advantage of threading is its speed and the fact that it can beapplied to large proteins. in contrast, ab initio folding is computationally more demanding and is, in practice,currently limited to proteins smaller than 100 residues (ortiz, kolinski et al., 1998 a,b,c). however, ab initiofolding does not demand that an example of a native structure be already solved. thus, it can be used to identifyproteins having a novel native structure. recent results indicate that for small proteins (those less than 100residues), ab initio folding approaches can predict structures at a level of quality (4 to 6å coordinate root meansquare deviation for the backbone atoms) comparable to that provided by threading (ortiz, kolinski et al.,1998a,b).the role of computational biology in the genomics revolution45impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.description of ab initio protein folding methodologyin what follows, we describe a newly developed method for structure prediction, monsster, whichattempts to address the aforementioned problems. as depicted in figure 4.1, prediction of protein structure can beconceptually divided into four stages: (1) restraint derivation; (2) structure assembly; and (3) selection of thenative conformation. in addition, for those sequences whose structures are known either before or after theprediction is made, following the structure selection process, (4) objective, rigorous validation criteria are appliedto judge the success of the prediction.for (1), restraint derivation, a multiple sequence alignment with the sequence of interest is generated (sanderand schneider, 1991). then, predicted secondary structure restraints are obtained from a standard secondarystructure prediction scheme (rost and sander, 1993; rost, schneider et al., 1993) supplemented by our linkeralgorithm (kolinski, skolnick et al., 1997)ša quite accurate technique for predicting where the chain reversesglobal direction. we term such regions "uturns" (kolinski, skolnick et al., 1997). the predicted secondarystructural elements between these uturns define the predicted core regions of the molecule. tertiary contacts(restraints), termed "seeds," between these core elements are then predicted from multiple sequence alignments.multiple sequence information is used to derive such seed sidechain contacts based on patterns of residueconservation (aszodi, gradwell et al., 1995; mumenthaler and braun, 1995) or residue covariation in a set ofhomologous sequencesfigure 4.1schematic overview of the procedure for tertiary structure prediction.the role of computational biology in the genomics revolution46impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.(göbel, sander et al., 1994; thomas, cesari et al., 1996; olmea and valencia, 1997). both might be combined forincreased sensitivity (olmea and valencia, 1997). here, for the sake of simplicity, we slightly modify the approachof göbel and coworkers (göbel, sander et al., 1994) and calculate the covariation between all residues predicted tobe in the putative core of the molecule (olmea and valencia, 1997; ortiz, kolinski et al., 1998a,b). unfortunately,there are too few of these seed contacts to assemble a protein from the unfolded state using monsster. thus,these seed contacts between predicted topological elements (i.e., helices and strands between uturns) areenriched by an inverse folding approach that typically produces about n/4 contactsšthe number required forsuccessful topology assembly (olmea and valencia, 1997; skolnick, kolinski et al., 1997; ortiz, kolinski et al.,1998a,b).in (2), the structure assembly step, the set of predicted restraints is used in the monsster method(skolnick, kolinski et al., 1997) to drive the conformational search. this uses a reducedproteinlattice model toassemble the global fold. first, a series of up to 1,000 independent, simulated annealing structureassembly runsare performed, and the resulting structures are clustered on the basis of their pairwise coordinate root mean squaredeviation (crmsd). if the resulting structures do not cluster into several topologies, then no structural predictionis made. if at least a subset of the structures cluster, then we proceed to the structure selection step.the native structure selection stage (3) consists of long isothermal runs from which the putative nativetopology is chosen on the basis that it has the lowest average energy. if the differing topologies cannot be selectedon this basis, then the prediction consists of several lowestaverageenergy representatives of the various generatedtopologies. in all cases, we report the average crmsd values corresponding to the lowestaverageenergystructures and not the best crmsd values because in a blind prediction, we would have no means of selecting suchstructures.once the native conformation of the protein of interest is known, we judge the success of the prediction (4) asfollows: first, we calculate the global c crmsd between the predicted (lowest average energy) andexperimental structures. since our approach often results in structures whose c crmsd is in the range of 6 å,there may be substantial topological errors between the native and predicted structure; therefore, a more rigorousassessment of success is necessary. thus, the predicted fold is subjected to a structural similarity search over arepresentative database using the dali (holm and sander, 1997) structural superimposition program. we notethat a very similar approach has been used to assess the quality of structures predicted by threading techniques,where a sequence is matched to a fold in a library of known structures (wodak and rooman, 1993). when aknown homologue of the native structure is chosen (or the native structure itself), then the tertiary structuralprediction protocol is considered to be successful. if two or more topologies are isoenergetic, both would besubjected to this protocol; if one matches the native topology, we consider this to be a partial success. if the nextlowest average energy topology (as predicted by monsster) matches the native fold rather than the lowestaverage energy structure, this is also considered to be a partial success. otherwise, by this rigorous criterion, theprediction is unsuccessful.validation on proteins of known structurethe above protocol was applied to the set of 19 proteins listed in table 4.1. on average, for the set ofproteins whose native conformation was known in advance, the predicted secondary structure is 69 percentcorrect; this is slightly less than the reported average for this technique, which is 72 ± 9 percent (rost and sander,1993; rost, schneider et al., 1993). such a large test set is necessary to demonstrate that the current approach canhandle a wide variety of folds and different secondary structure types. allthe role of computational biology in the genomics revolution47impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.are outside the set of proteins employed in the derivation of the empirical potentials. it is very important toemphasize that all predictions use the identical parameter set and folding protocol. table 4.2 shows the accuracyof the predicted secondary structure and tertiary contacts, as well as the results from the folding simulations. onlyabout 78 percent of the native contacts are correct within ±2 residues; these are typical of results seen on an evenlarger class of proteins. often, there are also a number of grossly incorrect restraints that can lead to nonnativetopologies. using this information, in about 10 to 30 percent of the assembly runs, nativelike topologies, assubsequently assessed by their global crmsdtable 4.1 list of proteins of known structure that constitute the validation setproteinnresclassfold descriptionname3cti29smalldisulfidebound fold, beta hairpin withadjacent disulfidetrypsin inhibitor from squash (cucurbitamaxima)1ixa39smallegflike (disulfiderich fold; nearly all beta)factor ix from human (homo sapiens)prota47threehelix bundleprotein a1gpt47smalldisulfidebound fold, beta hairpin withadjacent disulfidegammathionine from barley (hordeumvulgare)1tfi50smallrubredoxinlike (metalbound fold, with 2cxxc motifs)transcriptional factor sii from human(homo sapiens)6pti58smallbptilike (disulfiderich + fold)pancreatic trypsin inhibitor from bovine (bostaurus)1fas61smallsnake toxinlike (disulfide rich; nearly allbeta)fasciculin from green mamba (dendroaspisangusticeps)1shg62sh31ike barrel (partly opened; n* = 4, s* =8; meander)alphaspectrin, sh3 domain from chicken(gallus gallus)1cis66+ci2 family (+ sandwich; loop across freeside of )hybrid protein from barley (hordeumvulgare) hiproly strain1ftz70dnabinding 3helix bundle (righthandedtwist; updown)fushi tarazu protein from fruit fly(drosophila melanogaster)1pou71dnabinding domain (4 helices, folded leaf,closed)octi pouspecific domain from human(homo sapiens)1c5a73anaphylotoxins (4 helices; irreg. array,disulfide linked)c5a anaphylotoxin from pig (sus scrofadomestica)3icb75efhand (2 efhand connected with ca bindloop)calbindin d9k from bovine (bos taurus)1ubi76+grasp (singlehelix packs against sheet)ubiquitin from human (homo sapiens)1lea84dnabinding 3helix bundle (righthandedtwist; updown)lexa repressor, dnabinding domain(escherichia coli)1ego85/thioredoxinlike (3 // layers; sheetorder 4312)glutaredoxin from bacteriophage t41hmd85four helical upanddown bundle(lefthandedtwist)hemerythrin from sipunculid worm(themiste dyscrita)1poh85++ sandwichhistidinecontaining phosphocarrier proteins(escherichia coli)life100+if31ike (š(2); 2 layers; mixedsheet 1243)translation initiation factor if3 fromescherichia colithe role of computational biology in the genomics revolution48impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.and dali (holm and sander, 1997), are recovered for all classes of proteins. but on average, helical proteins arepredicted better than alpha/beta proteins, which are predicted better than beta proteins.in 14 of 19 cases, success or partial success was obtained, with the lowest average c crmsd valuesranging from 3.5 to 6.7 å. for one partial success whose lowest average energy structure has a higher crmsdfrom native life, the lowestenergy fold basically adopts the native topology despite its unsatisfactory crmsd.here, a strand region is found at the back of the protein rather than at the edge of the fold. the topology with thenext higher energy, i.e., the first excited state, is the native one. for the five unsuccessful casesš3cti, 1tfi, 6pti,1lea, and 1pohšdali fails to find any structure that is significantly related to the lattice model; thus theprediction is labeled as being unsuccessful. this is true in spite of the fact that the topology of 6pti is native, andfor 1poh, a slightly misfolded state is recovered, but by the dali selection criterion, this simulation isunsuccessful. furthermore, for mainly helical proteins such as lpou, the alternative lowenergy fold is thetopological mirror image (where the helices are righthanded, but the chirality of the turns is reversed from thenative conformation). in some situations, e.g., for life and 1poh, the alternative topology differs in the placementof one or two topological elements. in other cases, the alternative and native topology do not have much incommon.blind predictionswe next present a representative prediction of the tertiary structure of the 81residue kix domain of thecreb binding protein, which is involved in gene expression as mediated by ampc (brindle and montminy,1992; radhakrishnan, perezalvarado et al., 1997).as shown in figure 4.2, the secondary structure prediction scheme suggests that kix should adopt a threehelix bundle fold. correlated mutation analysis provides four seed contacts (2235, 2273, 3573, 1772) thatyielded 38 predicted tertiary contacts when enriched; this is a rather large number as compared to other entries intable 4.2. a series of 10 independent fold assembly simulations were done; all yielded either a left or righthanded threehelix bundle. as indicated in table 4.2, on the basis of their average energies, the two topologies areessentially isoenergetic. decomposing the energy into its constituent contributions (ortiz, kolinski et al., 1998c),the pair interactions, secondarystructure preferences, and hydrogenbond terms favor the righthanded bundle,whereas the burial energy and terms designed to generate proteinlike densities favor the lefthanded bundle. thedifficulty in distinguishing topological mirror images is a problem that this method often experiences with helicalproteins, and indicates that improvements in the empirical potential are necessary. when subsequent predictionswere done using the subset of restraints that satisfy each of the two topologies, then the native topology was foundto be substantially lower in energy than the incorrect alternative.structure to functionin the prediction of protein function from sequence, there are a number of key questions that must beanswered. in particular, does one need a protein structure to predict protein function or is sequence informationsufficient? if a protein's tertiary structure is needed, how close does it have to be to the native state to permit theprotein's function to be identified? is there a onetoone relationship between protein structure and proteinfunction? if not, can one construct a library of active sites so that one can search structures for appropriate activesites? in what follows, we address each of these questions in turn.the role of computational biology in the genomics revolution49impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.table 4.2 summary of prediction resultsproteinatypenbq3cncdnpenwfd=0gd=2gproteins with known structure in advance of prediction3ctismall2950.5396083.3100.01ixasmall3970.54850100.0100.01gptsmall4770.67013046.1100.01tfismall5060.08437021.688.8protao4777.691170070.51ftz5663.514912125.058.31c5a6685.810543124.473.31pou7178.812249028.689.83icb7582.315425028.068.01hmd8590.615720210.065.01shg5767.110939028.2100.01fas6167.19825126.378.96pti5658.89219068.4100.01cis6664.71442308.678.21lea7363.51314129.775.61ubi7662.315317023.594.11poh8565.91623638.355.51ego8560.022333015.193.91ife10075.314821314.238.0result from blind predictionkix8187.7320371126.357.9notes:a protein refers to the brookhaven national laboratory's protein database (pdb) access number for the protein studied.b n is the number of residues in the protein in the pdb file.c q3 is the percent of correctly predicted secondary structure. all proteins have a q3 within one standard deviation of the average.d nc is the number of contacts in the native structure.e np is the number of predicted contacts.f nw is the number of contacts that are incorrect when no native contact is found within ±5 residues of a predicted contact.g percent of predicted contacts within d residues of a native contact.h rmsn is the crmsd deviation in angstroms from the native structure.i en is the lowest average energy (in kt) after refinement for the nativelike topologyj rsn is the number of restraints satisfied in the nativelike topology.k rmsw is the crmsd deviation from native in angstroms of the alternative topology of lowest energy.l ew is the lowest average energy (in kt) in the alternative topology after refinement runs.m rsw is the number of restraints satisfied in the alternative topology.n relationship of lowest average energy structure to the native conformation if known. s indicates that the full structural selection criterionas assessed by the energy and dali are "successful," ps indicates that the tertiary structure prediction is "partially successful," and uindicates that the tertiary structure prediction is "unsuccessful."o the b domain of protein a.the role of computational biology in the genomics revolution50impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.native topologylowest energy, nonnative topologyrmsnhenirsnjrmswkewlrswmfinal scoren3.810766.71036u5.613057.71315s5.927696.614210s5.9202287.019131u3.124629.424010s5.12771110.127015s4.2194209.818226s3.54181811.936422s4.54062112.634211s4.645839.346013ps4.5420196.739718s6.2330199.3728420s4.7410199.739718u6.424077.62327s6.1136269.411527u6.1238911.52038s6.53364211.729923u5.7417209.039616s6.7419158.248216ps5.84771910.747926psfigure 4.2for kix, the primary sequence and a comparison of the predicted and observed secondary structure. here, h denotes ahelix, u a uturn; prdsec (obsec) is the predicted (observed) secondary structure from phd and linker.the role of computational biology in the genomics revolution51impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.limitations of sequencebased methodsas residue identity falls into the twilight zone, standard sequencealignment algorithms will pick up falsepositive sequences as well as miss false negative sequences. similarly, as sequence diversity increases, the localsequence signatures found in the prosite (bairoch, 1990; bairoch, bucher et al., 1995), blocks (henikoff andhenikoff, 1991), and prints (attwood and beck, 1994; attwood, beck et al., 1997) databases will no longer bestrong enough to recognize protein sequences as belonging to a functional family, even though the specific activesite residues might be strictly conserved. (see table 4.3.) to illustrate this inability to recognize local sequencesignatures as the sequences diverge, we performed an analysis of the prosite database (release 13.0, november1995). of 1,152 patterns in this release of prosite, 908 (79 percent) of the patterns were absolutely specific fortheir sequences (using the set of true and false positives and negatives as identified by the prosite developers).however, as the number of instances of a local pattern increases, the number of false positives also tends toincrease. for 10.5 percent of the patterns, 90 to 99 percent of the selected sequences were true positives, while forthe remaining 10.5 percent of the patterns, fewer than 90 percent of the selected sequences were true positives. toovercome this deficiency, the developers of the prosite database have begun to use weight matrices or profiles fordetection of domains. unlike the typical prosite, blocks, and prints methods, they create profiles of sequenceinformation such as residue type and solvent accessibility (gribskov, mclachlan et al., 1987) based on thecomplete protein sequence, not just a small segment. as with domainmatching methods, problems inherent inmatching highly divergent parts of the sequence, as well as the highly conserved functional regions, still reassertthemselves.similarity of global tertiary structure does not always imply similarity of functionin principle, additional information might be provided by comparing the complete tertiary structures ofproteins; however, comparison of overall structure is also not enough to classify protein function unequivocally.the structural databases such as scop (murzin, brenner et al., 1995), cath, and dali (holm and sander,1997) show significant redundancy in domain structures. proteins such as the barrels and the sandwiches canexhibit very similar structures even though they have very different functions. valuable information can beobtained from overall tertiary structure comparison (murzin, 1996), but two proteins with the same global tertiarystructure do not necessarily have the same function.proteins with similar function conserve the local structure around the active site, even ifthe global fold is dissimilaras the families become more diverse, the sequence similarity among many proteins in the family falls intoand below the twilight zone. then, standard sequence alignments have difficulty establishing a significantrelationship between sequences even though one might exist. for example, the mammalian and bacterial serineproteases demonstrate that proteins with very similar functions can have very different threedimensionalstructures (branden and tooze, 1991). the geometry of the active site would not be recognized by local sequencesignatures or by overall comparison of global tertiary structures, but only from an analysis of the structure of thefunctional residues around the active site.the role of computational biology in the genomics revolution52impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.table 4.3 data for classification of possible thioredoxin sequences by the prosite, prints, and blocks algorithmsprositea,bprintsb,cblocksb,dtpfpfntpfpfntpfpfnpossible thioredoxinsdsbchaeinxxxthiochlltxx(2)exthiochrvixxxthiorhoruxxxyx09myctuxxxy039myctuxxxyb59haeinxxxmay be thioredoxins (treated here as if they arethioredoxins)bs2trybbxxxfixwrhilexxxgsbpchickxxxresabacsuxx(2)exyme3thiff fxxxprobably not thioredoxinsync4caeelxpolgpvycxpolgpvynxpolgpvyhuxpolgpvyoxa prosite: recent prosite database online (thioredoxin examples updated 9/10/97).b tp = true positives; fp = false positives; fn = false negatives.c prints: search of owl26.0 database.d blocks: search of swissprot32.e prints uses three different sequence signatures to recognize the thioredoxins. ﬁ2ﬂ means that this sequence was recognized by only two ofthe three signatures.f a plasmid in e. coli expressing this gene product complements a thioredoxin mutant, providing experimental evidence that this proteinmay be a glutaredoxin or thioredoxin.development of a threedimensional library of functional motifswhat these examples suggest is that one might be able to excise the local structure around the active site anduse this local conformational signature to identify function. in fact, proteins function because of the arrangementof specific residues in threedimensional space, the residues involved in protein function, particularly those atenzyme active sites, will be highly conserved throughout evolution. this statement seems obvious and it wasclearly demonstrated experimentally by the serine protease presented above. the problem with recognizing theseresidues by sequence alignment is that they are likely to be distant along the sequence, even if they are closetogether in threedimensional space. this makes recognition by multiplesequencealignment methodsproblematic. if protein function relies on thethe role of computational biology in the genomics revolution53impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.specific tertiary placement of residues, then one should use that geometric information to describe functionalfamilies. we term these geometric (e.g., distances and angles) and conformational (e.g., a residue must be in ahelix) descriptors "fuzzy functional forms" (fffs). these methods do not rely on evolutionary conservation oflocal sequence as do the local sequence signature methods, but instead involve the construction of threedimensional descriptors of protein function.there are several distinct advantages to using geometric and conformational descriptors rather than localsequence signatures to describe protein function. it permits classification of proteins into families, even if there islittle or no sequence identity to other proteins in the database. thus, proteins that fall below the twilight zone ofsequence identity will still be amenable to analysis. nor does it rely on matching of the overall protein structure.thus, proteins with similar structures but different functions will be classified differently by this method. notethat the term "function," as used here, is defined very narrowly; what is meant is the biochemical activity of theprotein of interest.the one major disadvantage of this method is that the structure of the protein must be known. however, asdescribed below, fffs are specific and unique enough that the structure does not have to be known to highresolution. low to moderateresolution structures are sufficient for functional recognition, and current stateoftheart prediction algorithms can often predict protein structure at sufficient resolution to allow identification offunction using the fffs. finally, these prediction algorithms can be scaled up to analyze complete genomes.representative case: the glutaredoxin/thioredoxin familyoverviewin what follows, we consider the glutaredoxin/thioredoxin protein family. these proteins were selectedbecause members of these families have tertiary structures that have been predicted by ab initio methods (e.g., intable 4.2, 1ego is a glutaredoxin). this family also satisfies the requirement that the functional motif is not simplylocal in sequence, which could mean that difficulties might be expected in identifying all members of the familyfrom sequence basedmethods. members of the glutaredoxin/ thioredoxin protein family are small proteins thatcatalyze thioldisulfide exchange reactions via a redoxactive pair of cysteines in the active site. whileglutaredoxins and thioredoxins catalyze similar reactions, they are distinguished by their differential reactivity.glutaredoxins contain a glutathione binding site, are reduced by glutathione (which is itself reduced by glutathionereductase), and are essential for the glutathionedependent synthesis of deoxyribonucleotides by ribonucleotidereductase. thioredoxins are reduced directly by the specific flavoprotein thioredoxin reductase and act as moregeneral disulfide reductases. ultimately, however, reducing equivalents for both proteins come from nadph.protein disulfide isomerases (pdis) have been found to contain a thioredoxinlike domain and thus also have asimilar activity.the active site of the redoxin family contains three invariant residues: two cysteines and a cisproline.mutagenesis experiments have shown that the two cysteines separated by two residues are essential for significantprotein function. the side chains of these two residues are oxidized and reduced during the reaction (yang andwells, 1991; bushweller, aslund et al., 1992). however, this local sequence signature is not sufficient tospecifically select the members of the family. these two cysteines are also located at the nterminus of an ahelix.peptide studies suggest that the positive pole of the helix macrodipole affects the ionization of the cysteines and isimportant for protein function (kortemme and creighton, 1995, 1996). another unique feature of the redoxinfamily is the presence of a cisproline located close to the two cysteines in structure, but not in sequence. whilethis proline isthe role of computational biology in the genomics revolution54impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.structurally conserved in all glutaredoxin and thioredoxin structures (katti, robbins et al., 1995) and is invariant inaligned sequences of known glutaredoxins and thioredoxins, its functional importance is unknown. otherresidues, particularly charged residues, are also important for the specific thiol ionization characteristics of thecysteines, but are not essential and can vary within the family (dyson, jeng et al., 1997).the fff for the glutaredoxin/thioredoxin family is based on the threedimensional structural comparison ofbacteriophage t4 glutaredoxin, 1aaz (eklund, ingelman et al., 1992), human thioredoxin, 4trx (kay, clore et al.,1990), and proline disulfide isomerase, ldsb (martin, bardwell et al., 1993), as well as on literature searches to findresidues and structures shown to be functionally important. it consists of two cysteines separated by two residuesat the nterminus of a helix and close to a proline residue. the exact distances are described elsewhere (fetrowand skolnick, 1998).ability of the fff to identify the active site in experimentally determined structuresthe fff is sufficient to distinguish proteins belonging to the redoxin family uniquely from a data set of 364nonredundant proteins from the brookhaven database. for this set of 364 proteins, 13 have the sequence signaturecxxc. of these, three have a proline within the requisite distances. of these three, only 1thx (a thioredoxin)and 1dsb (chain a, a disulfide binding protein) have the cysteines at or near the nterminus of a helix. these twoproteins are the only two true positives in the test data set, showing that this simple fff is quite specific for theredoxin protein family. thus, the fff can be applied to experimental structures to identify active sites.application of the fff to predicted structuresis this fff sufficient to identify the function of an inexact model of a protein, or is a highresolution crystalor solution structure required? the structure of glutaredoxin, 1ego, was predicted with a 5.7å crmsd bymonsster (ortiz, kolinski et al., 1998a,b). the sequence of this glutaredoxin exhibits less than 30 percentsequence identity to any of the three structures used to create the fff. the redoxin fff was applied to 25 correctstructures and 56 incorrect or misfolded structures generated by monsster on the 1ego sequence during theisothermal runs. it specifically selects all 25 egolike structures as belonging to the redoxin family and rejected all56 misfolded structures. a set of 267 correctly and incorrectly predicted structures produced by the monssteralgorithm for five different proteins was then created. the glutaredoxin/thioredoxin fff was specific for thecorrectly folded ego structures and did not recognize any of the other correctly or incorrectly folded structures.screening of entire genomesthis sequencetostructuretofunction concept has been applied to the analysis of the complete e. coli genome; i.e., all e. coli open reading frames (orfs) are screened for the thioldisulfide oxidoreductase activity ofthe glutaredoxin/thioredoxin protein family. the method can identify the active site residues in 10 sequences thatare known to or proposed to exhibit this activity. furthermore, oxidoreductase activity is predicted in two othersequences that have not been previously identified. these results are summarized in table 4.4. the methoddistinguishes protein pairs with similar active sites from protein pairs that are just topological cousins, i.e., thosehaving similar global folds, but not necessarily similar active sites.the role of computational biology in the genomics revolution55impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.table 4.4 glutaredoxins and thioredoxins identified in e. coli strain k12functional motifadatabase namebthrd/fffcblst/fffdpsppspbbdatabase descriptionglr1ecolixxxxxxglutaredoxin 1glr2ecolixxxexglutaredoxin 2glr3ecolixxxxxxglutaredoxin 3thioecolixxxxxxthioredoxindsbaecolixxxxfxthioldisulfide interchange proteindsbcecolixxxexthioldisulfide interchange proteindsbdecolixxxxxxctype cytochrome biogenesis protein(innermembrane cu tolerance protein)dsbeecolixxxexxthioldisulfide interchange protein; (cyto cbiogenesis protein ccmg)yfigecolixxxxxxhypothetical thioredoxinlike proteinnrdhecolixxfxglutaredoxinlike nrdh proteinnrdgecolixanaerobic ribonucleoside triphosphateinactivating proteinb0853xorf; putative regulatory proteinyiejecolixhypothetical protein in tnabbglbintergenic regiona functional motif: search of each sequence found by either blast/fff or thread/fff protocols against the local signature databasesprosite, prints using the prosite scoring method, prints using the blocks scoring method, or blocks. each motif database was searched withthe given sequence, and the returned scores were analyzed to see if the thioredoxin or glutaredoxin families were identified.b database name: this is the database identifier for each sequence. all sequences come from the swissprot database, except b0853,which is the label given by the e. coli genome database. this sequence can also be accessed by the genbank accession numberecae000187.c thrd/fff: alignment of e. coil open reading frame (orf) to the sequences of 1ego, 1dsb (chain a), or 2trx (chain a) using a threadingalgorithm, followed by analysis of the resulting sequencesequence alignment for the active site residues specified by the fuzzy functionalform (fff) for the thioldisulfide oxidoreductase activity of the glutaredoxin/ thioredoxin family. threading results are for a combinationof three different scoring methods, sq, br, and tt, as described by godzik and coworkers (jaroszewksi et al. 1998).d blst/fff: alignment of each e. coli orf to the sequences of the lego, ldsb, chain a, and 2trx, chain a proteins using the blast searchprotocol, followed by analysis of the resulting sequencesequence alignment for the active site residues specified by the thioldisulfideoxidoreductase activity of the glutaredoxin/thioredoxin family. results reported here are for a combination of the gappedblast protocoland the psiblast alignment protocols. all sequences marked are found by both gapped and psiblast, except yifjecoli, whichis found only by gappedblast.e prints has three patterns for glutaredoxin/thioredoxin activity. this sequence hits only one of the patterns.f prints has three patterns for glutaredoxin/thioredoxin activity. this sequence hits only two of the patterns.computational requirements for genome scale structure/functionpredictionthe computational requirements of this type of genomic screening analysis are quite substantial. forexample, contemporary ab initio proteinfolding methods are applicable to single domain proteinsšup to about150 or so residues in lengthšand can identify possible novel protein folds. threading isthe role of computational biology in the genomics revolution56impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.significantly less expensive. table 4.5 gives a summary of the cpu requirements for protein structure predictionon the genomic scale. thus, given the extensive cpu requirements and the large number of genomic sequences,this type of sequencetostructuretofunction paradigm would greatly benefit from the availability of teraflopsclass machines. this would allow for the construction of low to moderateresolution predicted structures of asubstantial fraction of proteins in the genome, as well as the prediction of their molecular function. since thesecalculations are basically data parallel, they should be done on a machine composed of a large number of looselycoupled processors; e.g., farms of pcs are one means of achieving this. this is typical of many but not all types ofcalculations at the interface of chemistry and biology.table 4.5 cpu requirements for protein structure prediction on the genomic scalegenomenumber of orfsnumber of orfs <150 residuesab initio folding cpu timeacpu timebm. genitalium408824,9202h. influenzae1,68036922,4108.4m. jannaschii1,73542525,5008.9e. coli4,29087952,74021.5s. cerevisiae5,8851,43385,98029.4a assumes an average of 60 cpu days to perform 1,000 folding simulations per sequence on a single processor of an sgi origin 200running at 180 megahertz.b 200 sequences threaded through 1,000 structures takes 1 cpu day.outlook for the futurewhile low to moderateresolution models can be used to predict protein biochemical activity, they are toocrude to be used in drug ligand design. techniques that allow for refinement of these lowresolution to higherresolution models must be developed. one can imagine a hierarchical approach where the overall topology of theprotein is predicted using a reduced protein model, and then atomic detail is added. such simulations being doneat atomic detail will be very cpuintensive and can profitably exploit the parallelism of current moleculardynamics codes such as amber (pearlman, case et al., 1991) or charmm (brooks, bruccoleri et al., 1983).recently there has been encouraging progress along this direction both for folding of small proteins at atomicdetail (duan, wang et al., 1998) and for the refinement of protein structures starting from a reduced protein modeland finishing with molecules at atomic detail (simmerling, lee et al., 1998). to accomplish this goal, in general,will require the development of more efficient conformational sampling algorithms as well as better potentials thatcan discriminate the native conformation from the myriad of alternative structures.in the area of structural genomics, where the objective is to determine the structure of all possible types ofprotein folds (holm and sander, 1996), computation will also play a key role. this will happen in sequenceselection where the goal is to identify sequences likely to adopt novel folds and where ab initio techniques mayprove to be particularly useful, as well as in the development of techniques that will allow for more rapid structuredetermination. here, approaches that combine a limited amount of experimental data with structure prediction mayprove to be particularly powerful (monge, friesner et al., 1994; aszodi, gradwell et al., 1995; monge, lathrop etal., 1995; mumenthaler and braun, 1995; dandekar and argos, 1996; skolnick, kolinski et al., 1997; kolinski andskolnick, 1998). suchthe role of computational biology in the genomics revolution57impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.experimental data may come from nuclear magnetic resonance, from electron microscopy, and from lowresolutionxray crystal structures.another promising area of investigation will be in the prediction of protein binding regions. this will be thefirst step toward identifying multidomain interactions, both in the sense of predicting which proteins interact aswell as where they interact. then, the simulation of more complex interactions involving the components ofvarious signaling pathways and metabolic cascades will have to be addressed. the very elegant studies of schultenand coworkers on the light harvesting complex are an excellent example of the power of such approaches (hu andschulten, 1998). more generally, the simulation of membrane proteins and the prediction of their structure andfunction will also be a very important, computerintensive area of investigation (milik and skolnick, 1992, 1993;heijne, 1994, 1995; stowell and rees, 1995; casadio, fariselli et al., 1996) and will be the active focus of futureresearch in the next 5 to 10 years. in addition to studies at full atomic detail, hierarchical approaches that representthe system at different levels of detail will be developed. in this regard, an interesting preliminary study is found inthe simulation of virus coat protein assembly (rapaport, johnson et al., 1998).another very important area of investigation that touches on the areas of computer science, biology, andchemistry will be in the development and presentation of large databases containing all that is known about a givenprotein, its structure, and molecular and physiological function. basically, since so much information is and willbe available, means must be developed to make it usable and understandable to both the specialist and thenonspecialist alike. this is a very outstanding unsolved problem, but it is a reasonable guess that webbased toolsare going to be very important.summarythese studies demonstrate that protein function prediction based on the sequencetostructuretofunctionparadigm can successfully compete with more standard sequencebased approaches and may well identify thefunction of additional proteins in the twilight zone of sequence identity. what is very encouraging is that lowresolution structures as provided by stateoftheart tertiary structure predictions can identify active sites by usingappropriate threedimensional conformational descriptors, the fuzzy functional forms. future methodologicaldevelopments may allow for the prediction of protein structures at the resolution required for automated drugdesign. this will enable the sequencetostructuretofunction paradigm to realize its full potential. moregenerally, largescale simulations that describe the interactions of large protein (and/or membrane) aggregates willbe undertaken in the near future. such simulations will not only provide fundamental insights into how variouscellular processes work at the microscopic and mesoscopic level, but may also suggest therapeutic approaches atthe molecular level for the treatment of numerous diseases. these advances in algorithms and techniques at theinterface of biology and chemistry will rely on the use of large numbers of inexpensive computers. often, thesecan be loosely coupled, but other problems demand closely coupled, parallel machines. whatever the mode ofparallelism, advances in computational biology will, depending on the specific problem, require the availability of 1to 100 teraflopsclass machines. given the advances in raw cpu power as well as theoretical understanding, thereis every reason to believe computational biology and chemistry will play a major role in the genomics revolution.literature citedaltschul, s., t. madden, et al. (1997). gapped blast and psiblast: a new generation of protein database search programs. nucleic acidsres. 25: 33893402.the role of computational biology in the genomics revolution58impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.aszodi, a., m.j. gradwell, et al. (1995). global fold determination from a small number of distance restraints. j. mol. biol 248: 308326.attwood, t. and m. beck (1994). printsša protein motif fingerprint database. protein eng. 7: 841848.attwood, t., m. beck, et al. (1997). novel developments with the prints protein fingerprint database. nucleic acids res. 25: 212216.bairoch, a. (1990). prosite: a dictionary of protein sites and patterns . department de biochimie medicale, universite de geneva, geneva.bairoch, a., p. bucher, et al. (1995). the prosite database, its status in 1995. nucleic acids res. 241: 189196.branden, c. and j. tooze (1991). introduction to protein structure . new york and london, garland publishing, inc.brindle, p., k. and m.r. montminy (1992). the creb family of transcription factors. curr. opin. genet. develop. 2: 199204.brooks, b.r., r. bruccoleri, et al. (1983). charmm: a program for macromolecular energy minimization, and molecular dynamics. j.comp. chem. 4: 187217.bryant, s.h. and c.e. lawrence (1993). an empirical energy function for threading protein sequence through folding motif. proteins16:92112.bult, c.j., o. white, et al. (1996). complete genome sequence of the methanogenic archaeon methanococcus jannaschii. science 273:10581073.bushweller, j.h., f. aslund, et al. (1992). structural and functional characterization of the mutant escherichia coli glutaredoxin (c14s) andits mixed disulfide with glutathione. biochemistry 31: 92889293.casadio, r., p. fariselli, et al. (1996). a predictor of transmembrane ahelix domains of proteins based on neural networks. eur. biophys. j. 24:165178.casari, g., c. ouzounis, et al. (1996). genequiz ii: automatic function assignment for genome sequence analysis. the first annual pacific symposium on biocomputing. world scientific, pp. 708709.dandekar, t. and p. argos (1996). identifying the tertiary fold of small proteins with different topologies from sequence and secondarystructure using the genetic algorithm and extended criteria specific for strand regions. j. mol. biol. 256: 645660.duan, y., l. wang, et al. (1998). the early stage of folding of villin headpiece subdomain observed in a 200 nanosecond fully solvatedmolecular dynamics simulation. proc. natl. acad. sci. u.s.a. 95: 98979902.dyson, h.j., m.f. jeng, et al. (1997). effects of buried charged groups on cysteine thiol ionization and reactivity in escherichia coli thioredoxin: structural and functional characterization of mutants of asp 26 and lys 57. biochemistry 36: 26222636.eklund, h., m. ingelman, et al. (1992). structure of oxidized bacteriophage t4 glutaredoxin (thioredoxin). refinement of native and mutantproteins. j. mol. biol. 228: 596618.fetrow, j., a. godzik, et al. (1998). functional analysis of the escherichia coli genome using the sequencetostructuretofunction paradigm:identification of proteins exhibiting the glutaredoxin/thioredoxin disulfide oxidoreductase activity. j. mol. biol. 282: 703711.fetrow, j. and j. skolnick (1998). method for prediction of protein function from sequence using the sequencetostructuretofunctionparadigm with application to glutaredoxins/thioredoxins and t1 ribonucleases. j. mol. biol. 281: 949968.göbel, u., c. sander, et al. (1994). correlated mutations and residue contacts in proteins. proteins 18: 309317.godzik, a., a. kolinski, et al. (1993). de novo and inverse folding predictions of protein structure and dynamics. j. comp. aided mol. design7: 397438.godzik, a., j. skolnick, et al. (1992). a topology fingerprint approach to the inverse folding problem. j. mol. biol. 227: 227238.gribskov, m., a.d. mclachlan, et al. (1987). profile analysis: detection of distantly related proteins. proc. natl. acad. sci. u.s.a. 84:43554358.heijne, g.v. (1994). membrane proteins: from sequence to structure. annu. rev. biophys. biomol. struct. 23: 167192.heijne, g. v. (1995). membrane protein assembly: rules of the game. bioessays 17(1): 2530.henikoff, s. and j. henikoff (1991). automated assembly of protein blocks for database searching. nucleic acids res. 19: 65656572.holm, l. and c. sander (1996). mapping the protein universe. science 273: 595602.holm, l. and c. sander (1997). dali/fssp classification of three dimensional protein folds. nucleic acids res. 25: 231234.hu, x. and k. schulten (1998). model for the light harvesting complex i (b875) of rhodobacter spheroides. biophys. j. 75: 683694.the role of computational biology in the genomics revolution59impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.jaroszewski, l,rychlewski, l., et al. (1998). fold prediction by a hierarchy of sequence, threading, and modeling methods. protein sci. 7:14311440.katti, s.k., a.h. robbins, et al. (1995). crystal structure of thioltransferase at 2.2 å resolution. protein sci. 4: 19982005.kay, j.d.f., g.m. clore, et al. (1990). studies on the solution conformation of human thioredoxin using heteronuclear15n1h nuclearmagnetic resonance spectroscopy. biochemistry 29: 15661572.kolinski, a. and j. skolnick (1998). assembly of protein structure from sparse experimental data: an efficient monte carlo model. proteins 32:475494.kolinski, a., j. skolnick, et al. (1997). a method for the prediction of surface uturns and transglobular connections in small proteins.proteins 27: 290308.kolinski, a.k. and j. skolnick (1996). lattice models of protein folding, dynamics and thermodynamics. austin, tex., r.g. landescompany.kortemme, t. and t.e. creighton (1995). ionisation of cysteine residues at the termini of model alphahelical peptides. relevance to unusualthiol pka values in proteins of the thioredoxin family. j. mol. biol. 253: 799812.kortemme, t. and t.e. creighton (1996). electrostatic interactions in the active site of the nterminal thioredoxinlike domain of proteindisulfide isomerase. biochemistry 35: 1450314511.martin, j.l., j.c. bardwell, et al. (1993). crystal structure of the dsba protein required for disulphide bond formation in vivo. nature 365:464468.milik, m. and j. skolnick (1992). spontaneous insertion of polypeptide chains into membranes: a monte carlo model. proc. natl. acad. sci. u.s.a. 89: 93919395.milik, m. and j. skolnick (1993). insertion of peptide chains into lipid membranes. an offlattice monte carlo dynamics models. proteins 15:1025.miller, r.t., d.t. jones, et al. (1996). protein fold recognition by sequence threading: tools and assessment techniques. federation ofamerican societies for experimental biology (faseb) journal 10: 171178.monge, a., r.a. friesner, et el. (1994). an algorithm to generate lowresolution protein tertiary structures from knowledge of secondarystructure. proc. natl. acad, sci. u.s.a. 91: 50275029.monge, a., e.j.p. lathrop, et al. (1995). computer modeling of protein folding: conformational and energetic analysis of reduced and detailedprotein models. j. mol. biol. 247: 9951012.mumenthaler, c. and w. braun (1995). predicting the helix packing of globular proteins by selfcorrecting distance geometry. prot. sci. 4:863871.murzin, a.g. (1996). structural classification of proteins: new superfamilies. curr. opin. struct. biol. 6: 386394.murzin, a.g., s.e. brenner, et al. (1995). scop: a structural classification of proteins database for the investigation of sequences andstructures. j. mol. biol. 247: 536540.olmea, o. and a. valencia (1997). improving contact predictions by the combination of correlated mutations and other sources of sequenceinformation. folding & design 2: s25s32.orengo, c.a., a.d. michie, et al. (1997). cathša hierarchic classification of protein domain structures. structure 5: 10931108.ortiz, a., a. kolinski, et al. (1998a). fold assembly of small proteins using monte carlo simulations driven by restraints derived from multiplesequence alignments . j. mol. biol. 277: 419448.ortiz, a., a. kolinski, et al. (1998b). nativelike topology assembly of small proteins using predicted restraints in monte carlo simulations.proc. natl. acad. sci. u.s.a. 95: 10201025.ortiz, a., a. kolinski, et al. (1998c). tertiary structure prediction of the kix domain of cbp using monte carlo simulations driven byrestraints derived from multiple sequence alignments. proteins 30: 287294.pearlman, d.a., d.a. case, et al. (1991). assisted model building with energy refinement (amber) code. university of california, sanfrancisco.pearson, w. and d. lipman (1988). improved tools for biological sequence comparison. proc. natl. acad. sci. u.s.a. 85: 24442448.radhakrishnan, i., g.c. perezalvarado, et al. (1997). solution structure of the kix domain of cbp bound to the transactivation domain ofcreb: a model for activator:coactivator interactions. cell 91: 741752.rapaport, d.c., j.e. johnson, et al. (1998). supramolecular selfassembly: molecular dynamics modeling of polyhedral shell formation.comput. phys. commun., submitted.rastan, s. and l. beeley (1997). functional genomics: going forwards from the databases. curr. opin. genet devel. 7: 777783.rost, b. and c. sander (1993). prediction of secondary structure at better than 70% accuracy. j. mol. biol. 232: 584599.rost, b., r. schneider, et al. (1993). progress in protein structure prediction? tibs 18: 120123.the role of computational biology in the genomics revolution60impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.sali, a. and t. blundell (1993). comparative protein modelling by satisfaction of spatial restraints. j. mol. biol. 234: 779815.sander, c. and r. schneider (1991). database of homology derived protein structures and the structural meaning of sequence alignment.proteins 9: 5668.simmerling, c., m. lee, et al. (1998). combining monsster and les/pme to predict protein structure from amino acid sequence:application to the small protein cmti1 j. am. chem. soc., submitted.skolnick, j., a. kolinski, et al. (1997). monsster: a method for folding globular proteins with a small number of distance restraints. j.mol. biol. 265: 217241.stowell, m.h.b. and d.c. rees (1995). structure and stability of membrane proteins. adv. protein chem. 46: 279311.thomas, d.j., g. cesari, et al. (1996). the prediction of protein contacts from multiple sequence alignment. protein eng. 11: 941948.wodak, s.j. and m.j. rooman (1993). generating and testing protein folds. curr. opin. struct. biol. 3: 247259.yang, y.f. and w.w. wells (1991). identification and characterization of the functional amino acids at the active center of pig liverthioltransferase by sitedirected mutagenesis. j. biol. chem. 266: 1275912765.discussionwilliam winter, sunyesf, syracuse: glycosylation has to play a major role in the final selection of aparticular protein conformation in many proteins where it does occur. are you doing anything at all to use thatkind of information to make further selections once you have determined a family of possible structures?jeffrey skolnick: not yet, but we are aware of the problem. so far we have picked molecular functions thatare basically selfcontained by design because we did not pick the hardest case first. but you are absolutely right,glycosylation is extremely important. the problem there is that not a lot is known. even the potentials that youshould put in to describe the conformational spectrum are not well established. people are still developing these, sothat field is very much in its infancy. our view has been, yes, we recognize it is important, and especially in abiological context it is very, very important; it protects the proteins and keeps them from being chewed up, but wequite frankly wanted to consider the simplest cases first to see if the basic approaches could workšchoosemolecular functions or biochemical functions where it is apparently not believed to be important and then workour way up. but, yes, you are absolutely right. one day we or someone else will have to deal with that problem,but i think it is premature at this stage of the game.david dixon, pacific northwest national laboratory: jeff, have you looked at or have you startedthinking about the fact that there is also spatial resolution within a cell, and have you looked at how you connectyour proteins up into cell signaling pathways?jeffrey skolnick: yes, we have already started, at least on a very schematic level, simulating peptideinsertion and protein insertion into membranes, treating the system, you know, with spatial anisotropy. you have amembrane region that could be treated at various levels of detail in the interfacial regions, bulk regions, but onlyon a very, very schematic level at this point. as it is, these kinds of calculations really tax any resources that wecan get hold of, and we are not sure about adding additional details other than on a very simplified level. and thenwe are not even sure that the descriptives are sufficiently good that it would be worthwhile. i mean, we are tryingto proceed on a very buildingblock basis: establish something that works, validate it, move on, make it morecomplicated, move on. my guess is the next thing we are going to do is membrane protein tertiary structureprediction, and there there are some encouraging results.the role of computational biology in the genomics revolution61impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.5needs and new directions in computing for the chemicalprocess industriesw. david smith, jr.e.i. dupontthis paper is organized into six sections. the first provides some general background information andhighlights the main factors driving the chemical process industries (cpi) today in order to provide a context for allthat follows. the second presents my view of the total process simulation requirements of the cpi and defines thescope of the problem. the next section then provides a brief overview of the current tool set that is being used inthe cpi. the fourth section introduces the european cape open project, funded by briteeuram and nearingcompletion; it also discusses briefly the followon project, global cape open. the fifth section considers theclosely related topic of process control, and the final section lists conclusions.background and factors driving the chemical industrylet's begin with a few general observations about the cpi. chemical and petroleum plants require huge,longterm capital investments. the lifetime of a plant, once built, will range from 25 to 50 years. processes andprocess designs are almost always customized. we (dupont) have quite a few nylon plants, but no two are alike.each time a new plant is built, we incorporate improvements that have been learned from the operation of the lastplant that was built. this places some constraints on the costs thatauthor's note: this paper did not go through the approval process that is normally required by dupont. therefore, thisshould not be viewed as an official dupont perspective on the needs and new directions in computing for the chemical processindustries. rather it is my personal view of the subject that is based on 20 years of university teaching and 20 years ofexperience with dupont, the latter being most relevant to the topic at hand. furthermore, at times it is necessary to refer tocommercial software products and make contrasts between their respective features. these references are not meant to beeither a recommendation or a condemnation of a particular product but are meant to illustrate different approaches that havebeen taken to try to satisfy the simulation needs of the chemical process industries. in fact, it is safe to say that the completesimulation tool for all of our needs does not and is never likely to exist because of the enormous diversity within a companylike dupont and the industry as a whole. also, generalizations will inevitably be used to make a point about the field for whichsomeone will be able to provide an exception or a counterexample, but in a broadbrash overview of any field that is to beexpected.needs and new directions in computing for the chemical process industries62impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.one is willing to accept for a particular plant. for example, if you are designing an airplane and you expect to build athousand of them, then you can afford to design and build in a supersophisticated control system because the costwill be spread out over many units. that does not happen in the cpi.our plants are large, very complex, and, i am sorry to say, still relatively poorly understood in the sense thatwe don't know all the details of the chemical reactions and their associated thermodynamics and transportproperties needed for the design of these facilities. because of this lack of knowledge and the inherent safetyconcerns of handling some very dangerous chemicals, our plants tend to be overdesigned to ensure that bothsafety and production goals will be met. even for some of our oldest and bestknown products we are stillimproving our fundamental understanding. this is happening largely because of vastly improved analyticalinstrumentation. in some cases we have benefited significantly from the application of computational chemistry toprovide basic data that otherwise would have been too costly and time consuming to measure in the laboratory.most of our large plants, particularly those that make polymers, were justified on economies of scale. initiallythat made sense because we were only expecting to make one or two products. over time, because of goodchemistry and market demand, the number of products has grown, which has often meant that the plant has had amuch broader range of operating conditions. in many cases we have designed and built very good ﬁbattleshipsﬂbut now, because of the proliferation of products, we are forced to try and run them as if they were "pt boats."another characteristic of those polymer plants that make filament and sheet products is that they typically have avery wide range of time scales. at the front end, the reactor might have a time constant that is measured in hourswhile at the back end of the process where the filament or sheet is being formed, the time constant may bemeasured in milliseconds or seconds. the combination of multiproduct transitions with wide ranges of operatingconditions and time constants leads to some very challenging manufacturing problems.the essential problem facing the chemical process industries is shown in figure 5.1. the graph shows thetrend of capital productivity with time for both total manufacturing and chemicals. capital productivity tries tomeasure how effectively capital is being utilized. simply stated, for each dollarfigure 5.1capital productivity for all manufacturing and for chemicals.needs and new directions in computing for the chemical process industries63impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.invested in manufacturing facilities, how much profit do i make? the breakeven point was assigned arbitrarily avalue of 100, so it easy to see that since 1987 the chemical industry has not been the best place to invest yourmoney. the major contributors to the decline have been increased energy costs after the oil embargo, overcapacityand tougher global competition, increased costs for environmental programs, and new business ventures outside ofcore competencies that have been costly and unprofitable.in this setting, what are chemical companies focusing on? certainly the answer will vary from company tocompany but, using dupont as an example, the top five items are asset productivity, growth, quality,environmental issues, and shorter development cycle. the middle three really don't need any explanation. assetproductivity deals with the problem of squeezing even greater profits from the large investment we already have inthe ground. three of the major factors that govern the productivity of a plant are yield, uptime, and instantaneousproduction rate. yield, unfortunately, can be defined in many ways and is often defined to suit the needs of thedefiner. in an effort to eliminate that ambiguity we use firstpass, firstquality yield, which counts only the amountof the onaim desired product made in one pass of the fresh feed through the reactor. one wants to eliminaterework and blending of offspecification material as well as product reclassification. in multiproduct plants onealso tries to minimize transition losses. uptime focuses on preventive maintenance, as you can't make a product ifthe plant isn't running. assuming you can sell what you make, one always wants to run the plant at the maximuminstantaneous rate, which is usually based on the best past performance of the plant.at a recent meeting, a speaker from dow reported that from the time the chemist comes up with a new ideaat the bench to the time that the first pound comes out of a commercialscale plant, is typically about 12 to 13years. that process development cycle is just a little longer than the 10 to 11year average that dupont likes toclaim. now that is much too long in today's very competitive and rapidly changing marketplace. if it takes youthat long to go from idea to finished plant, you will probably have competition from a lowercost replacementinkind product or the market could be dramatically altered by the appearance of another completely new product.either possibility could invalidate the whole basis for the investment that you have made. in one recent example indupont, cycle time was reduced to 5 years. the challenge we face now is to institutionalize what we learned inthat very successful project.process simulation needs of the cpiwhat is the process of designing a plant, and what software tools are needed? figure 5.2 presents a blockdiagram of the major steps in the process of designing a chemical plant. the impetus to build a plant generallycomes from one of three sources: a new discovery from the lab, a response to a customer request, or a neededcapacity increase for an existing product. in the third case, the design process should be easier because a lot isalready known from the existing plant. since, in principle, the first two options require more work and essentiallythe same steps, we will assume that we are dealing with a potential new product that has been discovered in thelab.borrowing from edward tufte1 in the preparation of figure 5.2, we intend the thickness of the arrows toconvey the bandwidth of the data and information that are transferred between the activities represented by theboxes. the idea for a new product and the process to manufacture it would originate in either box 1 or 3. ideally,one would get a process engineer, skilled in process synthesis, working1 tufte, edward. 1992. the visual display of quantitative information . chesire, conn.: graphics press.needs and new directions in computing for the chemical process industries64impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.with the chemist(s) as soon as the potential new product begins to look interesting and preliminary cost estimatesfor a process to manufacture it are needed. at this stage it is important to examine alternate raw materials anddifferent reaction conditions such as temperature, pressure, choice of solvent, and choice of catalyst. theselectivity, yield, and nature of the byproducts of the reaction determine the complexity and cost of the rest of theprocess. the rapid testing of alternatives at this stage is the key to successful process development and plantdesign. the heavy arrows connecting boxes 2 and 3 indicate the intensity of that interaction. unfortunately, thereis no commercially available software to aid in the process synthesis step. one very interesting program, pipii,has been developed at the university of massachusetts by james douglas and his students to address theseactivities. the difficulty in using commercial process simulators for process synthesis is that the equipmentmodules they provide require complete kinetic and thermodynamic data that are never available at this early stageof process design. in some cases one can make effective use of molecular modeling, box 4, to help developestimates of the missing data. that played a prominent role in the dupont example cited above with adevelopment cycle time of only 5 years. this part of the overall design process usually produces a small numberof "attractive" process alternatives that have been obtained by using consistent but approximate design andeconomic calculations to screen many potential processes. during the process synthesis step many calculationswere done on the basis of ideal vaporliquid equilibria data. since the next step requiresfigure 5.2process of plant design.needs and new directions in computing for the chemical process industries65impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.more accurate kinetic and thermodynamic data, every opportunity should be taken during this stage of thedevelopment process to extract this information from the available experimental data.at this time in the process, one needs to develop more rigorous and detailed estimates of process performanceand cost so that the final choice can be made between the alternatives developed in the process synthesis step. thisis generally accomplished by the use of commercial process simulators from the three major vendors, such asaspen plus from aspen tech, hysys from hyprotech, and pro ii from simulation sciences (see box 5). a lot ofwork gets done here, and it is not unusual for about 80 percent of the effort to be spent on getting reasonablyaccurate kinetic and thermodynamic data. to the extent that this has been done during the process synthesis step itwill speed up this part of the process. while the steadystate process design is being evaluated it is also veryimportant to evaluate the controllability and operability of the candidate processes. this is accomplished bydeveloping an appropriate dynamic model of the process (box 6). several of the commercial simulation programsnow claim to be able to convert the steadystate simulation into a dynamic simulation. for a single piece ofequipment, that claim may be correct, but for a process flow sheet that is neither possible nor, in general,desirable. in a steadystate flow sheet the lines connecting the different pieces of equipment do not have to haveany diameters or lengths specified since their role is simply to pass information to the next unit. in a dynamicsimulation, the size of the lines is essential to track the propagation of disturbances from one vessel to the next.furthermore, in most process control analyses, approximate or lowfidelity models are perfectly adequate and onewould not want to use models of the complexity typically used in a detailed flow sheet calculation.in the case where the design is for a multiproduct plant, we should try to ensure that the equipment isdesigned such that the product scheduling can be accomplished with minimum cost. at present there are no designmethodologies to solve this problem. the design is typically evaluated by doing case studies (box 8). whencontrollability and operability with scheduling considerations are satisfied, a basic data package is assembled andtransmitted to the organization that will do the detailed equipment design and construction (box 9).after the plant is built, we still have to run the plant and worry about what our competitors are doing. if theplant has been built to increase capacity of an existing product, then it is very likely that the other plants will bedistributed around the world. we have global operating, scheduling, and inventory decisions that have to be made.these are usually formulated as supply chain optimization problems (box 9).every box in figure 5.2 requires a different computer programšif one exists at allšand none of themcommunicate in any reasonable way. this leads to inefficiency in the design process, as some of the output fromone program needs to be reentered into one or more additional programs. the programs come from differentvendors, and they invariably use different methods for the evaluation of thermodynamic properties, which adds toour problems. it should also be obvious that this is a significant interdisciplinary process that requires skills thatrange from wet chemistry to molecular modeling to the optimization of large supply chain problems.commercial physical property databases do not cover our needs. we run reactions that range from less than100 degrees in the liquid phase to solid reactions at 1,200 degrees to plasma reactors at 5,000 degrees. we runpolymer reactions that span a pressure range from one to 2,000 atmospheres. most companies have their ownproprietary physical property databases that have to be incorporated into one or more of the simulation programsthat are used in boxes 2, 5, 6, and 9. all of the current commercial simulation products are closed, proprietaryprograms, which makes the job of incorporating proprietary thermodynamics or equipment modules much moredifficult.needs and new directions in computing for the chemical process industries66impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.another serious problem that none of the vendors has addressed in a significant way is the visualization ofresults. simulation programs generate masses of numbers, and there is no easy way to take the output from thesesimulation packages and look at it in a way that makes sense. a recent development has been to allow the exportof results to an exceltm spreadsheet. i suppose that is a small step in the right direction, but i don't consider it to be ageneral solution to this problem.an overview of current simulation programsto make sense out of some of the comments that follow, it would be helpful to look at figure 5.3. in myview, there are at least three important views of software. two views belong to the user community; one group isthe casual or infrequent user, while the second is the power user who tries to squeeze as much capability out of theprogram as he or she can. the final view is that of the person or team designing and writing the software. issuesthat the developers need to resolve are experience level of the user for whom the product is intended, choice ofcomputer platform(s), design for easy maintenance, management of versions and updates, provision of help anderror diagnostics to the user, etc. for this discussion the first issue is the key one. is the product intended for thecasual user or the power user? if the choice is made to design for the casual user, a lot of care will be taken toprevent the user from inadvertently gaining access to sections of the code that might govern the choice ofconvergence algorithm or criteria for convergence or other performance parameters to prevent the user from"making mistakes." once the decision has been made to "bulletproof" the code in this way, the ability of thepower user to use the software in sophisticated ways has been significantly curtailed. the power users tend to befrustrated by these imposed limitations. the major concern of the vendors is to be profitable so they need toconcentrate on the largest market, which, without question, is the casual user. in dupont the ratio of casual topower users is about 20/1.having decided which category of user to target, there are two choices for the simulation "technology" toimplement. they are the sequential modular approach and the equationbased approach. thefigure 5.3three important perspectives from which to view software.needs and new directions in computing for the chemical process industries67impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.sequential modular approach is the oldest and most common approach and is used by all the major vendors. in thisapproach the vendor provides a library of equipment modules that the user can connect in a sequence that bestrepresents the process. the problem is that the vendors have the experience and ability to provide only a limitedrange of equipment modules to industry. in particular, the ability to model reactors in these systems is very, verylimited.sequential modular simulators are best suited to do equipment rating instead of equipment design.equipment rating means that one wants to evaluate the performance of a piece of installed equipment. taking adistillation column as an example, the user would know the number of trays, location of the feed stream, and theappropriate set of physical properties and would wish to determine the effect of changes in feed rate, reflux ratio,or reboiler heat input on column performance. in contrast, the design problem is usually posed as how big thecolumn has to be to separate a feed stream of known rate and composition to products of specified purity. it isclumsy to do design calculations in sequential modular simulators, but it can be done by trial and error. in myopinion, the sequential simulator has been designed for the casual user. key word input is giving way to click anddrag input on pcs. convergence is still a problem for large flow sheets, and the diagnostics that are available whenconvergence fails are not extremely useful.equationbased simulators are for the power user. the learning curve is much steeper. one big advantage isthat both steadystate and dynamic models are possible. here the achilles heel is that it is essential to scale theequations that describe your equipment or you face severe stiffness problems. until very recently the diagnostics inthese systems were abysmal (that is an understatement), and they can still be better.one of the other advantages of the equationbased approach is that, for a specialized piece of equipment, it ismuch easier for the engineer to write down the equations that describe it, put them into the equationbasedsimulator, and let it solve them. before the solution can proceed, the user has to specify which variables areindependent and which ones are fixed. this means that, once the equations have been scaled and entered in thesimulator, the user can easily go from the rating problem to the design problem just by specifying what thedependent and the independent variables are in a particular problem. other advantages of the current generation ofequationbased simulators are that partial differential equations can now be handled much more easily and thedynamic models that can be developed in them can easily be linked to a distributed control system to provideoperator training systems.since they didn't appear in figure 5.2, some comment should be made about codes for computational fluidmechanics. they involve a very steep learning curve and tend to be used only by specialists. one of the majorproblems they have is a very serious lack of physical property support. one important application area is multiphase reactor problems, but those are still very difficult to solve. accuracy can be a problem because ofaccumulated roundoff errors in these very lengthy calculations. we solve some problems on a clay computer,running for anywhere from 10 to 24 hours. when a run is complete, the question is, how many significant digitsdo you believe you really have in the answer? even with doubleprecision calculations on the cray we believe thatin some problems we can count on only two significant digits in the answer.optimization codes also deserve some comment. these are still largely the domain of specialists and powerusers. global optimization methods for general problems are still an active research area. algorithms often need tobe tailored to the structure of the problem to get reasonable solution times.needs and new directions in computing for the chemical process industries68impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.the european cape open projectcape is an acronym for computeraided process engineering. open refers to having the vendors supportnonproprietary open standards for commercial simulators. the funding from briteeuram was approximately $3.5million for 3 years, and this first phase of the project ends in the middle of 1999.although this is a european activity, the only major software vendors at the time were all u.s. or canadian.so by default they were invited to participate. since that time hyprotech and simulation sciences have beenpurchased by large british companies, leaving aspen as the only north american simulation company. quantisci, asmall consulting company, is also a member. the universities participating are aachen, imperial college, andtoulouse. the companies involved are basf, bayer, bp, elf, ici, ifp, and dupont iberica.why were people interested in cape open? the driving force for this activity was provided by the largegerman chemical companies under the leadership of bayer. they had a real concern about the large quantity of oldbut valuable code that they wanted to keep on using in existing commercial simulators without having to rewriteit. this is often referred to as legacy code.every large chemical company that i know probably uses at least two of the three available commercialsimulators; some use all three. this is expensive, and when you use two of these simulators to look at the sameproblem in different parts of the company you waste a lot of time trying to figure out which one of the two is reallygiving you the right answer, because they will be different. since the current commercial simulators are closedproprietary systems, it means that if hyprotech has a good azeotropic distillation model and aspen has therequired thermodynamics, you could not use the two together to solve the problem. it was also recognized that nomatter how hard the vendors try, they are not going to be able to provide all the modules that we need to model thewide range of systems that we deal with on a routine basis.the goal of the cape open project is to develop a nonproprietary framework for simulation software thatis based on software components. that requires standards for the functional interfaces between classes of softwarecomponents and the use of a binary standard such as microsoft's active x or corba. a software component iscompiled software, written in any language, that supports the binary standard. a software component can be either aserver or a client to any other software that supports the binary standard.the functional interfaces for which we have developed standards are thermodynamics, unit operations, andnumerics. prototypes exist for these three interfaces. they are in the final stages of testing now and they will bereleased in the next 6 months. what does this effort buy us? it buys us "plugandplay" capability. it means that iwill be able to take the thermodynamics from aspen and use them in a hyprotech simulation and it will run. wehave demonstrated that capability, and the vendors are working with us to complete the interface testing. whatwill a cape opencompliant simulator look like? i have tried to indicate this in figure 5.4.the part of the simulator that will be proprietary, which enables the vendors to compete, will be theexecutive. the rest of the simulator is divided into three main sections: the unit operations, which includes reactorsand other specialized pieces of processing equipment; thermodynamics and transport properties; and numericalroutines of all sorts. each unit operation will be a software component that will be able to communicate with theother major pieces of the simulator to get any services or information it needs to complete its calculations throughthe standard interfaces that have been developed. this means that all executives will be able to "use" a unitoperations software component that has the standard interfaces regardless of its origin. clearly similar statementscan be made about thermoneeds and new directions in computing for the chemical process industries69impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.dynamics and numerical components. the end result is a plugandplay simulation environment that will contain amuch broader choice of components.figure 5.4a cape opencompliant simulator.one of the most important aspects of having cape open software is that it will facilitate the transfer oftechnology from universities to industry. dupont had supported research on azeotropic distillation at theuniversity of massachusetts and we were interested in extending the contract to work on reactive distillation.recent research had shown that a change of variables would convert the reactive distillation equations into thesame form as the azeotropic equations. near the end of the first project, the university had negotiated a deal withhyprotech to commercialize the azeotropic code. hyprotech was using objectoriented programming (oop) aswas dupont, so during the planning session for the reactive distillation project the question was raised about usingthe inheritance features of oop to build the reactive distillation code from the azeotropic code. two dupont staffmembers, neither accomplished programmers, went to hyprotech and, with help from hyprotech programmers tounderstand the structure of the azeotropic code, they had in 2 weeks a working reactive distillation module thatwould run in hysys. we had budgeted 1 year of time for an experienced postdoc to code the problem infortran.one of the questions that this workshop was intended to address was how the advances in computer andcommunication technology would change the way industry works. you can imagine that a pump vendor, forinstance, would have on its web site a cape opencompliant software component for each of its products. if anengineer were doing a simulation and wanted to test a new pump or consider a different pump, he or she could goto the web site and either download the appropriate software component or run it over the web on the server,complete the simulation, and evaluate the results. if the results were satisfactory the engineer could check todetermine the availability of that model pump and perhaps place an order directly. one can imagine that allequipment suppliers to the cpi would provide similar services.what is global cape open? the people involved in cape open are very conscious of the fact that theirsis a relatively small european activity. they would like the standard interfaces that have beenneeds and new directions in computing for the chemical process industries70impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.developed to be accepted as universal or global standards and extended to cover a broader range of applications.some that have not been addressed for lack of time are solids handling, polymer processing, and pharmaceutical/biological systems. briteeuram has granted additional funding for another 2.5 to 3 years. the membership hasbeen extended to 18 partners from europe, japan, and north america. there is a problem, however, as the briteeuram funding is only for the european partners and, as in the past, most of it is directed to university research.other participants have to get local funding, and many universities in the united states would like to be a part ofthis research program but are faced with the problem of finding a u.s. funding agency that is willing to deal with amanufacturing question like this. industrial participation from north america has not been established as we arestill looking for a regional coordinator.process control in the cpimost processes in the chemical industry now have distributed control systems (dcss) and all new plantswould almost certainly have one included in the design specification. furthermore, as a rule of thumb, roughly 80percent of the control loops can be handled well with simple pid controllers. the other loops usually require someform of advanced control. as the power of the process control computers has increased, more and more advancedprocess control functionality has been incorporated into the dcs. now several of the major vendors provide theability to do moderately sized linear model predictive control applications. on the surface it would appear thatmost of industry's process control needs have been addressed by current dcs technology.the process control options that are available are all based on linear control theory because that is what weknow best. unfortunately, almost all chemical processes are nonlinear, and established linear techniques workwell only for mild nonlinearities. the business drivers that i described earlier are pushing the operation of ourprocesses into regions where the nonlinearities are emphasized, so the popular linear techniques may no longer begood enough.in a new application the first question is, do i need to use nonlinear control? there is no simple answer tothis question nor is there a simple calculation that would provide the answer. in practice the question is decidedwhen standard linear techniques do not work, but then another question immediately arisesšwhat nonlinearcontrol strategy should be used? again experience or trial and error will provide an answer. the final questionbecomes, how will the nonlinear control algorithm be implemented in the dcs? currently, this question is alwayspresent because the dcs vendors only provide linear control options.nonlinear control technology is inherently more complicated and is generally inaccessible to most controlpractitioners. nonlinear model development is several orders of magnitude more difficult than that for linearmodels. each application is usually unique and a specialized implementation is required. in spite of thesedifficulties we believe that there is a vast untapped potential economic benefit in cases of moderate nonlinearitieswhere linear methods can be used but where nonlinear methods would yield significant performanceimprovements.there is interesting work on nonlinear control being done in the universities that we would like to try, butthere is a major barrier to be overcome: the monolithic code that the dcs vendors have implemented in theirdcss. this makes it very difficult for them, let alone industrial control practitioners, to add or try new controlalgorithms on any commercial dcs. if the dcs vendors adopted the componentbased software developmentstrategy of the simulation software vendors, this problem could be solved and we would get a much faster andeffective transfer of advanced process control technology from the universities to industry. everyone wouldbenefit.needs and new directions in computing for the chemical process industries71impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.conclusions1. both process simulation and process control will benefit from componentbased redesign of theirsoftware.2. technology transfer in both areas will improve dramatically.3. componentbased software operating as clients or servers over the web will significantly reduce thecycle time for new process development. componentbased software has the potential to alter the waythat equipment suppliers market and sell their products to the cpi.4. the potential benefit from the application of nonlinear control is large, and vendors need to providethe capability for easier implementation of these techniques.discussionjack pfeiffer, air products and chemicals, inc.: dave, regarding your diagram on the modeling processthat you said moved from left to rightšone of the things that we have been toying with is a recycle loop thatwould bring information back from plant operations into the r&d or the process synthesis step. have you thoughtabout the value of that, and especially have you thought about how the software that we have today, or thedirections that you are proposing, may enhance that capability?david smith: well, i do not think there is anything that prevents you from doing that. i think the tools toanalyze historical process data do exist but they are not integrated with the tools that i have discussed. that is onearea that has not been considered by cape open. however, i think the biggest problem you actually have is thecultural problem of what the production or operations people get rewarded for. in dupont, that is significantlydifferent from what r&d folks get rewarded for, so the problem ultimately becomes the willingness of the plantpeople to accept change. they get rewarded for continuity of operation, no labor problems, no safety problems,and pounds of product shipped. if you go to the typical operations superintendent with a proposal to improveoperations, there will usually be little interest because it means downtime, retraining, and a whole bunch of thingsthat they do not want to deal with. so, it is the cultural problem that is much more difficult than the problem ofhaving that recycle loop of process information.jack pfeiffer: as a further clarification on that, one of the things we are thinking about is that we collect apot full of data in plants, and these data may be valuable if analyzed using the creative concepts of r&d.david smith: that is definitely true. we spend an inordinate amount of money collecting data that we neverreally analyze. the question is, why do we do that? one of the major reasons is that we have reduced the numberof people at the plants to the minimum, and those people are so busy that they do not have time to go back andlook at historical data until the plant is having problems.gintaris reklaitis, purdue university: if global cape open is such a wonderful thing for the industry,why is the industry not "ponying up" and supporting the activity? it does not appear that there are key technologybottlenecks in terms of how to structure objectoriented codes, how to develop them. why not "pony up" and doit?needs and new directions in computing for the chemical process industries72impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.david smith: the issue was that it was very difficult to convince the vendors to accept the concept of open,nonproprietary simulation systems. also the business of defining standard interfaces that will support the broadrange of modeling activities encountered in the cpi was a very difficult task. one of the reasons for formingglobal cape open was to have greater participation in testing and extending those standards. finally, mymanagement says we have ﬁponied upﬂ for 3 years with three people from dupont participating in and leadingparts of this activity. my current management feels that it is time for another u.s. company to step forward andlead the north american effort in global cape open.christos georgakis, lehigh university: dave, what do you perceive as the computational challenges inachieving industrialscale green chemistry, where green plants produce no pollution, only products? cape openis among the challenges, but what other computational challenges exist?david smith: cape opencompliant simulation software that is integrated as i suggested in figure 5.2 willhelp because it will allow the evaluation of more process alternatives and thus increase the chances of findingprocess designs that will have minimal environmental impact. however, i believe the fundamental challenge isstill one of chemistry. greg mcrae and his students have looked at some approaches to this problem. oneapproach is to start with different raw materials that might make a broader range of salable products but have muchless or no waste. i think our businesses would not support this approach, as the yield to products other than thedesired product could be significant. i think there is more hope in developing more selective catalysts and biocatalysts.tom edgar, university of texas: i had an industrial chemist ask me recently about a problem that he isencountering. he says that his business people are on his case because every time they design a plant they find outthey have about 20 percent overcapacity because of the intrinsic conservatism in designing the plant. do you thinkone of the bottlenecks is this software problem?david smith: no, the problem is not with the existing software, nor will cape opencompliant softwaresolve the problem. i think the problem we all have is the fundamental uncertainty about kinetics, thermodynamics,and transport data during the design. it is costly, timeconsuming work that is perceived as slowing down thedesign process. since you do not know those properties accurately enough you tend to overdesign the plant.initially that is viewed as a "bad" thing, but then 10 years down the road when you need incremental capacity youlook like a hero because you get it cheaply.needs and new directions in computing for the chemical process industries73impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.6vision 2020: computational needs of the chemical industryt.f. edgaruniversity of texasd.a. dixonpacific northwest national laboratoryandg.v. reklaitispurdue universityintroductionthere are a number of forces driving the u.s. chemical industry as it moves into the 21st century, includingshareholder return, globalization, efficient use of capital, faster product development, minimizing environmentalimpact, improved return on investment, improved and more efficient use of research, and efficient use of people.as the chemical industry tries to achieve these goals, it is investigating the expanded use and application of newcomputational technologies employed in areas such as modeling, computational chemistry, design, control,instrumentation, and operations. the key technology driver over the past 20 years has been the continuingadvances in digital computing. the 100fold increase in computer speed, and the same in software, each decadehas led to significant reductions in hardware cost for computers of all types and has increased the scope ofapplications in chemistry and chemical engineering.a forecast of future advances in process modeling, control, instrumentation, and optimization is a major partof the recently completed report technology vision 2020: report of the u.s. chemical industry. this report wassponsored by five major societies and associations (american institute of chemical engineers [aiche], americanchemical society [acs], council for chemical research [ccr], chemical manufacturers association [cma],and society of organic chemicals manufacturers association [socma]) and involved more than 200 businessand technical leaders from industry, academia, and government. it presents a road map for the next 20 years for thechemical and allied industries.the collaboration among the five societies, as well as government agencies (department of energy, nationalinstitute of standards and technology, national science foundation, and u.s. environmental protection agency),has spawned many additional workshops, generating more detailed r&d roadmaps on specific areas of chemicaltechnology. several workshops pertinent to this paper have been held during 1997 and 1998, covering the areas ofinstrumentation, control, operations, and computationalvision 2020: computational needs of the chemical industry74impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.chemistry. other vision 2020 workshops have been held on subjects such as separations, catalysis, polymers,green chemistry and engineering, and computational fluid dynamics.1this paper reviews the computational needs of the chemical industry as articulated in various vision 2020workshops. subsequent sections of this paper deal with process engineering paradigm in 2020, computationalchemistry and molecular modeling, process control and instrumentation, and process operations.process engineering in 2020increased computational speeds have spurred advances in a wide range of areas of transport phenomena,thermodynamics, reaction kinetics, and materials properties and behavior. fundamental mathematical models arebecoming available due to an improved understanding of microscopic and molecular behavior, which couldultimately lead to ab initio process design. this will enable design of a process to yield a product (e.g., a polymer)with a given set of target properties, predictable environmental impact, and minimum costs. ideally one wouldwant to be able to start with a set of material properties and then reverseengineer the process chemistry andprocess design that gives those properties.historically the chemical industry has used the following sequential steps to achieve commercialization:1. research and development,2. scaleup,3. design, and4. optimization.note that steps (1) and (2) generally involve several types of experimentation, such as laboratory discovery,followed by benchscale experiments (often of a batch nature), and then operation of a continuous flow or batchpilot plant. it is at this level that models can be postulated and unknown parameters can be estimated in order tovalidate the models. a plant can be designed and then optimized using these models. if the uncertainty in processdesign is high, pilotscale testing may involve several generations (sizes) of equipment. with the advent ofmolecularscale models for predicting component behavior, some laboratory testing can be obviated in lieu ofsimulation. this expands upon the traditional relationship of scientific theory and experiment to form a newdevelopment/design paradigm of process engineering (see figure 6.1).the development of mathematical models that afford a seamless transition from microscopic to macroscopiclevels (e.g., a commercial process) is a worthy goal, and much progress in this direction has occurred in the past 10years in areas such as computational fluid dynamics. however, due to computational limitations and to someextent academic specializations, process engineering research has devolved into four more or less distinct areas:1. process design,2. structure property relationships,3. process control,4. process operations.1 see <http://www.chem.purdue.edu/v2020/>, the vision 2020 web site for workshop reports.vision 2020: computational needs of the chemical industry75impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.figure 6.1process engineering paradigm for the 21st centuryin fact, research conferences will be held during the next 2 years in each of these areas, but only a few hardysouls will participate in crossfertilizing the areas by attending multiple conferences. consider the interaction ofprocess design and control; process design decisions can be made that simultaneously optimize plant profitabilityand the controllability of the plant, rather than the traditional twostep approach of designing the most profitableplant and then considering how to control it in a subsequent design phase. the different models, problem scope,and terminology used in each of these areas is an indicator that no lingua franca has emerged. actually areas (1),(3), and (4) fall under a broad umbrella of systems technology, but until these three areas begin to use a commonset of mathematical models, progress toward a more catholic view of process design will be impeded.a molecularlevel understanding of chemical manufacturing processes would greatly enhance the ability ofchemical engineers to optimize process design and operations as well as ensure adequate protection of theenvironment and safe operating conditions. currently there is considerable uncertainty in thermodynamic andreaction models, so plants are normally overdesigned (above required capacity) to allow for this uncertainty. alsoplants are operated conservatively because of an inadequate understanding of dynamic process behavior and thedire consequences if an unsafe condition arises. chemical reactors are at the heart of this issue, with uncertaintiesin kinetic mechanisms and rate constants and the effects of reactor geometry (such as catalyst beds) on heat andmass transfer. clearly the availability of better microscopic mathematical models for macroscopic plant simulationwill help the chemical industry operate more profitability and more reliably in the future.besides providing fundamental data for process simulations, computational chemistry plays an important rolein the molecular design process beginning at the basic research level. by predicting accurate thermochemistry, onecan quickly scope out the feasibility of reaction pathways as to whether a reaction is allowed or not.computational chemistry can also reliably predict a wide range of spectroscopic properties to aid in theidentification of chemical species, especially important reaction intermediates. electronic structure calculationscan also provide quantitative insights into bonding, orbital energies, and form, facilitating the design of newmolecules with the appropriate reactivity.computational chemistry and molecular modelingthe computational chemistry subgroup of vision 2020 under the sponsorship of the ccr has outlined a setof computational "grand challenges" or "technology bundles" that will have a dramatic impact on the practice ofchemistry throughout the chemical enterprise, especially the chemical industry. the computational ''grandchallenges" are given in box 6.1.vision 2020: computational needs of the chemical industry76impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.box 6.1 computational "grand challenges" for materials and processdesign in the chemical enterprisea. reliable prediction of biological activity from chemical structureb. reliable prediction of environmental fate from chemical structurec. design of efficient catalysts for chemical processesd. design of efficient processes in chemical plants from an understanding of microscopic molecularbehaviore. design of a material with a given set of target properties"grand challenge a" or "bundle a" in box 6.1 has received recent emphasis because this area includes drugdesign. however, the biological activity due to a specific chemical is needed for other areas such as agriculturalpesticide design and predictive toxicology. the potential for toxic impact of any chemical must be addressedbefore a chemical is manufactured, sold to the public, or released to the environment. furthermore, the toxicbehavior must be evaluated not only for human health issues but also for its potential ecological impact on plantsand animals. examining chemical toxicity is currently an extremely expensive process that can take a number ofyears of detailed testing. such evaluations usually occur late in development, and the inability to anticipate theevaluation of toxicological testing can place large r&d investments at risk. also, the possibility exists thatunanticipated toxicological problems with intermediates and byproducts can create liabilities. the cost oftoxicology testing is generally too high to complete testing early in the development process. thus reliable, costeffective means for predicting toxicological behavior would be of great benefit to the industry.grand challenge b in box 6.1 is focused on the need to predict the fate of any compound that is released intothe environment. for example, even if a compound is not toxic, a degradation product may show toxic behavior.besides being toxic to various organisms, chemicals released into the environment can affect it in other ways. adifficulty in dealing with the environmental impact of a chemical is that the temporal and spatial scales covermany orders of magnitude from picoseconds to 100,000 years in time, and from angstroms to thousands ofkilometers in distance. furthermore, the chemistry can be extremely complex and the chemistry that occurs ondifferent scales may be coupled. for example, chemical reactions that occur on a surface may be influenced notonly by the local site but also by distant sites that affect the local electronic structure or the surrounding medium.grand challenges c and d in box 6.1 are tightly coupled but are separated here because differentcomputational aspects may be needed to address these areas. catalysis and catalytic processes are involved inmanufacturing most petroleum and chemical products and account for nearly 20 percent of the u.s. gross domesticproduct. improved catalysts would increase efficiency, leading to reduced energy requirements, while increasingproduct selectivity and concomitantly decreasing wastes and emissions. considerable effort has been devoted tothe ab initio design of catalysts, but such work is difficult because of the types of atoms involved (often transitionmetals) and because of the fact that extended surfaces are often involved. besides the complexity of the materialsthemselves, an additional requirement is the need for accurate results. although computational results can oftenprovide insight into how a catalyst works, the true design of a catalyst will require the ability to predict accuratethermodynamic and kinetic results. for example, a factor of two to four in catalyst efficiency canvision 2020: computational needs of the chemical industry77impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.determine the economic feasibility of a process. such accuracies mean that thermodynamic quantities should bepredicted to within 0.1 to 0.2 kcal/mol and rate constants to within ~ 15 percentšcertainly difficult, if notimpossible, by today's standards. even for the nominally simple area of acid/base catalysis, many additionalfeatures may have to be included in the model, for example, the effects of solvation.another example of complexity is found in zeolites, where the sheer size of the active region makes modelingstudies difficult. modeling of the surfaces present in heterogeneous catalysts is even more challenging because ofthe large numbers of atoms involved and the wide range of potential reactive sites. if the catalyst containstransition metals, the modeling task is difficult because of the problems in the treatment of electronic structures ofsuch systems with singleconfiguration wave functions in a molecular orbital framework.a molecularlevel understanding of chemical manufacturing processes would greatly aid the development ofsteadystate and dynamic models of these processes. as discussed in subsequent sections, process modeling isextensively practiced by the chemical industry in order to optimize chemical processes. however, one needs to beable to develop a model of the process and then predict not only thermochemical and thermophysical propertiesbut also accurate rate constants as input data for the process simulation. another critical set of data needed for themodels are thermophysical properties. these include such simple quantities as boiling points and also morecomplex phenomena such as vapor/liquid equilibria phase diagrams, diffusion, liquid densities, and the predictionof critical points. the complexity of process simulations depends on whether a static or dynamic simulation is usedand whether effects such as fluid flow and mass transfer are included. examples of complex phenomena that arejust now being considered include the effects of turbulence and chaotic dynamics on the reactor system. a key roleof computational chemistry is to provide input parameters of increasing accuracy and reliability to the processsimulations.grand challenge e in box 6.1 is extremely difficult to treat at the present time. given a structure, we canoften predict at some level what the properties of the material are likely to be. the accuracy of the results and themethods used to treat them depend critically on the complexity of the structure as well as the availability ofinformation on similar structures. for example, various quantitative structure property relationship (qspr) modelsare available for the prediction of polymer properties. however, the inverse engineering design problem, designingstructures given a set of desired properties, is far more difficult. the market may demand or need a new materialwith a specific set of properties, yet given the properties it is extremely difficult to know which monomers to puttogether to make a polymer and what molecular weight the polymer should have. today the inverse design problemis attacked empirically by the synthetic chemist with his/her wealth of knowledge based on intuition and onexperience. a significant amount of work is already under way to develop the "holy grail" of materials design,namely, effective and powerful reverseengineering software to solve the problem of going backwards from a setof desired properties to realistic chemical structures and material morphologies that may have these properties.these efforts are usually based on artificial intelligence techniques and have, so far, had only limited success.much work needs to be done before this approach reaches the point of being used routinely and with confidence bythe chemical industry.the achievement of the goals outlined in box 6.1 will require significant advances in a number of science andtechnology areas. box 6.2 summarizes the important scientific research areas needed to accomplish the goalsoutlined in box 6.1, and box 6.3 summarizes technical issues that need to be addressed. below we highlight someof these issues.there are a number of methods for obtaining accurate molecular properties. one can now push thethermochemical accuracy to about 0.5 kcal/mol if effects such as the proper zeropoint energy, core/vision 2020: computational needs of the chemical industry78impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.box 6.2 research areas for implementation of grand challenges1. accurate methods for calculating thermochemical and thermophysical properties, spectroscopy, andkinetics (a,b,c,d,e)12. efficient methods for generating accurate potential functions for molecular mechanicsbased methods(a,b,c,d,e)3. improved methods for molecular dynamics simulations at long times for large ensembles (a,b,c,d,e)4. improved methods for including quantum effects (a,b,c,d,e)5. improved methods for including environmental effects such as solvent effects (a,b,c,d,e)6. efficient and accurate computational methods for treating solid state structures (b,c,d,e)7. improved optimization strategies for the determination of large, complex structures such as predictingprotein structure from sequence (a,b,c,d,e)8. accurate methods for treating the scaringup problem: molecular  microscopic  mesoscopic macroscopic (a,b,c,d,e)9. new techniques for materials design and bulk property prediction (e)10. new methods for predictive toxicology (a,b)11. integration of computational fluid dynamics (including latticeboltzmann approaches) with physics,chemistry, and biology to predict the behavior of reacting flows at different spatial and temporal scales2(b,d)1 impact on grand challenge from box 6.1 given in parentheses.2 additional research area for implementation of grand challenges.box 6.3 technology needs for implementation of grand challenges1. highperformance, scalable, portable computer codes for advanced (massively parallel) computerarchitectures (a,b,c,d,e)12. improved problemsolving environments (pses) to make computational tools more widely accessible(a,b,c,d,e)3. improved database and dataanalysis technologies (a,b,c,d,e)4. computeraided synthesis methods with a focus on materials (e)5. computer architectures, operating systems, and networks (a,b,c,d,e)1 impact on grand challenge from box 6.1 given in parentheses.vision 2020: computational needs of the chemical industry79impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.valence effects, and relativistic effects are considered. predicting kinetics can be considered as an extensionof thermochemical calculations if one uses variational transitionstate theory. instead of just needing an optimizedgeometry and calculated second derivatives at one point on the potential energy surface, this information isrequired at up to hundreds of points. it is necessary to incorporate solvent effects in order to predict reaction rateconstants in solution. the prediction of rate constants is critical for process and environmental models. predictedrate constants (computational kinetics) have already found use in such complex systems as atmospheric chemistry,design of chemical vapor deposition reactors, chemical plant design, and combustion models. spectroscopicpredictions are increasing in their accuracy, but it is still difficult to predict nmr chemical shifts to better than afew parts per million, vibrational frequencies to a few cm1, or electronic transitions to a few tenths of an electronvolt for a broad range of complex chemicals.there is a real need for accurate methods for predicting accurate thermophysics for gases and liquids. forgases, certain properties can be predicted with reasonable reliability based on the interaction potentials ofmolecular dimers and transport theory. for liquids, such properties can be predicted by using molecular dynamicsand grand canonical monte carlo (gcmc) simulations. the gcmc simulations are quite reliable for someproperties for some compounds, but they are very dependent on the quality of the empirical potential functions.such predictions, today, are much less reliable for mixtures or if ions are present.the whole area of potential functions needs to be carefully addressed. potential functions are needed for allatomistic simulations, (e.g., molecular dynamics and energy minimizations of materials, polymers, solutions, andproteins), monte carlo methods, and brownian dynamics. however, reliable potential functions are not availablefor all atoms and all bond types or for a wide range of properties such as polarization due to the medium. atpresent, it is very timeconsuming to construct potential functions. a robust, automated potential functiongenerator for producing a polarizable force field for all atom types needs to be developed. it needs to be able toincorporate both the results of quantum mechanical calculations and the empirical data.there is a critical need to be able to take atomistic simulations such as molecular dynamics to much longertime scales. at present, it is routinely possible to study atomistic systems (or systems represented as interactingatoms, such as proteins and polymeric systems) for periods on the order of nanoseconds. however, much longertime scales are needed for the study of such problems as phase transitions, rare events, kinetics, and long timeprotein dynamics for protein folding. even today, long runs on current computing systems create asyetunresolveddata issues due to massive amounts of data generated. for example, a single time step of a millionatom simulationeasily manipulates tens of megabytes of data. while a reasonable strategy for short simulations of small systems isto dump configurations every 10th or 50th time step for later analysis, this is clearly not an option for largescalesimulations over long time frames. methodologies for implementing and modifying data analysis "onthefly"must be developed and refined.the question of reaching macroscopic time scales from molecular dynamics simulations cannot be solvedsolely by increases in hardware capacity, since there are fundamental limitations on how many time steps can beexecuted per second on a computer, whether parallel or serial. one can scale the size of the problem withincreasing numbers of processors, but not to longer times. to cover macroscopic time scales measured in secondswhile following molecular dynamics time steps of 1015 seconds requires the execution of on the order of 1015time steps. even with fiveordersofmagnitude increases in clock cycles, the required computations will takedays. between now and 2020, clock rates will undoubtedly increase, but not by this magnitude. hence, the longtime problem in molecular dynamics will not be solved purely by hardware improvements. the key is thedevelopment of theoreticallyvision 2020: computational needs of the chemical industry80impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.sound, timecoarsening methodologies that will permit dynamicsbased methods to traverse long time scales.brownian dynamics with moleculardynamicssampled interactions and dynamic monte carlo methods arepromising possibilities for this purpose.a technology issue that will have an enormous impact on computational chemistry is that of computerarchitectures, operating systems, and networks. the highest performance today is pushing 1.0 teraflops ofsustainable performance on highly tuned code. the biggest technical issue is how to deal with nonuniform memoryaccess and the associated latency for data transfer between memory on distributed processors. the latest step forlargescale computers is massively parallel computing systems based on symmetric multiprocessors (smps). thegoal is tensofpetaflops performance by 2020. this will be achieved by improvements in the speeds of individualchips, which have been doubling every 18 months, although the cost of building plants to produce them may leadto a lengthening of the time to double processor speed. there will have to be significant improvements in switchesas well as in memory speeds, and i/o devices (disks) will need to be much faster and cheaper. there is a real needfor significant advances in application software for usable teraflops to petaflops performance to be achieved aswell as improvements in operating systems (oss). one major issue will be the need for singlethreaded oss thatare faulttolerant, as the reliability of any single processor means that some will fail on a given day. it is the issueof operating systems, especially for largescale batch computing, that is likely to hold up the ability to broadlyaddress the computational grand challenge issues raised above.in summary, rapid advances on many fronts suggest that we will be able to address the complexcomputational grand challenges outlined above. this will fundamentally change how we will do chemistry in thefuture in research, in development, and in production. getting there will not be simple and will require novelapproaches, including the use of teams from a range of disciplines to develop the software, manage the computersystems, and perform the research.process control and instrumentationthe process control and instrumentation issues identified in vision 2020 include changes in the way plantsoperate, computer hardware improvements, the merging of models for design, operations, and control,development of new sensors, integration of measurement and control, and developments in advanced control. inthe factory of the future, the industrial environment where process control is carded out will be different than it istoday. in fact, some forwardthinking companies believe that the operator in the factory of the future may need tobe an engineer, as is the case in europe. because of greater integration of the plant equipment, tighter qualityspecification, and more emphasis on maximum profitability while maintaining safe operating conditions, theimportance of process control will increase. very sophisticated computerbased tools will be at the disposal ofplant personnel. controllers will be selftuning, operating conditions will be optimized frequently, fault detectionalgorithms will deal with abnormal events, total plant control will be implemented using a hierarchical(distributed) multivariable strategy, and expert systems will help the plant engineer make intelligent decisions(those he or she can be trusted to make). plant data will be analyzed continuously and will be reconciled usingmaterial and energy balances and nonlinear programming, and unmeasured variables will be reconstructed usingparameter estimation techniques. digital instrumentation will be more reliable and will be selfcalibrating, andcomposition measurements that were heretofore not available will be measured online. there are many industrialplants that have already incorporated several of these ideas, but no plant has reached the highest level ofsophistication over the total spectrum of control activities.we are now beginning to see a new stage in the evolution of plant information and control architectures.over the last 20 years, progress in computer control has been spurred by acceptance across a widevision 2020: computational needs of the chemical industry81impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.spectrum of vendors of the distributed control hub system for process control, which was pioneered during the1970s by honeywell. a distributed control system (dcs) employs a hierarchy of computers, with a singlemicrocomputer controlling 8 to 16 individual control loops. more detailed calculations are performed usingworkstations, which receive information from the lowerlevel devices. set points, often determined by realtimeoptimization, are sent from the higher level to the lower level. with the focus now on enterprise integration,automation vendors are now implementing windows nt as the new solution for process control, utilizingpersonal computers in clientserver architectures rather than the hubcentric approach used for the past 20 years.this promotes an open application environment (open control systems) and makes accessible the wide variety ofpc objectoriented software tools (e.g., browsers) that are now available.the demand for smart field devices is rising rapidly. it is desirable to be able to query a remote instrumentand determine if the instrument is functioning properly. of course digitalbased rather than analog instrumentshave the key advantage that signals can be transmitted digitally (even by wireless) without the normal degradationexperienced with analog instruments. in addition, smart instruments have the ability to perform selfcalibration andfault detection/diagnosis. smart valves include proportionalintegralderivative (pid) control resident in theinstrument that can permit the central computers to do more advanced process control and informationmanagement. it is projected that installations of smart instruments can reduce instrumentation costs by up to 30percent over conventional approaches. there has been much recent activity in defining standards for the digital,multidrop (connection) communications protocol between sensors, actuators, and controllers. in the united states,the concept is called "fieldbus control," and vendors and users have been working together to develop and testinteroperability standards via several commercial implementations.when data become readily available at a central point, it will be easier to apply advanced advisory systems(e.g., expert systems) to monitor the plant for performance as well as detect and diagnose faults. recent effortshave built on the traditional single variable statistical process control approach and extended it to multivariableproblems (many process variables and sensors) using multivariate statistics and such tools as principal componentanalysis. these techniques can be used for sensor validation to determine if a given sensor has failed or exhibitsbias, drift, or lack of precision.in the area of process modeling, industrial groups are beginning to examine whether it is possible to achieve aseamless transition between models used for flowsheet design and simulation and models used for control. thecape open industrial consortium in europe and other groups in the united states are working toward an openarchitecture for commercial simulators to achieve "plug and play" using companyspecific software such asphysical property packages. the extension of these steadystate flowsheet simulators to handle dynamic cases isnow becoming an active area (e.g., linking aspenplus to speedup). the goal is to have models for realtimecontrol that run at 50 to 500 times realtime, but this will require increased computational efficiency and perhapsapplication of parallel computing.a new generation of modelbased control theory has emerged during the past decade that is tailored to thesuccessful operation of modern plants, addressing the "difficult" process characteristics encountered in chemicalplants shown in box 6.4. these advanced algorithms include model predictive control (mpc), robust control, andadaptive control, where a mathematical model is explicit in developing a control strategy. in mpc, control actionsare obtained from online optimization (usually by solving a quadratic program), which handles process variableconstraints. mpc also unifies treatment of load and setpoint changes via the use of disturbance models and thekalman filter. mpc can be extended to handle nonlinear models, as shown in figure 6.2.the success of mpc in solving large multivariable industrial control problems is impressive. modelvision 2020: computational needs of the chemical industry82impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.box 6.4 process characteristics that must be treated by advanced controltime delaysnonminimum phase disturbancesunmeasured variablesnoisetimevarying parametersnonlinearitiesconstraintsmultivariable interactionsfigure 6.2generalized block diagram for model predictive control.vision 2020: computational needs of the chemical industry83impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.predictive control of units with as many as 10 inputs and 10 outputs is already established in industrialpractice. computing power is not causing a critical bottleneck in process control, but larger mpc implementationsand faster sample rates will probably accompany faster computing. improved algorithms could easily have moreimpact than the improved hardware for the next several years. mpc will appear at the lowest level in the dcs,which will reduce the number of pid loops implemented.adaptive control implies that the controller parameters should be adapted in realtime to yield optimalperformance at all times; this is often done by comparing model predictions with online plant data and updatingthe process model parameters. the use of nonlinear models and controllers is under way in some applications.some of the new versions of mpc are incorporating model adaptation, but so far adaptive control has not hadmuch impact. this is due to problems in keeping such loops operational, largely because of the sensitivity ofmultivariable adaptive controllers to model mismatch.recent announcements by software vendors indicate that the combination of process simulation,optimization, and control into one software package will be a nearterm reality, i.e., a set of consistent modelsacross r&d, engineering, and production stages, with increased emphasis on rigorous dynamic models and thebest control solutions. software users will be able to optimize plantwide operations using realtime data andcurrent economic objectives. future software will determine the location and cause of operating problems andprovides a unified framework for data reconciliation and parameter estimation in real time.there are still many questions to be answered regarding the connection between modeling and control. thisincludes the explicit modeling information needed to achieve a particular level of control performance, thefundamental limitations on control performance even for perfect models, and the tradeoffs between modelingaccuracy, control performance, and stability.process measurement and control workshopin recognition of the needs and challenges in the areas of process measurement and control, a workshopentitled ﬁprocess measurement and control: industry needs" was convened in new orleans, march 68, 1998.2the goals of the workshop were as follows:1. to survey the current state of the art in academic research and industrial practice in the areas ofmeasurement and control, particularly as they apply to the chemical and processing industries. theextent of integration of measurements with control is a particular focus of the survey.2. to identify major impediments to further progress in the field and the adoption of these methods byindustry; and3. to determine highly promising new directions for methodological developments and applicationareas.the workshop emphasized future development and application in eight areas:3 characterization and separations, predictive control, performance monitoring,2 material from the workshop will appear in vol. 23, issue no. 2 (1999) of computers and chemical engineering.3 see <http://fourier.che.udel.edu/ÿdoyle/v2020/index.html> for further information on workshop findings.vision 2020: computational needs of the chemical industry84impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.instrumentation systems, and control and identification.as an example of a specific road map, the second topic (nonlinear model predictive control [nmpc]) hasbeen mainly of academic interest so far, with a few industrial applications involving neural networks. what isneeded is an analysis tool to determine the appropriate technology (nmpc vs. mpc) based on the processdescription, performance objective, and operating region. there is also a desire to represent complex physicalsystems so that they are more amenable to optimizationbased (and modelbased) control methods. the improvedmodeling paradigms should address model reduction techniques, loworder physical modeling approaches,maintenance of complex models, and how common model attributes contribute pathological features to thecorresponding optimization problem. hybrid modeling, which combines fundamental and empirical models, andmethodologies for development of nonlinear models (e.g., input sequence design, model structure selection,parameter adaptation) deserve attention. more details are contained in the web site for this workshop.chemical instrumentationchemical analysis is a critically important enabling technology essential to every phase of chemical science,product and process development, and manufacturing control. advances in chemical measurement over the pasttwo decades have greatly accelerated progress in chemical science, biotechnology, materials science, and processengineering. chemical measurements also play a key role in numerous related industries, such as pharmaceuticalsand pulp, paper, and food processing. during recent years, impressive advances have been made in the resolution,sensitivity, and specificity of chemical analysis. the conduct of analytical chemistry has been transformed byadvances in highfield superconducting magnets, multiplewavelength lasers, multiplexed array detectors, atomicforce microscopes, scanning spectral analysis, and the integration of computers with instrumentation. thesemethods have been extended to the detection and spectral characterization of molecular structure at the atomiclevel.a vision 2020 workshop was held in march, 1997, to assess future directions for r&d in chemicalinstrumentation. research needs identified included:4 transfer of analytical laboratory capabilities into plants, incorporating ease of maintenance and support,utilizing new technology and molecularscale devices;realtime characterization of polymers (molecular weight distribution, branching); improved structure/property/processing modeling capability, especially macromolecular products such asbiomolecules and biopolymers;hysical/chemical characterization of solids and slurries;characterization of biotechnological processes; approaches for sampling and system interlinks to control and information systems; and selfdiagnostic (smart) sensors; identification of processes needing microfabricated instruments and development of correspondingmodels/control systems;4 for more details see <http:www.nist.gov/cstl/hndocs/externaltechnologybundles.html>vision 2020: computational needs of the chemical industry85impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved. integration of data from multiple sensors for environmental compliance, product development, andprocess control, including soft sensors; andmeasurement techniques to support combinatorial chemistry in catalysis and drug discovery.process operationsthree of the four technology thrust areas of the vision 2020 documentšnamely, supply chain management,information systems, and manufacturing and operationsšaddress the business and manufacturing functions of thechemical enterprise. this clearly reflects the importance that efficient production and distribution of chemicalproducts have on the economic viability of the enterprise now and over the next 25 years. in this section, wehighlight the role that technical computing and information systems play as technology enablers for effectiveoperation and present the most important challenges and needs that must be addressed in the future. the discussionof research issues draws on ﬁr&d needs in systems technologies for process operations," a workshop that wasconvened in july 1998. for full details of the workshop report the reader is invited to consult the vision 2020 website.5in the present context, "process operations" refers to the management and use of human, capital, material,energy, and information resources to produce desired chemical products safely, flexibly, reliably, costeffectively,and responsibly for the environment and community. the traditional scope of operations encompasses the plantand its associated decision levels, as shown in figure 6.3.the key information sources for the plant operational decision hierarchy are the enterprise data, consisting ofcommercial and financial information, and the process itself. the unit management level includes the processcontrol, monitoring and diagnosis, and online data acquisition functions. the plantwide management level servesto coordinate the network of process units and to provide costeffective set points via realtime optimization. thescheduling decision layer addresses timevarying capacity and manpower utilization decisions, while the planninglevel sets production goals that meet supply and logistics constraints. ideally there is bidirectional communicationbetween levels, with higher levels setting goals for lower levels and the lower levels communicating constraintsand performance information to the higher levels. in practice the information flow tends to be topdown, invariablyresulting in mismatches between goals and their realization.in recent years this traditional view of operations has been expanded to include the interactions betweensuppliers, multiple plant sites, distribution sites and transportation networks, and customers. the planning andmanagement of this expanded network, referred to as the supply chain, pose challenging decision problemsbecause of the wide temporal scale and dynamics of the events that must be considered, the broad spatialdistribution and dimensions of the entities that must be managed, and the high degree of uncertainty due tochanging market factors and variable facilities uptimes and productivity. clearly the supply chain is a highlycomplex dynamic system. nonetheless, the vision proposed for the operational domain is that in 2020 the successof a chemical enterprise will depend upon how effectively it generates value by dynamically optimizing thedeployment of its supply chain resources. the seven factors critical to the achievement of the vision are: speed to marketštime from piloting to the market place; operation in terms of operational cost and asset utilization;5 see <http://www.chem.purdue.edu/v2020/>, the vision 2020 web site for workshop reports.vision 2020: computational needs of the chemical industry86impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.figure 6.3plant decision hierarchy.vironment, and safety: factors affecting workers and the community;anagement, engineering, and process and business literate operational staff; technology infrastructurešprocesses, instrumentation, and equipment as well as information systemsand technical computing; integrity: work process for producing the product right the first time; and integration: bidirectional linkage of all decision levels of the supply chain.to allow the vision of the dynamically optimized supply chain to be realized under each of these factors,innovations extending beyond developments in information and computing technology alone are required.however, it is clear that the infrastructure for storing and sharing information and technical computing tools thatexploit such information constitute the key enabling technology. the information that must be stored and sharedincludes transactional information, resources costs and availabilities, plant status information, models, and modelsolutions. this diversity of information types must be effectively organized and must be sharable using reliablehighspeed networks. the enabling technical computing components include modelbuilding methods and tools;solution algorithms using numerical, symbolic, and logicbased methods; visualization and interpretationmethods; interfaces for use and training; and integration of all of these components into usable decision supporttools.present statusat present, the essential elements of information technology to support operations are at hand, in terms ofboth data infrastructure and network connectivity. commercial database management systems and transactionalsystems are common in the industry. plant information systems and historians are in widespread use, andenterprisewide database system installations are growing explosively. unix or windows ntbased networks arecommon, and internet and webbased applications are growing rapidly. despite this growth there is only limitedintegration of business and manufacturing data andvision 2020: computational needs of the chemical industry87impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.tools to facilitate effective use of this data. indeed, the general consensus is that corporations are drowning in a seaof data. the challenge is to extract information and knowledge and thus to derive value from this data.the present status of technical computing of relevance to process operations can best be characterized as apatchwork of areas at different levels of development. at the planning level, multitimeperiod linear programmingtools capable of handling largescale systems are well developed and have been in use, especially in thepetroleum/petrochemical sector, since the 1970s. realtime, plantwide optimization applications using steadystate process models are growing rapidly in the petrochemical domain, although some of the statistical andcomputational formulations and algorithms remain under active development. the methodology for scheduling ofmultipurpose batch and continuous production facilities has been under investigation since the late 1970s, initiallyusing rulebased and heuristic randomized search methods and more recently using optimizationbased (mixedinteger linear programming) methods. application of the latter in industry is limited but growing. successfulsolutions of problems involving more than one hundred equipment items and several hundred distinct productiontasks have been reported, although the deployment of the technology still requires high levels of expertise andeffort. as noted in the section above titled "process control and instrumentation," tools for abnormal situationmanagement are in their infancy, although significant industryled developments are in progress. linear modelpredictive control has been practiced in the field since the early 1980s, although the theoretical supports for themethodology were developed later. plant data rectification has been practiced since the mid1980s, but typicallyapplications have been confined to linear models and simple statistical descriptions of the errors in themeasurements.challengesthe longterm challenges for the application of computing technology can be divided into four major areas: of data into knowledge,tools for the process,tools for the business, andthe development of tools that would facilitate conversion of the extensive data contained in enterpriseinformation systems into actionable information and ultimately knowledge is of highest priority. some of thecapabilities that need to be pursued include softsensors, data rectification techniques, trend analysis andmonitoring methods, and data visualization techniques. softsensors are critical to simplifying the detection oferroneous measurements by localizing the detection logic. data rectification refers to the process of condensingand correcting redundant and inaccurate or erroneous process data so as to obtain the most likely status of theplant. trend analysis and monitoring refers to the process of using process knowledge and models to identify andcharacterize process trends so as to provide timely predictions of when and what corrective action needs to betaken. data visualization is an essential element for facilitating understanding of process behavior and tendencies.the decision support tools for the process include streamlined modeling methodology, multiview systemsfor abnormal situation management, nonlinear and adaptive model predictive control, and process optimizationusing dynamic, and especially hybrid, models. model building is generally perceived to be a key stumbling blockbecause of the level of expertise required both to formulate processvision 2020: computational needs of the chemical industry88impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.models and to implement them using contemporary tools. the goal is to make model building and managementrapid and reliable and to create environments in which the models associated with the various levels of theoperational decision hierarchy will be consistent and unified. the role of abnormal situation management systemsis to identify plant trends, to diagnose likely causes and consequences, and to provide intelligent advice to plantpersonnel. while components that address portions of this entire process have been under investigation for thepast decade, full integration of the various qualitative and quantitative support tools remains to be realized. neededdevelopments in process control have been discussed in an earlier session and hence will not be reiterated here,except to note that control of batch and other intentionally dynamic processes needs to be given considerably moreattention. finally, the optimization of models consisting of differential algebraic systems, and especiallydifferential algebraic systems with discrete elements, is essential to the realization of the vision for processoperations. the latter type of socalled hybrid systems is particularly relevant to processes that involve batch andsemicontinuous operations.the overall goal of these decision support methodologies for the process is to realize the integrated modelcentered paradigm for process operation shown in figure 6.4. under this paradigm all of thefigure 6.4integrated modelcentered operation.vision 2020: computational needs of the chemical industry89impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.decision levels of the operational hierarchy are fully integrated through the shared use of consistent, robustmodels. models serve as the central repository of process knowledge. information flows from the lower levels tothe higher levels to ensure that decisions fully consistent with the status and capacity of the production resourcesare made.the third area of need is in the development of tools to support the overall business decision processes. theobjective is to expand the envelope beyond the process itself and to encompass the business processes that areessential to driving manufacturing and the entire supply chain. the tools include improved sales and marketforecasting methodologies, supply and logistics planning techniques, methodologies for quantitative riskassessment, optimizationbased plant scheduling methods, business modeling frameworks, and approaches todynamic supply chain optimization. optimizationbased scheduling requires the solution of very highdimensionality models expressed in terms of discrete 01 variables. the key need is to be able to solve schedulingproblems with hundreds of thousands of such variables reliably and quickly. such capabilities need to be extendedto allow treatment of models that encompass the entire supply chain and to quantitatively address business issuessuch as resource and capital planning associated with the supply chain, siting of new products, and the impact ofmergers and acquisition on the supply chain.finally, in order to realize the benefits of the developments in the other three areas, it is necessary, indeedessential, to create training methodologies for the work force. these computerbased training methodologies mustmake efficient use of students' time, recognize differences in levels of expertise, and employ extensivevisualization tools, including virtual reality components. methods must also be developed to aid process staff inthe understanding of models and the meaning of the solutions resulting from the various decision support toolsthat are based on these models. such understanding is critical both to the initial adoption of such models and to thecontinuous improvement process, as it is only from understanding the constraints of the existing operation andtheir implications that costeffective improvements can be systematically generated.in conclusion, the processoperationsspecific information systems and technical computing developmentsoutlined above are essential to the realization of the goal of the dynamically optimized supply chain. continuingincreases in computing power, network bandwidth, and availability of faster and cheaper memory will no doubtfacilitate achievement of this goal. however, the scope and complexity of the underlying decision problemsrequire methodological developments that offer effective gains orders of magnitude beyond the likely increases inraw computing power and communication bandwidth. processoriented technical computing really does play thepivotal role in the future of process operations.vision 2020: computational needs of the chemical industry90impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.session 2panel discussionrobert lichter, camille & henry dreyfus foundation: during this joint presentation, all of you in oneway or another talked about the way in which people will be an integral part of the whole revolution by 2020. iwould like to hear your comments on how we will get there. that is, what are the implications for the education ofthe people who would be part of that revolution that would presumably begin now?david dixon: i will try and make a stab at it, although i am not in the formal education business these days.much of what we are going to see will actually be onthejob training. when i was at dupont, for example, we didnot see the broad training you need to actually solve some of these problems in new staff coming directly out ofuniversities.in the future, we are going to be seeing teams of people working together. we are going to have to changefundamentally the concept of how individual research is done. if we want to solve complex software issues andthings like that, we need to look at team approaches.we have been developing very complex parallel code at pnnl, and the only way we have been able to do itis by putting teams of applied mathematicians, computer scientists, and chemistry domain specialists and userstogether to solve the problem. fundamentally, academics will have to change to allow teaming to happen and forthat to be a profitable part of the university curriculum.david smith, dupont: can i just try to add to that incrementally? the issue seems to me that the universitymust and will continue to rely on the individual contribution in the thesis area, and that is an essential part ofgetting your ph.d. on the other hand, when you come to work at a company like dupont, and i am sure, dow,teamwork is important, and i think that most of the exciting work that is going on in our corporation today is notso much in any one particular field, but in the intersection between fields where interesting things are happening.so, the ability to have people from different disciplines work together and to be able to understand one another'slanguage is really an important part of the process, and it takes time, and that time comes from dupont's time. i donot think that there is any way the university can give that to us.vision 2020: computational needs of the chemical industry91impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.gintaris reklaitis: i would like to add to that point from the perspective of the software tools. the questionthat i have is, how do you teach people to use the computing tools that are getting increasingly more complex? ireally think we need a lot of new ideas in this area. i am struck by the fact that we have tools like processsimulators that are very time consuming to master. if you look at the user's manual for complex software tools, thefirst thing the user is asked to do is to spend 2 hours going through all the menus and clicking on all of thecommands. have you tried this with a group of undergraduates? it lasts about 5 minutes. this is absolutely not theway anyone wants to learn to use a software tool. we have to come up with intelligent ways around this trainingproblem.specifically, i think that there is a lot of creative work that we need to do with our education colleagues todevise new models for learning and intelligently using complex software tools that have many options andpossibilities. this is particularly relevant for the casual user, which is what most industrial users are. mostengineers are focused on projects rather than tools and will only revisit the tools periodically as the need arises.such a practitioner surely does not want and does not have the time to reinvestigate all of the menus every time torecall what is available.evelyn goldfield, wayne state university: i want to follow up on the previous question because i thinkthat a lot of universities, including my own, are rethinking some of the ways that we train graduate students andreally do want to focus on some of these interdisciplinary team approaches, particularly in computational science.we found that there were a lot of different people at the university who should be and could be training togetherand working together but until recently were almost totally isolated. there tends to be duplication of effort, ofreinventing the wheel. also, too many students are not taking certain courses that would benefit them becausethese courses are offered in a different department or a different part of the university, whereas they are all usingbasically the same algorithms. and so, i wondered if you had any comments for how that these sorts of team andinterdisciplinary approaches at the university would have any beneficial effects on your projects?gregory mcrae, massachusetts institute of technology: i would like to address that question from an mitperspective because, in fact, just last week a whole new division has been formed in the engineering school calledthe engineering systems division, which is specifically directed at dealing with the issues that you talked about.it is across disciplines. it involves not only interaction with the engineering school but also with the socialsciences as well as with the management sciences.it is by no stretch of the imagination an easy thing to do in a university, but there are people with vision tobasically say this has to be done and they are putting the faculty slots on the table to actually make it happen. so, ithink that there are some universities quite committed to doing that.david smith: i would just like to comment that the design center at carnegie mellon university had exactlythe same strategy and was very, very effective for those of us who participated in it. it was a very goodexperience.judith hempel, university of california, san francisco: i was just going to ask you, the panel, what yousee for the year 2020 in the sort of division that we currently see in the chemistry modeling area between whatsome people call materials research and on the other side, biological materials research. many of the techniquesare very similar when you go all the way from pharmaceutical modeling over to materials modeling. in 2020 willthere be a division, do you think, of this kind? or will it come together?: in principle, one would hope that they would come together. i would not guarantee it atvision 2020: computational needs of the chemical industry92david dixonimpact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.this point, and the reason is that it is not clear to me yet that the potential function needs for both are going to bethe same. i think you will have similarities, but right now if you look at industry, for example, i would say that inthe use of computational chemistry throughout industry, including chemistry and pharmaceuticals, you probablyhave 80 percent of the people in the pharmaceutical/biological and about 20 percent in the materials/chemicalside. and i think it is going to depend on a lot of features. one would hope that they would come together, but onedoes not know that yet.stanley sandler, university of delaware: the one thing i was surprised to see on your list was thecomment for the year 2020 of the need for efficient methods for generating accurate potentials for molecularsimulation. intermolecular potentials generally imply twobody effects, not considering pairwise nonadditivity.wouldn't you think by 2020 we would have done away with that completely, and be using quantum mechanics andsimulation together to calculate the total energy functions, and not calculating individual twobody potentials?david dixon: one would hope that we would be able to do the quantum mechanics then, but, as peter taylormentioned this morning, it is extremely expensive to get those very small energies correctly. it is not clear whenwe will be able to carry out the kind of simulations that peter cummings is doing in order to obtainthermophysical properties that will not require potential functions.the potential function issue has come up at about three or four workshops we have been at over the last yearas being one of the key foci for the next 10 years, if we are going to make headway in predicting thermophysicalproperties.stanley sandler: how do you take into account nonpairwise additivity or non twobody effects?david dixon: that is part of generating the potential functions. do we put in polarization potentials, and howdo we put in threebody terms? there is a whole branch of science in how to do this correctly. people are nowputting in polarization potentials so you can actually treat the electrostatics of a molecule as it interacts with othermolecules and solve for the electric field changing over time with the dynamics. we need to worry about threebody effects, fourbody, and up to nbody effects. these are areas that have to be studied in order for the field toprogress.christos georgakis, lehigh university: besides the industrial need for people who have been trained inseveral disciplines, maybe one other issue that we need to discuss is the type of academic degree(s) these peopleshould have: b.sc., m.s., or ph.d.? more specifically, do we need more m.s. graduates than ph.d.s?david smith: i will make an attempt at answering that. the first thing i would like to comment on is thatchemical plants have a very long lifetime. they are on the ground for anywhere from 25 to 50 years. no matterwhat we do today in research, those plants are still going to be making adipic acid well into the next century, so westill need students trained in traditional chemical engineering who are going to go out and run those plants andmanage our businesses in the traditional way. we just cannot walk away from that responsibility. there are toomany billions of dollars of equipment on the ground to think that we should change the way we have been doingchemical engineering because in the next 10 years we are going to be building biological processes to replacethem. this is just unrealistic.now, because of where we believe the growth to be, there is a trend in dupont toward hiring more in thebiological sciences, but that does not mean that we are going to stop hiring ph.d.s, well trained invision 2020: computational needs of the chemical industry93impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.chemical engineering and other disciplines, especially chemistry, to do our work. so i do not see dupont r&dshifting away from hiring ph.d.s.i see us perhaps not hiring quite as many as we did 10 years ago, but there is another complicating factorhere, and that is the demographics. if you walk into the dupont cafeteria at lunchtime and take a look at the peoplethere, there are a lot of chronologically gifted people present. we face a problem in terms of maintaining ourtechnical capability in the next decade as these people retire. we have had a hard time getting through to our hrpeople that they cannot solve that problem in one year. there are not enough highquality ph.d.s in chemicalengineering graduating in any one year for us to replace the people we are going to lose at that particular time.there is a fundamental problem but i am not sure that it is being addressed in the best possible way.tom edgar: we talked a little bit about what the factory of the future is going to look like and the sort oftechnical demands that are going to be placed on people who are there. there is also this ongoing pressure toreduce the number of personnel in chemical plants as they become more automated.what is the operator of the future going to look like? is that person going to need a b.s. in chemicalengineering rather than a community college degree? someone in an editorial recently likened that person toessentially having the same responsibility as an airplane pilot in terms of the financial implications as well aspossible safety implications. so, it is, again, something to think about. the differential costs between an operatorand a b.s. chemical engineer are not all that great.david smith: i would like to say that the salary for a senior operator it is not that much different from thestarting salary for a ph.d. i had a young ph.d. go to the plant and discover what the lead operator was making, andhe came back absolutely incensed and wanted to know why i wasn't paying him more. i simply asked him, ﬁcanyou run the plant as well as he can?" the answer, of course, was no.gregory mcrae, massachusetts institute of technology: there is also another dimension of your questionthat i do not think we should ignore, especially if we are looking at 2020. the training that students have inchemical engineering makes them very attractive to a lot of people other than those who make chemicals. in atypical graduating class at mit, not many of those kids finish up as chemical engineers because of the tremendoussalary offers they are getting from many other disciplines. so, it is not only the question of how you can feedpeople into the existing industry to deal with the problem that dave is describing, it is also how to make theindustry itself attractive to these people, because they are able to go to many other different places.for example, the last three of my ph.d. students have gone to wall street, and for one of them, his firstyearbonus is more than i will make in my lifetime.christos georgakis: let me retry a more focused question. fathers and mothers see the incentive to pay for ab.s. education. university professors who have to do research see the incentive to educate ph.d. students. whereis the financial incentive to educate m.s. students, which, i personally believe, will be in much larger demand?these m.s. graduates are needed to operate modeldirected plants, or to utilize and apply the sophisticatedcomputer technologies that the 2020 framework envisions.david smith: christos, i am not sure whether it is a chicken and egg problem. today if i wanted to, i couldnot go out and hire master's degree candidates in the numbers that we might be interested in hiring. so if they wereavailable and we had experience dealing with that situation i might be able to give you a meaningful answer. inpoint of fact, i think that we are going to hire more b.s. engineers tovision 2020: computational needs of the chemical industry94impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.run our plants. i do not see us requiring more master's degrees. i think a lot of the mining that we want is going tobe onthejob training because of the increasing interdisciplinary nature of the work we will be doing in the future.it is too difficult to target that. i just think we are going to rely on more onthejob training.gintaris reklaitis: one might ask whether industry will continue to be wanting to invest in onthejobtraining because at universities, we hear from our industrial colleagues that they want graduates ready to hit theground running. they do not want to invest in training or in career development.david smith: oh wow. i do not work at one of those places, i am happy to say.sam kounaves, tufts university: parents pay for b.s. degrees and faculty pay for the ph.d. degrees. at ouruniversity nobody wants to pay for m.s. degrees because no master's student is willing to encumber another$20,000 tuition bill on top of the bill for the b.s. degree, and we cannot support our master's students because theyare not there long enough to do any research for us.at least that is my perspective on the problem. the other question i have is more general. greg talked about abalanced approach to doing modeling, both in terms of software and hardware, and i was just curious about youropinion on the allocation of resources for supporting computational chemistry. at first it sounded like we need allthe highpowered hardware to do advanced modeling in all branches of chemistry and then later it sounded like theproblem is actually in the software. i am curious about what your opinions are in terms of the resources. are we atthe point now where the hardware is really all right except for specific cases and that a larger effort needs to beput into funding programming and algorithms? i think people would rather put funding into hardware in thecomputer sciences, but software has been relegated down to the bottom. but that is exactly where the resourcesneed to be put in order to address some of the problems, like interfacing and operating systems andinteroperability.david dixon: actually, i do not think that there has been a decision actually to put all the money only intohardware. i think one needs to continue to have hardware that is going to give us new ways of solving larger andlarger problems, but you have to have a balance between the software and the hardware. i think asci and thestrategic simulation plan are trying to be balanced on both. i think the point that i was trying to make earlier isthat the operating system part of the hardware has been the weakest part for us in terms of making it available to abroad range of users. we are significantly investing in all of the software pieces, the algorithm development andthe theory, and we are trying to have a balanced approach. i think everybody has been trying to do that.paul messina, california institute of technology and department of energy: but, yes, in terms of theinvestments, even the asci program, which is known for buying big machines, is spending less than half of itsmoney on hardware. the rest of it is on developing the software and the applications. and your comment aboutcomputer scientists wanting the hardware, i am afraid, is wrong, because computer scientists typically do not wantany hardware.tom edgar: there is one other aspect of computational speeds that i would like to clarify. in realtimeprocess control, there is an imperative to try to generate an answer within a sampling time or within a timeconstant of a process. that is a little bit different than for a simulation environment that is offline. so thereactually is a fair amount of pressure to have hardware that is really fast. but, of course, having said that, with thedoubling of processor speeds every 18 months according to moore's law, you canvision 2020: computational needs of the chemical industry95impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.always figure that with a good algorithm you can just wait 18 or 36 months and the computers will be fast enoughso you can use your algorithm then.gintaris reklaitis: from the perspective of combinatorial problems arising in supplychain management andscheduling, doubling of computer time every 18 months will not suffice. these are problems for whichcomputational effort grows exponentially with problem size, at least in the worst case. for the kinds ofincreasingly larger applications that people want to solve in these domains, waiting for the hardware to becomefaster is not the solution. the improvements must be found through algorithm research.david smith: i do not know. i used to have a group that did supply chain optimization, and in thereorganization that group went elsewhere. i would say that for a lot of very good sized, really significant supplychain problems that we were tackling, we were getting solutions in a couple of hours. the problem that we ran intowas that, for reasons i could not understand, the people who wanted the answers were upset because they had towait 2 hours for them; they did not realize that the business time scale that were dealing with probably involveddays or weeks. these guys are used to running excel spreadsheets in minutes, and the fact that they had posed aproblem to the computer and had to wait for 2 hours for an answer was a very difficult cultural thing for them todeal with.in general, business people are not trained in optimization, and they are usually very defensive when we bringthose kinds of solutions to them. so it is really an educational problem that we have internally.judith hempel: are they senior staff of long standing?david smith: no. neither were the people who were solving those problems for me. the young people insupport positions in dupont supplying that kind of information to business managers do not have the right kind oftraining and background to solve those problems. solving the problem for them turns out to be an iterative processbecause they do not really understand how much information they have to give us so we can give them a goodsolution to their business problem.vision 2020: computational needs of the chemical industry96impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.7collaboratory life: challenges of internetmediated science forchemiststhomas a. finholtuniversity of michiganabstractsince the birth of modern chemistry in the early 19th century there has been tremendous growth in theknowledge and the practical application of chemical principles. however, in many important ways, the practice ofchemistry research and teaching has remained unchanged. the advent of the internet as a worldwide mechanismfor conducting scientific communication challenges this status quo. specifically, innovations like collaboratories,or networkbased virtual laboratories, remove constraints of distance and time on scientific collaboration. inparticular, collaboratories increase access to scarce instruments, accelerate the flow of information, and place newdemands on senior scientists to mentor students. chemists need to appreciate how these new ways of doingscientific work will influence the conduct of chemistry research so that they can effectively anticipate andinfluence the development of emerging interact technologies.collaboratory life: challenges of internetmediated science forchemiststhe internet,1 the world wide web,2 and sophisticated collaboration technologies3 represent the rawingredients for a revolution in the practice of chemistry. yet today, for many chemists, this revolution is onlypartially realized or has not begun. as a result, the field of chemistry rests squarely1 see hafner, k., and lyon, m. (1996). where wizards stay up late: the origins of the internet. new york: simon &schuster; hauben, m., and hauben, r. (1997). netizens: on the history and impact of usenet and the internet. los alamitos,ca: ieee computer society press; and neil, r. (1997). the soul of the internet: net gods, netizens and the wiring of theworld. boston: thomson computer press.2 schatz, b.r., and hardin, j.b. (1994). ncsa mosaic and the world wide web: global hypermedia protocols for theinteract. science, 265, 895901.3 olson, g.m., and olson, j.s. (1997). research on computer supported cooperative work. in m.g. helander, t.k.landauer, and p.v. prabhu (eds.), handbook of humancomputer interaction, second edition (pp. 14331456). new york:elsevier.collaboratory life: challenges of internetmediated science for chemists97impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.on techniques and methods, many of which are over a century old. that is, while the content of chemicalknowledge has advanced dramatically in the last 200 years, the organization of chemical research and educationhas remained relatively constant. by contrast, other disciplines race to embrace change, such as physicists'invention and rapid adoption of the world wide web and the widespread use of the web for data disseminationamong biomedical scientists. an apt metaphor to describe the challenge of the internet for chemistry is paulgauguin's masterpiece (figure 7.1), where do we come from? what are we? where are we going?in the title of his painting, gauguin evokes the fear and uncertainty that accompany the transition from thepast (where do we come from? ), through the present (what are we?), and into the unknowable future (where aregoing?). the style and content of the painting also underline gauguin's personal status as a bridging figurebetween impressionism and modernist schools, such as cubism and fauvism. while gauguin was captivated by theimpressionists early in his career, and worked and showed with them, later in his career he broke away and defined anew kind of art, often labeled postimpressionism. in this later work, gauguin experimented with the use of colorand symbolism in a way that paved the way for those who followed, including matisse, picasso, and munch.therefore, at many levels, this painting represents the tension of being caught between familiar traditions and thebirth of new ways.chemists confront a similar tension between tried and true practices from the past and unknown alternativepractices made possible through advances in information technology. in this sense where do we come from? is aquestion about the traditions and conventions that have defined chemistry, especially with regard to theorganization of research and education. the question what are we? offers an opportunity to reflect on the presentstate of the internet, while the question where are we going? forces consideration of the various new paths thatinteractmediated chemistry might follow into the future. the ﬁgauguin problem," then, is a statement about thedifficulty any community faces when past and current success precludes full examination or experimentation withpotentially transformational practices and approaches. in chemistry, the gauguin problem can be framed as theenduring legacy from innovation at the dawn of modern chemistry in the late 18th and early 19th centuries, themixing of inherited tradition with capabilities provided by the internet that is occurring today, and alternativeviews of the future defined by new uses of the internet.figure 7.1where do we come from? what are we? where are we going? paul gauguin, 1897. tompkins collection.courtesy, museum of fine arts, boston.collaboratory life: challenges of internetmediated science for chemists98impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.where do we come from?as an emergent discipline in the late 18th and early 19th centuries, chemistry had little received tradition interms of either how to conduct chemical research or how to teach about chemical relationships. therefore, the first30 years of the 19th century saw development of many of the research and pedagogical practices that are still inuse today. three particularly important innovations were the creation and elaboration of the research laboratory,the use of laboratory classes in chemical education, and the use of lecture demonstrations to illuminate and clarifychemical principles.the roots of the modern research laboratory, that is, a physical concentration of personnel and apparatusdedicated to systematic chemical research, can be found in humphry davy's laboratory at the royal institution atthe turn of the 19th century. under the patronage of count rumford, the founder of the royal institution, davysimultaneously mastered the arts of building voltaic devices for the discovery of new elements as well as raisingthe funds to construct new devices.4 the impact of davy' s efforts in terms of new knowledge is obvious in termsof his identification of sodium, potassium, and so forth. nearly as important, however, is the model that davy's labestablished for subsequent scientists. that is, the halls of the royal institution defined not only a physical spacethat housed critical instruments but also a social organization that produced tradition, fostered networks, andbecame a place to train future generations of researchers. indeed, among davy's greatest contributions was hismentorship of michael faraday. the broader success of the royal institution as a research organization isrepresented in the work of the institution's nine nobel laureates. davy's approach carries through to the presentlargely unchanged. that is, the imperatives that drive modern research laboratoriesšhiring good people, installingstateoftheart equipment, and getting fundingšdiffer in magnitude and sophistication, but not in fundamentalcharacter, when compared to davy's day. in fact, it seems likely that were davy to travel forward in time to a labat michigan, or dupont, or cambridge, he might be amazed at the focus of research and the instruments in use,but the organization of scientists and resources to conduct the research would be entirely familiar.a second great innovation from chemistry's founding era was the invention of the laboratory class. theintroduction of laboratory classes is associated with justus yon liebig and the organic chemistry curriculum hedeveloped at the university of giessen, beginning in 1824. prior to liebig, chemistry was taught largely throughexample and demonstration and not through active participation at the lab bench. while the demonstrationapproach was developed to a high art, it did not produce many chemists, since few students gained handsonexperience in manipulating chemicals.5 by contrast, liebig's "practical" approach immersed students in exercisesthat were reasonable analogs to techniques used by practicing chemists. therefore, students in liebig's lab gainednot only experience but also a sense of the thrill and challenge of conducting research. pictures of liebig'slaboratory drawn in 1842 show a scene not too different from a modern undergraduate chemistry lab. studentsclustered around lab benches produce and analyze specified compounds, guided by an instructor or lab supervisor.again, as with davy, we could bring liebig forward in time and most of what he would observe about theorganization of modern lab courses would be similar to his own era.a final innovation was the public lecture and accompanying demonstrations. this practice origi4 fullmer, j. (1989). humphry davy: fund raiser. in f.a.j.l. james (ed.), the development of the laboratory: essays onthe place of experiment in industrial civilization (pp. 1121). london: macmillan press.5 fenby, d.v. (1989). the lectureship in chemistry and the chemical laboratory, university of glasgow, 17471818. inf.a.j.l. james (ed.), the development of the laboratory: essays on the place of experiment in industrial civilization (pp.2233). london: macmillan press.collaboratory life: challenges of internetmediated science for chemists99impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.nated with faraday's christmas lectures at the royal institution, which began in 1826. the christmas lectures havecontinued to the present and are now broadcast via television and available via download from the web. theessence of this tradition is for prominent scientists to communicate the excitement and value of their research to alay audience, particularly young people. the main mechanism used in the christmas lecture is explanationaccompanied by interesting demonstrations. this pedagogical approach is not just the foundation of the christmaslecture series; it is also the continuing basis for most secondary and undergraduate instruction in chemistry.indeed, most large university chemistry lecture halls have an accompanying room where special equipment, justfor doing demonstrations, is prepared under the guidance of a demonstrations supervisor. therefore, as in the twopreceding examples, there is not much about contemporary lectures that would be surprising or strange to someonefrom faraday's time.these three innovations do not represent a comprehensive treatment of the historical tradition in chemistry.however, they do signify important cornerstones of past and present practice in chemical research and education.more important, the legacy of the research laboratory, the laboratory course, and the public lecture defines thestarting place for thinking about the organization of alternative approaches. specifically, with the advent of theinternet, the web, and collaboration tools, chemists confront a choice between extrapolation from a known andsuccessful past (i.e., joining the capabilities of these new technologies to familiar practices) and exploration ofentirely new ways of doing and teaching chemistry.what are we?the previous section examines chemistry's past and how that past determines the present. this sectionconsiders the present, particularly the new opportunities available to chemists through the expansion of computerand internet technologies. these opportunities can be thought of in terms of the raw performance of computerprocessors, the capacity of communication networks, the scope of networks, and the evolution of software.the engine of progress in computing is moore's law, or the observation by former intel ceo gordon moorethat the performance of computer processor chipsšwhen measured as the number of transistors per processoršdoubles roughly every 2 years. figure 7.2 illustrates the progress of processor development over the last 26 years,using intel chips as a benchmark. a corollary of moore's law says that for a constant price, a computer purchasergets twice as much power every 2 years. either way, this is a trend toward phenomenal increases in computingcapability over time. for instance, it is often observed that current pentium 2 workstations are roughly comparablein speed to supercomputers sold in the early 1980s.recent explosive growth in the size of the internet points to a new metric of computing performance: networkbandwidth. contemporary performance, or lack of performance, within the internet is legendaryšhence thepopular observation that www stands for "world wide wait." examining plans for installation of highcapacityfiber across both the pacific and the atlantic oceans, however, suggests that current network delays may soondisappear. for example, capacity across the pacific is slated to increase to 300 gigabits per second (gbps) by theyear 2000 from a 1998 level of 25 gbps, while capacity across the atlantic will increase to 250 gbps from 110gbps.6 for comparison purposes, i6 staple, g. (1998). the global bandwidth boom: something's happening here . . . but what? bandwidth economyconference, columbia business school. may 1, 1998.collaboratory life: challenges of internetmediated science for chemists100impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.gbps is equivalent to 70,000 phone calls. estimates are that the expansion of international bandwidth will easilymeet projected growth in voice traffic and that the bulk of increased use will be data traffic. this means that manyapplications that impose prohibitive bandwidth overhead today (such as desktop video conferencing) may, in thenear future; become more practical. plans are already launched in the united states for nextgeneration networktechnologies that will exploit increasing bandwidth, such as the university consortium for advanced internetdevelopment (ucaid; <http://www.ucaid.org>). ucaid hopes to deliver network performance of 150 megabitsper second, which in most cases will represent dramatic improvement in network throughput. this could mean, forinstance, easy transfer of large data sets around the internet, routine use of bandwidthintensive applications (suchas audio or video), and increased use of applications that require high quality of service (such as remotemanipulation of instruments).figure 7.2illustration of moore's law using intel cpu chips. courtesy of george watson, university of delaware, <http://www.physics.udel.edu/ÿwatson/scen103/intel.html>.a third hallmark of change in the internet is the tremendous expansion of hosts and the worldwide penetrationof the internet. recent examination of connected countries shows host domains for all but three nations, with eventhe tiny island nations of nauru and comoros linked to the global network.7 this scope means that, to anunprecedented extent, scientists with access to the internet can reasonably expect to communicatešand possiblycollaboratešwith colleagues located anywhere on the face of the earth. similarly, through the internet, scarceresources, such as libraries and rare instruments, can be made available to larger populations of users.the changes noted above are momentous, but each represents more potential progress than realized progress,at least in terms of practice and behavior. this may be because unprecedented increases in7 kaiser, jocelyn, ed. (1999). netwatch. science, 283, 295.collaboratory life: challenges of internetmediated science for chemists101impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.computing power, network bandwidth, and network scope have not been matched by corresponding improvementin the usability of applications and software. figure 7.3 represents this trend. the yaxis indicates raw performanceof computing technology, such as the benchmarks listed above (processor power, bandwidth capacity). the xaxisindicates time. the upward slope of each curve shows overall improvement. however, the contrast between thecurve labeled "raw performance" and the curve labeled "real performance" reflects the difference between what wecould do with information technology (shown at the extreme left with the ﬁhype" curve) and what we can actuallydo. this difference, called the "reality gap," is in part why scientists may be reluctant to launch or adopt boldinformation technology innovations. for instance, an oftcited reason for staying with a specific platform orapplication is the cost of learning a new program. some have argued that this essential difficulty is the root of theapparent productivity paradox in computing, where massive investment in computing technology has often failedto produce significant increases in output or performance.8 a way out of this bind might be broader application ofusercentered design philosophies that, in contrast to traditional development approaches, attempt to evolveapplications with constant feedback drawn from users in authentic settings.figure 7.3the "reality gap" in information technology performance. courtesy of dan atkins, university of michigan.in summary, the present is a time of fantastic change in the raw capabilities of information and networktechnologies. however, the impact of these changes is somewhat reduced by the difficulty of effectivelyharnessing the potential of new technologies. for instance, producing usable software is still more difficult than itshould be, and many software applications don't produce benefits to justify the often painful process of learning touse them. a particular challenge for chemists will be finding ways to train the next generation of chemicalsoftware designers to more effectively design code, such that broad populations of users can adopt applicationsquickly and easilyštherefore more fully tapping the rich potential of advances in hardware and network systems.8 landauer, t.k. (1995). the trouble with computers. cambridge, ma: mit press.collaboratory life: challenges of internetmediated science for chemists102impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.where are we going?the collaboratory conceptone way to think about the future is to seek examples in the present of possible new modes of doing researchand teaching. a dream of internet proponents has been the creation of collaboratories, or centers without walls ". . .in which the nation's researchers can perform their research without regard to geographical locationšinteractingwith colleagues, accessing instrumentation, sharing data and computational resources, [and] accessing informationin digital libraries."9 the earliest and most extensive collaboratory r&d project is the space physics andaeronomy research collaboratory (sparc). this project began in late 1992, and by early 1993 produced theworld's first operational collaboratory.10the sparc project started by addressing the needs of space physicists who used observational data collectedfrom a suite of groundbased instruments at the sondrestrom upper atmospheric research facility, located ingreenland. between 1992 and 1995, sparc evolved to provide data viewers for five sondrestrom instruments:(1) the 60meter incoherent scatter radar, (2) an allsky camera, (3) a fabryperot interferometer, (4) an imagingriometer, and (5) a local magnetometer. during this period, the primary use of the collaboratory involved realtimeaccess to these instruments either for ionospheric observations or for instrument testing. at this early stage,collaboratorybased science resembled traditional research practices, although mediated by the internet.between 1995 and 1997 sparc transformed dramatically to accommodate three major changes. first, earlysuccess with the collaboratory led to increased interest from scientists and to demands to include moreinstruments. second, the rapid emergence and adoption of the web suggested the importance of a webbasedinterface to sparc. to accommodate this need, the core technology of sparc was rebuilt in java. third, havingseen what might be possible with the early sparc system, users proposed three new types of uses: (1) expansionof the data sources to produce a global "field of view" in real time; (2) inclusion in real time of theoretical modeloutput side by side with observational data; and (3) use of the sparc technology to support distributed, onlineworkshops or conferences.figure 7.4 shows a snapshot of the sparc interface during a recent campaign. the sparc interface hasthree main components. first, the sparc "session manager," shown in the upper left of figure 7.4, organizesscientific activity by topic into groups called "rooms."11 within these rooms, scientists find useful urls, chatstreams specific to that room, and saved configurations for data viewers relevant to that room. note that each roomname is followed by a number in parentheses, which represents the number of scientists currently using that room,and that the names of participants within a selected room are displayed below the session manager. thisinformation provides a crucial form of presence awareness in the virtual setting that would be obtainedautomatically in a shared physical setting. the chat window, shown in the lower left of figure 7.4, is a textbasedchannel for communication among sparc users. this chat application is persistent, meaning that scientists canjoin a conversation in progress and scroll back to review earlier comments. finally, the bulk of the interface9 national research council, computer science and telecommunications board (1993). national collaboratories: applyinginformation technology for scientific research. washington, dc: national academy press.10 finholt, t.a., and olson, g.m. (1997). from laboratories to collaboratories: a new organizational form for scientificcollaboration. psychological science, 8, 2836.11 lee, j.h., prakash, a., jaeger, t., and wu, g. (1996). supporting multiuser, multiapplet workspaces in cbe.proceedings of the acm 1996 conference on computersupported cooperative work (pp. 344353). new york: acm press.collaboratory life: challenges of internetmediated science for chemists103impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.is devoted to data displays. in this case, figure 7.4 shows time series plots of electron densities against altitude asobserved by five incoherent scatter radars spanning the northern hemisphere from the norwegian arctic to puertorico. an important feature of the data viewers is the presentation of observations from multiple instruments on acommon time axis.figure 7.4the sparc interface showing data from five incoherent scatter radars during a campaign of april 1998. from top tobottom, the data sources are eiscat tromso, eiscat svalbard, sondrestrom, millstone hill observatory, andarecibo. the session manager is in the upper left comer and the chat window is below it.while sparc has had many kinds of impact on the space physics community, two consequences areparticularly noteworthy. first, by relaxing constraints of time and place, sparc makes it possible to carry outcollaborative campaigns with more flexibility in scheduling and participation. for example, sparc makes itmuch easier to access complementary expertise and to mentor students. in the past, scientists were restricted toexpertise available at the remote observatory site. similarly, students gained the best opportunities to learn aboutdata collection only by traveling to a remote observatory to participate in a campaign. today, sparc allowsscientists with complementary expertise to workcollaboratory life: challenges of internetmediated science for chemists104impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.together, without imposing demanding travel burdens. for instance, figure 7.5 shows the pattern ofcommunication from a 1995 campaign. in this campaign, a floridabased space physicist with no incoherentscatter radar experience was guided through a data collection run by a californiabased colleague with anextensive background in radar operations (with the added benefit that the florida physicist could have his studentswatch as well).a second notable impact of sparc is that the collaboratory has accelerated a paradigm shift in theorientation of space physicists to their data. specifically, upperatmospheric phenomena reflect a global system inwhich the atmosphere, the solar wind, and the magnetosphere produce effects over very broad regions. in the past,understanding this system took months of careful integration of multiple data sources. today, sparc makes itpossible to examine realtime data coordinated on a common time scale from any source on the internet. thecommon framework for viewing data means that events at one location are easily correlated with events at otherlocations. for example, in recent campaigns sparc has provided simultaneous data from as many as sixincoherent scatter radars, from spacecraft, and from unattended instrument arrays across europe, asia, and northamerica. in addition, sparc provides a mechanism for the simultaneous display of data and model predictions.traditionally, the substantial computational demands of such models have meant that most of this work was donelong after the observational data were collected. today, improvements in the models and less expensivesupercomputing have made it possible to do data/theory evaluation in real time.figure 7.5the pattern of computermediated communication among participants in two interleaved campaigns over a 3dayperiod in 1995. the participants at florida, california a, and the site crew member in greenland constituted onecampaign. the participants at california b, the scientist in greenland, and the site crew member in greenlandconstituted the other campaign. the michigan participant was a scientist/programmer who contributed to scientificconversations while monitoring the sparc systems. the width of the lines connecting persons indicates thefrequency of communication. lines to the node "no one" indicate questions asked that received no response.collaboratory life: challenges of internetmediated science for chemists105impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.chemistry on the internetthe emergence of systems like collaboratories represents an opportunity for chemists but also poses asignificant number of challenges. assuming that many of the technical barriers to internet use disappear withimproved computational and network performance, these challenges can be summarized in terms of potentialchanges to existing practices. three changes, in particular, demand attention. first, the introduction ofcollaboratoriesšspecifically, collaboratories designed to provide remote access to scarce instrumentationšmaytransform traditional ideas about instrument ownership and control. second, collaboratories represent newchannels for communication; however, additional information flow may be undesirable in many areas ofchemistry, particularly if the flow is unregulated (e.g., a threat to proprietary content) or unqualified (e.g., claimsthat haven't been reviewed or validated). finally, collaboratories create new arenas for learning by expandingopportunitiesšspecially for studentsšto join in with experienced scientists in the conduct of research projects.however, this new style of participatory education may require teaching and mentoring skills beyond the demandsof familiar lecture and labstyle learning.the value of collaboratoriesa key component of the collaboratory concept, at least as realized in the sparc project described above, isthe use of mediarich information technologies to link scientists with each other and with instrument facilities,independent of distance and time. this idea is particularly attractive in fields like space physics, which rely on alimited number of observatories and spacecraft, and where the primary data collection mode is passive. bycontrast, in a field like chemistry, experiments often involve direct manipulation of compounds by investigators.that is, while analytic instruments may be viewable and controllable at a distance via network interfaces, manykinds of sample preparation require close proximity between the lab bench and instruments. in these cases,collaboratory technology may not be that useful for chemists. however, there may be productive ways to use theinternet to link chemists to papers, results, or data. for instance, collaboratories may become the mechanism forongoing electronic workshops where chemists can present and discuss findings while drawing on the tools andliterature used to conduct the initial research, such as visualizations or analyses.an important mechanism in an electronic workshop might be tools for presence awareness. that is, in aphysical setting we can know who is present, paying attention, and so forth. in a virtual setting, particularly withparticipants drawn from multiple time zones, there is a need to more explicitly represent who is doing what andwhen. visual who (<http://judith.www.media.mit.edu/judith/visualwho/visualwho.html>), developed at themit media lab, is one instance of a device for helping people navigate a virtual space.12 in visual who the userdisplay indicates who is active (those with names shown in the display) as well as the recency of activity (indicatedby color, where red is more recent, and blue is in the past). another feature of visual who is that the displaygroups people by subdimensions, which could correspond to specialty, status, organizational affiliation, and soforth. such a tool would help scientists identify experts in unfamiliar specialties, as well as help find oldcolleagues at distant sites.12 donath, j. (1995). visual who: animating the affinities and activities of an electronic community. acm multimedia 95šelectronic proceedings . november 59, 1995. san francisco, california.collaboratory life: challenges of internetmediated science for chemists106impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.free flow of informationthe major impact of the internet on chemistry so far is probably the use of electronic mail for scientificcommunication. as described by one chemist at michigan, ". . . exciting things are happening. i have nowpublished papers with people that i've never met, and in one case, never even talked with aside from electroniccommunications. sort of a virtual person as far as i can tell, although real samples did arrive by real courier." it isnot difficult to imagine this kind of process accelerating with the adoption of collaboratories, where remotecollaborators might jointly analyze a sample, then share their results via an electronic workshop, and ultimatelyuse collaboration tools for editing publications.a variant of this process exists today in chemistry in the form of online conferences. for example, inchemical education, the series of confchem "meetings" have all been conducted online (see <http://www.chem.vt.edu/confchem/1998). in these sessions, papers are published on web sites, and over a designatedperiod other scientists read the papers and comment on them via chat rooms and email distribution lists. theauthors respond to these comments and over the period of the conference, authors and readers engage incomputermediated dialog. if ventures like electronic workshops and conferences are to succeed, chemists need tosolve the problem of chemical markup languages. today, equations can be represented on web pages asgraphical elements, such as bitmaps or gif images, but these equations have no formal markup syntax, whichmeans documents can't be searched for compounds or equations. for example, efforts to base chemical markup onstandards adopted by the world wide web consortium, such as xml, suggest that in the future chemists willhave convenient tools for writing and reading equations, and for searching (see < http://www.xmlcml.org>).participatory educationvisible efforts to introduce new technology into the chemistry curriculum include the use of web pages topresent course content and the creation of cdrom supplements to textbooks. these innovations have theirprimary impact on individual learners, and even in this case there is some skepticism. for instance, using a cdrom instead of paper is largely a substitution of one medium for another and not a fundamental shift inpedagogical orientation. more exciting is the possibility that the web, through collaboratories, may open newavenues for participation by a wide variety of students in chemistry research. an illustration of this approach is thecollaboratory for undergraduate research and education experiment conducted at the environmental molecularscience laboratory (emsl) of the pacific northwest national laboratory.13 in this setup, a class of honorschemistry students at the university of washington used the emsl collaboratory facility to use advanced analyticinstruments at pnnl and to interact with expert users of these instruments at pnnl (see <http://www.emsl.pnl.gov:2080/docs/collab/projects/cure/index.html>). within sparc, mentioned above, undergraduates used thecollaboratory facility to participate "alongside" senior investigators during a combined optical/incoherent scatterradar campaign. as the diagram in figure 7.5 shows, through the collaboratory students in florida viewed livedata and discussed it with scientists in northern california, michigan, and greenland. for these students, theopportunity to view phenomena as they occurred brought to life material that previously was only the stuff oflectures and textbook explanations.13 myers, j., chonacky, n., dunning, t., and leber, e. (1997). collaboratories: bringing national laboratories into theundergraduate classroom and laboratory via the internet. council of undergraduate research quarterly , 17, 116120.collaboratory life: challenges of internetmediated science for chemists107impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.conclusionthe internet offers exciting new opportunities for chemists. the collaboratory concept is just one illustrationof how internetmediated science may affect the relationship of researchers to instruments and data, of colleaguesto each other, and of teachers and advisors to students. while webbased tools may alter much of the currentfamiliar landscape of practice and pedagogy, it is important to recognize what the web cannot do. specifically,simply ﬁsurfing" for information is not a replacement for learning. amidst the temptation to browse endlesslyamong an ever widening array of online resources, students and researchers must still take time to absorb andreflect on ideas in order to master and understand key concepts.acknowledgmentsthanks to james finholt, albert finholt, peter murrayrust, and james pennerhahn for feedback from thechemistry perspective. thanks to dan atkins for his ideas on the reality gap in information technology. andfinally, thanks to stephanie teasley for helpful comments and suggestions on earlier drafts. requests for reprintsshould be addressed to (a) thomas a. finholt, collaboratory for research on electronic work, c2420, 701tappan st., ann arbor, mi 481091234; or (b) <finholt@umich.edu>.collaboratory life: challenges of internetmediated science for chemists108impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.8a computer science perspective on computing for thechemical sciencessusan l. grahamuniversity of california, berkeleyas a computer scientist whose formal education in chemistry stopped at high school, my goal is not todescribe aspects of computational chemistry. instead, i will try to suggest to you, from a computing perspective,why highperformance computing is difficult. then from a technical point of view i will summarize some of theissues that we have to contend with if we are really going to take advantage of all the exciting computationalopportunities outlined by other participants in this meeting.let us first consider performance. if we want to get more out of computing, the way we get that is by usingparallelism (box 8.1). the reasons we use parallelism are (1) to reduce the overall elapsed time in doing ademanding computation; (2) to keep the calculation moving when delays arise in sequential computation; and (3)to overcome fundamental limitations, like the speed of light, that bound the speed of sequential computation.parallelism has been used in computing for a very long time, and it exists at many, many levels of the computinghierarchy. so, there is a great deal of parallelism in the box that sits on your desk, in a single processor.higherlevel parallelism than that found in a single processor can be achieved by using multiplebox 8.1 parallelism as source of computing speed is a remedy for fundamental limitsexists at many different levelsšsingle processorsšshared memory multiprocessorsšdistributed memory multiprocessorsšnetworks of machinesdoesn't come freea computer science perspective on computing for the chemical sciences109impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.processors in a variety of organizations, some of which share memory, some of which communicate over anetwork, some of which are tightly coupled, some of which are communicating over very long distances. thatparallelism really does enhance our capability, but it doesn't come free.let us consider where some of the parallelism comes from (box 8.2), to try to understand what it is that theprogrammers and the computer scientists do and also why the world, which is already complicated, is getting evenmore complicated from a computing point of view. since the early days of computing there has been parallelism that goes on at the bit level. in other wordsthe computer can access multiple bits of information at once and can operate on them in parallel.fundamental hardware operations such as addition take advantage of that kind of parallelism. is also parallelism at the singleinstruction level. in virtually any modern processor it is possible toexecute multiple instructions at the same time. in one instant, multiple instructions are both issued andexecuted. there is overlap between computational operations such as addition and data movement such as readingand writing values to memory. thus, it is possible to write the data that must be saved and to read the datathat will be used next at the same time that computation is going on. software system one uses has parallelism in that multiple jobs can be executing at thesame time. in particular, when one job stalls because it is waiting for data, some other job takes over anddoes its thing. that timesharing technology is actually quite old at this point.part of the difficulty in exploiting the parallelism available even on a single processor is that theimprovements in different aspects of a computer are not occurring uniformly (figure 8.1). there are manyversions of moore's law, which predicts the rate of improvement in computing technology. it almost doesn't matterwhich one we use. the basic message in moore's law is that the speed of processors doubles every 18 months,roughly speaking.the speed of accessing memory improves as well, but it is improving at a much slower rate. so, over time,the gap between how quickly a processor can execute instructions and how quickly the same processor can readand write data is widening. that means that any time you have a computation that depends on accessing data, youcan lose all the performance that you might have gained from the faster mips (millioninstructionspersecond)execution rate because the processor is waiting to write what it has just computed and read what it needs next.box 8.2 parallelism in modern processorsbitlevel parallelismšwithin floating point operations, etc. instructionlevel parallelism (ilp)šmultiple instructions execute per clock cyclememory system parallelismšoverlap of memory operations with computation operating system parallelismšmultiple jobs run in parallel on commodity spmsa computer science perspective on computing for the chemical sciences110impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.figure 8.1processordram gap (latency).manufacturers aren't stupid. they are aware of that problem, so there are a number of technical devices thatcomputer designers and manufacturers have used to try to mitigate that problem. the result is that storage hasbecome more and more complicated.first consider the processor. figure 8.2 shows the memory components and underneath indicates theapproximate speeds of access and the sizes. there is very fast socalled main memory available to the processor.the processor also has a limited number of highspeed registers that provide even faster access to data values.next are the disks attached to the processor directly. disks and tape available through a network are at the far rightof the diagramšthey have substantially higher storage capacity, but much slower access.on or between the processor and main memory are other storage devices called caches. caches wereintroduced to bridge the gap between the speed of the processor and the speed of the memory. a cache can bethought of as a faster memory that holds a copy of the recently accessed data, or in the case of a program cache, acopy of a recently executed (or abouttobe executed) portion of the program. (in aintended to mitigate processor/memory gapsas storage capacity increases, speed of access decreasescaches hold values that are spatially and temporally closespeed (ns):1s10s100s10s ms10s secsize (bytes):100sks.msgstsfigure 8.2memory hierarchy.a computer science perspective on computing for the chemical sciences111impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.computer, the program is stored in memory, and part of the slowdown in execution can come from accessing thenext portion of the program.)the point about the caches is that they hold chunks. so, when a cache is loaded, it is filled with a sequence ofvalues that happen to be stored close together. what is held in this cache is that which has been used mostrecently, together with the values stored physically just before and just after. so, if the program jumps aroundwildly, accessing data that is over here and accessing data that is over there, the data that is over there overwriteswhat was in the cache. now the processor has lost its very fast access to what was in the cache before.consequently, the strategy from a programmer's point of view is to try to cluster all the information that is beingused at approximately the same time so that while it is in use, it all lives in this very fast memory called the cache.if a program is fetching chunks at a time, the only way to keep the needed information close by is to haveactually stored it close together, because the hardware is going to grab it close together. that involves a certainamount of strategy in designing a program so that its data is close together and in the right place at the right time.now, suppose we consider highend computing and some of the ideas that lead up to the kinds of systemspaul messina has described, in which there are multiple processors (box 8.3). now this gap between the processorspeed and the memory speed gets much more complicated. there are multiple processors executing at the sametime, and logically at least they are sharing data. there are data written by a part of the computation executing onone processor and read by other parts of the computation executing on other processors. maybe the processors areactually physically sharing memory as well; maybe they are not. so the latencyšthe delay in waiting to get thedatašnow increases. furthermore, there is contention for the data because multiple processors may want the sameinformation. consequently, one needs mechanisms for synchronization to make sure that one processor does nottry to read something before another processor has written it, to make sure that two or more processors don't try towrite the same data at the same time, and so on. now bandwidth, which is the volume of information that one cancommunicate at one time, also becomes more difficult to deal with because, in fact, it becomes less predictable.the time it takes to get a certain amount of data from here to there is not necessarily precise, and it dependson what is going on with all these other processors. so, the different components that allbox 8.3 multiple processors complicate the problemdata is shared among processorslatency (data access time) increases bandwidth (volume of data communication) variesperformance components become more nonuniformprocessors must be scheduled (assigned a part of the overall computational task)strategies are different depending on architectureendtoend performance requiresškeeping together all the processors busy doing useful workšhaving the right data in the right place as soon as it's neededa computer science perspective on computing for the chemical sciences112impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.together constitute the performance of one of these computing systems become very nonuniform, veryinterdependent on effects that are going on at all these different levels of memory hierarchy, and therefore veryhard to understand and to predict.in some sense the processors are now executing independently, but they are all cooperating to do the samecomputation. and so, there is an issue of how to use the different processors: what part of the calculation is doneby each one of them, and how is that done so that the overall performance is as good as possible?the real issue is endtoend performancešhow long it takes to execute the program and get the results. in amultiprocessor setting, endtoend performance requires that all of the processors be kept busy all of the timedoing useful work. otherwise it doesn't matter what the theoretical speed isšyou are not getting the benefit of it ifsome of the processors are sitting idle some of the time. in order to keep the processors busy all of the time doinguseful work, the data they need must be in the right place in the right time. that means moving information aroundfrom one place to another in the memory hierarchy: from one memory to another, from memory to cache toregisters and so on.some of that is under the control of the programmer, and some of it is not, but all of it affects the endtoendperformance of a calculation. for that reason, sometimes one sees that the peak performance of a system issomething wonderful, but the actual performance that a given person is getting on his or her calculation is muchworse, and it is very frustrating.in order to cope with this memory/processor speed disparity and the scalingup that paul messina talkedabout, different system designers have taken different architectural approaches. one of the other complications isthat the strategies that cause computations to be done very efficiently on certain architectures are different from thestrategies one would use on other architectures, even though the different architectural approaches all have theirmerits with benefits in terms of performance.what software designers do to cope with this situation, in part, is to develop programming modelsšways ofthinking about what is going on with all the varieties of parallelism that attempt to match the architecturalorganizations. there are a number of these models. i am just going to show you two of them very briefly, just tolook at their differences and to give you some sense of why what works well on one platform doesn't necessarilywork well on another platform. where i am going with this is to point out how very vulnerable legacy code is. inother words, code that has had an enormous development investment, that has really done good things forscientists, is aging very quickly. as we move through these architectural changes, it no longer performs very well,even though it might still manage to get some work done.without going into all the details, the first architectural paradigm in the multiprocessor world is that there is acollection of processors and they all share the same physical memory (figure 8.3). there is some hardware inbetween the processors and the memory called the interconnect that allows them all to get to the same memory.the natural programming model that goes along with that architecture is to have the programs on each processortake the view that they are all looking at the same data. for example, if one has an array a, then all of theprocessors can access that same array. therefore not only is it the case that one processor can write someinformation and the other one has it right there to read, but also the way that the different computations running onthe different processors can communicate is by one writing data that the other one reads. the conceptual idea isthat all the information is shared, and it is just the computational part that is being done in parallel.in contrast, a lot of people believe that the only way you really scale up is to have a setup in which there is acollection of separate processors, and each one has its own memory (figure 8.4). when the processorscommunicate, they communicate over a very high speed network. they each have their own local memories.therefore the issue of when data is really shared and when a processor is reading ora computer science perspective on computing for the chemical sciences113impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.writing a copy of datašand therefore not necessarily the same copy that some other processor is looking atšnowbecomes more complicated. deep down, communication is going on by sending messages back and forth over thishighspeed network.processors all connected to a large shared memorymultiple processes (threads) use shared address spaceeach has a set of private variablescollectively have a set of shared variablescommunicates implicitly through shared variablescoordinate explicitly by synchronization operations on shared variablesfigure 8.3shared memory machine/programming model.now if there is an array a, it is actually physically scattered among the memory of the different processors.one might then choose to organize a computation so that each processor works on part of the data that is in its ownmemory because it can get to it much faster. ultimately if they are doing a collaborative calculation, there is apoint at which the different parts of the computation need to look at data that reside in memory on otherprocessors, so latency kicks in again.latency is a problem because a program has to go out on the network to read the data on other processors,get it back, and write it in local memory, and only then can the program use those data. as part of theprogramming model, one of the big issues is how much the user needs to know about all the message passing. towhat extent does that complexity need to be exposed to a computational scientisteach processor connected to own memory (and cache)each "node" has a network interface (ni)all communication and synchronization done through niprogram consists of a collection of named processesprocesses communicate by explicit data transfers (message passing)logically shared data is partitioned over local processesfigure 8.4distributed memory machine/programming model.a computer science perspective on computing for the chemical sciences114impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.whose real concern is not with moving data from here to there but whose real concern is solving a set of equationsand getting some scientific answers?so, what does a scientist do in order to create a parallel program in this environment (box 8.4)? one of thehardest problems, and presumably one of the issues that came up repeatedly in other talks, is that in order to takeadvantage of parallelism one has to identify the work that can be done in parallel. work can be done in parallel ifone part of it isn't dependent on the results of another part.the goal is to break up your calculations into components that can all be done at the same time, and then theycan collaborate on communicating their results. that requires a highlevel strategy about the problem, and itinvolves different problemsolving techniques, and it involves people who understand the domain in which theyare solving the problems. it may not be something that can be decided in advance. it may be that where theparallelism is depends on the data, so that as the calculation proceeds it has to reorganize itself in order to maintainparallelism.if that decomposition of the problem is done wrong, then you have the situation in which some processors sitidly waiting for other processors to complete. that situation sometimes is described as a load imbalance. if thecomputational load isn't equally distributed across the system, then the overall computational speed goes down.the computation is not getting the advantage of some of the processors.so the user has to figure out where the parallelism is. then he or she has to figure out how to partition thework across the processors, and how to partition the data so that the data sit close to the clients who want to use it(because of all the communication delays). finally, at the lower level somewhere in the system, some piece ofsoftware or some programmer has to worry about actually describing the communication, describing thesynchronization, and scheduling which work gets carried out on which processor.the limitation in the speed of a parallel computation is determined by the parts that are not parallel, i.e., thatare serial (box 8.5). there is an inequality called amdahl's law that says that the increase in speed you get byparallelism is limited by the portions of the computation that are sequential. that is where the delays are. that iswhere you have to wait. the best you can possibly do if you have p degrees of parallelism is to get a pfoldincrease in speed; the advantage of more processors is going to be diminished by the sequential portions.so, in figuring out how to find the parallelism in a computation you have to find it all to really gainbox 8.4 creating a parallel programidentify work that can be done in parallelšrequires highlevel strategyšcould depend on input datašinsufficient parallelism or unequal task sizes cause load imbalancepartition work and perhaps data among processes and processorsšexcess communication and synchronization reduce parallelismšminimize communication by maximizing locality (and doing redundant work)manage the data access, communication, synchronization, and schedulinga computer science perspective on computing for the chemical sciences115impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.the benefit. finding wonderful parallelism for a while and then having the world stop while the computationreconfigures itself can cause an enormous degradation in endtoend performance.box 8.5 performance goalmaximize speedup due to parallelismamdahl's lawlet s be the fraction of total work done sequentiallyeven if the parallel part speeds up perfectly, it may be limited by the sequential part.problem size is critical!finally, let me mention one more complication. i said earlier that one must partition the work across theprocessors, to achieve parallelism, and also partition the data across processors, to reduce communication latency.alas, these two strategies can conflict (box 8.6). if there is more simultaneous computation, using local data, thenthere can be more data that needs to be pushed through the network to integrate the results of the parallelcomputations. moving data around only indirectly advances the computation, and too much slow data movementcan undo the benefits of high degrees of parallelism. and again, the tradeoffs are different for differentarchitectures.the research challenges in box 8.7 deal mostly with performance. given a new problem that we want tosolve, can we, the chemical scientists, find enough parallelism in the problem to be able tobox 8.6 locality and load balance tradeoff optimizations that increase parallelism decrease localityšsubdivide datašmigrate computationoptimizations to improve locality decrease parallelismšmove everything near one processorhard problem is optimizing bothnonuniform costs make it even harder!a computer science perspective on computing for the chemical sciences116impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.exploit the platforms that are coming along? that depends on the problem, but it also depends on the strategy forsolution.box 8.7 research challengesincreased speed comes from greater parallelismšcan we find sufficiently parallel problem solutions?how much of the details can be hidden from the applications programmer?what are natural ways to express computation that also yield efficient code?how can platform portability be achieved?how much reuse is possible, e.g., via libraries and packages?how are debugging and tuning done (how does the user understand what's going on)?to what extent can the computational science and computer science community hide from the applicationsprogrammer the lowerlevel details? can we hide details about sending a message, receiving a message,synchronizing, and so on so that it doesn't clutter up the thinking about what is going on at the higherlevelstrategy? how can one describe these calculations so that they are readable by chemists, but so the description willstill enable the generation of efficient code, meaning that there is a path that allows all that lowerlevel parallelismto be exploited as well? how can we do that so that it transcends changes in platform, so that once you have astrategy, it has some persistence over some of these changes in latency, in bandwidth, and in various aspects ofperformance?given that the strategy shift is a platform shift, how much can we continue to depend on libraries or on reuseof codes that have actually had a great deal of intellectual investment in them? will they continue to give us thebenefits that we want to believe they have, or does the performance simply degrade almost without people noticinguntil it is too late? finally, given these complicated systems, such as the asci system that paul messina hasdescribed, how does one get a grasp on what is going on overall? how do we understand what the overallperformance is and what contributes to it, and how does one get a mental model of what to do when things seem tobe going wrong?now, as if that weren't complicated enough, the scientific community is becoming even more ambitious. notonly is there attention to high performance in the sense of computation, but now we also want to buildcomputations, simulations in particular, that use massive amounts of data (box 8.8).box 8.8 now generalize the situation high performancemassive amounts of diverse data at many sites diverse platforms and devicescollaborations really big and challenging problemsa computer science perspective on computing for the chemical sciences117impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.those data are gathered in a variety of ways. they live in a variety of places around the world and arerepresented in very diverse ways. we want to carry out simulations and modeling using components created byother groups, using other platforms and other data representations.as we contemplate building these large, coupled, dataintensive simulations and models, the platforms thatwe might want to couple together to solve very hard problems are diverse. it is difficult enough to think aboutsolving one application on one of these complicated parallel computing systems; now we want to reach out andget data from some other computational platform that has different characteristics.creating ambitious simulation and modeling systems requires collaboration. collaboration is not just amongpeople but also between people and devices, people and instruments, people and computational platforms. theproblems that we want to solve are getting much bigger as well.consider the following situation, illustrated by two figures that come from andrew grimshaw at theuniversity of virginia. we are now in a situation in which, given the ascendancy of networks and the promise thatthey are going to get even better, we can think of doing an electronic experiment, having an electronic laboratorythat can reach out electronically to other sites that have diverse, possibly unique capabilities (figure 8.5). forexample, there can be ways of doing reallime data collection from observational data obtained from instrumentsthat are attached to the network, and data repositories that exist at various remote sites. figure 8.5 shows a map ofthe united states, but the resources might be anywhere in the world. in order to use them there needs to be someway of getting all these very diverse pieces to fit together. if you are a chemist carrying out an electronicexperiment, you configure the experiment conceptually, assemble all the pieces, and describe at a very high levelwhat it is you want to do, where you are getting the resources from, and how you are going to use them.what a number of people are doing is building systems, such as the legion system pictured in figure 8.6,which provide socalled metacomputing capabilities. computing at that level consists of staging the experiment:assembling the pieces, getting permission to use them, and getting them to interoperate, i.e., to work together. thevision is that the user sitting at a workstation puts this virtual environment together, identifies all the components,runs the electronic experiment, and looks at the results.in addition to all the performance issues that are going to get even worse, there are issues about security.there are issues about fault tolerancešif you finally manage to get all the pieces coordinated and get yourexperiment going, you don't want it to die because one component failed, one processor went down. the way thenetwork remains robust is by finding an alternative path if one path getsfigure 8.5the opportunity.a computer science perspective on computing for the chemical sciences118impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.blocked. the goal is to do that in a transparent way, so that the user focuses attention on the experiment and not onthe details that are below the surface of what is intellectually important in solving the problem.figure 8.6the legion visionšone transparent system.there are also some nontechnical issues of how we live in this emerging world (box 8.9). there is a seriousconcern among the community about where the people are going to come from who are going to have theknowledge and skills to allow us to live in this computational world. the strategies one needs to solve scientificproblems in the kinds of computing environments i have described require deep knowledge of the domain, in thiscase chemistry, and also deep knowledge of information technology in order to put things together and make it allwork.ideally, one would have very expert computational chemists, people who are wonderful chemists andwonderful information technologists and understand how to put those two areas together. an alternative is to havemultidisciplinary teams. to make multidisciplinary teams work, there are a lot of sociological issues that have toget solved about mutual respect for what the other side does, about talking the same language, about understandinghow to identify what the real scientific and technical problems are. we are making progress there, but we still have aways to go.box 8.9 more issueswhere will the software developers come from?šstrategies require both deep domain knowledge and deep information technology knowledge.šboth expert computational chemists and multidisciplinary teams are needed.where will the languages, libraries, and tools come from?šsome needs are generic and will be met by the marketplace.šscientific computing is not an economic driver nor an important marketšdon't count on commercialvendors.where will the computing and communications resources come from?a computer science perspective on computing for the chemical sciences119impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.box 8.10 pitac recommendations: executive summaryfindings:information technology is vital to our wellbeing in the 21st century.total federal information technology r&d investment is inadequate.federal it r&d is excessively focused on nearterm problems.recommendations:create a strategic initiative in longterm it r&d. the investment for research in software, scalable information infrastructure, highend computing,and socioeconomic and workforce impacts.diversify modes of support.establish an effective structure for managing and coordinating it.source: summarized from the president's information technology advisory committee's report to thepresident, information technology research: investing in our future, february 1999 advance copy,national coordination office for computing, information, and communications, arlington, va.where are the languages, the tools, and the libraries going to come from to do computation in thisincreasingly complicated world? there are needs that aren't peculiar to chemistry, and there are needs that aren'tpeculiar to science, and a lot of those will come from the marketplace. that is going to give us a lot of thenetworking technology we need.there are other issues that are more peculiar to what we do in computational science. scientific computingdoes not drive the market, and so we cannot expect that over time the vendors are going to step up and provide thespecialized software solutions that are essential to at least part of what we are trying to do. that means that thescientific community is going to have to find a way to develop that software, and we are going to have to find away to do it that provides highquality robust software and doesn't consume all of everybody's time.finally, there is an issue that i know many people in the chemistry community have struggled with, namely,that even though in principle we can build these very high performance, very powerful systems such as the onepaul messina described, how is the average bench scientist going to get access to them?in closing, let me just say a little bit about the committee i am on, the president's information technologyadvisory committee (pitac). when the high performance computing and communication act was passed in1991 or so, part of the legislation said that there was to be an advisory committee for highperformancecomputing. it took until 1997 for the committee to be established and, by the time it was appointed, there were lotsof issues on the table besides highperformance computing. the committee was given the additional task oflooking at the nextgeneration interact program that was then emerging in the government, and then broadened itsagenda to look at information technology overall.the committee is drawn primarily from the computer science and communications areas. it issued an interimreport in july and will issue a final report early in 1999. the highlevel findings and recommendations are shownin box 8.10.a computer science perspective on computing for the chemical sciences120impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.box 8.11 software researchfindings:demand for software far exceeds the nation's ability to produce it. the nation depends on fragile software.technologies to build reliable and secure software are inadequate. the nation is underinvesting in fundamental software research.recommendations:fund more fundamental research in software development methods and component technologies.sponsor a national library of software components.make software research a substantive component of every major it research initiative.support fundamental research in humancomputer interfaces and interaction.make fundamental software research an absolute priority.source: summarized from the president's information technology advisory committee's report to thepresident, information technology research: investing in our future, february 1999 advance copy,national coordination office for computing, information, and communications, arlington, va.the investment in research and development in information technology is not keeping up with the growth inthe importance of the area. furthermore, because of the tension caused by the shortage of money, the investment inr&d is increasingly short term. in other words, if we spend our money only to solve today's problems, which havemeasurable milestones and goals, we shortchange the longerterm investment in developing the new ideas that aregoing to fuel us in 10 or 15 years.there are also recommendations concerning how to educate more people and how to give the averagetaxpayer access to computing and the like. i don't mean to dismiss these recommendations, but they are lessrelevant to this workshop than the research recommendations.when this committee first met, everybody, no matter what field they came from, said, ﬁthe real problem issoftware." our recommendation is that the research investment of the government be increased, especially insoftware, in scalable information infrastructure, in highend computing, and in work force and socioeconomicissues. by scalable information infrastructure we mean not only networking, but also all of the information thatlives on our networksšall the web pages, databases, and mail systems; everything that involves using informationin a networked environment. thus there are significant software issues there as well.in making software research a priority (box 8.11), as with any grand challenge problem, you have to worryabout more than how you are going to solve that problem with any strategy you can figure out. we also seek todevelop some underlying solution technology that is reusable and that will allow you to solve next year's problemsand the problems of the years after.the committee work is an ongoing process. we are currently in the process of gathering feedback from all ofthe communities we talk to, so that the final report can be as strong as we can possibly make it so that we canactually see this exciting computational environment move ahead.a computer science perspective on computing for the chemical sciences121impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.acknowledgment: my thanks go to my colleagues kathy yelick, jim demmel, and david culler for sharingthe slides from their berkeley graduate course in parallel computing, cs267.discussionbarbara garrison, pennsylvania state university: i do computational chemistry, and i am inherentlylimited by amdahl's law, yet i have problems where i could use parallel computing, but at times i feel like i amwasting cycles because of imperfect scalability. has there been any effort to design hybrid serial parallel machinesor operating systems where there could be sharing of the parallel resources so we are not in fact wasting thecycles?susan graham: that is what the old operating systems notion of multiprocessing is really all about. thesystem can detect when you are waiting for something, and it can go and run somebody else's programmeanwhile.the complication when you now get these memory hierarchies is that if the system is going to run somebodyelse's problem while yours is waiting for its data, then the staging is going to fetch their data and flush your dataout of the caches. that will affect your performance as well. thus what you suggest can be done, but the extent towhich it is really going to help is something that we don't totally understand. having the system wait for you isgoing to be the most efficient thing for your job, but there is a question of throughput and how you share thatresource with other people.evelyn goldfield, wayne state university: i have two questions. i am, also, a computational chemist whouses parallel programming and computing. one of the things that i have been wondering about is the differentarchitectures. i use massively parallel distributed memory computers, but all these shared memory computers arecoming along. i wonder to what extent the memory is actually shared, and to what extent we really do have tochange our computing paradigms. how important is it for people to have different programs for differentcomputing paradigms for different types of architecture? if we have to have completely different programs fordifferent machines, it is going to be quite dismaying for people.susan graham: this, again, is something that we understand imperfectly at this time. if you have distributedmemory solutions, distributed memory strategies, it is not that hard to get them to do well on shared memorysystems. it is going in the other direction that is harder, in my experience.what one really wants, and this may not be something that is completely within one's control, is to have acertain amount of adaptivity. if your program makes it apparent where the parallelism is, where the opportunitiesare, and if a compiler can figure out where the communication has to be, where the sharing is and so on, then youwant the system software to adapt the program communication, synchronization, etc. to the architecture. so, it isat the strategy level that it is really most important that you are not locking yourself in at the high level toparticular details of the architectural model.evelyn goldfield: i have one other question that i think is really key and that is, i think, in the minds of a lotof chemists. we are willing to waste computer time if it is a choice between wasting the computer's time andwasting our own time. the only thing we really, really care about, truthfully, is how long it takes us to get the jobdone as long as we get the cycles. i thought of this when you talked about load imbalance. you can have a lot ofload imbalance and still get your job through on time. how much are chemists going to have to really worry aboutthese computer science questions if you don't want us to waste the cycles? it seems to me it is the computerscientists that have to come up with that solution.a computer science perspective on computing for the chemical sciences122impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.susan graham: it is not that we don't want you to waste the cycles. somebody once said to me, "you know,people have telephones, and they don't worry about keeping the telephone busy all the time. it is an appliance, andit is there when they need it." that gets back to the kinds of problems you are trying to solve, and the economics.if you can afford to have a machine that sits on your desk that is powerful enough for what you want to do, thenthe primary issue is going to be ease of making it do what you want it to do. but there are chemists who haveproblems that even the asci system can barely handle. they cannot afford to have part of the system sitting idle,particularly when their access to it is once every so often for a limited amount of time.thom dunning, pacific northwest national laboratory: i had a couple of comments on yourpresentation. one is the emphasis on software that came out of the pitac report. i have always felt that one of themajor limitations in realizing the potential of the computing systems that we have had and are going to have in thefuture is software. so i was absolutely delighted to see this committee recognizing the importance of software,because it is so easy in a system like we operate in to look at the hardware, a physical object, and not recognize thefact that the hardware is absolutely useless without the software. also, one comment relative to the question thatevelyn goldfield asked is that we at pacific northwest national laboratory, and people in other places, havedeveloped computing infrastructures. ours are called global arrays that actually run on distributed memorycomputer systems, shared memory computer systems, and a collection of workstations, all of which is entirelytransparent actually to the applications programmer. this is the case because we have computer scientists whohave implemented global arrays on all of these different types of architectures, and the only thing the applicationsprogrammer has to do is issue the calls to those particular subroutines. so, there are ways that one can actuallywrite software that performs well on a number of different architectures.now, i am not at all clear that that is going to perform well on the types of architectures that paul messinadescribed, but clearly many of these problems have been solved, and i have confidence in the creativity of both thechemistry and the computer science community and that we will see it solved for the types of very large systemsthat paul described.robert lichter, camille & henry dreyfus foundation: as a noncomputational chemist and a noncomputer scientist i want to thank you for an extraordinarily lucid description of how these things work. i thinkthis is the first time i have run into the topic in a way that is comprehensible.i was also struck by how much chemistry you do know, because the strategy that you described for solvingproblemsšdoing isolated computations and then pulling them togetheršis very much the way a synthetic organicchemist synthesizes a complex molecule. you do little pieces and then glue them together.when looking at the kind of global picture of marrying hardware and software, we are limited by what exists. iwould be very much interested in your wildest vision as a computer scientist of what could exist either in hardwareor in software that we haven't even begun to think about now.for example, one thing that even i am aware of is the concept of dna computing, which nobody has talkedabout here. i don't know whether that's because it is not worth talking about, or whether it is not developed enoughto talk about, but that is the kind of thing i'm referring to. i'm just curious to see if you could speculate wildly.susan graham: i don't know whether i can do that. i can comment a little about dna computing and thingslike that. that is one example of the part of the research agenda that we feel has been neglected of late. in otherwords, dna computing is wildly speculative in the sense that the computational modela computer science perspective on computing for the chemical sciences123impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.is totally different, and yet the attempt is to draw it from nature, from something that exists, and it potentiallygives huge amounts of parallelism.the issue from a computer science point of view is figuring out what the algorithms might be that wouldactually do well in that computational model, and of course, there are issues on how you build such a computer. ithink it is really important to explore those directions.there are people in my field who have wonderful imaginations about these things. i am not one of them andso i don't want to take up your immediate challenge except to say that i think you are right. people are mostcomfortable thinking sequentially. people are most comfortable thinking in the way i described, take some data,move it from here to there, and so on. we have to break out of that a little bit to at least experiment and see whatmight happen if we had a very different model.gintaris reklaitis, purdue university: you described an interesting model of a realtime environment inwhich you gather data from different sources and different instruments, run the data through a model, and then actupon the results. this is very much along the lines of the supply chain of interest in process operations. in yourcase, the information is obtained asynchronously, yet the computational models that you describe operatesynchronously. although you are parallelizing the computational tasks, under the synchronous model you areforced to wait for the slowest task to execute and you do not use all of the latest information in executing thetasks. is there any work in progress on asynchronous parallel computing models?susan graham: i was describing synchrony as a problem. there are times when you really have to worryabout the temporal order in which things happen. it is possible, for example, in my shared memory situation thatsome of the processors are just filling that memory with interesting stuff while other processors are going on anddoing their business and not worrying about that until they are ready. asynchronous models in which one isnotifiedšor the status is posted and whoever cares can look and find out the statusšare actually in some waysmuch more comfortable. they can be easier to build, but they are harder sometimes for people to think about.now, it is possible we are not puffing as much attention on that as we might, and that is where the interactionwith the application domain is so important. you describe to me what you want to do and then i start thinkingabout how can i help you do that.a computer science perspective on computing for the chemical sciences124impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.9collaboratories: building electronic scientific communitiesraymond a. bairpacific northwest national laboratoryabstracthighspeed computation now provides the means to examine and simulate systems at unprecedented levels ofdetail and accuracy. the combination of computation with largescale databases enables analysis of the prodigiousvolumes of data coming from today's experiments and simulations. however, when these enabling technologiesare coupled with new capabilities in communications, an opportunity is created that can revolutionize not only thescope but also the process of scientific investigation. in physical science research, new distributed computing andcommunications technologies are being employed that enable researchers to access data, instruments, andexpertise independent of their location.while the term ﬁcollaboratory" (or "virtual laboratory") is often used to refer to a set of technologies, perhapsthe most significant impact of collaboratories will be the generation of new opportunities to create and sustainactive scientific communities. the development and adoption of electronic collaboration capabilities will providegeographically distributed research teams with greater abilities for the organization, closeknit interaction, andrapid response, needed to address increasingly challenging research problems. this paper examines some of theopportunities and challenges presented by scientific collaboratories, and the interplay between emergingcollaboration technologies and the research communities they support. experiences to date point to requirementsand success factors for virtual facilities. examples are drawn from technology development and chemical/materials pilot collaboratory projects of the u.s. department of energy.introductionone of the scarce resources in chemical research is timešfor scientists, instruments, and comnote: pacific northwest national laboratory is a multiprogram national laboratory operated by battelle memorial institutefor the u.s. department of energy under contract deac0676rlo 1830.collaboratories: building electronic scientific communities125impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.putersšto explore and understand complex phenomena and to unlock the principles governing them. advances incomputing and communications systems are having profound impacts on the capabilities that we can bring to bearon research and development problems, providing extraordinary instrument control and data acquisitioncapabilities, powerful data analysis and visualization capabilities, and simulations capable of ever more detail andscope. this aspect of the computer revolution is rapidly magnifying our science capabilities while reducing thetime needed to perform measurements and simulations. however, outside of these areas wholly new impacts ofcomputing advances are emerging, which will dramatically expand our options for using our time to organize andconduct scientific efforts.the term "collaboratory" (colaborate + laboratory) is attributed to william wulf, who envisioned thepotential impact of the information age on science, creating a ". . . 'center without walls,' in which the nation'sresearchers can perform their research without regard to geographical locationšinteracting with colleagues,accessing instrumentation, sharing data and computational resources, [and] accessing information in digitallibraries."1 other terms that are used almost interchangeably with collaboratory are "virtual laboratory,"ﬁlaboratory without walls,ﬂ and "collaboratorium." they all encompass the use of information and communicationsystems to remove barriers of geographic distance and time from research collaborations, not just scientistsworking remotely, but working together regardless of their location. a major emphasis of collaboratories isnatural, informal work processes, going beyond text exchange and presentation metaphors, to indepth,collaborative work.2collaboratories have potential roles in all stages of the scientific process, from the initial planning andorganization of a new project idea and project team, to the design of the experiments and development ofsoftware, to the execution of those experiments and simulations and their analysis, to the preparation anddissemination of the results. however, one does not simply deploy a collaboratory like a desktop publishingprogram; one builds a collaboratory with scientists, information, and tools. the collaboratory tools required arevaried and challenging to develop, requiting both generic capabilities like video conferencing and screen sharing,and domainspecific capabilities to handle the manipulation and display of data types particular to each type ofscientific work. integration is a major component of collaboratory development, spanning groupware, legacymodeling and analysis applications, instrument software, files, and databases. because of their uniquerequirements, collaboratories are often leadingedge examples of knitting together new distributed systemstechnologies.collaboratories are an emerging capability that provides new resources for chemical science. this paperprovides an overview of collaboratories from the perspective of scientific research, discussing opportunities forcollaboratories, examples of the use of collaboratories in chemistry and related disciplines, the kinds of softwarethat are being developed for collaboratories, the impacts that collaboratories are having, and the requirements andprospects for the future.opportunities for collaboratoriesthere are a number of arenas that are fertile ground for the development of collaboratories, particularlyamong scientific user facilities and institutes that provide unique and specialized resources to the scientificcommunity. although the examples given in this discussion are drawn from the u.s. department of energy(doe) arena, there are many analogous scenarios in our university and industrial1 national research council, computer science and telecommunications board, national collaboratories: applyinginformation technologies for scientific research, national academy press, washington, d.c., 1993, p. vii.2 r.t. kouzes, j.d. myers, and w.a. wulf, 'collaboratories: doing science on the internet," ieee computer, 29(8), 4046,august 1996.collaboratories: building electronic scientific communities126impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.communities, as well as other government agencies. doe builds and supports a wide range of national scientificuser facilities, ". . . built with the express purpose of being available for the performance of research by a broadcommunity of qualified users."3 national scientific user facilities make unique research resources available to doescientists and researchers from academia, industry and other federal laboratories, and provide opportunities neededto educate and recruit young scientists to meet the demanding challenges of the future.figure 9.1 shows 19 of the doe scientific user facilities most often used for chemical, biological, andmaterials research. they include four synchrotron radiation light sources, five highflux neutron sources, fourelectron beam characterization centers, and five other centers, two specializing in doe missions of environmentalscience and combustion. eighteen of these facilities are operated by doe's office of basic energy sciences. theenvironmental molecular sciences laboratory in washington state is operated by the doe office of biologicaland environmental research. the spallation neutron source in tennessee is under construction. each of thesefacilities supports research by individual investigators and collaborative teams from across the country and aroundthe world. today, scientists often travel to a facility to do their work. many have limited time and resources totravel and must carefully optimize their experiment time, often limiting their benefit from the facility. thefacilities themselves are widely dispersed, and increasingly often scientists need to use more than one facility inthe course of a complex investigation.collaboratories can increase the effectiveness and value of user facilities like these in many ways. more canbe done before the scientific team arrives on site, e.g., detailed planning of the research campaign, and training forthe specific equipment to be used there. while the team is on site, communication is enhanced with colleagues atthe home institution and collaborators at other institutions. for example, although a professor may not be able tostay long at the user facility to mentor his/her students, collaboratory capabilities facilitate following the detailedprogress of the work remotely, and helping with problems as they arise. after scientists leave the user facility,shared analysis and discovery are enhanced through collaboratory capabilities. thus, collaboratories enhance theproductivity of research. more complex problems can also be taken on, as collaboratories support the assembly ofinterdisciplinary teams. expertise can be drawn from many more sources, including industry and smaller colleges.this ability to handle more complex problems can also have an impact on what science is done. thus, byenhancing collaboration, collaboratories enable new scientific processes and new science.although there are enough commonalities between collaboratories to speak about them in general, scientificcollaboratories are very individual, customized to the style of a research community and to the nature of theirresearch. the essential suite of capabilities needed is particular to the kind of research being performed, as is therelative importance of those capabilities. the processes used to collect and analyze information are also diverse,and the collaboratory needs to reflect that. to date, we've only scratched the surface in building effectivechemistry collaboratories; there's still much to learn. the development of a new chemistry collaboratory typicallyinvolves adding domainspecific capabilities to a base of collaboratory tools. however, the first step in the processis to learn about how the scientists work, what their information is like, how they need to store and share it, andwhere the communications and information management problems are that collaboratories might facilitate. thenthe computer scientists can team with the chemists to develop and integrate the necessary tools and applicationsinto a working collaboratory.3 u.s. department of energy, "office of energy research facilities," in pricing of departmental materials and services,order doe 2110.1a, change 2, may 18, 1992, chapter iii, section 10.collaboratories: building electronic scientific communities127examples of collaboratoriesimpact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.figure 9.1distributed scientific facilities and researchers: selected doe scientific user facilities often used for chemistry, biology, and materials research.collaboratories: building electronic scientific communities128impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.experience with many groups indicates that each collaboratory seems to have an overall organizing principle,one of a small number of ways that people create and interact with research results. for example, a couple ofpioneering collaboratories had quite different approaches. the worm community system4 created an extensivecentralized, shared data repository about a single biological organism, caenorhabditis elegans.5 today's humangenome databases are likely to evolve into such collaboratories centered on a community effort to create andunderstand the human genome. another very successful early effort, the upper atmospheric researchcollaboratory (uarc)6,7 is largely organized around community experimental campaigns, using instruments at adozen remote experiment sites (including such inhospitable environments as greenland). uarc continues into thenext generation as sparc.8 more recently, several pilot collaboratories have been set up by doe projects. threeof them provide good examples of the diversity and impact of collaboratories on the chemical and materialssciences: the diesel combustion collaboratory,9 the materials microcharacterization collaboratory,10 and theenvironmental molecular sciences collaboratory.11the diesel combustion collaboratory (dcc) assists the partners of the longstanding heavy duty dieselcombustion crada, a collaborative research and development agreement among doe researchers and dieselengine manufacturers. it involves four national laboratories: sandia (snl), lawrence berkeley (lbnl), lawrencelivermore (llnl), and los alamos (lanl), scientists at the university of wisconsin, and three companies:caterpillar, cummins engine, and detroit diesel. dcc is a part of the doe2000 collaboratory pilot projectsprogram, and gives scientists capabilities that do not exist at any single location.12 the dcc enhances the flow ofinformation between and among the experimentalists and modelers at the national laboratories and the enginedesigners at the industrial sites. so, this collaboratory is focused around the information base of results fromexperiments and modeling runs, providing visualizations that the investigators share. the dcc also providescapabilities for industrial researchers to run chemical and numerical models and simulations remotely on doecomputers. because proprietary industry research is involved, there is a significant concern about security, whichmust be addressed by the collaboratory tools. snl has enabled the collaboratory partners to have a secureencrypted connection, to discuss engine drawings, experimental data, output from a model, or results of avisualization. they may share secure information or applications from a collaborator's computer. dcc alsoprovides a shared workspace via the bscw product,13 an image library, a data archive, and shared electroniclaboratory notebooks.the materials microcharacterization collaboratory (mmc) has a different set of requirements.4 b.r. schatz, "building an electronic community system," j. management & information systems, 1992.5 the current web site for c. elegans is <http://elegans.swmed.edu/>.6 c.r. clauer, d.e. atkins, et al., "a prototype upper atmospheric research collaboratory (uarc)," visualizationtechniques in space and atmospheric science, e.p. szuszczewicz and j.h. bredekamp (eds.), pp. 105112, nasa sp519,nasa, washington, d.c., 1995.7 n. rossflannigan, "the virtues (and vices) of virtual colleagues," technology review, pp. 5259, march/april 1998.8 see the space physics and aeronomy research collaboratory (sparc) web site at <http://www.crew.umich.edu/uarc/>.9 see the diesel combustion collaboratory web site at <http://wwwcollab.ca.sandia.gov/>.10 see the materials microcharacterization collaboratory web site at <http://aem005.amc.anl.gov/mmc/>.11 see the environmental molecular sciences collaboratory web site at <http://www.emsl.pnl.gov:2080/does/collab/>.12 the doe2000 program is described on the web site at <http://www.mcs.anl.gov/doe2000/>.13 the basic support for cooperative work (bscw) product comes from the german national research center forinformation technology. see the bscw web site at <http://bscw.gmd.de/>.collaboratories: building electronic scientific communities129impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.this collaboratory was constructed by researchers at argonne national laboratory (anl), lbnl, thenational institute of standards and technology (nist), oak ridge national laboratory (ornl), the universityof illinois, and several instrument/computer manufacturers, including gatan inc., r.j. lee instruments ltd.,emispec systems inc., philips electron optics, nsašhitachi scientific instruments, jeol usa inc., sunmicrosystems inc., and graham technology solutions inc. mmc is also a part of the doe2000 collaboratorypilot projects program.14 each of the research labs had diverse microscopy capabilities when the project began,and several had developed remote microscopy tools.a major goal of the mmc is to explore and develop a shared electronic virtual environment around acommon theme of microscopy and microanalysis, encompassing leadingedge instrumentation and applied to botheducation and research. mmc has defined a common set of capabilities that are needed for electron microscopy,and is working on data models, graphical user interfaces, and application program interfaces (apis) for thecommon architecture. although essentially all commercial microscopes have computer control, previousgenerations had some features that were mechanically adjusted, and the instrument control programs were oftendifficult to drive from another application. mmc has been working with the instrument manufacturers to conveyrequirements for remote control. essentially all are now implementing more useful and complete apis in theirproducts. mmc not only lets one control the microscopes, but also has established tools for sharing analysesamong distributed collaborators, and comparing the images from multiple simultaneous experiments at differentlocations. the results of the work of the mmc are stored in electronic notebooks.the instrument control architecture that mmc employs (figure 9.2) is very similar to that for other remotecollaborative instruments being developed in the chemical sciences. a commercial microscope may have multipledevices that can be controlled through serial interfaces or network interfaces (tcp/ip protocols). a local serveraccepts commands, validates them (ensuring the instrument is not operated out of its ranges), issues commands tothe appropriate instrument actuators and sensors, and collects data from the spectral and image data acquisitionsystems. the user interface uses web technologies so that scientists can interact with the instrument through theirweb browser. note that the user interface (client) is separate from the microscopy server, and the microscopyserver is different from the instrument computer. this is key to flexible and extensible designs. of course, all ofthe client and server processes can (and sometimes do) run on the same computer.the environmental molecular sciences collaboratory is yet another type of collaboratory. theenvironmental molecular sciences laboratory (emsl) is doe's newest user facility, located at pacificnorthwest national laboratory (pnnl). instead of providing a particular kind of capability, emsl is a collectionof many unique capabilities and expertise for a particular mission, environmental molecular science. the focus ison developing a molecularlevel understanding of the physical, chemical, and biological processes that underlieremediation of contaminated soils and groundwater, processing and disposal of stored waste materials, and humanhealth and ecological effects of exposure to pollutants. emsl has three major facilities: the molecular sciencecomputing facility, the high field magnetic resonance facility, and the high field mass spectrometry facility.other specialized capabilities and facilities provide resources targeting research areas in nanostructural materialssynthesis, interfacial structures and compositions, reactions at interfaces, and gasphase monitoring and detection.consequently many emsl projects and collaborations cross disciplines.the preparations for collaboratories at the emsl began during its construction. emsl's networks, computersecurity, shared file systems, and user services were designed to support both individual users14 the doe2000 program is described on the web site at <http://www.mcs.anl.gov/doe2000/>.collaboratories: building electronic scientific communities130impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.collaboratories: building electronic scientific communities131figure 9.2instrument control architecture for the telepresence microscopy instruments at argonne national laboratory.impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.and teams, internal and external.15 a scientific data management (sdm) system was developed to manageemsl's 20terabyte robotic tape archive.16 sdm captures and stores metadata (information about the data) so thatneeded files can be located easily, even by scientists in other disciplines. collaboratory tool development began in1993 and is currently in its third generation, now part of the doe2000 national collaboratories program. usersupport is provided for new collaboratory tools as they are deployed. this is very important, as collaboratory toolsoperate in a complex environment and scientists are often not accustomed to using them. emsl's inhouseinstrument development lab (idl) provides custom instrument electronics and software developmentcapabilities. remote operation, "fly by wire," automatic metadata capture, and automatic data archival capabilitiesare becoming a routine part of idl instrument designs and upgrades. together, these capabilities provide thefacility computing infrastructure upon which collaboratories can be built.although many collaboratory projects have begun in emsl, there are three highly developed examples. thevirtual nmr facility provides a set of extensions to the generic collaboratory tools for nmr spectroscopy. 17this collaboratory provides secure remote access to operate emsl nmrs, employing the commercial consolesoftware provided by varian (vnmr). an nmr spectroscopist's notebook adds electronic notebook capabilitiesto view instrument parameters and threedimensional molecular structures, and to capture nmr spectra from thespectrometers. in the custom instrument arena, emsl developed an on line radio frequency ion trap massspectrometer.18 all of the primary instrument parameters can be controlled remotely, including sample injection.the instrument server ensures that only reasonable parameters are passed to the spectrometer. two versions of thesoftware are available, a visual basic version that is standalone and a java version that permits shared remoteoperation as part of the core2000 tools, discussed in the next section. educational uses of collaboratories are alsobeing explored in undergraduate and graduate programs. for example, the rf ion trap is regularly used in remotelectures and experiments by undergraduate chemistry classes in the pacific northwest. the collaboratory forundergraduate research and education is a consortium of colleges and universities formed to exploreopportunities for collaboratories to have an impact in education through workshops and pilot programs.19 it isalready clear that collaboratories will have a substantial impact on curriculum enhancement, faculty development,and undergraduate and graduate research.tools for collaborationthe major technologies that distinguish a collaboratory from remote computer/instrument access are thecollaborative tools that permit scientists to share the scientific process. it's easy to see that tools15 the capabilities of the emsl are described on the web site at <http://www.emsl.pnl.gov/>.16 d.r. adams, d.m. hansen, k.g. walker, and j.d. gash, "scientific data archive at the environmental molecularsciences laboratory," proceedings of the sixth goddard conference on mass storage systems and technology, gsfc/cp1998206850, pp. 409417, march 1998; and d.m. hansen and d.r. adams. "a database approach to data archivemanagement," proceedings of the first ieee metadata conference, ieee computer society mass storage systems andtechnology technical committee, ieee computer society press, los alamitos, calif., 1996.17 see the virtual nmr facility web site at <http://www.mcs.anl.gov/doe2000/>.18 see the on line radio frequency ion trap mass spectrometer web site at <http://eol.emsl.pnl.gov/>, and j.m. price,m.v. gorshkov, j.a. mack, and b. rex, "an internet accessible, online ion trap mass spectrometer for collaborativeresearch," proceedings of the 44th asms conference on mass spectrometry and allied topics, p. 1175, 1996.19 j. myers, n. chonacky, t. dunning, and e. leber, "collaboratories: bringing national laboratories into theundergraduate classroom and laboratory via the internet," council on undergraduate research (cur) quarterly, 17(3),116120, march 1997.collaboratories: building electronic scientific communities132impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.that support realtime interactions will be important in building collaboratoriesštools that let scientists carry onthe research process when they are geographically apart, with all of the richness of the interactions they can havewhen they are together in the same room. however, such realtime tools cover only one aspect of scientific work.when we collaborate, we don't always do everything together, yet it is vital that we be able to share the record ofwhat we have been doing (e.g., parameters, data, theories, simulations, analyses), our views on and additions to thecontributions of others, and also the record of discussions that have occurred (especially when one or more of thecollaborators was not present).electronic laboratory notebookstraditionally, the primary record of the scientific process has been the ubiquitous laboratory notebook. thelab notebook has existed through the agesšleonardo da vinci's notebooks are a great example. however, if onelooks at notebooks through the ages, an interesting change can be observed. for centuries, the lab notebook wasthe complete record of a scientist's work. everything was within the pages of the notebook: theories, proofs,equipment designs, experimental conditions and results, analyses, and conclusions. today's notebooks oftenexclude the raw data because it would take up too much space and it's only read by computers anyway. insteadthere are references to external data archives: files, tapes, floppies, etc. tables, charts, and graphs are produced bycomputers, printed, and pasted in. increasingly the lab notebook contains less and less of the full scientific record.on the other hand, computers can easily manage all of those data forms, and more. the concept of an"electronic lab notebook" has been around for a while. however, it has been hard to execute in software. only inthe last couple of years have we begun to have the tools to build an electronic notebook without heroic effort. webstandards provide a ubiquitous interface, and new object languages and distributed object standards support facileinteroperability (the ability of different applications to exchange information and work together without customprotocols) and extensibility (the ability to add features to an application without getting into the "guts" of it).lab notebooks have many roles:strument log book,log book,record,workspace.most notebooks play more than one role. our electronic lab notebook will need to hold lots of different kindsof datašfrom instruments, simulations, analysis results, and twodimensional and threedimensionalvisualizations. it also needs to be able to store data files in forms that preserve all of the information (or perhaps alink to the original data), as well as abridged summaries of data in the forms of tables, images, charts, etc. there'salso information that individual scientists enter, such as text and sketches, and information that comes from groupactivities such as presentations, conversations, and planning sessions. an electronic notebook can capture thesewithout a lot of effort. for each of these kinds of data, it's crucial to keep some metadatašinformation about thedata, for example, who made it, when, the chemical system under study, and experimental parameters such astemperature, laser frecollaboratories: building electronic scientific communities133impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.quency, or acceleration potentials. this metadata is the key to a real strength of electronic notebooks. we want thenotebook software to be able to retrieve anything we need from the notebook without paging through it, and alsoorganize that information into useful forms. metadata makes that much easier and more accurate than "full text"searching (which can also be done). the collection of metadata should be as automatic as possible, relieving thescientist from the tedium of recording things like instrument parameters.the doe2000 project is building technologies for just such a notebook, and prototypes are in use today.20this is a collaboration between three national laboratories: pacific northwest national laboratory (pnnl), oakridge national laboratory (ornl), and lawrence berkeley national laboratory (lbnl). defining sharedstandards for notebook data has been a challenge, but the doe2000 notebook has standard apis for adding editorsand viewers to any notebook, and for exporting and importing notebook data. the electronic notebook provides asecure, shared webbased space, interactive input, and rich media types. it is modular and extensible. theprototypes are exploring additional features ranging from sophisticated querying and searching capabilities, toautomated notification of new contents to the collaborators, to mobile, offnetwork use.notebooks are also an essential repository of intellectual property. one of the most pressing issues inelectronic notebooks is that of legal defensibility. technically, signing and witnessing a page of an electronicnotebook (or an object in the notebook) is not difficult. authentication and digital signature technologies beingdeveloped for banking and commerce can handle the job nicely. however, there are aspects of the legaldefensibility of electronic records that have not been tested in court. censa, the collaborative electronicnotebook systems association, is an industrial consortium promoting the development of commercial electronicnotebook systems, with a large fraction of its partners from chemical and pharmaceutical companies. 21 censaaims to more rapidly advance the state of the art in electronic record keeping in ways suitable to largescaledeployment and preservation of intellectual property. one of censa's programs involves dialog with federalagencies and regulators around the issues of legal defensibility.when scientists put information into an electronic notebook, they would certainly like to be able to retrieve itlater. that's not a problem on the short time scale, but what about 25 years from now? the issue is not the media.the information technology industry is good at managing the process of migrating data from one media toanother, as disk and tape capacities grow. the issue is the format of the data. will we have the programs to readthe data in the future? achieving a reasonable degree of longevity will require planning. document storagesoftware providers are going to need to guarantee that their software will continue to read today's formats manyyears from now. we'll also need methods for document translation, methods that preserve the digital signature andlegal defensibility of a document. market forces should drive document technologies toward a solution. scientistsare not the only ones who need these capabilities, so there's a lot of incentive to solve document problems.however, much of the contents of a notebook is data, so scientists have a responsibility, too. we will need toarchive data specifications and/or software applications more scrupulously.realtime interactionsmapping the many ways that scientists interact face to face into the internet world is very challenging(figure 9.3). when they talk about science, many different kinds of media come into play: speaking20 see the doe2000 electronic notebook project web site at <http://www.epm.ornl.gov/enote/>.21 see the collaborative electronic notebook systems association (censa) web site at <http://www.censa.org/>.collaboratories: building electronic scientific communities134impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.collaboratories: building electronic scientific communities135figure 9.3collaborative tools for chemistry research.impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.and gestures, notebooks, notes, sketches on a white board, physical models, and a myriad of computerapplications. people may also come and go during the course of a discussion, according to the need for theirexpertise or as their availability changes. for people to collaborate electronically, all of these kinds of interactionsneed to be supported, with natural, fluid changes among them.realtime collaboration tools work in a complex arena of computers and the internet. they inherently involvemany users, at many places, having many platforms, with many tools. the scientific computing environment isalso very heterogeneous. although there are ﬁpreferredﬂ platforms (macintosh, pc, or unix) in some domains ofchemistry, it is still seldom that a set of collaborators all have the same type. hence, realtime collaborationenvironments need to support multiple platforms. to make matters even more complex, collaboration tools comefrom many sources, so each tool has its own user interface for connecting to other users. this is a level ofcomplexity that nobody wants to deal with, especially busy scientists. therefore one would like a commoninterface that knows how to start up any tool at the click of an icon. finding your colleagues and discoveringactive collaborative sessions should also be simplified through user and session directories.integrating all of the tools together, knowing who is collaborating, and keeping track of what tools are in useis usually referred to as session management. the session manager used in the emsl collaboratory is calledcore2000 (collaborative research environment). it is based on the habanero framework from the nationalcenter for supercomputing applications, ncsa, at the university of illinois.22 habanero is written entirely injava, a portable objectoriented language. java machine code runs on pcs, macintoshes, and many versions ofunix. however, there are enough quirks that java code still needs to be tested on each platform. habanero isdesigned to make it easy to add new tools, by providing an eventsharing model for an application to tell a remotecopy what it is doing, and vice versa. core2000 uses several tools from the basic habanero tool set, including achat box, white board, voting tool, and molecule viewer. core2000 adds other general capabilities, includingscreen sharing (via the emsl televiewer23 ) and internet audio and videoconferencing (via mbone vic andvat,24 and cuseeme25). these have machinespecific code, because there is currently no way to implement themin portable java, though that is expected to change.the emsl televiewer is a general screensharing tool with many applications in collaboratories.televiewer lets users identify any window on their screen, or define any rectangle on their screen, and share itwith anyone else, anywhere. as the contents of that area change, all of the remote copies are updated. becauseonly the compressed changes are sent, the network bandwidth required is typically much less than for video. withteleviewer, other scientists can see exactly what is happening remotely, even if they don't have the same kind ofcomputer. the application(s) being run do not need to be collaborative. one can share a spreadsheet, document,instrument console, etc. this is a powerful tool for activities like mentoring, consulting, support, and sharedanalysis.in the future, session managers will provide more sophisticated floor control. when just a couple of scientistsare working together, it's not too difficult to see when the other person wants to talk or show you something.however, beyond three or four collaborators, additional mechanisms are needed to22 see the ncsa habanero web site at <http://www.ncsa.uiuc.edu/sdg/software/habanero/>.23 p. e. keller and j.d. myers, "the emsl televiewer: a collaborative shared computer display," proceedings of thefifth workshop on enabling technologies: infrastructure for collaboration enterprises (wet ice'96), pp. 1620, ieeecomputer society, los alamitos, calif., 1996.24 see the mbone web site at <http://wwwitg.lbl.gov/mbone/>, and m.r. macedonia and d.p. brutzman, "mbone providesaudio and video across the internet," ieee computer, 27(4), 3036, april 1994.25 see the cuseeme web site at <http://cuseeme.cornell.edu/>.collaboratories: building electronic scientific communities136impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.mediate discussions and the use of tools, managing how control is passed around for driving the visualization,controlling the instrument, or marking up the document.there are important software engineering advantages to having a common collaborative tool framework.core2000 is a fairly comprehensive set of generic tools for collaboration in science. however, there arespecialized tools that need to be constructed to reach "critical mass" in each particular chemistry domain, asdescribed above for the virtual nmr facility. shared, visualization, database access, and instrument control arejust a few examples that are usually specific to the particular kind of chemistry one is doing. common frameworksenable many development groups to contribute to a powerful collaborative toolkit. this approach is necessary tomove collaboratories from a cottage industry to broad application. frameworks available from academic sourcesinclude habanero and tango, a project from the northeast parallel architectures center (npac) at syracuseuniversity.26below the level of collaboration managers, there is the need for other "middleware" to support the distributedapplications for collaboratories. one successful framework of this type is the product realization environmentfrom snl.27 pre is a lightweight, common object request broker application (corba)based, horizontalintegration framework. corba is an industry software standard component technology for hardware andlanguageindependent distributed applications. pre defines how corba can be used to connect distributeddesign tools, databases, files, directory services, and user interfaces. many collaborative tool developers aremoving to corba to address interoperability requirements from a standards basis.impacts of collaboratoriesat this point, it's reasonable to ask how effective collaboratories are in getting chemistry work done. onestudy performed in our laboratory followed many groups and looked in detail at how two groups used the tools andwhat impacts the collaboratories had on their work. 28 one group involved intelligence analysts, and the othernmr spectroscopists. the nmr project is a typical peertopeer collaboration aimed at determining the detailedthreedimensional structure of a segment of a heat shock factor protein. the protein was expressed at lbnl andshipped to pnnl. researchers at lbnl and pnnl then collaborated on experiments using emsl's highfieldnmrs and shared analysis of the threedimensional protein structure. for each project, work activities wereidentified and followed, including experiment planning, experiment setup and monitoring, analysis, and reporting.feedback was obtained through observation, interviews, discussions, and comments.across the studied groups, four broad modes of collaboration were observed:peertopeer, where researchers with a common background and vocabulary work closely together;mentorstudent, where knowledge and experience are unequal, e.g., one scientist is helping anotherscientist or a student to understand a new topicšlecture modes are common;interdisciplinary, where researchers may share highlevel concepts but not a common background, andtherefore must translate results into terms each can understand; andproducerconsumer, where the producer provides information to address a need of the consumer, usuallywithout much common knowledge.26 see the tango web site at <http://trurl.npac.syr.edu/tango/>.27 see the product realization environment (pre) web site at <http://daytona.ca.sandia.gov/pre/>.28 anne schur, kelly a. keating, deborah a. payne, tom valdez, kenneth r. yates, and james d. myers, "collaborativesuites for experimentoriented scientific research," acm interactions, 3, 4047, may/june 1998.collaboratories: building electronic scientific communities137impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.the character of the work varied naturally during the collaborative sessions. as collaborative workprogressed, scientists changed their mode of interaction to suit the task at hand, often several times during thesame collaboration session. thus, it is important that collaboratory tools support fluid transitions between modesof collaboration, as well as the many types of media used in science. some feared that local researchers helping tooperate the instruments would be relegated to technicians. however, this did not happen. there were ampleopportunities for each of the nmr spectroscopists to contribute to the science problem.collaboratory tools are designed to manage information to facilitate collaboration. as scientists used thecollaboratory over a period of time, they noted a highly desirable shift in the distribution of their effort from datamanagement to analysis. they also benefited from the impromptu and informal forms of interaction that thecollaboratory supports. screen real estate is a valuable commodity. as a team worked together for a while, thecenter of attention shifted from looking at each other (video conferencing) to concentrating on the data. often thevideo was eliminated in favor of devoting more screen space to the data under discussion. overall, thecollaboratory supported nonlinear work processes, which increased the productivity of the collaborations.these and other experiences provide a glimpse of the expected impacts of collaboratories on chemical scienceand technology. unlike many technological advances, collaboratories affect both the techniques and the processesof science. this makes their impacts difficult to gauge, and easy to underestimate. at a minimum, collaboratorieswill change where components of research and analysis are done, and how experts are brought into a project. theability to better share facilities across a company or a scientific community will change the equations governingtheir feasibility and viability. the collaboratory's ability to marshal facilities, information, and expertise acrossdisciplines and nations will affect how quickly complex problems can be solved, and thereby what importantproblems are addressed. the roles of a scientist as researcher, mentor, and educator tend to blur in collaboratories.this creates new and exciting opportunities, and some problems. it would certainly be beneficial to expose morestudents to "real" science and science students to better mentors. however, time is one of the scientist's mostprecious resources; the nation's expert on nmr pulse sequences cannot field everyone's questions. however,techniques to help understand the new dynamics and strike the balance are in their infancy. today, the applicationsof collaboratories are still within "sight" of current practice in research. the initial focus has been on implementingcurrent work processes, at a distance, and making the logical extensions to them. however, as these techniquesbecome more familiar, one would fully expect to move significantly beyond current practice, to new paradigmsfor scientific work. this will be very exciting.the promise of collaboratoriesbecause of the work being done in universities and government labs, the chemical sciences are one of thefirst domains benefiting from collaboratories. however, there is a great deal to be done to achieve the promise ofcollaboratories throughout chemistry and the broader scientific community. our experience is limited, and thereare many technical hurdles to overcome. to succeed, chemical science collaboratories need to be developed bymultidisciplinary partnerships of chemists and computer scientists.29 collaboratories are not offtheshelfproducts; within chemistry, each domain has particular29 "national high field magnetic resonance collaboratorium," a report to the committee for high field nmr: a newmillennium resource, published by the national high field magnet laboratory, tallahassee, florida, august 1998.collaboratories: building electronic scientific communities138impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.kinds of data and ways of manipulating and analyzing it, creating the need for specific collaboratory capabilities.to meet the needs of these scientists, collaboratory frameworks must be flexible and extensible. in addition tomaking tools, we must learn to deploy and support collaboratories in the field, and evaluate collaboratory sciencein action. this requires "research by doing," the research and development projects that create and support pilotcollaboratories in the chemical sciences. the nation's major scientific user facilities are a fertile ground for theseinitial projects, with the potential for performing new science while improving facility access and efficiency formany scientists.advances in computers and networking, coupled with new developments like the world wide web and java,have fueled collaboratory r&d; however, continued progress is needed to support the widespread developmentand use of collaboratories in the scientific community. there is much to be learned about the representation anduse of shared knowledge. standards and infrastructure for security and authentication are also important fordistributed applications like collaboratories. the frameworks that form the foundation for collaboratories to sharedata, events, and programs are much more complex than normal internet tools or clientserver applications. thearchitecture and industry/community standards for these frameworks are still an open research issue. as in thepast, the nation's research community has much to contribute to the development of the next generation of internetstandards. the needs of science and business communities differ, and so it will be important for the scientificcommunity to be heard against the background of new internet standards driven by commerce. networkingdevelopments are also primary enabling factors for collaboratories. to scale up the deployment and use ofcollaboratories, we will need higherperformance networks, more scalable and capable network standards, andbetter network management capabilities.conclusionoverall, collaboratories are an emerging capability that will remove many barriers of distance and time in thesciences. the present confluence of developments in computing, databases, and networking creates a uniqueopportunity to develop and deploy collaboratories. the impact promises to be great, not only on what science wedo, but also for how we accomplish scientific endeavors. there are several important drivers for the developmentof collaboratories in the chemical sciences. the opportunity to make more progress on a project, the opportunity toemploy expertise, data, experiments, or computations that would not otherwise be available, and the opportunity tobe first to explore a research question or solve a problem, all represent competitive advantages of collaboratoryuse. collaboratories can also affect the complexity and scale of chemical problems that can be considered.although collaboratories can have value in any size collaboration, collaboratories may be crucial for projects thatneed large or multidisciplinary research teams. collaboratories will expand opportunities for timely informationexchange between basic and applied r&d efforts. of course, collaboratories also provide opportunities to managecosts, by optimizing travel, equipment use, and information value.acknowledgmentsmany people contributed information and concepts to this paper; however, special thanks are extended to mycolleagues lames myers, elena mendoza, deborah payne, kelly keating, nestor zaluzec, larry rahn, and anneschur. much of the work described is supported by the doe2000 program of the mathematical, information, andcomputational sciences division of the office of science in the u.s. department of energy. a portion of theresearch was performed at the w.r. wiley environmental molecular sciences laboratory, a national scientificuser facility sponsored by thecollaboratories: building electronic scientific communities139impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.department of energy's office of biological and environmental research and located at pacific northwestnational laboratory.discussionrandy collard, dow chemical company: for industry to use collaboration and extranets effectively, it iscritical for us to have security in place and to have flexible security as well as encryption. you spoke about that alittle bit with respect to the diesel project. could you talk a little more about the state of that as you see it and whatyou see as necessary?raymond bair: the state of security is not as good as i would like it to be. i think some of the capabilitiesthat are coming out of what people classify as the next generation of internet protocols and capabilities will makethis a lot easier.a number of places have found reasonable success in pointtopoint security by using extant tools likesecureshell and virtual private networks, but the virtual private networks are not trivial to set up and administer,and so it is not a technology that i would advocate, except perhaps for cases where the secure interactions are fairlystatic, as between one institution and another.william winter, sunyesf, syracuse: i have two questions. many of the applications and instrumentsthat you are using have vendorcontrolled software. the first question is, how do you deal with the issue offloating licenses across a network as opposed to just local floating?raymond bair: it depends ultimately on what that license says, and so one cannot predict beforehand. in anarchitecture with the server independent of the instrument, usually you are interfacing with proprietary instrumentsoftware through a meta language that is available for the instrument through serial ports. in that case you are notactually running that instrument's software. but in terms of sharing the screen of the instrument elsewhere, forexample, by using a remote x windows display on an instrument, i am not personally familiar with whether thereis a legal intellectual property issue with that. it is a common enough practice, but the terms would, once again,depend on the particular license.william winter: my other question is a bit more philosophical. yesterday the comment was made thatalthough industry is very much involved with multidisciplinary and team approaches, realistically the ph.d. isgoing to remain an individual effort. do you really think that has to be true? can we talk about havingmultidisciplinary, integrated, team ph.d.s where it still is compartmentalized enough that somebody can takecreditš"i did this"šas an individual?raymond bair: i think so. i see multidisciplinary teams forming around a number of the environmentalresearch areas where experiments in any one domain aren't going to be sufficient to address the issue at hand andwhere collections of ph.d.s have become engaged in addressing a larger and more complex issue by workingtogether in their respective disciplines and sharing information. given the kinds of problems we are trying tosolve, there is definitely encouragement from the funding agencies in the kinds of proposals being solicited in anumber of these areas that almost virtually requires working together, and so we will see examples of thishappening.collaboratories: building electronic scientific communities140impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.10the world wide laboratory: remote and automated access toimaging instrumentationbridget carragherandclinton s. potteruniversity of illinois at urbanachampaignintroductionfor the past several years we have been involved in the development of remote and automated access toimaging instrumentation for an overall project known as the world wide laboratory (wwl).1 there are anumber of advantages to providing remote access to imaging instrumentation. first, it provides access to uniqueand/or expensive instruments without requiring the user to be physically present at the site of the instrument. inaddition it provides the opportunity for collaboration and/or consultation with researchers anywhere in the world,thus providing for a network of distributed expertise. finally, this technology presents unprecedentedopportunities for education and training. these opportunities might otherwise be limited only to those institutionswith the means to support expensive and unique instruments.we propose that there are at least six ways in which remoteaccess technology can be used in practice:service. in the service mode, a local operator can consult with a remotely located principal researcherproviding a specimen and who would provide input about the quality of the images and the parameters tobe used to acquire data. this mode is extremely important for extending service capabilities at thenational research resources.collaboration. in the collaboration mode, the principal researcher using the instrument can consult withother experts from around the world.education and training. imaging instruments can be made accessible in the classroom for k12 educationand undergraduate and graduate training.remote research. the instrument can be used by a remote researcher with minimal local operatorintervention.automated control. the instrument can be used by a remote researcher, and functions normally performedmanually by a local operator are performed automatically by a computer system.1 see <http://wwl.itg.uiuc.edu>.the world wide laboratory: remote and automated access to imaging instrumentation141impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.intelligent control. an intelligent system can perform the same functions as an operator and can learn fromthe researcher's experience.in this paper we discuss our experience with the wwl project and some specific examples that we believedemonstrate the ideas behind a successful collaboratory.2 we address some of the issues involved in providingremote and automated access to instrumentation and its advantages to various categories of users.wwl: current implementations for service, collaboration, andeducationinstrumentation currently supported by the world wide laboratory includes a transmission electronmicroscope (philips cm200), nuclear magnetic resonance imaging spectrometers (surrey medical imagingsystems, varian, techmag), and a video light microscope. all of these instruments are accessible through webbrowserbased user interfaces.remote access to temjavascope is a webbased application designed to operate a philips cm200 transmission electron microscope(tem) and to view digital images remotely. javascope has been written as a client/server application (seefigure 10.1). the javascope applet is the client and presents the application interface to the user. javascoperesponds to actions by the user by sending commands to a camera control server and microscope control serverthat run on a unix workstation attached to the tem and ccd camera. these servers are responsible forcontrolling the tem and digital camera using applications and libraries already developed as part of the emscopelibrary.3 the user interface is shown in figure 10.2.as a readily accessible tool for remote consultation and exploratory grid browsing, the basic javascopeimplementation has been successful. javascope has been used by our collaborators in california (researchinstitute at scripps clinic) to control the tem in illinois and provide advice as to the worth of acquiring data from aparticular specimen. it has also benefited students at the microscope by providing them with a means to consultwith a remotely located advisor.remote access to mrithe second example in the world wide laboratory is remote control of a nuclear magnetic resonance(nmr) imaging spectrometer by means of a web browser. this system evolved from our work in developing adistributed control, acquisition, and processing interface for an nmr imaging spectrometer (4t, 31cm bore) withan acquisition console from surrey medical imaging system.4 this system,2 m. hamalainen, s. hashim, c. holsapple, y. suh, and a. whinston, "structured discourse for scientific collaboration: aframework for scientific collaboration based on structured discourse analysis," journal of organizational computing, 2(1),126, 1992.3 n. kisseberth, m. whittaker, d. weber, c.s. potter, and b. carragher, "emscope: a toolkit for control and automationof a remote electron microscope," j. struct. biol., 120, 309319, 1997.the world wide laboratory: remote and automated access to imaging instrumentation142impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.called nscope, has been interfaced to a unix workstation that provides a realtime processing and controlcapability. the significant features of this system are realtime control of all acquisition and control aspects of themagnetic resonance imaging (mri) system, realtime processing of dynamic mri data, and distributed processingmodules for highperformance computing systems.figure 10.1architecture underlying the javascope user interface.the nscope was initially developed to support functional magnetic resonance imaging and dynamicimaging applications. the modular software architecture of the system has allowed us to adapt the system to otheruser interfaces. for example, the experimentalist's virtual acquisition console (evac) used the nscope systemfor realtime control and visualization of the mri system from within an immersive visualization environment.5this system used voice commands to control the mri system from within a roomsized virtual environment called acave. a realtime stereo capability was also developed for evac that used the flexible processing capabilitiesof the nscope.6a webbased control interface was added to the nscope in early 1996 (figure 10.3). this allows the mrisystem to be controlled from anywhere with internet access using a standard web browser. significant featuresinclude realtime acquisition and processing of images from the mri system;4 c.s potter, zp. liang, c.d. gregory, h.d. morris, and p.c. lauterbur, "toward a neuroscope: a realtime imagingsystem for the evaluation of brain function," proceedings of first ieee international conference on image processing,november 1316, 1994, austin, tx, vol. iii, pp. 2529.5 c.s. potter, r. brady, p. moran, c. gregory, b. carragher, n. kisseberth, j. lyding, and j. lindquist, "evac: a virtualenvironment for control of remote imaging instrumentation," computer graphics and applications, 16(4), 6266, july 1996.6 c.d. gregory, c.s. potter, and p.c. lauterbur, "interactive stereoscopic magnetic resonance imaging," u.s. patentnumber 5708359, 1998.the world wide laboratory: remote and automated access to imaging instrumentation143impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.figure 10.2java applets responsible for camera control (left) and microscope control (right).the world wide laboratory: remote and automated access to imaging instrumentation144impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.figure 10.3webbased control system provides interactive access to all imaging parameters on the mri system.the world wide laboratory: remote and automated access to imaging instrumentation145impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.passwordprotected login system to limit access to authorized users; and scheduling mechanism to limitpermission for use of acquisition capabilities to specific users at selected times.the current system provides complete control of all instrument acquisition parameters from the webbrowser. the web browser interface allows users from various domains and levels of expertise to run the mrisystem without the need for extensive systemspecific training.chickscope: a k12 education project using remote mrithe remote mri system was used in the spring of 1996 in a project called chickscope,7,8 which demonstratedthe feasibility of remotely controlling an mri device through the world wide web. the purpose of the projectwas to enable students and teachers from 10 classrooms ranging from kindergarten through high school to controlan mri system in order to study the maturation of a chicken embryo during its 21day cycle of development. fromclassroom computers with access to the internet, students used web browsers to control the mri system andmanipulate experimental conditions through a simple online form. students could generate their own data and thenview the resulting images of the chick embryo in real time. the objectives of the chickscope project weretwofold. first, we sought to make extraordinary hardware, software, and human resources available to theclassrooms and study the impact of such a system on k12 education. second, we set out to ﬁstresstestﬂinteractive, remote control of the mri system for further development by scientific researchers. overall thechickscope project was very successful in that it was well received by the teachers and students, and there hasbeen a great deal of interest and enthusiasm for repeating the project. in addition, it also allowed us to demonstratethat very complex technology could be used effectively by students at all grade levels.remote instrumentation for service, collaboration, and education:lessons learneduser interfacethe basic requirements for the world wide laboratory in the service, collaboration, and education modes arerelatively straightforward. on the user end we need a network connection and a standard web browser. on theinstrument end we need a network connection and interface software to interpret the commands coming in over thenetwork.there are several advantages to using a web browser interface. first, because almost everyone knows how touse a web browser, there is no need for training on a specific user interface. second, web browsers are nowubiquitous on all computer systems, and third, there are no special software or hardware requirements. as a result,we can be reasonably sure that a remote user anywhere in the world with access to the internet will have the toolsto run the instruments remotely.there are other remote user interfaces that use techniques such as remote screen copy (e.g., timbuktu) orlowlevel distributed windowing libraries such as xwindows. these systems require specialized software to beinstalled and maintained on the remote user's computer system.7 b.c. bruce, b.o. carragher, b.m. damon, m.j. dawson, j.a. eurell, c.d. gregory, p.c. lauterbur, m.m. marjanovic, b.masonfossum, h.d. morris, c.s. potter, and u. thakkar, "chickscope: an interactive mri classroom curriculum innovationfor k12," computers and education journal special issue on multimedia, 29(2), 7387, 1997.8 see <http://chickscope.beckman.uiuc.edu>.the world wide laboratory: remote and automated access to imaging instrumentation146impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.modes of collaborationour experience with the world wide laboratory indicates that at least three instrument access modes need tobe supported:single user. allows dedicated access to an imaging system by a single user.multiple noncooperating users. allows several users to access the system simultaneously. the users arenot aware of each other. commands from the users are queued, and the data are returned to the requestinguser. this mode is useful in education projects like chickscope in which several classrooms may beaccessing the instrumentation simultaneously.multiple cooperating users. allows several users to use an instrument collaboratively by usingmechanisms for passing instrument control among the users.wwl: current implementations for researchthe above examples demonstrate the use of the world wide laboratory architecture for service,collaboration, and education. using this architecture for remote scientific research in which reliance on a localinstrument operator is minimal or nonexistent poses additional requirements that are discussed below.automated/intelligent control for scientific researchour group has been extensively involved in a project involving scientific research on remote and automatedinstrumentation. the goal of the project is to acquire very large numbers of goodquality images from a temcompletely unattended by a human operator. the motivation for developing this automated system arises from thefield of electron crystallography, in which a tem is used to study the structure of proteins at moderate to highresolution (5 to 30 angstroms). the technique most commonly used to preserve the proteins in the tem is knownas cryoem, in which the protein is preserved in a very thin layer of vitreous ice.9 the ice is usually suspendedover a carbon grid, and the goal of the microscopist is to identify the holes in the grid where the ice is potentiallyof the right thickness and acquire a highmagnification image of this area (figure 10.4).a number of practical problems with the cryoem procedure tend to make it extremely time consuming andtedious for the operator. the first is that producing ice of precisely the right thickness is not straightforward; as aresult the ice is quite often either too thick or too thin, and a lot of searching around the grid is required to findsuitable areas. second, because the electron beam is extremely damaging to the specimen and will destroy it after avery short exposure, the grid can be examined only at very low magnification if a reasonable length of time isdesired. the highmagnification image is never examined prior to shooting the micrograph, and this leads, not toosurprisingly, to a rather high rejection rate; many of the acquired images are simply thrown away, and only a fewturn out to be suitable for further analysis. finally, because the beam damages the specimen, the micrographs mustbe acquired with a very small dose of electrons, and the images are very "noisy" as a result. thus, to determine theprotein structure to high resolution properly, the signaltonoise ratio of the structure must be increased, and this9 j. dubocher, m. adrian, j.j. chang, j.c. homo, j. lepault, a. mcdowall, and p. schultz, "cryoelectron microscopy ofvitrified specimens," quarterly review of biophysics, 21(2) 129228, 1988.the world wide laboratory: remote and automated access to imaging instrumentation147impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.requires averaging together many images. the end result is that this technique by its nature requires the acquisitionof large numbers of micrographs, perhaps thousands, or tens of thousands, to achieve high resolution. manualmethods are clearly impractical, and it was with this in mind that we embarked upon the project of completelyautomating the acquisition of large numbers cryoelectron micrographs.figure 10.4acquisition of cryoelectron micrographs. a copper grid (a) is covered with a carboncoated perforated plastic mesh(b). a droplet of buffer containing the protein of interest is applied to the grid, blotted to a thin film, and then rapidlyplunged into a liquid cryogen. the protein of interest © is reserved in its native form in the vitreous ice.as a prototype for automated acquisition of tem images, we have developed a system, called leginon,10 toautomatically acquire large numbers of acceptable quality images from specimens of negatively stained catalase, abiological protein that forms crystals. acquiring goodquality images of this specimen is often used as a test forstudents taking a course in electron microscopy and thus provides an excellent driver for the research methods thatmust be developed to solve the general problems of automated image acquisition. furthermore, as catalase is anordered crystalline structure, assessment of this order provides us with an objective measure of the quality of theautomatically acquired images (figure 10.5). each lowmagnification image (figure 10.5a) is processed to identifylarge contiguous areas of density by a template matching method. image feature metrics (size, mean, variance,centroid) are calculated and stored for each of the identified contiguous regions. these image features are laterused in deciding whether a highmagnification image (figure 10.5b) of the region will be acquired; for example,regions that are too small are rejected. the image quality of each highmagnification image is automaticallyassessed by calculating the power spectrum (figure 10.5c), identifying diffraction spots (figure 10.5d), andmeasuring the signaltonoise ratio of each diffraction spot.currently, the automated system can acquire approximately 1,000 images in a 24hour period. in oneexperiment, we have compared the performance of the automated system to that of a human operator. a total of288 highmagnification images were acquired manually, and 79 percent of these were acceptable as defined by ananalysis of the order of the crystal. in comparison, using the same10 c.s. potter, h. chu, b. frey, c. green, n. kisseberth, t.j. madden, k.l. miller, k. nahrstedt, j. pulokas, a. reilein, d.tcheng, d. weber, and b. carragher, "leginon: a system for fully automated acquisition of 1000 micrographs a day,"manuscript submitted to ultramicroscopy , october 1998; see <http://www.itg.uiuc.edu/techreports/98008/>.the world wide laboratory: remote and automated access to imaging instrumentation148impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.specimen, the fully automated image acquisition system was used to acquire 380 images, of which 51 percent wereacceptable.figure 10.5automated acquisition of electron micrographs of negatively stained catalase crystals.the system can be further improved by adding intelligence to the feature selection criteria. for example,analysis of the results indicated a correlation between average feature intensity and image quality. this featureintensity is related to the thickness of the catalase crystal and indicates that thinner specimens result in moreacceptable images. the fully automated target selection criteria were further refined by incorporating anassessment of specimen thickness into the model. by acquiring highmagnification images of only those featuresthat have an average intensity greater than a preset threshold the percentage of acceptable images can besignificantly improved. for example, if the threshold, is set to 6,000, the percentage of acceptable imagesimproves to 86 percent from a baseline of 51 percent. thus, the automated system does as well as or slightly betterthan a human operator.remote instrumentation for scientific research: lessons learnedour experience with the development of the tem project has shown the necessity for incorporatingautomation and intelligent algorithms into the dataacquisition system. to develop such a system effectivelyrequires a distributed hardware and software environment. the basic architecture of the leginon system isillustrated in figure 10.6. the system has these components:instrument interface. to develop an instrument interface requires information from the manufacturer, anddistributed control is needed for accessing the instrument over a network. this process isthe world wide laboratory: remote and automated access to imaging instrumentation149impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.complicated by the lack of open systems and industry standards. ideally the instrument should requireminimal human interaction during an automated experiment. for example, the time between refillingcryogens on the tem should be extended to support overnight runs.figure 10.6major components of the leginon system.database. it has become clear in the course of the leginon project that there is a critical need for adatabase to support the thousands of images that are acquired and the acquisition parameter data that isassociated with each image. incorporation of a database would provide improved data management aswell as the ability to track acquisition, control, processing, and modeling parameters.processing and analysis. developing intelligent imageacquisition systems requires that the instrument isclosely integrated with processing and analysis software packages. there is a need for integration withcommercial and community software packages, and these need to support the interfaces for distributedaccess.control. a distributed control program must effectively synchronize all of the components of thedistributed system and needs to be adaptable to each experiment. ideally, the control program should beportable between systems and extensible by the end user.user interface. the user interface must be flexible and suit the needs of the user. the system must beflexible enough to support new technologies such as nextgeneration web interfaces and virtual reality.additional features that would be desirable for use in the world wide laboratory include audio and videoconferencing and realtime updates of the system status. these extra capabilities enhance the remote researcher'sunderstanding of the current status of the instrument. we also believe that scheduling and security considerationsare not only desirable but also essential for turning this technology into a practical reality.the world wide laboratory: remote and automated access to imaging instrumentation150impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.conclusionswe have demonstrated that the wwl architecture can be used for service, collaboration, education, andscientific research. the remote instrumentation supported by the wwl is one component of a collaboratory.although the chickscope project was not originally intended to do so, we believe in retrospect that this projectdemonstrated all of the components defined for a working collaboratory. chickscope provided access to remoteinstrumentation from the classroom and gave students access to distributed expertise. all of the participantsactively contributed to and used an image database that is now used by others, and the project served to develop acommunity composed of students and researchers from a number of different disciplines.there is an increased interest in developing the technology to support remote instrumentation. to improve theacceptance of collaboratories in the general scientific community, we need to demonstrate the impact of thistechnology in the scientific research environment and systematically evaluate collaboratories' contribution toproductivity.acknowledgmentsthe world wide laboratory project is a collaboration with several groups at the university of illinois aturbanachampaign, including members of the beckman institute for advanced science and technology, thebiomedical magnetic resonance laboratory, and the national center for supercomputing applications. we aregrateful for the enthusiastic participation of all the many individuals who have contributed to these projects.financial support and equipment were provided by the national science foundation (grant no. 9730056,9871103), ibm shared university research program, informix inc., and the lumpkin foundation.discussionpeter taylor, san diego super computer center: you talked at the end about this issue of culturaladjustment to collaboratories. highenergy physics and both radio and optical astronomy are areas where peoplehave been running consortia and working collaborativelyšand i am distinguishing that from a collaboratoryšfor anumber of years. do we turn to groups like that to find out how to make that cultural adjustment, or do you thinkthe problems there are sufficiently different from chemistry or biology that there is not much to learn?bridget carragher: in truth i don't know, but i think we could look at examples like that to get someexperience of what it is like. but more important than that is to get people involved in using these systems. ifpeople are collecting their data and just doing so much better than they were before, that is going to be picked upby 10 other groups immediately, and there will be a groundswell of support. you knowšit is a wordofmouththing. just to talk about collaborations isn't it a great thing. i think that in my field, this does no good at all. theonly thing to do is to demonstrate that it works. i could say, "it works great in highenergy physics," and peoplewill just shrug. it means nothing to them unless you demonstrate it in their own particular paradigm.william winter, sunyesf, syracuse: i would like to point out to the polymer and materials people in theaudience that electron crystallography is a very powerful technique in that area, as well as protein crystallography,and it has been demonstrated by douglas dorsett in this country and henri chanzy inthe world wide laboratory: remote and automated access to imaging instrumentation151impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.france, among others. the question would be, is it really the best use of very expensive instrumentation to have itdedicated online 24 hours a day for seven days a week for use in elementary school education? i don't know. i amasking you.clint potter: absolutely not, and i guess one of the reasons we do projects like chickscope or bugscope isthat it is a technology driver. putting together the chickscope project, having second graders bulletproof yoursystem, is a great test of how well your system actually works.i think it also gets back to this idea of cultural training. the second graders have no qualms aboutcollaboratories even if they don't know what one is. they are doing it, and they are not set in their ways of doingresearch. maybe it is that generation that is going to be doing the scientific collaborations 20 years from now, ormaybe it will come quicker. we do not have our instruments available 24 hours a day. it is for a specific project.bridget carragher: in bugscope we will have a period of time once a week, maybe, for a few hours foraccess, which is a good thing. i think outreach and training are very good things, but of course we wouldn't want todo that all the time. one of the lessons learned from the chickscope project was that the second graders thoughtthis was entirely natural: why shouldn't they have an mri? why shouldn't they be speaking to researchers fromall over the world and accessing thousands of images? there was no surprise for them at allšno "gee, wow."clint potter: they really didn't feel that this was unusual. that was one of the big things that came out ofthis project. they were talking to the researchers and the scientists, and it was not just a oneway thing. they didnot just suck our energy out of the project. it involved having users at the other end who were really dependent onour system being up for their one shot at it during that day, and having questions coming back. we put together thechickscope project in about a month and a half from scratch. it was exciting, and there was a lot of energy builtout of it because we had these real customers out there.sue fratkin, southeastern universities research association: let me also inform all of you, havingworked with chickscope on the hill, that there is a third element to that project, and that is that the members ofcongress could see, touch, taste, and feel that kind of an experiment and so could relate to it in terms of funding.they could see that the young kids were relating to it, were doing very well. the congressmen andcongresswomen themselves were playing with it because we were online right then and there, and their reactionwas worth all the money because that is where your funding is going to come from. and if they can understand itand relate to it, then you have a much better chance of getting your point across.clint potter: we have never had any funding to do collaboratories. we do it because we think it is actuallyneeded, and even an educational project, we thought, was a good way to prove to our own communities that ifsecond graders could run an nmr spectrometer, then maybe the principal investigator could do it as well.allen bard, university of texas: in getting back to the issue of the collaboratory in the culture ofchemistry, i think the model might be centers for instrumentation. historically there were nmr centers andcomputer centers, and i think by and large they weren't terrifically successful. a little aspect of the chemistryculture is that if chemists can get it in their own backyard they are going to do it, but a chemistthe world wide laboratory: remote and automated access to imaging instrumentation152impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.is not going to go to a center or send samples to centers, and that is really ingrained in the culture. so, i thinkwhere this and the model of highenergy physics and radio astronomy and so on will be most successful is wherethere are instruments that no one is going to have in his own backyard. i am not so sure about electronmicroscopes. maybe very high level electron microscopes might work, but i think that is where you will get acollaboratory. if i can do the experiment in my own backyard or find somebody to finance it, i will probably dothat.bridget carragher: absolutely. and i would always choose to do that myself. i run a big facility with 12microscopes in it, but if people have it in their own network they should stay there. that is where all their stuff is.that is where their colleagues, their students, and their notebooks are. but that is really also the idea of runningthese experiments remotely and automatically. you are just sitting at your desk with all your stuff around you, andyou are not going to a center. and certainly really decent electron microscopes these days cost $1.5 million to $2million each. you are not going to have more than a dozen or so of those in the country. you could havethousands of people doing this. the other idea with running these instruments continuously is that if one costs $2million, you want it up 7 days a week, 24 hours a day. you do not want that machine sitting idle.the world wide laboratory: remote and automated access to imaging instrumentation153impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.11the wired laboratorydavid r. mclaughlineastman kodak companythis presentation describes how kodak has used computers and information technology to enhanceoperations in its research laboratories. this has been an effort to create an electronic or computerized laboratory,and to deliver information at the scientist's fingertips. some perspective is given on what is meant by the "wiredlaboratory" and why a commercial enterprise would be interested in having one. it includes areas of impact,examples from our operations, and some speculation about the future.this discussion is presented from the perspective of an analytical division, within a materials researchorganization, supporting a commercial business. the business environment requires that a profit be made. this isdone by selling more products, by making them at decreasing cost, and by generating new products that sellšmore efficiently than the competition. the materials research organization supports the business by developing newmaterials that can be used to produce new or better products more efficiently. the analytical division contributes tothis efficiency by providing measurements and information that are key to understanding and controlling materialproperties and manufacturing processes. the examples used in this discussion originate from a spectroscopicchemical structure characterization laboratory. the concepts, however, apply equally well for other analytical andchemical information.the drive to be efficient in all aspects of business is intense. the wired laboratory allows research to be moreefficient in the generation and use of information and knowledge. it is these gains in efficiency that have made thiswork worthwhile in a business producing hightechnology chemicalbased products.how have advances in computing technology helped with efficiency inthe analytical chemistry laboratory?advances in computing technology have helped with efficiency in the analytical chemistry laboratory in fourmain ways. the first is through automation and simplification of analytical and synthetic tasks. this area includesthe use of computercontrolled robots and measurement systems and canthe wired laboratory154impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.improve repeatability and increased utilization of equipment. there are numerous examples of applications andvendors of combinatorial testing and synthesis; these are not reviewed here.the second area is in information and knowledge management. combinatorial methods produce largeamounts of data. managing the data and extracting useful information and knowledge from it has been madefeasible through advances in computing technology. even without the use of combinatorial methods, goodinformation and knowledge management is valuable. information provides value over time. previous analyses canhelp with current problems, and historical information can help with the design of new materials and products.making this information available in a usable and timely manner is an important benefit of a wired laboratory.the third area is that of generating and maintaining data in electronic (digital) form. while this may seem likean obvious requirement for modern information management, it is a valuable first step on its own. having data inelectronic form greatly reduces the barriers to its use. much of the time involved in applying modeling andchemometricsšthe analysis of analytical data to extract more informationšis consumed in collating andformatting data. simply collecting and saving the data in electronic form allows more time to be devoted todeveloping more sophisticated calculations.the fourth area is data analysis and chemometrics. one of the general efficiency tradeoffs in routineanalytical measurements is between sample preparation and data analysis and interpretation. analytical techniquesrequiring less sample preparation often produce larger, more complicated data sets that increase interpretationtime. the phenomenal increases in computing power and capacity have helped to reduce that time. in addition, thechemometrics techniques available today yield information not otherwise obtainable. the direct exponential curveresolution algorithm (decra) for separating mixture spectra is an example.1examples from a wired laboratoryquantumšmodel of an integrated spectroscopy information systemin the late 1970s, the molecular spectroscopy laboratory at kodak began to utilize computing technology toimprove the efficiency and quality of structure elucidation using nuclear magnetic resonance (nmr) and infrared(ir), mass (ms), and ultraviolet and visible (uv/vis) spectroscopy data. the ultimate aim was to automate theanalysis of routine samples completely. at that time, our expert spectroscopists would receive a number ofdifficult analysis problems, but would also receive many samples that were routine characterization problems. forexample, did the chemist successfully synthesize the material he wanted? we recognized that we could usecomputers and information systems to make our operation more efficient by automating routine analyses and byproviding tools to aid with difficult analyses.the components of the system that resulted from this project are illustrated in figure 11.1. the completesystem, called quantum, combines spectral and structural analysis software with a sample management system(softlog) and a spectral database (sdm).historically, the system began with the research and development of analysis tools. as john pople mentionedearlier, in order to test our success, we needed to have data. reference spectra associated with chemical structureswere needed to develop and test analysis software for predicting spectra, given the structure, or the structure, given aspectrum. databases of literature spectra were purchased and put into the system, but they were not adequate. themajority of compounds made at kodak have never1 willem windig and brian antalek, chemometrics and intelligent laboratory systems 37, 241254 (1997).the wired laboratory155impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.appeared in the public literature. to test the analysis routines, it was necessary to gather information on kodakspecific compounds. to gather that knowledge efficiently, the structure and associated spectra need to be readilyaccessible to the computer. this information was also needed for the analysis software to successfully eliminateroutine work. it is not practical to enter that information into the computer just to get a routine analysis answer. itneeds to be there for some other reason.figure 11.1components of an integrated spectroscopy information management environment.integrated sample management software (softlog) that incorporated chemical structures and spectral datawas developed to meet this need. these systems are now commonly known as lims (laboratory informationmanagement systems).2 softlog incorporated several important features not typically available in lims. theseinclude full and substructure searches, spectral display and search, easy2 limsource, limsource: the best site for lims and lab data management system info, <http://www.limsource.com/>(1998).the wired laboratory156impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.incorporation of results into the reference database, and a logical interface to fully automated analysis software. inaddition, the user is automatically informed of previous sample or material analyses, inconsistent results, referencedata, and related compounds such as impurities, models, byproducts, or precursors. the notification of otheranalyses of the same material included data from spectroscopy laboratories dispersed across the entirecorporation. this was particularly useful, because it made analysis information initially obtained in a researchenvironment available to analytical laboratories supporting development.there is a lot of interaction between the components of quantum. the analysis software is used to checkthe quality of data going into the reference database and provide predictions for current analyses. the referencedatabase provides models for current analyses and the data necessary to develop the analysis software. softlogprovided the primary daily interface for users and means for gathering information in electronic form. this is alsothe part of the system that has changed the most with changes in user interface and desktop technology.this integrated model of a spectroscopy information system is useful. even though quantum is 15 yearsold, the underlying systems are still in use. the greatest use is now through a netscape browser using the corporateintranet.walkup spectroscopy laboratoryšinstruments onlineas mentioned above, the initial goal was to make the work of spectroscopists more efficient. part of that goalwas to free them from spending time analyzing simple or routine samples. from the company's perspective ofefficiency, the real goal was to reduce the amount of time between the chemist's initial awareness that he neededan analysis and when he got the data or analytical result. the pursuit of this goal has led us to develop walkuplaboratories where the chemists interact directly with automated analytical instruments.the walkup laboratories provide rapid access to highquality, stateoftheart analytical instrumentation via aonestop structure characterization area staffed by experts. the laboratory staff maintains the quality of theinstrumentation and the integrity of both the methodology and the data. they also work to develop new analyticaltechniques and methods to the point where they are robust, rapid, and automated enough to function in a walkupenvironment.one last goal of the walkup laboratory is to provide access to the analytical data and information through asimple, userfriendly, atyourdesk interface. this interface should allow all scientists involved in the chemicalcommercialization process access to all analytical data generated throughout the company.the first technique we provided in walkup mode was nmr. to use the system, chemists enter thelaboratory, place their nmr tube in an empty slot for the sampleloading robot, enter some information identifyingthemselves and their sample on a computer, and select their choice of experiments. a label is then printed, whichis placed on a board next to the slot containing their sample. this label is used to identify the samples after theexperiments are completed. the nmr spectra are usually available over the network, automatically phased andtransformed, by the time the chemists walk back to their office. if needed, the raw data is available forreprocessing.the nmr facility offers a 300mhz varian spectrometer with a 4nucleus probe and experiments including1h,13c,19f,31p, dept, cosy, and hetcor. the autosampler will hold 50 samples that are processed in aprioritized order to minimize the turnaround time for most users. the facility is available 24 hour/day, 7 days/week, with an average analysis time of 10 minutes. all of the data are saved onthe wired laboratory157impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.servers and are available remotely. spectral predictions are available using both kodak and advanced chemistrydevelopment software.these nmr experiments provide information on the chemical environments surrounding individual atoms in amolecule. they can also provide connectivity information such as the number of attached protons or, from thetwodimensional experiments, what protons and carbons are next to each other. this information allows a chemistto confirm the structure of most molecules. chemists recently hired by kodak have all used this kind ofinformation themselves in graduate school and are very comfortable with it. they have also remarked, ﬁif only ihad had something like this when i was in graduate school, it would have saved me so much time.ﬂ that, ofcourse, was our objective.in addition to nmr, the walkup laboratory provides access to advanced ms, chromatography, and irtechniques. the ms system provides a 3,000atomicmassunit range, atmospheric pressure chemical ionization(apci) and electrospray ionization techniques, and loop injection, or short column, separation. data outputincludes averaged, backgroundsubtracted spectra for both positive and negative ions and a theoretical isotopepattern display based on the chemist's proposed formula. this provides a molecular weight and formulaconfirmation in an average of 3 minutes.the chromatography system produces integrated chromatograms at five wavelengths with area percentagesand a diodearray spectrum (160 to 600 nm) for each peak. the average analysis time for this technique is down to15 minutes. one of these instruments will soon be linked to a mass spectrometer system. when this is complete, asingle 10to 15minute experiment will provide concentration, spectral, and molecularweight information onmixtures.the walkup facility in place at kodak is a nice example of how efficiency can be improved. turnaround timefor sample analysis has improved by 7 to 10 times, or probably more from the chemist's perspective. this kind ofroutine analytical analysis is no longer a bottleneck. in fact, chemists often use this facility rather than runningthinlayer chromatography plates because it is just as fast and produces more useful information presented in areadily interpretable form. these efficiency gains have been accomplished by using automation to reduce analysistimes and by placing the analysis in the hands of the people who need the results. it is successful because it ismanaged by analytical personnel who maintain quality, provide training and consultation for the users, and arerewarded for improving the efficiency of others. it provides additional value because the information is savedelectronically and made available to all who need it.improvements in both analytical and computer technology have been critical factors in making the walkuplaboratory possible. the computer and networking technologies provide a robust interface between the users andexpensive analytical equipment. they are used for data acquisition, processing, storage, retrieval, andpresentation. analytical and computing technologies have combined to make a system that is robust and automatedenough for routine use and that has significant business value.electronic information and knowledge managementinformation is data in context. information has value, but only if it can be readily combined with otherinformation. many sciencebased companies have been generating information in compartmentalized laboratoriesin ways that make it difficult to access. paper and peoplebased methods of information management delay ascientist's ability to use this knowledge at a pace consistent with the business need for cycle time improvement andincreased efficiency. this is why electronic access to research information and analytical data is necessary.one area where this efficiency is important is in the movement of new chemicals from research throughscaleup to manufacturing. as a chemical moves from inception to final product, fitnessforthe wired laboratory158impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.use specifications need to be established, and regulatory information must be filed with governments. governmentfilings require gathering all the compositional and safety information we know about a material. knowledge ofthis information for related materials at the time of new research can be used in designing safer materials initially,rather than disqualifying them late in the commercialization process. when fitnessforuse specifications areestablished, all of the byproducts that may be produced are considered. the spectra of many of these are identifiedinitially in research samples. when manufacturing problems occur, quite often the problem is related to a minorcomponent that has been identified somewhere earlier in the commercialization process. having available a trailof information on that material saves a tremendous amount of time in an environment where time has a significantfinancial impact.information and knowledge management helps scientists learn from the previous experiences of others acrossthe corporation. this becomes increasingly difficult as organizations and their collections of proprietaryknowledge grow large and research occurs at worldwide locations. without electronic access to it, efforts to useinformation would be very inefficient. an important area where the need is to access the data, rather than the finalreports and conclusions, is modeling. modeling to develop new and better materials is an important part ofincreasing research efficiency, but it requires electronic access to structures and property data.electronic access to analytical data also helps us to perform analyses the least number of times, maximizingthe effects of previous results. multiple scientists working on the same, or different, projects can make use of theresults of the same tests. data collected for one project may be of use years later on a new project. compoundsthat were not suitable for one application may be good on another or may be good data points for modeling onother projects.wimsšwebbased information management systemthe types of analytical information needed in an analytical information management system include projectand sample information, chemical structures and reactions, reports and conclusions, and spectral and image data.the ideal system would provide an intuitive, easytouse graphical user interface that is platform independent (pc,mac, or unix). it would be capable of easily displaying and manipulating images (spectra, structures, andfigures) along with all other analytical information, would allow easy downloading of data for local reprocessing,and would easily link or crosslink to existing proprietary and legacy databases. in addition, it should be based ontechnology that is widely accepted and not unique to our own environment, is dynamic and continually developedfor improved capability by many other people, is low in cost for software development and maintenance, andprovides worldwide access. about 4 years ago, we realized that "the web is the way" to meet these needs.based on the knowledge gained from the softlog sample management system, the new webbasedinformation management system, wims, was developed for spectroscopy.3 wims has since been expanded toprovide extensive sample tracking and data management across the analytical community. significant advantageshave been gained through data searching and allowing our clients to examine spectral information, reports (as text,word, excel, or html documents), and other analytical data directly. as a sample manager, wims is used to logsamples in and out with descriptive fields customized by technologies, to attach structures, reports, and spectradirectly to a sample or group of3 stu borman, c&e news 75(4), 2527 (1997); douglas brown, antony williams, and david mclaughlin, trends anal.chem. 16(7), 370380 (1997).the wired laboratory159impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.samples, and to calculate throughput statistics. the database can be searched by any combination of sample fields,technologies, and reports. it also allows automated reports to be generated and emailed to clients. thecommercial webbased s3lims product from advanced chemistry development was inspired by these originalconcepts.4figure 11.2typical wims sample view page for nmr.figures 11.2, 11.3, and 11.4 provide some examples of wims displays. a typical display for a sample in thenmr technology area is shown in figure 11.2. in this view, the descriptive sample fields for this technology aredisplayed along with most of the important functional links, like uploading and viewing reports. an importantfeature of this view worth highlighting is the lack of clutter. an attempt has been made to fill the screen with asmuch information as is useful without wasting space that causes users to scroll in their browsers. interestinggraphics that do not add utility may be neat initially, but quickly become annoying to users.other links on the sample view page include links to display associated spectra (figure 11.3) and associatedstructures (figure 11.4). following the associated spectral link retrieves the nmr spectrum from an nmr dataserver and displays it along with the acquisition parameters. the display can be zoomed and printed. the data arealso available for local reprocessing if needed.4 advanced chemistry development, s3lims: spectral laboratory information management system, <http://www.acdlabs.com/slims/> (1998).the wired laboratory160impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.figure 11.3typical wims display of a sample nmr spectrum.the wired laboratory161impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.figure 11.4typical wims display of a chemical structure associated with a wims sample, showing links to additionalinformation.the associatedstructures link retrieves the structures from the quantum structure database and displaysthem with links to other information resources. typical links allow for display of spectra from our referencedatabases, predictions of chemical shifts, and links to other wims samples. in addition, the structure can bedownloaded and used in a local drawing or modeling package. this capability was developed by building a webwrapper around existing software. it makes this information available to a much larger collection of less frequentusers.changes in computing technology have had a significant impact on sample management. the most notableone is the advent of web technology and the realization that the web is the way for user interfaces. the use ofweb interfaces is cascading through all of the chemical property databases in the company. modeling tools arealso becoming available on the web. sets of compounds may be submitted to a calculation server to haveparameters and estimated properties determined.simple interfaces achieve the greatest use. the quantum system illustrated in figure 11.1 was originallyused primarily by experts. when a web interface was added, usage increased by approxithe wired laboratory162impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.mately 50 times. the web interface has put valuable information within easy access of many more users. thisprovides real payback to the company.another point to learn from this experience is the value of information servers working as peers. the wimssystem involves several servers, each providing a service that is linked together via web technology to provide theillusion of one system to users. a chemical structure server put up on the web provides a simple mechanism forother developers to include structures on their web pages. this is an important point, because it has been verydifficult to acquire commercial software that will operate in this fashion. most information system vendorsdevelop software from the perspective that they are the center of the universe. all interactions happen initially fromwithin their software, which is always in control. their software will not operate as a peer.the electronic laboratory notebooka model of the datatoinformation pyramid is shown in figure 11.5. a major research program may involvemultiple projects, each with several experiments that may produce several samples requiring many tests that canproduce lots of results. the amount of data present at a given level increases toward the bottom of the pyramid.consequently, the bottom area has been first to make use offigure 11.5the datatoinformation pyramid.the wired laboratory163impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.computing technology to help with its management. so far, the examples presented here have focused on results,tests, samples, and their associated information management and data analysis tools.over the past 2 years, work has progressed at the kodak research facilities in england to apply computingtechnology to the next levels up in the diagram. specifically, an electronic laboratory notebook has been developedto assist with the management of experiments, projects, and programs. above this level are program measures andsummaries useful for research management. these have not yet been addressed, as the traditional methods ofproviding summary reports and presentations will likely be adequate for several more years.the electronic laboratory notebook (eln) at kodak is implemented as a collection of lotus notes databasesand applications that enable the electronic storage and retrieval of experimental aims, methods, results, andconclusions. a few years ago, kodak decided to switch to lotus notes for email, making it a reasonable choicefor this development. the eln provides an environment that facilitates the sharing of information across researchby means of controlled database access, searches, and hotlinks. it supports the principle of entering new data onceand only once.before there was an electronic sample management system, chemists would submit analysis requests bycompleting paper forms. with the advent of an electronic system, some chemists would enter the information intothe analytical data system and some would still submit paper requests that an analytical technician would enter.the paper request forms for spectroscopic analysis included space for a chemical reaction to be entered. this isuseful for identifying impurities in structure characterization problems, but there was never enough added value toenter it into the analytical systems. now the entire reaction and experimental conditions are maintained in the elnand are available to the analyst through a hypertext link. the information is captured and maintained at its originalsource in a way that is useful to the originating scientists. now that it is in electronic form, the information can beleveraged throughout the corporation.there are four main lotus notes databases underlying the eln. they are used to store experiments, projectsand programs, reports, and summaries. each database has templates for creating entries. summaries can begenerated automatically by tools in the eln or entered by the scientists as part of their experiment. it is expectedthat this database of summaries will provide an important means for searching the elan.an example experiment from the kodak eln is shown in figure 11.6. this is a page from a typical organicchemist's notebook. it shows the aim, chemical reaction, experimental details, results, summaries, and security.the security functions allow the author selective control over who can read and modify the document. there arealso areas for entering or attaching any information or file the chemist wishesšin this case she added informationabout the starting materials. it is probably worth noting that this page closely resembles what the organic chemistwould have entered into her hardcopy notebook.notice that the chemist performed a number of walkup tests herself and included the results. there was somequestion about the mass spectroscopy results, and an expert analysis was requested. the result of that is entered inthe eln as "1 component with correct fragmentation for product" with a bookmark to the complete report, whichis shown in figure 11.7. the full report shows the reaction from the eln, the spectra, and the comments from theexpert analyst who adjusted the spectroscopic experimental conditions to obtain the result.the electronic laboratory notebook is envisioned to be the tool of choice for scientists to log experimentalaims, results, and conclusions. it is expected to enable knowledge sharing while maintaining security, allowinggreater collaboration between researchers and increased productivity. there are good reasons to believe that it willbe successful. first, computing and computing technology have progressed to a point where a successful eln canfinally be delivered. second, and most important, thethe wired laboratory164impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.figure 11.6display of a page from a chemist's electronic laboratory notebook, showing a bookmark to an ms report.the wired laboratory165impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.figure 11.7the ms report linked to the electronic laboratory notebook page shown in figure 11.6.the wired laboratory166impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.eln prototype was built at the request of chemists. once the prototype was in place, 80 to 90 percent felt itwas good enough to use and would recommend its use to their colleagues. this desire is widely shared among theresearch scientists. it is a reflection of the strong drive for continually improving research efficiency and thegeneration of more knowledge per unit time.the futureinformation managementin the wired laboratory of the future, all scientists will use an intelligent electronic laboratory notebook linkedto all datagenerating equipment. this will automatically provide all relevant information to the scientist andcapture all of the knowledge that is generated. it will allow research to progress using sophisticated experimentaldesign and modeling. most materials will be modeled before they are made. in the intelligent electronic laboratorynotebook, objects will be recognized as they are entered and linked to underlying databases automatically. theenvironment will be built with integrated links between systems communicating as peers. much of this vision willbe become reality over the next 5 to 10 years. although electronic laboratory notebooks have been discussed andprototyped in the literature for some 10 years, there is now enduser pull for them, and computer hardware andsoftware technology can now support them. they will drive much of the electronic laboratory of the future.success, however, will require the development of better search systems. knowledge is information incontext. searches through large information resources must provide good contextual searching and answer setrefinement. without such tools, so many false positives or irrelevant answers are returned that the results areuseless to the researcher. the summary database in the eln may help with this problem. it is also an area of activeresearch and development driven by the need for web search engines.information systems will also benefit from better input devices. they will make it easier for scientists tointeract with computer systems, particularly when generating information. these devices will be easily carded intothe laboratory environment and written or drawn on, like pages in a paper notebook. interfaces will allow chemiststo draw their chemical structures on a sheet of paper and have them upload directly into the computer with aconnection table that is searchable. there are already products on the market that are consistent with thisdirection. one example is the cross notepad, 5 which allows information to be written on a portable tablet anduploaded as an image. it comes with software that can be trained to convert neatly written words and phrases totext.the continued move to information systems will result in a nearly paperless laboratory. it will no longer benecessary to print reports so that they can be mailed or archived as they are today. this prediction applies to thelaboratory and not to the office environment. people will still print information until another medium is found thatis just as convenient and costeffective. the need to archive information in printed form will be replaced by digitalmeans.some obstacles exist for information management. one is the need for the current rate of increase incomputer processing and storage capacity to continue or accelerate. the information systems will need to storestaggering amounts of data. perhaps a more limiting obstacle is the lack of good commercial software. softwaremust be available at reasonable cost and quality that is simple to support and maintain and is not wasteful ofhardware resources. software from multiple sources will need to5 a.t. cross company, cross pen computing group: the pad, <http://www.crosspcg.com/crosspad/> (1998).the wired laboratory167impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.cooperate in a peertopeer manner (standards may help with this). better methods for dealing with softwaredevelopment are needed to improve quality and reliability and to deal with disruptive changes in technology.unfortunately, the realities of the current software market do not appear to reward quality adequately.differentiation and time to market seem to drive the greatest shortterm profits.data analysis and instrumentationanalytical technology in combination with dataanalysis techniques will continue to advance, reducing thetime required for sample preparation and data interpretation. in some cases, current analyses will be replaced bydrastically different technologies that, perhaps after more calculation, yield the same or greater informationalvalue. there will be a strong drive toward smallscale automated syntheses and testing. there will also be moreapplications of online and inline sensing and control, with analytical instrumentation connected to networks andcontrolled remotely. these applications will extend the concepts of speed and robustness demonstrated in thewalkup analytical laboratory toward smaller size and price.success in these areas will likely involve greater use of embedded systems and plugandwork components.proposed extensions to javatm may prove useful in this arena.6 increasingly, instrumentation will be controlledthrough web interfaces. several companies are beginning to produce lowcost boxes that can be used to connectalmost any device to the web. one example is advertised as the world's smallest web server.7 the complete webserver is less than 4 inches square and is configured to provide realtime weather data from cambridge,massachusetts. it is designed to be simple and low cost. these devices may be excellent alternatives to pcs orunix boxes for connecting instruments to the network, particularly from a support perspective.as more information becomes available only in electronic form and computerized processes become acritical part of business processes, dependence on the network increases. a robust, highbandwidth networkbecomes a requirement. this may be more a cost issue than a technology issue.virtual reality techniques have been studied for decades. they provide excellent mechanisms for people tounderstand and interact with visual information. particularly useful applications of virtual reality have occurred forfighter pilots and people with disabilities. virtual reality is a tool that uses the senses to transmit a lot ofinformation to the brain quickly in the form that it normally processes. it is meant to help unlock the power of thehuman mind. interfaces common in the laboratory today are very poor by comparison. widespread use of virtualreality is limited by our ability to collect appropriate information to display in this mode.tandem techniques that produce multidimensional data are now common in analytical chemistrylaboratories. they are useful because they generally use a small amount of sample, require minimal samplepreparation, and have relatively fast analysis times. at the same time, they generally produce large amounts ofdata that often require relatively long interpretation times. techniques that produce information in threedimensional space, such as 3d nmr, are easily understood when viewed through virtual reality techniques.at present, it is not clear that virtual reality is necessarily the appropriate tool for understanding much of themultidimensional analytical data produced today. this is unfortunate, because computer6 sun microsystems, inc., jinitm technology lets you network anything, anytime, anywhere, <http://java.sun.com/products/jini/> (1998).7 phar lap software inc., the world's smallest web server, <http://smallest.pharlap.com/> (1998); dr. dobb's journal,october, 4046 (1998).the wired laboratory168impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.monitors are also inadequate for displaying this data. spectral interpretation often involves the fine detail spreadacross a wide, highresolution xaxis. a computer monitor allows only a small portion of the detail to be viewed atone time.there are some areas where scientists would like to ﬁseeﬂ results to understand them. these include seeinghow molecules are interacting with each other or flowing through processing equipment. the challenge to theanalytical community is to devise realtime measurements that, when displayed with virtual reality, will enablethis understanding.summarythere is a strong business need to generate new products with ever increasing efficiency. automation of thedata acquisition and information management functions of the laboratory can increase the efficiency with whichthe knowledge generated by research is applied to new products. this arises through increased ease of use,accuracy, and quality of information and knowledge made possible, in part, by the advancements in computers andcomputing technology.as systems are developed, it is important to remember to keep them simple. they must be simple to build,simple to use, and simple to maintain. software technology changes relatively rapidly. keeping it simple helpsincorporate new technology faster, deal with obsolescence, and deliver functionality to users faster. rapid deliveryof simple systems returns the greatest value.the web is the way for user interfaces, as long as the web belongs to everybody. the peertopeer model ofthe web is an excellent way to build sophisticated information systems from simple components.goodquality, reliable software available at reasonable cost is one of the most critical needs for the future.contributorsmany people have worked on the wired laboratory at kodak, mostly part time. they have contributed to thedevelopment, philosophy, recognition, and acceptance of the value of working electronically. they are as follows:brian adams, christine alvarez, brian antalek, gustav apai, todd beverly, derek birch, caroline d. bradley,doug brown, don bushman, juris l. ekmanis, nancy ferris, tammi flannery, john flynn, susan m. geer, joanm. hessenauer, j. michael hewitt, peter horne, andrew j. hoteling, thomas c. jackson, emily jones, thomas f.kaltenbach, philip lafleur, mary lee lasota, william c. lenhart, ilia levi, thomas marchincin, williammckenna, david mclaughlin, frank m. michaels, stephen d. miller, wendy f. miller, peter monacelli, dominicj. motyl, vi neri, ian newington, william f. nichols, ed osborne, alan payne, julia pich, bob price, ted sears,craig shelley, john p. spoonhower, john trigg, john paul twist, jon waterhouse, luann weinstein, antonywilliams, willem windig, barry wythoff, and agyare yeboah.discussiondavid smith, dupont: david, i am very interested in how this walkup lab is located with respect to thecommunity that it is trying to serve. i have worked at kodak park, so i have an idea of how large it is. it is as largeas the experimental station, larger in fact, and frankly, i have a hard time imagining that a chemist is going towalk from one end of the experimental station to a walkup laboratory to get a sample back in 10 minutes.the wired laboratory169impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.so, the question is, do you have multiple instantiations of this laboratory in various parts of kodak park, or is itjust located in one place with a highdensity of chemists in the area?david mclaughlin: the walkup facilities are located in areas where there are high densities of people thatneed to use it. so, for example, in the main research complex where a lot of synthetic organic chemists arelocated, there is a concentration of analytical tools for structural characterization. in kodak park where the scaleup and delivery to manufacturing operation occur, there is, also, a lot of synthetic work that goes on. so, we have avery similar facility, but with techniques that are suited specifically to that environment. in yet another set ofbuildings there is testing of emulsions and photographic properties.so, the walkup facilities are distributed where the need is. when we first put up a walkup facility there wasonly an nmr. we put it on the second floor. there was another nmr on the third floor. the chemists on the sixthfloor would come and use the one in the walkup facility on the second floor. the ones on the third floor wouldgenerally use the one on the third floor, even though it was an old instrument and gave poorerquality results. theyused it because it was convenient. convenience is a big part of it. it is similar to the ease of use i described for theweb interface. if you create an easy, simpletouse interface, then all of a sudden lots of chemists will begin usingit. so, you do need to locate the walkup facilities close to where they are needed.the wired laboratory170impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.session 3panel discussionsam kounaves, tufts university: i have two questions. first, a comment directed to the people sponsoringthis conference. since it is on computers i think it would be interesting, if it is possible, for the participants tomake some of their slides available on a web site for the rest of us to use. most of us are going to go back to ourdepartments and to our institutions, and some of us would like to present this information to a wider audience.having a simple powerpoint summary, credited appropriately, that we could integrate into our own talks would bevery useful for distributing this information more widely.my question deals with an issue that is going to arise again in this afternoon's talk, that is, archiving. when idid my ph.d. thesis years ago, i did it on an apple ii and i even kept some of my lab notebook on an apple hcomputer. when we wanted to go back to it one day to get some information, it was practically impossible. i hadto find an apple ii computer, plug into it, and try to get the information out. several of our speakers this morning,and i guess all of them in some ways, implied that there was software that they had been using to archive theinformation, lab notebook, lotus notes, etc. i am still wondering how this is going to work out in the future.i know one way my thesis can still be available, for example, is through an institution that is dedicated toarchiving. university of michigan microfilms, for example, is, in theory, still archiving that information and willeventually switch over to a digital format, and it will still be available. what are your thoughts on archivinginformation? how can we go about doing this so that the lab notebooks are useful years from now? are there anyways that you can see this happening, or do you have any thoughts on this process?raymond bair: there are a couple of approaches that people are taking, largely along the lines you mightexpect. one approach is to require the makers of the notebook or document management system to provide acertain degree of compatibility with future formats, and future versions of their product. this kind of requirementis coming out of some of the commercial electronic notebook efforts. this is going to be mandated by the peoplethat are buying these notebooks, the large companies.however, that doesn't solve all of the problems; it just addresses the document managementthe wired laboratory171impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.company's formats. there are some interesting challenges ahead. for example, what if you stuck another kind ofdocument into this commercial notebook or document management system? they are not really responsible for allof the different kinds of documents and data you might use. there's also an issue of progressive conversions, forkeeping file formats up to date as time passes. that brings in issues of fidelityšif we convert files, how can weassure ourselves that the converted objects are still correct? there are challenges with electronic signaturesystems, too, since you have signed the original binary file, which has not been translated. so, what does it meanlegally when you convert that file in the future? how do you retain that authentication that you had in the past?so, in addition to the format issues there are also issues of authentication.by the way, my slides are on the web at <http://www.emsl.pnl.gov:2080/docs/collab/presentations/ppt/csr/>.bridget carragher: i think this is a problem. you cannot even read a microsoft document that is one versionbehind the version on your desktop. if you cannot do that with microsoftšwhich is probably the most ubiquitoussoftware aroundšwe are in a lot of trouble. but i think we are not the only ones facing this problem, and i thinkagain, the scientific community isn't going to drive this problem. this is a huge problem for the world as the worldmoves onto the web, and i think there are going to become tools that do automated updating. but where is yourthesis now? probably a printout somewhere is your real evidence that you wrote it, and i think that in part willcontinue to be the case.clint potter: in a sense i think you also have to throw stuff away because you cannot keep everything.perhaps you don't have the original data for your thesis anymore. so, you have got to be smart about what yousave and think about what the things you save arešthe things that go into libraries or university microfilmservices.bridget carragher: you publish things that you want to keep. the publishing record is partly what is there.david mclaughlin: i think you should try to save the information in a format that you can easily moveforward. the more open the format, the better. if you store your word documents in rich text format, that maygive you more forward viability than a binary word document. we often store spectral data in an ascii formatthat is easily readable. it takes more space to store it in ascii, but we know we can move it forward. animportant part of our plan is to convert the format of our information as future versions of software may require.david smith, dupont: we said several times during the course of the meeting that software is important,and perhaps just as a sanity check for myself, i would like to ask susan about the componentbased, objectoriented paradigm for software development. from your viewpoint as a computer scientist, is this really a viableapproach for the future development of software or is it just a fad that is going to disappear in the next 5 years?susan graham: i think it is more than a fad, and the reason is that structure and structuring are very, veryimportant, and this provides a structuring mechanism. objectoriented programming is just a structuringmechanism, and if you get the structure wrong then it is going to be just as bad as any other disorganization. but ithink it is an approach that is only now possible because it requires more heavyduty computing and in particularimage sizes. storage is used much more, and so the ideas are actuallythe wired laboratory172impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.quite old. the ideas of objectoriented programming are from the 1960s, and the technology has now caught up tothe point where it is viable.so, i think it is going to evolve, but i actually think it is a step forward.david smith: in the presentations we have heard the word "complexity" used quite often, and yet i haven'theard anyone mention, for example, the work on complexity that is going on in the santa fe institute, such asconcepts like autonomous intelligent agents in the software field. does the panel believe that they will have anyreal impact on the class of problems that confront us?susan graham: clearly they have impact. there are opportunities there, and there are risks, and the risk withautonomous agents is that you no longer have control, and so, with the best of intentions the agent may be gettingin the way, particularly if it is imperfect.those are all ways of managing intellectual complexity, and that is one of our biggest limiting factorsšthatit is hard for a person to get his head around everything that is going on, and the more some of those issues can becompartmentalized, the better off we are going to be.raymond bair: there is no question that agents are going to have value in doing a number of useful things.however, tom finholt's hype curve comes to mind (from his talk last night). there is a considerable gap betweenthe reality of what can be done now with intelligent agents, and some of the talk about them.thomas finholt: the digital library projects are a good illustration of the gap between reality and sciencefiction, if you will, with regard to intelligent agents. particularly in the michigan digital library projects, thestrategy has been to use an intelligent agent architecture for organizing bodies of information. i don't do that work,but i have followed those projects closely. today, there is a huge gap, in my opinion, between the prototypeapplications that have been demonstrated and systems that will stand up to the rigors of everyday use (i.e.,operational production systems). i think we can say that with respect to intelligent agents, we may be where wewere with objectoriented coding in the 1960s and 1970s. that is, the software architectures are not there yet totruly implement the idea, but the further development of intelligent agents is definitely something to monitor forthe future.stephen heller, national institute of standards and technology: just a couple of comments aboutarchiving, which i think is more a red herring than anything else. in fact there is no obvious ultimate solution,because of the changes in technology, so i would like to ask a question of the panel. how many of you actuallyhave a real printout on a piece of paper of your bank statement or actually have physical stock certificates asopposed to all this stuff being stored somewhere electronically?my feeling is that between the stock market and the bank accounts in the world, most people have fairlysignificant concerns about their resources, and concern about some of these scientific lab notebooks probablypales in comparison to the amount of concern people would have with problems with those financial resources.i don't think people walk into their banks and ask for proof that their bank accounts are being properlyarchived, and their dividends and stock certificates are properly recorded.so, it is a questionable issue to bring up at this point and for the foreseeable future, i think.susan graham: i have a question for the people who were talking about electronic notebooks. one of thepurposes of an electronic notebook is to have a historical record that is used, among other things, forthe wired laboratory173impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.establishing priority and for integrity concerns in science. once the record is electronic, what are the safeguardsthat you are using to make sure that you have the benefit of binding and the benefit of the fact that you chemistscan analyze the page to see whether it has been altered and things like that?raymond bair: i am not quite clear on what you mean by binding.susan graham: traditionally the notion was you didn't use a looseleaf notebook; you used one with abinding so that you knew the order in which the record had been kept.raymond bair: the approach that we have taken in our notebook conforms to the traditional model. if youwould like to remove an item in your notebook from view you may delete it, but it doesn't go away. it becomes anicon, and it says, "deleted," but you can retrieve what was there. there is a genuine need to be able to mark outstuff that was wrong, for example, so you do not get confused in future searches. however, that doesn't solve allthe problems scientists have. there is a genuine need for something we haven't fully developed a concept for, ascratch pad of sorts: temporary information that exists for some intermediate time before it is canonized in anotebook. people are still working on concepts like this.participant: how do you prevent altering of the notebook?raymond bair: you prevent alteration with the same kinds of technologies that electronic commerce isadopting to prove that you are an owner of a transaction. you can compute a hash code of an object of any size,and use public/private key technology to validate that the document has not been changed. this is your digitalsignature. you can also use a trusted time authority, along with your document hash and public/private keys, toestablish an unchangeable date for the signature.susan graham: but my question was actually prompted by something david said in which he explained howbeneficial it was to have links. if you have links and particularly if you have urls, then how do you know that thedocument you are referencing hasn't been changed?raymond bair: if you are really going to have this for a record, for example, to determine priority, youcannot put a link to something that is temporary in the notebook.david mclaughlin: before devising a solution to this problem, i think you must give some consideration tothe amount of effort it will take versus the need to prove the case you propose. for example, i have heard of caseswhere scientists have published fabricated results. from a scientific perspective, results are not considered validuntil somebody else has repeated them.patents are used to protect intellectual property. with the exception of the united states, the critical date iswhen a patent application is filed, not the date the discovery was made.in the united states, the date of invention is often established using laboratory notebooks. the primaryrequirement is that the pages be dated, signed, and witnessed. it is fine to keep the notebook pages in a looseleafbinder. one pragmatic way to deal with the legal issues of electronic laboratory notebooks is to print out eachpage, including all the links, sign it, date it, witness it, and put it in a binder. most of the lawyers i have spokenwith believe that this is not really necessary. they believe that the patent office would accept an electronic labnotebook when a log is kept of every modification that is made. the logs can be written to optical disks with a dateand time stamp and, if warranted, a digital signature.the wired laboratory174impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.if every change you make to your notebook is written into a log that you do not have access to, then itbecomes very hard to fake entries. deception would probably require a conspiracy, more than one person. i see noneed to make an electronic notebook any more tamperproof than a paper system. restricting unauthorized accessto the information is of greater concern.stanley sandler, university of delaware: i am concerned about remotely operated or weboperatedequipment, because we had problems in our department with, a word i haven't heard here yet, a hacker.there is great potential for a hacker to unknowingly cause equipment damage or real safety problems. nomatter what degree of security we have, there is always a hacker that is going to be able to get through. how doesone protect oneself and one's equipment?bridget carragher: you cannot. you can do the best you can, and again, adopt all the tools that areavailable. we passwordprotect our instruments and we take various precautions like that, but in truth you cannotguarantee protection. but most of the interfaces we can build using web browsers are pretty failsafe. we have hadkindergartners using these instruments, and they do not obey the rules. they bang on all the buttons and hiteverything at once, and maybe the high schoolers are even worse. they treat an instrument as a video game. so,you can protect your instrument by your user interface and disallow things that would be dangerous. i think that isthe more important thingšto build in those safeguards in the software.clint potter: i think you could take the essentially same steps taken in the security world for workstationsand computers. as that technology gets better, it can be incorporated into remote instrument technology. i don'tthink we should be inventing new security mechanisms.bridget carragher: no, we will take advantage of whatever is out there, but we have 100 machines in ourfacility. they get hacked into all the time. there is nothing we can do about that except deal with the problemwhen it comes up.john pfeiffer, air products and chemicals, inc.: let us take that question one step further if we can. oneof the things that you at illinois and at kodak are doing is making very sophisticated tools and very sophisticatedmethodologies accessible to knowledgeable but maybe not expert users. so, a risk is that the knowledgeable userwill abuse the capability unknowingly. how do you reflect on that? how do you, if you will, put some bounds onthat as you provide these tools via web interfaces or whatever easytouse interface?bridget carragher: you mean they can gather data and misinterpret it?john pfeiffer: exactly. one can complete a statistical analysis that is invalid, extending beyond theassumptions built into the technique, and then the scientists may draw incorrect conclusions.bridget carragher: you can do that right now. you can sit in front of an electron microscope and twirlthose knobs and get it completely wrong and yet believe the data you are getting. i don't think that is any differentwhether you put an intelligent or unintelligent user interface in the front of it. we have had this argument manytimes in my communityšyou know, that if you make it too easy to use, everybody will come along and misuse it. ithink that is not so. i think that if you make it easy to use, you can help people understand what they are doing.you can make it much easier to repeat things using differentthe wired laboratory175impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.parameters. you know, if you are sitting in front of that instrument, and it takes you 3 days to get the data, you aremuch more inclined to believe the results and not try to repeat them than if you can just say, "oh, i will just rerunthis experiment and check it again with three different parameters."so, i absolutely don't agree that making things easy to use necessarily lets them be more abused. i think youcan abuse data any way that you gather it.clint potter: i think the same thing is shown in molecular simulation packages in the sense that you don'thave to write your own code anymore, and you don't have to understand the exact details of all the algorithms, butpeople are using these things, i guess. i don't know anything about chemistry.raymond bair: also, i think that distance from the user of the instrument doesn't absolve you from doingsome training with that person. you're trying to accomplish the same kinds of things electronically that you woulddo if you had that person visiting in your lab. the training requirement doesn't go away just because theinstrument is remote.bridget carragher: but we are not paternalistic about it. if people want to use the instrument, they shoulduse the instrument. it is ultimately the scientist's responsibility, just as it is now. i don't think we know how remoteaccess changes the way data are gathered except to maybe make it easier.david mclaughlin: from a walkup lab perspective, the question could be stated as, is it appropriate to takean expert analytical chemist out of the loop given that then the end scientist could misinterpret the data? i believethat scientists have a vested interest in not misinterpreting the data that they obtain, because proper interpretationhelps them continue with their work and meet their goals. one practice that we follow is to place the walkupfacilities right next to the analytical experts who are working on the more difficult problems. there is alwayssomeone generally available during regular working hours to answer questions. we also require training beforeanybody can use the instruments and offer training classes on how to interpret the data, usually once a year. whilethe training helps meet some of that need, i think the solution still is having an expert available and approachable.this approach is similar to the examples of collaboratories discussed here that allow you to send email andestablish working sessions with an expert. in our case, the expert is physically nearby, making it very easy for aperson to ask a question. that is how we try to avoid misinterpretation of data.william winter, sunyesf, syracuse: i wish it was that simple, but i don't really believe that it is. iremember that when the first pc versions of things like mm2 came out, an organic chemist in my departmentcame running up to me with a picture he had drawn on his pc plotter clearly showing a planar cispeptide linkagethat he had obtained and claiming this must be right. actually, it was an nacetyl glucose linkage but it was thesame idea, and it should have been trans. my colleague's conclusion was that because it came from a program from arespected person the result had to be right, and that was the end of it as far as he was concerned. similarly, wehave to do something to make people question these things and not think that just because it comes from aninstrument on the web it is right.david mclaughlin: really, i think that some people will always believe that if the computer tells you it isso, it must be so. for these people, i agree that the problem is very much an educational issue. it is also an issue ofthe quality of the software or instrumentation and its use. we attempt to make all of the techniques in the walkuplab environment quite robust.the wired laboratory176impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.bridget carragher: but not kindergartners. they do not believe anymore. they live on the web and theydon't trust any of it.clint potter: i guess an issue is people using software now that they would never have been able to usebefore because it wasn't on the pc, and that they know about these mistakes and have learned about thesemistakes, so maybe it is just an education issue.bridget carragher: it is an education issue, yes.the wired laboratory177impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.12chemical data in the "internet age"w. gary mallardnational institute of standards and technologyintroductionit is difficult to determine whether discussion of the internet as a force shaping the way we work is growingfaster than the growth of the internet itself. however, within chemistry and chemical engineering the use of theinternet as a resource for communication is exploding. scientific publication on the internet is just beginning. theuse of the internet as an information source in science is also still in its infancy. this paper discusses the changesthat are driving the growing use of the internet and what needs to be done to ensure that the new resourcesemerging fulfill the needs of the chemical community. three factors can be identified as the primary drivers:1. reduction in traditional data resources. the loss of funding for a number of activities that providedinformation to chemistsšcuts in library budgets, reductions in central research laboratories byindustries, changing funding priorities at federal agenciesšhave all led to a reduction in the methodsfor finding needed data. many libraries have had to cut out information specialists just when theincreasing costs of journals have forced users to spend more time finding data. it is no longer possiblein a number of large chemical companies to call on a department that specializes in physical propertymeasurement and estimation. it is difficult to find funding for detailed critical evaluation projects,especially in the area of thermodynamics, thermophysical properties, or kinetics.2. demand for faster access to data. the need to obtain more information faster is not new, but theability of computer databases to supply that information in new ways has driven a desire for ever moreinformation. there is a growing sense that information should be available instantly, even if the realneed for it is far more long term. in addition, the use of new drag discovery techniques and thedevelopment of substructure searching have also fueled demands for more information about a largerset of compounds.3. increase in need for data for modeling and simulation. the use of modeling has dramaticallyincreased the need for data and, as is discussed below, has changed the nature of the data needed.simulation of combustion, the atmosphere, urban airsheds, and chemical reactors has requiredextensive data on kinetics, transport properties, and photophysics.chemical data in the "internet age"178impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.data needsthe type of data needed in chemistry is changing. the traditional data requirements were for limited sets ofdata that were used to create correlations, to provide estimates, to test theories. this was a ﬁretailﬂ version of datausage. in industry, government, and academia, this work was typically done by individuals who had a strongbackground in the underlying physical principles embodied in the data. errors in transcription were clear, and baddata usually stood out because data were used typically in sets and plotted against other related data. the datacorrelations were often extended to domains where measurement was either difficult or expensive. predictionswere made from the correlations but again, the background of the practitioners was such that the fundamentalphysical principles and "reasonableness" of the data were uppermost in their minds. the errors were mostly wellappreciated, because the underlying science was closely coupled to the data analysis. the use of the resulting datawas related to the confidence that the data were correct or at least that a firm understanding of the bounds of theuncertainty existed.the use of modeling and simulation has placed new demands on data resources. these result in part from thedifferent and often more complex systems that are being modeled, but also in part from the new requirements forcomplete data sets. the need for completeness comes about from the very nature of modern modeling programs,which take all aspects of the physics and chemistry into accountšat least in principle. since all physical andchemical processes are included, it is necessary to have data for the parameters that are used in describing theindividual subprocesses of the model: diffusion coefficients, heat capacities, heats of formation, rates of reaction,and so on. because it is essential that some value be placed in the model, there is a need to supply values forparameters for which there are little or no experimental data. this has given rise to a host of estimations and agreater need to determine the role of uncertainty in the modeling process.for many of the unknown parameters, it is possible to show that any physically reasonable value will beacceptable since the underlying process is not a determinative of the outcome of the model. thus, if one is in needof a diffusion coefficient for a radical, one can take the limits of the h atom (for which there are experimentaldata) and some molecule with a molecular weight twice that of the radical. barring very unusual effects ofpolarity, the actual value of the diffusion constant will be in that range. by looking at the effect of the high and lowvalues, it becomes possible to set limits on how much of an effect the high level of uncertainty will have on thefinal result. however, if the same calculation is to be applied to an ion, a completely different set ofapproximations must be used. the number and scope of the processes modeled in a modern simulation are so largethat it is unlikely that anyone has the scientific background to ensure that all of the estimates are "reasonable."this is especially true since the definition of reasonable is a strong function of the problem: what is a small effectin one system may be large in another because of the difference in the process controlling the outcome of themodel. for the most part we do not have modeling code that determines the "reasonableness" of the values used asinput, nor do we have data resources that can provide physical limits for otherwise unknown data.types of data resourcesto satisfy the needs discussed above will require changes in the way that data resources are managed. threebroad categories of data resources are discussed to illustrate the problems in meeting these needs.1. archive. the archive is a set of numeric data of specific properties for specific chemical compounds withfull literature references. the data should be clearly identified as to property (heat ofchemical data in the "internet age"179impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.combustion per mole or per gram), including phase (liquid, solid, gas, amorphous), conditions (pressure,temperature, etc.), experimental technique, and ancillary data used to derive the property. chemical compoundsshould be identified by structures, chemical abstracts service registry numbers or beilstein numbers, formulas,and namesšincluding synonyms, common names, and trade names.in addition, the archive should have removed any obvious errors in data transcription in the original text andadjusted the data for changes in ancillary information (for example, the definition of the calorie, or changes in theheat of formation of a byproduct in the reaction).wherever possible, automatic comparisons should be made to further detect errors. this may even extend toautomatic comparison of the data with estimation programs. the obvious errors revealed by automatic checkingshould be corrected. however, archive data are not presumed to have been examined in detail as to their accuracy.the uncertainty assigned to each data element in the archive is presumed to be the value assigned by the originalauthor. the archive is not presumed to have extended this definition.while this represents an ideal minimum, it is never realized fully.2. review. the review is expected to meet all of the requirements of the archive, but also to have beenexamined by a qualified scientist. where appropriate, an attempt must have been made to reconcile data fromdifferent experimental methods, as well as from estimations and from highquality calculations if they exist.specific experimental and computational results should have been merged to provide an uncertainty assignmentthat reflects the range of values in which consensus scientific judgment expects the value to fall. for the commoncase where only a single experimental determination is available, it may be necessary to examine that datum inlight of other related compounds. in many cases the experimental data can only be compared to estimated values.3. critical evaluation. critically evaluated data should meet all of the criteria set for the review and archivedata elements, but the evaluation should also place the data in the context of other related data. thus, to evaluatethe data for reaction of the oh radical with butane critically, it is essential to examine the data not only for thereaction of oh + butane, but also for oh + propane, oh + pentane, and more broadly oh + hydrocarbons. to dosuch a critical evaluation clearly requires a thorough review to have been made of each of the components. forsome experimental data it is possible to use thermodynamic arguments to ensure the overall consistency of thedata. using the kinetics example above, if an independent measure of the free energy of reaction and the reverserate constant are available, then there is a constraint on the forward rate. the evaluation must then examine thequality of these additional components also. as might be expected, the number of data sets that can be regarded ascritically reviewed is very small. there will always be significant fractions of the data that cannot be criticallyreviewed owing to lack of experiments.in the "retail" model of data usage, the difference between these types of data resources is not as important asit is when the "user" of the data is a modeling program. even when there is direct personal use by a scientist, a lackof specific technical background to which the data relate may cause many of the same problems as would occurwith direct computer usage. in both of these cases, the absence of an informed user can cause serious problems.only when there has been a critical evaluation with a clear indication of the uncertainty of the values reportedcan data be used in a fully automated fashion. even in this case there is an obligation by the user to respect theuncertainty values and to assess how they affect the final output of the model. for complex models with highlevels of uncertainty in a number of critical parameters, the computational cost of such an assessment can be high,but the data resource has provided the information needed to solve the problem.chemical data in the "internet age"180impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.given the limits on critical evaluation, it is fortunate that for many problems a set of values that are of only"review" quality will suffice. in this case there is a single value for the parameter and a single uncertainty. theoverall quality cannot be assumed to be as high, but in many cases it is sufficient. again, the model must make useof the uncertainty.the use of archive data in automatic systems is problematic. often there are multiple values for a singleparameter, and the reported uncertainties do not encompass all the data. in other cases the archive will contain datathat upon examination will be viewed as inaccurate. there is no simple automatic mode to deal with the range ofproblems that will be encountered here, although it can be sufficient to take data with multiple values and use theaverage with an appropriately large uncertainty. the success of such an approach will depend on the problem.problems in providing datathe problem of providing good data for modern computer models can be broken down into three broadclasses:1. incomplete data sets. as noted above, a model must have data for each physical property, rate constant, andthermodynamic parameter within that model. there are broad classes of data for which there are simply noexperimental data. for example, there are very few data for any radical diffusion constants, entropy, or heatcapacity. good estimates can be made, but experimental data are very scarce. in many cases the use of valuesessentially equal to zero will cause the model to fail, so some physically reasonable data must be included. formany properties this requirement is addressed by a combination of data for related properties plus models. this isthe approach taken by the design institute for physical property data (dippr) committee of the americaninstitute of chemical engineers (aiche), which has created an extensive set of data for use by the chemicalprocess industry. a similar approach has been taken by nasa in stratospheric modeling. in general, this methodhas not been used outside very specialized areas.2. uncertain data. the reported uncertainty in most data is, at best, the experimental variation found. it is rarefor any attempt to be made to assess systematic uncertainty in a measurement. data in older literature can often be"rescued" by a better understanding of some systematic error that was not appreciated by the original investigator.while it is tempting to ignore data with identifiable errors, if the error is systematic and can be corrected for, thedata may be useful. in many cases they are the only data available for the property of that compound. byeliminating the systematic error and at the same time recognizing that the correction probably carries its ownuncertainty, it is possible to provide data that are useful. this is an important role for reviews of data.one problem in some data sets is the uncertainty of the chemical identity. as noted above, there is a need forabsolute chemical identity. this need is often not met in smaller data collections.in addition, the failure to account for changes in auxiliary data can lead to serious errors in the reported datathat are not present in the experiment. as an example, much of the data on fluorinecontaining compounds in theliterature before 1970 used an incorrect value for the enthalpy of formation of cf4 to derive the enthalpy offormation for other compounds. simply providing the original enthalpy of formation from the literature will give avery incorrect sense of the state of the data. in this case providing the correct number from the original paper isnot sufficient.3. errors in data compilation. extracting the literature into electronic format is in itself an errorproneproject: digits are inverted, signs are ignored, states are not defined. the goal of the compilation into electronicformat is to add value to the data; these kinds of errors, for the most part, do the reverse.chemical data in the "internet age"181impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.data resources currently availablethree of the relatively few extensive electronic databases generally available today are discussed. currently,there are no resources that will meet all the needs pointed out above. examination of the resources that areavailable illustrates both the strengths of these resources and the unmet needs.all resources are available via internet connections, and one is free. the data sets are the beilstein database,currently owned by elsevier; the dippr 801 project of the aiche, currently at brigham young university, butduring most of its development at pennsylvania state university; and the national institute of standards andtechnology (nist) chemistry webbook.1 these are very different efforts in size and history. beilstein goes backinto the 19th century and until recently was partially funded by the german government. the dippr projectstarted in 1980 as a response to the need for highquality data in the chemical process industry, and is funded by aconsortium of members from industry and government. the nistfunded chemistry webbook has been inexistence for only 3 years and was developed specifically to deliver data over the internet.beilstein databasebeilstein is by far the largest of the three databases. table 12.1 shows several of the types of queries to thebeilstein database and the number of molecules in the hit set. the list illustrates the origin of the beilsteindatabase as a database of organic chemistry. the properties useful in organic chemistry are well covered; forexample, the fraction of molecules for which there are reaction data is quite high. table 12.1 also gives some senseof the sheer scope of the database: there are more than 7 million distinct chemical species. beilstein differentiatesbetween optical isomers if there are data on the distinct isomers, so the number is higher than it would otherwisebe. this again reflects the organic chemistry origins of the database. for physical property datašfor example, theenthalpy of formationšthe amount of data is not all that great. however, this may well represent all of theenthalpy of formation data for organic compounds.the beilstein database is strictly archival; no attempt is made to do any evaluation and the review literature isnot covered. the database is excellent in terms of its chemical identity. in fact, of the three databases discussedhere, it is by far the best. because of its size it will have the most errorsšno matter how carefully a database iscreated, as it becomes larger the number of errors grows.there are some problems in beilstein that are unique. as an example, two enthalpyofformation values fromthe same reference are given for 3oxatricyclo[3.2.1.02.4]octane (figure 12.1) as 53,900 j/mol and 98,000 j/mol.there is no indication that the first value is the enthalpy of formation for the gas phase and the second the valuefor the liquid phase. in order to determine what the values refer to, it is necessary either to observe that theenthalpy of vaporization is the difference between these values, or to go to the original paper. given completelyelectronic access to the data, the information about the enthalpy of vaporization may not have been accessed. inaddition, both values are sign reversed. the problem of sign reversal is fairly common in the beilstein electronicdatabase and probably arises from the convention in much of the thermochemical data literature of giving a tableof values as hfor rather than showing the sign in the table.1 for more information on the beilstein database, see <http://www.beilstein.com/products/xfire/>. a subset of the dipprdatabase can be accessed at <http://dippr.byu.edu/>. the nist webbook can be found at <http://webbook.nist.gov>.chemical data in the "internet age"182impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.table 12.1 molecules in the beilstein database, by query typetype of data as defined by beilstein querynumber of moleculesenthalpy of formation (hfor)7300entropy data (all)3655heat capacity (cp)1946boiling point at pressure (bp.p)642000viscosity (bulk, kinematic, dynamic)5000nmr spectra for1h32000reaction (all)4,600,000total number of chemical species7,300,000another example, which in many respects represents a more serious problem, is the entry forhexamethyldisiloxane. this gives two values for its enthalpy of formation, 815,800 and 815,400 j/mol, which arereported to be measured at 25 and 298.2 °c, respectively. these data are referenced to the same authors within a3year period. again, there are a number of mistakes obvious to the expert, and in this case even to the nonexpert,but the information is not usable in a system seeking to obtain highquality information automatically.the final example of problems in beilstein is one that is inherent in the way that data are taken for thedatabase. in this case the literature is cited correctly: there are two experimental determinations of the enthalpy offormation of 1,2difluoro1,1,2,2tetrachloroethane: 891.788 kj/mol and 928 kj/mol from 1954 and 1982,respectively. however, both determinations are based on different enthalpy of formation data for cf4. when theexperimental values are corrected for the currently accepted codata value for the enthalpy of formation datafor cf4, the results are 925.5 kj/mol and 937 kj/mol. the agreement between the two experiments is quite good,but without the adjustment this would not be seen.the first two examples are criticisms of the data quality in beilstein. the last is not, but it is a warning toanyone who uses the data to proceed cautiously. it is not clear to what extent the problems illustrated above aregeneral in the database. used as a resource to find available literature data it is invaluable, but it cannot be useduncritically as a direct resource for numerical values.figure 12.13oxatricyclooctane.chemical data in the "internet age"183impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.dippr projectthe dippr project 801 of the aiche was designed from the start to provide complete physical property datafor the molecules in the database. for each of the more than 1,700 molecules, the available experimental data listedin box 12.1 are collected and evaluated. for properties for which experimental data do not exist, the dipprproject estimates the data and, if necessary, their temperature dependence.the data in the dippr database is ideal for use in modeling. it is reviewed, and a recommended value orequation as a function of temperature is given for each property for each molecule. there are some problems inthat the estimations are not as clearly indicated as might be desired, but this is being corrected. the uncertaintyvalues are all expressed in terms of ranges and not as absolute values. for some properties this is reasonable, butfor thermochemical data, it is essential to know the uncertainty directly.the dippr database illustrates the difficulty of providing highquality complete data. the level and qualityof the effort in the dippr project have been very high and the project has been going on for more than 17 yearswith fairly extensive resources, and yet only 1,700 compounds (all stable species) have been added to thedatabase. for the molecules and properties in the database, dippr is usually a first choice.box 12.1 data evaluated in the dippr projecttemperatureindependent data (k) point pressure (pa) of fusion at melt pt (j/kmol)aals volume (m3/kmol) boiling point (k) net heat of combustion (j/kmol)aals area (m2) (m3/kmol) volume (m3/kmol) factor (unitless) index (unitless) factor (unitless) gas heat of formation (j/kmol × k) of gyration (m) point (k) of formation (j/kmol) parameter ((j/m3)0.5) lower/upper flammability limit temperature (k) point temperature (k) entropy (j/kmol × k) moment (debye) temperature (k)temperaturedependent properties. density virial coefficient vapor pressure viscosity of vaporizationd/liquid/vapor thermal conductivity heat capacity tension gas heat capacitychemical data in the "internet age"184impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.the nist chemistry webbookthe webbook is a hybrid database. it is not complete (in the dippr sense of having all properties for allmolecules in the database), yet is not just an archive (in the sense that reviews from the literature are included, asare reviews and evaluations done just for the webbook). table 12.2 gives some sense of the data in the three mostrecent releases of the webbook.phasechange and thermochemical data have multiple data types. data are both single points andtemperaturedependent equations. as can be seen, the data for any given molecule are likely to be incomplete. thekinds of data are greater than in the case of the dippr database (not all data types are shown in table 12.2) but arenot currently as extensive as in the beilstein database. dippr does have more extensive coverage of transportproperties.the webbook makes use of the review literature in order to allow for later corrections arising from changesin auxiliary valuesšcorrections from the authors and evaluations of the relative uncertainty of the variousexperimental methods to be incorporated. however, a large portion of the webbook's data is archival, even ifcorrected for these changes.the existence of a large set of data with extensive indexing has allowed the first steps toward evaluation to bemade. a list of enthalpies of formation for carbonyl compounds from the webbook (figure 12.2) serves as anexample of the kinds of problems that are revealed in the data.in each of these cases, the data are as they appear in the literature and are fully corrected for auxiliary data.the first value for each molecule is from a single author, the remaining values from a number of authors. a patternof higher stability appears to be measured by one author. how is this to be evaluated? the issue is that there is nosimple way to evaluate this son of problem. the level of uncertainty here, which is on the border of what would beresolvable using the best of quantum calculations, may be significant in some applications. moreover, there areother cases where this author has published values for which there are no other data. how is this to be evaluated aswell? while these questions can be answered by expert evaluation of the experimental methods, the differencesbetween experimental methods and highlevel calculations, this level of evaluation cannot be done automatically,table 12.2. attributes of the nist chemistry webbookversiondata type345gasphase ionenergetics data14,20014,30014,300gasphase thermochemical data2,8005,8006,100condensedphase thermochemical data4,6005,3005,500phasechange thermochemical data8,8009,4009,500reaction thermodynamic data7,4008,7009,400ir spectra5,2005,2005,200mass spectra8,30010,60010,600fluid property data sets131616vibrational/electronic spectra & energy levelsš2,6003,300spectroscopic constants of diatomic moleculesš600600total species with data27,30031,60032,400release dateaug97mar98nov98chemical data in the "internet age"185impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.so what data are to be used by a modeling code accessing this data? averaged values may well be skewed becausethere is a systematic error in one of the measurements. the answer lies in part with the level of effort indicated bythe dippr data project. if these data are important, then the effort needs to be made. in part the answer lies insome assessment of how accurate data need to be. the uncertainty given above is still small compared to that formany enthalpyofformation values. these may be "good enough," and an automatic average with high uncertaintywill be all that is needed. however, the degree to which one can model, predict, and control a system sets many ofthe economic costs for a system. in general the cost of uncertainty in chemical operations is strongly nonlinear,and small improvements in the prediction and control can yield large improvements in costs.figure 12.2.enthalpies of formation for selected carbonyl compounds from the nist chemistry webbook.user demand for dataone point needs to be made: the need for even lessthanperfect data is very large. the usage of the nistwebbook in the third release is given in figure 12.3. access by a wide variety of users is running at over 5,000hits per week, with between 40 and 50 percent of the users returning in any given week. in the 220 days that thisedition was out, over 120,000 distinct internet addresses (ip addresses) used the webbook. usage clearly tracksthe academic calendar of the northern hemisphere. usage over christmas and the new year is very low, but evenso is more than 1,000 hits per week. summer usage is lower than at other times, but still is more than 3,000 hitsper week.comparable data were not available to the author for either the dippr or the beilstein databases. both havesome charges associated with them, whereas the webbook is currently free.chemical data in the "internet age"186impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.figure 12.3usage of nist webbook in the third release. number of users (left, bars) and percentage of returning users (right).what is neededthe demand for data is clearly large, as can be seen by the usage for resources such as the webbook. thewebbook, dippr, and beilstein are currently not equipped to handle direct requests from modeling programs.the need for communication standards among modeling programs and the databases that they rely on has not beentouched on here, but the absence of agreedupon query structure that would make it reasonable for a databaseprovider to support direct access by modeling programs is not the real limiting step for future use of the data. thelimiting factor will be the lack of resources to produce highquality evaluated data that can be used withconfidence.building data collections, reviewing and evaluating the data, and distributing the resulting information arenot free, but if done well, yield a very large return on investment. a highquality data collection can saveindividual researchers thousands of hours collectively. in addition, the evaluation can reduce the uncertainty indata with the corresponding economic benefit of higherquality modeling and prediction. it is essential that thedata provided be subjected to high levels of quality control, that the uncertainty be evident, and that the modelingprograms make use of the uncertainty.venues such as the webbook can make some of these data more readily available. the webbook has beenactively seeking researchers who have developed extensive sets of data that they have at least reviewed, if notcritically evaluated. these sets are being made available on the webbook with full credit going to the reviewer. inmany cases the archival journals are not interested in this work; often even if it is published, it is lost. much of theolder literature is also being actively evaluated; the authors and copyright holders are being sought for permissionto add the data to the webbook. at a minimum this kind of effort will bring more attention to data that are oftenvaluable but difficult to find. it is hoped that bringing this kind of data to a wider audience will stimulate moresuch reviews and realization of the need to update some of the older reviews. the ultimate goal is to haveelectronically accessible allchemical data in the "internet age"187impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.numerical data for which there are good experimental data and to extend the data with highquality predictions ofknown uncertainty where there are no experimental data.discussionallen bard, university of texas: who pays for this now? the government is paying for this and willcontinue to do so?gary mallard: at least for the foreseeable future the answer is yes. if you go to the webbook you will find alittle blurb there that says, "nist reserves the right to charge for this in the future." we have tossed around theidea of having of a $50ayear usage fee. i don't know whether we are ever going to do that. i think that the usageis general enough that we can justify it as a reasonable use of the taxpayers' money, but whether that continues ineras of tightening budgets is a question i cannot answer. but now we are doing it with internal funds.sam kounaves, tufts university: were these actual unique users, or return users, or just hits?gary mallard: no, they are not hits. that is a very deceptive way of determining use. these are unique ipaddresses, although we don't know the people behind them. in fact, we know there are more users than thisbecause many commercial suppliers come through single addresses. the big companies all come throughgateways. so, we don't know how many total users there are. we just know that there are 110,000 distinct ipaddresses, and of those, on any given week about 45 to 50 percent of them have been there before.sam kounaves: have you ever considered advertising to have people come back on this web like having aninstrument company and some chemical company to support this sort of stuff?gary mallard: it is a little tricky if you are the government. they think they pay taxes, and they do.david smith, dupont: i have a suggestion for some of my friends in the audience. for those of you who areteaching thermodynamics, perhaps it would be a good exercise to have your students calculate the thermodynamicconsistency of these articles for some of the compounds that you are interested in. a couple of hundred a yearwould probably be a worthwhile effort.gary mallard: in the very early days when we had fewer users, there was a sudden spike. whenever we see asudden spike we wonder what is going on, and this spike was coming from someplace in canada. it is pretty easyto trace these things back on the web, and it turned out that in fact somebody had done just thatšhad told peopleto look up a certain number of compounds on the web and get those data and put them into a report. i don'tremember the details anymore. so for about 4 or 5 days, we had a lot of organic chemistry students at oneuniversity using this site, and at the time it made a spike. today you wouldn't even see it.david dixon, pacific northwest national laboratory: what is the structure of the database, and what werethe manpower requirements to do the electronic version as compared to the manpower already working on thestandard printed versions from which much of the electronic versions are derived?chemical data in the "internet age"188impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.gary mallard: the resources needed to do the data evaluation and collection do not really change, whetheryou are putting data out in an electronic format or in printed format, and so that number is constant and reallyrepresents whatever we can find at nist. in a lot of cases, we have been taking data that we have had for a longtime in printed format and just putting it in electronic format. so that is relatively inexpensive and fairly costeffective because it really just takes somebody with good data entry skills to put it in a spreadsheet, and we do alittle processing on it.one person works full time on the database itself. there is a java applet that displays spectra that can beenlarged. the address is <webbook.nist.gov> and i would urge all of you to go there and try it. a lot of what thatone person does is deal with issues on the web, and we work very hard to make sure that if we display a greekcharacter it displays on a sun system, on a macintosh, and on windows, and that is a nontrivial exercise. a lot oftime is spent in making sure that this is a highquality product that looks the same on everybody's browser, that thejava applet works the same on everybody's browser, and none of that is easy. so, in that sense there is one persondevoted full time just to keeping this thing up on the web.the structure of the database is basically a file that is indexed under ctree, an available piece of softwarethat is all c code and has been known to compile on more platforms than anything else known to man. we storeeverything as ascii, because we feel that when you deal with things like the number of significant figures, youwould like to capture that information and not lose it, and so we have some fairly sophisticated algorithms forlooking at the number of significant figures. also, when we convert from kilocalories, which is perhaps what thedata was originally entered in, into kilojoules we try to retain all of that information and not have six significantfigures of zeroes which aren't significant, but the structure of the database itself is basically just ascii.jack kay, drexel university: are the janaf thermochemical tables included in this database?gary mallard: yes, not as tables but as equations in the new format, the shomate coefficients where there is a1/t term in the last term.robert cordova, elf atochem: i was wondering about the relationship between this and database 19 forstructures and properties.gary mallard: what i didn't show you on that form was that in the very beginning we actually had someestimates in the database, and those estimates came out of some of the kind of code that was in the database forstructures and properties. we removed all of the estimates. the webbook really is a kind of evolutionaryextension of the structures and properties database. there are a lot more thermochemical data in the webbook,but the estimation tool that was a part of structures and properties is not there. i think eventually we will put itback, but we just haven't had the resources to do it yet.chemical data in the "internet age"189impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.13the digital library: an integrated system for scholarlycommunicationrichard e. lucieruniversity of californiascientific journals have served scientists well for many decades. they have provided a viable means forscientists to communicate their findings to their peers and have served as well as an archival record of scientificprogress. now, however, we are seeing the beginnings of a significant evolution away from what we know as thetraditional scientific research journal.this article is divided into three parts. the first part provides context by outlining what is driving some of theissues that are discussed. second, the notion of what could be meant by a digital library is discussed. in doing so, idescribe what we are doing in californiašthe california digital libraryšas well as the digital library in general.what we are doing in california is an example of what might occur in other places, so that it is useful andinstructive to talk about some of the specifics of that project. finally, i discuss alternative forms of scholarlycommunication, i.e., alternatives to the tradiltional research journals.crisis in scholarly communicationwe have been hearing for the past decade that academic research libraries are in crisis. the fact of the matteris that these libraries, as we know them, including the services that we have come to expect from them, are simplyno longer sustainable in their current form. projected costs, for both the acquisition and the storage of information,are significantly higher than the universities can possibly sustain. while current data indicates that university feesare going up 8 percent, library acquisition costs (in the sciences) have been rising at a significantly higher rate,namely 15 to 20 percent annually for the past several years.it is convenient to blame publishers for these increases and this crisis. certainly some publishers, particularlycommercial publishers, are part of the cause of these difficulties. in many respects, however, the productivity ofscientists is a more significant causal factor. scientists are producing more and more information, e.g., theamerican chemical society is publishing 10 percent more pages each year, and many other publishers reportannual increases of 10 to 20 percent. if this increase in the rate at which information is published continues, andthat seems likely, university libraries will be unable tothe digital library: an integrated system for scholarly communication190impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.provide scientists with access to that information if the traditional process of scientific communication ismaintained.at the same time, we are now at an early evolutionary stage in the use of digital technology in scholarlycommunication. what will happen as this evolution continuesšhow scholars and scientists will eventuallyintegrate digital information technologies into their workšis not yet clear. a number of issues like cost, ease ofuse, and academic culture are going to have a major impact on the future application of digital technology in thisarea. there are many who believe that the application of digital technologies to scholarly communication is asrevolutionary as the use of the printing press, and indeed i think that is the case. but it is going to take severalyears to see how this evolves and what the implications are.solutions and strategiesa number of universities are exploring optimal strategies for dealing with this transition in the managementof scholarly information and optimizing the opportunities presented by digital technologies. the university ofcalifornia (uc) has recently completed a 4year planning process examining all of these issues. the followingconclusions form the basis for strategic action: comprehensive access to information is going to replace comprehensive ownership of information.remember when you expected to be able to get your favorite journal from your university or corporatelibrary? in the future your library will provide you access to that information in some reasonable periodof time. unfold organically. a traditional plan is not desirable. digital library is an agent of change. uc should build one digital library.the california digital libraryas part of our planning for libraries and scholarly information at the university of california, we developed ashared vision of the library appropriate for the university: a worldclass research library for the 21st centuryconsisting of complementary paper and digital libraries comprising a universitywide knowledge networkwith services delivered at the point of need.the digital component of this library has been named the california digital library (cdl), to reflect itspotential to serve all of california, not just uc. created in october 1997 by the university of california board ofregents and the president of the university, the cdl will open its ﬁdigital doorsﬂ in january 1999.the digital library can be viewed as an integrated system for the management of scholarly information. thismoves the library beyond its traditional roles of storage, preservation, and access to an active player in scholarlycommunication through support for alternative forms of publication, which exploit digital technology.in this context, the digital library will have the following components:quality digital resources;interface; integrated with digital resources at the point of service;the digital library: an integrated system for scholarly communication191impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved. of publication and the digital dissemination of scholarship; and business and economic models that are sustainable.it is important to note that the digital library is not going to replace the paper library for the foreseeablefuture. rather, the two are going to coexist, complementing one another. how we implement this complementarityis going to be critical to the continued smooth functioning of the scholarly enterprise.content is the heart of all libraries, and that is true for the cdl, as well. already, we see that there are manydifferent kinds of digital collections. one is the traditional published journal literature in digital format. while thisis important at these early stages of building a digital library, it will likely become less important in time. even atthe outset, we have a second kind of content, "digital at birth" content, which has always been in digital form only.third is primary source data. for libraries, that includes special collections, but our faculty also have a lot ofprimary source data that we are trying to build into our collections. a fourth important area of content for digitallibraries is museum collections. stronger relationships between libraries and museums are developing in thedigital era. last is what we refer to as alternative forms of scholarly communication. over the next 15 years thedivision between journals on the one hand and these alternative communication forms on the other is expected tochange significantly. by the year 2015, we expect that less than half of the kind of information that we will beproviding in the cdl will be in the form of traditional journals in digital format.it is important to review the changes taking place with respect to the acquisition of digital content. in manyinstances, libraries are not buying digital journals and databases; instead, they are licensing them. this change hasseveral important implications. licenses have very different terms and conditions, depending on the publisher. ofcritical importance to the library and user communities is the notion of perpetual access. the approach of manypublishers has been to license information for the year in which you pay for it. thus, if you paid for a 1998subscription in 1998 but not in 1999, you would lose access to 1998 content in 1999. this is in marked contrast tothe past. traditionally, if the library bought a paper journal, it always had that journal. that is not the case withelectronic material. these and many other issues surrounding licensing mark a new relationship betweenpublishers and libraries, one full of challenges and opportunities, requiting great vigilance and care on the part ofthe library community if it is to ensure access for research and education.licensing has allowed us to develop large collections of digital material in a short time frame. when we openthe california digital library, we will have more than 3,000 electronic journals in scientific and technical areas.that, along with the other databases, is a significant amount of content. licensing has also provided a platform forcost control. by forming and joining large consortia or groups of libraries to license electronic material, we areable to leverage our collective buying power. this has allowed us to buy more materials than we would have, if wehad individually tried to license this material. it also sets the stage for how we are going to work with thepublishing community in the future. one of the first digital journal licenses we signed at uc was with theamerican chemical society. it is important to note that acs has been willing to develop a model that is morebeneficial to users than the models developed by many other publishers, particularly commercial ones. we haveappreciated acs's willingness to listen and respond to our concerns.licensing is a useful strategy to aid in the transition from paper to digital journals, in moving from anownership model to a service and access model. in 10 years, however, its various components may not be assignificant as they are now. hopefully, licensing at some level will evolve into a standard operating procedure,with much less emphasis placed on negotiating individual terms with each content provider.the digital library: an integrated system for scholarly communication192impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.alternative forms of scholarly communicationwe are currently using technology in its first phase of adoption: modernization; that is, we are replacingtraditional paper journals with traditional digital journals. what we really need to do is to lay the foundation for afuture in which how scholars communicate and access the results of research around the world will truly betransformed. we need to invest a significant component of our resources into developing innovations that willfacilitate this transformation. it is my belief that the digital library provides the appropriate infrastructure todevelop and leverage these innovations, to make collective investments across universities to do selfpublishing,digital publishing, and to directly compete with the existing model of scholarly communication.there are a number of activities to pursue in developing alternative forms of scholarly communication. one isto develop prototype projects with faculty, based on their needs for better ways to disseminate and accessinformation. the most important innovations will come from the scholarly community itself, not fromadministrators or librarians. a second is policy development. in this area, it is important to examine currentcopyright policies and behaviors such as the assignment of rights to publishers. third, the development of newforms of scholarly communication is best pursued in concert with colleagues on national and international levels.there are a number of potential scenarios for alternative forms of scholarly communication that have beenput forward in the last few years. one is called near, the national electronic article repository. what auniversity provost has proposed is that authors would retain certain rights when they publish a journal article. therights would permit, within 90 days of the appearance of the paper publication, placing the article on a nationalserver, which is a depository for scholarly articles. everybody would then have free, perpetual access to thearticle.a second scenario, put forward by the association of american university presidents, recommendsdecoupling certification from publication. this is based on the belief that it is the coupling of the promotion andtenure process with publication that is the cause of current financial problems in the publication of scholarlymaterials. in this scenario, universities would pay professional societies to review the work of their faculty and tocertify the work for promotion and tenure purposes. the universities would then place the work on servers so thatit would be available to the external community at no or low cost. only a small portion of this material would thenmake its way through the regular publication process. so, instead of publications continually increasing, actualpaper publications (expensive publications) would decrease, to be replaced by electronic publications on readilyaccessed servers.a third scenario calls for universities to assume responsibility for publishing the work of their faculty. afourth identifies the notion of peerreviewed servers as the viable alternative. what we have seen in the physicscommunity at los alamos, for example, is the establishment of a physics preprint server where not only thephysics community, but also the mathematics community as well as others place articles prior to publication.adding peer review to that preprint process would ensure technical merit, and it would not be necessary to gothrough the entire publication process.what is absolutely key to further progress in identifying and implementing sustainable alternatives forscholarly communication is the development of business models for all scenarios. while some of these ideas maysound desirable, their financial feasibility must be demonstrated. replacement models are not necessarily lessexpensive, and appropriate due diligence must be rigorous.the digital library: an integrated system for scholarly communication193impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.a 1998 essay, "to publish and perish,"1 recommends five actions as we move toward new alternatives: ﬁturn down the volume"; i.e., we need to concentrate more on the quality of publication rather thanquantity in the promotion and tenure process. must be smarter shoppers, just as we are trying to be through consortia and licensing. and property rights issues. universities must invest in electronic forms of scholarly communication and should support new effortsthat faculty are putting forward in this area. decoupling of publication and faculty evaluation should be seriously investigated.national efforts are under way, which should lead to some breakthroughs in the coming years. one issparc, the scholarly publishing and academic resources coalition, originally developed by the association ofresearch libraries. it currently consists of more than 100 research libraries from around the country that havejoined together for the following purposes: create a more competitive marketplace, reduce journal prices, ensure fair use of electronic materials, and apply new technologies to information creation and storage.sparc:ighquality, fairly priced publications and guarantees a subscription base;startup capital for new projects; andfrom important groups like our own faculty and administrators and provosts.the american chemical society and the royal society of chemistry are initial sparc partners.conclusionthe challenges that we face in trying to make our way through this evolutionary change are very significant.they range from the political to the technical and financial. as we move toward the realization of digital libraries,basic research is absolutely critical in this area. there are no models; we are in uncharted territory, and we needthe research community to inform the way. it is critical that faculty participate in this research. the solutions mustreflect your "way of doing business."discussionstephen heller, national institute of standards and technology: i have a couple of questions. first, areyou working at all with highwire press, and do you have any comments about what they have been doing?1 "to publish and perish," based on a roundtable hosted by johns hopkins university and convened jointly by theassociation of research libraries, the association of american universities, and the pew higher education roundtable,march 1998; published in policy perspectives , knight higher education collaborative, philadelphia (1998).the digital library: an integrated system for scholarly communication194impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.richard lucier: highwire press is a very interesting operation. it has taken over the production for anumber of societies that would not have had the money individually to invest in digital technology, and so hasallowed those societies to remain competitive as we move into this digital environment. highwire press has donegreat work. we work with them in the sense that we pay for all of those publications. we have had discussionsabout more substantive cooperation, but nothing yet has come out between stanford and uc on that.stephen heller: the second question is, should libraries be responsible for the actual archiving when areasonable solution is found? right now it is a sort of random process in which the publishers have decided to gointo the new business of archiving, which is providing information on a longterm basis, not just selling asubscription and washing their hands of any responsibility to provide anything after the subscription expires.richard lucier: i think it depends on the publisher. we need to have someone of repute take responsibilityfor archiving the world's knowledge base. i am very leery about saying that commercial entities should take thatresponsibility.commercial entities will take that responsibility only as long as it is profitable for them, and as informationgets older it may no longer be profitable.i don't know if it should be libraries. i don't know if it should be universities. i don't know if it should be thefederal government or some other organization. i think there has to be a national strategy initially, and there willhave to be an international strategy. no onešnot the library, the community, the academic disciplines, societies,or publishersšhas yet really tackled that problem very well.a couple of years ago a commission came out with a report that rather scared everyone, and so no one hastouched the issue in the last couple of years, and i hope the dialogue can continue again soon. but there are nogood answers, and i am not sure who ought to do it, but i guess i would trust universities before i would trustcommercial organizations.i believe, and lorrin might correct me, that even our license with the american chemical society onlyguarantees access for 5 years, and i am assuming that some of that literature is still important to you after it is 5years old.stanley sandler, university of delaware: i guess i am overwhelmed with the amount of information that isavailable and being generated, particularly the number of journal pages and such. so, maybe this is more anappeal to my colleagues. i think there is an increasing difference between, let us say, the cpu and the lpu. weknow that the cpu is increasing speed and power and likewise the rate at which we can generate experimentaldata. the lpu, the least publishable unit, i would say hasn't changed anywhere nearly as fast as the rate at whichwe generate simulation and experimental data. so consequently a paper today may contain about the sameinformation as a paper years ago. years ago it may have required years of intellectual effort. maybe 2 years ago itrequired a month of intellectual effort. maybe this year it requires a week of intellectual effort, and i submit thatwe as reviewers are not doing a careful enough job of keeping the lpu together with the cpu, and that is why weare overwhelmed with so much published literature.richard lucier: i think that one of my frustrations in this uphill battle is that i cannot solve these problems. iam willing to support you in any way that you want, but it is the disciplines and the academic communities thathave to come to some solution. that last point was one of the reasons that we are very concerned about makingsure that we are providing only highquality information within the digitalthe digital library: an integrated system for scholarly communication195impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.library, not just everything that is out there, and it is, also, when different groups like the american association ofuniversities look at this decoupling process. it is a recognition that everything that is going into print probablyought not to go into print and that somewhere along the line we have to make some qualitative judgments. youhave to make some qualitative judgments. i am not trying to impose anything.allen bard, university of texas: it seems to me that this idea of decoupling publication and certification is agame, because as everybody will know if you decouple it, you say, "yes, we certify this as great work, but it is notworth publishing; we certify that other work, but it is worth publishing." everybody will know that game.richard lucier: i think there are other ways to look at it. one can say that we certify this work, but we don'tneed to publish it in its complete form in the way that we did in the past, and we might only publish in a formalpublication a certain excerpt but maintain on file servers that would be a lot cheaper to do over time, the ability tobe able to get access to that information.i think what decoupling does is allow us to look at the publication process differently so that we can find acosteffective way, and one that exploits technology in a way that makes the data more useful to you as well.tom edgar, university of texas: what is your business model as you are constructing it at the universityof california? if you look down the road, say 10 to 15 years, do you see any changes in human resources needs forthe collective libraries of the university of california system?richard lucier: how many years into the future?tom edgar: ten to 20, let us say.richard lucier: i cannot look 10 to 20. the most i can look is 5, and yes, we do see changes.tom edgar: i guess the gradient is what i am interested in; is it positive or negative in terms of the numberof people it is going to take to provide the california digital library services compared to the number you havetoday?richard lucier: i think that what we have projected is that it is going to take probably an equal number ofresources, but ones more focused on providing quality access to information than they currently are. what i cantell you with respect to saving money is that in the first year we can document that we have saved the campuses, inlicensing costs alone, about $2.5 million for access to information. if they had gone about it separately and boughtthis information themselves, it would have cost them that much more.the other thing that we are able to do is to provide access independent of location. it doesn't matter anylonger if you are a chemist at berkeley or if you are a chemist at santa cruz; you can get access to the same kindof information. we feel that it is really important for our faculty and students to be able to have that kind of accessirrespective of their particular physical location.tom edgar: the second question is one i will ask and then head for cover. you said that the physicists andmathematicians have agreed to go toward putting publications on web servers. my impression is that the chemistshave really not agreed to do that in the same way. i am curious. what are the differences between chemists and theother group that make chemists behave differently?the digital library: an integrated system for scholarly communication196impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.richard lucier: i think you could answer that question better than i, and i would be really interested in theanswer.steven heller: it is a cultural thing. actually the story with the los alamos preprint server is that they hadbeen doing preprint exchanges for decades before computers, and when computers came they just put the preprints on the computer.richard lucier: is that true with the mathematics community as well?steven heller: yes.richard lucier: there is a new biological sciences server that you may or may not know about, a preprintserver that has begun as well. having spent most of my career in the biomedical sciences, i was very surprisedabout that because the exchange of preprints has not been traditional in that field, but they see what is happeningin physics and mathematics and have moved to that.evelyn goldfield, wayne state university: first i would like to say something about the preprint servers.one of the problems that chemists feel, at least the ones i have talked tošand i think this is a problem that you aregoing to seešis the question of peer review or multiple versions or error corrections, because from what iunderstand, things can go on to preprint servers without any review at all. as a physicist friend of mineexplained, "oh, we will just correct it as we go along," which is fine if you are in that community and you know.but a student could easily be getting incorrect information, and i think there is a resistance on the part of a lot ofpeople to risk that.steven heller: that is not true. there is a link between the versions.evelyn goldfield: i believe that many chemists are wary about nonjournal webbased publishing onaccount of quality control issues, and how it will impact the review process. they are worried about having a lotof nonrefereed papers and multiple versions of papers out there.my question is that if libraries can no longer afford to purchase commercial academic publications or books,then what do you think the future holds? are academic commercial publishers going to remain viable? what is thefuture of paper and books? do you think there is any future at all and if so, what is it? how do you see that?richard lucier: as i mentioned, i don't see electronic versions replacing paper wholesale at this time. ithink it is going to be a long evolution, that there are problems such as archiving that have to be solved before onecan replace the other.we are going through a period now, i think, of trying to understand how our faculty and the researchcommunity will use the electronic versions, what they prefer about them.what we are seeing with things like highwire press, for example, is that the print version and the electronicversion are getting further and further apart, and the electronic technology is being exploited to provide productsthat are much more beneficial to you than the paper might have been.so, there is an evolution going on. i hope at uc that we will be able to cancel some print publications in theyear 2000. right now we have as many as, if not more than, nine copies of a particular journal, one at each of ourcampuses. we could potentially in 2000, if we provide good electronic accessthe digital library: an integrated system for scholarly communication197impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.to some of these titles, cancel all the paper except for two and save one in the north and one in the south forarchiving purposes.evelyn goldfield: that will cost the publishers money.richard lucier: right, it will cost the publishers money. that is correct.robert de levie, georgetown university: you have talked about journals. how about books?richard lucier: it depends on what kind of books you are talking about. i think that digital technology canbe very useful for reference books and reference databases. if you are talking about certain kinds of scholarlytreatises in the humanities, i don't think we are going to see widespread replacement there at all in the immediatefuture. i think the digital technologies are going to take much greater hold in the sciences early on. thehumanities, and less so the social sciences, are probably 5 to 10 years behind.robert de levie: even though those books nowadays are produced mostly in digital form?richard lucier: yes.robert de levie: you mentioned $2.5 million gained. is that because you reduced the number ofsubscriptions from nine to one, and what is the offsetting cost of not knowing whether 5 years from now you willhave to buy the paper copies anyway?richard lucier: we won't because we won't have gotten rid of all of the paper. we are making certain thatwe maintain in storage facilitiesšwe have a storage facility in the north and the southšpaper copies should weneed to do that.the $2 million plus was gained by expanded access. so, for example, you might have had 30 acssubscriptions at berkeley and at la but only 5 at riverside and 8 at santa cruz, and now everybody at all ninecampuses is getting access to all as well as the fact that the access for, let us say, berkeley alone, which may havesubscribed to all of them, costs less because we went as part of a consortium. so, there are savings in those twoareas.gintaris reklaitis, purdue university: one of the most important and underappreciated resources in theentire publications review process is the reviewers. clearly as the publications process continues to expand, thedemands on the reviewers will also. do any of the business models that you are examining for scientificpublication take into account this important resource and how we might stimulate it to handle this expansion?richard lucier: the model where the university moves into publishing very much takes advantage of thatresource, which is part of the university already. essentially what we do now for the most part is give it away thecommercial publishers at no cost so that they can then add a huge mark up to it when we buy back that informationthat has been peer reviewed by our faculty, and so it makes perfect sense for the university or federations ofuniversities to do that together.my problem with the highwire model is that it is one university, and science and scholarship cut acrossuniversities too much, and it makes much more sense in my opinion to try to federate this in some way acrossgroups of universities rather than try to go solo, and that is why uc isn't pursuing that particular strategy.the digital library: an integrated system for scholarly communication198impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.14electronic journal publishing at the american chemicalsocietylorrin r. garsonamerican chemical societyscientific, technical, and medical (stm) publishing is a unique enterprise with the following characteristics: number of subscribers: 1,000 to 10,000 subscribers per title.costs for quality control, which includes peer review and technical editing. high production costs because of complex information such as mathematics and highquality graphics. pressure from authors to publish more material.or decreasing funds for purchasing publications.publishers for highquality content, good editors, and subscribers. stm title publication by commercial publishers. largely priceindependent competition for subscription sales. each journal is a "limited monopoly"; thatis, an article published in one journal does not appear in any other. there is a longstanding traditionagainst duplicate publication. steady decline in subscriptions to stm journals in the past 15 to 20 years, and the trend is expected tocontinue.growth of scientific literaturealthough all of these factors contribute to the growing crisis in stm information transfer, the pressure topublish an increasing amount of material is arguably the greatest single factor in the growth of the scientificliterature. figure 14.1 shows the growth of chemical papers (excluding patents, monographs, and books) duringthe decades starting when chemical abstracts began publication in 1907.1 except during the periods of worldwars i and ii, the increase has followed an exponential growth1 data from "cas statistical summary 19071997," chemical abstracts service, columbus, ohio.electronic journal publishing at the american chemical society199impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.pattern through the years, including the most recent decade, 1987 to 1996. chemistry, a relatively mature science,is probably reasonably representative of the growth of stm publishing in general.figure 14.1number of abstracts of papers in chemical abstracts, 1907 to 1996 by decade.to remain competitive, publishers publish more material by introducing new journal titles in response toemerging fields and publish more papers in existing titles.2 figure 14.2 shows the growth in the acs journalpublishing program from 1980 through 1997, in terms of both articles and pages published per year. during this17year period, the number of articles grew 114 percent and the number of pages published increased by 229percent; this is an average annual growth rate of 6.71 percent and 13.5 percent, respectively.this exponential growth of stm literature is exacerbated by the increase in article length. for acs journals,the average article has grown from 5.39 pages/article in 1980 to 7.30 pages/article in 1997 (see figure 14.3). thedecrease in article length from 7.34 pages/article in 1995 to 7.26 pages/article in 1996 is the result of a concertedeffort made by acs editors to encourage authors to reduce the length of their manuscripts. unfortunately, at thisrime there is no indication that the exponential growth of the stm literature is slowing.many subscribers object to subscription prices rising faster than the rate of monetary inflation, ignoring "pageinflation" in most titles brought about by the increasing number of manuscript submissions and growth inmanuscript length. this situation provides a significant marketing challenge for all2 the development of new scientific fields and increasing specialization also contribute to the development of new journaltitles.electronic journal publishing at the american chemical society200impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.figure 14.2growth in acs journal publishing, 1980 to 1997.figure 14.3growth in length of average article in acs journals, 1981 to 1997.electronic journal publishing at the american chemical society201impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.publishers. also, many subscribers do not differentiate between an 8 percent annual subscription priceincrease for a $1,000 subscription for a journal from a notforprofit publisher and an 8 percent increase for a$5,000 journal from a commercial publisher. from the customer's perspective, they are both an 8 percent increase.yet in terms of dollars, one has increased by $80 and the other $400ša difference of $320. this perceptionpresents an additional challenge in marketing for notforprofit publishers.financial considerationsany journalpublishing endeavor must generate adequate revenue to meet expenses or that endeavor willcollapse. in addition, publishers in the commercial sector must also distribute dividends to stockholders and paycorporate taxes. notforprofit publishers, which are largely professional societies and a few universities, do notdistribute dividends to shareholders or pay corporate taxes, but typically use the small amount of excess revenuesover expenses to support their core objectives and programs. the acs is a notforprofit publisher (as well as aﬁnotforloss publisherﬂ) and has had a successful journalpublishing program since 1879. surpluses from itsjournal publishing operations, which are targeted at less than 10 percent of gross revenues, are used to invest infuture publishing activities as well as to support a variety of acs scientific and educational programs.historically, the great majority of revenue has come from subscription sales. author page charges andadvertising revenue have also been minor sources of income, but subscription sales have been and continue to bethe major source of revenues for stm publishers. tables 14.1 and 14.2 show the sources of revenues andexpenses, respectively, in 1996 for acs journal publishing operations.3subscription sales constitute over 80 percent of revenues and of these revenues, 90 percent are frominstitutional sales and 10 percent from sales to acs members. sales of journals to acs members are financiallyneutral; that is, members obtain journals at "run off" costs. although sales to members are of little financialconsequence, distribution to members is an important part of the acs's mission to disseminate scientificinformation broadly, and one could argue that without sales of subscriptions to acs members, pressure onlibraries to purchase more subscriptions would increase. reprint revenues are rapidly declining with theavailability of the journals in electronic form.expenses for journal production fall into two categories: firstcopy costs and distribution costs. firstcopycosts include the first five items in table 14.2 and constitute 84.3 percent of all expenses. distribution costsinclude the last three items in table 14.2 and constitute 15.7 percent of all costs. the stm publishing industryaverage for firstcopy costs is about 80 percent. data for 1996 were selected because this was the last year inwhich there were no production expenses and revenues associated with electronic delivery of journal products,although there were significant r&d costs for these activities. thus, as electronic delivery of informationincreases, costs associated with that mode of distribution will increase while costs associated with paper, printing,and shipping will decrease, assuming fewer copies of print journals are produced.the notion of firstcopy cost is important when considering electronic publishing. regardless of whetherdistribution of information is via the traditional, printed journal or by electronic means, production of the first copyfor the acs constitutes 84.3 percent of all expenses. database building and composition are the single largestexpense. this includes assignment of journalarticle components into appropriate data elements; appropriaterendering of tabular and mathematical material; handling of graphic data, including halftones and color; pagelayout; and generation of sgml and html.3 from "economics of scientific publishing," l.r. garson, 214th acs national meeting, las vegas, nevada, september 8,1997.electronic journal publishing at the american chemical society202impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.table 14.1 acs journal publishing revenues, 1996revenue sourcepercentagesubscriptions81.3reprints & page charges10.5microfilm & back issues3.4copyright royalties1.9other2.9total100table 14.2 acs journal publishing expenses, 1996expensepercentagepeer review and external editors19.3technical editing12.8database building and composition43.4marketing and sales7.3r&d1.5paper, printing, and distribution8.3reprint and microfilm reproduction5.3miscellaneous2.1total100publishers are often admonished, "if you would only modify your production methods to accommodate anelectronic environment rather than a printoriented world, you could save 70 to 80 percent of production costs."this admonition is usually based on the assumption that publishers are not using computerbased manufacturingsystems, or are applying these technologies inappropriately. this presupposition is generally false and is certainlyuntrue for the acs's journal publishing program.chronology of acs electronic journal developmentlike most "overnight successes" in the entertainment world, the success of electronic journals did not occurovernight. for the acs, the ability to make its journals available on the world wide web in a costeffectivemanner is the consequence of 25 years of investment in computerbased systems and staff training. below areseveral milestones that led to the first acs journal being made available on the web in april 1996šthe journalof physical chemistryšon the occasion of its 100th anniversary.1975journal production initiated inhouse using a database approach.19801,000 articles from the journal of medicinal chemistry loaded on bibliographic retrieval systems (brs) asan experimental prototype electronic journal.1981experimental file of 16 acs journals loaded on brs.1982fulltext file of acs journals becomes a commercial product on brs. the file remained active until 1985.electronic journal publishing at the american chemical society203impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.1986cjacs (chemical journals of the american chemical society) file made available worldwide on stninternational.4 this file remained active until september 1998.1993supporting information5 for the journal of the american chemical society made available worldwide viagopher.1995supporting information for all acs journals made available via gopher.1996the journal of physical chemistry released on the web in june followed by biochemistry andenvironmental science & technology in august.1997the journal of the american chemical society and the journal of organic chemistry released on the web inapril.1997remaining 20 acs journals released on the web at 5:15 pm on september 7th.1998asap articles6 in production on january 1st.what customers want in electronic journalsthe producer of any product or service ignores its customers at great peril. customers for scientific journalsare its suppliers of manuscripts (authors) and purchasers of journal subscriptions (subscribers). subscribers areboth institutions, such as libraries, and individual acs members. acs members may be authors as well assubscribers. our market research has shown that subscribers want the following attributes in electronic journals:1. useful, accurate information,2. fast delivery,3. low cost,4. access in perpetuity, and5. seamless access across publishers and databases.usefulness is difficult to characterize, although each individual has an intuitive sense of what is useful at themoment. often what is not useful or interesting today may become so in the future. accurate information seems tobe best attained by vigorous peer review and author integrity. fast delivery is largely controlled by the efficiencyof the publisher and the speed of peer review. low cost is also dependent on the efficiency of the publisher as wellas a wide variety of business considerations. access in perpetuity and seamless access across publishers anddatabases have not yet been realized and are the focal points of a variety of experiments and studies. perpetualaccess is of greater concern to institutional rather than individual subscribers.4 subsequently several other stm publishers also made their fulltext, scientific journals available on stn international:why from john wiley & sons, cjelsevier from elsevier science publishers, and cjrsc from the royal society ofchemistry in england.5 "supporting information" is an important component of an article, but is not essential to the major thrust of the paper orcritical to its readability. supporting information contains information such as experimental details, spectra, xraycrystallographic data, and various other types of numeric data. supporting information is not printed with the correspondingjournal article but has been distributed on microfilm and microfiche. historically, the acs has published about 80,000 pagesper year of this information. starting in 1999, supporting information will only be available electronically on the web, not onmicro forms.6 asap (as soon as publishable) articles are papers that have been through the peer review process, revision, and authorapproval of page proofs, and then made available on the web within 48 hours of final corrections being made. these articlesare identical to the corresponding article printed in the journal and made available on the web, except that asap articles donot contain page numbers.electronic journal publishing at the american chemical society204impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.what do authors want in electronic journals?1. to publish in prestigious journals,2. peer review,3. rapid publication,4. wide dissemination, and5. attractive presentation.in any particular field, experienced practitioners are aware of the prestigious journals in that discipline, andpublishing in those journals is more desirable and generally more rewarding to one's career than publishing injournals of lesser prestige. although an author may feel peer review has treated his work badly at one time oranother, peer review is widely held by the scientific community to be valuable if not essential. a wellrun peerreview operation undoubtedly contributes to the prestige of a journal. speedy publication is an importantconsideration for authors in choosing a journal to which to submit their manuscripts. authors are also aware of thecirculation of journals and use this as another consideration in selecting where to publish. science and nature areexamples of highcirculation publications to which many authors wish to submit their work. although there areimportant publications that use cameraready material, and which consequently have less than optimalappearance, an attractive presentation of a journal is important to many authors and of even greater importance toreaders.the advent of electronic publishingto borrow from one of winston churchill's famous speeches,7 1997 marks "the end of the beginning" in thedevelopment and deployment of electronic journals. this is undoubtedly the consequence of improvements madein computer and telecommunications technology over the years and in particular the development of the worldwide web. web information systems are becoming readily available among the scientific community,increasingly powerful, and easier to use. researchers and librarians are generally accepting the inevitability of thisgreat transformation. figure 14.4 shows the increase in use of the acs publications division web server fromfebruary, 1995 to september, 1998šfrom 647 to 7,976,036 pages. it is interesting to note that the volume ofinformation delivered via the web to paying subscribers in september 1998 was equal to that delivered in march1998 when the acs journals were available without charge and open to the public. as expected when free accessto the journals on the web was turned off, the amount of information delivered dropped, but paying customers aredramatically increasing their use of web journalsša most gratifying trend.as one might expect, the use of web information is international. during the week of january 28th tofebruary 3rd, 1998, the geographic distribution of customers accessing the acs's publications web site wasmeasured. the results are shown in figure 14.5.although 53 percent of the traffic came from the united states, 47 percent came from outside the unitedstates. japan was the largest consumer of web information, followed by germany, the united kingdom, canada,france, italy, south korea, spain, the netherlands, switzerland, and 69 other countries.7 on november 10, 1942, churchill made a speech at the lord mayor's luncheon at mansion house on the occasion of thedefeat of erwin rommel's forces at el alamein in egypt. in that speech he made the now famous comment: "now this is notthe end. it is not even the beginning of the end, but it is perhaps, the end of the beginning."electronic journal publishing at the american chemical society205impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.figure 14.4total pages (in millions) transmitted from the acs publications division world wide web server, 1995 to 1998.figure 14.5geographic distribution of customers, january 28 to february 3, 1998.electronic journal publishing at the american chemical society206impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.transformation from print to electronic publishingwhile the use and sales of electronic journals are promising, there are many unanswered questions facing thescientific community and publishers in the near future. among these are the following:1. how can current web systems become more comprehensive in content?2. how can access and use be made simpler and "seamless" across databases?3. how will researchers, librarians, and publishers fund necessary investments in technology?4. what are fair, reasonable, and acceptable prices for subscriptions to web journals?5. what terms and constraints for use of web journals are appropriate?6. how much should be charged for an individual article online?7. who is responsible for archiving web journals, and what is the commitment?8. what are the function and status of journals that are available online only?9. will researchers and librarians value and pay for costly enhancements to web journals?10. where will the money come from to pay for both print and web journals?11. what is the outlook for individual subscriptionsšprint and online?among these challenges, two merit special comment. significant progress is being made in simplifyingaccess across databases. the digital object identifier (doi) holds promise for providing persistent, seamlesslinking. the doi affords a mechanism for a persistent link to digital objects, such as web articles or theircomponents, and is inherently a lookup mechanism.8 it is likely that a minimal set of metadata will be madeavailable with each doi that will provide for identification of digital objects. it is also likely that abstracting andindexing services will include dois for items they cover and thus provide a much richer set of metadata forlocating digital information. other services, such as the national library of medicine's pubmed9 in the medicalsciences, and the nascent pubref service also offer promise for improving linking between databases.the digital archive is a particularly vexing issue, which is critically important to the longterm preservationof the scientific record. broad acceptance by institutional subscribers of electronic journals as a replacement forprint will not likely take place until this issue is resolved. traditionally, libraries have served as the institution forarchiving information in print. remember the discussions on acidfree paper? however, libraries are not wellpositioned to serve this role for electronic journals. publishers have not served in an archival capacity and havedepended on libraries for this service. commercial publishers are unlikely to serve as a repository for archival dataunless there is an adequate market to support the expense of maintaining such an archive. it has been suggestedthat "trusted third parties" serve as institutional repositories of digital archives, but such institutions are not yetforthcoming, despite the claims of a few organizations purporting to serve this role. the acs has made a publiccommitment to preserve its digital journals, as have other society publishers, but such commitments are notadequate by themselves.expenses associated with maintaining a digital archive are highly uncertain. costs associated with migrationfrom one hardware system to another are more predictable than conversion of current file formats to unknown fileformats in the future. the diversity of file formats exacerbates the complexity and expense of future conversion.new file formats for various types of scientific data are also likely to8 for more information on the doi, see the web site of the international doi foundation at <http://www.doi.org/>.9 for more information on pubmed, see <http://www.ncbi.nlm.nih.gov/pubmed/>.electronic journal publishing at the american chemical society207impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.be adopted and included with digital journals. an uncertain market for the archive, coupled with an uncertain butprobably high cost for its maintenance, is working against a solution to this problem.discussionstephen heller, national institute of standards and technology: a couple of things, lorrin. first of all, itwas a very nice presentation. you mentioned 50 free hits to articles posted on the web, but what happens if 50web crawlers come looking through your site, and they are the first 50 that come in? what happens to the firstreal chemist?lorrin garson: we think we can outsmart the web crawlers in this instance. we will put them in a protectedpartition, and i think we can keep web crawlers out.stephen heller: a second item for ejournals. i think there is a chemical abstracts web site home pageindicating that there are a couple of dozen journals that chemical abstracts right now does process. there are allof the electronic journals that it does, in fact, abstract and index. so, it is not just three or four journals. chemicalabstracts has been very, very good about looking for journals that in fact are there for more than a millisecond. ialso want to mention the internet journal of chemistry, which i have been involved with as a member of theeditorial board, and suggest that people take a look at it. right now it is free until the year 2000. one thing about itšand we were talking with richard (lucier) and a little bit with lorrin (garson) about the pricing and the costšisthat the way we put it together, it is not a big major organization operation. the cost of putting it together isinfinitely less, and the breakdowns that you had there are quite different. it seems to me that a major problem withthe economics that you are talking about for all existing print journals is the overhead and baggage that exists rightnow. the fact that you are trying to carry two things at one time makes it extremely difficult to reduce costs, and ifyou just went electronic on a separate operation even the acs could do it for less than it costs the way you aredoing it now.lorrin garson: we think we can, indeed, lower the cost of some operations, but i am afraid we are going tojust basically disagree on the general picture that producing electronically would be significantly less expensive.allen bard, university of texas: isn't it, though, a question of taking a cameraready copy from what is nowprepared for presentation? i think if you use cameraready copy, that is, you use a disk the author supplies withvery little massaging or fixing up, it is going to be a lot cheaper than what acs does in a fair amount ofproduction.lorrin garson: that is true. however, we get 95 to 97 percent of our manuscripts in soft copy, and we do alot of manipulation to get that data into the database. if it were cameraready copy and we could just print it as wegot it, the cost would be dramatically lower.allen bard: right.stephen heller: lorrin, that is one of the things this particular journal does. everything comes in alreadyformatted in html. so, it is essentially the cameraready copy that al is talking about. it is not that it is cheaper;it is just that the burden of the cost goes to the author as opposed to the publisher.electronic journal publishing at the american chemical society208impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.allen bard: but the author, if i can interject, from our experience doesn't do that great a job. most authorsdon't want to be publishers, and so they will do something, but it is not what the publisher does. it may be stilllegible, and it depends on what you want to get out of it.robert lichter, camille & henry dreyfus foundation: i have a series of questions. from discussions ihave had over the last couple of years with a number of folksšlibrarians, publishers, acs peopleši do want tocommend acs for really taking the lead on this and looking out for the interests of its membership.one librarian, in a discussion about the societies versus commercial publishers, said with considerable heatand no small amount of vigor that acs is a business, and one should not distinguish between acs andcommercial publishers. i tried to argue that, but i would like to hear from other people about some of thearguments for making that distinction.another question i have regards your comment about scientists preferring to publish in prestigious journals.what actually defines a prestigious journal when you really get down to it?third, there have been increasing calls for sort of selfpublished journalsšthe journal that publishes perhaps20 papers a year on a superspecialized topicšarguing that because the costs can be so low, it is possible to haveany number of these small highly specialized journals, more specialized than the current specialized journals, andthat that is a good thing for communication of scientific results. you have probably heard this, too.i would like to hear your views and others' views on that assertion.lorrin garson: on the issue that acs is a business, the acs does run in a businesslike manner, no questionabout that. if we didn't, we would be out of business. the fact is the acs is a legitimate notforprofitorganization, and as i said earlier, we are also notforloss. a librarian or any other individual certainly could lookat the acs as a commercial business because it is run in a businesslike manner, but the reality is that the acs is itnot a commercial publisher. i doubt if there will be any resolution of this debate.robert lichter: the next question was that of the very small, specialized journals that can be published soeasily. the argument is whether that is good for scientific unity.lorrin garson: i think that the biggest single thing missing in that model is marketing. that notion impliesthat one doesn't have to market, that you can put up a journal on the web, send a few email messages to yourfriends, and everybody will know about it. it doesn't work that way. marketing is a very important part ofpublishing.electronic journal publishing at the american chemical society209impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.session 4panel discussionsam kounaves, tufts university: there were some questions earlier about the increasing volume of ourpublications. most of us are both producers and consumers and at least in my opinion, and i don't know if it is mycolleagues', the volume will not ever decrease. in fact, it is probably going to get worse, similar to grade inflation.nobody is going to take the fast step to turn the volume down, because we use publications in tenure and so on.so, it's never going to happen.we have some small societies that seem to have been successful in publishing their journals cheaply. i don'tknow if anybody has heard of the society of entomologyšits journal of entomology is basically free to themembers, a small group. i also belong to a small society, with about 500 or 600 members, for which i have set up aweb site and put the society newsletter on it. so i have had some experience with trying to start one, and have seensome of the problems involved with it, but on the other hand, it seems to me we may be starting afresh. we are notbound by tradition, as some of the larger societies might be, as to what we can or cannot do. some of mycolleagues don't agree with this, but i feel it is possible to publish a refereed, highquality journal.one of a society's missions many years ago was to gather information about members' research anddisseminate it among the members. thus, it seems it is very appropriate that the societies take a leading role injournal publishing. libraries can be an archiving location for certain things. the societies could be an archivingsource for the society publications. for, example, the acs could be an archiving location for acs journals. onehundred years from now the journals would still be there. granted they might undergo several transitions inarchiving media, but they would still be there and would still be archived in some format.so, one question is, is it not feasible for a society like acs to take on archiving and to look at such things aspage charges for authors and look at these smaller societies that seem to be doing successful publishing, like thesociety of entomology? is acs looking at this and is it possible to do this sort of thing?lorrin garson: we are committed to archiving the acs journals, no question about that. the issue of pagecharges brings back memories. the acs has page charges in a few of its journals. these areelectronic journal publishing at the american chemical society210impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.vestiges of a time when there were page charges in most of our journals. however, in competing with thecommercial journals, which do not charge page charges, we felt we could no longer charge page charges.there are, also, tax implications. the irs said that if page charges are mandatory then scientific journals areadvertising, and you must mark every piece of paper published in that journal, "this is an advertisement." wedidn't want to do that. i don't know if the irs has changed its position on this.the acs board of directors has said that we should eliminate all page charges, which even now arevoluntary, as soon as we can. that decision was made about 15 years ago, and we are still tagging along with asmall number of journals with page charges. page charges don't seem to be a way to go as best as we can judge.richard lucier: i would like to comment on the notion that small societies handle a large amount of theinformation that needs to be managed. i agree with you that that seems to be a reasonable way to go, but myconcern is how that scales over time. what i have observed over the years with respect to the development ofscientific databases is that they often start out as a "cottage industry" product. developing the database was aninteresting and innovative activity; an individual researcher/member of a society took it on for a while, but as theneed for access, the importance of reliability, and changes in technology occurred, an infrastructure more robustthan the "cottage industry" could provide was needed. societies like acs can provide some of that infrastructure.it is my belief that a federation of universities could logically provide that to groups of small societies as well. insome respects, highwire press is doing that in the biomedical sciences for a number of societies, providing aninfrastructure that does scale. each of those societies would have a great deal of difficulty building thatinfrastructure on its own.gary mallard: one thing you said, lorrin, that i would have to disagree with, concerns marketing. youactually don't have to market things. the webbook, which gets a lot of usage, has never spent a nickel onadvertisement, has never done anything, and it is a little bit like the baseball field in iowa. if you build it, they docome, and if you build it with a reasonably highquality product, i think they will come in droves.lorrin garson: especially if it is free.gary mallard: of course.robert lichter, camille & henry dreyfus foundation: that gets back to the question of prestige.allen bard, university of texas: i can tell you my view. i think there are quantitative measures of it forwhatever you want to look at, like impact factors, and although i don't fully agree with that, that goes along prettymuch with a kind of community opinion. i know that certain departments, when they make tenure decisions, lookat the list of publications, have a numerical multiplication factor, a division factor for different journals, youknow, science, nature, jacs, and so on. so, there is, i think, a culture that believes this.lorrin garson: another factor that might play into prestige is rejection rate. prestigious journals, shall wesay, tend to have a much higher rejection rate, which probably leads to a higher quality. they publish the morehighquality material, and i think the community, certainly people who have published for any period of time,have a pretty good sense as to which journals have a high rejection rate and which don't.electronic journal publishing at the american chemical society211impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.now, after you get beyond a certain point, if you are rejecting 90 percent of the papers, obviously you are rejecting alot of good papers. you can identify the trash and the truly outstanding, but 95 percent of the material in themiddle is much more difficult to judge. but yes, i think rejection rate is a factor in prestige.christos georgakis, lehigh university: i wanted to ask a question about copyright in this electronic age.let me run you through the steps that one might take. i write a paper. i put it on my web site and i label itﬁsubmitted for publication.ﬂ i get the reviews back. i do some editing, and of course i assign the copyright to thejournal that is going to publish it, but i still leave the original copy of the paper on my web page for people todownload. am i in conflict of the copyright?lorrin garson: this is good question, but i dislike answering questions about copyright because i know justenough to be dangerous. the issue is that you owned copyright when you first created that article. the question iswhether you signed over those rights to anybody. if you haven't you are not in violation of copyright. you own thecopyright.to publish in acs journals a requirement is that you transfer copyright. at that point you would indeed be inviolation of copyright because you no longer own the article. so, you would be asked to take the article down. wehave a policy that if authors want to post acs articles on their intranet, that is, for their institution, that is quite allright, but it is not allowed to post them for general access.others of our journals have a policy with regard to what has been posted on the web. some of the editorsfeel rather strongly that if something is on the web it is already published. it is open to the public, and they wouldnot consider it for publication. other acs editors feel that posting it on the web is equivalent to speaking at aconference. it is not true publication. it is something in between, and they would consider it for publication. so,there are many factors that come into your question.gary mallard: i would like to ask you a question, lorrin. do you think that that mode of thinking serves thescientific community, or does it just serve the acs?lorrin garson: here is the problem from the publisher's standpoint. if we do not ask for copyright, forexample, then let us say 5 years from now we want to do something with the collection of journals that we cannotanticipate, that means we would have to go back to all 30,000 authors per year to get permission. this isimpractical, which limits the opportunities we would have.gary mallard: like what?lorrin garson: i cannot imagine what we might want to do. what we have done in the past, for example, ismake the journals available on the web. had we not owned copyright we could not have done that.gary mallard: you could have been assigned copyright for that purpose though but not necessarily haveowned itšcopyright can be held jointly. you don't have to have exclusive copyright and as a matter of fact theacs does not have copyright to any work by government employees. so, things get published by the acswithout the acs holding the copyright, and it doesn't in any way restrict you from using that material. thatwould, i think, answer this question about whether authors can put articles up on their own web sites, which ithink a lot of people would like to do,electronic journal publishing at the american chemical society212impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.jack kay, drexel university: i was curious about your statement that the length of the articles beingpublished is increasing regularly. what is the explanation for that? do you know?lorrin garson: i am not sure. we haven't done a formal study. in looking at articles i see two things. one isan increasing number of references. there is more literature to reference, and authors seem to be referencing it.so, the number of references has gone up.i think there are in some cases a lot more data to be published than there have been before just because ofmodern instrumentation, but these are only casual observations on my part.al, do you have any sense of this issue?allen bard: i sense that papers now are more complex and more data intensive, and the effort we have beenmaking in what i consider an evolutionary period is to get more of that put on the web, in other words at least inthis intermediate period to let the printed version reflect the core of an article but try to get more and more of thisother stuff and supporting information onto the web. different communities within chemistry agree with this todifferent extents. that would be my best guess without any formal study.david smith, dupont: as i listen to this conversation i believe there is an issue here. i look at threeimportant transformations in this process; one is data to information, then information to knowledge, and finallyknowledge to understanding. it seems to me that understanding usually ends up in textbooks, and where we arehung up in the publication game is in whether we should be publishing information or publishing knowledge.it seems to me that it usually requires a lot of work to transform information into useful knowledge, and isubmit that in some of the journals that i read, most of what gets published is information. a lot of it is probablyuseless information, and we are not doing a good enough job of reviewing papers to weed them out.allen bard: anybody want to address that? david, do you want to specify some journals? i tend to agree,and i think the librarians should be stronger in gauging the quality of the journals they are taking and in findingand probably not subscribing to journals that are largely of that nature.richard lucier: let me add briefly that librarians subscribe to the journals that are mandated by theirfaculty, and we do not feel that it is our responsibility to do that quality control, that it is really faculty'sresponsibility to do that.stephen heller, national institute of standards and technology: full copyright is not necessary for apublisher. a license to publish, with the commercial rights to use it, is really sufficient for a publisher to earnincome, and full copyright isn't needed. a committee of which i am a member wrote a policy article on this issueof copyright versus a license to publish. this group was sponsored, in pan, by the dreyfus foundation, and thecommittee, the transition from paper, put something in science about 2 months ago (september 4, 1998, pages14591460), and i was wondering if there are any comments from the panel members about the notion of having alicense to publish and what would be insufficient (for publishers) about that?lorrin garson: i think i have said enough about copyright and done all the damage i care to do! i will let theother panelists try to address this.electronic journal publishing at the american chemical society213impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.gary mallard: i am not going to touch it because we are at the other end of it. we actually are going out andgetting data out of the literature. i think the risk that you run when creating an electronic database is that peoplemay feel like you are infringing upon their copyright when you take the data out of the literature and put it into anelectronic format. the whole issue of fair use, which i don't think has been addressed in any very realistic waywhen it comes to electronic format, is still something that may well end up in the courts. i don't know whether it isgoing to end up in the courts for scientific data because there is just not enough money sloshing around inscientific data, but it may well end up there for other reasons. there is this historic notion of fair use and what youcan and cannot do, and i don't know where we are on that.richard lucier: i think you as a community have to decide what you want to do about copyright. if you as acommunity decide that you are going to do what was specified in that editorial i cannot imagine that the acs, aswell as other publishers, won't continue to publish your materials. i think it is truly up to you as a community, and iwould encourage you as a community to look very carefully at the recommendations that were in that editorial asbeing potentially desirable ones to act upon.randy collard, dow chemical company: just a quick question for lorrin. you identified in your costanalysis the database as the largest factor. as you look at the advances in technology and expertise, what do yousee that is a breakthrough necessary in that area?lorrin garson: i think it is likely there will be a number of small things that accumulate to provide greaterefficiency, not necessarily one large piece of technology. for example, within our own environment we are gettingmore and more clever at writing parsers so that we can take information in various word processing packages andparse them and identify elements algorithmically. that particular piece in itself i think will lead to significantsavings.as far as technology coming down the road, it is very difficult to predict what may have a dramatic effect.certainly as i look back over the last 20 to 25 years the issue of database publishing in itself undoubtedly hascontributed very significantly to our cost containment and the ability to produce electronic products. the numberof people involved with producing the electronic journals and doing quality control is two or three people. tomanage all that data is a tribute to the database approach to publishing.robert lichter: i would like the views from the remaining panelists about the following: recently there hasbeen a lot of legislative heat and not too much light about the issue of copyright, which seems to have beentemporarily resolved. do you think this issue is going to emerge again? how do you think the concept ofcopyright will change in light of electronic publishing?gary mallard: from the point of view of someone who tries to pull information out, the european view ofcopyright is distinctly dangerous, i think, because in effect where the european community is heading is basicallythat you can copyright the telephone book. if that becomes the case, it seems to me that you are a very small stepaway from copyrighting the boiling point of methane, and certainly american law has said that you cannotcopyright the boiling point of methane. i don't know where this issue is goingšyou can argue that the current lawthat was just passed by congress and signed comes very close. if you have put the boiling point of methane into acompilation, then it has the potential to be copyrighted. i think it is extraordinarily dangerous for our ability todeliver scientific information economically.electronic journal publishing at the american chemical society214impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.appendixes215impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.216impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.appendix alist of workshop participantsmadeline adamczeski, american universityrichard c. alkire, university of illinois at urbanachampaignherman l. ammon, university of marylandraymond a. bair, pacific northwest national laboratoryallen j. bard, university of texasrodney bartlett, university of floridajohn s. binkley, sandia national laboratoriesdonald m. burland, national science foundationbridget carragher, university of illinois at urbanachampaignrandy s. collard, the dow chemical companyrobert cordova, elf atochem, inc.peter t. cummings, university of tennesseelawrence a. curtiss, argonne national laboratoryrobert de levie, georgetown universitydavid a. dixon, pacific northwest national laboratorydouglas j. doren, university of delawarerichard dubois, national institutes of healththom h. dunning, jr., pacific northwest national laboratorythomas f. edgar, university of texaskarolyn k. eisenstein, national science foundationstephen elbert, national science foundationthomas a. finholt, university of michiganfarley fisher, national science foundationjoseph francisco, purdue universitysusan fratkin, southeastern universities research associationjean h. futrell, pacific northwest national laboratorybarbara j. garrison, pennsylvania state universityappendix a217impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.lorrin r. garson, american chemical societychristos georgakis, lehigh universityevelyn goldfield, wayne state universitysusan l. graham, university of california, berkeleyjane griffith, national research councilstephen r. heller, national institute of standards and technologyjudith hempel, university of california, san franciscorichard l. hilderbrandt, national science foundationrichard hirsh, national science foundationdaniel hitchcock, u.s. department of energyjack kay, drexel universityrichard kayser, national institute of standards and technologywilliam h. kirchhoff, u.s. department of energymichael l. knotek, u.s. department of energysamuel p. kounaves, tufts universityrobert l. lichter, camille & henry dreyfus foundationrichard e. lucier, university of californiaw. gary mallard, national institute of standards and technologyrobert s. marianelli, office of science and technology policypaul maupin, u.s. department of energydavid r. mclaughlin, eastman kodak companyl. eugene mcneese, oak ridge national laboratorygregory j. mcrae, massachusetts institute of technologypaul messina, california institute of technology and u.s. department of energywilliam s. millman, u.s. department of energyraul miranda, national science foundationjanet nelson, american chemical societyjanet g. osteryoung, national science foundationaristides patrinos, u.s. department of energyjohn b. pfeiffer, air products and chemicals, inc.john a. pople, northwestern universityclint potter, university of illinois at urbanachampaigngintaras v. reklaitis, purdue universitymichael e. rogers, national institute of general medicineceleste m. rohlfing, national science foundationl. david rothman, the dow chemical companyjoel h. saltz, university of marylandstanley i. sandler, university of delawarepeter schmidt, office of naval researchgustavo e. scuseria, rice universityjohn sessler, ballistic missile defense organization (retired)donald singleton, national research council of canadajeffrey skolnick, scripps research laboratorymichael e. smith, exxon r&d laboratoryw. david smith, e.i. du pont de nemours and companypeter r. taylor, san diego super computer center, university of california, san diegoappendix a218impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.michael thompson, pacific northwest national laboratoryjohn s. tse, national research council of canadamarek w. urban, north dakota state universitydavid l. venezky, naval research laboratoryandrew b. white, jr., los alamos national laboratorycarter white, naval research laboratoryg. edwin wilson, university of akronwilliam d. wilson, lawrence livermore national laboratorywilliam t. winter, environmental science & forestry, state university of new york, syracuserobert s. wood, rohm & haas research laboratoriesjeff yellets, allied signalcynthia zoski, university of rhode islandstaffdouglas j. raberdavid grannisruth mcdiarmidsybil paigeappendix a219impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved.appendix borigin of and information on the chemical sciencesroundtablein april 1994, the american chemical society (acs) held an interactive presidential colloquium entitled"shaping the future: the chemical research environment in the next century." the report from this colloquiumidentified several objectives, including the need to ensure communication on key issues among government,industry, and university representatives.1 the rapidly changing environment in the united states for science andtechnology has created a number of stresses on the chemical enterprise. the stresses are particularly importantwith regard to the chemical industry, which is a major segment of u.s. industry, makes a strong, positivecontribution to the u.s. balance of trade, and provides major employment opportunities for a technical work force. aneutral and credible forum for communication among all segments of the enterprise could enhance the futurewellbeing of chemical science and technology.after the report was issued, a formal request for such a roundtable activity was transmitted to dr. bruce m.alberts, chairman of the national research council (nrc), by the federal interagency chemistry representatives(ficr), an informal organization of representatives from the various federal agencies that support chemicalresearch. as part of the nrc, the board on chemical sciences and technology (bcst) can provide anintellectual focus on issues and fundamentals of science and technology across the broad fields of chemistry andchemical engineering. in the winter of 1996, dr. alberts asked bcst to establish the chemical sciencesroundtable to provide a mechanism for initiating and maintaining the dialogue envisioned in the acs report.the mission of the chemical sciences roundtable is to provide a scienceoriented, apolitical forum toenhance understanding of the critical issues in chemical science and technology affecting the government,industrial, and academic sectors. to support this mission, the chemical sciences roundtable will do thefollowing:1 shaping the future: the chemical research environment in the next century, american chemical society report fromthe interactive presidential colloquium, april 79, 1994, washington, d.c.appendix b220impact of advances in computing and communications technologies on chemical science and technology: report of a workshopcopyright national academy of sciences. all rights reserved. identify topics of importance to the chemical science and technology community by holding periodicdiscussions and presentations, and gathering input from the broadest possible set of constituenciesinvolved in chemical science and technology. organize workshops and symposia, and publish reports on topics important to the continuing health andadvancement of chemical science and technology.disseminate the information and knowledge gained in the workshops and reports to the chemical scienceand technology community through discussions with, presentations to, and engagement of other forumsand organizations. bring topics deserving further, indepth study to the attention of the nrc's board on chemical sciencesand technology. the roundtable itself will not attempt to resolve the issues and problems that it identifiesšit will make no recommendations, nor provide any specific guidance. rather, the goal of the roundtableis to ensure a full and meaningful discussion of the identified topics so that the participants in theworkshops and the community as a whole can determine the best courses of action.appendix b221
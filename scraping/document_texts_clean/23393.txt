detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/23393continuing innovation in information technology: workshopreport100 pages | 8.5 x 11 | paperbackisbn 9780309437240 | doi 10.17226/23393committee on continuing innovation in information technology; computer scienceand telecommunications board; division on engineering and physical sciences;national academies of sciences, engineering, and medicinecontinuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.20102005200019951990198519801975197019652010200520001995199019851980197519701965universityindustry r&dproducts$1 billion market$10 billion marketareas of fundamental research in itit sectors with large economic impactcomputer architecturesoftware technologiesnetworkingparallel & distributed systemsdatabasescomputer graphicsai & roboticsdigital communicationsinternet & webcloudcomputingentertainment& designenterprisesystemsrobotics & assistivetechnologiespersonalcomputingmicroprocessorsbroadband& mobilemotorolaqualcommiphonenvidiaamd inteltexas instrumentshpappledellsymantecebay akamaihpjuniperciscoyahoo!ibmelectronic artsfacebook twittergooglemicrosoftvmwareamazonoracleadobe autodeskipodnvidia pixarxboxnuanceintuitive surgicalirobotin information technology continuing innovationworkshop reportcontinuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.in information technology continuing innovationworkshop reportcommittee on continuing innovation in information technologycomputer science and telecommunications boarddivision on engineering and physical sciencescontinuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.the national academies press 500 fifth street, nw washington, dc 20001this publication is based on work supported by the national science foundation under grant no. iis 1343663. any opinions, ndings, conclusions, or recommendations expressed in this publication are those of the authors and do not necessarily re˚ect the views of the national science foundation.international standard book number13: 9780309437240international standard book number10: 0309437245digital object identier: 10.17226/23393additional copies of this report are available for sale from the national academies press, 500 fifth street, nw, keck 360, washington, dc 20001; (800) 6246242 or (202) 3343313; http://www.nap.edu.copyright 2016 by the national academy of sciences. all rights reserved.printed in the united states of americasuggested citation: national academies of sciences, engineering, and medicine. 2016. continuing innovation in information technology: workshop report. washington, dc: the national academies press. doi:10.17226/23393.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.iiiacknowledgment of reviewersthis report has been reviewed in draft form by individuals chosen for their diverse perspectives and technical expertise, in accordance with procedures approved by the report review committee. the purpose of this independent review is to provide candid and critical comments that will assist the institution in making its published report as sound as possible and to ensure that the report meets institutional standards for objectivity, evidence, and responsiveness to the study charge. the review comments and draft manuscript remain condential to protect the integrity of the deliberative process. we wish to thank the following individuals for their review of this report:ben shneiderman, university of maryland, butler w. lampson, microsoft research,kathleen kingscott, ibm corporation,robert f. sproull, university of massachusetts, amherst,edward d. lazowska, university of washington, andmark a. horowitz, stanford university.although the reviewers listed above have provided many constructive comments and suggestions, they were not asked to endorse the conclusions or recommendations, nor did they see the nal draft of the report before its release. the review of this report was overseen by samuel h. fuller, analog devices, who was responsible for making certain that an independent examination of this report was carried out in accordance with institutional procedures and that all review comments were carefully considered. responsibility for the nal content of this report rests entirely with the authoring committee and the institution.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.ivcommittee on continuing innovation in information technologypeter lee, microsoft research, chairmark dean, university of tennessee, knoxvilleedward frank, brilliant lime, inc., and cloud parity inc.yann lecun, new york universitybarbara h. liskov, massachusetts institute of technologyelizabeth mynatt, georgia institute of technologystaffvirginia bacon talati, program ofcershenae bradley, senior program assistantjon eisenberg, director, computer science and telecommunications board computer science and telecommunications boardfarnam jahanian, carnegie mellon university, chairluiz andre barroso, google, inc. steven m. bellovin, columbia universityrobert f. brammer, brammer technology, llcedward frank, brilliant cloud, inc., and lime parity, inc.seymour e. goodman, georgia institute of technology laura haas, ibm corporationmark horowitz, stanford universitymichael kearns, university of pennsylvaniarobert kraut, carnegie mellon university susan landau, google, inc.peter lee, microsoft corporation david e. liddle, us venture partners fred b. schneider, cornell universityrobert f. sproull, university of massachusetts, amherstjohn stankovic, university of virginiajohn a. swainson, dell, inc.ernest j. wilson, university of southern californiakatherine yelick, university of california, berkeleystaffjon eisenberg, director lynette i. millett, associate director virginia bacon talati, program ofcershenae bradley, senior program assistantemily grumbling, program ofcerrenee hawkins, financial and administrative manager for more information on cstb, see its website at http://www.cstb.org, write to cstb,national academies of sciences, engineering, and medicine, 500 fifth street, nw,  washington, dc 20001, call (202) 3342605, or email the cstb at cstb@nas.edu.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.vprefacethe 2012 national research council report continuing innovation in information technology, produced by the computer science and telecommunications board (cstb), illustrates how fundamental research in information technology (it), conducted at industry and universities, has led to the introduction of entirely new product categories that ultimately became billiondollar industries. it uses examples to depict the rich interplay between academic research, industry research, and products and indicates the crossfertilization resulting from multidirectional ˚ows of ideas, technologies, and people. it uses a graphic (reproduced with a correction in the introduction to this report) to portray and connect areas of major investment in basic research, universitybased (and largely federally funded) research, and industry research and development; the introduction of important commercial products resulting from this research; billiondollarplus industries (by annual revenue) stemming from it; and presentday it market segments and representative u.s. rms whose creation was stimulated by the decadeslong research. the graphic, which is of necessity incomplete and symbolic in nature, provides a framework within which additional contributions and connections can be documented and illustrated. at a cstbhosted workshop on march 5, 2015, leading academic and industry researchers and industrial technologists described key research and development results and their contributions and connections to new it products and industries, and illustrated these developments as overlays to the 2012 ﬁtire tracksﬂ graphic (see box p.1 for the statement of task). the principal goal of the workshop was to collect and make available to policy makers and members of the it community rstperson narratives that illustrate the link between government investments in academic and industry research to the continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.viultimate creation of new it industries. although the original plan was to have speakers also prepare papers, it proved more effective to prepare summaries of the workshop presentations based on a transcript of the proceedings and give the speakers an opportunity to review the summaries for accuracy and completeness.this report provides summaries of the workshop presentations organized into ve broad themesš(1) fueling the innovation pipeline, (2) building a connected world, (3) advancing the hardware foundation, (4) developing smart machines, and (5) people and computersšand ends with a summary of remarks from the concluding panel discussion. the narratives provide only a limited sample of the it research ecosystem and cannot capture the full range of challenges, failures, or successes that are inherent to any research eld. they do, however, provide compelling illustrations of how academic and industry research has underpinned innovation in it and has had signicant economic and other societal impacts. peter lee, chaircommittee on continuing innovation in information technologybox p.1 statement of taskan ad hoc committee will plan and conduct a public workshop that would highlight additional examples of the impacts of computing research using the framework established in the ﬁtire tracksﬂ gure published in cstb™s 2012 report  and explore further uses of the gure and framework. the committee will develop the agenda for the workshop, select and invite speakers and discussants, and moderate the discussions. invited technical leaders and researchers (primarily from industry) would use the framework to make presentations describing how academic and industry research has underpinned innovation in information technology with signicant economic or other societal impact. workshop participants would engage in discussions that build on these presentations to consider how the framework can be used to collect, display, and analyze what is known about the interplay between academic and industry research; the multidirectional ˚ows of ideas, technologies, and people; and the impacts of research. a summary report will be prepared of the presentations and discussions at the workshop. continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.viicontentsintroduction 11 fueling the innovation pipeline 7 applicationengaged research for computer science 8 grounding your work 8 taking turns 9 government funding and the amplication of good ideas 9 motivators and outcomes for government, academia, and industry 10 a brief history of innovation in technology 10 course corrections and unexpected turns 11 the implications of motivations 11 government funding for industry? 13 retaining the leading edge 14 the need for research to match our aspirations 15  the unpredictability paradox 17 building bridges between academia and industry 18 2 building a connected world 20  evolving the internet 20 in the beginning: the story of arpanet 21 the internet takes shape 22 the internet reaches out 23 developing the world wide web 24  lessons learned 25 the internet of everything 25  the evolution of enabling technologies 26  a tipping point 27 the wireless future 28  confronting our bandwidth shortage 29  the need to rethink network design 30 toward a seamless network experience 31 the path from research to innovation 323 advancing the hardware foundation 33  developing disruptive architectures 34  hitting inevitable limits 34 a history of innovation from the niche to the mainstream 35 a disruptive moment 36continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.viii the winding path of wearables 37  why wearables? 37 from fiction to fact 384 developing smart machines 41  making machines learn 42  how to school a computer 42 tapping big data 43  creating the multilingual computer 44  harnessing the wisdom of crowds 45 achievements in articial intelligence 46 creating the theoretical foundation 46  impacts and achievements of research on intelligent machines 48  promising prospects for the future 49  robotics: from vision to reality 50 slam dunk 50 learning from nature 51  cultivating a softer side 52 5 people and computers 54  seeking cybersecurity 55 security in transportation 56  fighting spam and piracy 57 the usercentered design renaissance 59  building toward a sea change 60  a new way of creating technology 61  harnessing big data for social insights 62  from social science to social media (and back again) 63 a new way to do research 63  a prolic data ecosystem 66 6 wrapup discussion 67 appendixesa committee biographies 73 b presentations 78c presenter biographies 80 continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.1the past 50 years have brought tremendous advances in information technology (it). rapid improvements in hardware, software, and networking capabilities have transformed our everyday lives and enabled extraordinary scientic discoveries and engineering achievements. people today are virtually surrounded by information, thanks to the myriad technologies they have developed to capture, store, process, and share it. from the inner workings of the human brain to the complex mechanics of the global economy, it is crucial to revealing how the world works and to developing datapowered innovations to improve people™s lives. the evolution from roomsized punched card computers to today™s ubiquitous mobile devices, social networks, and ever˚owing streams of big datašall in an exceedingly short period of human historyšis remarkable. yet these developments were not a foregone conclusion. few of the technologies now taken for granted could have been imagined at their beginning. even for keen observers and visionaries, it is rarely obvious how incremental improvements, or even signicant technological leaps, will spark radically new applications across diverse elds and industries. it is thus only in hindsight that the true value of precursor technologies becomes apparent. take for example, two major technological breakthroughs that occurred in 1969: man walked on the moon, and a group of computer scientists used a new approach called packet switching to send the rst message from one computer to another, a step that paved the way for the development of the internet. although hundreds of millions of people breathlessly watched the moon landing on live television, it is only in hindsight that it can be appreciated how profoundly packet switching would come to affect the daytoday lives of future generations. it is clear that technology profoundly matters in today™s economy and society. it underpins economic prosperity and national security and accelerates the pace of scientic introductioncontinuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.2discovery across all elds. the internet has been shown to directly support 21 percent of gross domestic product (gdp) growth in mature economies.1 the value that information and communication technologies bring to the u.s. gdp grew by nearly 10 percent between 20082013, and this sector represented 5.7 percent of the u.s. gdp in 2013.2 with federal funding in scal year 2010 of less than 0.03 percent of u.s. gdp for networking and information technology research and development, this area brings a substantial return on investment for government funding.3 jobs in software development are projected to grow 17 percent from 2014 to 2024 to keep up with industry demand.4 what propelled past technological developments, and how can that momentum continue to be built on? what lessons can be gleaned from past successesšand from failures? how can technological creativity and knowhow be channeled across government, academia, and the business sector to support a more prosperous, healthy, and secure future? these are some of the questions the national academies of sciences, engineering, and medicine5 has tackled in a series of workshops and consensus studies over the past 20 years. the 1995 national research council (nrc) report evolving the high performance computing and communications initiative to support the nation™s information infrastructure, by the computer science and telecommunications board (cstb), offered an overview of the development of highperformance computing and communications technologies along with 13 recommendations for supporting these technologies.6 a notable gure included in that report, often called the ﬁtire tracksﬂ diagram because of its resemblance to such markings, garnered signicant attention in the halls of congress, among the leadership of federal agencies, and across the research and innovation policy community. the gure, which has subsequently been updated several times, illustrates the degree to which the it industry builds on governmentfunded university research, often over incubation periods of years or decades. a few years after that seminal report, the 1999 nrc report funding a revolution: government support for computing research reviewed key advances fueled by governmentsupported research and articulated the economic rationale for government funding in this 1j. manyika and c. roxburgh, 2011, the great transformer: the impact of the internet on economic growth and prosperity, mckinsey global institute, http://www.mckinsey.com/industries/hightech/ourinsights/thegreattransformer.2u.s. department of commerce, bureau of economic analysis, ﬁinteractive access to industry economic accounts data: gdp by industry,ﬂ release date april 21, 2016, http://www.bea.gov/itable/itable.cfm?reqid=51&step=1#reqid=51&step=51&isuri=1&5114=a&5102=15 [path from www.bea.gov: interactive data/gdpbyindustry/begin using the data/gross output by industry/gross output by industry (a) (q)/annual/next step].3networking and information technology research and development, 2009, fy2010 supplement to the president™s budget, may, http://www.nitrd.gov/pubs/2010supplement/fy10suppfinalformatweb.pdf.4u.s. department of labor, bureau of labor statistics, 2016, occupational outlook handbook, 201617 edition: software developers, http://www.bls.gov/ooh/computerandinformationtechnology/softwaredevelopers.htm.5effective july 1, 2015, the institution is called the national academies of sciences, engineering, and medicine. references in this report to the national research council are used in an historical context identifying programs prior to july 1.6national research council (nrc), 1995, evolving the high performance computing and communications initiative to support the nation™s information infrastructure, national academy press, washington, d.c.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.3area.7 the 2003 report innovation in information technology explored how decisions about fundamental computer science research affect progress in information technology and expanded upon the tire tracks diagram.8 the 2009 report assessing the impacts of changes in the information r&d ecosystem: retaining leadership in an increasingly global environment examined the erosion of the u.s. leadership role in the it sector.9 it contains a summary of key lessons from the 2003 report, reproduced here in box i.1.most recently, the 2012 nrc report continuing innovation in information technology described the growing size and importance of the it sector and offered the most recent update of the tire tracks diagram10 (reproduced, with a correction,11 in figure i.1).the diagram illustrates how fundamental research in it, conducted in industry and universities, has led to the introduction of entirely new product categories that ultimately became billiondollar industries. it uses examples to depict the rich interplay between academic research, industry research, and products and to indicate the crossfertilization resulting from multidirectional ˚ows of ideas, technologies, and people. each arrow linking tracks in the gure represents a documented ˚ow of technology within or across areas. it uses a graphic to portray and connect areas of major investment in basic research, largely at universities and largely federally funded, and industry r&d. it also shows the introduction of signicant commercial products resulting from this research, billiondollarplus industries (by annual revenue) stemming from this research, and presentday it market segments and representative u.s. rms whose creation was stimulated by the decadeslong research. the graphic, which is of necessity incomplete and symbolic in nature, provides a framework within which additional contributions and connections can be documented and illustrated. a common thread running through these past academies reports has been a core nding that many of the technological breakthroughs and impacts seen over the past decades have resulted from a innovation ecosystem at the intersection of the federal government, academic research, industry research and development, and product development. these reports demonstrate how the governmentœacademiaœindustry it innovation ecosystem works, why it works, and what the future prospects for such research could 7nrc, 1999, funding a revolution: government support for computing research, national academy press, washington, d.c.8nrc, 2003, innovation in information technology, the national academies press, washington, d.c.9nrc, 2009, assessing the impacts of changes in the information r&d ecosystem: retaining leadership in an increasingly global environment, the national academies press, washington, d.c.10nrc, 2012, continuing innovation in information technology, the national academies press, washington, d.c.11the computer architecture to microprocessors track in figure 1.1 has been corrected from the 2012 version. the computer architecture to microprocessor track in the 2012 version of the figure had its origins in the reduced instruction set computing (risc) track in the 1995 figure. however, given that the current track is labeled in terms of computer architecture and microprocessors more generally, it is more accurate to (1) adjust the academic and industry research tracks to start in 1965 because architecture research predates microprocessors and indeed goes back to the origin of computing in the 1940s and (2) reflect the market size for microprocessors more generally by starting the product track in 1971 (when intel released the 4004), and making the line solid at 1981, when the microprocessor industry reached $1 billion in revenue.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.4box i.1 lessons about the nature of research in information technologyša summarythe results of researchšamerica™s international leadership in itšleadership that is vital to the nationšsprings from a deep tradition of research. . . .šthe unanticipated results of research are often as important as the anticipated resultsšfor example, electronic mail and instant messaging were byproducts of research in the 1960s that was aimed at making it possible to share expensive computing resources among multiple simultaneous interactive users. . . . šthe interaction of research ideas multiplies their impactšfor example, concurrent research programs targeted at integrated circuit design, computer graphics, networking, and workstationbased computing strongly reinforced and amplied each another. . . . research as a partnershipšthe success of the it research enterprise re˚ects the complex relationship between government, industry, and universities. . . .šthe federal government has had and will continue to play an essential role in sponsoring fundamental research in itšlargely universitybasedšbecause it does what industry does not and cannot do. . . . industrial and governmental investments in research re˚ect different motivations, resulting in differences in style, focus, and time horizon. . . .šcompanies have little incentive to invest signicantly in activities whose benets will spread quickly to their rivals. . . . fundamental research often falls into this category. by contrast, the vast majority of corporate research and development (r&d) addresses product and process development. . . .šgovernment funding for research has leveraged the effective decision making of visionary program managers and program ofce directors from the research community, empowering them to take risks in designing programs and selecting grantees. . . . government sponsorship of research, especially in universities, also helps to develop the it talent used by industry, universities, and other parts of the economy. . . .the economic payoff of researchšpast returns on federal investments in it research have been extraordinary for both u.s. society and the u.s. economy. . . . the transformative effects of it grow as innovations build on one another and as user knowhow compounds. priming that pump for tomorrow is today™s challenge.šwhen companies create products using the ideas and workforce that result from federally sponsored research, they repay the nation in jobs, tax revenues, productivity increases, and world leadership. . . .source: reprinted from national research council, 2009, ˙˛˘ the national academies press, washington, d.c., p. 33, summarizing national research council, 2003, ˙˛˘ the national academies press, washington, d.c., pp. 24.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.5figure i.1 examples of the contributions of federally supported fundamental research to the creation of it sectors, rms, and products with large economic impact. source: reprinted from national research council, 2012, the national academies press, washington, d.c. 20102005200019951990198519801975197019652010200520001995199019851980197519701965universityindustry r&dproducts$1 billion market$10 billion marketareas of fundamental research in itit sectors with large economic impactcomputer architecturesoftware technologiesnetworkingparallel & distributed systemsdatabasescomputer graphicsai & roboticsdigital communicationsinternet & webcloudcomputingentertainment& designenterprisesystemsrobotics & assistivetechnologiespersonalcomputingmicroprocessorsbroadband& mobilemotorolaqualcommiphonenvidiaamd inteltexas instrumentshpappledellsymantecebay akamaihpjuniperciscoyahoo!ibmelectronic artsfacebook twittergooglemicrosoftvmwareamazonoracleadobe autodeskipodnvidia pixarxboxnuanceintuitive surgicalirobotcontinuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.6be, depending on howšand how muchšthe nation invests in it. it is from this context that the impetus for the continuing innovation in information technology workshop emerged. with support from the national science foundation (nsf), the cstb of the national academies of sciences, engineering, and medicine convened a committee of experts to organize and host a workshop exploring how academic and industry research has underpinned innovation in information technology and has had signicant economic or societal impact. chaired by peter lee, corporate vice president of microsoft research, the workshop provided a venue for invited technical leaders and researchers, primarily representing the business sector, to exchange rstperson narratives illustrating the link between government investments in academic and industry research and the ultimate creation of new information technology industries. speakers were asked to build upon the framework of the tire tracks diagram to collect, display, and analyze what is known about the interplay between academic and industry research; the multidirectional ˚ows of ideas, technologies, and people; and the impacts of research in this area. held in washington, d.c., on march 5, 2015, the workshop included 15 presenters (see appendix c). by collecting and comparing narratives from multiple it elds and applications, this report offers a window into how government funding has directly and indirectly led to innovations that have yieldedšor are poised to yieldšhuge economic gains nationally and globally. speakers traced the roles of funding and leadership from government bodies such as nsf, the defense advanced research projects agency, the national aeronautics and space administration, the ofce of naval research, and the u.s. congress in enabling progress across a wide range of technologies and applications, including mobile applications, wearable technology, robotics, articial intelligence, wireless technology, cybersecurity, and numerous other areas. underlying many of these stories is a common theme that government funding and academic research not only have made considerable past contributions to the knowledge foundation on which the it industry is built, but also have played a unique and essentially irreplaceable role in the development of groundbreaking new technologies. while the structures and incentives of the business sector are ideal for incrementally improving products and capitalizing on new technologies to create valuable products and services, government and academia are best suited to advance transformative research. it is through the combination of and interchange among all of these sectors that we can reap the biggest gains. it has yielded uncountable economic, scientic, and qualityoflife benets over the past decades. understanding how the innovation ecosystem works is critical to keeping it going in the decades to come.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.ask someone to draw a picture of an innovator and you™re likely to get some version of thomas edison or benjamin franklinša prolic lone genius fueled by astounding creativity and an almost magical ability to intuit what the world needs. in truth, however, innovation is not so much a person as a processšand a rather meandering, messy, and long process at that. take a close look at any single information technology (it) advance and you™re bound to nd behind it a sprawling network of inventors, researchers, engineers, investors, and precursor technologies. though it certainly has its heroes, innovation in information technology is the story of collaboration, borrowing, and exchange among many, many contributors over the course of years and decades. neither the private sector, nor university researchers, nor the federal government has a monopoly on it innovation. it is the interplay among these contributors, with their disparate motivations, strengths, and limitations, that creates the innovation ecosystem in which theories and ideas can lead to the experimentation that spawns technologies and, ultimately, applications. in this chapter, three leading innovators dissect the researchtoapplication pipeline from different perspectives: deborah estrin on the value of applicationengaged research; robert colwell on the motivations of different stakeholders within the it innovation ecosystem; and farnam jahanian, on the sometimes unpredictable journey from insight to innovationšand the imperative for the united states to remain at the forefront of it. fueling the innovation pipeline continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.8applicationengaged research for computer science in the medical community, the process of translating ndings from fundamental research into practical applications is known as translational research. it™s a process that can take 1020 years as basic chemistry or biology research is applied to develop new drugs, medical devices, or other innovations that improve medical care. the analogous process in computer science is sometimes called ﬁapplicationengaged research,ﬂ and in this eld the inventiontoinnovation cycle can happen far more quickly. this rapid cycle is a signicant driver behind the enormous growth in the technology sector. a presentation by deborah estrin, professor of computer science at cornell tech and professor of public health at weill cornell medical college, focused on this critical relationship between the invention of a new tool or technique and the innovation that happens through its use. it was a theme echoed throughout much of the workshop, from rodney brooks™s exploration of the backandforth process of building robots to jaime carbonell™s description of datadriven machine learning techniques.estrin has spent her career on applicationengaged research, often at the intersection of technology and health. a pioneer in the eld of networked sensingšthe use of mobile and wireless technology to collect realtime data about the physical worldšshe currently directs the small data lab at cornell tech. there, her team develops technologies that harness what she terms ﬁsmall data,ﬂ the small bits of information generated from the personal technology we use every day, for applications that support healthy living and other goals. grounding your work estrin has long been focused on the applicationengaged research, which she calls ﬁgrounding your work.ﬂ she pointed to advice she received from jim waldo, now chief technology ofcer at harvard university, that helped crystalize the focus on the solutionoriented research that has characterized her career: waldo called for researchers to avoid wasting time thinking of creative problems, and instead spend time thinking of creative solutions to problems that someone in the world has articulated. another remark of estrin: she has always remembered judea pearl, professor of computer science at the university of california, los angeles, a 2011 turing award winner and a renowned researcher in the eld. pearl once commented to a ph.d. student, ﬁthat your approach is generalizable does not release you from the responsibility of showing us one thing it actually does.ﬂ to estrin, this sums up the idea that innovation is most successful if it is grounded in actual use. she cited government programs as having fueled much of her work. even serving on governmentled committees has had a huge impact on the applicationengaged work continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.9she does today: her tenure at the information science and technology study group of the defense advanced research projects agency (darpa) inspired her to pursue networked sensing. she said darpa™s other programs, such as sensit (sensor information technology) and nest (network embedded systems technology), allowed her to keep that research moving forward.taking turnsthe relationship between invention and innovation, or research and application, is a twoway street, not a unidirectional ˚ow, said estrin. she continued, ﬁwhen you™re doing this kind of multidisciplinary applicationdriven work, as margaret [martonosi] said a decade ago, you have to take turns.ﬂ to estrin, this means that researchers and technologists should not insist on innovating on the ﬁhowﬂ and the ﬁwhatﬂ at the same time, but rather oscillate between the two in order to solve both theoretical and practical problems effectively. furthermore, any line of inquiry may be rapidly and surprisingly enriched by development from another line, propelling one™s own work forward.as an example, estrin and her team leveraged funding from the national science foundation (nsf) to include different domain experts in their work in order to take turns to propel coinnovation in networked sensing. while she was conducting her research into networked sensing, smartphone use rose substantially and at the same time there was a big leap forward in methods of statistical analysis. pairing those developments with her own inventions inspired her research focus today: improving health management using mobile devices, sensors, and the digital transactions of individuals. estrin observed that the nsf™s science and technology centers program has given her research group the ﬁfunding and time to really bring the domain experts into the same room and process for a long enough period of time that we could take on authentically applicationdriven problems that transformed both the applications and the technology.ﬂ according to her, once a product can be used, ﬁreality gets to push back,ﬂ and it is this push and pull between research and the real world that drives innovation. in her own research experience, she said, ﬁscientists and engineers who were trying to measure something specically gave us concreteness; it pushed back on us to give up on some of the things that we thought were most elegant and focus on a different set of problems that turned into effective technology for them and also led us to new technical challenges.ﬂgovernment funding and the amplication of good ideasestrin credits government funding with allowing her to take on a wider range of problems than would be possible in an industry setting, with its necessary focus on nearterm business models. building, testing, and using working systems requires patience and committed funding. the interplay between invention and innovation also tends to amplify good continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.10ideas and make it clear what the nonuseful ideas are. shared and opensource tools, she said, represent one good idea with roots in governmentfunded research that was amplied through this push and pull. the internet protocol suite, web browsers, and tinyos are other examples of powerful opensource tools that industry would not or could not have invented without governmentsupported work.as was pointed out by several participants, a key difference between university research and industry research is the former is not constrained by specic business models and the nancial market™s pressure for annual revenue targets. ﬁit™s so important that this work also happen in the university, because the university can take on problems that extend beyond the incentive structure of any individual product, company, or industry,ﬂ said estrin. health care, she said, is a particularly clear example of a eld in which university research, especially in health care it, has propelled research that the private sector was not willing to do. with government funding, academic researchers have taken on, and can continue to take on, a broader range of problems in applicationengaged work that moves technology forward for health care and many other elds.motivators and outcomes for government, academia, and industry as someone with a long history working at the forefront of the technology industry, robert colwell offered a unique perspective on the processes and motivators behind technology research and development in government, academia, and industry. he spent most of his career engineering microprocessors at intel, where he was chief architect on the pentium pro, pentium ii, pentium iii, and pentium 4 microprocessors. after retiring from intel, he served as director of darpa™s microsystems technology ofce.although colwell has not himself been the recipient of federal grants for academic research since his graduate studies, he is a strong believer in the inherent value of such investments. in addition, colwell emphasized in his presentation the importance of military technology as a driver of technological advances in the commercial sector. as technology developed for military applications is adopted for public and commercial use, government investments in computer science and engineering pay double dividends. a brief history of innovation in technologycolwell presented a brief history of technological innovation, beginning with an example of one of the earliest known computers: in 1943, john mauchly and j. presper eckert built a machine for the u.s. military that quickly computed the math and physics relevant to the continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.11angles at which ballistic shells drop and explode. by the 1960s, computers were enormous but fast, becoming a part of mainstream research. at the same time, the military continued investing in improving computing capabilities to tackle more complex military problems. in the 1970s, the rst microprocessors were invented, allowing computers to get substantially smaller. the 1980s brought faster microprocessors and the invention of the personal computer. in the 1990s, computers continued to improve, and the internet and cell phones emerged. the 2000s brought smartphones and tablets, the rise of search engines, an explosion in social media, and hundreds of other innovations now prevalent in the daily life of billions of people. course corrections and unexpected turnscolwell stressed that one main lesson to be drawn from the history of technology in the modern era is that it is impossible to predict how governmentfunded technology will be adapted and used. a story from intel illustrates the unexpected turns innovation can take: when colwell began working on a computer chip in 1990, the internet wasn™t yet a pervasive aspect of personal computing, so the engineers did not factor it into the chip™s capabilities. by 1995 when the chip was nally ready, the internet had evolved, and it was partly a matter of luck that the chip did not need severe redesign to accommodate that new market. even at the frontlines of the technology industry, the internet came as a surprise. ﬁfundamentally, even for people in the industry designing the actual hardware, we didn™t know what was coming next,ﬂ he said. researchers have never been able to accurately predict what faster, stronger, better computers will enable. but, while one cannot predict the future, one can extrapolate from the past. and what the past tells us, according to colwell, is that the computing technology that has transformed our world would not have been possible without governmentfunded academic research. using a smartphone as an example, he listed numerous component technologies that stemmed from governmentfunded research, including the internet browser, the camera, gps, the embedded antenna, and the battery, among others (figure 1.1). ﬁwe™ve never been any good at predicting what better computers will enable . . . . we just have a faith that better technology is better technology, and smart people will gure out something really cool to do with it,ﬂ said colwell. the implications of motivationswhile industry, academia, and government all conduct research, they have very different motivations. based on his experience in a long industry career, colwell attested to the reality that forprot companies take a narrow, shortterm view of technology. whereas academic researchers might be able to step back and examine the larger picture, a company focused on earnings doesn™t always have that luxury. he said that industry is also continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.12looking to sell things on a large scale. the implication of this is that if a new technology is not immediately going to sell 10 million units, it may not be worth risking money to develop it. of course, no one knows what product or innovation will or will not sell millions of units at some point in the future. colwell shared that when he was at intel, they were blind to the potential of mobile computing. other examples of inventions we might not have without government research funding include the computer mouse, computer graphics programs, and the internet. government funding also supports ph.d. students to carry out all of this research and design work. colwell™s own doctoral work in the early 1980s was sponsored by the u.s. army.as a demonstration of one of industry™s inherent limitations, colwell recalled that in the rst years of the internet, aol, compuserve, and other early email and internet providers had carved out separate online spaces that worked ne on their own, but intercommunication was complex and cumbersome. today, in part because of government investment, we all benet from the convenience and speed of one giant, interconnected internet.turning his focus to the future, colwell identied some current problems that he believes only academic researchers will have the motivation and perspective to solve. these include working at both the exascale and the nanoscale, improved semiconductors, and energyefcient computing. semiconductors present an especially worrisome direct gov™tresearch impact on smartphones1gaasrf power ampsiriembedded antennaenergy efficient computing193nm photolithographymems accelerometer/gyro/barometerfinfetsgpsthe internetbetter batteriesphoto courtesy of ellen m. colwell, march 2015figure 1.1 impact of direct government research on smartphones. source: robert colwell, ﬁthe crucial role of government funding for it,ﬂ presentation to the workshop, march 5, 2015, http://sites.nationalacademies.org/cs/groups/cstbsite/documents/webpage/cstb160415.pdf.gaas rf power ampfinfetssirigpsbetter batteriesenergy efcient computingmems accelerometer/gyro/barometer193 nm photolithographyembedded antennathe internetcontinuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.13challenge: because transistors can only get so small, there will be an ultimate limit to how many circuits or transistors engineers can place on a computer chip. he warned that the time is coming when this inevitable limit will lead technological innovations to stagnate, with potentially major consequences for the u.s. economy and military. although technology companies will certainly benet from the research geared toward solving these problems, he urged that we cannot leave it to them, with their shortterm, protdriven view, to solve them.government funding for industry?although colwell stressed that academic research is a main driver of unexpected innovation, he also said the government stands to gain from directly funding industry research. although it companies invest large sums in research and development, they need assurances that they will see a return on that investment. granting government research funds directly to companies makes it more feasible for them to invest in highrisk, highpayoff innovations. industry is competitive, not cooperative, and government funding can encourage otherwise risky development that can lead to economic growth. another reason for government to fund industry research comes down to simple selfinterest, explained colwell. the success of the nation as a whole requires access to the best electronics. many branches of government, but the military especially, rely heavily on commercially produced electronics. counterfeit chips and cybersecurity concerns are very real threats. the car industry also relies heavily on industrially produced electronics. he noted that it is of great benet to the nation as a whole if those electronics are the best and the most secure that they can be. other important roles for the government in industry research and development, he added, include developing fair standards, creating cooperative task forces, and brokering disputes. it is virtually impossible to nd any sector of our economy today that does not rely heavily on computing innovations that have come as a result of governmentfunded research in both academia and industry. today™s health care, science, manufacturing, communications, and entertainment, to name just a few examples, are heavily computerdependent. government funding of research and development across the board increases the chances that our scientists can develop and exploit every technological opportunity and remain the world™s it leader. colwell concluded: ﬁwe don™t know what™s next, but we need to win.ﬂit is virtually impossible to nd any sector of our economy today that does not rely heavily on computing innovations that have come as a result of governmentfunded research in both academia and industry.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.14retaining the cutting edge with a distinguished career spanning academia, industry, and government, farnam jahanian, carnegie mellon university, has experienced the computer science discovery and innovation ecosystem from several distinct vantage points. he served as a computer science professor at the university of michigan, as a researcher at ibm™s t.j. watson research center, and as assistant director of nsf for computer and information science and engineering (cise). as leader of the cise directorate, he was responsible for a research budget of roughly $900 million. to frame his presentation about the role of governmentfunded research in technology innovation, jahanian shared two favorite quotes about innovation: šsteve jobs, apple ceo and renowned innovator˘˘ šsid caesar, comedianechoing a theme that pervaded the workshop, jahanian emphasized the tremendous importance of researchdriven it to america™s economy, security, and scientic leadership over the past 30 years. in his view, it is primarily it advances that have made the u.s. economy competitive and sustainable in a global market. in addition, it has undoubtedly accelerated the pace of scientic discovery in disciplines such as biology, chemistry, physics, and the social sciences, all of which have undergone remarkable transformations driven by computational and dataintensive approaches.today, jahanian said, it advances and interdisciplinary approaches are crucial to addressing society™s most pressing challenges, including health care, cybersecurity, transportation, and environmental sustainability. because it is now embedded in these elds, new advances or solutions must incorporate multidisciplinary approaches that involve computer scientists and technologists, as well as domain experts. ﬁour community is in the middle of all of these conversations, and many of these advances will depend on involvement of members of our community and computational and dataintensive approaches,ﬂ said jahanian. continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.15the need for research to match our aspirationsclearly, the united states has been a global leader in spurring the it advances we enjoy today. a 2013 report by international business consulting rm mckinsey & company lists 12 top ﬁdisruptive technologies,ﬂ or innovations that will transform the global economy and daily lives (box 1.1).1 according to jahanian, all of these technologies are rooted in basic research advances that scientists working in america have been responsible for inventing and advancing through innovations such as advanced robotics, the internet of things, and the mobile internet. furthermore, nearly all of those basic research advances have stemmed from government support. ﬁu.s. taxpayers have long been the most important investors in knowledge creation in this country,ﬂ jahanian said. but despite these past successes, he stressed that america™s work is far from over. america today faces relentless international competition to create or capitalize on the next disruptive technologies and to recruit the best talent from around the world. although u.s. scientists have been the recipients of the largest r&d budget for many years, other countries are beginning to understand how governmentfunded research leads to economic prosperity and are rapidly increasing their research spending. at china™s current rate of funding growth, for example, the chinese r&d budget is expected to surpass that of the united states by 2022.2 now that other countries are realizing just how critical this pipeline is, jahanian stressed the increasing need to align u.s. r&d funding to match its scientic and economic aspirations and national security requirements. 1j. manyika, m. chui, and j. bughin, 2013, disruptive technologies: advances that will transform life, business, and the global economy, mckinsey global institute, http://www.mckinsey.com/businessfunctions/businesstechnology/ourinsights/disruptivetechnologies. 2m. grueber and t. studt, 2013, 2014 global r&d funding forecast, battelle and r&d magazine, december, https://www.battelle.org/docs/tpp/2014globalrdfundingforecast.pdf.box 1.1 top 12 economically disruptive technologies (by 2025)mobile internetautomation of knowledge workthe internet of thingscloud technologyadvanced roboticsautonomous and nearly autonomous vehicles source: j. manyika, m. chui, and j. bughin, 2013,  mckinsey global institute, http://www.mckinsey.com/industries/hightech/ourinsights/thegreattransformer. nextgeneration genomicsenergy storage3d printingadvanced materialsadvanced oil and gas exploration and recoveryrenewable energycontinuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.16u.s. r&d expenditures, by source of funds: 1990œ2011sei 2014: sources of r&d funding, chapter 4. 0.0%0.5%1.0%1.5%2.0%2.5%0.0%2.0%4.0%6.0%8.0%10.0%12.0%14.0%federal r&d in the budget and the economyoutlays as share of total, 1962 2015r&d as a shareof the federalbudget (leftscale)r&d as a shareof gdp (rightscale)source: budget of the united states government, fy 2015. fy 2015 is the president's request. © 2014 aaasfigure 1.3 u.s. r&d in the budget and in the economy. source: intersociety working group, ˘ american association for the advancement of science, april 2014, http://www.aaas.org/page/aaasreportxxxixresearchanddevelopmentfy2015. figure 1.2 u.s. r&d expenditures by source of funds: 19902011. source: national science board, ﬁnational science board 2014 digest: science and engineering indicators,ﬂ national science foundation, february 2014, http://www.nsf.gov/statistics/seind14/content/digest/nsb1402.pdf. continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.17two more statistics drive home this point. right now, industry spends more money on research and development than does the u.s. government (figure 1.2).3 this is concerning, in jahanian™s view, because industry tends to focus on shortterm, applied research rather than on the longterm fundamental work that drives true innovation. even more concerning, there has been ˚at or no growth in the federal research and development budget as a share of the u.s. gdp (figure 1.3).4 as nearly every workshop presenter emphasized, the innovations that lead to new technologies, and thus to economic growth, come from unexpected places but have the common denominator of federal funding of basic research that is tied to meaningful problems. a growing funding gap thus threatens to undermine u.s. momentum in technological innovation. the unpredictability paradoxthe thriving u.s. research community drives the longterm discovery and innovation that is the foundation of our economic prosperity and domestic security. yet the paradox when funding research projects is that their outcomes are unpredictable. there is no single path or action that leads directly from invention to innovation, from product to prosperity. ﬁthe paradox of discovery and innovation is that no one actually knows how an idea or an innovation will impact the world,ﬂ said jahanian. sometimes an idea requires a long incubation period, during which it interacts with and reacts to other ideas and technologies before it blossoms into a groundbreaking new application. in fact, as many workshop presenters emphasized, it is most often the case that unanticipated research results are just as important or impactful as the anticipated ones. as a result, jahanian said, ﬁquantifying return on investment in the context of basic research often is a very very ambiguous proposition.ﬂthe united states can take great pride in its long history of research and development. in 1945, vannevar bush™s report on federal funding for scientic research laid a challenge the government quickly realized was worthwhile.5 in retrospect, there is a clear and direct path from cold warœera federal defense contracts to today™s silicon valley success stories. but when early federal defense contracts were awarded to develop arpanet, a crucial precursor to today™s internet, no one could have predicted that silicon valley and its businesses and innovations would be a downstream result. 3national science board, 2014, science and engineering indicators: 2014 digest, washington, d.c.4m. hourihan, 2015, federal r&d in the fy2015 budget: an introduction, american association for the advancement of science, washington, d.c.5v. bush, 1945, science, the endless frontier: a report to the president, u.s. government printing office, washington, d.c.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.18building bridges between academia and industrythe federal government is a major partner in america™s discovery and innovation ecosystem, both through direct funding of research and projects and through overarching investments in nurturing the ecosystem itself. in the entrepreneurial state, mariana mazzucato debunks the myth of a slowmoving government lagging behind frenzied innovators and reveals the opposite to be true.6 jahanian explained that recent national initiatives, such as the brain initiative, the national robotics initiative, and the materials genome initiative, demonstrate how targeted federal investments can help solve largescale, pressing national challenges that would be impossible for one company, university, or research organization to solve alone. of course, companies, universities, and research organizations are also crucial partners in the innovation ecosystem, he continued. it is where they intersect that research leads to the consumer products that drive our economy and encourage the government to reinvest in the research cycle. historically, university research labs are where knowledge creation and information dissemination begin. jahanian noted that in these labs, students, seed technologies, and scientic curiosity become the paths to startups, patents, and hardware and software prototypes that ultimately become the everyday technologies that are an integral part of our world. contrary to what some would assume, there is in fact a very healthy relationship between university research and industry products, and today startups and university labs are more connected than ever, he said. according to an annual study by the association of university technology managers, there were 4,200 actively operating university startups in 2013, double the number in 2000.7 this ecosystem can in part be traced to the bayhdole act, enacted in 1980, which permitted licensing agreements between university laboratories and companies, thus giving universities the ability to patent their inventions and retain the rights, creating additional incentives for them to partner with the private sector to further their innovations.jahanian emphasized that universities commercialize their technologies for many reasons, although the nancial incentive is often exaggerated. there is far more gain to be had in developing a product that becomes a public benet, contributes to the larger goals of a university™s mission, and enhances its reputation than in making a product that is merely protable. government funding frees universities from the trap of a narrowminded focus on return on investment, allowing them to pursue the risky and highly uncertain projects that are not feasible within the connes and nancial in˚uences of industry. 6m. mazzucato, 2013, the entrepreneurial state: debunking public vs. private sector myths, anthem press, united kingdom.7association of university technology managers, 2014, ﬁautm licensing activity survey: fy2013,ﬂ http://www.autm.net/resourcessurveys/researchreportsdatabases/licensingsurveys/.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.19this interplay among academics, government agencies, and industry has led to a gradual shift of mindset. transferring knowledge, whether in a general form or in the form of an actual new product, is no longer strictly about protecting intellectual property. today™s researchers see the long history of this backandforth and recognize that this relationship is about the pipeline from knowledge dissemination to economic development, to societal benets. in summary, jahanian reiterated his rm belief that to date, federal investments in basic research have returned exceptional dividends to our nation, while also providing a foundation for economic prosperity and national security. there is no reason to slow down or stop these amazing yet unpredictable results. the jobs of the future are in this discovery and innovation ecosystem, in the elds of engineering, computing, or information technology; he urged that the federal research funds of the future must be there, too.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.20building a connected worldin a relatively short time, the connecting of just a few computers in the early 1970s has become an internet that billions of people can tap into anywhere, anytime. over the past 10 years, this connectivity has exploded into an era when nearly any device can be internetenabled. in a framework known as the internet of everything, we are now connecting not only computers and people, but also our phones, wearable technology, and home devices such as lightbulbs and thermostats. as we look to the wireless future, government investment will be key to developing new technologies to redesign cellular networks, overcome limitations on bandwidth, and advance the sensor technology that will pave the way for networked sensors in the body, increased automation of cars, and other applications that cannot yet be predicted. the government™s investment in research, early implementation of networks, and collaborations with both academic institutions and industry were instrumental in bringing about what we know today as the internet. in this chapter, internet pioneers share the stories of innovation in three key areas: vint cerf re˚ects on the emergence of the internet; david culler describes the integration of the internet into the objects that surround us; and andrea goldsmith shares her perspective on the past and future of wireless technologies. evolving the internet vint cerf, vice president and chief internet evangelist at google, focused his presentation on the government™s central role in collaborations that led to the creation of the internet. continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.21known as one of the fathers of the internet, cerf has had a frontrow seat and played a leading role in the internet™s creation and has remained at the cutting edge of networking innovation throughout an illustrious career. the history of the internet is a clear demonstration of the crucial role of interplay among government, researchers, and industry in breaking new ground for it advances and applications. ﬁevery sector in our social and economic system has been engaged and continues to be engaged in the internet,ﬂ he said, adding that ﬁwe have managed to mutually reinforce the interest, capacities, and capabilities of many different parts of our social and economic system to keep the internet growing and going.ﬂ in the beginning: the story of arpanetcerf began with a history of the internet™s most direct predecessor, the arpanet, which connected academic institutions funded by arpa (now known as the defense advanced research projects agency, or darpa) to conduct articial intelligence and computer science research. a primary impetus for its development was to allow collaborating institutions to share computing capacity and codevelop software. an early collaboration between the lincoln laboratory at the massachusetts institute of technology and the systems development corporation led to a groundbreaking test around 1966 that demonstrated the potential for two separate computers to exchange blocks, or ﬁpacketsﬂ of information. about 2 years later, arpa sent out a request for quotation to build packet switches, called interface message processors (imps), for the arpanet. bolt, beranek, and newman (bbn)ša technology company in cambridge, massachusettsšwon the contract. as a result, bbn contributed a key industry component to arpanet™s evolution: bob kahn at bbn wrote host to imp specication 1822, describing how to implement an interface that lets host computers connect to the imp; this specication was subsequently made available to the academic participants in the arpanet project. in today™s internet terminology, the imp served as the arpanet™s ﬁrouter,ﬂ a system for orchestrating the exchange of packets of information between networked computers. nearly all of the foundational technologies underlying arpanet were developed not by one person or organization but by an interdependent, collaborative network of academic, industry, and government experts and experimenters. the development of the host protocol, for example, which allows networked computers to recognize one another™s identities and locations, was led primarily by steve crocker at the university of california, los angeles (ucla), with earlystage involvement from the university of utah and other institutions. in recounting this story, cerf pointed out that the development of the arpanet and subsequent internet was not purely a u.s. pursuit. several foreign visiting scientists at ucla contributed to the early phases of hosttohost protocol development. continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.22in addition, the development of telnet, an important remote access protocol, resulted from collaborations involving ucla, stanford research institute (sri), rand, bbn, and the massachusetts institute of technology (mit). development of the file transfer protocol was led by abhay bhushan at mit, and the development of networked email was led by bbn; both efforts involved numerous collaborators. even arpa was directly involved, with arpanet director larry roberts writing one of the rst teco macros to parse and display email message lesšan example of arpa not only funding research but also engaging in technology development itself. the rst public demonstration of arpanet came in october 1972, about 3 years after nodes were installed at ucla, sri, the university of california, santa barbara, and the university of utah. in 1975, arpa handed over arpanet operation to the defense communications agency, now called the defense information systems agency (disa). however, bbn continued to handle the key technical operation and ran the network operation center in cambridge, massachusetts. the internet takes shape from 1973 to 1974, the initial network design for the internet began as a collaboration between bob kahn, who was then at arpa, and cerf, then a professor at stanford university. together they designed the tcp protocol (later, the tcp/ip internet network protocols), a core set of protocols still used to communicate across the internet.1 multiple academic, industry, and government players interacted frequently to develop the internet™s foundation. cerf was appointed to the international packet network working group, spawned during the 1972 arpanet demonstration. the research and development company xerox parc, located close to his lab at stanford, sent its researchers to attend stanford seminars on internet design, contributing their experience with the parc universal packet and ethernet, two important communications technologies. in the mid1970s and early 1980s, cerf said, numerous academic and government researchers were using tcp/ip. with arpa™s encouragement, industry players, including some at ibm research, hp research, and the digital equipment corporation (dec) systems research center, implemented tcp/ip in a research context. despite having their own proprietary networking protocols, the companies™ research teams were interested in and excited about a nonproprietary 1v.g. cerf and r.e. kahn, 1974, a protocol for packet network intercommunication, ieee transactions on communications 22(5):637648.nearly all of the foundational technologies underlying arpanet were developed not by one person or organization but by an interdependent, collaborative network of academic, industry, and government experts and experimenters.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.23global network. because these companies implemented tcp/ip in their operating systems, it was possible by january 1983 to ask that all the computers on the arpanet and packet satellite and packet radio networks convert to tcp/ip, a step that would later play an important role in the commercialization of the internet.2 as the internet gained steam, the u.s. government helped to fuel its momentum with both formal and informal collaborations. seeing the value of quick information exchange and shared computing power, the department of energy (doe), nasa, the national science foundation (nsf), and darpa all implemented their own networks: the doe energy science network, the nasa science internet, nsf™s csnet, and later nsfnet, and at darpa, the packet satellite net, the packet radio net, and arpanet. these networks were aggregated to form the internet.ﬁgovernment representatives themselves also collaborated very directly with regard to program planning and nancing [of the internet],ﬂ explained cerf. ﬁthey formed something called the federal research internet coordinating committee, which was mostly represented by program managers from doe, nasa, nsf, and darpa. eventually that became formalized as the federal networking council, which had representatives from many other parts of the u.s. government in addition to the four initial funding agencies.ﬂ the internet reaches outin the early 1980s commercial organizations began to recognize the potential prots in providing equipment to support the internet. for example, 3com, a spinoff from xerox parc, made commercial ethernet devices and eventually software that ran tcp/ip. proteon was spun off from mit, cisco systems from stanford university, and bridge and sun microsystems from stanford and the university of california, berkeley. in an important shift toward the late 1980s, companies began to move past the physical equipment and began offering internet services. one step toward commercialization resulted from a collaboration between mci, which was participating in the nsfnet backbone, and bob kahn™s nonprot organization, corporation for national research initiatives the mci commercial mail service was connected to the nsfnet backbone (with permission from the federal networking council), even though commercial trafc was normally prohibited in the nsfnet appropriate use policy. around the same time, three commercial internet service providers emerged: uunet, psinet, and cerfnet. these were interconnected over a commercial internet exchange that mirrored the federal internet exchanges connecting networks at doe, nasa, nsf, and darpa.2reviewers of this report noted the important contribution of berkeley unix, an academic project that enhanced at&t unix in many ways, including by adding tcp/ip networking support. at&t allowed it to be distributed widely to academia, which spread the use of tcp/ip. tcp/ip adoption was also encouraged by darpa, through its funding of the sun workstation, which ran berkeley unix.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.24a hearing before tennessee senator al gore in september of 1986 marked an important milestone in the internet™s expansion. during this hearing, senator gore asked whether the supercomputer centers that nsf was funding should be interconnected with an optical ber network; subsequently, nsf commissioned the design and the implementation of a backbone network for its national research and education network program while also providing subsidies for the creation of intermediatelevel networks. in 1991, the passage of the high performance computing act essentially established a broad initiative to create a national information infrastructure. the next crucial milestone came in 1992, when the boucher bill formally permitted commercial trafc on the nsfnet backbone. this collaboration between congress, nsf, and the rest of the internet community was a key step in the internet development because it made it feasible to commercialize the service so that the general public could use it. re˚ecting on this history, cerf explained that many of the developments that led to the creation and expansion of the internet would never have happened without the strong linkages between governmentsponsored research programs at the universities and spinoffs that could commercialize the technology.developing the world wide webthe world wide web, a term coined by tim bernerslee, represented the next major step toward the internet we know today. the rst components needed to make the world wide web were developed by bernerslee at cern in 1991: the http protocols, a client and server mechanism, and browser and html specications. as the 1990s progressed, a number of browsers were developed, including erwise, violawww, midaswww, tkwww, cello, spyglass, internet explorer, and firefox. a dening moment came in 1993 when the nsffunded national center for supercomputer applications announced the rst widely used graphical browser, mosaic, the result of an effort pushed forward by mark andreessen and eric bina despite not being a specically sanctioned project. with the release of mosaic it became clear to users that the internet could be more than a unix command line and could include imagery, formatted text, color, andševentuallyšvideo and audio. the browser quickly gained popularity: ﬁmosaic represented a massive transformation for the way the internet was perceived,ﬂ said cerf. just a year later, jim clark and marc andreessen cofounded netscape communications, a company whose 1995 stock market launch would trigger the dotcom boom. as with previous internet developments, this milestone re˚ected a mix of academic and industry contributions, but this time, with the involvement of the stock market. the dotcom boom continued unabated until april of 2000. continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.25lessons learnedcerf emphasized that none of these important early internet developments would have been possible without the u.s. legislature funding the research agencies involved. ﬁthe lessons i take away from this in terms of the power of collaboration is that there was a very effective synergy realized between the government research agencies, which were persistent in their funding and willingness to take risk,ﬂ he said. ﬁthere was no guarantee that this program or this project would actually materialize successfully.ﬂ during the internet™s collaborative development, numerous institutions were created to ll important needs, a process that lent the internet its robustness and sustainability. ﬁwhen we run into an issue that requires attention that seems to require institutionalization, the internet community simply invents these things,ﬂ said cerf. all the collaborators played key roles in the development and spread of the internet. the academic community invented and explored networking technologies in a nonproprietary fashion. industry set about to commercialize the technology, making it widely available. the stock market was equally important because it brought about the rapid expansion of this capitalintensive business. finally, the u.s. legislature provided the funding and the very light regulation of the internet environment that enabled it to grow. the positive reinforcement cycles brought by these groups continue to this day. ﬁthe ball bounces between the legislative side, the academic side, and the industry side, and these cycles are all mutually reinforcing, and they continue to increase the availability of the internet everywhere,ﬂ said cerf. the internet of everythingas remarkable as the emergence of the internet was, its early development was constrained to the realm of full˚edged computers. developments since the early 2000s have cast aside these constraints and ushered in a new era in which practically anything can be connected to the internet, from phones and watches to thermostats and lightbulbs. this framework in which people, data, processes, and objects are all interconnected through the internet is known as ﬁthe internet of everythingﬂ (or, when referring primarily to devices, ﬁthe internet of thingsﬂ). david culler of the university of california, berkeley, discussed research that is helping make possible a world of ubiquitous internetenabled devices. before delving into the technological evolutions that have enabled the internet of everything, culler began with a review of key consumer products illustrating the expansion of the internet outside the (computer) box. in the late 1990s, the handheld palm continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.26pilot appeared on the market, wifi began allowing people to connect to the internet on their laptops, and qualcomm produced a small mobile phone. by the mid2000s the rst apple iphone brought the internet to the mobile person and nintendo™s wii video game console introduced sensing to computer games. the arrival of the ﬁconnected home,ﬂ in which home appliances and accessories can be controlled from the internet, was marked by the introduction of connected lightbulbs in late 2012 and the nest thermostat in 2013. this trend continued, with the 2014 international consumer electronics show revealing a shift away from tablets and smartphones to connected home devices and wearables. the evolution of enabling technologiesculler then described how the technology necessary to connect devices such as home security systems, lightbulbs, and game controllers to the internet stemmed from a mixture of academic, government, and industry research and development initiatives. in particular, darpa funding that started in 1978 and increased in the mid to late 1990s helped encourage the development of some key underlying networking technologies to enable interconnected devices. darpa™s sensor information technology (sensit) program, for example, was created to develop software for networks of distributed microsensors for military purposes; the program led to ad hoc deployable microsensors and distributed computing methods to accurately extract timely information for detecting, classifying, and tracking a target from a sensor eld.3 later, the darpa network embedded systems technology (nest) program arranged for culler™s group at the university of california, berkeley, to create the building blocks for networkembedded systems. the program, launched in 2001, led to a microplatform known as tinyos, which was scalable to extremely small devices in extremely large numbers and able to operate at extremely low power while remaining vigilant to potential stimuli. nest was designed so that all other contractors could use the platform and contribute to it, creating a rare nationwide open source hardware and software effort. at the end of the project, nest demonstrated thousands of nodes with contractors in various locations. although nest was focused on military applications, these sensor networks quickly found use in many other applications. for example, the nsffunded center for embedded network sensing at ucla focused on applying sensor networks for environmental monitoring, tracking changes in habitats, and for other scientic applications. in parallel with these developments, industry and academic researchers were pushing forward on numerous other technologies that would ultimately converge to enable a plethora of internetconnected devices. the development of linux, a free operating 3s. kumar and d. shepherd, 2001, sensit: sensor information technology for the warfighter, in proceedings of the 4th international conference on information fusion, fusion 2001, http://bit.csc.lsu.edu/~iyengar/images/contributions/tuc11.pdf.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.27system that could run on any computer, was an important development in the 1990s. the 1990s also brought the development of smaller microprocessors and sensors and the proposal that a sensing and communication unit that at the time was about the size of a silver dollar could be made on a millimeter scalešapproaching the size of dust. culler pointed to several academic projects from the late 1990s and early 2000s that further pushed the envelope. the endeavour project at the university of california, berkeley, for example, focused on making it more convenient for people to interact with information, devices, and other people.4 although open source software seeded this community, it suffered from a lack of hardware. in 2000, to address this deciency, intel formed a network of universitybased ﬁlablets,ﬂ some of which worked on how to get a tremendous amount of computing into a constrained space. in 2003, the emergence of ieee 802.15.4 standard for lowrate wireless personal area networks was another milestone. this standard gave fundamental lower network layers a wireless personal area network that offered lowcost, lowspeed, and lowpower ubiquitous communication between devices. several years later this standard and the new internet protocol version 6 (ipv6) came together into a new routing standard. around 2005, nsf added momentum with its networking technology and systems (nets) program, a grant program soliciting proposals in four research areas: programmable wireless networks, networking of sensors, broadly dened networking, and future internet design.5 these developments have paved the way for a burst of new devices and applications.a tipping pointby 2008, technology had converged to a point where many of the challenges of connecting small, diverse devices to the internet had been addressed. similarly, the development of ﬁidle listeningﬂ solved the power consumption problem by allowing devices to monitor inputs only when they sense there is something to detect. idle listening allowed the development of the ieee 802.15.4e wireless standard, which is incorporated into eventdriven devices and also underlies the energyefcient ethernet. these developments were complemented by innovations in information routing and volume management such as local rerouting and the trickle algorithm, allowing networks and devices to continually adjust to changes in available networks and the density of users to avoid ˚ooding the network. 4university of california, berkeley, electrical engineering and computer science department, 2014, ﬁthe endeavour expedition: charting the fluid information utility,ﬂ last modified july 22, http://endeavour.cs.berkeley.edu.5national science foundation, program solicitation, nsf 06516 to replace nsf 05505, in the matter of: furtherance of the president™s management agenda in fy 2016 from the directorate for computer and information science and engineering, division of computer & network systems, march 6, 2006, http://www.nsf.gov/pubs/2006/nsf06516/nsf06516.htm.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.28the combination of these advances led to a tipping point in smalldevice capabilities, and subsequent years have seen an explosion in the number and diversity of internetenabled devices for personal, home, business, and military use. looking forward, culler said, technology is entering a point where ensembles can be connected. ﬁit™s not my smart device, it™s what happens when my smart device and a handful of them walk in with me to my home with its family of smart things that are also connected and are connected to various kinds of societal infrastructure, whether that be electric utilities or transportation,ﬂ he said. future developments would be focused on discovery, integration, physical mashups, and metadata, culler concluded, although along with these developments would come new challenges in another key area: privacy.the wireless futurethe advent of wireless technologies has been crucial to our transition toward the internet of everything, and these technologies will undoubtedly grow more crucial as the trend continues. a presentation by andrea goldsmith of stanford university examined technical challenges facing wireless networks and the key role of governmentfunded research in advancing solutions. re˚ecting on her 30year career in wireless communication, goldsmith said today is the most exciting time for this technology. in her view, a big difference between the wireless past and the wireless future lies in whošor whatšis exchanging information. whereas in the past most information exchange was initiated or mediated by people, in the future devices themselves will likely be driving much of the communication: ﬁwe™re going from a world where we used to have people using wireless to communicate with each other and access information, and now we™re moving into a world of devicetodevice communication,ﬂ said goldsmith. ﬁthat™s going to require a complete rethinking of how we build wireless systems.ﬂdevicetodevice communication will not only lead to new systems that we can imagine from today™s vantage point, such as the nextgeneration cellular phone or wifi, but will also enable sensors in everything, even inside the body, she said. one potential application of these sensor networks is to develop smart homes and buildings that could, for example, lead to greater energy efciency or detect when an elderly person suffers a fall and call for help. goldsmith noted that health is another area where wireless technology is poised to make a huge impact. cell phones are already changing the way medicine is done; for example, the technology already exists for someone in africa to use a cell phone to take a photo of a blood sample and send it to a remote location for malaria detection. goldsmith went on to explain that inbody sensors and networks also hold continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.29tremendous potential, though these will require completely new ways of communication, perhaps using chemicals or sensors on neurons to power devices. for example, sensors around an articial heart might detect a problem and send a wireless signal to a device that could initiate a lifesaving intervention. neuroscience offers other exciting opportunities: the whole brain initiative, for example, is starting to decipher how neurons in the brain are connected and what the signals do. already it is possible to inject a signal into a particular part of the brain and reduce some of the symptoms of parkinson™s disease; a better understanding of signal encoding and decoding in the brain might allow scientists to build a tiny transmitter and receiver to compensate for damage or disease. despite the allure of nextgeneration wireless technologies and the internet of things, however, goldsmith described signicant challenges on the horizon and the need for innovative solutions to enable the wireless future. confronting our bandwidth shortage one big challenge facing wireless technology is the inherent limits of the radio frequency spectrumšthe medium through which all wireless signals are transmitted, along with signals from television, radio, gps, and other data. the federal communications commission (fcc) grants companies licenses to use slivers of this limited physical spectrum. based on trends in the use of smartphones, a 2010 fcc report projected an almost 275 megahertz cellular spectrum decit by 2014ša spectrum decit that exceeded even the amount of spectrum being used at the time (225 megahertz).6 the years 20102014 indeed saw exponential growth in demand for wireless data, primarily driven by video, and this growth exceeded the spectrum available in the cellular bands. however, these same years saw a growth in the availability of wifi networks, so users did not actually experience the full brunt of the cellular spectrum crunch. while we may have weathered that storm, goldsmith said current trends toward the internet of things point to a more concerning bandwidth shortage on the horizonšone that affects both the radio spectrum generally and wifi specically. forecasts indicate we will have on the order of 50 billion devices by 2020. since wireless demands already exceed the spectrum available in the license band, wifi is making up for the current shortfall, but this cannot continue indenitely. wifi also is interference limited, so when 20 billion devices are using the same unlicensed spectrum, wifi will face a major crunch as well. while acknowledging a signicant amount of hype building around the internet of things, goldsmith explained that there is enough evidence of the trend™s emergence and impact for it to be taken seriously when projecting future wireless demands. in the trans6b. reed, 2010, fcc projects 275 mhz ‚spectrum deficit™ by 2014, network world, october 21, http://www.networkworld.com/article/2192490/wireless/fccprojects275mhzspectrumdeficitby2014.html.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.30portation sector, there has been growth of automated highways, and semiautomated cars are already among us; the newest tesla car, for example, can change lanes without driver input.7 similar trends are happening in the health care sector, where people are already using wearable sensors to track heartbeat, physical activity, and other variables. ﬁif you just look at these two sectors as already emerging as economically viable, i think there™s no question that the internet of things is going to be very real,ﬂ goldsmith said. for this reason, she said the prediction of 50 billion connected devices is not unreasonable, and even if it turns out to be only 10 billion or 20 billion, that is still much more than today™s wireless communication infrastructure can handle.goldsmith said there is still an open question whether the decit in bandwidth is a result of poorly designed systems, or because the systems have reached their physical capacity (also referred to as the shannon limit of the physical layer, or the maximum rate at which data can be sent over a particular bandwidth with zero error).8 pointing out that the shannon capacity of wireless channels is unknown and even less is understood about the shannon limit of ad hoc and sensor networks, she said more research is needed in this very theoretical eld to understand whether better network design could help to solve the bandwidth shortfall.the need to rethink network designa second problem lies in the design of cellular networks. even though cellular technology is in its fourth generation, goldsmith explained, the underlying design principles of today™s cellular systems are identical to those of rstgeneration analog systems: it is still assumed, for example, that the system is interferencelimited. however, multiple technological advances have emerged to address the problem of interference, including using multiple antennas, or mimo, and multiuser detection, which was invented in the 1980s but only recently became implementable thanks to increases in computer processing power. despite being no longer interferencelimited, the overall design of the networking system has not changed to take advantage of these developments. in addition, there is a growing need for cellular networks to become more energy efcient: one unknown, for example, is the minimum amount of energy necessary for a network to operate when power is limited, such as during an event affecting the power grid.as a result of these trends and needs, it is time for a complete rethinking of cellular design, said goldsmith, adding that this effort needs to be driven by the research world. only after researchers show that a new design can net an orderofmagnitude improve7in october 2015, tesla motors announced that its new software release would incorporate additional selfdriving technology (tesla motors, ﬁyour autopilot has arrived,ﬂ october 14, https://www.teslamotors.com/blog).8c.e. shannon, 1948, a mathematical theory of communication, the bell system technical journal 27:379423, 623656.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.31ment in a cellular system will it be adopted by industry. because of industry™s focus on shortterm revenue, companies cannot afford to spend the time and money on research and development to completely rethink cellular system design. goldsmith said that this effort will in part involve determining the most important aspects of the cellular network: is capacity the primary concern? or power consumption? for example, if someone is trying to connect a device powered from an energyharvesting battery, speed may not matter as much as connecting to the cellular network with minimum energy. coverage is another issue: can we build a cellular system that gets coverage everywhere, including indoors? goldsmith pointed to millimeter wave mimo technology as a possible solution. there is a great deal of unregulated open spectrum at 60 gigahertz or higher. however, operating at these frequencies comes with challenges. antenna arrays containing hundreds of elements can compensate for the high attenuation, but this approach will require a new design approach. ﬁin my view, and we™re doing some research on this, we really need a complete rethinking of system design to take advantage of these technologies,ﬂ she said. toward a seamless network experienceanother challenge is how to use bluetooth, wifi, cellular, and even other networks in a seamless way. ﬁwhat i really want is a big wireless cloud,ﬂ goldsmith said. ﬁi don™t care what network i™m on, i don™t need the icon on my phone telling me what wireless network i™m on, i just want it to work for whatever application i™m using.ﬂ in wired networks, a big wave of research has focused on softwaredened networking, an approach that might be usable for wireless networks too. however, wireless networks are fragmented, so switching from cellular to wifi typically requires closing a session on one network and opening another. goldsmith envisions a potential softwaredened networking design for wireless devices that uses a unied control plane to match the wireless network to the application being used. for example, a lowdatarate, lowenergy application might use millimeter wave, bluetooth, or lower power wifi instead of cellular. energy is the driving constraint for sensor networks, explained goldsmith. some sensors harvest energy from the environment while others are powered by batteries. some batterypowered sensors, such as those embedded in a structure like a bridge, must last decades without recharging. to build communication systems that use extremely low amounts of energy, goldsmith said, we need to start from scratch. modulation, coding, and multiple antenna techniques are all power hungry, not only in terms of transmitting energy but also in the processing power. for shortrange networks, it is important to examine how much energy the circuitry consumes. zigbee and bluetooth continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.32may be a lot better than wifi or cellular, but it is not known if they are anywhere close to the minimum energy consumption possible, said goldsmith, pointing to the need for more research in this area. the path from research to innovation to conclude, goldsmith discussed how theoretical research translates into practice and how practice can also circle back and inform research. as a case in point, goldsmith shared the story of her rst startup, quantenna, which she launched after about 20 years as a researcher. at the heart of this effort was her desire to build somethingša desire she traced back to her rst job building an antenna array in the mid1980s, an experience she said made her fall in love with wireless communication and inspired her research career. quantenna makes wifi chips with the goal of achieving the best performance on the market, based on goldsmith™s research in communications theory, and the company recently announced a 10 gigabytes per second wifi system that uses the most sophisticated physical layer in existence. goldsmith cites this achievement as an example of applying deep theoretical research to build better systems. at the startup, she said she learned that many aspects of wireless systems are poorly understood and that actually building a system revealed many questions that later fed back into her research and teaching. she concluded her talk by pointing out that much research is still needed to realize a wireless vision, but that doing this work will allow wireless technology to change people™s lives worldwide. she also said that although she thinks that research has a profound impact on technology development and vice versa, a stronger connection or feedback loop from industry to universities would offer more synergy and allow researchers in universities to solve even more important problemsšand government has an important role in making this happen. ﬁgovernment and governmentfunded research were key for the development of wireless technologies. these technologies are central to the growth and success of mobile devices, but there is still more that needs to be done to get us where we want to go,ﬂ she said. continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.33advancing the hardware foundation while it is software that comes to mind most readily when we think about using computersšthe interfaces that we interact with, the applications we use to perform tasksšhardware is the foundation that makes all computing and communications technology possible. behind instagram is a camera; behind your favorite mapping app is a global positioning system (gps) satellite; powering your laptop is a lithiumion battery; central to your smartphone is its liquid crystal display (lcd) touchscreen display. as this handful of examples make clear, there are a tremendous number of component technologies making up the devices we depend on every day. in a vast majority of cases these technologies are rooted in pioneering research conducted or funded by the federal government, which later fed into other research and commercial endeavors. current trends suggest a future populated with ever more computing and communications technologies, from selfdriving cars to gadgets that are worn on or even embedded inside the body. as we move toward this future we will continue to depend on fundamental research to overcome the limitations of current technologies and usher forth new hardware and computer architectures. this chapter summarizes presentations by margaret martonosi, on the evolution of computer architectures that balance capabilities and speed against the limitations of energy and heat, and thad starner, on the history and development of wearable computers. continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.34developing disruptive architectures as our computers and communication tools evolve, each new device must strike the right balance between capabilities, speed, power, and thermal constraints in order to achieve a higher level of functionality and speed without overheating or draining power too quickly. exchange between researchers and industry has been crucial to the development of new, disruptive architectures that can overcome previous constraints and enable radically new products. margaret martonosi, a professor of computer science at princeton university, is known for her work on computer architecture. coined in a 1964 ibm paper,1 the term computer architecture refers to the eld of computer design concerned with balancing competing factors such as computing performance, power needs, cost, and reliability. in particular, martonosi™s research has focused on power efciency. martonosi described computer architecture as a mediator between computer technologyšthe technical challenges of building computersšand computing applicationsšwhat you can do with those computers once they are built. today™s technology landscape brings challenges and opportunities in both realms. ﬁsince it™s a very dynamic time for both the application side and the technology side, that makes it a particularly interesting time to be a computer architect,ﬂ said martonosi. hitting inevitable limitscurrent computing applications are dramatically widening the scope of what computers can do. today™s computers work with a lot of data highly distributed across a diversity of devices and are much more communicationintensive than computers in the past. at the same time, new applications and functionalities are demanding more performance for computations, storage, and communication. for many years, improvements in architecture and technology enabled computers to get smaller and faster without increasing power usage. unfortunately, although engineers are still making everything smaller, they are having difculty increasing speed or reducing power use: computers are hitting the inevitable limits of speed and power constraints. the transition toward the internet of everything raises the stakes on overcoming these challenges. for example, biomedical researchers are exploring multiple promising opportunities to improve people™s health and save lives by embedding sophisticated, computerenabled medical devices inside the body. but for these devices to become widely applicable, computers are needed that do not quickly burn through their batteries 1g.m. amdahl, g.a. blaauw, and f.p. brooks, jr., 1964, architecture of the ibm system/360, ibm journal of research and development 87101.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.35or throw off heat that could harm the patients they are intended to help. current technologies are not sufcient to realize all of the important computing applications envisioned for the future. a history of innovation from the niche to the mainstreammartonosi described the evolution of research and technologies that are re˚ected in today™s computer architectures. much of this work, for example, has been informed by darpafunded supercomputing research conducted in the 1970s through the 1990s. although those investments were targeted at niche areas of computer science, experience has proven that sustained governmentfunded research ultimately trickles into the mainstream; the results of darpa™s supercomputer research investments are central to the architecture of the computers and smartphones we use today. martonosi noted, ﬁit™s a real success story of research that was done, viewed as niche, that later we had to pull it out of our pockets and use it in a much more mainstream way.ﬂenergy use, martonosi™s focal area, is not a new area of computer architecture; in fact, power has been a consideration since the very rst computers. even the developers of eniac, the rst electronic computer, developed by mauchly and eckert in the mid1940s, paid careful attention to its power use, which was about 150175 kilowatts.2 ever since those early days, when a computer architecture design hit its power limit, there was a new technology to switch to: rst there were relays, then vacuum tubes, then bipolar transistors, and then metaloxide semiconductors. today, martonosi said, computer architects are once again facing power limits, but the difference this time is that there is no ready new technology to switch to that would enable computers to increase their productivity without hitting thermal constraints.computer architecture research in the 1990s led to several important developments making computers more energy efcient. one is dynamic voltage scaling. stemming from infopad, a darpafunded project at the university of california, berkeley,3 this innovation enabled a computer to reduce its power supply voltage in order to optimize power use. dynamic voltage scaling is now standard on every phone and computer. other technical solutions to power efciency, also darpafunded, include narrow bitwidth optimizations and speculation control. these solutions, developed by basic computer science researchers, were also quickly incorporated into product design.power modeling is another innovative idea with roots in basic computer architecture research that took place in the 1990s. for the rst time, power models allowed architects to evaluate new ideas for optimizing power usage much earlier in the design 2g. farrington, 1996, eniac: birth of the information age, popular science (march):7476.3university of california, berkeley, ﬁinfopad: wireless multimedia computing,ﬂ http://www.wireless communication.nl/reference/chaptr01/dtmmsyst/infopad/infopad.htm, accessed november 18, 2015.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.36process. as a result, computer architects were able to in˚uence design, and power models became critical to understanding the powerœcapability tradeoffs. martonosi™s early efforts in power modeling were adopted by the computer industry, specically ibm, becoming part of mainstream computer design. by the early 2000s, although researchers had been steadily making improvements, they still needed to do more to address power constraints. a big breakthrough came in 2005, with the invention of onchip parallelism: the addition of more processors to a silicon chip in order to allow a computer to carry out multiple calculations simultaneously. this development enabled more computation with less power usage. at this point, parallelism research and power research, once separate computer science areas, converged, and the joint research led to the invention of computer chips that contain many specialized, heterogeneous processors to carry out the hundreds of computations made by today™s devices.4 martonosi explained that the progress from one technological advancement to the next has not always been linear. as onchip parallelism was integrated into more products, starting in the mid2000s, computer architects reached back into decades of darpa and nsffunded parallelism research to improve capabilities in this area. several key projects, such as those focused on sharedmemory cache coherence, scalable protocols, and hydra chip multiprocessors, led directly to technologies now widely used in today™s computer servers, network processors, and smartphones. martonosi said that parallelism, like the early research on supercomputer architectures, is another area in which early government funding of a niche research area has led directly to technologies that are now in wide use. a disruptive momentcomputer architecture straddles hardware, which allows software to operate, and software, which lets us use computers to perform tasks. today™s seismic changes in both hardware and software are creating signicant changes in computer architecture as well, said martonosi: ﬁi see this as an interesting, exciting, and disruptive moment in computer architecture.ﬂ devices today are full of computer chips that are highperforming and powerefcient but incredibly complex to program, including processors for audio, video, face recognition, and dozens of other capabilities. pointing to the diagram of a modern chip, martonosi noted, ﬁthe manual for this chip is 5,000 pages long.ﬂmartonosi raised the concern that the commercial computing industry is unlikely to address fundamental challenges facing computer architecture, for several reasons. first, as other presenters have said, most companies™ goals are too short term and modest to 4j. li and j.f. martinez, 2005, powerperformance implications of threadlevel parallelism on chip multiprocessors, pp. 124134 in ieee international symposium on performance analysis of systems and software, 2005, ispass 2005, doi:10.1109/ispass.2005.1430567. continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.37pursue solutions to highlevel, crosscutting problems. also, hardware and software are in most cases developed by different companies, so few companies have a nancial motivation to take on the burden of research and development in the noman™s land between them. in addition, individual pieces of software, also designed by different companies, may not work well together when combined on one device (a bit like the tower of babel, according to martonosi). finally, companies are also unlikely to collaborate with their competition and may not even feel enough market pressure to tackle the problem. martonosi said these issues are likely to lead to an increase in software development costs and a decrease in software reliability and security. the u.s. military systems™ reliance on computers, which comprise many different software and hardware parts from many different vendors, illustrates the importance of taking up this challenge at a broad level. there are multiple areas ripe for research, martonosi added. for example, there is a need for solutions that can help manage chip heterogeneity and establish a better balance between communication needs, which dominate device use today, and computational needs. basic research advances in these areas would support computer architects™ important role as mediators between power usage and computation and hopefully usher in a new generation of computers that can be both faster and more energy efcient.the winding path of wearables computers that are small and lightweight enough to be worn on or in the human bodyšsuch as the apple watch, fitbit, google glass, and othersšhold tremendous potential for a variety of uses. but the technology behind today™s wearable computers has followed a circuitous and at times surprising path through governmentfunded academic research, the experiments of hobbyists and tinkerers, and commercialization by multinational technology companies. today™s wearables are made possible by myriad component technologies, such as speech recognition, lithiumpowered batteries, cloud computing, and innovative architectures that allow computers to be lightweight, low power, and seamlessly integrated into people™s daily lives. thad starner, a professor at the georgia institute of technology™s college of computing and a pioneer in the eld of wearable computers, delivered a presentation on the history of wearables while himself sporting google glass, a breakthrough wearable product he helped to develop. why wearables? while wearable computers perform many of the same functions as a smartphone or traditional computer, they offer a number of additional advantages. one is that performing a quick task using a wearable computer requires signicantly less movement and attention than accessing a laptop or phone that might be across the table or in another room. continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.38perhaps most relevant to today™s busy, multitasking society is that wearing a computer shortens the length of time between intention and action: as soon as you realize a task needs to be done, you can complete it within seconds. because wearables are closer to the body, they can also access health information such as temperature or pulse, making them useful for monitoring tness activities or providing medical alerts. starner asserted that more advances are expected in the medical elds. pacemakers, wearable glucose monitors, wearable insulin pumps, and other such devices, for example, could read signals from a patient™s body and personalize treatment accordingly. rehabilitation specialists are looking into wearable robotics, a truly crossdisciplinary eld, to help patients recover muscle strength or limb movement after an injury. of course, starner noted, giving computers such access to our bodies and our health information means that privacy and data protection are crucial whenever wearable computing is discussed or designed. starner also pointed out that fashion is likely to be a signicant driver in the development of wearables. to some extent, wearables may get the most traction from rst being fashionable, then becoming more functional as they spread and evolve. the most exciting aspect of wearable computing, in starner™s view, is that the industry is really just getting started: ﬁthe interaction between man and machine, between the computer and the user, is just getting interesting. i think we™ll see more advances in the next 10 years than in all the previous years combined.ﬂ from fiction to factwhile a number of wearable computing technologies are gaining steam on the commercial market, the history of research and innovation leading up to this point has been a somewhat bumpy road. ﬁin the press, people are suddenly discovering wearable computing. . . and the question i get often is, ‚why now?™ there are actually some very good reasons why we could not do this before,ﬂ starner said.people have been envisioning wearable computers at least as long as computers have been around. a 1945 issue of life magazine featured an article titled ﬁas we may think,ﬂ in which computer pioneer vannevar bush imagined a futuristic device he postulated would someday assist scientists in their work: a foreheadmounted camera used to record experiments onthego. at the time, computers were the size of an entire room and photographic technology was still far from its current digital form, but that didn™t stop visionaries like bush from assumingšcorrectly, in this instancešthat wearable computing would eventually become a reality. early work on articial intelligence and virtual reality (such as ﬁaugmenting human intellect: a conceptual frameworkﬂ by douglas engelbart in 1962 and ivan sutherland™s sword of damocles headmounted display in 1968) was also built on the assumption continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.39that as computers advanced and connected the world, people would be wearing them and they would truly be a part of us. however, although computer interfaces did get smaller and more advanced, largely thanks to darpafunded research, they rst evolved into the personal computer, away from the body as separate machines. starner recounted how he has long tinkered with wearable computing. as a student at mit in the late 1990s, he created a wearable computer for taking notes more effectively that included goggles, a keyboard that could be held in one hand, and a 1pound hard drive stowed in a backpack. starner™s homegrown device presaged his eventual involvement in developing google glass, a cuttingedge commercial product designed to fulll a similar need, albeit in a format that is more appealing to the general public than starner™s original bulky apparatus. the early 2000s saw a signicant push toward mobile wearable computing as smartphones took off and displays grew smaller. a key development in the path to google glass was the ability to embed the display inside a glass lens, removing the need for clunky goggles. the resulting technology (see figure 3.1) allows regular peoplešnot only computer scientists or technology hobbyistsšto use this intuitive and seamlessly integrated, wearable technology. many of the component technologies that enable today™s wearables can be traced to research and development advanced or funded by the federal government. a microdisplay pioneered by hubert upton at bell labs in the 1960s was later further developed by darpa for use in military helicopters; the u.s. army also incorporated the microdisplays into wearable computers to increase efciency and reduce the number of personnel needed for inspecting tanks. nasa, driven by a need for better cameras for its missions, advanced key technologies that were later incorporated into webcams, smartphone cameras, and google glass. sensing, gps, and speech recognition are other key ingredients of wearable technology that can be traced back to governmentfunded research. figure 3.1 google glass. source: martin missfeldt, ﬁhow google glass works,ﬂ infographic, february 2013, http://www.brillensehhilfen.de/en/googleglass, licensed under creativecommonslizenz ccby. continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.40although many of the early drivers for wearable technologies were rooted in government applications, wearables have clearly had commercial appeal as well. in 1995, nintendo stuck two microdisplays together and sold 1 million units of its virtual boy, the rst virtual reality consumer product. finding no existing standard for personalarea networks and unable to deploy wearables for its employees without such a standard, fedex convened experts to create the one used today, ieee 802.15.6. while wearables might seem, to the casual observer, as if they arrived overnight, starner stressed that today™s consumer wearables could be traced down a long path from the hobbyist researcher and the federally funded lab to military applications to consumer products. as these devices continue to improve and become an ever more integral part of our lives, he concluded, we owe a great debt to the governmentfunded research pioneers, from all areas of computer science, who created the technologies that enable presentšand futurešwearables. continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.41developing smart machines machines, ancient and modern, are tools to serve our needs. for eons they have carried out a huge variety of tasks, from manufacturing goods, to transporting people around, to helping us decipher the natural world, to simply entertaining us. machines can ght, protect, heal, and even teach us. but what they have not been able to do until quite recently is to learn, make decisions, and act on their own. today, intelligent machines are everywhere. from the net˚ix recommendation engine to google translate to apple™s siri voicerecognition system, articial intelligence has become sufciently accurate, reliable, and useful to nd its way into numerous devices and applications. these technologies have taken off in parallel with a dramatic expansion of the amount and complexity of data, which provides fertile teaching ground from which machines can learn to make intelligent decisions on their own. in the related area of robotics, engineers have made remarkable achievements by combining sophisticated software and articial intelligence with equally sophisticated hardware to create machines that perform useful tasks in diverse realworld contexts. these robots now provide a variety of valuable services and perform activities that it would be impossible or dangerous for humans to attempt. this chapter presents an introduction to key concepts in machine learning by jaime carbonell of carnegie mellon university; a history of articial intelligence achievements by eric horvitz of microsoft; and an exploration of robotics by rodney brooks of rethink robotics. continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.42making machines learn most tasks performed by computers today are the result of traditional programming: systems are developed to perform specic functions in response to specic inputs in order to fulll a predetermined set of requirements. but in an increasing number of scenarios, we need computers not only to do the things they are programmed to do, but also to be able to take inputs and tell us something we didn™t already know or perform a task we didn™t specically tell them to došin short, to acquire the skill of learning. machine learning is a eld that combines articial intelligence, which is the ability of machines to make intelligent decisions, with data analysis, which allows machines to gain knowledge. there is a great deal of crossover between machine learning and articial intelligence, and some see machine learning as a subeld within the broader scope of articial intelligence. essentially, machine learning is what lets computers discover patterns within data and then use those patterns to make useful, and ideally correct, predictions. those predictions can then be used to make decisions or take actions that are appropriate for a given situation, the same way a human would. jaime carbonell, a professor of computer science at carnegie mellon university and an expert in articial intelligence and machine learning, presented an overview of the key challenges and approaches involved in machine learning. how to school a computermachine learning has virtually unlimited economic and consumer applications for elds as varied as medicine, robotics, nance, entertainment, and transportation. ﬁmachine learning essentially is the engine that is driving modern articial intelligence,ﬂ said carbonell. ﬁand the big impact is everywhere.ﬂ while a traditionally programmed selfdriving car might be able to nd its way around a city, it takes machine learning in order for a car™s driving system to notice another driver™s behavior, predict that he or she is about to cut in front of it, and slow down to allow that event to happen safely. by harvesting information from the environment, machines can adapt to our dynamic world to make smarter decisions. carbonell described the complex, multistage process of teaching a machine to learn. central to machine learning is the process of feeding training data into a mathematical prediction model in order to test and rene the model to the point that the machine can use it to acquire and apply future knowledge. a key goal of this continuous learning process, carbonell explained, is to minimize errors by continually assessing the difference between actual outcomes and predictions made by the machinelearning system. minimal error is crucial to many machine learning and articial intelligence applicationsšfor example, when models are used to guide health care decisions or military activities or to design a system for manufacturing airplanes. continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.43to further rene the learning algorithms, engineers must train the machine to appropriately handle and learn from outliers, rather than just typical data. in traditional statistics and engineering applications, researchers seek the most accurate re˚ection of a data set as whole and try to weed out or deemphasize rare or extreme cases that do not re˚ect the norm. in machine learning, on the other hand, researchers cannot ignore rare cases; in fact, it is these outliers that give the machine some of its most important learning opportunities. for example, machine learning often deals with unbalanced data sets in which the ultimate focus of decision making is precisely the outlier cases. in medicine, very few patients will actually have the rare disease researchers are interested in. in airplane safety, very few ˚ights will result in accidents, yet these present the greatest learning opportunities therefore, in machine learning, such instances are not mere statistical noise, but central lessons for the system to learn from: if you ignore the outliers, carbonell said, ﬁyou could miss everything that is interesting.ﬂ broadly speaking, machine learning engineers select mathematical models, analyze historical data sets to generate and rene their models, and then apply the models to make predictions about new data. through this process computers can be developed that use mathematics the way humans use mental models when they encounter a new situation, recognize a pattern, and adapt their behavior to it. in computers, this is known as transfer learning. along with related theories such as deep neural networks and proactive learning, transfer learning is seen as an important driver for future machine learning advances. tapping big datarecent years have seen a surge of progress in machine learning thanks in large part to the rapid growth of big data, the enormous data sets now being generated by thousands of informationsensing devices in both the scientic world and the everyday world. big data is now integral to every branch of science: ﬁnot everything in the disciplines is big data or data sciences, but data sciences has a part of every single one,ﬂ said carbonell. ﬁdata science, which you can loosely dene as big data plus machine learning plus domain knowledge, is the big win in this areaštheir combination is the big win.ﬂ because big data sets are large scale, highly complex, and multidimensional, they are extremely difcult to work with. many layers of computing technologies, such as cloud storage, privacy and security controls, data merging and cleaning algorithms, and other tools and methods are required in order for a big data set to reach a state where analysis can occur. only once big data is in this state can it be incorporated into machine learning. carbonell explained that big data has propelled numerous recent advances in machine learning; on the ˚ip side, it is precisely because we are in an era of big data that we need continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.44machine learning systems more than ever. machine learning has become crucial to the ability to sift through, analyze, and understand today™s highly complex data. creating the multilingual computermuch of carbonell™s work has focused on imbuing machines with the ability to process natural human language. re˚ecting on the history and current state of this eld, carbonell noted that rather than being a linear progression of ideas and methods, the dominant theories in machinebased language translation systems today are the product of a collective development of numerous different theories that have evolved and converged over the years. in particular, carbonell identied two key moments in the development of machine translation. the rst took place in the mid2000s, when rulebased language translation systems (which retrieve information from dictionaries and grammar rule sets) were replaced by statistical translation systems (which use statistical models based on the analysis of parallel texts). this transition enabled the invention of google translate and similar services, which represented a signicant breakthrough, albeit still with relatively high rates of error compared to human expert translations. a second advance was structural learning. using structural learning, a system can translate whole language structures, as opposed to individual words or phrases. one of its advantages is that words and sentences can be reordered, even across large chunks of text, creating a smoother, more natural output as opposed to a clunky, wordbyword translation. it is a far more complicated, but more promising, area of research, and carbonell said ongoing work in this area has already greatly reduced machine translation error rates. and, the combination of structural learning and deep neural networks promises further improvements.one particularly complex problem facing machine translation today, according to carbonell, is dealing with rare languages. uncommon languages create two main hurdles for current machine translation techniques: first, there is generally not a lot of existing data a computer can use to learn the language, and, second, in some cases rare languages have substantially different structures, such as more complex morphology, than more common languages. yet incorporating rare languages into machine translation is worthwhile, because it could help to preserve rare languages, such as alaska™s iñupiaq or greenland™s kalaallisut, and also to make the internet and the outside world more accessible to speakers of such languages. another key challenge is decoding word ambiguity. carbonell illustrated this challenge by presenting different uses of the seemingly straightforward english word ﬁline.ﬂ line can refer to a power line, a subway line, an actor™s line in a play, online, or many other meanings, and machine translation systems must use context to decide which continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.45meaning is correct. big data and innovative algorithms are crucial to developing models that can better handle such challenges.harnessing the wisdom of crowdsengineering a machine learning system and giving it data to learn from is not sufcient to create a truly intelligent machine. carbonell pointed out that learning in machines, like learning in people, takes time and tinkering. he explained that active learning is crucial to this process in which the computer identies questions or missing data and actively seeks answers or data to ll in the gaps. it is a continuous cycle between a computer and a human that allows the machine to rene its knowledge and understand nuance. in the case of machine translation, for example, the computer attempts a translation, identies a missing piece of information, and then asks a human to supply it. the machine then incorporates that data, and all the other data gleaned from active learning, into its models to create better and better translations over time. instead of one expert supplying data, carbonell said this function can also be performed by a crowd of nonexperts, which can be a less expensive approach to training a machine using active learning. when working on a translation, for example, the computer would catch an obvious error and solicit suggestions from the crowd. the crowd, offering subtly different translations, cumulatively helps to reduce ambiguities and improve the translation model. the process ultimately results in translations that are better than any single nonexpert in the crowd could create alone. although machine translation might not be as good as a professional human translator, the large and nuanced body of information created by the crowd helps the machine produce language that is recognizably human. machine learning is successful, carbonell noted, because ﬁthere™s no data like more data.ﬂ more data bring more learning. although carbonell acknowledged challenges to working with a crowd of nonexperts to rene machine translation models, these can be overcome and are worth the excellent training the translation systems receive. in carbonell™s view, future machine learning research will benet from both the large increase in available data and the rise of crowdsourcing. crowdsourcing can work especially well with a crowd of experts. in computational biology, for example, harnessing the collective ideas of multiple biologists has helped to parse complex and variable protein structures or interactions between proteins, tasks that are exceedingly difcult for one person or machine to perform alone but that can provide important insights for the development of new drugs or vaccines. with the advent of the era of big data, today is an exciting time for machine learning. by taking advantage of new, vast data sets and new modeling techniques, machine continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.46learning researchers are making important strides toward intelligent machines that can bring enormous benets to medicine, education, energy, nance, and society as a whole.achievements in artificial intelligence the goal of articial intelligence (ai) is to create computers capable of making decisions that produce a realistic outcome that is as good as or better than the outcome when decisions are made by humans. there are numerous applications of these technologies, and they are largely used to augment or support human activities by going beyond human decisionmaking capability in some way. in some cases, articial intelligence enables decision making in situations that are beyond the reach of humans owing, for example, to danger or physical constraints. in other cases, it is used to inform decisions that require more data than any human could access or process alone. the idea that machines could be built to think like humans is as old as computers themselves. eric horvitz, managing director of microsoft research™s main redmond laboratory and an expert in articial intelligence, presented an overview of the history and primary achievements of articial intelligence research and development. creating the theoretical foundationalthough the term ﬁarticial intelligenceﬂ was not coined until the 1950s, an important predecessor eld, known variously as operations research or decision science, blossomed in the 1940s and laid much of the groundwork for the birth of articial intelligence. operations researchers studied analytical methods to create models that aid in decision making. building on this context, john mccarthy, one of the cofounders of articial intelligence, coined the term in a 1956 proposal to pursue work related to forming abstractions, selfimprovement, manipulating words, and developing a theory of complex intelligences. notably, these are still active areas for articial intelligence research today. horvitz described the development of articial intelligence in the years since as a multisector, cooperative process spanning decades. ﬁthis has been a very shared, collaborative process across industry and academia with great funding from the agencies,ﬂ he said. after branching away from operations research in the late 1950s, articial intelligence researchers became particularly interested in logic, searching, and nding acceptable but not necessarily optimal, results (known as ﬁsatiscingﬂ) to make decisions. although this divergence narrowed the focus of articial intelligence somewhat, it also led to signicant innovation. in the mid1980s, the eld went through another transformation focused on how to handle uncertainty. unknowns are inherent in any system, and dealing with these continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.47creates pressures in articial intelligence, such as how to make decisions in highstakes scenarios or within realistic constraints, how to learn in an environment where data keep increasing, and how to interact with real people in the real world. opting to reduce their emphasis on resolving uncertainty altogether, researchers in this period became more focused on using articial intelligence to solve specic problems. in one darpafunded study, for example, paramedics used articial intelligenceœenabled devices to receive realtime advice while treating a patient in crisis (see figure 4.1). one important outcome of this early research was what are known as ﬁapproximations.ﬂ when a patient is gasping for breath, there is not enough time to run through every possible reason why this is happening and create a subsequent care plan; using approximations allows a system to compute decisions and determine each outcome quickly and coherently.these theoretical and methodological advances were driven largely by government funding, horvitz explained. over time, numerous federal agencies have been interested in pursuing articial intelligence for a variety of applications, from health to space exploration. agencies including the national science foundation (nsf), the ofce of naval research (onr), the national library of medicine (nlm), and the national aeronautics and space administration (nasa) greatly advanced the eld in its early days, in horvitz™s view, through targeted funding of early articial intelligence and machine learning research. figure 4.1 aienabled devices used in a medical crisis. source: e. horvitz and m. shwe, handsfree decision support: toward a noninvasive humancomputer interface, ˘ 1995:955, 1995. courtesy of eric horvitz, technical fellow and managing director at microsoft research.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.48impacts and achievements of research on intelligent machinesthis early, governmentfunded articial intelligence research had an enormous impact, not just on technology but also directly on the u.s. economy. in the 1990s, pairing articial intelligence research with the growth of the internet enabled the creation of ecommerce, a crucial driver of today™s economy. for example, about 20 years ago researchers started working on what is now known as ﬁcollaborative ltering.ﬂ this articial intelligence fuels the recommender engines on websites like net˚ix and amazonšthe ﬁyou might also likeﬂ suggestions that propel a signicant proportion of ecommerce activity. researchers with the inclinationšand funding, largely from government sourcesšplayed an instrumental role in developing and rening collaborative ltering, enabling the eventual commercial applications that we depend on today.other key achievements in articial intelligence that can be traced to early governmentfunded research include computeraided perception, language, and movement tracking. horvitz described how research funded by darpa and other agencies followed a clear path to today™s face recognition technology, now used in myriad applications including military intelligence and national security, crimeghting, and consumer uses. today™s articial intelligence systems can process and match data based on images of faces as well as auxiliary information such as location, events, and even clothing. as techniques for perceiving visual cues and understanding language became more rened, these developments also paved the way for teaching machines how to track and understand human movement. research in this area led directly to consumer products like the xbox kinect and nintendo™s wii, which track and respond to the body™s movements. to articial intelligence researchers, horvitz said, these products seem shockingly inexpensive considering the enormous amount of hard work and innovation that led to their invention. another key innovation rooted in articial intelligence research is stacked representation, also known as neural networks. although this modeling approach emerged in the late 1980s, there were not enough data available at the time for neural networks to make accurate predictions. with the rise of big data and today™s dataintensive scientic methods, together with conceptual advances in how to structure the networks, neural networks have reemerged as a useful way to improve accuracy in articial intelligence models. they have been applied, for example, to reduce the error rate in speech recognition systems. these advances enabled many innovations, such as the skype realtime translation service, which, horvitz said, ﬁwould stun our colleagues 10 or 15 years ago.ﬂ since horvitz™s early experience with the darpafunded project to provide articial intelligence support for paramedics, the eld has advanced numerous applications in health care, including increasing hospitals™ ability to predict readmissions and allowing doctors to perform surgery remotely, a technique known as telesurgery (see figure 4.2). continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.49successful telesurgery research, funded by darpa, enabled researcher phil green at sri to create intuitive surgical, a company of substantial value specializing in methods that enable minimally invasive surgery through robotics and articial intelligence. ﬁit™s stunning what the darpa investment could do,ﬂ re˚ected horvitz. promising prospects for the futuretoday, articial intelligence work continues to advance through collaborations among industry labs and federal agencies. for example, work by microsoft and google, building on advances funded by darpa, has led directly to technologies we now use daily, including grammar checking and personal assistants like siri and cortana. in transportation and infrastructure, articial intelligence work has been applied to improve wind maps for aviation and urban trafc modeling, among many other things. future articial intelligence research, horvitz predicted, will likely include enhancing vehicle safety, improving selfdriving cars, and improving the ability of computers to answer deeper questions. humanœmachine collaboration, in which a problem is divided into two parts, one given to a computer and one to a human to work on together, also holds great promise. for example, a surgical approach in which a machine and a human work together could bring huge benets to patients and medical staff. horvitz identied augmented cognition, where machine learning complements human cognition in areas such as memory, attention, or judgment, as another exciting figure 4.2 full da vinci s surgical system. source: courtesy of intuitive surgical, inc.: full da vinci s surgical system, ©2016 intuitive surgical, inc.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.50research area. integrative articial intelligence, or the creation of systems that can interact with the complexity of realworld settings, also holds great promise. integrative articial intelligence, said horvitz, could be the key to transforming computers, which currently have deep but very narrow intelligence, into broader, more humanlike thinking machines.it is clear from horvitz™s many examples that governmentfunded articial intelligence research has reaped many benets for the technology sector, the economy as a whole, and our everyday technologies. continued research will no doubt bring future rewards in this promising and fastevolving eld, horvitz said.robotics: from vision to reality robotics is another area in which engineers have made remarkable gains in developing machines that can operate independently and make smart decisions. today™s robotics achievements re˚ect a strong governmentœindustryœconsumer pipeline that has had important impacts on science, industrial manufacturing, and our everyday lives. rodney brooks has long been at the forefront of this eld. among the earliest pioneers of robotics, brooks has seen his work go to mars and into people™s kitchens. his work at stanford university in the 1970s, funded by nasa, focused on creating simple mobile robots. at the time, creating robots also required that one either invent or implement needed components such as stereo vision, map building, and planning. at the time, creating a robot able to move 20 feet by itself over the course of 6 hours was considered a huge victory. as today™s martian rovers and vacuuming robot roombas make clear, we have come a long way. slam dunkaccording to brooks, one of the most crucial innovations that propelled robotics into the eld as it is known today is slam (simultaneous localization and mapping). slam is an essential skill for robots: it is what gives them the ability to enter an unfamiliar environment, map it, and understand their own place within that map. remarkably, two papers presented at the same conference in 1985, one by brooks and his team at mit and one from a laboratory in france, trying to solve the same problem independently, led to slam™s creation.1 after the conference, the two teams™ work was disseminated across the robotics research community.1r.a. brooks, 1985, visual map making for a mobile robot, pp. 824829 in 1985 ieee international conference on robotics and automation, proceedings, doi:10.1109/robot.1985.1087348; r. chatila and j.p. laumond, 1985, position referencing and consistent world modeling for mobile robots, pp. 138145 in 1985 ieee international conference on robotics and automation, proceedings, doi:10.1109/robot.1985.1087373.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.51slam turned out to be key to solving many thorny robotics issues, and by 1991 the academic research community had collectively made signicant improvements on initial slam approaches. this process, said brooks, illustrated how the community inspires itself and propels research forward, from one federally funded idea to another. it also showed how making a hardware prototype, however imperfect, freely available for others to tinker on moved robotics from a nebulous theoretical area to a series of welldened research problems that scientists could then collectively solve. initially the bulk of this work was led by federally funded labs at stanford, mit, and the university of pennsylvania; by the mid1990s, many more researchers were working on further improvements. some initial robotics projects were funded by darpa, nasa, and nsf for applications in defense, space, and science, respectively, but the consumer products industry also benetted from this research. in fact, the selfdriving google car and other highend cars with highly computerized functioning are direct descendants of slam and darpafunded research. federal grand challenge and urban challenge grant programs were specically launched to drive innovation and progress on functional autonomous vehicles; industry then took slam out of the labs and put it on real roads. ﬁthere™s a long history, from the late 1970s to now, of an idea that wasn™t about selfdriving cars when it startedšit was about navigation on other planets,ﬂ re˚ected brooks. learning from natureof course, robots do not only need to understand and map their environments; they also need to physically navigate them. stuck on the problem of improving robots™ ability to navigate the rough and unpredictable terrain on other planets, brooks turned to an approach known as behaviorbased robotics. in behaviorbased robotics, engineers use the natural movements of creatures such as insects, spiders, and birds to inspire new robot structures and ways of moving (see figure 4.3). these approaches, for example, can improve a robot™s ability to right itself if knocked over or avoid getting stuck in crevasses. after some early success based on these new robotics models, brooks was awarded nasa funding that enabled him to develop the mars rover. the success of the mars rover encouraged brooks to start his own private company to build robots, irobot. in this capacity, he continued to develop robots for government applications; for example, darpa funded work to create robots to search for and dispose of improvised explosive devices (ieds) in iraq and afghanistan. the company also pursued consumeroriented robots, including the roomba, a robot vacuum that has sold 14 million units (and inspired countless youtube videos of cats riding roombas). one story from irobot™s early days illustrates the serendipity of innovation and just how difcult it is to predict when a research project might go from the theoretical to the practical. in the mid1990s, the japanese government provided irobot some initial fundcontinuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.52ing to begin developing robots to support operations in japanese nuclear power plants. the project was later aborted after the japanese government decided the robots would not be needed. two decades later, when the 2011 fukushima nuclear reactor meltdown rendered the plant too unsafe for people to enter, irobot™s battlehardened ied disposal robots were called upon to enter the plant and survey the damage. cultivating a softer sidebrooks highlighted the fact that research advances in humanoid robotics have also made it easier for robots to be deployed in factories among people. he explained that before recent improvements in user interfaces and robotic design, factories had to separate their human employees from the robots used in manufacturing processes. the robots, with their complicated user interfaces, awkward movements, and enormous size, were too dangerous for most people to work with. today™s humanoid robots allow factory workers with no scientic or robotic expertise to easily and safely train and monitor their robotic partners (see figure 4.4).these industry robots, now ubiquitous in thousands of manufacturing facilities, can trace their lineage back to agencies like darpa and nasa, which, despite not knowing exactly what the outcomes would be, led the way toward key robotics breakthroughs figure 4.3 hexapod robot. source: courtesy of burhan saifullah.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.53by funding basic research in the eld™s early days. without this initial early government funding, robotic factories and selfdriving cars would likely still be mere mirages on the faroff horizon.despite today™s remarkable technological capabilities, however, we still have a long way to go before we can use some of these technologies to their fullest potential. the adoption of driverless cars and trains, for example, will require not just better technology but also more trust and acceptance on the part of the public, brooks explained. the 2009 crash of a selfdriving metro train in washington, d.c., set social acceptance of autonomous vehicles back despite being a more efcient way to run subway lines. it is often the case, he said, that even when a new technology is ready for the consumer market, the consumers might not be ready for it.sharing their own perceptions of the eld, several attendees noted that robotics research really took off once mobile robotbuilding platforms became inexpensive enough that every lab could afford one. instead of a few teams working on research problems, suddenly there were dozens or hundreds of teams actively building off of each other™s innovations, and the eld thrived. while governmentfunded research was clearly crucial for the eld™s beginnings, brooks noted, it is the ongoing synergy of research funding, academic labs, and industry products that continues to fuel innovation. figure 4.4 industrial assembly robots. source: courtesy of rethink robotics, inc.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.54people and computers as all of the workshop presenters made clear, computers and communications technologies have had a profound impact on everyday lives and will continue to do so in the foreseeable future. a common thread of the workshop was the constant push and pull between technology and people: the ways people in˚uence technology and, in turn, the ways technology in˚uences people. this chapter focuses on three presentations exploring different aspects of the relationships between humans and technologies: cybersecurity, usercentered design, and social science research. in his presentation, stefan savage shared key challenges and trends in cybersecurity. in the next presentation, scott hudson described the emergence and key contributions of usercentered design in computer science. a basic premise of usercentered design is that no matter how innovative or elegant a new program or piece of hardware is, technology sinks or ˚oats based on how usefulšand usablešit is. drawing examples from academic research and commercial products, hudson described the practice of usercentered design and its crucial role in the success of many of the technologies depended on today. the third presentation, by duncan watts, explored how technology has been informed byšand has itself opened up vast new opportunities foršsocial science. insights about how people behave and interact with each other are of tremendous value for businesses and governments alike, and watts described how this research has been fueled by a strong governmentœacademiaœindustry ecosystem. continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.55seeking cybersecurity while much of the workshop focused on the positive outcomes of the many technological innovations that have been enjoyed over the past few decades, stefan savage discussed a darker aspect of this technological growth: cybersecurity. savage, a computer science professor and researcher at the university of california, san diego, has spent his career working on cybersecurity topics such as network worms, malware, and wireless security. dening ﬁsecurityﬂ as freedom from fear and danger, savage described how technological changes have brought both new fears and new dangers. he attributed most of today™s security challenges to ve major developments: the internet and its pervasive connectivity, ecommerce, data centralization, mobile technologies, and the emergence of the internet of things. in short, savage said, ﬁwe have handed over control of our lives to computers and to the networks that interconnect them.ﬂ what makes cybersecurity such a pernicious problem, he explained, is that it is not merely a technological challenge that can be ﬁxed.ﬂ because cyberattackers are human and stand to gain nancially from these activities, it is actually a socioeconomic problem with adversaries, victims, and defenders. technology simply provides the tools and setting for these battles to unfold. another key challenge, according to savage, is that ﬁwe have no way to evaluate security solutions except by how they fail.ﬂ this confusion leads to a morass in the cybersecurity eld with many problems but few solutions.savage discussed how cybersecurity is also a crosscutting discipline. the challenge for this eld has long been how to build security solutions into the entire array of quicklydeveloping technologies and applications, from machine learning algorithms to wearable computers and personal monitors to robots and autonomous vehicles. savage traced many of the major components of cybersecurity in use today to 30 years of federally funded academic research. thanks to government funding, academic researchers not only were able to focus on developing individual security solutions for individual problems but also had the time and funding to incorporate cybersecurity research into larger public policy problems. this government funding has spurred industrywide advances in cybersecurity that extend far beyond the reach of one specic product or technology.rather than delving into the long history of computer security, savage focused on two recent stories that illustrate how governmentfunded academic cybersecurity research has been essential in creating industrywide standards that protect consumers and businesses. the rst story, from the automobile industry, led to safer cars, and the second story, relating to software piracy, helps businesses protect intellectual property. continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.56security in transportationalthough most people may not be aware of it, savage said transportation today is ﬁdeeply computerized.ﬂ a single car, for example, may have up to 30 different computers all networked together, from the radio to the brakes to the air conditioning (see figure 5.1). many of these computers are designed to provide functions that humans cannot; for example, adjusting the fueloxygen mixture to regulate emissions. others enhance safety or provide entertainment. the net result is a huge amount of digital information being created and transmitted by incredibly complicated systems; yet, most incar computer systems are equipped with far fewer security protections than a typical personal computer. savage™s research has shown that it is possible to tap into these systems to remotely take over a car. in one experiment, for example, his team was able to deactivate the brakes of a brand new car, straight off of the dealer™s lot, from 1,000 miles away using several undefended virtual access points. savage pointed out that the car industry has some ﬁextratechnicalﬂ challenges that make incorporating cybersecurity especially difcult. car manufacturing is a complex process involving numerous thirdparty suppliers; many of the component computers that wind up in a single car come from different manufacturers and use different programming languages, making it difcult to secure both the component parts and the car as a whole. the modern automobile–exhaustengine control unittcutransmissionbrake linerrrraaaakkkkkeeeelllliiiinnnneeeebbbbbbbrrrrabsairbag control unitbody controllerlocks/lights/etcradiotelematics internet/pstnhvackeyless entryantitheft1figure 5.1 computers in the modern automobile. source: courtesy of karl koscher and stefan savage.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.57in addition, in many cases the systems are so complex that traditional cybersecurity tools do not apply. ﬁif you bought an american car in the last few years, it probably has a phone number and an internet address,ﬂ said savage. ﬁthere are lots of good reasons why it has that, but the side effect is that we now have this kind of systemic risk.ﬂcars are just one example of the broader trend toward the ﬁinternet of things,ﬂ in which computers and connectivity are being built into many types of products beyond typical computer products like laptops and smartphones. this trend is especially active in the area of transportation. ﬁthere is almost no trip that you take, whether it is up or down a ˚oor or whether it is through the air, that you are not ultimately depending on a computer to do the right thing,ﬂ said savage. anything with computer control and connectivity is subject to risk, whether it is a car, airplane, train, or refrigerator. yet the companies that make these products do not feel the same pressure to increase their security as do companies that make traditional computer products; because no one is known to be attacking such products, companies have so far made only modest security investments.but there is good news. by exposing weaknesses, research by savage and others has propelled a wholesale overhaul of how the american auto industry designs software. savage noted that this was not something that would have been advanced by the private sector alone; only academic researchers had the time and longterm vision to unravel these problems, work with regulators and industry leaders, and convince the national transportation and safety board that these were pressing problems requiring industrywide solutions and standards. even after new security standards and recommendations were in place, savage said that the auto industry might have been tempted to ignore them, except for the fact that around the same time, toyota was forced to pay more than $1 billion in nes because of its ﬁunintended accelerationﬂ problems.1 this quantied a previously unquantiable problem for the auto industry by revealing what a security breach could cost them. in savage™s view, the prospect of future enormous payouts scared industry into nally adopting the cybersecurity standards that had been developed by academia with government support. fighting spam and piracywhereas cars and other computerized devices are examples of an underappreciated cybersecurity threat, industry and the general public have a much greater awareness of the problems of spam and piracy. in particular, the sale of pirated software is a particularly active problem and one that the software industry spends hundreds of millions of dollars to stem.savage described an innovative approach his team developed to ght internet spammers selling pirated software. until recently the traditional approach has been to lter 1c. woodyard, 2012, toyota to pay $1.1b in ‚unintended acceleration™ cases, usa today, december 26, http://www.usatoday.com/story/money/cars/2012/12/26/toyotaunintendedaccelerationrunawaycars/1792477/.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.58spam emails, shut down websites hawking pirated software, and seize any goods. but this approach rarely solves the problem permanently, because the same spammers can easily pop up again using new email and web addresses (see figure 5.2). also, the spam system works in large part because there is consumer demand for cheap pirated software. ﬁup to 40 percent of all revenue from email spam comes from people who go into their spam folder and click on the items there, because they want those things,ﬂ savage said. given this challenging context, savage and his team approached the problem from a different angle. realizing that it™s a game being played for nancial gain, they tried to nd a way to undermine the nances of the software pirates. targeting email spam as just one symptom of the larger problem of piracy, the team untangled the complex connections going from the email offer, to the web proxy, to the domain server and several other points along the way until getting to the actual nancial processing. in order to undermine spammers™ nancial gains, savage™s team purchased more than 600 items from spam emails (using no government money, savage noted, though the research was otherwise governmentfunded), and followed the processing chain to determine how the spammers were receiving their money. the study revealed that 95 percent of the money acquired through pirated software spam was going through just three banks.2 once alerted to the penalties of working with spammers, the banks quickly dropped these accounts, leaving spammers with no way to monetize their sales. while 2c. kanich, n. weavery, d. mccoy, t. halvorson, c. kreibichy, k. levchenko, v. paxson, g.m. voelker, and s. savage, 2011, show me the money: characterizing spamadvertised revenue, pp. 219234 in proceedings of the 20th usenix conference on security (sec™11), usenix association, berkeley, calif.figure 5.2 spam email complex value chain. source: k. levchenko, a. pitsillidis, n. chachra, b. enright, m. félegyházi, c. grier, t. halvorson, et al., click trajectories: endtoend analysis of the spam value chain, pp. 431446 in —–ƒƒ˘ doi:10.1109/sp.2011.24. courtesy of christian kreibich.credit cardcontinuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.59switching emails or domains is easy, switching banks is far more difcult for spammers, and this strategy has proved effective in shutting down certain types of spammers. as a result, there has been a substantial drop in sales of pirated software as a whole, and the process of targeting spammers™ nances is now widely used by virtually all companies looking to protect their intellectual property from unauthorized distribution. in computerized cars and software piracy and on numerous other cybersecurity fronts, savage said academia has a crucial role to play and that solutions cannot be left to industry or government alone. with government funding, academic researchers have the ideal funding structures and culture to experiment with strategies before their value is obvious. companies and missionoriented government agencies, on the other hand, are often crisisfocused and unable to invest in the longview, experimental solutions cybersecurity requires. this critical, governmentfunded academic work has exposed weaknesses in consumer products and bolstered the intellectual property rights of businesses, said savagešand our citizens and our economies are safer because of it. the usercentered design renaissance scott hudson, a professor of humanœcomputer interaction at carnegie mellon university, presented an overview of the evolution and spread of usercentered design and its impacts on the adoption and success of technology. although it™s easy to assume this approach was a foregone conclusion, he described how computer scientists spent decades working hard behind the scenes in order to ﬁmake it easy to make things easy.ﬂthe enormously successful photosharing application instagram, developed by a team of two, gained 30 million users over a 2year period until being acquired by facebook for $1 billion. although it™s an extreme example, this story illustrates the enormous value of usercentered design in the creation of software products: easeofuse was critical both to the meteoric rise of instagram among consumers and to the ability for such a small team to develop and quickly deploy such a wildly successful app. in the early days of the personal computer and the internet, using software to perform even relatively simple tasks took knowledge and experience. today, most software is designed to be so intuitive to the user that there is virtually no learning curve involved. but the benets of usercentered design do not stop with the user: software developers and the technology industry as a whole have benetted from this trend as well. where it used to take experienced developers weeks or months to create the user interface for a new software tool, now nearly anyone can create technology applications quickly, easily, and well, even with minimal technical skills. continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.60building toward a sea change computer scientists were not initially focused on usercentered design. in the 1980s, many academic researchers were working on simplifying computer programming, but from a topdown, systemslevel perspective. the useršthe person who would ultimately interact with the software to perform tasksšwas an afterthought. early academic research projects such as tiger3 and adm4 were intended to simplify user interfaces but were too systemsfocused; these and other processes born of academic research were clunky and complicated for nonexperts. it was from this context that a gradual sea change emerged in the 1980s and 1990s, creating a growing recognition that computer science needed to be usercentered, not systemscentered, in order to succeed. more and more, people began to realize that the solution to clunky, difcult software would come not from a new technology or algorithm, but from a new approach to design altogether. this new usercentered mindset, although ultimately the key to the success of many technologies and companies, complicated everything. even compared to the programming required to make a highly complex computer system work, guring out the user™s needs and preferences is an extremely tricky challenge. ﬁusers are hard to deal with, because you can™t open the user™s head and pour in the right mental model,ﬂ said hudson. recognizing that creating something that can be used is the fundamental end goal for computers, computer scientists had to come to terms with the fact that you must design for the users you actually have, not the users you wish you had, he added.hudson described how the rst generation of developerfriendly toolkits for creating userfriendly interfaces emerged from exchange and interplay between academic researchers at places like carnegie mellon, stanford, and mit and companies such as sun, apple, and microsoft. in the 1990s, three major projects led by linton at stanford, myers at carnegie mellon, and hudson at the university of arizona and georgia tech created functional user interface design toolkits including interviews and fresco, garnet and amulet, and artkit and subarctic, respectively. contributions from these projects, such as concurrency models, resizable icons, and layout abstractions, are evident today in apple, android, and adobe user tools.another successful tool coming out of that era was graphic user interface buildersšprograms that allow developers to actually draw the graphical parts of a graphical user interface instead of creating them only with lines of code. these visual tools were a big success and led to what is now a maxim of usercentered design: ﬁvisual things 3d.j. kasik, 1982, a user interface management system, computer graphics 22(4):113120.4a.j. schulert, g.t. rogers, and j.a. hamilton, 1985, admša dialogue manager, pp. 177183 in proceedings of sigchi conference on human factors in computing systems (chi ‚85), association of computing machinery, new york, n.y.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.61should be expressed visually.ﬂ today, every modern development environment has visual components. microsoft even used the term ﬁvisualﬂ in a series of programming products: visual basic, visual c++, and visual studio.a new way of creating technologyonce usercentered design became a more universal and widely accepted part of the technology development process, more innovations followed. ﬁnow we see the ngerprints of this work all over modern interactive systems of all sorts,ﬂ re˚ected hudson. one important example of early usercentered design research was columbia university™s 1997 touring machine project, in which researchers experimented with deploying, in realworld situations, mobile computing contraptions that included a computer with wireless internet access, gps, and a handheld display and input (figure 5.3). the machines, despite being impractical, were an essential part of the userinteraction research that laid the groundwork for mobile devices to come. building and using them helped researchers explore how people might use a mobile device with internet connectivity, and the project™s devices are seen as important early precursors to the iphone. another example, from a subarea of usercentered design focused on interaction techniques, is the ﬁpinchﬂ gesture: a smooth interaction for simultaneous translating and scaling text or an image. while it may seem new, this feature actually traces its roots to myron kruger™s 1983 videoplace gaming system. the zoomable interface was invented in 1994, and adding zoom to the pinch gesture allows users to maximize space and readability on the small screens of today™s smartphones and wearable devices.in hudson™s view, these and many other innovations, enabled by the major cultural shift away from systemsfocused design and toward the user, were signicant drivers behind today™s ﬁthere™s an app for thatﬂ world. in less than 10 minutes, as opposed to days, weeks, or months not so long ago, a novice can now create a working app and start accumulating users and revenue. ﬁcomputing has now spread out into lots of places it hasn™t been before, . . . [so] it has had a tremendous impact that we haven™t seen before,ﬂ said hudson.figure 5.3 prototype 3d mobile augmented reality. source: s. feiner, b. macintyre, t. hollerer, and a. webster, a touring machine: prototyping 3d mobile augmented reality systems for exploring the urban environment, pp. 7481 in ˘ 1997, doi:10.1109/iswc.1997.629922. courtesy of steven k. feiner.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.62taking a usercentered approach has led computer scientists to change the questions that they ask when conceptualizing a new product, explained hudson. instead of wondering if they can build a technology, or how to start building it, following a usercentered approach means asking, ﬁhow well does it work with the user?ﬂ usercentered design has been revolutionary for many applications and areas of computer science, including wearable computing, contextaware computing, and data visualization.a general lesson from the story of usercentered design, hudson explained, is that rather than investing in research to develop only individual technologies, it is important to target work that has an amplifying effect across the broader eld. ﬁeven more than lots of individual technologies, things like thisšideas that amplify other ideas and enable other ideasšare really what we should be after,ﬂ he said. while it is impossible to predict at the outset which research project will lead to the next industrywide innovation, there is still a great deal of room for improvement in user experience and other crosscutting areas of computer science. in the usercentered design space, for example, hudson said highperformance computing could be made much more accessible with simple, userfriendly tools. such tools could enable nonscientists, such as small business owners, to learn more from the specialized data they collect.the mindset change from a topdown, systems view of computer design to a usercentered view has had an enormous impact across multiple technologies and the technologydriven economy in general, mostly by amplifying the impact of technologies and by making them easier to develop. in hudson™s view, usercentered design is an idea that then inspires other ideas, but there was no direct path or single research project that led to this epiphany. rather, it took a winding path and even some seeming dead ends to gradually build into a sea change that enabled the ˚exible, userfriendly tools and technologies we enjoy today. harnessing big data for social insights technological developments over the past several decades have opened up powerful new opportunities for understanding people and societies. new ways of generating, collecting, and analyzing social data have shed new light on economics, politics, sociology, anthropology, and many other areas within the social sciences. research into the questions posed by these elds touches every aspect of human society, from families and interpersonal relationships to highstakes topics like presidential elections, international politics, and economic markets. government funding has long been central to enabling new insights in these areas. duncan watts, a principal researcher at microsoft research, offered his views on how continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.63the online world has changed social science, the emerging importance of computational science as a way to understand and solve social science research questions, and the role of the governmentœacademiaœindustry ecosystem in advancing this eld. watts began his research career in mathematics but quickly became fascinated by the dynamics of connections and networks among people and has conducted research at columbia university, yahoo! research, and other organizations. at microsoft, he studies the social networks that dominate today™s online culture. from social science to social media (and back again)watts described how federal funding has been instrumental in laying the groundwork for a whole new sector of the economy: the online social world. to the casual observer, it is easy to assume that wildly popular social media companies like facebook and buzzfeed simply stumbled upon winning formulas for connecting and engaging their users. in reality, facebook and other social networks are built, in part, on research from the early 2000s examining the drivers of network structure, network growth, and social contagion, while buzzfeed and other news sites build from research on the nature of social in˚uence to tailor their articles to what readers are most likely to enjoy and share with friends, watts explained. in addition to underpinning such applications in the forprot sector, watts said fundamental research on networks and relationships is also being integrated into basic and translational research in other areas of science, such as medicine, physics, and biology. conversely, just as the social sciences helped to fuel the growth of social media, social media are providing new fodder to advance social science research. now that online social networks are thriving, watts and other researchers in government, academia, and industry are tapping into this new online world and its data to further study human networks and social problems. a new way to do researchsocial scientists have long studied ﬁofflineﬂ social networks, but in the past there was no easy way to harness large amounts of social and behavioral data, and data collection was often a painstaking and timeconsuming process. ﬁif you™re trying to understand how information ˚ows through a society, you need to know what people think, you need to know when they change their minds, you need to know whom they are talking to. this is a tremendous observational challenge when you are talking about millions of people,ﬂ said watts. now, with people constantly interacting with media and each other through technologies capable of recording and storing their behavior, researchers are making progress far more quickly. ﬁwe realized after [the invention of social networks] that this is a tremendous wealth of information that can inform us about social interaction and behavior,ﬂ said watts.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.64in particular, online surveys and crowdsourcing tools such as mechanical turk have made it easier for researchers to connect with research subjects and gather data. for example, researchers can now take advantage of the ﬁbored at workﬂ networkšpeople who are constantly on computers and willing to ll out surveys as a brief distraction from their daily tasksšto quickly and cheaply gather survey data. in addition, the emergence of virtual meetings has made it more feasible for researchers to study group dynamics because it is no longer necessary in every case to gather people together in the same room at the same time.one of the most powerful aspects of social media for social science is that data can be collected passively, without relying on information that is actively solicited through surveys or focus groups. because the use of social networks is so widespread and users of these services are generating so much data, social science is increasingly becoming a computational science, in which researchers tap extremely large data sets for insights about people™s behavior. ﬁit™s clear to us working in the eld now that social science over the last decade or so is rapidly becoming a computational science,ﬂ said watts, adding that the use of highperformance computing has beneted greatly from the interplay between governmentfunded academic research and industry data and tools. watts said, ﬁi think it™s also very clear that both federal funding and support from industry labs have been critical.ﬂwatts highlighted a handful of examples of how social media and online networks have shed light on human behavior. for example, one nsffunded study watts™s group conducted in the mid2000s, when he was at columbia university, showed how social media can create a snowball effect in which the perceived popularity of an item in˚uences more people to like it. the study revealed that the more popular a previously unknown song appeared to be (as indicated by how many ﬁlikesﬂ it had), the more popular it became, while songs with fewer ﬁlikesﬂ were basically ignored.5 so, although we tend to assume that we make our own decisions about our purchases and preferences, people are more subject to others™ tastes than we think. the study provided valuable evidence that the consumer market doesn™t merely reveal preferences but can construct them through a process of social in˚uence.in an earlier study, watts and his collaborators used email to replicate stanley milgram™s famous ﬁsmall worldﬂ experiment, which determined that there are a median of six degrees of separation between people. the nsffunded study achieved the same results as milgrim™s, revealing that people could reach a stranger, even across the globe, through a chain of 57 contacts, on average (see figure 5.4).65m. salganik, p. dodds, and d. watts, 2006, experimental study of inequality and unpredictability in an artificial cultural market, science 311:854856.6p. dodds, r. muhamad, and d. watts, 2003, an experimental study of search in global social networks, science 301(5634):827829.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.65a more recent study, which watts oversaw in his current position at microsoft research, tracked information dissemination on twitter and attempted to quantify and categorize what makes news items ﬁgo viralﬂ on social networks.7 social media hold virtually limitless potential for insights into human behavior; further applications of these data, for example, include crisis mapping, the realtime gathering and analysis of social media information during a political crisis or natural disaster, and digital ethnography, the study of relationships in a digital rather than a physical space. in addition, watts said smartphones offer fertile ground for research and could be used as ﬁsocial sensorsﬂ or to mine data on productivity. the thousands of data points being generated by smartphones and all of our other interactive technologies hold ﬁprofound implications for what we could know about the state of the world, what we could know about the collective mind, what we could do in terms of interventions, peer in˚uence, and collective behavior or crowd computing,ﬂ said watts. 7s. goel, a. anderson, j. hofman, and d. watts, 2015, the structural virality of online diffusion, management science 117.figure 5.4 milgram™s six degrees of separation experiment replicated by email.  source: duncan j. watts, ﬁfrom small world networks to computational social science,ﬂ presentation to the workshop, march 5, 2015, http://sites.nationalacademies.org/cs/groups/cstbsite/documents/webpage/cstb160426.pdf. continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.66a prolic data ecosystemsocial science is an area in which the governmentœacademiaœindustry ecosystem is particularly evident. watts pointed out that while most social science research is supported by the government, much of the data used in these studies is generated by companies. the technology industry is also funding and advancing research, not merely providing the datašparticularly in the area of computational research in which large data sets are mined for insights. yahoo! labs, for example, conducts its own research while also making data sets freely available for academic use. the marriage of computer science and social science will be crucial to advancing the next generation of social science questions and solutions, watts said, adding that both federal and industry funding will be critical to this effort. looking back, watts admitted that he never would have expected to see an online social network capable of reaching a billion people, yet this has come to pass. as this example shows, it is impossible to predict what will be available 10 or 15 years from now, even for experts embedded in the eld. although reluctant to make specic predictions given this inherent uncertainty, watts said a likely key to future social science insights will be an increasing trend toward interdisciplinary work. social science is interdisciplinary by nature, yet social scientists and computer scientists often work separately in academic departments with little overlap. a greater emphasis on more interdisciplinary, mixedmethod research focused on solving problems, not just publishing papers in journals, will be important to keeping social science research moving both forward and sideways into other scientic elds, said watts. finally, watts noted that the pushandpull between humans and technologies becomes ever more critical to understand as the computing world moves closer to the human world. as robotic technologies become more integrated into our lives and wearable computers literally become a part of us, new approaches to computer design will be needed in order to fully understand the needs of the user and design the best possible solutions. this is a key area in which interdisciplinary work uniting social science, usercentered design, and computer science will be crucial to advancing effective solutions. continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.67wrapup discussion this chapter sums up some of the discussions presenters and attendees had at the workshop. in a relatively short time, technology has transformed the human experience from the personal and familial to the level of entire societies and economies. looking back over the tremendous technological achievements of the past several decades, some participants said it might be tempting to imagine that each advancement built on its predecessor and laid the foundation for the capabilities to follow. indeed, to a casual observer, it might seem logical, even inevitable, that the personal computer led to the laptop, the smartphone, and the smart watch, or that the rst computertocomputer connection led to the internet, to cellular networks, and to wifi.however, as one workshop attendee pointed out, innovation does not automatically follow innovation like dominoes set on end. rather, innovation is a messy, unpredictable, and at times convoluted process. ideas emerge, diverge and converge, blossom and wither, only to reemerge in unexpected places decades later. each stepšfrom personal computer to laptop, from laptop to smartphonešrequires radically new architectures, new hardware and batteries, new software and user interfaces, new ways to store and transmit information. and powering it all is an incredible amount of human ingenuity. given the long list of research organizations, including the federal, academic, and industrial entities mentioned over the course of the workshop, it was pointed out that none of the outcomes could have been achieved by a single company, research enterprise, or government. as another participant put it, nearly every component of every incremental technological innovation has its roots in the complex interplay among fundamental research and development in federal agencies and universities and further research, development, and deployment by privatesector companies. continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.68in some cases, such as the early development of the internet, as highlighted in cerf™s presentation and the ﬁtire tracksﬂ graphic, the relationship among these players can be roughly visualized as a pipeline, starting with a visionary government funding program, which in turn powers academic research, which then generates insights or technologies that are ultimately adopted for commercial use by industry. in other cases, such as the parallel progress of social science and social media, advances emerge from an interwoven, interdependent ecosystem in which government and academic research and industry data and tools alternately build off of each other. at the heart of each of the stories presented at the 2015 continuing innovation in information technology workshop lies a common theme: several workshop attendees could not imagine that today™s incredible technological landscape would have emerged as quickly or as fully had it not been for the rich body of work conducted with government funding. discussions among some workshop attendees highlighted that industry clearly has played a crucial role in applying, scaling, and commercializing technologies and has even conducted a good deal of early research and development. but the incentives and funding structures that drive industry are not sufcient, alone, to support the highly experimental, uncertain, and broadbased basic research that lays the foundation for truly revolutionary innovations. from critical infrastructure such as the internet to techniques and regulations that support cybersecurity, government and governmentfunded research has played a central role in the development of the vast majority of computer science methods and tools that we depend on today. as discussed by jahanian, when the government funds computer science research to fuel engineering innovations, the costs are shared by the u.s. population. so, too, are the benets. several other examples presented during the workshop, including advances in the use of big data, intelligent machines, and robotics, governmentsupported technology innovations have supported our military and national security, fueled our global leadership in science and medicine, helped empower citizens as a whole, and enriched the economy. the benets of industry investment in technology research and development, too, extend not only to the people [i]ndustry clearly has played a crucial role in applying, scaling, and commercializing technologies and has even conducted a good deal of early research and development. but the incentives and funding structures that drive industry are not sufcient, alone, to support the highly experimental, uncertain, and broadbased basic research that lays the foundation for truly revolutionary innovations.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.69who can afford to buy the commercial products they create, but also our own military and government, as well as the economy as a whole. the nation stands on the brink of yet new transformations. as discussed by horovitz and brooks, recent developments make it feasible to envision that robots will, in the notsodistant future, routinely work alongside humans to rescue, protect, and serve us. that selfdriving cars, trains, and planes will regularly deliver us safely to our destinations. that new types of immersive, responsive digital environments will instruct and amuse us. and that many of us will benet from sophisticated devices worn in and on our bodies to support our health and wellbeing. as noted by several attendees, the technologies of the past and present suggest that all of the innovations envisioned for the futurešas well as those we cannot yet imaginešwill likely emerge not from the genius of one person or one company, but from a complex, symbiotic cycle between governmentsupported longterm applicationengaged research alongside industrydriven solutions and applications. tomorrow™s technologies, like innovations past, are not a foregone conclusion. to realize envisioned innovationsšand even to maintain access to the technologies already relied onšthere are substantial obstacles in terms of the hardware, software, the infrastructure, and society itself. for innovation to thrive, it is crucial to cultivate the entire governmentœacademiaœindustry ecosystem that supported this eld in the past and that is essential to driving it forward into the future. continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved. appendixescontinuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.73committee biographies peter lee, chair, is a computer scientist and technology innovator at microsoft corporation. as corporate vice president, dr. lee™s mission is to create researchpowered technologies and products for microsoft, while at the same time advancing human knowledge through the open dissemination of fundamental research. he leads the company™s new experiences and technologies group (msr next), a global organization that conducts r&d in a wide range of technology areas. recent scientic contributions and technology innovations from next include advances in deep neural networks for computer vision, as well as the simultaneous language translation feature in skype; new silicon and postsilicon computing technologies; experimental undersea data centers; nextgeneration augmentedreality experiences for hololens and virtual reality devices; and largescale sociotechnological experiments such as xiaoice and tay.dr. lee joined microsoft in 2010 as distinguished scientist and managing director of the microsoft research redmond laboratory and later took on leadership of microsoft™s u.s.based research operations, comprising seven laboratories and more than 500 researchers, engineers, and support personnel. before joining microsoft, he held key positions in government and in academia. his most recent position was at the defense advanced research projects agency (darpa), where he founded and directed a major technology ofce that supported research in computing and related areas in the social and physical sciences. one of the highlights of his work at darpa was the darpa network challenge, which mobilized millions of people worldwide in a hunt for red weather balloonsša unique experiment in social media and open innovation that fundamentally altered how the department of defense thought about social networks. before darpa, dr. lee served as head of carnegie mellon university™s (cmu™s) computer science department, continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.74topranked in the nation. he also served as the university™s vice provost for research. at cmu, he carried out research in software reliability, program analysis, security, and language design. he is wellknown for his codevelopment of proofcarrying code techniques for enhanced software security and has tackled problems as diverse as programming for largescale modular robotics systems and shape analysis for c programs. dr. lee is a fellow of the association for computing machinery (acm) and serves the research community at the national level, including policy contributions to the president™s council of advisors on science and technology and membership of both the national academies of sciences, engineering, and medicine™s computer science and telecommunications board and the advisory council of the computer and information science and engineering directorate of the national science foundation (nsf). he was the former chair of the computing research association and has testied before both the u.s. house science and technology committee and the u.s. senate commerce committee. dr. lee holds a ph.d. in computer and communication sciences from the university of michigan, ann arbor, and bachelor™s degrees in mathematics and computer sciences, also from the university of michigan, ann arbor. mark dean is the john fisher distinguished professor at the university of tennessee (ut) college of engineering. his research focus is advanced computer architecture (beyond von neumann systems), datacentric computing, and computational sciences. before joining ut, dr. dean was chief technology ofcer, middle east and africa, for ibm and an ibm fellow. in this role he was responsible for technical strategy, technical skills development, and exploring new technologybased solutions for the region. these responsibilities include the development of solutions specic to the emerging needs of the businesses and cultures in industry segments such as mobile services (banking, health care, education, government), natural resource management (oil, gas, mining, forest, water), cloudbased business services, and security (fraud protection, risk management, privacy, cybersecurity). dr. dean was also vice president world wide strategy and operations for ibm research. in that role, he was responsible for setting the direction of ibm™s overall research strategy across eight worldwide labs and for leading the global operations and information systems teams. these responsibilities include management of the division™s business model, research strategy, hiring, university relations, internal/external recognition, personnel development, innovation initiatives and the division™s operations. during his career, dr. dean has developed all types of computer systems, from embedded systems to supercomputers, including testing of the rst gigahertz cmos microprocessor, and establishing the team that developed the blue gene supercomputer. he was also chief engineer for the development of the ibm pc/at, isa systems bus, ps/2 model 70 & 80, the color graphics adapter in the original ibm pc, and holds three of the nine continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.75patents for the original ibm pc. one inventionšthe industry standard architecture (isa) ﬁbus,ﬂ which permitted addon devices like the keyboard, disk drives and printers to be connected to the motherboardšwould earn election to the national inventors hall of fame for dr. dean and his colleague dennis moeller. dr. dean™s most recent awards include national institute of science outstanding scientist award, member of the american academy of arts and sciences, member of the national academy of engineering (nae), ieee fellow, black engineering of the year, the university of tennessee coe dougherty award, member of the national inventor™s hall of fame, and recipient of the ronald h. brown american innovators award. dr. dean received a b.s.e.e. degree from the university of tennessee in 1979, an m.s.e.e. degree from florida atlantic university in 1982, and a ph.d. in electrical engineering from stanford university in 1992. edward frank is cofounder and ceo of brilliant lime, inc., and cloud parity, inc., both social/mobile software rms. previously, dr. frank was a vice president at apple, inc., and corporate vice president for research and development at broadcom. before becoming corporate vice president of r&d, he cofounded and led the engineering group for broadcom™s wireless lan business, which is now one of broadcom™s largest business units. dr. frank joined broadcom in may 1999 following its acquisition of epigram, inc., where he was the founding ceo and executive vice president. from 1993 to 1996, he was a cofounder and vice president of engineering at netpower, inc., a computer workstation manufacturer. from 1988 to 1993, dr. frank was a distinguished engineer at sun microsystems, inc., where he coarchitected several generations of sun™s sparcstations and was a principal member of sun™s green project, which developed the precursor to the java(tm) crossplatform web programming language. dr. frank holds more than 40 issued patents. he is a university life trustee of cmu and a member of its board™s executive committee. he received a b.s.e.e. and an m.s.e.e. from stanford university and a ph.d. in computer science from cmu. yann lecun is director of articial intelligence research at facebook and silver professor of computer science at the courant institute of mathematical sciences. he is the founding director of the new york university (nyu) center for data science and holds appointments as professor of neural science with the center for neural science and professor of electrical and computer engineering with the ece department at nyu/poly. in 1987, dr. lecun joined geoff hinton™s group at the university of toronto as a research associate. he then joined the adaptive systems research department at at&t bell laboratories in holmdel, new jersey, in 1988. in 1991, he spent 6 months with the laboratoire central de recherche of thomsoncsf in orsay, france. upon his return to the united states, he rejoined bell labs. shortly after at&t™s second breakup in 1996, he became head of continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.76the image processing research department, part of larry rabiner™s speech and image processing research lab at at&t labsresearch in red bank, new jersey. in 2002, he became a fellow of the nec research institute in princeton. dr. lecun joined the courant institute of mathematical sciences at nyu as a professor of computer science in 2003. he was named silver professor in 2008. in 2013, he became the founding director of the nyu center for data science. dr. lecun has been associate editor of plos one, international journal of computer vision, ieee transactions on pattern analysis and machine intelligence, pattern recognition and applications, machine learning journal, and ieee transactions on neural networks. since 1997, he has served as general chair and organizer of the learning workshop, held every year since 1986 in snowbird, utah. he is also a member of the science advisory board of the institute for pure and applied mathematics, university of california, los angeles. dr. lecun has given numerous invited talks at various international conferences and workshops. he has published more than 180 technical papers and book chapters on machine learning, computer vision, robotics, pattern recognition, neural networks, handwriting recognition, image compression, document understanding, image processing, vlsi design, and information theory. his handwritingrecognition technology is used by several banks around the world, and his image compression technology, called djvu, is used by hundreds of websites and publishers and millions of users to access scanned documents on the web. an image recognition model he devised, convolutional network, is used by such companies as facebook, google, microsoft, nec, baidu, and at&t/ncr for products and services such as image recognition and tagging, document recognition, intelligent kiosk, and other applications. dr. lecun is the recipient of the 2014 ieee neural network pioneer award, awarded by the computational intelligence society. he received a diplôme d™ingénieur from the ecole superieure d™ingénieur en electrotechnique et electronique (esiee), paris, in 1983, a diplôme d™etudes approfondies (dea) from université pierre et marie curie, paris, in 1984, and a ph.d. in computer science from the same university in 1987.barbara liskov is an institute professor at the massachusetts institute of technology (mit). dr. liskov™s research interests include distributed systems, replication algorithms to provide faulttolerance, programming methodology, and programming languages. her current research projects include byzantinefaulttolerant storage systems and online storage systems that provide condentiality and integrity for the stored information. dr. liskov is a member of the nae, the national academy of sciences, and the national inventors hall of fame (inducted in 2012). she is a fellow of the american academy of arts and sciences and the acm and a charter fellow of the national academy of inventors. she received the acm turing award in 2009, the acm sigplan programming language achievement award in 2008, the ieee von neumann medal in 2004, and a continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.77lifetime achievement award from the society of women engineers in 1996. in 2003, dr. liskov was named one of the 50 most important women in science by discover magazine. dr. liskov received a b.a. in mathematics from university of california, berkeley, and m.s. and ph.d., both in computer science, from stanford university.elizabeth mynatt is the executive director of the institute for people and technology (ipat), a college of computing professor, and director of the everyday computing lab. themes in her research include supporting informal collaboration and awareness in ofce environments, enabling creative work and visual communication, and augmenting social processes for managing personal information. she is also one of the principal researchers in the aware home research initiative; investigating the design of future home technologies, especially those that enable older adults to continue living independently as opposed to moving to an institutional care setting. dr. mynatt is an internationally recognized expert in the areas of ubiquitous computing and assistive technologies. her research contributes to ongoing work in personal health informatics, computersupported collaborative work and humancomputer interface design. she is a member of the sigchi academy, a sloan and kavli research fellow, and serves on microsoft research™s technical advisory board. dr. mynatt is also a member of the computing community consortium, an nsfsponsored effort to engage the computing research community in envisioning more audacious research challenges. she has published more than 100 scientic papers and chaired the chi 2010 conference, the premier international conference in humanœcomputer interaction. before joining the georgia institute of technology faculty in 1998, she was a member of the research staff at xerox parc, working with the founder of ubiquitous computing, mark weiser. her research is supported by multiple grants from nsf, including a 5year nsf career award. other honorary awards include being named the top woman innovator in technology by atlanta woman magazine in 2005 and the 2003 college of computing™s dean™s award. dr. mynatt earned her b.s. (summa cum laude) in computer science from north carolina state university and her m.s. and ph.d. in computer science from georgia tech.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.78presentations introduction and welcome, peter lee, microsoft research, chairrobotics, automation, and the future of transportation, rodney brooks, rethink robotics usability, human factors, and social computingmoderator: beth mynatt, georgia institute of technologyfrom smallworld networks to computational social science, duncan watts, microsoft research there™s an app for that: how we got here and where to take it, scott hudson, carnegie mellon university history of wearables, thad starner, georgia institute of technology  computer architecture, hardware, and systemsmoderator: barbra liskov, massachusetts institute of technologycomputer architecture and the path of parallelism and power research, margaret martonosi, princeton universitythe crucial role of government funding for it, robert colwell, intel (retired) continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.79appendix bmachine learning and articial intelligencemoderator: peter lee, microsoft researchdata sciences = big data + machine learning + domain expertise, jaime carbonell, carnegie mellon university investments and outcomes in ai: paradigm shifts and a renaissance, eric horvitz, microsoft research  communicationsevolving the internet, vint cerf, google, inc.the once and future internet of everything, david culler, university of california, berkeley the wireless future: dreams and challenges (and how will this research impact technology), andrea goldsmith, stanford university cybersecurity research: stories from the trenches, stefan savage, university of california, san diego value of research funding for innovationapplication engaged research, deborah estrin, cornell tech unleashing the discovery and innovation ecosystem, farnam jahanian, carnegie mellon university continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.80presenter biographies rodney brooks is a robotics entrepreneur and founder, chairman, and cto of rethink robotics (formerly heartland robotics). he is also a founder, former board member (19902011), and former cto (19902008) of irobot corp. dr. brooks is the former director (19972007) of the massachusetts institute of technology (mit) articial intelligence laboratory and then the mit computer science and articial intelligence laboratory. he received degrees in pure mathematics from the flinders university of south australia and a ph.d. in computer science from stanford university in 1981. he held research positions at carnegie mellon university (cmu) and mit and a faculty position at stanford university before joining the faculty of mit in 1984. he has published many papers in computer vision, articial intelligence, robotics, and articial life. dr. brooks served for many years as a member of the international scientic advisory group of national information and communication technology australia and on the global innovation and technology advisory council of john deere & co. he is currently an xconomist at xconomy and a regular contributor to edge. since june 2014, he has been a member of the visiting committee on advanced technology at the national institute of standards and technology. dr. brooks is a member of the national academy of engineering (nae), a founding fellow of the association for the advancement of articial intelligence (aaai), a fellow of the american academy of arts and sciences, a fellow of the american association for the advancement of science (aaas), a fellow of the association for computing machinery (acm), a fellow of the institute of electrical and electronics engineers (ieee), a corresponding member of the australian academy of science, and a foreign fellow of the australian academy of technological sciences and engineering. among his awards are the following: the computers and thought award at the 1991 international joint conference on articial intelligence, continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.81appendix cthe ieee inaba technical award for innovation leading to production (2008), the robotics industry association™s engelberger robotics award for leadership (2014),and the 2015 ieee robotics and automation award. he has been the cray lecturer at the university of minnesota, the mellon lecturer at dartmouth college, and the forsythe lecturer at stanford university. he was cofounding editor of the international journal of computer vision and is a member of the editorial boards of various journals, including adaptive behavior, articial life, applied articial intelligence, autonomous robots, and new generation computing. he starred as himself in the 1997 errol morris movie ﬁfast, cheap and out of control,ﬂ named for one of his scientic papers. jaime carbonell is a university professor and the allan newell professor of computer science at cmu. dr. carbonell joined the cmu community as an assistant professor of computer science in 1979 and went on to become a widely recognized authority in machine translation, natural language processing, and machine learning. he has invented a number of wellknown algorithms and methods during his career, including proactive machine learning and maximal marginal relevance for information retrieval. his research has resulted in or contributed to a number of commercial enterprises, including carnegie speech, carnegie group, and dynamix technologies. in addition to his work on machine learning and translation, dr. carbonell also investigates computational proteomics and biolinguisticsšelds that take the computational tools used for analyzing language and adapt them to understanding biological information encoded in protein structures. this process leads to increased knowledge of proteinœprotein interactions and molecular signaling processes. his career has had an enormous impact on both cmu and the school of computer science. he created the university™s ph.d. program in language technologies and is cocreator of the universal library and its million book project. he founded cmu™s center for machine translation in 1986 and led its transformation in 1996 into the language technologies institute, which he currently directs. he has advised more than 40 ph.d. students and authored more than 300 research papers. before joining the cmu faculty, dr. carbonell earned bachelor™s degrees in mathematics and physics at mit and a master™s degree and a ph.d. in computer science at yale university.vinton g. cerf is vice president and chief internet evangelist for google, inc.. he is responsible for identifying new enabling technologies and applications on the internet and other platforms for the company. widely known as a ﬁfather of the internet,ﬂ dr. cerf is the codesigner, with robert kahn, of tcp/ip protocols and basic architecture of the internet. in 1997, president clinton recognized their work with the u.s. national medal of technology. in 2005, dr. cerf and dr. kahn received the highest civilian honor bestowed in the united states, the presidential medal of freedom. it recognizes the fact that their continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.82work on the software code used to transmit data across the internet has put them ﬁat the forefront of a digital revolution that has transformed global commerce, communication, and entertainment.ﬂ from 19942005, dr. cerf served as senior vice president at mci. prior to that, he was vice president of the corporation for national research initiatives (cnri), and from 198286 he served as vice president of mci. during his tenure with the defense advanced research projects agency (darpa) from 19761982, dr. cerf played a key role leading the development of internet and internetrelated data packet and security technologies. since 2000, he has served as chairman of the board of the internet corporation for assigned names and numbers (icann), and he has been a visiting scientist at the jet propulsion laboratory since 1998. he served as founding president of the internet society (isoc) from 19921995 and was on the isoc board until 2000. dr. cerf is a fellow of the ieee, acm, aaas, the american academy of arts and sciences, the international engineering consortium, the computer history museum, and the nae. dr. cerf has received numerous awards and commendations in connection with his work on the internet, including the marconi fellowship, charles stark draper award of the nae, the prince of asturias award for science and technology, the alexander graham bell award presented by the alexander graham bell association for the deaf, the a.m. turing award from the acm, the silver medal of the international telecommunications union, and the ieee alexander graham bell medal, among many others. he holds a ph.d. in computer science from the university of california, los angeles (ucla), and more than a dozen honorary degrees. robert p. ﬁbobﬂ colwell is an electrical engineer who worked at intel and was director of the microsystems technology ofce (mto) at darpa. he was the chief ia32 architect on the pentium pro, pentium ii, pentium iii, and pentium 4 microprocessors. dr. colwell retired from intel in 2000. he was an intel fellow from 1995 to 2000. he attended the university of pittsburgh and gained an undergraduate degree in electrical engineering. he later attended cmu to get a ph.d., also in electrical engineering. dr. colwell worked at a company called multi˚ow in the late 1980s as a design engineer. in 1990, he joined intel as a senior architect and was involved in the development of the p6 ﬁcore.ﬂ the p6 core was used in the pentium pro, pentium ii, and pentium iii microprocessors, and designs derived from it are used in the pentium m, core duo, and core solo, and core 2 microprocessors sold by intel. dr. colwell earned the acm eckertmauchly award in 2005 and wrote the ﬁat randomﬂ column for computer, a journal published by the ieee computer society. he is as well the author of several papers in addition to the pentium chronicles: the people, passion, and politics behind intel™s landmark chips. dr. colwell has spoken at universities on the challenges in chip design and management principles needed to tackle them.continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.83appendix cdavid culler is a professor of electrical engineering and computer sciences (eecs) at the university of california, berkeley. he received his b.a. from uc berkeley in 1980 and his m.s. and ph.d. degrees from mit in 1985 and 1989, respectively. he joined the eecs faculty in 1989 and is the founding director of intel research, uc berkeley, and was associate chair of the eecs department, 20102012, and chair from 2012 through june 30, 2014. he won the okawa prize in 2013. he is a member of the nae, an acm fellow, and an ieee fellow. he has been named one of scientic american™s top 50 researchers and the creator of one of mit™s technology review™s 10 technologies that will change the world. he was awarded the national science foundation (nsf) presidential young investigator and the presidential faculty fellowship. his research addresses networks of small, embedded wireless devices, planetaryscale internet services, parallel computer architecture, parallel programming languages, and highperformance communication. it includes tinyos, berkeley motes, planetlab, networks of workstations (now), internet services, active messages, splitc, and the threaded abstract machine (tam).deborah estrin is a professor of computer science at cornell tech in new york city and a professor of public health at weill cornell medical college. she is founder of the healthier lift hub and directs the small data lab at cornell tech. dr. estrin is also cofounder of the nonprot startup open mhealth. her current focus is on mobile health and small data, leveraging the pervasiveness of mobile devices and digital interactions for health and life management. previously, dr. estrin was on the ucla faculty, where she was the founding director of the nsf center for embedded networked sensing (cens), pioneering the development of mobile and wireless systems to collect and analyze realtime data about the physical world. her honors include the acm athena lecture (2006) and the anita borg institute™s women of vision award for innovation (2007). she is a member of the american academy of arts and sciences and the nae.andrea goldsmith is the stephen harris professor in the school of engineering and a professor of electrical engineering at stanford university. she was previously on the faculty of electrical engineering at caltech. she cofounded and serves as chief scientist of accelera, inc., which develops softwaredened wireless network technology, and previously cofounded and served as cto of quantenna communications, inc., which develops highperformance wifi chipsets. she previously held positions at maxim technologies, memorylink corporation, and at&t bell laboratories. dr. goldsmith is a fellow of the ieee and of stanford university, and she has received several awards for her work, including the ieee communications society and information theory society joint paper award, the ieee communications society best tutorial paper award, the nae gilbreth lecture award, the ieee comsoc communications theory technical achievement award, the ieee comsoc continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.84wireless communications technical achievement award, the alfred p. sloan fellowship, and the silicon valley/san jose business journal™s women of in˚uence award. she is author of the book wireless communications and coauthor of the books mimo wireless communications and principles of cognitive radio, all published by cambridge university press. her research includes work on wireless information and communication theory, cognitive radios, sensor networks, ﬁgreenﬂ wireless system design, control systems closed over wireless networks, smart grid sensing and control, and applications of communications and signal processing to biology and neuroscience. she received b.s., m.s., and ph.d. degrees in electrical engineering from uc berkeley. dr. goldsmith is currently on the steering committee for the ieee transactions on wireless communications and previously served as editor for the ieee transactions on information theory, the journal on foundations and trends in communications and information theory and in networks, ieee transactions on communications, and ieee wireless communications. dr. goldsmith participates actively in committees and conference organization for the ieee information theory and communications societies and has served on the board of governors for both societies. she has been a distinguished lecturer for both societies, served as the president of the ieee information theory society in 2009, founded and chaired the student committee of the ieee information theory society, and currently chairs the emerging technology committee and is a member of the strategic planning committee in the ieee communications society. at stanford, she received the inaugural university postdoc mentoring award and has been active in committees to innovate and revise both graduate and undergraduate education universitywide. she served as chair of stanford™s faculty senate in 2009 and currently serves on its faculty senate and on its budget group.eric horvitz is a distinguished scientist at microsoft research. his interests span theoretical and practical challenges with developing systems that perceive, learn, and reason. his contributions include advances in principles and applications of machine learning and inference, search and retrieval, humanœcomputer interaction, bioinformatics, and ecommerce. he has been elected a fellow of the aaai and of the aaas. he currently serves on the nsf computer and information science and engineering (cise) advisory board and on the council of the computing community consortium. he received his ph.d. and m.d. degrees at stanford university.farnam jahanian serves as vice president for research at cmu. he brings to cmu extensive leadership and administrative expertise, not only in supporting and nurturing research within and across disciplines, but also in translating research into innovative tools and technologies that succeed in the marketplace. the ofce of the vice president for research at cmu is responsible for nurturing excellence in research, scholarship, continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.85appendix cand creative activities across the entire campus. it has overall responsibility for research administration and policy, provides oversight for responsible conduct of research and compliance, and focuses on facilitating and accelerating the movement of research and technology from the university to the marketplace. the ofce of sponsored programs, ofce of research integrity and compliance, center for technology transfer, and ofce of government relations, and the software engineering institute, among others, report to the vice president for research. before cmu, dr. jahanian led the nsf directorate for cise from 2011 to 2014. he was on the faculty at the university of michigan from 1993 to 2014, where he held the edward s. davidson collegiate professorship in the college of engineering and served as chair for computer science and engineering from 2007 to 2011 and as director of the software systems laboratory from 1997 to 2000. previously, he held research and management positions at the ibm t.j. watson research center. while at the university of michigan, dr. jahanian led several largescale research projects that studied the growth and scalability of the internet infrastructure, which ultimately transformed how cyberthreats are addressed by internet service providers. his research on internet infrastructure security formed the basis for the successful internet security services company arbor networks, which he cofounded in 2001. dr. jahanian served as chairman of arbor networks until its acquisition in 2010. he has been an active advocate for how basic research can be uniquely central to an innovation ecosystem that drives global competitiveness and addresses national priorities. he received numerous awards for his innovative research, commitment to education, and technology commercialization activities. he was named distinguished university innovator at the university of michigan in 2009 and received the governor™s university award for commercialization excellence in 2005. dr. jahanian holds a master™s degree and a ph.d. in computer science from the university of texas, austin. he is a fellow of the acm, ieee, and the aaas. margaret martonosi is the hugh trumbull adams ‚35 professor of computer science at princeton university, where she has been on the faculty since 1994. she also holds an afliated faculty appointment in princeton™s department of electrical engineering. from 2005 to 2007, she served as associate dean for academic affairs for the princeton university school of engineering and applied science. in 2011, she served as acting director of princeton™s center for information technology policy. dr. martonosi™s research interests are in computer architecture and mobile computing, with particular focus on powerefcient systems. her work has included the development of the wattch power modeling tool and the princeton zebranet mobile sensor network project for the design and realworld deployment of zebra tracking collars in kenya. her current research focuses on hardwareœsoftware interface approaches to manage heterogeneous parallelism and powerperformance tradeoffs in systems ranging from smartphones to chip multiprocescontinuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.86sors to largescale data centers. dr. martonosi is a fellow of both the ieee and the acm. she was the 2013 recipient of the anita borg institute technical leadership award. she has also received the 2013 ncwit undergraduate research mentoring award and the 2010 princeton university graduate mentoring award. in addition to having authored many archival publications, dr. martonosi is an inventor on six granted u.s. patents and has coauthored a technical reference book on poweraware computer architecture. she serves on the board of directors of the computing research association. dr. martonosi completed her ph.d. at stanford university and also holds a master™s degree from stanford and a bachelor™s degree from cornell university, all in electrical engineering. stefan savage is a professor at the department of computer science and engineering at university of california, san diego (ucsd). dr. savage™s research interests lie at the intersection of distributed systems, networking, and computer security, with a current focus on embedded security and the economics of cybercrime. he currently serves as director of ucsd™s center for network systems and as codirector for the cooperative center for internet epidemiology and defenses, a joint effort between ucsd and the international computer science institute. dr. savage received his ph.d. in computer science and engineering from the university of washington and a bachelor™s in applied history from cmu. thad starner is a wearable computing pioneer. he is a professor in the school of interactive computing at the georgia institute of technology and a technical lead on google glass. he has been wearing a computer with a headup display as part of his daily life since 1993, perhaps the longest such experience known. besides glass, his projects include a wireless glove that teaches how to play piano melodies without active attention by the wearer; a game for deaf children using sign language recognition that helps them acquire language skills; creating wearable computers to enable twoway communication experiments with wild dolphins; making wearable computers for working dogs to better communicate with their handlers; recovering phraselevel sign language from brain signals; and recognizing speech without vocalizing. dr. starner is a founder of the annual acm/ieee international symposium on wearable computers, now in its 18th year, and has produced more than 400 papers and presentations on his work. he is an inventor on more than 60 u.s. patents awarded or in process.duncan watts is a principal researcher at microsoft research and a founding member of the msr new york city laboratory. he is also an a.d. white professor at large at cornell university as well as a visiting fellow at columbia university and at nufeld college, oxford. before joining msr in 2012, he was, from 2000 to 2007, a professor of sociology continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.87appendix cat columbia university and then a principal research scientist at yahoo! research, where he directed the human social dynamics group. his research on social networks and collective dynamics has appeared in a wide range of journals, including nature, science, physical review letters, the american journal of sociology, and harvard business review. he has been recognized by the 2009 german physical society young scientist award for socio and econophysics, the 2013 lagrangecrt foundation prize for complexity science, and the 2014 everett rogers prize. he is the author of three books: six degrees: the science of a connected age, small worlds: the dynamics of networks between order and randomness, and everything is obvious: once you know the answer. he holds a b.sc. in physics from the australian defence force academy, from which he also received his ofcer™s commission in the royal australian navy, and a ph.d. in theoretical and applied mechanics from cornell university.  continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.the national academy of sciences was established in 1863 by an act of congress, signed by president lincoln, as a private, nongovernmental institution to advise the nation on issues related to science and technology. members are elected by their peers for outstanding contributions to research. dr. ralph j. cicerone is president.the national academy of engineering was established in 1964 under the charter of the national academy of sciences to bring the practices of engineering to advising the nation. members are elected by their peers for extraordinary contributions to engineering. dr. c. d. mote, jr., is president.the national academy of medicine (formerly the institute of medicine) was established in 1970 under the charter of the national academy of  sciences to advise the nation on medical and health issues. members are elected by their peers for distinguished contributions to medicine and health. dr. victor j. dzau is president.the three academies work together as the national academies of sciences, engineering, and medicine to provide independent, objective analysis and advice to the nation and conduct other activities to solve complex problems and inform public policy decisions. the academies also encourage education and research, recognize outstanding contributions to knowledge, and increase public understanding in matters of science, engineering, and medicine. learn more about the national academies of sciences, engineering, and medicine at www.nationalacademies.org. continuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved. other recent reports of the computer science and telecommunications boardtelecommunications research and engineering at the institute for telecommunication sciences of the department of commerce: meeting the nation™s telecommunications needs (2015)telecommunications research and engineering at the communications technology laboratory of the department of commerce: meeting the nation™s telecommunications needs (2015)a review of the next generation air transportation system: implications and importance of system architecture (2015)bulk collection of signals intelligence: technical options (2015)future directions for nsf advanced computing infrastructure to support u.s. science and engineering in 20172020: an interim report (2014)at the nexus of cybersecurity and public policy: some basic concepts and issues (2014)geotargeted alerts and warnings: report of a workshop on current knowledge and research gaps (2013)professionalizing the nation™s cybersecurity workforce? criteria for future decisionmaking (2013)public response to alerts and warnings using social media: summary of a workshop on current knowledge and research gaps (2013)continuing innovation in information technology (2012)computing research for sustainability (2012)the future of computing performance: game over or next level? (2011)wireless technology prospects and policy options (2011)limited copies of cstb reports are available free of charge fromcomputer science and telecommunications boardnational academies of sciences, engineering, and medicinekeck center of the national academies500 fifth street, nw, washington, dc 20001(202) 3342605/cstb@nas.eduwww.cstb.orgcontinuing innovation in information technology: workshop reportcopyright national academy of sciences. all rights reserved.20102005200019951990198519801975197019652010200520001995199019851980197519701965universityindustry r&dproducts$1 billion market$10 billion marketareas of fundamental research in itit sectors with large economic impactcomputer architecturesoftware technologiesnetworkingparallel & distributed systemsdatabasescomputer graphicsai & roboticsdigital communicationsinternet & webcloudcomputingentertainment& designenterprisesystemsrobotics & assistivetechnologiespersonalcomputingmicroprocessorsbroadband& mobilemotorolaqualcommiphonenvidiaamd inteltexas instrumentshpappledellsymantecebay akamaihpjuniperciscoyahoo!ibmelectronic artsfacebook twittergooglemicrosoftvmwareamazonoracleadobe autodeskipodnvidia pixarxboxnuanceintuitive surgicalirobot20102005200019951990198519801975197019652010200520001995199019851980197519701965universityindustry r&dproducts$1 billion market$10 billion marketareas of fundamental research in itit sectors with large economic impactcomputer architecturesoftware technologiesnetworkingparallel & distributed systemsdatabasescomputer graphicsai & roboticsdigital communicationsinternet & webcloudcomputingentertainment& designenterprisesystemsrobotics & assistivetechnologiespersonalcomputingmicroprocessorsbroadband& mobilemotorolaqualcommiphonenvidiaamd inteltexas instrumentshpappledellsymantecebay akamaihpjuniperciscoyahoo!ibmelectronic artsfacebook twittergooglemicrosoftvmwareamazonoracleadobe autodeskipodnvidia pixarxboxnuanceintuitive surgicalirobot
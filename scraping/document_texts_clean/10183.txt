detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/10183looking over the fence at networks: a neighbor's view ofnetworking research36 pages | 8.5 x 11 | paperbackisbn 9780309076135 | doi 10.17226/10183committee on research horizons in networking, computer science andtelecommunications board, national research councillooking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.looking over the fence at networksa neighbor™s view of networking researchcommittee on research horizons in networkingcomputer science and telecommunications boarddivision on engineering and physical sciencesnational research councilnational academy presswashington, d.c.looking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.notice: the project that is the subject of this report was approved by the governing board of thenational research council, whose members are drawn from the councils of the national academy ofsciences, the national academy of engineering, and the institute of medicine. the members of thecommittee responsible for the report were chosen for their special competences and with regard forappropriate balance.support for this project was provided by core funds of the computer science and telecommunicationsboard. core support for the cstb is provided by its public and private sponsors: the air force office ofscientific research, defense advanced research projects agency, department of energy, nationalaeronautics and space administration, national institute of standards and technology, national libraryof medicine, national science foundation, office of naval research, at&t, hewlettpackard, intel,microsoft, and timewarner cable.international standard book number 0309076137additional copies of this report are available from:national academy press2101 constitution ave., n.w.box 285washington, dc 2081480062462422023343313 (in the washington metropolitan area)http://www.nap.educopyright 2001 by the national academy of sciences. all rights reserved.printed in the united states of americalooking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.the national academy of sciences is a private, nonprofit, selfperpetuating society of distinguishedscholars engaged in scientific and engineering research, dedicated to the furtherance of science andtechnology and to their use for the general welfare. upon the authority of the charter granted to it by thecongress in 1863, the academy has a mandate that requires it to advise the federal government onscientific and technical matters. dr. bruce m. alberts is president of the national academy of sciences.the national academy of engineering was established in 1964, under the charter of the nationalacademy of sciences, as a parallel organization of outstanding engineers. it is autonomous in itsadministration and in the selection of its members, sharing with the national academy of sciences theresponsibility for advising the federal government. the national academy of engineering also sponsorsengineering programs aimed at meeting national needs, encourages education and research, and recognizesthe superior achievements of engineers. dr. wm. a. wulf is president of the national academy ofengineering.the institute of medicine was established in 1970 by the national academy of sciences to secure theservices of eminent members of appropriate professions in the examination of policy matters pertaining tothe health of the public. the institute acts under the responsibility given to the national academy ofsciences by its congressional charter to be an adviser to the federal government and, upon its owninitiative, to identify issues of medical care, research, and education. dr. kenneth i. shine is president ofthe institute of medicine.the national research council was organized by the national academy of sciences in 1916 to associatethe broad community of science and technology with the academy™s purposes of furthering knowledge andadvising the federal government. functioning in accordance with general policies determined by theacademy, the council has become the principal operating agency of both the national academy ofsciences and the national academy of engineering in providing services to the government, the public,and the scientific and engineering communities. the council is administered jointly by both academiesand the institute of medicine. dr. bruce m. alberts and dr. wm. a. wulf are chairman and vice chairman,respectively, of the national research council.looking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.looking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.vcommittee on research horizons in networkingdavid a. patterson, university of california at berkeley, chairdavid d. clark, massachusetts institute of technologyanna karlin, university of washingtonjim kurose, university of massachusetts at amherstedward d. lazowska, university of washingtondavid liddle, u.s. venture partnersderek mcauley, marconivern paxson, at&t center for internet research at icsistefan savage, university of california at san diegoellen w. zegura, georgia institute of technologystaffjon eisenberg, senior program officermarjory s. blumenthal, executive directorjanet briscoe, administrative officermargaret huynh, senior project assistantdavid padgham, research assistantlooking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.vicomputer science and telecommunications boarddavid d. clark, massachusetts institute of technology, chairdavid borth, motorola labsjames chiddix, aol time warnerjohn m. cioffi, stanford universityelaine cohen, university of utahw. bruce croft, university of massachusetts at amherstsusan l. graham, university of california at berkeleyjudith hempel, university of california at san franciscojeffrey m. jaffe, bell laboratories, lucent technologiesanna karlin, university of washingtonmichael katz, university of california at berkeleybutler w. lampson, microsoft corporationedward d. lazowska, university of washingtondavid liddle, u.s. venture partnerstom m. mitchell, whizbang! labs inc.donald norman, unext.comdavid a. patterson, university of california at berkeleyhenry (hank) perritt, chicagokent college of lawburton smith, cray inc.terry smith, university of california at santa barbaralee sproull, new york universitymarjory s. blumenthal, executive directorherbert s. lin, senior scientistalan s. inouye, senior program officerjon eisenberg, senior program officerlynette i. millett, program officercynthia patterson, program officerjanet briscoe, administrative officermargaret huynh, senior project assistantsuzanne ossa, senior project assistantdavid drake, project assistantdavid padgham, research assistantbrandye williams, office assistantlooking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.viiprefacethis report is the result of a new approach by the computer science andtelecommunications board (cstb) of the national research council to developing researchagendas in key areas of information technology. typically, only the members of a particularresearch community participate in defining an agenda for their future research activities. cstbconvened a small workshop in which more than half of the attendees were researchers in otherfields. the premise behind this approach was that working together with a smaller number ofnetwork research insiders, these outsidersšpeople whose primary research interests were not innetworking but who represented instead additional subdisciplines of computer science, as well asother disciplines such as earth science, economics, and information studiesšwould functionmuch like a visiting committee, providing a fresh perspective on research topics and directionsand helping to stimulate development of a strategic view of future research directions. cstbpicked networking as the subject of this boardinitiated projectšthe first in a planned series ofworkshopsšsince it is a field that has enjoyed both success and great attention due to its mostvisible creation, the internet. as this report illustrates, it is also a compelling field in which toexplore alternative research visions because that very success has constrained some avenues ofresearch.the presence of outsiders was critical to the dialogue at the january 2001 workshop. tokick off the discussion, both insiders and outsiders were asked to identify nearterm and longterm research topics, as well as topics that should probably be deemphasized, at least for a while(box p.1). in the discussions that followed, outsiders posed provocative questions thatchallenged conventional wisdom and suggested different research approaches drawn from theirown research communities. they also brought the perspectives of experienced network users tothe discussion and repeatedly expressed frustrations about the inadequacies of the internet fromtheir user perspective.workshop participants noted but did not elaborate on a number of topics that are ofcurrent interest in the network community. in some cases, topics were explicitly taken off thetable. a shared sense among outsiders and insiders that topics other than network performancewere at least as important and receiving less attention in the research community meant thatworkshop participants paid little attention to the issue of how to build higherspeed networks.limitations of time and expertise precluded an indepth examination of the implications ofwireless and optical technologies, but participants did observe that such examination would be animportant activity for the research community. in other cases, subjects arose in discussions butwere not ultimately identified as areas meriting greater attention by networking researchers (e.g.,lastmile access links). other topics provoked mixed reactions; for example, some felt thatmulticast continues to be important while others felt that it should be abandoned as a researchtopic.the workshop proved educational for everyone involved. the outsiders learned aboutsome surprising characteristics of networking culture. for example, the research community isprotective of the internet; reviewers often reject papers that make proposals perceived aspotentially deleterious to the internet. also, even when confidentiality is not at issue, networkresearchers are reluctant to identify by brand name specific products or services with featuresthey find undesirable. the insiders, in turn, were surprised to hear that the outsiders were notvery interested in seeing great efforts expended on more research to improve raw networkperformance (distinct from work on better characterizing network performance, which was ofinterest to some participants). there were other surprises: for example, outsiders were surprisedby how mistakes made by a few people in the configuration of routing tables could bring asignificant portion of the internet to its knees.looking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.viiibox p.1 questions posed in advance to workshop participants1. what are three pressing problems in networking (that is, shortterm problems that ideally would havebeen research problems 5 to 7 years ago)?2. what are two fundamental research problems in networking (that is, things that would be important to putinto practice in 5 to 7 years)?3. what is one topic in networking that you would rather not read about again (that is, a topic that could bedeferred to allow work on other problems)?this report does not provide answers to these specific questionsšthe questions were posed as away of stimulating discussions at the workshop.the report that follows was written by the committee on research horizons innetworking, composed of six networking researchers and four researchers from other areas incomputer science, based on the 2 days of discussions among a larger group of workshopparticipants that was dominated by outsiders. the committee met immediately following theworkshop and conducted a series of discussions by email to formulate a fresh look at networkingresearch, drawing on the workshop experience.the report is organized around the three major themes, closely connected to the processof networking research, that emerged at the workshopšmeasuring, modeling, and creating anddeploying disruptive prototypes. it is not a report that seeks to lay out a detailed research agendaper se. the issues raised in this report, which reflect in large part the concerns of the outsiders,would certainly require further consideration by the network research community to be translatedinto an actual research agenda that would help meet the needs of network users. for example,while outsiders bring a valuable fresh perspective, they can also miss obstacles that insiders see.the intent of this report is to stimulate such an examination.david a. patterson, chaircommittee on research horizonsin networkinglooking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.ixacknowledgment of reviewersthis report has been reviewed in draft form by individuals chosen for their diverseperspectives and technical expertise, in accordance with procedures approved by the nrc™sreport review committee. the purpose of this independent review is to provide candid andcritical comments that will assist the institution in making its published report as sound aspossible and to ensure that the report meets institutional standards for objectivity, evidence, andresponsiveness to the study charge. the review comments and draft manuscript remainconfidential to protect the integrity of the deliberative process. we wish to thank the followingindividuals for their review of this report:craig partridge, bbn technologies,larry peterson, princeton university,scott shenker, at&t center for internet research at icsi, andjames p.g. sterbenz, bbn technologies.although the reviewers listed above provided many constructive comments and suggestions, theywere not asked to endorse the conclusions or recommendations nor did they see the final draft ofthe report before its release. the review of this report was overseen by jerome h. saltzer,massachusetts institute of technology, appointed by the nrc™s division on engineering andphysical sciences, who was responsible for making certain that an independent examination ofthis report was carried out in accordance with institutional procedures and that all reviewcomments were carefully considered. responsibility for the final content of this report restsentirely with the authoring committee and the institution.looking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.xlooking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.xicontents1introduction...................................................................................................................12measuring: understanding the internet artifact..................................................3the challenges of scale......................................................................................................4measurement infrastructure.................................................................................................5nontechnical factors..........................................................................................................53modeling: new theory for networking....................................................................7performance........................................................................................................................7theory: beyond performance.............................................................................................7applying theoretical techniques to networking.................................................................84making disruptive prototypes: another approach to stimulating research.....9challenges in deploying disruptive technology.................................................................9external drivers................................................................................................................125concluding observations...........................................................................................13appendixesa biographies of committee members...........................................................................17b list of workshop participants...................................................................................21looking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.looking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.looking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.looking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.11 introductionthe internet has been highly successful in meeting the original vision of providingubiquitous computertocomputer interaction in the face of heterogeneous underlyingtechnologies. no longer a research plaything, the internet is widely used for production systemsand has a very large installed base. commercial interests play a major role in shaping its ongoingdevelopment. success, however, has been a doubleedged sword, for with it has come the dangerof ossification, or inability to change, in multiple dimensions:· intellectual ossificationšthe pressure for compatibility with the current internet risksstifling innovative intellectual thinking. for example, the frequently imposed requirement thatnew protocols not compete unfairly with tcpbased traffic constrains the development ofalternatives for cooperative resource sharing. would a paper on the netblt protocol thatproposed an alternative approach to control called ﬁratebasedﬂ (in place of ﬁwindowbasedﬂ) beaccepted for publication today?· infrastructure ossificationšthe ability of researchers to affect what is deployed in thecore infrastructure (which is operated mainly by businesses) is extremely limited. for example,pervasive networklayer multicast remains unrealized, despite considerable research and efforts totransfer that research to products.1· system ossificationšlimitations in the current architecture have led to shoehornsolutions that increase the fragility of the system. for example, network address translationviolates architectural assumptions about the semantics of addresses. the problem is exacerbatedbecause a research result is often judged by how hard it will be to deploy in the internet, and theinternet service providers sometimes favor more easily deployed approaches that may not bedesirable solutions for the long run.at the same time, the demands of users and the realities of commercial interests present anew set of challenges that may very well require a fresh approach. the internet vision of the last20 years has been to have all computers communicate. the ability to hide the details of theheterogeneous underlying technologies is acknowledged to be a great strength of the design, but italso creates problems because the performance variability associated with underlying networkcapacity, timevarying loads, and the like means that applications work in some circumstancesbut not others. more generally, outsiders advocated a more usercentric view of networkingresearchša perspective that resonated with a number of the networking insiders as well.drawing on their own experiences, insiders commented that users are likely to be less interestedin advancing the frontiers of high communications bandwidth and more interested in consistencyand quality of experience, broadly defined to include the ﬁilitiesﬂšreliability, manageability,configurability, predictability, and so forthšas well as nonperformancebased concerns such assecurity and privacy. (interest was also expressed in higherperformance, broadband lastmileaccess, but this is more of a deployment issue than a research problem.) outsiders also observedthat while as a group they may share some common requirements, users are very diversešinexperience, expertise, and what they wish the network could do. also, commercial interests havegiven rise to more diverse roles and complex relationships that cannot be ignored whendeveloping solutions to current and future networking problems. these considerations argue thata vision for the future internet should be to provide users the quality of experience they seek andto accommodate a diversity of interests. 1other instances of infrastructure ossification noted by networking researchers include challengesassociated with deploying various flavors of quality of service and ipv6.looking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.2looking over the fence at networksthis report explores how networking research could overcome the evident obstacles tohelp achieve this vision for the future and otherwise better understand and improve the internet.the report, which reflects interactions among networking researchers and outsiders (researchersfrom fields other than networking) at cstb™s january 2001 workshop, as well as subsequentdiscussion by the committee on research horizons in networking, stresses looking beyond thecurrent internet and evolutionary modifications thereof and aims to stimulate fresh thinkingwithin the networking research community. since it is not a formal research agenda (whichwould, among other things, entail a much more intensive effort than is afforded by an exploratoryworkshop such as this), the report does not, for example, review past literature and currentresearch programs but instead briefly characterizes past progress, current efforts, and promisingdirections. it focuses on three key areas in which networking research might be invigorated:measuring the internet, modeling the internet, and making disruptive prototypes.looking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.32 measuring: understanding the internet artifacta remarkable creation, the internet encompasses a diversity of networks, technologies,and organizations. the enormous volume and great variety of data carried over it give it a richcomplexity and texture. it has proved difficult to characterize, understand, or model in terms oflargescale behaviors and a detailed understanding of traffic behavior. moreover, because it isvery difficult to prototype new networksšor even new networking ideasšon an interesting scale(see chapter 4), datadriven analysis and simulation are vital tools for evaluating proposedadditions and changes to its design.experimental science is an important approach in many areas of computer science andengineering, especially where the artifacts being studied are complex and have properties that arenot well understood.1 central to the experimental method is the repeated measurement ofobserved behavior. without acquiring such data it is impossible to analyze and understand theunderlying processes, let alone predict the impact of a change to the environment being observed.further, data often help suggest new theoretical approaches. measurement is at least in partdriven by a particular question at hand, and changing questions over time may well lead todifferent measurement needs.however, there are also strong arguments for collecting data in anticipation of future use.citing the heavy dependence of our knowledge and understanding of global climate change on arecord of atmospheric carbon dioxide measurements that charles david keeling started onmauna loa in 1957, workshop participant jeff dozier observed that ﬁgood data outlives badtheory.ﬂ2 hence a data set with typical days from the next 10 years of the internet might be atreasure chest for networking researchers just as the carbon dioxide record has been to earthscientists. also, outsiders at the workshop observed that in other areas of computer science, olderversions of artifactsšold microprocessors, operating systems, and the likešare important asbases for trend analysis and before/after comparisons of the impacts of new approaches.3archived internet snapshots could provide an analogous baseline for evaluating the largescaleimpact of both evolutionary and revolutionary changes in the internet. archived data could alsobe used by internet researchers to determine if newly identified traffic phenomena (for example, afuture equivalent of heavytailed behavior) existed in earlier instantiations of the internet.unfortunately, the ability of network researchers or operators to measure the internet issignificantly limited by a number of interdependent barriers. the extreme scale of today™sinternet poses a challenge to acquiring a representative set of data points. the internetarchitecture itself also makes measurement difficult. factors such as the endtoend design, 1for more discussion of this issue, see computer science and telecommunications board, nationalresearch council. 1994. academic careers for experimental computer scientists. national academypress, washington, d.c.2established initially with nsf support, the measurement program in the 1970s required cobblingtogether support from multiple sources and continuation of the measurement program was at risk. todaythe carbon dioxide record is one of science™s most famous data sets, and an illustration of the principle thatﬁgood data can outlast bad theory.ﬂ the measurements that keeling started, and maintained under someadversity, are the cornerstone of our analysis of the human effects on our climate. the data show thatatmospheric co2 has increased about 1/2 percent per year since 1957. measurements in other locationsshow a interhemispheric transport and seasonal variability. comparison with data from ice cores allows usto extend the record backward more than 100,000 years. see charles d. keeling. 1998. ﬁrewards andpenalties of monitoring the earth,ﬂ annual review of energy and the environment, vol. 23, pp. 2582.3the importance of archiving artifacts of complex software systems was discussed earlier in computerscience and telecommunications board, national research council. 1989. scaling up. nationalacademy press, washington, d.c.looking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.4looking over the fence at networkslayering, and the statelessness of the basic datagram4 make it hard to identify some types offlows. factors such as routing asymmetry and multipathing make it hard to gather necessaryinformation even about selfdescribing flows such as tcp. also, business concerns andincreased sensitivity to privacy limit the willingness of many stakeholders to participate in datacollection and constrain the release of data to a wider research community. the resulting paucityof sound or representative data has severely limited the ability to predict the effects of evenincremental changes to the internet architecture, and it has undermined confidence in moreforwardthinking research.progress in measuring the internet artifact will thus require the effort, ingenuity, andunified support of the networking community. in other fields, grand challengesšsuch asmapping the entire human genomešhave served to expose and crystallize research issues and tomobilize research efforts. along those lines, a challenge that could stimulate the necessaryconcerted effort is the following: (1) to develop and deploy the technology to make it possible torecord a day in the life of the internet, a data set containing the complete traffic, topology, andstate across the internet infrastructure and (2) to take such a snapshot. even if the goal wererealized only in part, doing so would provide the infrastructure for establishing a measurementbaseline.a ﬁday in the lifeﬂ should be understood as a metaphor for a more precise formulation ofthe measurement challenge. for example, the appropriate measurement period might not literallybe a single 24hour period (one might want to take measurements across a number of days toexplore differences between weekdays and weekends, the effects of events that increase networktraffic, and the like) and, as discussed below, the snapshot might sample traffic rather than recordevery single packet. to achieve many of the goals, one would also measure on an ongoing basisrather than as a onetime event.this ambitious goal faces many hurdles that together form the foundation for a valuableresearch agenda in their own right. although the overarching goal is the ability to collect a fullsnapshot, progress on each of the underlying problems discussed below would be a valuable stepforward toward improving our understanding of actual network behavior.the challenges of scaleaccommodating the growth in link speeds and topology is a significant challenge forlargescale internet traffic measurement. early versions of equipment with oc768 links (40gigabits per second) are already in trials, and the future promises higher speeds still. worse yet,each individual router may have many links, increasing the overall computational challenge aswell as making perlink measurement platforms extremely expensive to deploy and difficult tomanage. addressing these problems presents both engineering and theoretical challenges. highspeed links demand new measurement apparatus to measure their behavior, and efficientmeasurement capabilities must be incorporated into the routers and switches themselves toaccommodate high port densities. even with such advances it may be infeasible to collect acomplete record of all communication in a highly loaded router, and we may be forced to sampletraffic instead. to do so effectively will require developing a deeper understanding of how tosoundly sample network traffic, which is highly correlated and structured. an especiallyimportant statistics question is how to assess the validity of a particular sampling approachšitsaccuracy, representativeness, and limitationsšfor characterizing a range of network behaviors.one particular challenge in measuring the network today is incomplete knowledge aboutthe internal configuration of parts of the network, a reflection of network operators™ reluctance todivulge information that may be of interest to their competitors. one way to cope with thisimpediment is the use of inference techniques that allow one to learn more about a network based 4for further discussion of these design issues, see computer science and telecommunications board,national research council. 2001. the internet™s coming of age. national academy press, washington,d.c.looking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.measuring: understanding the internet artifact5on incomplete, publicly accessible/observable information. for example, there has been researchusing border gateway protocol (bgp) routing table information to infer the nature ofinterconnection agreements between internet service providers (isps). inference techniques willnot, in general, provide complete information, and more work is needed on how to make use ofsuch incomplete information. workshop participants noted that these statistical issues (alongwith the modeling issues discussed in the next chapter) would benefit from the involvement ofstatisticians.a snapshot of an internet day would contain an immense amount of data. like otherscientific communities faced with enormous data sets (for example, astronomy or the earthsciences), the internet research community must grapple with analyzing data at very large scales.among these challenges are effectively mining large, heterogeneous, and geographicallydistributed datasets; tracking the pedigree of derived data; visualizing intricate, highdimensionalstructures; and validating the consistency of interdependent data. an additional challenge posedby measuring internet traffic, which is also found in some other disciplines such as highenergyphysics, is that data arrive quickly, so decisions about data sampling and reduction have to bemade in real time.measurement infrastructurein addition to the significant theoretical challenges, largescale measurement of theinternet presents enormous deployment and operational challenges. to provide widespreadvantage points for measuring network activity, even a minimal infrastructure will comprisehundreds of measurement devices. there is some hope that advances in remote managementtechnologies will support this need, and lessons from several currently deployed pilotmeasurement projects could aid in the design of any such system. however, such an effort wouldalso requires funding and personnel able to deploy, maintain, and manage the largescaleinfrastructure envisioned here. in the long run, the value of this investment will be the creation ofa foundation for watching network trends over time and establishment of an infrastructureavailable to researchers for new questions that are not adequately addressed by previousmeasurements.many of the challenges found in measuring today™s internet could have been alleviatedby improved design, which underscores the importance of incorporating selfmeasurement,analysis, and diagnosis as basic design points of future system elements and protocols. this isparticularly critical to providing insight into failures that are masked by higher layers ofabstraction, as tcp does by intentionally hiding information about packet loss from applications.nontechnical factorsalthough many of the challenges to effective internet measurement are technical, thereare important nontechnical factorsšboth within the networking community and in the broadersocietal contextšthat must be addressed as well. the committee recognizes that gathering thisdata will require overcoming very significant barriers. one set of constraints arises because theinternet is composed in large part of production commercial systems. information on trafficpatterns or details of an isp™s network topology may reveal information that a provider prefersnot to reveal to its competitors or may expose design or operational shortcomings. a related setof challenges concerns expectations of privacy and confidentiality. users have an expectation(and in some instances a legal right) that no one will eavesdrop on their communications. as aconsequence of the decentralized nature of the internet, much of the data can only be directlyobserved with the cooperation of the constituent networks and enterprises. however, before theseorganizations are willing to share their data, one must address their concerns about protectingtheir users™ privacy. users will be concerned even if the content of their communications is notlooking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.6looking over the fence at networksbeing capturedšrecording just the source, destination, type, or volume of the communicationscan reveal information that a user would prefer to keep private.if network providers could find ways of being more open while protecting legitimateproprietary or privacy concerns, considerably more data could be available for study. currentunderstanding of data anonymization techniques, the nature of private and sensitive information,and the interaction of these issues with accurate measurement is rudimentary. too simplistic aprocedure may be inadequate: if the identity of an isp is deleted from a published report,particular details may permit the identity of the isp in question to be inferred. on the other hand,too much anonymity may hide crucial information (for example, about the particular networktopology or equipment used) from researchers. attention must therefore be paid to developingtechniques that limit disclosure of confidential information while still providing sufficient accessto information about the network to enable research problems to be tackled. in somecircumstances, these limitations may prevent the export of raw measurement datašprovoking theneed to develop configurable ﬁreduction agentsﬂ that can remotely analyze data and return resultsthat do not reveal sensitive details.finally, realizing the ﬁday in the lifeﬂ concept will require the development of acommunity process for coming to a consensus on what the essential measurements are, the scopeand timing of the effort, and so forth. it will require the efforts of many researchers and thecooperation of at least several internet service providers. the networking research communityitself will need to develop better discipline in the production and documentation of results fromunderlying data. this includes the use of more careful statistical and analytic techniques andsufficient explanation to allow archiving, repeatability, and comparison. to this end, thecommunity should foster the creation of current benchmark data sets, analysis techniques, andbaseline assumptions. several organizations have engaged in such efforts in the past (on asmaller scale than envisioned here), including the cooperative association for internet dataanalysis (caida)5 and the internet engineering task force™s ip performance metrics workinggroup (ippm).6 future measurement efforts would benefit from the networking community atlarge adopting analogous intergroup datasharing practices. 5see <http://www.caida.org>.6see <http://www.ietf.org/html.charters/ippmcharter.html>.looking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.73 modeling: new theory for networkingthe coming of age of the internet has brought about a dual set of challenges andopportunities. the intellectual tools and techniques that brought us this far do not appear to bepowerful enough to solve the most pressing problems that face us now. additionally, concernsthat were once relegated to the background when the internet was small and noncommercial arenow of crucial importance. in these challenges lies the opportunity for innovation:· understanding scaling and dynamics requires the development of new modelingmethodologies and the undertaking of new modeling efforts (employing both wellknown andnewly developed techniques) to take our understanding beyond that afforded by today™s models.· concerns of manageability, reliability, robustness, and evolvabilityšlong neglectedby researchersšare of critical importance and require the development of new basicunderstanding and theory.· even traditional problem areas, such as routing, must be addressed in a new context inlight of how the global internet has evolved.performanceeven as the internet has grown more complex, those who study and use it seek to answerincreasingly difficult questions. what sorts of changes in the scale and patterns of traffic couldlead to a performance meltdown? what are the failure modes for largescale networks? how canone characterize predictability?researchers have worked for years to develop new theory and improved models. whilethis work has yielded many insights about network behavior, understanding other aspects of thenetwork has proven a difficult challenge. workshop participants encouraged the networkingresearch community to develop new approaches and abstractions that would help model anincreasingly wide range of network traffic phenomena. simple models are more easily evaluatedand interpreted, but complex models may be needed to explain some network phenomena.queues and other resources cannot always be treated in isolation, nor can models always be basedon simplified routerlink pictures of the network. smallscale, steadystate, packetorientedmodels may not adequately explain all internet phenomena. it is also well known that moresophisticated input models (such as heavytailed traffic distributions) are required to accuratelymodel some behaviors. in other cases, the need is not for increased model complexity ormathematical sophistication but for just the opposite: new simple models that provide insightsinto widescale behavior. these may well require dealing with networking traffic at a coarser timescale or higher level of abstraction than traditional packetlevel modeling. here, theoreticalfoundations in such areas as flowlevel modeling, aggregation/deaggregation, translation betweenmicro and macro levels of analysis, and abstractly modeling the effects of closedloop feedbackand transients could be helpful. simulation is another important tool for understanding networks.advances in largescale simulation efforts would aid model validation and permit higherfidelityresults to be obtained.theory: beyond performanceover the past three decades, several bodies of theory, such as performance analysis andresource allocation/optimization, have contributed to the design and understanding of networkarchitectures, including the internet. however, as the internet has evolved into a criticalinfrastructure used daily by hundreds of millions of users, operational concerns such asmanageability, reliability, robustness, and evolvability have supplanted performance of the datalooking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.8looking over the fence at networksforwarding plane as the limiting factors. yet theoretical understanding of these crucial areas ispoor, particularly in comparison with their importance. the reasons for this disparity are many,including the lack of commonly accepted models for research in these areas, the difficulty ofdefining quantitative metrics, and doubts about the intellectual depth and viability of scholarlyresearch in these areas.as an example, consider the use of ﬁsoft stateﬂ1 in the internet, long hailed as a robusttechnique (when compared with hardstate approaches) for building distributed applications. yetwhat, precisely, is the benefit of using soft state? the notions of robustness, relative ﬁsimplicity,ﬂand ease of implementation generally associated with soft state have not been defined, much lessquantified. to take another example, the notion of plugandplay is widely believed to makenetworked equipment more manageable. however, the implications of such factors as increasedcode complexity and the cost of reconfiguring default settings remain elusive.at the heart of this endeavor is the seemingly simple but deceptively elusive challenge ofdefining the problems and an appropriate set of starting assumptions. the next steps includedeveloping new concepts or abstractions that would improve present understanding of theinfrastructure, defining metrics for success, and pursuing solutions. because the basicunderstanding and paradigms for research here have yet to be defined, the challenges are indeeddaunting.applying theoretical techniques to networkingoutsiders observed that more progress on fundamental networking problems might comefrom greater use of theoretical techniques and understanding from algorithm design and analysis,complexity theory, distributed computing theory, general system theory, control systems theory,and economic theory. for example, routing has been well studied, both theoretically andpractically, but remains a challenging and important problem for the networking community.some of the open research questions relevant to routing noted by workshop participants includethe following:· developing a greater understanding of the convergence properties of routingalgorithms such as the border gateway protocol (bgp) or improvements to it. bgp has beenfound to suffer from much slower than expected convergence and can fail if misconfigured.· developing a better theoretical framework for robustness and manageability toinform the development of less vulnerable designs.· designing new routing algorithms that take into account realworld constraints suchas the absence of complete information (and, often, the presence of erroneous information),peering agreements and complex interconnections among isps, and local policy decisions.· developing routing schemes that take into account the fact that the network is notsimply composed of routers and linksšnetwork address translators, firewalls, proxies, underlyingtransport infrastructures, and protocols all come into play. which of these elements are relevantand how should they be abstracted to better understand routing?· developing an understanding of the conditions under which load balancing andadaptive multipath routing work effectively and the conditions under which they can lead toinstability and oscillation. 1ﬁstateﬂ refers to the configuration of elements, such as switches and routers, within the network. softstate, in contrast to hard state, means that operation of the network depends as little as possible onpersistent parameter settings within the network.looking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.94 making disruptive prototypes: another approach tostimulating researchin addition to measurement and modeling, a third approach to stimulating continuedinnovation is to build prototypes. very successful, widely adopted technologies are subject toossification, which makes it hard to introduce new capabilities or, if the current technology hasrun its course, to replace it with something better. existing industry players are not generallymotivated to develop or deploy disruptive technologies (indeed, a good example of disruptivetechnology is a technology that a major network hardware vendor would not considerimplementing in its router products). researchers in essence walk a fine line between twoslippery slopes: either carry out longterm research that may be difficult to apply to the internetor work on much shorterterm problems of the sort that would be of interest to a routermanufacturer or venture capitalist today, leaving little middle ground in which to invent newsystems and mechanisms. so it is no surprise that as the scale and utility of the internet haveincreased, it has become immensely difficult to develop an alternative vision of the network, onethat would provide important new benefits while still supporting the features of today™s internet,especially at the enormous scale of today™s network.the internet itself is, of course, a classic example of a disruptive technology that wentfrom prototype to mainstream communications infrastructure. this section considers how toenable a similar disruptive innovation that addresses the shortcomings of today™s internet andprovides other new capabilities. box 4.1 lists some research directions indentified by workshopparticipants as ways of stimulating such disruptive network designs. research communities incomputer architecture, operating systems, databases, compilers, and so on have made use ofprototypes to create, characterize, and test disruptive technologies. networking researchers alsomake use of prototyping, but the barriers discussed above make it challenging to apply theprototype methodology to networking in a way that will result in disruptive change.challenges in deploying disruptive technologyone important consideration in any technology areaša key theme of the book theinnovator™s dilemma1šis that a disruptive technology is likely to do a few things very well, butits overall performance and functionality may lag significantly behind present technology in atleast some dimensions. the lesson here is that if innovators, research funders, or conferenceprogram committees expect a new technology to do all things almost as well as the presenttechnology, then they are unlikely to invent, invest in, or otherwise encourage disruptivetechnologies. thus (re)setting community expectations may be important to foster disruptiveprototypes. expectation setting may not be enough, however; a new technology must offer somesort of compelling advantage to compensate for performance or other shortcomings as well as theadditional cost of adopting it. those applications that do not need some capability of thedisruptive technology will use the conventional internet since it is larger and more stable.also central to the notion of developing a disruptive technology is suspending, at leasttemporarily, backward compatibility or requiring that technology developers also create a viablemigration strategy. outsiders observed, for example, that rigid adherence to backwardcompatibility would have made the development of reduced instruction set computers (riscs)impossible.another key factor in the success of a disruptive technology is the link to applications.the popularity of many disruptive computer technologies has been tied to the applications thatpeople can build on top of the technologies. one example is the personal computer. early on, it 1clayton m. christensen. 2000. the innovator™s dilemma. harpercollins.looking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.10looking over the fence at networksbox 4.1 some potentially disruptive ideas about network architecture and designworkshop participants discussed a number of architectural/design issues that could stimulatedisruptive network designs. the items that follow, though not necessarily points of consensus among theauthoring committee, were identified as interesting questions worthy of further consideration and perhapsuseful directions for future networking research.where should the intelligence in the network reside?the traditional internet model pushes the intelligence to the edge, and calls for a simple dataforwarding function in the core of the network. does this continue to be the correct model? a number of adhoc functions are appearing in the network, such as nat boxes, firewalls, and content caches. there aredevices that transform packets, and places where the network seems to operate as an overlay on itself (e.g.,virtual private networks). do these trends signal the need to rethink how function is located within thenetwork? what aspects of modularity need to be emphasized in the design of functions: protocol layering,topological regions, or administrative regions? is there a need for a more complex model for howapplications should be assembled from components located in different parts of the network? there was asense in discussions at the workshop that the active networks research may have explored some of theseissues, but that the architectural questions remain unanswered.is the endtoend model the right conceptual framework?the endtoend model implies that the center of the network is a transparent forwarding medium,and that the two ends have fully compatible functions that interwork with each other. from the perspectiveof most application developers and, in some sense, from the perspective of users, this model is notaccurate. there is often a lot of practical complexity in a communication across the network, with caches,mirrors, intermediate servers, firewalls, and so on. from a user perspective, a better model of networkcommunication might be a ﬁlimited horizonﬂ model, in which the application or user can see the detail of whatis happening locally but beyond that can interact with the network only at a very abstract level. could such aview help clarify how the network actually works and how application designers should think aboutstructure?how can faults be better isolated and diagnosed?when something breaks in the internet, the internet™s very decentralized structure makes it hard tofigure out what went wrong and even harder to assign responsibility. users seem to be expected toparticipate in fault isolation (many of them know how to run ping and traceroute but find it odd that theyshould be expected to do so). this perspective suggests that the internet design might be deficient in that itdoes not pay proper attention to the way faults can be detected, isolated, and fixed, and that it puts thisburden on the user rather than the network operator. the fact that this situation might arise from aninstance of the endtoend argument further suggests that the argument may be flawed.are data a firstclass object inside the network?the traditional model of the internet is that it moves bytes between points of attachment but doesnot keep track of the identity of these bytes. from the perspective of the user, however, the namespace ofdata, with urls as an example, is a part of the network. the users view the network as having a ratherdatacentric nature in practice, and they are surprised that the network community does not pay moreattention to the naming, search, location, and management of data items. should contentbased addressingbe a network research problem?does the internet have a control plane?the original design of the internet stresses the datatransport function but minimizes attention tomanagement protocols, signaling, and control. a number of ad hoc mechanisms supply these functions, butthey do not receive the research attention and architectural definition that the data movement functions do.this seems out of balance and may limit what can be achieved in the internet today.abstractions of topology and performancethe internet hides all details of topology and linkbylink measures of performance (for example,bandwidth, delay, congestion, and loss rates) beneath the ip layer. the simple assumption is that theapplication need not know about this, and if it does need such information, it can obtain it empirically (bytrying to do something and observing the results). as more complicated applications such as contentlooking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.making disruptive prototypes: another approach to stimulating research11caches are built, the placement of these devices within the topology of the internet matters. could anetwork provide an abstract view of its performance that simplifies the design of such systems? how couldthe various performance parameters be abstracted in a useful way, and would more than one abstraction berequired for different purposes? what, for example, would it take for the network to provide information tohelp answer the question of which cache copy is most appropriate for a given user?beyond cooperative congestion controlthere seem to be a great number of papers that improve the current internet scheme forcongestion control. however, this scheme, which depends on the end nodes doing the right thing, seemsless and less suitable in general as one can trust the end nodes less and less, suggesting that one needs toexplore different tradeoffs of responsibility between the users and the network. while some research isbeing done that explores alternatives to cooperative congestion control, this may be an area that deservesgreater emphasis.incorporating economic factors into designit was noted that many of the constraints on the current network are economic in nature, nottechnological. research, to be relevant in the immediate future, needs to take the broader economic, social,and governmental environment into account. one attendee noted that in many situations, the way to getpeople to behave how you want is to construct economic incentives, not technical constraints. this could bea useful way of thinking about network design issues.finding common themes in user requirementsmany user communities feel that they are expending energy trying to solve problems faced bymany other groups of users in areas such as performance, reliability, and application design. thesecommunities believe that their requirements are not unique but that the network research community doesnot seem to be trying to understand what these common requirements are and how to solve them. thetendency within the network community is to focus attention on issues at lower layers of the protocol stackeven if significant, widespread problems would benefit from work at higher layers. one reason is that whennetworking researchers become heavily involved with application developers, the work becomesinterdisciplinary in nature. ongoing work in middleware development is an example of this researchdirection. workshop participants noted that this sort of work is difficult and rarely rewarded in the traditionalmanner in the research community.using an overlay approach to deploying disruptive technologyalong with specific disruptive ideas, workshop participants discussed the important implementationquestion of how one could deploy new technology using the existing network (to avoid having to build anentirely new network in order to try out ideas). the internet is generally thought of as being composed of acore, which is operated by the small number of large isps known as the tier 1 providers; edges, whichconsist of smaller isps and networks operated by organizations; and endpoints, which consist of the millionsof individual computers attached to the internet.1 the core is a difficult place to deploy disruptivetechnology, as the decision to deploy something new is up to the companies for which this infrastructure isthe golden goose. technical initiatives aimed at opening up the core might help, although isp reluctance todo so would remain an issue. one of the successes of the internet architecture is that the lack ofintelligence within the core of the network makes it easy to introduce innovation at the edges. following theendtoend model, this has traditionally been done through the introduction of new software at the endpoints.however, the deployment of caching and other content distribution functionality suggest ways of introducingnew functionality within the network near the edges. the existing core ip network could be used simply as adata transport service, and disruptive technology could be implemented as an overlay in machines that sitbetween the core and the edgeuser computers.2 this approach could allow new functionality to bedeployed into a widespread user community without the cooperation of the major isps, with the likelysacrifice being primarily performance. successful overlay functions might, if proven useful enough, beﬁpushed downﬂ into the network infrastructure and made part of its core functionality.1for a more detailed description of the internet™s design and structure, see computer science andtelecommunications board, national research council. 2001. the internet™s coming of age. national academypress, washington, d.c.2the overlay approach has been used in several experimental efforts, including the mbone (multicast), 6bone(ipv6), and abone (active networks).looking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.12looking over the fence at networkswas a lowcost computing platform for those who wanted to write programs. like the internet,the pc was immediately seen as valuable by a small user community that sustained its market.but it was not until the invention of the spreadsheet application that the popularity of pcs wouldrise rapidly. similarly, in the networking world, the world wide web dramatically increased thepopularity of the internet, whose size went from roughly 200,000 computers in 1990 to 10 millionin 1996, to a projected 100 million in 2001. although the inventors of these applications weretechnically sophisticated, they were not part of the research community that invented theunderlying disruptive technology. these examples illustrate an important caveat: it is hard toknow up front what the ﬁkiller appﬂ for new enabling technologies will be, and there are nostraightforward mechanisms to identify and develop them. with any proposed technologyinnovation, one must gamble that it will be compelling enough to attract a community of earlyadopters; otherwise it will probably not succeed in the long run. this chickenandeggtypeproblem proved a significant challenge in the active networks program (as did failure to build asufficiently large initial user community from which a killer application could arise).there is a tension between experimentation on a smaller scale, where the environment iscleaner, research is more manageable, and the results more readily interpreted, andexperimentation on a very large scale, where the complexity and messiness of the situation maymake research difficult. a particular challenge in networking is that many of the toughest, mostimportant problems that one would look to a disruptive networking technology to solve have todo with scaling, so it is often important to push things to as large a scale as possible. oneofakind prototypes or even small testbed networks simply do not provide a realistic environment inwhich to explore whether a new networking idea really addresses scale challenges.this suggests that if the research community is to attract enough people with newapplication ideas that need the disruptive technology, there will be a need for missionary workand/or compelling incentives for potential users. natural candidates are those trying to dosomething important that is believed to be very hard to do on the internet. one would betrustworthy voting for public elections; another, similar candidate would be developing a networkthat is robust and secure enough to permit organizations to use the public network for applicationsthat they now feel comfortable running only on their own private intranets.external driverswhile focused on the disruptive ideas that could emerge from within the networkingresearch community, workshop participants also noted the potential impact of external forces andsuggested that networking (like any area of computer science) should watch neighboring fieldsand try to assess where disruptions might cause a sudden shift in current practice. the internet iscertainly subject to the possibility of disruptive events from a number of quarters, and manynetworking researchers track developments in related fields. will network infrastructuretechnologiesšsuch as highspeed fiber or wireless linksšbe such a disruption? or will newapplications, such as video distribution, prove a disruptive force? workshop participants did notexplore these forces in detail but suggested that an ongoing dialogue within the networkingresearch community about their implications would be helpful.looking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.135 concluding observationsa reviewer of a draft of this report observed that this proposed frameworkšmeasure,develop theory, prototype new ideasšlooks a lot like research 101. why did this exploratoryeffort end up framing a research program along these lines? from the perspective of theoutsiders, the insiders did not show that they had managed to execute the usual elements of asuccessful research program, so a backtobasics message was fitting.both insiders and outsiders agreed that progress on each of these fronts would requireeffort, attention, and resources, and that each posed its own special challenges, and they alsoagreed that such investment could have significant payoffs. it is, to be sure, a daunting challenge,because the three dimensions of internet ossification identified in chapter 1 stifle the design ofinnovative alternatives. it is also possible that the workshop participants™ enthusiasm aboutopportunities for change might be tempered by seeing new ideas realized. the outsiders seriouslyconsidered the words of harry s. truman: ﬁi have found the best way to give advice to yourchildren is to find out what they want and then advise them to do it.ﬂ1 if they had beentrumanesque, they would have applauded continuing research on higher internet bandwidth, onquality of service protocols, and so forth. however, the outsiders expressed the view that thenetwork research community should not devote allšor even the majorityšof its time to fixingcurrent internet problems.instead, networking research should more aggressively seek to develop new ideas andapproaches. a program that does this would be centered on the three m™sšmeasurement of theinternet, modeling of the internet, and making disruptive prototypes. these elements can besummarized as follows:· measuringšthe internet lacks the means to perform comprehensive measurementon activity in the network. better information on the network would provide the basis foruncovering trends, as a baseline for understanding the implications of introducing new ideas intothe network, and would help drive simulations that could be used for designing new architecturesand protocols. this report challenges the research community to develop the means to capture aday in the life of the internet to provide such information.· modelingšthe community lacks an adequate theoretical basis for understandingmany pressing problems such as network robustness and manageability. a more fundamentalunderstanding of these important problems requires new theoretical foundationsšways ofreasoning about these problemsšthat are rooted in realistic assumptions. also, advances areneeded if we are to successfully model the full range of behaviors displayed in reallife, largescale networks.· making disruptive prototypesšto encourage thinking that is unconstrained by thecurrent internet, ﬁplan bﬂ approaches should be pursued that begin with a clean slate and onlylater (if warranted) consider migration from current technology. a number of disruptive designideas and an implementation strategy for testing them are described in chapter 4.when contemplating launching a new agenda along these lines it is also worth noting, asworkshop participants did repeatedly during the course of the workshop, that in the past, thenetworking research community made attempts at broad new research initiatives, some of whichfailed at various levels and others of which succeeded beyond expectations. there is littlesystematic process for learning from these attempts, however. failures are rarely documented,despite the potential value of documentation to the community. failures can be embarrassing tothe individuals concerned, and writing up failures is unlikely to be considered as productive as 1edward r. morrow television interview, ﬁperson to person,ﬂ cbs, may 27, 1955.looking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.14looking over the fence at networkswriting up successes. accordingly, it would be useful to convene an ﬁautopsy workshopﬂ fromtime to time, perhaps even annually, devoted to learning from past history, at both the individualsolution level and the larger research areas level. documenting negative results will help avoidwasted effort (the reinvention of faulty wheels). a postmortem on larger research areas will helpto guide future research by increasing understanding of the specific successes and failures, as wellas the underlying reasons for them (economics, for example, or politics). within research areas,the successes and failures of interest include attempts at disruption to conventional networking,which so far have met with mixed success. two initial candidates suggested by discussions atthis workshop would be quality of service and active networking.looking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.appendixeslooking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.looking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.17appendix abiographies of committee membersdavid patterson, chair, is the e.h. and m.e. pardee chair of computer science at theuniversity of california at berkeley. he has taught computer architecture since joining thefaculty in 1977 and has been chair of the computer science division of the electricalengineering and computer science department at berkeley. he is well known for leading thedesign and implementation of risc i, the first vlsi reduced instruction set computer, whichbecame the foundation for the architecture currently used by fujitsu, sun microsystems, andxerox. he was also a leader of the redundant arrays of inexpensive disks (raid) project,which led to highperformance storage systems from many companies, and the network ofworkstation (now) project, which led to cluster technology used by internet companies such asinktomi. he is a member of the national academy of engineering and a fellow of the instituteof electrical and electronics engineers (ieee) and the association for computing machinery(acm). he served as chair of the computing research association (cra). his current researchinterests are in building novel microprocessors using intelligent dram (iram) for use inportable multimedia devices and in creating intelligent storage (istore) to provide available,maintainable, and evolvable servers for internet services. he has consulted for many companies,including digital, hewlettpackard, intel, and sun microsystems, and he is the coauthor of fivebooks. dr. patterson served on the cstb committees that produced computing the future andmaking it better.david d. clark graduated from swarthmore college in 1966 and received his ph.d. from themassachusetts institute of technology (mit) in 1973. he has worked since then at the mitlaboratory for computer science, where he is currently a senior research scientist in charge ofthe advanced network architecture group. dr. clark™s research interests include networks,network protocols, operating systems, distributed systems, and computer and communicationssecurity. after receiving his ph.d., he worked on the early stages of the arpanet and on thedevelopment of token ring local area network technology. since the mid1970s, dr. clark hasbeen involved in the development of the internet. from 1981 to 1989, he acted as chief protocolarchitect for this development and chaired the internet activities board. his current research areais protocols and architectures for very large and very high speed networks. specific activitiesinclude extensions to the internet to support realtime traffic, explicit allocation of service,pricing, and new network technologies. in the security area, dr. clark participated in the earlydevelopment of the multilevel secure multics operating system. he developed an informationsecurity model that stresses integrity of data rather than disclosure control. dr. clark is a fellowof the acm and the ieee and a member of the national academy of engineering. he receivedthe acm sigcomm award, the ieee award in international communications, and the ieeehamming award for his work on the internet. he is a consultant to a number of companies andserves on a number of technical advisory boards. dr. clark is currently the chair of the computerscience and telecommunications board. he chaired the committee that produced the cstbreport computers at risk: safe computing in the information age. he also served on thecommittees that produced the cstb reports toward a national research network, realizing theinformation future: the internet and beyond, and the unpredictable certainty: informationinfrastructure through 2000.anna karlin is a professor in the computer science and engineering department at theuniversity of washington. after receiving her ph.d. in computer science at stanford universitylooking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.18appendix ain 1987, she did postdoctoral work at princeton university. she then joined digital equipmentcorporation™s systems research center in 1988 as a research scientist and worked there until shecame to the university of washington in 1994. her research interests include competitiveanalysis of online algorithms, design and analysis of probabilistic algorithms, and the design andanalysis of algorithms for problems in operating systems, architecture, and distributed systems.she is currently a member of the computer science and telecommunications board.jim kurose received a b.a. degree in physics from wesleyan university in 1978 and his m.s.and ph.d. degrees in computer science from columbia university in 1980 and 1984, respectively.he is currently professor and chair of the department of computer science at the university ofmassachusetts, where he is also codirector of the networking research laboratory and themultimedia systems laboratory. professor kurose was a visiting scientist at ibm researchduring the 19901991 academic year and at inria and eurecom, both in sophia antipolis,france, during the 19971998 academic year. his research interests include realtime andmultimedia communication, network and operating system support for servers, and modeling andperformance evaluation. dr. kurose is the past editor in chief of the ieee transactions oncommunications and of the ieee/acm transactions on networking. he has been active in theprogram committees for ieee infocom, acm sigcomm, and acm sigmetrics conferencesfor a number of years. he is the sixtime recipient of the outstanding teacher award from thenational technological university (ntu), the recipient of the outstanding teacher award fromthe college of natural science and mathematics at the university of massachusetts, and therecipient of the 1996 outstanding teaching award of the northeast association of graduateschools. he has been the recipient of a general electric fellowship, an ibm faculty developmentaward, and a lilly teaching fellowship. he is a fellow of the ieee and a member of acm, phibeta kappa, eta kappa nu, and sigma xi. with keith ross, he is the coauthor of the textbookcomputer networking, a top down approach featuring the internet, published by addisonwesley longman in 2000.edward d. lazowska is professor and chair of the department of computer science andengineering at the university of washington. lazowska received his b.a. from brownuniversity in 1972 and his ph.d. from the university of toronto in 1977. he has been at theuniversity of washington since that time. his research concerns the design and analysis ofdistributed and parallel computer systems. dr. lazowska is a member of the nsf directorate forcomputer and information science and engineering advisory committee, chair of thecomputing research association, a member of darpa isat, and a member of the technicaladvisory board for microsoft research. dr. lazowska is currently a member of the computerscience and telecommunications board. he served on the cstb committee that produced thereport evolving the high performance computing and communications initiative to support thenation™s information infrastructure. he is a fellow of the acm and of the ieee.david liddle is a general partner in the firm u.s. venture partners (usvp). it is a leadingsilicon valley venture capital firm that specializes in building companies from an early stage indigital communications/networking, ecommerce, semiconductors, technical software, and ehealth. he retired in december 1999 after 8 years as ceo of interval research corporation.during and after his education (b.s., e.e., university of michigan; ph.d., computer science,university of toledo, ohio), dr. liddle spent his professional career developing technologies forinteraction and communication between people and computers, in activities spanning research,development, management, and entrepreneurship. first, he spent 10 years at the xerox palo altoresearch center and the xerox information products group where he was responsible for thefirst commercial implementation of the graphical user interface and local area networking. hethen founded metaphor computer systems, whose technology was adopted by ibm and whichwas ultimately acquired by ibm in 1991. in 1992, dr. liddle cofounded interval research withlooking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.appendix a19paul allen. since 1996, the company formed six new companies and several joint ventures basedon the research conducted at interval. dr. liddle is a consulting professor of computer science atstanford university. he has served as a director at sybase, broderbund software, metricom,starwave, and ticketmaster. he was honored as a distinguished alumnus from the university ofmichigan and is a member of the national advisory committee at the college of engineering ofthat university. he is also a member of the advisory committee of the school of engineering atstanford university. he has been elected a senior fellow of the royal college of art for hiscontributions to humancomputer interaction. dr. liddle currently serves on the computerscience and telecommunications board.derek mcauley is director of marconi labs, cambridge, england. he obtained his b.a. inmathematics from the university of cambridge in 1982 and his ph.d. addressing issues ininterconnecting heterogeneous atm networks in 1989. after 5 years at the university ofcambridge computer laboratory as a lecturer, he moved in 1995 to the university of glasgow aschair of the department of computing science. he returned to cambridge in july 1997 to helpfound the microsoft research facility in cambridge, a onceinalifetime opportunity. in january2001 he was presented with a second onceinalifetime opportunity as founder of marconi labs.his research interests include networking, distributed systems, and operating systems.vern paxson is a senior scientist with the at&t center for internet research at the internationalcomputer science institute in berkeley and a staff scientist at the lawrence berkeley nationallaboratory. his research focuses on internet measurement and network intrusion detection. heserves on the editorial board of ieee/acm transactions on networking and has been active inthe internet engineering task force (ietf), chairing working groups on performance metrics,tcp implementation, and endpoint congestion management, as well as serving on the internetengineering steering group (iesg) as an area director for transport. he has participated innumerous program committees, including sigcomm, usenix, usenix security, and raid;cochairs the 2002 sigcomm networking conference; and is a member of the steeringcommittee for the sigcomm internet measurement workshop, 2001. he received his m.s. andph.d. degrees from the university of california, berkeley.stefan savage is an assistant professor in the department of computer science and engineeringat the university of california, san diego. prior to joining the faculty at ucsd in 2001, hisdoctoral work was at the university of washington. dr. savage™s current research interests focuson widearea networking, reliability, and security. previously he has worked broadly in the fieldof experimental computer systems, including research on realtime scheduling, operating systemconstruction, disk array design, concurrency control, and performance analysis.ellen w. zegura received the b.s. degrees in computer science and electrical engineering(1987), the m.s. degree in computer science (1990), and the d.sc. degree in computer science(1993), all from washington university, st. louis, missouri. she has been on the faculty at thecollege of computing, georgia institute of technology, since 1993. she is currently an associateprofessor and assistant dean of facilities planning. her research interests include activenetworking, server selection, anycast and multicast routing, and modeling largescaleinternetworks. her work in topology modeling is widely recognized as providing the best currentmodels to use in simulationbased studies of internet problems. a software packageimplementing these models is in frequent use by other research groups and has been incorporatedinto one of the leading public domain software tools for internet simulations. her work in active(or programmable) networking is among the earliest in this relatively new field. her focus onapplications of active networkingšand rigorous comparison of active solutions to traditionalsolutionsšdistinguishes her work from that of the many other groups who have focused onenabling technologies. dr. zegura is currently leading a darpa working group on composablelooking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.20appendix aservices and applications and editing a document intended to serve as a foundation for researchand development efforts in the darpa community. her work in the server selection areafocuses on techniques for the effective use of replicas of servers distributed over the wide area.she has developed an architecture to support server selection on a wide range of performance andpolicy criteria, thus supporting diversity in the service or in client preferences. she has alsoexplored the implications of server selection in the presence of emerging technologies, includingmulticastcapable servers and qoscapable networks. dr. zegura has served on a variety of nsfaward selection panels and numerous conference program committees, as well as threeconference executive committees (as publicity chair for icnp™97 student travel grant, chair forsigcomm™97, and tutorials chair for sigcomm™99). she was cochair of the 2nd workshop oninternet server performance, held in conjunction with acm sigmetrics. she also served on theselection committee for the cra distributed mentor program. dr. zegura™s work has beenfunded by darpa, the nsf, the cra, and nasa. she has also received industrial grants fromhitachi telecom, usa, and internal university grants from the packaging research center andthe broadband telecommunications center.looking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.21appendix blist of workshop participantschristine borgman, university of california, los angelesdavid d. clark,* massachusetts institute of technologydavid culler, university of california, berkeleyjeff dozier, university of california, santa barbaraanna karlin, university of washingtonjim kurose,* university of massachusetts, amherstderek mcauley,* marconi researchjohn ousterhout, interwovendavid patterson, university of california, berkeleyvern paxson,* at&t center for internet research, international computer science institutesatish rao, university of california, berkeleystefan savage, * university of california, san diegohal varian, university of california, berkeleyellen zegura,* georgia institute of technologyadditional input, in the form of answers to the three questions posed in box p.1, wasprovided by andy bechtolsheim* (cisco), eric brewer (university of california at berkeley),stephanie forrest (university of new mexico), ed lazowska (university of washington), andtom leighton (mit). *indicates a networking insider.looking over the fence at networks: a neighbor's view of networking researchcopyright national academy of sciences. all rights reserved.
detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/5505massive data sets: proceedings of a workshop218 pages | 8.5 x 11 | paperbackisbn 9780309056946 | doi 10.17226/5505committee on applied and theoretical statistics, national research councilmassive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.massive data setsproceedings of an workshopcommittee on applied and theoretical statisticsboard on mathematical sciencescommission on physical sciences, mathematics, and applicationsnational research councilnational academy presswashington, d.c. 1996imassive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.notice: the project that is the subject of this report was approved by the governing board of the national research council, whose members are drawn from the councils of the national academy of sciences, the national academy of engineering, and the institute of medicine.the national academy of sciences is a private, nonprofit, selfperpetuating society of distinguished scholars engaged in scientific and engineering research, dedicated to the furtherance of science and technology and to their use for the general welfare. upon the authority of thecharter granted to it by the congress in 1863, the academy has a mandate that requires it to advise the federal government on scientific andtechnical matters. dr. bruce alberts is president of the national academy of sciences.the national academy of engineering was established in 1964, under the charter of the national academy of sciences, as a parallel organization of outstanding engineers. it is autonomous in its administration and in the selection of its members, sharing with the national academyof sciences the responsibility for advising the federal government. the national academy of engineering also sponsors engineering programs aimed at meeting national needs, encourages education and research, and recognizes the superior achievement of engineers. dr.william a. wulf is interim president of the national academy of engineering.the institute of medicine was established in 1970 by the national academy of sciences to secure the services of eminent members of appropriate professions in the examination of policy matters pertaining to the health of the public. the institute acts under the responsibility givento the national academy of sciences by its congressional charter to be an adviser to the federal government and, upon its own initiative, toidentify issues of medical care, research, and education. dr. kenneth i. shine is president of the institute of medicine.the national research council was organized by the national academy of sciences in 1916 to associate the broad community of scienceand technology with the academy's purposes of furthering knowledge and advising the federal government. functioning in accordance withgeneral policies determined by the academy, the council has become the principal operating agency of both the national academy of sciences and the national academy of engineering in providing services to the government, the public, and the scientific and engineeringcommunities. the council is administered jointly by both academies and the institute of medicine. dr. bruce alberts and dr. william a.wulf are chairman and interim vice chairman, respectively, of the national research council.the national research council established the board on mathematical sciences in 1984. the objectives of the board are to maintain awareness and active concern for the health of the mathematical sciences and to serve as the focal point in the national research council for issuesconnected with the mathematical sciences. in addition, the board conducts studies for federal agencies and maintains liaison with the mathematical sciences communities and academia, professional societies, and industry.support for this project was provided by the department of defense and the national science foundation. any opinions, findings, or conclusions expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors.international standard book number 0309056942copyright 1996 by the national academy of sciences. all rights reserved.additional copies of this report are available from:board on mathematical sciencesnational research council2101 constitution avenue, n.w.washington, d.c. 20418tel: 2023342421 fax: 2023341597 email: bms@nas.eduprinted in the united states of americaiimassive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.committee on applied and theoretical statisticsjon r. kettering, bellcore, chairrichard a. berk, university of california, los angeleslawrence d. brown, university of pennsylvanianicholas p. jewell, university of california, berkeleyjames d. kuelbs, university of wisconsinjohn lehoczky, carnegie mellon universitydaryl pregibon, at&t laboratoriesfritz scheuren, george washington universityj. laurie snell, dartmouth collegeelizabeth thompson, university of washingtonstaffjack alexander, program officeriiimassive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.board on mathematical sciencesavner friedman, university of minnesota, chairlouis auslander, city university of new yorkhyman bass, columbia universitymary ellen bock, purdue universitypeter e. castro, eastman kodak companyfan r.k. chung, university of pennsylvaniar. duncan luce, university of california, irvinesusan montgomery, university of southern californiageorge nemhauser, georgia institute of technologyanil nerode, cornell universityingram olkin, stanford universityronald peierls. s, brookhaven national laboratorydonald st. p. richards, university of virginiamary f. wheeler, rice universitywilliam p. ziemer, indiana universityex officio memberjon r. kettering, bellcore chair, committee on applied and theoretical statisticsstaffjohn r. tucker, directorjack alexander, program officerruth e. o'brien, staff associatebarbara w. wright, administrative assistantivmassive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.commission on physical science, mathematics, and applicationsrobert j. hermann, united technologies corporation, cochairw. carl lineberger, university of colorado, cochairpeter m. banks, environmental research institute of michiganlawrence d. brown, university of pennsylvaniaronald g. douglas, texas a&m universityjohn e. estes, university of california, santa barbaral. louis hegedus, elf atochem north america, inc.john e. hopcroft, cornell universityrhonda j. hughes, bryn mawr collegeshirley a. jackson, u.s. nuclear regulatory commissionkenneth h. keller, council on foreign relationskenneth i. kellermann, national radio astronomy observatoryken kennedy, rice universitymargaret g. kivelson, university of california, los angelesdaniel kleppner, massachusetts institute of technologyjohn krieck, sanders, a lockheed martin companymarsh i. lester, university of pennsylvaniathomas a. prince, california institute of technologynicholas p. samios, brookhaven national laboratoryl.e. scriven, university of minnesotashmuel winograd, ibm t.j. watson research centercharles a. zraket, mitre corporation (retired)norman metzger, executive directorvmassive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.vimassive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.prefacein response to a request from the chief of statistical research techniques for the national security agency(nsa), the committee on applied and theoretical statistics (cats) commenced an activity on the statisticalanalysis and visualization of massive data sets. on july 78, 1995, a workshop that brought together more than50 scientists and practitioners (see appendix) was conducted at the national research council's facilities inwashington, d.c.massive data sets pose a great challenge to scientific research in numerous disciplines, including modemstatistics. today's data sets, with large numbers of dimensions and often huge numbers of observations, havenow outstripped the capability of previously developed data measurement, data analysis, and data visualizationtools. to address this challenge, the workshop intermixed prepared applications papers (part ii) with small groupdiscussions and additional invited papers (part iii), and it culminated in a panel discussion of fundamental issuesand grand challenges (part iv).workshop participants addressed a number of issues clustered in four major categories: concepts, methods,computing environment, and research community paradigm. under concepts, problem definition was a mainconcern. what do we mean by massive dam? in addition to a working definition of massive data sets, a richerlanguage for description, models, and the modeling process is needed (e.g., new modeling metaphors).moreover, a systematic study of how, when, and why methods break down on mediumsized data sets is neededto understand tradeoffs between data complexity and the comprehensibility and usefulness of models.in terms of methods we need to adapt existing techniques, find and compare homogeneous groups,generalize and match local models, and identify robust models or multiple models as well as sequential anddynamic models. there is also the need to invent new techniques that may mean using methods for infinite datasets (i.e., populationbased statistics) to stimulate development of new methods. for reduction of dimensionalitywe may need to develop rigorous theorybased methods. and we need an alternative to internal crossvalidation.consideration of computing environment issues prompted workshop participants to suggest a retooling ofthe computing environment for analysis. this would entail development of specialized tools in general packagesfor nonstandard (e.g., sensorbased) data, methods to help generalize and match local models (e.g., automatedagents), and integration of tools and techniques. it was also agreed that there is a need to change or improve dataanalysis presentation tools. in this connection, better design of hierarchical visual display and improvedtechniques for conveying or displaying variability and bias in models were suggested. it was also agreed that amore broadbased education will be required for statisticians, one that includes better links between statistics andcomputer science.research community and paradigm issues include a need to identify success stories regarding the use andanalysis of massive data sets; increase the visibility of concerns about massive data sets in professional andeducational settings; and explore relevant literature in computer science, statistical mechanics, and other areas.discussions during the workshop pointed to the need for a wider variety of statistical models, beyond thetraditional linear ones that work well when data is essentially "clean" and possesses nice properties. a dilemmais that analysis of massive, complex data generates involved and complicated answers, yet there is a perceivedneed to keep things simple.prefaceviimassive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.the culminating activity of the workshop was a panel discussion on fundamental issues and grandchallenges during which participants exchanged views on basic concerns and research issues raised to varyingextents in the workshop's three group discussion sessions. to facilitate the discussion the panel moderator posedfour questions selected from among those generated by panel members prior to the session. the proceedingsreflect attempts by workshop participants to address these and related questions. there were significantdifferences of opinion, but some agreement on items for ongoing exploration and attentionšsummarized aboveand listed in part ivšwas reached.in addition to these proceedings, an edited videotape of the workshop will be available on the world wideweb in december 1996 at url: http://www.nas.edu/.prefaceviiimassive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.contents opening remarksjon kettenring, bellcore 1part i participant's expectations for the workshopsession chair: daryl pregibon, at&t laboratories 3part ii applications paperssession chair: daryl pregibon, at&t laboratories 13 earth observation systems: what shall we do with the data we are expecting in 1998?ralph kahn, jet propulsion laboratory and california institute of technology 15 information retrieval: finding needles in massive haystackssusan t. dumais, bellcore 23 statistics and massive data sets: one view from the social sciencesalbert f. anderson, population studies center, university of michigan 33 the challenge of functional magnetic resonance imagingwilliam f. eddy, mark fitzgerald, and christopher genovese, carnegie mellon university audris mockus, bell laboratories (a division of lucent technologies) 39 marketingjohn schmitz, information resources, inc. 47 massive data sets: guidelines and practical experience from health carecolin r. goodall, health process management, pennsylvania state university 51 massive data sets in semiconductor manufacturingedmund l. russell, advanced micro devices 69 management issues in the analysis of largescale crime data setscharles r. kindermann and marshall m. deberry, jr., bureau of justice statistics, u.s.department of justice 77 analyzing telephone network dataallen a. mcintosh, bellcore 81 massive data assimilation/fusion in atmospheric models and analysis: statistical, physical, and computational challengesgad levy, oregon state university carlton pu, oregon graduate institute of scienceand technology paul d. sampson, university of washington 93contentsixmassive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.part iii additional invited papers massive data sets and artificial intelligence planningrobert st. amant and paul r. cohen, university of massachusetts 105 massive data sets: problems and possibilities, with application to environmentnoel cressie, iowa state university anthony olsen, u.s. environmental protectionagency dianne cook, iowa state university 115 visualizing large datasetsstephen g. eick, bell laboratories (a division of lucent technologies) 121 from massive data sets to science catalogs: applications and challengesusama fayyad, microsoft research padhraic smyth, university of california, irvine 129 information retrieval and the statistics of large data setsdavid d. lewis, at&t bell laboratories 143 some ideas about the exploratory spatial analysis technology required for massivedatabasesstan openshaw, leeds university 149 massive data sets in navy problemsj.l. solka, w.l. poston, and d.j. marchette, naval surface warfare center e.j. wegman, george mason university 157 massive data sets workshop: the morning afterpeter j. huber, universität bayreuth 169part iv fundamental issues and grand challenges 185 panel discussionmoderator: james hodges, university of minnesota 187 items for ongoing consideration 203 closing remarksjon kettenring, bellcore 205 appendix: workshop participants 207contentsxmassive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.opening remarksjon kettenringbellcoregood morning everybody, and welcome! it is nice to see so many people here to talk about opportunitiesfor statistics in dealing with massive data sets. i hope that this workshop is as exciting for you as i think it isgoing to be. my colleague, daryl pregibon, and i are here on behalf of the committee on applied andtheoretical statistics, the sponsor of this workshop. cats is a committee of the board on mathematicalsciences, which is part of the national research council. we try to spotlight critical issues that involve the fieldof statistics, topics that seem to be timely for a push by the statistical community. the topic of massive data setsis a perfect example.looking at the names of the 50 or more people on our attendance list, i see that it is quite an interestinggroup from many perspectives. first, i am happy that we have a number of graduate students here with us. if weare able to make progress in this area, it is most likely the graduate students who are going to lead the way. so iwould like to welcome them particularly.we also have a number of seasoned statisticians who have been grappling with some of these issues now fora number of years. we are hoping that we can draw on their talents to help us understand some of thefundamental issues in scaling statistical methods to massive data sets.we are particularly happy to have so many people here who have had genuine, realworld experiencethinking about how to deal with massive data sets. to the extent that this workshop is successful, it is going to bebecause of the stimulation that they provide us, based on what they are actually doing in their areas of interest,rather than just talking about it.we also have a nice cross section of people from business, government, and academia. i think it is also thecase that nobody in the room knows more than just a small fraction of the other people. so one of the challengesfor us is to get to know each other better.let us turn now to the agenda for the workshop. first, the only formal talks scheduledšand indeed, wehave kept them as brief as possiblešare all applicationsoriented talks. there are 10 of these. our purpose is tolet you hear firsthand from people actually working in the various corners of applications space about theproblems they have been dealing with and what the challenges are. the hope is that these talks will stimulatemore indepth discussion in the small group sessions. i hope that we can keep ourselves grounded in theseproblems as we think about some of the more fundamental issues.we have organized the small group discussions according to particular themes. the first small groupdiscussion will deal with data preparation and the initial unstructured exploration of a massive data set. thesecond theme will be data modeling and structured learning. the final one will be confirmatory analysis andpresentation of results. in each small group session, we hope to focus on four questions: what existing ideas,methods, and tools can be useful in addressing massive problems? are there new general ones that work? arethere specialpurpose ones that work in some situations? where are the gaps?the closing session of the workshop will offer a chance to grapple with some of the fundamental issues thatunderlie all of these challenges posed by massive data. we have a veryopening remarks1massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.interesting crosssectional panel of people with a wide range of experience in thinking about fundamental issues.in addition to having the proceedings of the workshop published by the national academy press, we hopethat various segments will be available on videotape and on the world wide web so that more people will beable to benefit from the results of the work that we are going to do together in the next couple of days.i wonder about the expectations you may have brought to this workshop. for myself, i am looking forinsights from real experiences with data, e.g., which methods have worked and which have not. i would like toget a deeper understanding of some of the fundamental issues and the priorities for research. i am hopingšandthis is something that cats generally is particularly interested inšfor momentum that might serve as a catalystfor future research in this area. finally, i am hoping that one result will be a network of people who know eachother a little bit better and can communicate about going forward. indeed, that is one of our notsohiddenagendas in gathering such a disparate group of people here.opening remarks2massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.part iparticipants' expectations for the workshopsession chair: daryl pregibonat&t laboratories3massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.4massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.daryl pregibon. we worked hard to get a very diverse cross section of people at this workshop. thus someof the participants (including myself!) are familiar with only a small number of the other participants, eitherpersonally or professionally. yet we are going to be working together closely for the remainder of the workshop.we should therefore introduce ourselves to the group, thereby allowing us to put a face to each name that wehave been seeing on the email circulation list. the key information we are looking for is your name, affiliation,and what you want to get out of the workshop.jon kettenring (bellcore). i have a personal interest in applied multivariate analysis. we have manypractical techniques and tools that work reasonably well on moderatesized problems. i fear that these methods,such as principal components analysis for dimensionality reduction and cluster analysis for segmenting data, arenot going to scale very well. yet these methods seem to me to be the sort of tools that i hear a crying need for asi read through the various applications areas. given a hundred or a thousand variables, should we do a principalcomponents analysis to reduce the number of variables to something manageable? i am not sure that is what ireally need to do. i think one of the big challenges in the massive data set area is going from the global to thelocal, and being able to carve out segments of the space to work in. again, the techniques of statistical clusteranalysis could be very helpful for localizing the space. but i also am well aware of the numerous deficiencieswith these methods, and their frequent ineffectiveness even in moderatesized data sets. so i have a feeling that ifwe're going to try to solve the same problems in massive data sets, we're going to need algorithms and methodsquite different from those we have now.pregibon. one reason that i am here is because of my membership in cats. there are mounds ofopportunities for the statistical communityšso much opportunity and so little effort being expended. cats istrying to build the interest and the relationships to involve the statistical community in these problems. thesecond reason i am here concerns the problems that i see initially in manufacturing and now more in the serviceside of the corporation. we are dealing with transactions on our network consisting of about 200 millionmessages a day. we need to harness the information in these data for a variety of applications, including networkplanning, service innovation, marketing, and fraud detection.william eddy (carnegie mellon university). i am a former chair of cats, and so i have a longstandinginterest in cats activities. i have always been interested in large data sets. in the last year and a half, my notionof what was large has changed substantially. about a year ago, i had 1 gigabyte of disk storage on my workstation, and i now have 12, and i have been adding at the rate of 2 gigabytes a month because the data sets thatwe are collecting are growing explosively.lyle ungar (university of pennsylvania). i spent many years looking at modeling for chemical processcontrol, using neural networks combined with prior knowledge in the form of mass energy balances, comparingthese approaches with things like mars and projection pursuit regression. i am now looking at informationretrieval, with the natural language people at the university of pennsylvania, trying to see what techniques canbe pulled from things like pc8, and how well they scale when one gets much bigger data sets. i am interested inseeing what techniques people have for deciding, for example, which variables out of the space of 100,000 arerelevant, and using those for applications.5massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.brad kummer (lucent technologies). i am also not a statistician. i guess i am a physicist mined engineeror engineering manager. i manage a group of folks who are supporting our optical fiber manufacturing facility inatlanta. we have huge amounts of data on the fiber as it progresses through different stages of manufacture. oneof the problems is mapping the corresponding centimeters of glass through the different stages and combiningthese data sets.ralph kahn (jet propulsion laboratory). i study the climate on earth and mars.noel cressie (iowa state university). i am very interested in the analysis of spatial and temporal data.within that, i have interests in image analysis, remote sensing, and geographic information systems. theapplications areas in which i am interested are mainly in the environmental sciences.john schmitz (information resources, inc.). my training is in statistics and economics, but i have workedas a computer programmer all of my life. i have two reasons for wanting to be here. one is that i hardly ever seeother people who are thinking in terms of statistics and data analysis. the other concerns the enormous databasesthat we have. we are trying to figure out how to do subsampling, or something else, to get quick responses tocasual questions. i hope to get ideas in that area.fritz scheuren (george washington university). i have been very heavily involved in the statistical use ofadministrative records. until recently, social security records and tax records and other things were thought to belarge; i'm not sure anymore, but they are very useful starting points. currently, my interests are in star, sties andadministrative records (federal, state, and local) and how they fit together. i was worrying about being asampling statistician, but i guess maybe i shouldn't anymore. i am also a member of cats.david lewis (at&t bell laboratories). i work in information retrieval. over the past 6 or 7 years, i havemade the transition from someone who has spent a lot of time thinking about linguistics and natural languageprocessing to someone who spends a lot of time thinking about statistics, mostly classification methods for doingthings like sorting documents into categories, or deciding whether documents are relevant. my strongest interestrecently has been in what the computer science community calls active learning and what the statisticscommunity calls experimental design. if you have huge amounts of data and you want to get human labeling ofsmall amounts of it for training purposes, how do you choose which small amounts to use? we have been able toreduce the amount of data people have to look at by up to 500fold. the theory says you can reduce itexponentially, and so there is a long way to go.susan dumais (bellcore). i have a formal background in cognitive psychology. i suspect i am probably theonly psychologist here. i work for a company that is interested in a number of problems in the area of human andcomputer interaction. one of the problems we have worked on a lot is that of information retrieval. i am going totalk later this morning about some dimension reduction ideas that we have applied to rather large informationretrieval problems.jerome sacks (national institute of statistical sciences). the niss was formed a few years ago to do andstimulate research in statistics that had crossdisciplinary content and impact, especially on6massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.large problems. after this weekend, i trust we will now be working on massive problems, or at least hope to. infact, some of our projects currently are in transportation, education, and the environment, but they do not involvethe massive data questions that people have been discussing and will report on these next two days. we see thelocomotive coming down the tracks very rapidly at us, threatening to overwhelm almost anything else that wemay be doing with the kind of data sets that we do confront. another aspect of my own life in connection withthis particular project is that i am on one of the governing boards of the cats committee, namely, thecommission on physical sciences, mathematics, and applications. i see this particular workshop as leading tostronger policy statements in connection with the future of science research, and the impact of statistical researchon science.james maar (national security agency). the national security agency is a cosponsor of this workshop.we have done four of these projects with cats, the best known of which is represented by the reportcombining information [national academy press, washington, d.c., 1992], which i commend to you. myinterest in large data sets started 17 years ago, when we posed some academic problem statements. we havemillions of vectors and hundreds of dimensions. we cannot afford to process them with a lot of computer time.we want a quick index, like a matrix entropy measure.albert anderson (public data queries, inc.). i have spent most of my life helping demographers andgraduate students try to squeeze more information out of more data than the computers would usually let us get.we have made some progress in recent years. five years ago, we targeted data sets such as the public use microdata samplešpums (of the 5 percent census sampling in 1990), as the kind of data that we would like tohandle more expediently. we have got this down to the point that we can do it in seconds now instead of thehours, and even months, that were required in the past. my interest in being here is largely to have some of thepeople here look over my shoulder and the shoulders of colleagues in the social sciences and say, ''ah ha, whydon't you try this?''peter huber (universität beyreuth, germany). my interests have been, for about 20 years now, primarilyin the methodology of data analysis and in working on interesting problems, whatever they are. at one time, ifelt that data analysis was the big white spot in statistics; now i guess that large data sets are becoming the bigwhite spot of data analysis.lixin zeng (university of washington). i am an atmospheric scientist. i am working on satellite remotesensing data, whose volume is getting bigger and bigger, and i believe it is going to be massive eventually. mymain concern is the current numerical weather prediction model. i am not sure that atmospheric scientists aremaking full use of the huge amount of satellite data. i believe my horizons will be much broader as a result ofthis workshop.marshall deberry (bureau of justice statistics, u.s. department of justice). our agency is responsible forcollecting the crime statistics for the united states that you read about in the papers. one of the major programswe have worked on is the national crime victimization survey, which has been ongoing since 1973. it used tobe about the second largest statistical survey conducted by the federal government. the other area that we arestarting to move into is the niber system,7massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.which is going to be a replacement for the uniform crime reports, the information that is put out by the fbi. thathas the potential for becoming a rather large data set, with gigabytes of data coming on a yearly basis from localjurisdictions. we are interested in trying to learn some new ways we can look at some of this data, particularlythe national crime victimization survey data.fred bannon (national security agency). i think it is fair to say that i have lived with this massive dataproblem for about 5 years now, in the sense that i have been responsible for the analysis of gigabytes of datamonthly. there are all sons of aspects of this problem that i am interested in. i am interested in visualizationtechniques, resolution of data involving mixed populations, all involved together to form the data stream, anysort of clustering techniques, and so on. anything that can be done in nearreal time i am interested in as well.john tucker (board on mathematical sciences, national research council). my background is in puremathematics. i am an applied mathematician by experience, having spent 4 to 5 years with a consulting firm. mybackground in statistics is having studied some as a graduate student as well as having been the program officerfor cats for 4 years, prior to assuming directorship of the board. i am interested in this problem because i see itas the most important crosscutting problem for the mathematical sciences in practical problemsolving for thenext decade.keith crank (division of mathematical sciences, national science foundation). i was formerly programdirector in statistics and probability and have been involved in the liaison with cats ever since i came to thefoundation. we are interested in knowing what the important problems are in statistics, so that we can help interms of providing funding for them.ed george (university of texas). i am involved primarily in methodological development. in terms of nand p, i am probably a large p guy. i have worked extensively in shrinkage estimation, hierarchical modeling,and variable selection. these methods do work on moderateto smallsized data sets. on the huge staff, they justfall apart. i have had some experience with trying to apply these methods to marketing scanner data, and insteadof borrowing strength, it seems that it is just all weakness. i really want to make some progress, and so i aminterested in finding out what everybody knows here.ken cantwell (national security agency). in addition to the other nsa problems, our recent experienceshave been with large document image databases, which have both an image processing and an informationretrieval problem.peter olsen (national security agency). i came to statistics and mathematics late in my professionalcareer. i spent my first 15 years flying helicopters and doing data analysis for the coast guard. i now do signalprocessing for the national security agency. i routinely deal with the problem of grappling with 500 megabytesof data per second. there are 84,000 seconds in the day, and so i want to be able to figure out some way tohandle that a little more expeditiously. sometimes my history does rise up to bite me from the coast guard. i amthe guy who built the mathematical model that was used to manage the exxon valdez oil cleanup.8massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.luke tierney (university of minnesota). my research interests have been twofold. one is developingmethods for computation supporting bayesian analysis, asymptotic, monte carlo, and things of that sort. mysecond interest is in computing environments to support the use and development of statistical methods,especially graphical and dynamic graphical methods. both of those are highly affected by large data sets. myexperience is mostly in small ones. i am very interested to see what i can learn.mark fitzgerald (carnegie mellon university). i am a graduate student working on functional magneticresonance imaging. i have to admit that when i started on this project, i didn't realize i was getting into massivedata sets. we had 3 megabytes of pretty pictures, and we soon found out that there were a lot of interestingproblems, and now we are up to many gigabytes of data.tom ball (mckinsey & company, inc.). i am probably one of the few mbas in the room, but i also have 8years of applied statistical experience wrestling with many of the methodological questions that have been raisedthis morning.rob st. amant (university of massachusetts, amherst). i am a graduate student in the computer sciencedepartment. i am interested in exploratory data analysis. i am building a system to get the user thinking aboutguiding the process rather than executing primitive operations. i am interested in how artificial intelligencetechniques developed in planning and expert systems can help with the massive data problem.daniel carr (george mason university). i have a longtime interest in large data sets. i started on a projectin about 1979 for the department of energy, and so i have some experience, though not currently with themassive data sets of today. i have a longtime interest in graphics for large data sets and data analysismanagement, and in how to keep track of what is done with these data sets. i am very interested in software, andam trying to follow what is going on at the jet propulsion laboratory and so on with the eosdis [earthobserving system data and information system] and things like that. one issue is that much of the statistics thatwe use just is not set up for massive data sets. some of it is not even set up for moderatesized data sets.ed russell (advanced micro devices). i am a senior program manager. my involvement in large data setsstarted in the seismic industry; i then moved into computer simulation models, and now i am working with theelectronics industry. all the industries in which i have worked are very data rich. i have seen the large n, thelarge p, and the large n and p problems. i have come up with several techniques, and i would like to validatesome of them here, and find out what works and what does not work. i did try to start a study while i was atsematech, to compare analytical methods. i would like to encourage cats to push on that, so that we can startcomparing methods to find out what their real strengths and weaknesses are with respect to extremely large datasets, with either large n, large p, or both.usama fayyad (jet propulsion laboratory, california institute of technology). i have a machine learningsystems group at jpl. we do statistical pattern recognition applied to identifying objects in large imagedatabases, in astronomy and planetary science. nasa sponsors us to basically develop systems that scientistscan use to help them deal with massive databases. i am interested in both9massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.supervised learning and unsupervised clustering on very large numbers of observations, say, hundreds ofmillions to potentially billions. these would be sky objects in astronomy. one more announcement that isrelevant to this group is that i am cochair of kdd95, the first international conference on knowledge,discovery, and data mining.david scott (rice university). i started out working in a medical school and doing some contract workwith nasa. it has been fun to work with all these exploratory tools, which started out on the back of theenvelope. i have a sneaking suspicion that my own research in density estimation may be key to expanding thetopic at hand. i have a lot of interest in visualization. i hope that we see examples of that today. on the otherhand, i am sure that a lot of us do editorial work. i am coeditor of computational statistics, and i am an editoron the board of statistical sciences. i am very eager to make sure that any keen ideas put forth here see the lightof day that way, if possible.bill szewczyk (national security agency). i am interested in trying to scale up techniques. the problem ofscale concerns me, because many techniques exist that are useful, like mcmc [markov chain monte carlo], andthat work for small data sets. if you are going to real time, they need to work for large data sets. but we are in avery time critical arena. i am sure we are not the only ones. we have to get through information for our dataquickly, and we do not have the time to let these things run for a couple of days. we need it 5 minutes from now.so i am interested in seeing how some of these new techniques could be applied to realtime processing.gad levy (oregon state university and university of washington). i am an atmospheric scientistinterested in the application and use of massive data sets, mostly from satellites in the atmospheric sense. i havebeen working for some years in satellite data, one data set at a time, and recently started to think about how tolook at them together. i have been collaborating with colleagues in computer science, who are trying to handlethe data management and utilization aspects at the oregon graduate institute and with statisticians at theuniversity of washington.wendy poston (naval surface warfare center, dalton, virginia). i am interested in signal processing ofonedimensional and twodimensional signals, where the size of the data sets, as well as the dimensionality, isextremely large.carey priebe (johns hopkins university). i am most interested in the realtime implementation of patternrecognition techniques and visualization methods. my interest in the subject of this workshop has developedover probably 10 years of working on navy and related remote sensing problems. contrary to the accepteddefinition, i define massive data sets as data sets with more data than we can currently process, so that we are notusing whatever data is there. there is a lot of data like that in other areasšsatellite data, image data. there justare not enough people to look at the data. so my statistical interests are in what might be termed preprocessingtechniques. if you already have clean data and you know that what you are interested in is there, then that is verynice. that is what i am trying to get to. when you have more data than you can look at with either the availablehuman power or with computationally intensive computer statistical techniques, the first thing you have to do istake all the data and do some sort of clustering. this would be one way to look at it, to get down to saving thedata that appears to be usable, so that you can look at it with more extensive processing, and save pointers intothe data that tell you what might be useful and10massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.what is not useful. in preprocessing, i am not trying to solve the problem, i am not trying to find the hurricane orfind the tumor in digital mammography. i am trying to reduce the load, find places where it might be valuable touse the more expensive processes.dan relies (rand corporation). i must say, if this turns into a competition of who has the biggest data set, iam going to lose! like all statisticians, i am interested in error, but not the kind that you learn about in graduateschool, random error or temporal error or those kinds of things. i am interested in human error, the fact that as ngrows, the probability of our making mistakes grows. it is already pretty high in megabytes, and gigabytes scareme a lot. i used to be idealistic enough to think that i could prevent errors, but now i believe the main problem isto figure out how to deal with them when they come along. if any of you watched the o.j. simpson trial a coupleof weeks ago, you would perhaps appreciate that. what i have tried to do over the years is write software anddevelop ideas on how to organize empirical work, so that i and the people around me at rand can be effective.art dempster (harvard university). i'm afraid i'm one of the oldtimers, having been essentially in thesame job for 37 years. i am interested not so much in multidisciplinary problems per se, although i think that"multidisciplinary" is getting to be a good term, but rather in more complex systems. i think that highlystructured stochastic systems is the buzzword that some people use for an attempt to combine modeling andbayesian inference and things of that sort. i do not necessarily agree that mcmc is not applicable to largecomplex systems. for the past year, i have been getting involved in climate studies, through the new statisticsproject at the national center for atmospheric research in boulder. that brings me into spacetime andcertainly multidisciplinary considerations almost instantly. that is, i think, one of the themes i am interested inhere. i am also interested in combining information from different sources, which has been mentioned earlier asa possible theme. i am working to some degree in other fields, medical studies, a couple of things i am thinkingabout there. i have worked in the past on government statistics and census, labor statistics, and so on. thoseinterests are currently dormant, but i think there are other people here who are interested in that kind of thing, too.colin goodall (health process management, pennsylvania state university (previously at quadramedcorp. and healthcare design systems)). hds is a company of about 250 who do information processingsoftware development and consulting for the health care, particularly the hospital, industry. these health caredata sets are certainly very large. there are 1.4 million hospital discharges in new jersey each year. multiplythat by 50 states. there are countless many more outpatient visits to add to that. the health care data sets that iwill be talking about this afternoon are special, in that not only are they very large or even massive, but thehuman input that goes into them is very massive also. every patient record has been input by a nurse or someoneelse. there has been a lot of human interaction with these data, and a lot of interest in individual patient records.the data we are concerned with are driven by federal and state mandates on data collection, which is collectionfor uniform billing data for about 230 fields per patient. in the future we might include image data, although thisis some way off.steven scott (harvard university). i am here because one of the database marketing position papers caughtmy eye. i have had limited contact with marketing people in the past, and i have11massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.noticed that massive data sets seem to be very commonšthe business people out there seem to have a lot moredata than they know what to do with. so i thought i would come and pick your brains while i had the chance.michael cohen (committee on national statistics). i am a statistician interested in large data sets.allen mcintosh (bellcore). i deal with very large data sets of the sort that john schmitz was talking aboutdata on local telephone calls. up until now, i have been fairly comfortable with the tools i have had to analyzedata. but recently i have been getting data sets that are much larger than i am used to dealing with, and that ismaking me uncomfortable. i am here to talk a little bit about it and to try to learn new techniques that i can applyto the data sets that i analyze.stephen eick (bell laboratories). i want to make pictures of large data sets. we work hard on how to makepictures of big networks, and how to come up with ways to visualize software. i think the challenge now, at leastfor at&t, is to learn how we can make a picture to visualize our 100 million customers.jim hodges (university of minnesota). there have been two streams in my work. one is an integralinvolvement in a sequence of applied problems to the point that i felt i actually knew something about thesubject area, originally at the rand corporation in the areas of combat analysis and military logistics, and now atthe university of minnesota in clinical trials related to aids. the other is an interest in the foundations ofstatistics, particularly the disconnect between the theory that we learn in school and read about in the journals,and what we really do in practice. i did not know i was interested in massive data sets until daryl pregiboninvited me to this conference and i started reading the position papers. prior to this, the biggest data set i everworked on had a paltry 40,000 cases and a trifling hundred variables per case, and so i thought the positionpapers were extremely interesting, and i have a lot to learn.12massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.part iiapplications paperssession chair: daryl pregibonat&t laboratories13massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.14massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.earth observation systems: what shall we do with the datawe are expecting in 1998?ralph kahnjet propulsion laboratory and california institute of technologyabstractthe community of researchers studying global climate change is preparing for the launch of the first earthobserving system (eos) satellite. it will generate huge amounts of new data, filling gaps in the informationavailable to address critical questions about the climate of earth. but many data handling and data analysisproblems must be solved if we are to make best use of the new measurements. in key areas, the experience andexpertise of the statistics community could be of great help.1 introductionthe first eos platform is scheduled for launch into polar orbit in june, 1998. it will carry five remotesensing instruments designed to study the surface and atmosphere of earth. in a broad sense, the purpose of theseobservations is to find indications of how earth's climate is changing, and to discover clues to the mechanismsthat are responsible for these changes. a 5 to 15 year program of global monitoring is planned, covering manywavelengths, with spatial resolutions as small as 0.25 km and temporal coverage as frequent as daily. higherresolution data on regional scales will also be acquired.the surface area of earth is about 5 × 108 km2. at 0.25 km resolution, a single instrument acquiring 36channels of data, such as the multiangle imaging spectroradiometer (misr) or the moderate resolutionimaging spectrometer (modis) on the eos platform, will generate upwards of 80 gbyte/day, or 30 tbyte/yearof basic data. the geophysical quantities are generally retrieved at lower spatial resolution, but must includequality flags and other ancillary information, resulting in a geophysical data set that will be no smaller than 3tbyte/year for the misr instrument alone.the sheer volume of data creates unprecedented challenges for accomplishing basic data handlingoperations, such as throughput and storage. but there are deeper issues regarding the scientific use of this hugeamount of data. the eos community has adopted a partial framework, and some terminology, for discussing thequestions we must face. however, in many areas the development of an approach to the underlying issues is inits infancy. this paper begins with a brief review of the data classification scheme we use to organize ourearth observation systems: what shall we do with the data we are expecting in 1998?15massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.thinking about data handling and analysis. this is followed by discussions of some issues relating to specificclasses of data, and a summary of areas to which the statistics community may be well equipped to contribute.2 data classification schemethe committee on data management and computing define five general classes of spacecraft data, basedon the degree of processing involved (codmac, 1982, and subsequent refinements): level 0 the raw data stream from the spacecraft, as received at earth level 1 measured radiances, geometrically and radiometrically calibrated level 2 geophysical parameters, at the highest resolution available level 3 averaged data, providing spatially and temporally "uniform" coverage level 4 data produced by a theoretical model, possibly with measurements as inputsthis paper focuses on level 2 and level 3 data, which are the main concerns of most global changeresearch scientists working on eos instrument teams. level 2 products are reported on an orbitbyorbit basis.for a polarorbiting satellite such as eos, the level 2 sampling of earth is highly nonuniform in space andtime, with coverage at high latitudes much more frequent than near the equator. level 2 data are needed whenaccuracy at high spatial resolution is more important than uniformity of coverage. these situations ariseroutinely for validation studies of the satellite observations, in the analysis of field campaign data, and whenaddressing other localand regionalscale problems with satellite data.the spatially and temporally uniform level 3 data are needed for globalscale budget calculations, and forany problem that involves deriving new quantities from two or more measurements which have differentsampling characteristics. to derive a level 3 product from level 2 data, spatial and temporal scales must bechosen. it is to this issue that we turn next.3 grinning and bidding to create level 3 datathe creation of level 3 data has traditionally involved the selection of a global, 2 or 3dimensional spatialgrid, possibly a time interval as well, and "binning" the level 2 data into the grid cells. the binning process forlarge data sets usually entails taking the arithmetic mean and standard deviation of all level 2 data points failinginto a grid cell, with possible trimming of outliers or of measurements flagged as "low quality" for other reasons.typically, all points included in a grid cell average are given equal weight. occasionally a median value will beused in place of the mean.the leading contender for the standard eos level 3 grid is a rectangularbased scheme similar to one thathas been used by the earth radiation budget experiment (erbe) (green and wielicki, 1995a). in the proposedimplementation for eos, the earth is divided zonally into 1.25 degree strips (about 140 km in width). each stripis then divided into an integral number of quadrilaterals, each approximately 140 km in length, with the origin atthe greenwich meridian. this produces a nearly equalarea grid.earth observation systems: what shall we do with the data we are expecting in 1998?16massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.a number of issues arise in using a grid of this sort for the level 3 data. anisotropy presents an obstacle forcalculating gradients, fluxes, and other quantities based on finite differences. some neighboring cells share anedge whereas others share only a point, and there is no general rule as to how the contributions of each should beweighted. only zonal gradients can be calculated in a consistent way on a global scale. even in the meridionaldirection, the northsouth cell boundaries are aligned only along the prime meridian. inhomogeneity presents asecond set of problems, since the distribution of grid cells varies with latitude, and there are singularities at thepoles.a third set of issues arises from the nesting properties of these grids. nested grids can be used to relate datasets taken at different spatial resolutions, such as data from groundbased, aircraft, balloon, and satelliteinstruments. it is often necessary to compare these types of data (particularly for validation work), and to usedata from multiple sources to calculate new quantities. to form subgrids at length scales below 140 km,decisions must be made as to whether the subdivisions will be equiangular, which are unique and relatively easyto define, or equal area, which has more desirable sampling properties, but requires more complex cellboundaries that increase anisotropy. performing analysis on data sets from nonnested grids introduces errorsthat may be significant on a global scale (green and wielicki, 1995b), and can be arbitrarily large in regionswhere the quantities of interest have significant gradients (kahn et al., 1991).there are alternative grids, based on triangle or hexagon subdivisions of the spherical surface or aprojection thereof, that may alleviate some of these issues (d. cart and p. huber, personal communication, mdsworkshop, 1995). a considerable body of work exists that explores the characteristics of nested systems of suchgrids (white et al., 1992, and references therein).an effort is being organized to develop such grid schemes into systems that eos scientists can use (kiester,kimerling, knighton, olsen, sahr, and white, personal communication, 1995). a specific choice of grid systemis being made, and its geometric properties characterized. schemes will be needed to address and store data atdifferent levels within the grid system. if the performance of a triangle or hexagonbased grid is promising,efficient translators to and from commonly used addressing systems, such as latitudelongitude, and conversionsto popular map projections would need to be derived and implemented in data processing and gis softwarepackages widely used by the eos community.one would like to embed each data set into a grid within a nested system that is appropriate to its resolutionand sampling structure. this raises the related issues of how to select a "native" grid size for a given data set, andhow best to calculate the value and associated statistics to be assigned to each grid cell from the level 2 data forboth continuousand discretevalued quantities. once this is done, methods may be developed to aggregate anddisaggregate grids at various spatial resolutions, calculating the associated error characteristics along with thedata (n. cressie, personal communication, mds workshop, 1995).such a system would revolutionize the way the global climate research community works with data.earth observation systems: what shall we do with the data we are expecting in 1998?17massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.4 generating level 2 datathe generation of level 2 geophysical quantities from calibrated radiances introduces a far more diverse setof issues, since the retrieval algorithms vary greatly with the type of measurement made and the retrieval strategyadopted. for specificity, i use the misr aerosol retrieval process (diner et al., 1994) as the basis for thediscussion in this section.two misrrelated issues similar to ones that arise elsewhere are: how to determine the sensitivity of theinstrument to differences in atmospheric aerosol properties, and how to develop climatologies for the retrievedgeophysical quantities based on existing constraints.4.1 sensitivity studiesfrom the point of view of retrieving aerosol properties from misr observations, the distinctions worthreporting are determined by the sensitivity of the instrument. we use a theoretical model to simulate themeasurements at the 4 wavelengths and 9 viewing angles covered by the misr instrument. we run simulationsfor a wide range of aerosol size distributions, compositions, and amounts. the full parameter space that must beexplored includes mixes of particle size distributions and compositions, atmospheric relative humidity, andsurface type.we designate the one set of simulated reflectances as the ''measured'' case, and step through "comparison"models covering a range of alternative size distributions, for example. we use simple x2 statistics to make thecomparisons, such as:where lmes, is the simulated "measured" reflectance, lcmp is the simulated reflectance for the "comparison"model, l and k are the indices for wavelength and viewing angle, n is the number of measurements included inthe calculation, and !abs is the absolute measurement error in the reflectance. mk is the weight for terms related toviewing angle k, and <mk> is the average of the weights for all the viewing angles included in the sum.comparisons made in this way reduce the information content of as many as 36 individual measurements (4wavelengths × 9 angles) to a single number. there is more information in the data. two partly independent waysto compare cases are the maximum deviation of all the measurements used, and a x2 statistic weighted by themeasurements at the nadir angle:where !rel is the relative measurement error. we are experimenting with combinations of these metrics asthe criteria to be used for evaluating the comparison cases, both in the sensitivity studies, and in the retrievalalgorithm.our approach to covering the parameter space is also simple. we are planning first to vary particle sizedistribution and amount for fixed composition, establishing the minimum number of sizes needed to representthe range of expected values within the instrument sensitivity. the discrete sizes will be used to determinesensitivity to composition, whichearth observation systems: what shall we do with the data we are expecting in 1998?18massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.is represented by the particle index of refraction. the sensitivity to mixtures will then be tested by a similarprocess.these procedures are welldefined and systematic. but they are empirical, and it is impractical to captureevery possible combination of conditions with them. in the absence of new ideas, we will live with theselimitations.4.2 climatologiesthe level 2 retrieval algorithms for eos must run in an automatic mode, rapidly processing huge amountsof data at computing facilities far from the purview of the instrument teams. as a first step in understanding theresults, we plan to automatically compare them with" the expectations" š a climatology initially based on thebest data available prior to launch.consider the aerosol climatology. the quantities of interest are the aerosol column amount and the aerosol"type", which summarizes particle composition, size distribution, and shape. there exist global satelliteestimates of aerosol amount at 1 km resolution, over oceans only, on a weekly basis for almost seven years. forthese observations, particle type is assumed. there are global models of four of the main particle types, at spatialresolutions ranging from about 100 km to about 1000 kin, at monthly or seasonal intervals. numerous in situmeasurements have also been made, with every conceivable spatial and temporal sampling. some report aerosolamount, others provide information about aerosol type, and a few include both.how do we merge all these data into a "climatology?" our current approach is to ingest monthly cases ofthe global satellite data set into our geographic information system (gis) as the primary constraint on aerosolamount. we will then use the global models to assign aerosol type, on a regionbyregion basis (figure 1). it isundecided as yet how the mix of particle types will be determined from the models, or how the uncertainty in theresults will be obtained. we plan to use in situ measurements where available, to improve the constraints placedby the global data sets. again we are undecided as to how to weight the information from different data sources,and how to assign uncertainties. lastly, we must develop the algorithm that compares the aerosol propertiesderived from the satellite data with the climatology, and assigns a measure of "likelihood" to the result.we will develop pragmatic approaches to each of these problems, but a formal procedure for constructing aclimatology of this sort is beyond our current capability.5 summary of issuesthis paper concentrates on matters of potential interest to the statistics community that relate to thegeneration of level 2 and level 3 data from eos instruments (table 1). for level 3 data, the main issues are:defining an effective system of nested grids, deriving procedures for ingesting level 2 data into the system, anddeveloping algorithms for aggregating and translating data that is in the system. level 2 data presents a morediverse set of issues; we focused on performing sensitivity studies and developing climatologies.the eos community is preparing to derive geophysical quantities from measurements that will beginappearing in june 1998. all being well, we will soon face the challengesearth observation systems: what shall we do with the data we are expecting in 1998?19massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.of actually studying the data, summarizing the trends, identifying and characterizing the exceptions, andexploring the implications of the results for further data acquisition, and for global climate change... more thanenough to keep several large and active communities of researchers very busy.acknowledgementsi thank my colleagues on the eos misr team for providing the context for this work. i also thank theparticipants in massive data sets workshop for their interest in our datahandling issues, and their patience withour naive approach to profound problems in statistics. this work is performed at the jet propulsion laboratory,california institute of technology, under contract with the national aeronautics and space administration,through the eos office of the mission to planet earth.references[1] paul r. cohen, michael l. greenberg, david m. hart, and adele e. howe. trial by fire: understanding the design requirements foragents in complex environments. ai magazine, 10(3):3248, fall 1989.[2] john d. emerson and michal a. stoto. transforming data. in david c. hoaglin, frederick mosteller, and john w. tukey, editors,understanding robust and exploratory data analysis. wiley, 1983.[3] usama fayyad, nicholas weir, and s. djorgovski. skicat: a machine learning system for automated cataloging of large scale skysurveys. in proceedings of the tenth international conference on machine learning , pages 112119. morgan kaufmann, 1993.[4] michael p. georgeff and amy l. lansky. procedural knowledge. proceedings of the ieee special issue on knowledge representation ,74(10):13831398, 1986.[5] peter j. huber. data analysis implications for command language design. in k. hopper and i. a. newman, editors, foundation forhumancomputer communication. elsevier science publishers, 1986.[6] amy l. lansky and andrew g. philpot. aibased planning for data analysis tasks. ieee expert, winter 1993.[7] stuart russell and peter norvig. artificial intelligence: a modern approach. prentice hall, 1995.[8] robert st. amant and paul r. cohen. toward the integration of exploration and modeling in a planning framework. in proceedings of theaaai94 workshop in knowledge discovery in databases, 1994.[9] robert st. amant and paul r. cohen. a case study in planning for exploratory data analysis. in advances in intelligent data analysis ,pages 15, 1995.earth observation systems: what shall we do with the data we are expecting in 1998?20massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.[10] robert st. amant and paul r. cohen. control representation in an eda assistant. in douglas fisher and hans lenz, editors, learning from data: ai and statistics v. springer, 1995. to appear.earth observation systems: what shall we do with the data we are expecting in 1998?21massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.earth observation systems: what shall we do with the data we are expecting in 1998?22massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.information retrieval: finding needles in massive haystackssusan t. dumaisbellcore1.0 information retrieval: the promise and problemsthis paper describes some statistical challenges we encountered in designing computer systems to helppeople retrieve information from online textual databases. i will describe in detail the use of a particular highdimensional vector representation for this task. lewis (this volume) describes more general statistical issues thatarise in a variety of information retrieval and filtering applications. to get a feel for the size of informationretrieval and filtering problems, consider the following example. in 1989, the associated press newswiretransmitted 266 megabytes of ascii text representing 84,930 articles containing 197,608 unique words. a termbydocument matrix describing this collection has 17.7 billion cells. luckily the matrix is sparse, but we still havelarge p (197,000 variables) and large n (85,000 observations). and, this is just one year's worth of short articlesfrom one sourcethe promise of the information age is that we will have tremendous amounts of information readilyavailable at our fingertips. indeed the world wide web (w has made terabytes of information available at theclick of a mouse. the reality is that it is surprisingly difficult to find what you want when you want it! librarianshave long been aware of this problem. end users of online catalogs or the www, like all of us, are rediscoveringthis with alarming regularity.why is it so difficult to find information online? a large part of the problem is that information retrievaltools provide access to textual data whose meaning is difficult to model. there is no simple relational databasemodel for textual information. text objects are typically represented by the words they contain or the words thathave been assigned to them and there are hundreds of thousands such terms. most text retrieval systems are wordbased. that is, they depend on matching words in users' queries with words in database objects. word matchingmethods are quite efficient from a computer science point of view, but not very effective from the end users'perspective because of the common vocabulary mismatch or verbal disagreement problem (bates, 1986; furnaset al., 1987).one aspect of this problem (that we all know too well) is that most queries retrieve irrelevantinformation. it is not unusual to find that 50% of the information retrieved in response to a query is irrelevant.because a single word often has more than one meaning (polysemy), irrelevant materials will be retrieved. aquery about "chip", for example, willinformation retrieval: finding needles in massive haystacks23massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.return articles about semiconductors, food of various kinds, small pieces of wood or stone, golf and tennis shots,poker games, people named chip, etc.the other side of the problem is that we miss relevant information (and this is much harder to know about!).in controlled experimental tests, searches routinely miss 5080% of the known relevant materials. there istremendous diversity in the words that people use to describe the same idea or concept (synonymy). we havefound that the probability that two people assign the same main content descriptor to an object is 1020%,depending some on the task (furnas et al., 1987). if an author uses one word to describe an idea and a searcheranother word to describe the same idea, relevant material will be missed. even a simple concrete object like a"viewgraph" is also called a "transparency", "overhead", ''slide", ''foil", and so on.another way to think about these retrieval problems is that wordmatching methods treat words as if theyare uncorrelated or independent. a query about "automobiles" is no more likely to retrieve an article about "cars"than one "elephants" if neither article contains precisely the word automobile. this property is clearly untrue ofhuman memory and seems undesirable in online information retrieval systems (see also caid et al., 1995). aconcrete example will help illustrate the problem.2.0 an small examplea textual database can be represented by means of a termbydocument matrix. the database in thisexample consists of the titles of 9 bellcore technical memoranda. there are two classes of documents 5 abouthumancomputer interaction and 4 about graph theory.title database:c1: human machine interface for lab abc computer applicationsc2: a survey of user opinion of computer system response timec3: the eps user interface management systemc4: system and human system engineering testing of epsc5: relation of userperceived response time to error measurementm1: the generation of random, binary, unordered treesm2: the intersection graph of paths in treesm3: graph minors iv: widths of trees and wellquasiorderingm4: graph minors: a surveythe termbydocument matrix corresponding to this database is shown in table 1 for terms occurring inmore than one document. the individual cell entries represent the frequency with which a term occurs in adocument. in many information retrieval applications these frequencies are transformed to reflect the ability ofwords to discriminate among documents. terms that are very discriminating are given high weights andundiscriminating terms are given low weights. note also the large number of 0 entries in the matrixmost wordsdo not occur in most documents, and most documents do not contain most wordsinformation retrieval: finding needles in massive haystacks24massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.table 1 sample termbydocument matrix (12 terms × 9 documents)c1c2c3c4c5m1m2m3m4human100100000interface101000000computer110000000user011010000system011200000response010010000time010010000eps001100000survey010000001trees000001110graph000000111minors000000011consider a user query about "human computer interaction", using the oldest and still most commonboolean retrieval method, users specify the relationships among query terms using the logical operators and,or and not, and documents matching the request are returned. more flexible matching methods which allowfor graded measures of similarity between queries and documents are becoming more popular. vector retrieval,for example, works by creating a query vector and computing its cosine or dot product similarity to the documentvectors (salton and mcgill, 1983; van rijsbergen, 1979). the query vector for the query "human computerinteraction" is shown in the table below.table 2. query vector for "human computer interaction", and matching documentsqueryc1c2c3c4c5m1m2m3m41human1001000000interface1010000001computer1100000000user0110100000system0112000000response0100100000time0100100000eps0011000000survey0100000010trees0000011100graph0000001110minors000000011this query retrieves three documents about humancomputer interaction (c1, c2 and c4) which could beranked by similarity score. but, it also misses two other relevant documents (c3 and c5) because the authorswrote about users and systems rather than humans and computers. even the more flexible vector methods arestill wordbased and plagued by the problem of verbal disagreement.information retrieval: finding needles in massive haystacks25massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.a number of methods have been proposed to overcome this kind of retrieval failure including: restrictedindexing vocabularies, enhancing user queries using thesauri, and various ai knowledge representations. thesemethods are not generally effective and can be timeconsuming. the remainder of the paper will focus on apowerful and automatic statistical method, latent semantic indexing, that we have used to uncover usefulrelationships among terms and documents and to improve retrieval.3.0 latent semantic indexing (lsi)details of the of the lsi method are presented in deerwester et al. (1990) and will only be summarizedhere. we begin by viewing the observed termbydocument matrix as an unreliable estimate of the words thatcould have been associated with each document. we assume that there is some underlying or latent structure inthe matrix that is partially obscured by variability in word usage. there will be structure in this matrix in so faras rows (terms) or columns (documents) are not independent. it is quite clear by looking at the matrix that thenonzero entries cluster in the upper left and lower fight comers of the matrix. unlike wordmatching methodswhich assume that terms are independent, lsi capitalizes on the fact that they are not.we then use a reduced or truncated singular value decomposition (svd ) to model the structure in thematrix (stewart, 1973). svd is closely related to eigen decomposition, factor analysis, principle componentsanalysis, and linear neural nets. we use the truncated svd to approximate the termbydocument matrix usinga smaller number of statistically derived orthogonal indexing dimensions. roughly speaking, these dimensionscan be thought of as artificial concepts representing the extracted common meaning components of manydifferent terms and documents. we use this reduced representation rather than surface level word overlap forretrieval. queries are represented as vectors in the reduced space and compared to document vectors.an important consequence of the dimension reduction is that words can no longer be independent; wordswhich are used in many of the same contexts will have similar coordinates in the reduced space. it is thenpossible for user queries to retrieve relevant documents even when they share no words in common. in theexample from section 2, a twodimensional representation nicely separates the humancomputer interactiondocuments from the graph theory documents. the test query now retrieves all five relevant documents and noneof the graph theory documents.in several tests, lsi provided 30% improvements in retrieval effectiveness compared with the comparableword matching methods (deerwester et al., 1990; dumais, 1991). in most applications, we keep k~100400dimensions in the reduced representation. this is a large number of dimensions compared with most factoranalytic applications! however, there are many fewer dimensions than unique words (often by several orders ofmagnitude) thus providing the desired retrieval benefits. unlike many factor analytic applications, we make noattempt to rotate or interpret the underlying dimensions. for information retrieval we simply want to representterms, documents, and queries in a way that avoids the unreliability, ambiguity and redundancy of individualterms as descriptors.information retrieval: finding needles in massive haystacks26massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.a graphical representation of the svd is shown below in figure 1. the rectangular termbydocumentmatrix, x, is decomposed into the product of three matricesx = t0 s0 d0', such that t0 and d0 have orthonormalcolumns, s0 is diagonal, and r is the rank of x. this is the singular value decomposition of x. t0 and d0 are thematrices of left and right singular vectors and s0 is the diagonal matrix of singular values which by conventionare ordered by decreasing magnitude.figure 1. graphical representation of the svd of a termbydocument matrix.recall that we do not want to reconstruct the termbydocument matrix exactly. rather, we want anapproximation that captures the major associational structure but at the same time ignores surface levelvariability in word choice. the svd allows a simple strategy for an optimal approximate fit. if the singularvalues of s0 are ordered by size, the first k largest may be kept and the remainder set to zero. the product of theresulting matrices is a matrix  which is only approximately equal to x, and is of rank k (figure 2).the matrix  is the best rankk approximation to x in the least squares sense. it is this reduced model thatwe use to approximate the data in the termbydocument matrix.we can think of lsi retrieval as word matching using an improved estimate of the termdocumentassociations using , or as exploring similarity neighborhoods in the reduced kdimensional space. it is the latterrepresentation that we work witheach term and each document is represented as a vector in kspace. to processa query, we first place a query vector in kspace and then look for nearby documents (or terms).information retrieval: finding needles in massive haystacks27massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.figure 2. graphical representation of the reduced or truncated svd4.0 using lsi for information retrievalwe have used lsi on many text collections (deerwester, 1990; dumais, 1991; dumais, 1995). table 3summarizes the characteristics of some of these collections.table 3. example information retrieval data setsdatabasendocsnterms(>1 doc)nonzerosdensitycpu for svd; k=100med1033583152012.86%2 minstm653516637327244.30%10 minsency30473757143071994.13%60 minstrecsample685598745713962041.23%2 hrstrec74233151225181901331.02%ššconsider the med collection, for example. this collection contains 1033 abstracts of medical articles and isa popular test collection in the information retrieval research community. each abstract is automatically analyzedinto words resulting in 5831 terms which occur in more than one document. this generates a 1033 × 5831matrix. note that the matrix is very sparsefewer than 1% of the cells contain nonzero values. the cell entriesare typically transformed using a term weighting scheme. wordmatching methods would use this matrix. forlsi, we then compute the truncated svd of the matrix keeping the k largest singular values and thecorresponding left and fight singular vectors. for a set of 30 test queries, lsi (with k=100) is 30% better than thecomparable wordmatching method (i.e., using the raw matrix with no dimension reduction) in retrievingrelevant documents and omitting irrelevant ones.the most time consuming operation in the lsi analysis is the computation of the truncated svd. however,this is a one time cost that is incurred when the collection is indexed andinformation retrieval: finding needles in massive haystacks28massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.not for every user query. using sparseiterative lanczos code (berry, 1992) we can compute the svd for k=100in the med example in 2 seconds on a standard sun sparc 10 workstation.the computational complexity of the svd increases rapidly as the number of terms and documentsincreases, as can be seen from table 3. complexity also increases as the number of dimensions in the truncatedrepresentation increases. increasing k from 100 to 300 increases the cpu times by a factor of 910 comparedwith the values shown in table 3. we find that we need this many dimensions for large heterogeneouscollections. so, for a database of 68k articles with 14 million nonzero matrix entries, the initial svd takes about20 hours for k=300. we are quite pleased that we can compute these svds with no numerical or convergenceproblems on standard workstations. however, we would still like to analyze larger problems more quickly.the largest svd we can currently compute is about 100,000 documents. for larger problems we run intomemory limits and usually compute the svd for a sample of documents. this is represented in the last two rowsof table 3. the trec data sets are being developed as part of a nist/arpa workshop on information retrievalevaluation using larger databases than had previously been available for such purposes (see harman, 1995). thelast row (trec) describes the collection used for the adhoc retrieval task. this collection of 750k documentscontains about 3 gigabytes of ascii text from diverse sources like the apnews wire, wall street journal, ziffdavis computer select, federal register, etc. we cannot compute the svd for this matrix and have had tosubsample (the next to last row, trecsample). retrieval performance is quite good even though the reducedlsi space is based on a sample of less than 10% of the database (dumais, 1995). we would like to evaluate howmuch we loose by doing so but cannot given current methods on standard hardware. while these collections arelarge enough to provide viable test suites for novel indexing and retrieval methods, they are still far smaller thanthose handled by commercial information providers like dialog, mead or westlaw.5.0 some open statistical issueschoosing the number of dimensions. in choosing the number of dimensions to keep in the truncated svd,we have to date been guided by how reasonable the matches look. keeping too few dimensions fails to captureimportant distinctions among objects; keeping more dimensions than needed introduces the noise of surface levelvariability in word choice. for information retrieval applications, the singular values decrease slowly and wehave never seen a sharp elbow in the curve to suggest a likely stopping value. luckily, there is a range of valuesfor which retrieval performance is quite reasonable. for some test collections we have examined retrievalperformance as a function of number of dimensions. retrieval performance increases rapidly as we move fromonly a few factors up to a peak and then decreases slowly as the number of factors approaches the number ofterms at which point we are back at wordmatching performance. there is a reasonable range of values aroundthe peak for which retrieval performance is well above word matching levels.information retrieval: finding needles in massive haystacks29massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.size and speed of svd. as noted above, we would like to be able to compute large analyses faster. since thealgorithm we use is iterative the time depends some on the structure of the matrix. in practice, the complexityappears to be o(4*z + 3.5*k), where z is the number of nonzeros in the matrix and k is the number ofdimensions in the truncated representation. in the previous section, we described how we analyze largecollections by computing the svd of only a small random sample of items. the remaining items are "folded in"to the existing space. this is quite efficient computationally, but in doing so we eventually loose representationalaccuracy, especially for rapidly changing collections.updating the svd. an alternative to folding in (and to recomputing the svd) is to update the existing svdas new documents or terms are added. we have made some progress on methods for updating the svd (berry etal., 1995), but there is still a good deal of work to be done in this area. this is particularly difficult when a newterm or document influences the values in other rows or columnse.g., when global term weights are computed orwhen lengths are normalized.finding near neighbors in high dimensional spaces. responding to a query involves finding the documentvectors which are nearest the query vector. we have no efficient methods for doing so and typically resort tobrute force, matching the query to all documents and sorting them in decreasing order of similarity to the query.methods like kdtrees do not work well in several hundred dimensions. unlike the svd which is computed once,these query processing costs are seen on every query. on the positive side, it is trivial to parallelize the matchingof the query to the document vectors by putting subsets of the documents on different processors.other models of associative structure. we chose a dimensional model as a compromise betweenrepresentational richness and computational tractability. other models like nonlinear neural nets or overlappingclusters may better capture the underlying semantic structure (although it is not at all clear what the appropriatemodel is from a psychological or linguistic point of view) but were computationally intractable. clustering time,for example, is often quadratic in the number of documents and thus prohibitively slow for large collections. fewresearchers even consider overlapping clustering methods because of their computational complexity. for manyinformation retrieval applications (especially those that involve substantial end user interaction), approximatesolutions with much better time constants might be quite useful (e.g., cutting et al., 1992).6.0 conclusionsinformation retrieval and filtering applications involve tremendous amounts of data that are difficult tomodel using formal logics such as relational databases. simple statistical approaches have been widely applied tothese problems for moderatesized databases with promising results. the statistical approaches range fromparameter estimation to unsupervised analysis of structure (of the kind described in this paper) to supervisedlearning for filtering applications. (see also lewis, this volume.) methods for handling more complex modelsand for extending the simple models to massive data sets are needed for a wide variety of real world informationaccess and management applications.information retrieval: finding needles in massive haystacks30massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.7.0 referencesbates, m.j. subject access in online catalogs: a design model. journal of the american society for information science, 1986, 37(6), 357376.berry, m. w. large scale singular value computations. international journal of supercomputer applications, 1992, 6, 1349.berry, m. w. and dumais, s. t. using linear algebra for intelligent information retrieval. siam: review, 1995.berry, m. w., dumais, s. t. and o'brien, g. w. the computational complexity of alternative updating approaches for an svdencodedindexing scheme. in proceedings of the seventh siam conference on parallel processing for scientific computing, 1995.caid, w. r, dumais, s. t. and gallant, s. i. learned vector space models for information retrieval. information processing andmanagement , 1995, 31(3), 419429.cutting, d. r., karger, d. r., pederson, j. o. and tukey, j. w. scatter/gather: a clusterbased approach to browsing large documentcollections. in proceedings of acm: sigir'92, 318329.deerwester, s., dumais, s. t., landauer, t. k., furnas, g. w. and harshman, r. a. indexing by latent semantic analysis. journal of thesociety for information science, 1990, 41(6), 391407.dumais, s. t. improving the retrieval of information from external sources. behavior research methods, instruments and computers, 1991,23(2), 229236.dumais, s. t. using lsi for information filtering: trec3 experiments. in: d. harman (ed.), overview of the third text retrievalconference (trec3). national institute of standards and technology special publication 500225, 1995, pp.219230.furnas, g. w., landauer, t. k., gomez, l. m. and dumais, s.t. the vocabulary problem in humansystem communication.communications of the a cm, 1987, 30(11), 964971.lewis, d. information retrieval and the statistics of large data sets , [this volume].d. harman (ed.), overview of the third text retrieval conference (trec3). national institute of standards and technology specialpublication 500225, 1995.salton, g. and mcgill, m.j. introduction to modem information retrieval . mcgrawhill, 1983.stewart, g. w. introduction to matrix computations. academic press, 1973van rijsbergen, c.j. information retrieval. buttersworth, london, 1979information retrieval: finding needles in massive haystacks31massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.information retrieval: finding needles in massive haystacks32massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.statistics and massive data sets: one view from the socialsciencesalbert f. andersonpublic data queries, inc.abstractgenerating a description of a massive dataset involves searching through an enormous space ofpossibilities. artificial intelligence (ai) may help to alleviate the problem. ai researchers are familiar with largesearch problems and have developed a variety of techniques to handle them. one area in particular. ai planning,offers some useful guidance about how the exploration of massive datasets might be approached. we describe aplanning system we have implemented for exploring small datasets, and discuss its potential application tomassive datasets.1 introductionthe computing resources available within a university have become determining forces in defining theacademic and research horizons for faculty, students, and researchers. over the past half century, mechanicalcalculators have been replaced by batchoriented mainframe processors which evolved into interactive hostsserving first hardcopy terminals, then intelligent display terminals, and finally pcs. today, high performanceworkstations integrated within local and external networks have brought unprecedented computing power to thedesktop along with interactive graphics, mass storage capabilities, and fingertip access to a world of computingand data resources. each transition has opened new opportunities for those who work with demographic, social,economic, behavioral, and health data to manage larger quantities of data and to apply more sophisticatedtechniques to the analysis of those data.social scientists, however, have not realized the potential of this revolution to the extent that it has beenrealized by their colleagues in the physical, natural, engineering, and biomedical sciences. distributed computingenvironments encompassing local and externalstatistics and massive data sets: one view from the social sciences33massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.networks could provide high speed access to a wide range of mass data using lowcost, high speed, online massstorage coupled to high performance processors feeding graphics workstations. social scientists, scholars,planners, students. and even the public at large could access, manage, analyze, and visualize larger data setsmore easily using a broader range of conventional and graphics tools than heretofore possible. but most of thosewho use these data today work with the data in ways remarkably similar to methods used by their predecessorsthree decades ago. large data sets comprising records for thousands and even millions of individuals or otherunits of analysis have been, and continue to be, costly to use in terms of the dollars, time, computing facilities,and technical expertise required to handle them. these barriers can now be removed.this paper looks briefly at the nature of the problem, the opportunities offered by computing andinformation system technology, and at one effort that has been made to realize the potential of theseopportunities to revolutionize the manner in which massive census and survey data sets are handled. as this oneexample illustrates, realization of these opportunities has the potential for more than just a change of degree interms of numbers of users, ease of use, and speed of response. users of demographic, social, economic,behavioral, health, and environmental data can experience a qualitative change in how they work, interactingwith data and tools in ways never before possible.2 the problemdemographers, social scientists, and others who work with census and survey data are often faced with thenecessity of working with data sets of such magnitude and complexity that the human and technologicalcapabilities required to make effective use of the data are stretched to their limitsand often beyond. even today,researchers may find it necessary to coordinate three or more layers of support personnel to assist them with theirefforts to retrieve information from data sets ranging to gigabytes (gb) in size. yet, these data are among themost valuable resources available for gaining insight into the social processes that are changing our world. thesechallenges are compounded today by the recognition that many of our pressing local, national, and internationalproblems require multidisciplinary approaches if the problems are to be understood and resolved. the success ofthese multidisciplinary endeavors will depend in part upon how readily and how effectively researchers andanalysts from the social sciences, environment, public policy, and public health can bring data from theirdisciplines, along with geographic and topological data, to bear on these problems.consequently, public data such as the public use microdata samples (pums). current population surveys(cps). american housing surveys (ahs), census summary tape files (stf), and national center for healthstatistics mortality files are of greater potential value to a broader range of researchers, scholars, students, andplanners than ever before. yet, these are data that, because of the cost and difficulty in working with them, havehistorically been underutilized relative to their potential to lend insight into social, economic, political, historical,health, and educational issues. these are data that are relevant at levels ranging from personal to global concerns.unlocking information requires more than just access to data. getting answers to evenstatistics and massive data sets: one view from the social sciences34massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.simple questions provides significant challenges to users of multigigabyte data sets. the challenges becomemuch greater when more complex questions are askedquestions that require the construction of indices within oracross records, matching and merging information across data sets, and displaying data graphically orgeographically. for example, to gain insight into college enrollment rates, one might wish to create an index foreach child in the pums representing the total years of siblings college attendance that would overlap with thegiven child college years, assuming that each child were to attend college for four years starting at the same age.such an index could reflect the economic pressure placed on a family by having multiple children in college atthe same time. availability of such an index could immediately suggest a host of questions and possible avenuesof inquiry to be pursuedfor example, establishing from other data sources the validity of the index as a predictorof college attendance or examining how the index varies over household and family characteristics such as therace, education, occupation, and age cohort of the head and spouse or the family structure within the pums data.one might also wish to generate thematic maps of the distribution of relationships within local, regional, ornational contexts. to investigate such questions today would require access to costly computing and dataresources as well as significant technical expertise. the task would challenge an accomplished scholar workingin a well endowed research center.challenges exist on the technology side, also. the optimal application of mass storage, high performanceprocessors, and high speed networks to the task of providing faster, cheaper, and easier access to mass datarequires that strategies for using parallel and multiple processing be developed, data compression techniques beevaluated, overall latencies within the system be minimized, advantage be taken of the internet for sharingresources, etc. in the case of latencies, for example, ten second startup and communication latencies are of littleconsequence to a five hour task, but a severe bottleneck for a one second task.3 the opportunitycreative application of currently available computing and information technology can remove the obstaclesto the use of massive demographic, social, economic, environmental, and health data sets while also providinginnovative methods for working with the data. tasks related to accessing, extracting, transforming, analyzing,evaluating, displaying, and communicating information can be done in seconds and minutes rather than thehours, days, and even weeks that users have faced in the past. networks can allow resources too costly to bejustified for a small number of users within a local context to be shared regionally, nationally, or eveninternationally. the models for sharing access to specialized instruments that have worked well in the physical,natural, and medical sciences can be applied equally well to the social sciences.dedicated parallel and multiple processing systems have the potential to essentially eliminate the i/o andprocessing bottlenecks typically associated with handling files containing millions of data records. servingsystems based on closely coupled high performance processors can, in a fraction of a second, reduce massivedata to tabulations, variancecovariance matrices, or other summary formats which can then be sent to desktopclients capable of merging, analyzing, and displaying data from multiple sources. bootstrap and jackknifestatistics and massive data sets: one view from the social sciences35massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.procedures can be built into the systems to provide estimates of statistical parameters. distributing the taskbetween remote servers and desktop processors can minimize the quantity of information that must be movedover networks.the rapidly falling cost of high performance systems relative to their performance is significantly reducingthe hardware costs associated with creating dedicated facilities optimized for the handling of massive census andsurvey data.4 one path to an answera collaborative effort involving researchers at the population studies center (psc) at the university ofmichigan and the consortium for international earth science information network (ciesin) at saginaw.michigan, led to a demonstration in 1993 of the prototype data access system providing interactive access via theinternet to the 1980 and 1990 land person records per file. the prototype, named xplore, as subsequentlystimulated the development of the ulysses system at ciesin and a commercial system, dqexplore, y publicdata queries. inc. users of the ciesin facilities, who currently number more than 1,000 researchers, scholars,analysts, planners, news reporters, and students around the world, can readily generate tables from these data setsin seconds. the prototype ran on a loosely clustered parallel system of eight hp 735 workstations. the use ofmore efficient algorithms in the new designs is allowing better performance to be achieved using fewerprocessors. other data sets are being added to the system. the prototype system has also been demonstrated onlarger parallel processing systems. ibm sp1/sp2s, to provide interactive access to the 1990 5 represent arealization of the promise of high performance information and computing technology to minimize, if noteliminate, the cost in terms of dollars, time, and technical expertise required to work with the pums and similarlarge, complex data sets.the explore prototype was designed by albert f. anderson and paul h. anderson to perform relativelysimple operations on data, but to do so very quickly, very easily, and through the collaboration with ciesin, atvery low cost for users. multidimensional tabulations may be readily generated as well as summary statistics onone item within the crosscategories of others. some statistical packages provide modes of operation that areinteractive or that allow the user to chain processes in ways that, in effect, can give interactive access to data, butnot to data sets the size of the pums and not with the speed and ease possible with explore. thirty years ago,interacting with data meant passing boxes of cards through a card sorter againand again, and again and... morerecently, users often interacted with hundreds, even thousands, of pages of printed tabular output, havingproduced as much information in one run as possible to minimize overall computing time and costs. the exploreapproach to managing and analyzing data sets allows users to truly interact with massive data. threads of interestcan be pursued iteratively. users can afford to make mistakes. access to the prototype and to ulysses on theciesin facilities has clearly succeeded in letting users access the 1990 pums data within an environment thatreduces their costs in using such data to such an extent that they are free to work and interact with the data inways never before possible.the current development effort at public data queries, inc., is funded by small business research anddevelopment grants from the national institutes of health (nih)specificallystatistics and massive data sets: one view from the social sciences36massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.the national institute for child health and human development (nichd) the pdqexplore system combineshigh speed data compression/uncompression techniques with efficient use of single level store file i/o to makeoptimum use of the available disk, memory, and hardware architecture on the host hardware. more simply, theactive data are stored in ram while they are in use and key portions of the program code are designed to be heldin the onchip instruction caches of the processors throughout the computing intensive portions of systemexecution. as a consequence, execution speeds can in effect be increased more than one thousand fold overconventional approaches to data management and analysis. because the task is by nature highly parallel, thesystem scales well to larger numbers of higher performance processors. public data queries. inc., is currentlyinvestigating the applicability of symmetric multiprocessing (smp) technology to the task with the expectationthat more complex recoding, transformation, matching/merging, and analytic procedures can be accommodatedwhile improving performance beyond present levels.current implementations on hp, ibm, and intel pentium systems allow records to be processed at rates onthe order of 300,000500,000 per second per processor for data held in ram. processing times are expected tobe reduced by at least a factor of two, and probably more, through more efficient coding of the server routines.system latencies and other overhead are expected to reduce to milliseconds. expectations are that within oneyear, the system could be running on servers capable of delivering tabulations in a fraction of a second from the5 data, more than 18 million person and housing records.for information on the ciesin ulysses system, contact: info@ciesin.orgstatistics and massive data sets: one view from the social sciences37massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.statistics and massive data sets: one view from the social sciences38massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.the challenge of functional magnetic resonance imagingwilliam f. eddy*mark fitzgerald**christopher genovese***carnegie mellon universityaudris mockus****bell laboratories(a division of lucent technologies)1 introductionfunctional magnetic resonance imaging (fmri) is an extremely promising and rapidly developingtechnique used by cognitive neuropsychologists to obtain images of the active human brain. images are obtainedwhile the subject is engaged in a set of cognitive tasks designed to isolate specific brain functions, and thepsychologists attempt to use the observed patterns of neural activation to understand and localize these functions.in contrast to traditional methods for mapping brain function, fmri is noninvasive and allows the study of highlevel cognitive processes such as language, visual attention, and problem solving. since fmri involves noknown toxicity, each subject can be imaged many times, which improves precision and facilitates moresophisticated analyses. as such. fmri promises to play a vital role in discerning the functional organization ofthe brain.2 functional magnetic resonance imaging (fmri)recent developments in magnetic resonance imaging (mri) have greatly increased the speed with whichimages of the human brain can be formed and this makes it suitable* professor of statistics. partially supported by onr contract n0001491j1024, and nsf grants ibn9418982 anddms9505007.** graduate student. partially supported by nih grant mh15758.*** assistant professor. partially supported by nsf grant dms9505007.**** member of technical staff, bell laboratories. partially supported by the center for the neural basis of cognition.the challenge of functional magnetic resonance imaging39massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.for studying brain functions. an mr scanner is a multimillion dollar device which, by subjecting its contents tocarefully modulated magnetic fields and recording the resulting radio signal, produces the fourier transform ofthe magnetic field spin density for a particular atomic isotope. then computing the inverse fourier transform ofthe digitized signal reveals an image of the (magnetic field spin density of the) contents of the scanner.without going into the detailed physics and neurobiology that relate the magnetic field to brain activity,suffice it to say that increased neuronal activity induces an increase in blood flow to the region of activity (todeliver glucose to the neurons). this increased flow results in an increase of oxygenated blood in the small veinsthat drain the active region because the increased activity does not require much extra oxygen. the more oxygencarried by the hemoglobin in the blood the smaller the magnetic field generated by the iron in the hemoglobin(the oxygen acts as a magnetic shield) and consequently the less interference with the local magnetic fieldgenerated by, e.g., hydrogen nuclei (protons). by mid1991 researchers had demonstrated that mri can detectthe changes in blood oxygenation caused by brain function and consequently the technique is known as fmri.among the first studies to use mri to assess functional neural activity in humans are [1,2, 3]. the latter twointroduced the now common blood oxygenation level dependent (bold) technique just described forcharacterizing activation in the brain.there are several important features of fmri compared to other imaging techniques. first, the signal comesdirectly from functionally induced changes. second, it provides both functional and anatomical information.third the spatial resolution is on the order of 1 or 2 millimeters. fourth, there is little known risk from fmri.finally, the change in signal due to brain activity is quite small (on the order of 1%) and, in particular, smallerthan the noise (on the order of 2%). this last feature means that, utilizing current technology, it is necessary toaverage a large number of images in order to detect the regions of activation.3 a typical experimentthe simplest fmri experiment entails the performance of two cognitive tasks which differ in some specificdetail. a number of images are gathered during each task and averaged within task. the difference between theaverage images for the two tasks provides information about the location in the brain of the cognitive functionrepresented by the difference of the two tasks.an actual experiment might proceed as follows. a subject lies in the mri magnet with a head restraintintended to minimize movement. a set of preliminary anatomical images are studied to determine the locationwithin the brain where the repeated functional images will be taken. the subject practices each of the two tasksfor about a minute each, responding to the task, for example, by pushing a button with the right thumb. thesubject performs one of the tasks repeatedly while images are recorded and then switches to the other task. insome of our smaller experiments we are recording 100 images for each task. in order to eliminate leftrighteffects the entire experiment is repeated with the subject using the left thumb to respond. thus there are a total of400 images in this simple experiment. it takes the scanner less than 20 minutes to acquire this amount of data.the acquired images are multislice images with, typically, seven slices: each slice isthe challenge of functional magnetic resonance imaging40massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.128×128 voxels with voxel dimensions roughly 2mm × 2mm × 7mm. because most of the work we have done todate has been on the twodimensional slices of these images we will henceforth think in terms of the7×400=2800 individual slices.4 data processingthe processing of the 2800 slices from this small experiment in order to detect the regions of activation is amassive task. the raw data is 128×128×2800 32bit words which occupies 256mb of disk storage. simplymoving this amount of data around is a timeconsuming task. currently, the available bandwidth between themr scanner and the workstation where we perform the processing is under 200k bytes per second; thus itrequires nearly 30 minutes to simply move the data for this small experiment to our workstation. there are plansin place to substantially increase the bandwidth by the end of this calendar year.currently, the actual data processing is roughly as follows. we begin with an adjustment to account forinhomogeneity in the main static magnetic field. (we expect, in the future, to implement a further adjustment toaccount for nonlinearity in the secondary dynamic magnetic field.) then, we perform a ''baseline'' adjustment tocorrect for miscalibration of the analogtodigital converter. (we expect, in the future, to implement a further"jitter" adjustment to account for very small errors in the timing of the data acquisition.) then we perform a"mean" adjustment to correct for uncontrolled drift in the signal strength. (we expect, in the future, to implementa further pixelwise detrending to account for local drift within the image.) then we perform an "outlier"adjustment to correct for shot noise. (we expect, in the future, to implement more sophisticated methods foraddressing the fact that the data do not follow a gaussian distribution.) we refer to the data at this point in theprocessing as the corrected data.unfortunately, because of the length (in time) of an experiment, the subject will almost certainly move. weaddress that problem both through the use of a head clamp and through a motioncorrection procedure. wecalculate the inverse fourier transform to produce an image for the purposes of estimating the motion. ourmotion correction procedure is complicated: first, by a nonlinear optimization technique we estimate the amountof movement required to align each image and, second, we adjust the corrected data to account for thismovement. we then calculate the inverse fourier transform of the corrected and motioncorrected data toproduce the actual image. at this point we are ready to begin what is called the "statistical" analysis of the data.the average is computed within slices within tasks and then the difference between tasks within slice iscalculated. finally, a statistical test is performed on each of the resulting differences to determine the regions ofactivation. depending on the computing power of the workstation performing the calculations and depending onthe precise details of the calculations, this can take anywhere from several days down to about twelve hours ofprocessing time.our data processing software is designed as a processing pipeline of separate programs, this has the greatadvantage that modules can be easily interchanged if this will benefit the results. also, new intermediate stepscan be easily inserted at any stage. there is some disadvantage in that the act of storing intermediate results canconsume considerable time. nonetheless, we feel quite strongly that keeping the processing highly modularizedis verythe challenge of functional magnetic resonance imaging41massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.beneficial because of the flexibility it provides.5 statistical challengesthe statistical challenges in the analysis of fmri data are difficult and manifold. they all revolve aroundour understanding the nature of the noise and its effect on successfully detecting regions of activation. there aretwo general approaches to dealing with the noise in fmri experiments. the first is to try to remove the source ofthe noise: we pursue this approach aggressively. the second is to model the noise through statistical methods:we also pursue this approach aggressively. we believe that both approaches are absolutely necessary.noise arises from a variety of sources. a fundamental source of noise is the vibration of the atomic nuclei inthe imaged material. this cannot be reduced except by lowering the temperature toward absolute zero.unfortunately, this noise is not spatially or temporally homogeneous but depends on both the anatomicalstructure and the function we are trying to detect. inhomogeneity of the magnetic field, mechanical vibration,temperature instability of the electronics, etc., are all machinebased sources of noise. the machinemaintenancetechnicians work to limit these sources. the details of how the magnetic field is modulated to produce an image(known as a pulse sequence) effect the noise; we are engaged in studies to assess the relationship.physiological processes of the body such as respiration, heartbeat, and peristalsis effect the signal in waysthat, in principle, can be modeled. we have begun planning experiments to gather data which might allow us tosuccessfully model the cardiac and respiratory cycles because our more experienced colleagues believe that thisis one of the primary sources of noise. such an experiment is going to require synchronized recording of manyimages and the associated cardiac and respiratory information. this will be followed by a modelling effort whichwill view the sequence of images as the dependent variable and the cardiac and respiratory variables aspredictors. unfortunately, there is an interaction between the pulse sequence and the noise caused byphysiological processes. this effort will thus require a family of models for each pulse sequence.movement of the subject between images is another source of noise. the standard algorithm for imageregistration in functional neuroimaging, called air [4]. works on reconstructed images. it is extremelycomputationally intensive: registration of images obtained from a single experiment can take as much as 24hours of computer time. subject movement appeared to us to be the simplest of the sources to understand andaddress. we have developed an alternative algorithm [6] for registering the images which operates in the fourierdomain. this method has proven to be more accurate than air, less prone to artifacts, and an order of magnitudemore efficient. by differentially weighting regions in the fourier domain, the method can also be made lesssensitive to spurious signals that have a strong influence on image domain techniques. it is also readilygeneralizable to threedimensional image registration, although we have not yet completed that work.finally, there is subject to subject variation. we have not yet focused on this question simply because theexperimenters focus their experiments on individual subjects.all of these sources affect our ability to detect regions of activation. when we began this work, activevoxels were being detected by performing an independent ttest on each of thethe challenge of functional magnetic resonance imaging42massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.16384 voxels in an image. we were approached with the question: how should we correct for the "multiplecomparisons?" bonferroni corrections do not result in any "significant" voxels. ultimately we will have to builda complex spatialtemporal model of the images which allows us to answer the real question: where are theactive regions?we have developed another approach [5] for identifying active regions, which is called the contiguitythreshold method. the idea is to increase the reliability of identification by using the fact that real activationtends to be more clustered than artifactual activation caused by noise. empirical evidence strongly suggests thatthis method provides a significant improvement in sensitivity. of course, although it is more robust than voxelwise tests, this method, too, depends on simplistic assumptions; we intend it as a stopgap measure, to beeventually supplanted by more sophisticated analyses.6 computational challengesthere are three important aspects of the computation. first, the amount of data from a large experimentapproaches 1 gb. any computations on a data set of this size require considerable time on a workstation.second, there are no sensible ways to reduce the data during the earlier processing steps to speed up theprocessing. third, because most of the computations are done on an imagebyimage basis (or even on a slicebyslice basis), there is a tremendous opportunity to speed things up with parallel or distributed methods.currently, our standard processing does not take advantage of the inherent parallelism. however, we havejust begun experimenting (on our local network of workstations) with parallel virtual machine (pvm)implementations of some of the most timeconsuming steps in the processing. simultaneously, we have begunplans to move the computations to a cray t3d with 512 processors. in addition to just wanting to get thingsdone faster, another reason for this plan is that we would like to perform the computations while the subject ofthe experiment is in the scanner and use the results as a guide for further experimentation during the samescanning session.7 discussionwe have begun a serious effort to study and improve the statistical methodology of fmri. and we havemade some important preliminary steps.one of the most fundamental questions about fmri experiments is the question of reproducibility. if we runthe experiment a second time immediately following the first with no intervening time, how similar will theresults be? if we wait for a period of time? if we remove the subject from the scanner? if we repeat theexperiment next month? we have begun to address this question; our preliminary results are reported in [8].the analysis of functional magnetic resonance imaging data can in many ways be viewed a prototype for aclass of statistical problems that are arising more and more frequently in applications: namely, large data setsderived from a complex process with both spatial and temporal extent. there is a wealth of opportunities for thedevelopment of new statistical methodologies, and many of these ideas will apply to a variety of problemsbeyond neuroimaging.the challenge of functional magnetic resonance imaging43massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.8 acknowledgementswe are indebted to a large number of colleagues at various local institutions. we list them herealphabetically both to thank them for their contributions to our work and to indicate the absolutely immense sizeof an undertaking of this kind. all of these people have made genuine contributions to our work either throughteaching us, collecting data for us, computer programming, providing problems to solve, or assisting us in ourwork in other ways. the list includes psychiatrists, psychologists, physicists, electrical engineers, computerscientists, statisticians, computer programmers, and technicians. the organizational codes are given in thisfootnote1.marlene behrman, ph.d., psychology, cmu; carlos betancourt, b.s., mrrc, upmc: fernando boada,ph.d., mrrc, upmc; todd braver. gs, psychology, cmu: patricia carpenter, ph.d., psychology, cmu:betty jean casey, ph.d., psychiatry, upmc: sam chang, gs, mrrc, upmc; jonathan cohen, m.d., ph.d.,psychology, cmu; wpic, upmc; denise davis, b.s., mrrc, upmc; michael decavalcante, undergraduate.cmu; steven forman, m.d., ph.d., vamc; wpic, upmc; joseph gillen, b.s., mrrc, upmc; nigelgoddard. ph.d., psc; mark hahn, b.s., lrdc. pitt; murali haran, undergraduate, cmu; marcel just, ph.d.,psychology, cmu; caroline kanet, undergraduate, cmu: timothy keller, ph.d., psychology, cmu; paulkinahan, ph.d., petrc, upmc; benjamin mccurtain, b.s., wpic, upmc; robert moore, m.d., ph.d.,wpic, upmc; thomas nichols, b.s., gs, statistics, cmu; petrc, upmc; douglas noll, ph.d., mrrc,upmc; leigh nystrom, ph.d., psychology, cmu; jennifer o'brien, b.s., gs, mrrc, upmc; brock organ,b.s., psychology, cmu; robert orr, b.s., psychology, cmu; julie price, ph.d., petrc, upmc; davidrosenberg, ph.d., wpic, upmc; waiter schneider, ph.d., lrdc, pitt; david servanschreiber, ph.d.,wpic, upmc; steven small, ph.d., wpic, upmc; john sweeney, ph.d., wpic, upmc; talin tasciyan,ph.d., mrrc, upmc; keith thulborn, m.d., ph.d., mrrc, upmc; david townsend, ph.d., petrc, upmc;james voyvodic, ph.d., mrrc, upmc: richard zemel, ph.d., psychology, cmu.1 cmu = carnegie mellon university; gs = graduate student; lrdc = learning research and development center:mrrc = magnetic resonance research center; petrc = positron emission tomography research center; pitt =university of pittsburgh; psc = pittsburgh supercomputer center; upmc = university of pittsburgh medical center;vamc = veterans administration medical center: wpic = western psychiatric institute and clinic.the challenge of functional magnetic resonance imaging44massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.references[1] belliveau, j.w., kennedy, d.n., mckinstry, r.c., buchbinder, b.r., weisskoff. r.m., cohen. m.s., vevea, j.m., brady, t.j., and rosen,b.r. (1991). "functional mapping of the human visual cortex by magnetic resonance imaging." science. 254, 716719.[2] kwong. k.k., belliveau, j.w., chesler, d.a., goldberg, i.e., weisskoff, r.m., poncelet, b.p., kennedy. d.n., hoppel, b.e., cohen,m.s., turner, r., cheng, h., brady, t.j., and rosen, b.r. (1992). "dynamic magnetic resonance imaging of human brain activityduring primary sensory stimulation," proc. natl. acad. sci. u.s.a., 89, .567.5.[3] ogawa, s., tank, d.w., menon, d.w., ellermann, j.m., kim, s., merkle, h., and ugurbil, k. (1992). "intrinsic signal changesaccompanying sensory stimulation: functional brain mapping using mri.'" proc. natl. acad. sci. u.s.a., 89, 59515955.[4] woods. r., cherry, s. and mazziotta, j. (1992). "rapid automated algorithm for aligning and reslicing pet images." journal ofcomputer assisted tomography. 16, 620633.[5] forman, s.d., cohen, j.d., fitzgerald, m., eddy, w.f., mintun, m.a., and noll d.c. (1995). "improved assessment of significant changein functional magnetic resonance imaging (fmri): use of a cluster size threshold," magnetic resonance in medicine. 33, 636647.[6] eddy, w.f., fitzgerald, m., and noll d.c: (1995). "fourier domain registration of mr images." submitted.[7] bandettini, p.a., jesmanowicz, a., wong, e.c.; and hyde, j.s. (1993). "processing strategies for timecourse data sets in functional mriof the human brain." magnetic resonance in medicine. 30, 161.[8] eddy, w.f., behrmann, m., carpenter, p.a., chang, s.y., gillen, j.s., just, m.a., keller, t.a., mockus, a., tasciyan, t.a., andthulborn, k.r. (1995). "testretest reproducibility during fmri studies: primary visual and cognitive paradigms. proceedings ofthe society for magnetic resonance, third scientific meeting, 843.the challenge of functional magnetic resonance imaging45massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.the challenge of functional magnetic resonance imaging46massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.marketingjohn schmitzinformation resources, inc.information resources is a forprofit corporation that was founded in 1979. the software part of it was anacquired company, originally founded in 1969. last year we sold about $350 million worth of data and software,with 3,500 employees. we collect consumer package goods data, the stuff that gets sold in grocery stores anddrugstores and places like kmart and walmart. we sell the data back to the manufacturers of those products sothat they can track and assess their marketing programs. we also sell it to the retailers, so that they can see howthey are doing.we use this information for sales support (that is, how do i sell more product?), for marketing (that is, whatproducts should i build and how should i support them and price them?), and also for logistical planning (that is,pushing stuff through the pipeline, and so forth.)our main database is called infoscan. it was founded in 1986. our expanded version of infoscan, called theri census, was an expansion of our sample from 3,000 to 15,000 stores. that took place at the beginning of lastyear.what we are in the business of doing is answering sales and marketing questions. most of our products aredeveloped and designed for interactive online use by end users who are basically sales people and planners. wedo that because our margins are higher on that than on our consulting work. on consulting work, we basically donot make any money. on interactive online syndicated products where we have to ship out tapes or diskettes, wemake fairly good margins.we are basically in the business of helping people sell groceries. for examplešwe went through this withpowdered laundry detergentšwe helped procter and gamble sell more tide. that is the only business that weare in. we do that by answering a group of questions for them. in a slightly simplified form, but not verysimplified, i can classify them into four groups.the first is tracking how i am doing? what are my sales like? what are the trends like? how am i doing interms of pricing? how much trade support am i getting? how many of my products axe being sold with displaysand advertising support?the other three questions are aimed at some causal analysis underneath. the first is what we generally callvariety analysis, that is, what products should i put in what stores? what flavors, what sizes? how much variety?does variety pay off or is it just a waste of space? the second is, what price should i try to charge? i say "try tocharge" because the manufacturers do not set the prices; the retailers do. the manufacturers have someinfluence, but they do not dictate it. the final area is what we call merchandising how much effort should i putinto trying to get a grocery store to put my stuff at the end of the aisle in a great big heap so that you trip over itand some of the items accidentally fall in your basket?anecdotally, displays are tremendously effective. for a grocery product we typically see that sales in aweek when there is an endofaisle display will be four or five or six times what they are normally.the main data that we collect is scanner data from grocery stores, drugstores, and mass merchandisers. ourunderlying database is basically very simple. it has three keys that indicatemarketing47massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.what store a product was sold in, what upc code was on the product, and what week k was sold. a lot of ourdata is now coming in dally rather than weekly.we have only a few direct measures: how many units of a product did they sell and how many pennies'worth of the product did they sell, and some flags as to whether it was being displayed or whether it was in afeature advertisement, and some other kinds of odd technical flags.we then augment that with a few derived measures. we calculate a baseline of sales by a fairly simpleexponential weighted moving average with some correction for seasonality to indicate what the deviations arefrom the baseline. we calculate a baseline price also, so we can see whether a product was being sold at a pricereduction. we calculate lift factors: if i sold my product and it was on display that week, how much of a riseabove normal or expected sales did i get because of the display. we impute that. we do it in a very simple wayby calculating the ratio of baseline sales to actual sales in weeks with displays. so you can imagine that this datais extraordinarily volatile.the data is reasonably clean. we spend an enormous amount of effort on quality assurance and we do haveto clean up a lot of the data. five to 15 percent of it is missing in any one week. we infer data for stores thatsimply do not get their data tapes to us in time.from this raw data we aggregate the data. we aggregate to calculate expected sales in boston, expectedsales for giant food stores in washington, d.c., and so on, using stratified sampling weights. we also calculateaggregate products. we take all of the different sales of tide 40ounce boxes and calculate a total for tide 40ounce, then calculate total tide, total procter and gamble, how they did on their detergent sales, and totalcategory.this is an issue that comes back to haunt us. there is a big tradeoff. if we do this precalculation at runtime, at analysis time, it biases the analysis, because all of these totals are precalculated, and it is very expensiveto get totals other than the ones that we precalculate.we also crosslicense to get data on the demographics of stores and facts about the stores. thedemographics of stores is simply census data added up for some defined trading area around the store. thesedata are pretty good. store factsšwe crosslicense thesešinclude the type of store (regular store or a warehousestore). the data is not very good; we are not happy with that data.our main database currently has 20 billion records in it with about 9 years' worth of collected data, of which2 years' worth is really interesting. nobody looks at data more than about 2 years old. it is growing at the rate ofabout 50 percent a year, because our sample is growing and we are expanding internationally. we currently add aquarter of a billion records a week to the data set.the records are 30, 40, 50 bytes each, and so we have roughly a terabyte of raw data, and probably threetimes that much derived data, aggregated data. we have 14,000 grocery stores right now, a few thousandnongrocery stores, generating data primarily weekly, but about 20 percent we are getting on a daily basis. ourproduct dictionary currently has 7 million products in it, of which 2 million to 4 million are active. there arediscontinued items, items that have disappeared from the shelf, and so forth.remember, we are a commercial company; we are trying to make money. our first problem is that ouraudience is people who want to sell tide. they are not interested in statistics. they are not even interested indata analysis, and they are not interested in using computers. they want to push a button that tells them how tosell more tide today. so in our case, a study means that a sales manager says, "i have to go to store x tomorrow,and i need to come up with a story for them. the story is, i want them to cut the price on the shelf, so i want topush a button that gives me evidence for charging a lower price for tide." they are also impatient; their standardfor a response time onmarketing48massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.computers is excel, which is adding up three numbers. they are not statistically or numerically trainedšthey aresales people. they used to have support staff. there used to be sales support staff and market researchers in thesecompanies, but they are not there anymore.analysis is their sideline to selling products, and so we have tried to build expert systems for them, withsome success early on. but when we try to get beyond the very basic stuff, the expert systems are hard to do.there are underlying statistical issues that in particular, i need to look for. on price changes, we think thatthere is a downwardsloping demand curve. that is, if i charge more, i should sell less, but the data does notalways say that, and so we have to do either some baysian calculations or impose some constraints.the databases are very large. something i alluded to earlier we are doing all these precalculations, so we areprojecting to calculate sales in washington through a projection array. we are aggregating up an aggregation treeto get some totals for category and so forth. we do this because it saves a whole lot of time at run times, so wecan get response times that are acceptable to people, and it saves a lot of space in the data, because we don't haveto put in all of the detail. but it forces me to examine the data in the way we have allowed based on theprecalculations. so we have a big tradeoff here. the relevant subtotal is, what is the total market for powderedlaundry detergent?those are all the nominal problems. what are the real problems? the real problem is that i have only twoprogrammers who work for me. the tools that we have at our disposal are pretty good, at least as a starting pointfor front ends. but on the back end, just using sql query against oracle or something simple is not fast enough.i do not have enough programmers to spend a lot of time on programming specialpurpose retrievers over andover again. i have competition for my staff from operational projects for relatively simple things that we knoware going to pay off. so time to spend on these interesting projects is being competed for by other projects.response time, particularly, is always a problem because of the questions that people ask, such as, what isthe effect of a price change going to be in hispanic stores if i increase the price of tide by 10 percent? they areguessing at what to ask, and so they are not willing to invest a great deal in these questions.the database setup time and cost are a problem. the setup time on these databases is mostly a "people"cost; it is not so much the computing time. it is getting people to put in all of the auxiliary data that we needaround the raw data. so i have difficulty with getting enough auxiliary information in there to structure theanalyses.discussioncarolyn carroll: when you say auxiliary data, what are you talking about?john schmitz: a lot of what we are doing is looking at competitive effects, for example. so when i amdoing an analysis on tide, i need to know who tide's competitors are. to a very limited extent you can do thatby looking at the data. to a large extent you have to have somebody go in and enter the list of competitivebrands. that is one basic thing.another is figuring out reasonable thresholds for defining exception points. a lot of that is manual. we startwith automated systems, but a lot of it has to be examined manually.marketing49massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.when i mention a lack of staff, it is not so much a lack of straight programmers, but people who know theprogramming technology, and people who also understand the subject matter well enough to not need a whole lotof guidance or extremely explicit specifications.stephen eick: so with your data, what are the privacy issues? i have noticed the few times i go to the storethat you now have store cards, and so the stores know everything i have bought; they know who i am; they knowmy historical buying pattern. i am sure they have squirreled all this data away in a database. i am expecting soonto show up at the store and be pitched with coupons as i walk in.schmitz: that has not happened to you yet: i am not being facetious; we do not currently run any programslike that, but there are programs of that nature.participant: i think since they know everything i have bought, they are going to start targeting me withcoupons. i personally resist, because i refuse to have a card. but others use every little coupon they can get.schmitz: there are two privacy issues. the privacy issue with our store audit database involves a contractthat we have with the grocery chains that we will not release data identified with specific individual stores. wewill not release sales data. so when we put out reports and so forth, we have to make sure that we haveaggregated to the extent that we do not identify individual stores and say how much of a product they have sold.the second privacy issue concerns individuals. we do have a sample of 100,000 u.s. households thatidentified themselves, and from whom we have a longitudinal sample that goes back anywhere from 3 to 10years. we release that data, but it is masked as to the individuals. we have demographics on the individuals, butwe do not identify them.eick: the other aspect of privacy involves the security camerasšat some point they are going to starttracking where i go in the store and what i look at. then when i buy it, they are going to know it was me. sothey are going to know not only what i bought, but also what i thought about buying.schmitz: security cameras are used not so much to track people through the stores as to indicate whenpeople are unhappy or happy about thingsšat hotels and so forth. we are not doing any of that yet.lyle ungar: are all your computations done offline, or do you do online calculations, and how complexare they? do you do factor analysis? do you run correlations with demographics?schmitz: we do factor analysis offline in order to reduce the dimensionalityšor principal componentsšrather than reduce the dimensionality on our demographic data. we do that offline and keep just componentweights. about the most complicated things we do online are some fairly simple regressions and correlationswith a little bit but not a whole lot of attention to robustization.marketing50massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.massive data sets: guidelines and practical experience fromhealth carecolin r. goodallhealth process management pennsylvania state universitynote: the author was previously affiliated with quadramed corp. and healthcare design systems.massive data sets: guidelines and practical experience from health care51massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.1 introductionfrom the moment of birth to the signing of the death certificate, medical records are maintained on almostevery individual in the united states (and many other countries). increasing quantities of data are abstractedfrom written records, or entered directly at a workstation, and submitted by providers of healthcare to payer andregulatory organizations. providers include physician offices, clinics, and hospitals, payers include managed carecorporations and insurance companies, and regulatory organizations include state and federal government.trends are towards making the flow of data easier, more comprehensive, and multifaceted: through edi(electronic data interchange), chins (community health information networks), and a seemingly ever moreintrusive, detailed, and specific involvement by payors in the handling of care and compensation by and forproviders.these socalled clinical and financial administrative health care data are routinely massive. the health carefinancing administration's annual medpar data base contains around 14 million discharge abstracts of everymedicarefunded acutecare hospital stay. individual state's administrative data of hospital discharges mayinclude several million records annually. data are collected in certain standard formats, including uniform billing(ub82, now ub92) for administrative data on hospital stays, and hedis (1.0, 2.0, 3.0) on patients in managedcare. the more detailed data is often proprietary: for example hedis data is often proprietary to the specificpayer organization, and includes data only on the organization's enrollees. more ambitious data collecting isunderway in selected locations, through the systematic abstraction of supplementary clinical measures of patienthealth from medical records, through recording of additional patient characteristics, or through recording of moredetailed financial information.in principal the entire medical record is available. a written version might be available online in digitalform as an image. record linkage, for example between members of a family (mother and child), or through theuse of unique patient identifiers across multiple episodes of treatment, or from administrative data to the registryof vital statistics (death certificates) and to cancer registries, provides important additional information.however, the availability of such information is, again, restricted.a traditional uses of health data is in public health assessment and the evaluation of clinical efficacy ofparticular treatments and interventions.massive data sets: guidelines and practical experience from health care52massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.these data are typically used differently: to analyze the system of health care delivery, seen as an aggregateof public and private, corporate and individual (physician) entities. these data are observational andcomprehensive, i.e. a census; thus questions of accuracy and specificity š the appropriateness of the data toaddress particular clinical and financial concerns š predominate over statistical issues of sampling, estimation,and even censoring of data. statistical tools may be relatively unsophisticated š with some exceptions (silber,rosenbaum and ross (1995), and harrell, lee, and mark (1995) and related papers) š but can be applied inmassive fashion. following a few further remarks to help set the scene, this paper is concerned withcharacterizing the interplay of statistical and computational challenges in analysis of massive data in healthcare.in a real sense, the pressures of the competitive market place are pushing through healthcare reform in amanner which the political process has, in recent years, been poorly equipped to mandate. an oftrepeatedcriticism of the us health care delivery system has been that its high quality is associated with very high costs. afrequent rejoinder has been that some form of rationing of healthcare would be necessary in order to reducecosts. and indeed, the criticism now appears muted, and accounts of high costs appear to have given way, in themedia, to accounts of the pitfalls of fairly apportioning care in a managed environment. legislation, in newyork and new jersey and other states, has turned to mandating minimum levels of hospital care, notably formothers delivering their babies.the analysis of health care data, indeed, the massive analysis of massive health care data sets, has a centralrole in setting health care policy and in steering healthcare reform, through its influence on the actual delivery ofhealthcare, one hospital and one health maintenance organization at a time. in a nutshell, the twin objectives arefor small consumption of resources, measured primarily in terms of cost, but also in terms of hospital lengthofstay, and for high quality. in practice the effective breakeven point, between sacrificing quality and flexibilityfor resource savings, is obscured in corporate and institutional decision making and policy. massive amounts ofdata analysis serves first of all to identify where savings might be made, or where quality might be inadequate.most often, other hospitals or other physicians provide benchmarks in studying performance. even relatively lowcost and high quality hospitals can find room for improvement in the practice patterns of particular physicians orin treating particular types of patients.massive data sets: guidelines and practical experience from health care53massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.no comparison is adequate without some effort at standardization of patient populations, or risk adjustmentthat accounts for individual patient characteristics. adequate risk adjustment requires massive data sets, so as toprovide an appropriately matched sample for any patient. issues of model uncertainty, and the presence ofmultiple models, appear obviously and often. orthogonal to patients are the number of possible measures, fromoverall measures of resource consumption and quality, to more detailed measures such as costs by individualcost center, the use of particular drugs and medications, and outcomes by type of complication. the next sectiongives a few details of coreplus and safs, two systems for outcomes analysis and resource modeling,including risk adjustment, developed in part by the author at healthcare design systems.the very large number of different questions in health care, and the specificity of those questions toindividual providers, payers, regulators, and to patients, are compelling reasons to do massive analysis of thedata. john tukey has advised that, as cpu cycles are now cheaper than fte's, the computers should beconstantly running. the challenge is to devise a series of meaningful analyses that will use these resources (thatare effectively free at the margin). in the following sections some graphical/tabular (section 3), statistical(sections 2 and 4), and computational (section 5) aspects of implementation are addressed.massive analysis of massive health care data finds consumers at all levels, from federal government to stategovernment, from payers to health systems, from hospitals to clinics, to physicians and to patients. the needsmay differ in detail, but the overall strategy is dear: to provide multifaceted insight into the delivery of healthcare. consumption here includes both more passive and more active roles in preparing questions to be addressedthrough data analysis. thus clinical professionals, their accountants and administrators, and patients, mayassimilate already prepared reports and report cards, and they may seek immediate answers to questionsgenerated on the spur of the moment, following a line of enquiry.massive data sets: guidelines and practical experience from health care54massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.2 coreplus and safs: case studies in mdathe following is a brief account of the evolution of two project in massive data sets undertaken byhealthcare design systems (hds) and its collaborators. with its parent company, latterly kaden arnone andcurrently quadramed, hds has provided consulting services and software services to the hospitals and managedcare companies. coreplus, for clinical outcomes resource evaluation plus, is a system for analyzingoutcomes of hospital care. safs, for severity adjustment factor computation, is a system for modeling resourceconsumption, including cost and length of stay. both systems have been developed through a collaborative effortbetween clinical experts, healthcare information processing staff, statisticians, management, and marketing.clinical expertise is obtained through the new jersey hospital association, as well as in house and by way ofhospitals and their physicians.coreplus includes 101 clinical outcomes in six major outcome categories (vaul and goodall, 1995).these are obstetrics, including cesarcan section and postdelivery complication rates, mortality, includingoverall and inpatient mortality, pediatric mortality, postoperative mortality (within 48 hours), stroke mortality,and mortality by major diagnostic category (mdc), subdivided into medical, surgery, oncology, and nononcology patients, neonatal, including newborn mortality by birthweight category and newborn trauma, surgery,including postoperative infections and various complication rates, general, including laparoscopic percent ofcholecystectomies, diabetes percent of medical cases, and cardiac, including c abg surgery mortality, cardiacvalve mortality, myocardial infarction, and male and female cardiac mortality.the sample sizes for the data included in each of these clinical outcomes ranges from several thousand tohalf a million for a single state (new jersey, with around 1.3 million annual hospital discharges), andproportionately more for national data. the definition of the clinical outcomes is determined by clinical experts.due to inevitable limitations in the data, some careful choices must be made in these definitions. hospitals arecompared in a comparative chart (figure 1), using a variety of peer groups for comparisons.severity adjustment factors are computed in each of approximately 600massive data sets: guidelines and practical experience from health care55massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.figure 1 interpreting coreplustm outputšcomparative chart.massive data sets: guidelines and practical experience from health care56massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.drgs. we model length of stay, and costed data.prediction models in coreplus, and somewhat similarly for safs, are built using several categories ofvariables, including general variables , such as a function of age, sex, race group, payer group, and surgical usevariables, general diagnosis and procedure variables, based on collections on one or more diagnosis andprocedure codes in areas such as decubiti, sepsis, diabetes, renal failure, vents, and separately by principal andsecondary diagnosis, specific diagnosis and procedure variables, specific to the outcome, and specific additionalvariables , for example birthweight. hospitals are then compared in a predictive chart (figure 2). the point hereis not to provide a detailed explanation and justification of the system, but rather to illustrate the size of theproblems that are encountered.variable selection is hindered by a ''large p'' problem: there are over 10,000 diagnosis and procedure codesto use singly, in aggregates (any one of the codes) or combinations (two or more codes simultaneously), asindicator variables in regression modeling. there is be a tight loop between statistical and clinical collaborators,within which data presentation is designed to convey information about potential variables and about the role ofvariables in fitting a statistical model to help elucidate clinical judgements.model building goes through several clinical and dataanalytic steps, including: (1) provisional definition ofan outcome in terms of elements of the data, (2) validation of the definition through running it against a database, observing frequencies and dumps of individual patient detail, (3) marginal analysis of the associationbetween potential predictor variables and the response, (4) determination of a set of candidate predictor variablesbased on clinical expertise supported by the data analysis, (5) predictive model building by a combination ofhierarchical and variable selection methods, (6) review of results of model building for reasonableness ofcoefficients, (7) goodness of fit analysis overall and for subsets of patients, including those defined by thepotential predictor variabels at step (3).beyond these steps, model validation is continual and ongoing. a typical application of the systems is forquality assurance or utilization review staff at a hospital to undertake analyses of specific categories of patients,using coreplus and safs as guides towards problem areas. these might be patients whose outcome iscontraindicated by the prediction. the patient medical record is consulted for further details, and that can exposefactors that might be better accommodated in the definition of the clinical outcome,massive data sets: guidelines and practical experience from health care57massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.figure 2 interpreting coreplustm outputšresidual plot.massive data sets: guidelines and practical experience from health care58massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.or in the predictive model.to summarize, the coreplus and saf applications are massive along several dimensions. the data are massive. the number of observations is in the millions or tens of millions, the number offields in the hundreds. the number of variables that can be meaningfully derived from the fields in the data is in the tens ofthousands, including aggregation and combinations of indicators. the clinical resource is massive, from the corpus of medical literature to live clinical expertise. the audience is massive, as every person is involved in a professional or in a patient capacity inhealthcare. the varieties of questions that can be addressed are massive.3 presentation of data analysismassive analysis of massive health care data finds consumers at all levels, from federal government to stategovernment, from payers to health systems, from hospitals to clinics, to physicians and to patients. the needsmay differ in detail, but the overall strategy is clear: to provide multifaceted insight into the delivery of healthcare. consumption here includes both more passive and more active roles in preparing questions to be addressedthrough data analysis. thus clinical professionals, their accountants and administrators, and patients, mayassimilate already prepared reports and report cards, and they may seek immediate answers to questionsgenerated on the spur of the moment, following a line of enquiry.massive analysis of massive data may be presented in two ways. one, as a carefully formatted, detailedreport, that presents in tabular, graphical and textual form a balanced and informative account of one or moreaspects of the data. such a report is likely to be condensed into one or a few pages (figure 1), but many suchreports can be combined together to give insight into many aspects of the data. some further specifics are givenin section 3. conceptually, this is akin to designing a new subway map for london or newmassive data sets: guidelines and practical experience from health care59massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.york, with its goal of clear but dense information content. in the subway map, each component, a subway stationor a stretch of track, has specific and individual meaning to many, if not all, users of the system; likewise eachcomponent of the graph must be labelled š the conventional scatter plot, comprising a scatter of points withperhaps a few outliers labelled, is not a high performer in this regard. tools to develop such graphics are nowavailable, eg using splus (mathsoft, inc.) and, increasingly, offtheshelf pc spreadsheet and graphics programs.the second presentation of massive analysis of massive data in health care is through user friendly,immensely flexible, software for data analysis. such software is a front end for analysis of data bases abstractedfrom the massive parent data, and is tailored to the particular needs of the health care investigator. the datathemselves are at a patient level, so that patient level detail can be used to help understand patterns among largergroups. however, it is reasonable to save summary statistics only for larger groups, which leads to a hierarchy ofdifferent data types within this software, and reports tailored to this structure.there is no absolute dividing line between reports generated directly from the massive parent data, andreports generated from software at distributed locations. what is clear, however, is that the analysis of massivedata is a specialized undertaking, and that exceptional computational resources, in terms of hardware, software,and personnel, as well as clinical and statistical expertise, must accumulate at those central locations. thecorresponding computational resources that are available in distributed fashion to the consumers of these data areincreasingly powerful, flexible, and userfriendly, but there must always be a significant gap between them andthe centralized resources. even without the difference in sheer computational horsepower, the expertise at acentral location allows flexibility in report generation that is beyond the scope of userfriendly, and thus to anextent sanitized, software.the internet provides additional dimensions that further facilitate massive analyses of data to suit manydiverse objectives. a straightforward use of the internet is for data collection and transmittal, in an extensive andmultilayered network. at its highest level this network involves the transmission of massive data sets amongstate, federal, academic, industrial, and commercial repositories. the rapid evolution of paradigms on theinternet, from ftp, to viewing world wide web (www) pages, to downloading software on an asneeded basisvia www, will find echos in the handling of massivemassive data sets: guidelines and practical experience from health care60massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.data sets in health care. for example, a massive data set need not reside at a single location; instead, applicationdaemons might visit a succession of sites, collecting and even summarizing information to compile together intoa comprehensive analysis.a first step in integrating the internet into the centraldistributed model might include transmitting data on adaily basis from providers (physicians and hospitals) to a central information processing center and its return tothe provider with added value, for example, including some standards and predictions for benchmarking. second,in generating reports at hospitals, queries for additional information might be sent directly to the informationprocessing (ip) center, perhaps complementing the summary statistics already in the provider data base withfully uptodate data, or summary data for a different category of patients. third, in generating a comparativereport for a provider with some additional fields beyond the standard set, the ip center might access comparabledata using its priviledged access to other providers databases.4 statistical challengesin analyzing massive data in healthcare, several concerns are paramount1. each patient is an individual. patients are not exchangeable.2. no single set of variables can capture all pertinent information on a collection of individual.3. even for only moderately large sets of variables, not even the most comprehensive efforts atmodeling are enough.if the cost of my hospitalization appears high, then that appearance is because of some internalbenchmarking or expectations. i might ask the physicians who direct my treatment program for an explanation. imay also refine my benchmarks by looking at the costs of hospitalizations for other patients, perhaps initially fora broad group of patients, but increasingly for patients with similar environment and medical history to my own,a process that culminates in the use of one, but, better, many statistical models that provide patientspecificpredictions (benchmarks). of course, even then, the variables that really matter might not be in the patient records.massive data sets: guidelines and practical experience from health care61massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.each patient has very many alternative peer groups in a massive data set. in the setting of healthcare,massive data does not provide a 'law of large numbers' as protection for standard statistical analyses, but insteadleads to a power law increase with sample size in the computational requirements.these considerations throw the burden of analysis towards exploratory techniques. a starting place issimple 'odds ratio' statements of the form: of 131,323 patients at risk for the outcome, 17,076 had the outcome, arate of 13.00%. these data comprise patients with myocardial infarction during 1993 or 1994 in 10 states, andthe outcome is death. of the patients at risk, 24,986 patients had an initial acute myocardial infarction of theinterior wall, with 4,065 deaths, a mortality rate of 16.27%. this particular ami, icd9 code 410.11, is just oneof around 40 codes for ami, which can be aggregated in diverse ways, or looked at in combination with othercodes. there is thus a massive number of statistical statements, and insufficient resources for immensely detailedanalyses of small data sets, such as the stack loss data. the need is for what might be called total analysis.beyond exploratory analysis, some statistical areas that appear particularly relevant are (i) advances inregression modeling, (ii) missing data methods, (iii) theory for observational data, and (iv) hierarchical modelsand bayesian statistics (where reasonable computationally with large data sets).three further aspects are important. one is statistical computation and computational performance. asecond is data, and the need for intense effort in understanding where the data come from and what are thealternatives. the third is organizational. in healthcare analysis, a divide and conquer strategy is natural given thepervasive subject matter knowledge: patients are naturally divided by major diagnostic category (mdc), forexample, those with respiratory problems (mdc 4) and those with cardiovascular problems (mdc 5). someorganizational/computational considerations of divide and conquer are considered in section 5.5 organization of computationsthe problems of massive data analysis in healthcare involves as much organization as statistical analysis. adivide and conquer strategy becomes allconsuming: total data analysis in which all resources are devoted to themassive data sets: guidelines and practical experience from health care62massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.maintenance, organization, and enhancement of the data.several statistical packages provide an "environment" for statistical analysis and graphics, notably sas(sas institute, cary, nc) and s (becker, chambers and wilks, 1988). although these systems provide aconsistent interface to statistical functions, a programming language, graphics, an interface to operating systemtools, and even a programmable graphical user interface, each has limitations. s has the more powerful andflexible environment, but sas programming expertise is easier to find, sas jobs are more likely to plug awayuntil complete š inevitably massive data analyses are left to run "in batch" overnight and over the weekend, andit is better to pay a performance penalty than to risk noncompletion. sas programs are a little easier to read,less encumbered by parentheses. neither environment fully supports imaging, graphics, and the worldwideweb.thus there is a strong use for integration tools, that allow the best and brightest software to work together ina projectspecific, possibly juryrigged, system. shell programming is important, but perl stands out, and otherapproaches (tcl/tk) are promising. another useful tool is dbms copy for transfering data between packages.modern operating systems, for example irix version 5 or later, allow iconographic maintenance and operations.however, a graphical user interface is not so useful without scripting, as an adhoc analysis may be repeatedmanyfold.organizationally, the multidimensional arrays found in the the storage of data, where each element of thearray is a single number, is echoed at a higher level in the organization of the components of massive data sets.in health care data, the dimensions of this metaarray might be year × state × outcome measure, where eachelement of the array is itself an array indexed by patient × measurement variable × hospital or episode. asummary of such data might be a table of csection rate by state by year, including both mean and variation.in principle these data might be organized into a sixdimensional array, either explicitly into an oracle orsybase database (say), or into a sas or s dataset. indeed, the present version of s, and even more so a futureversion, would support programming of a virtual array, in which the metaarray of arrays appears transparentlyas a six dimensional structure. massive data sets in health care are constantly changing and evolving, and it isvital not to impose too rigid a framework. there might be a new hospital's data today, of a different sort, or anew type of data (images of patient records), or amassive data sets: guidelines and practical experience from health care63massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.new measurement variable. it is most natural to maintain such data in the file system, not in a statistical package.thus the functions of operating system and statistical package meet one another. simple standalone filterswould be invoked from the unix shell, for example, to perform simple frequency analyses, merging, andtabulation directly from the command line. that is easily implemented using perl programs, possibly as wrappersfor analyses using a statistical package.massive data are too large to be hidden in a. data/ or sasdata/ directory. instead, the metaarray is found inthe file system. the hierarchical structure of the unix or dos file system is limiting. i might wish that my dataand their analyses are organized by outcome measure within state within year on one occasion, but, on anotheroccasion, that they are organized by year within state separately for each outcome measure. symbolic links canprovide a clumsy implementation of alternative hierarchies in the file system, a better implementation would bean explicit unix shell for massive data sets.a practical application of perl as an integration tool is to job scheduling. a specific example follows.5.1 example: job schedulingan environment for analysis of massive data sets will more often contain multiple workstations (withdifferent architectures), rather than be dominated by a single supercomputer. the analysis of massive data setsrequires that these distributed computational resources be utilized in an effective and efficient š in terms ofoperator time š manner. this is a problem in the general area of job scheduling, but a specialized approach doesnot appear necessary.as an example of implementation of the divide and conquer strategy, consider the script in figure 3. thesetup is of 101 outcome measures, each requiring a separate analysis of the data, to be performed on multiplemachines. this simple perl script can be hugely effective.the basic idea is to create an ascii 'state file' containing current state information to each outcome measure,or task, that can be read and written by different job schedulers. when a job scheduler is active on a task, the fileis locked. jobs, or subtasks, executed by the scheduler advance the state of the task; at all times the file containsthe current state. when a job scheduler has completed all the subtasks it can with a particular task; the filemassive data sets: guidelines and practical experience from health care64massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.figure 3 a perl script for job scheduling in a distributed computational environment.massive data sets: guidelines and practical experience from health care65massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.is unlocked. conflicts are avoided by keeping state file operations small and fast, compared to the sizes ofthe subtasks themselves. with relatively long subtask times careful optimization of state file operations isunimportant.there are several sophisticated features of (unix) operating systems that can be used, including sharedmemory, message passing (streams), file locking using flock, and shared data base technology. however, in aheterogeneous environment, including both unix workstations and personal computers say, scheduling must beextremely robust. it is the only dynamic component that must work across platforms. the tasks themselves mayutilize ascii files or various conversion utilities (unixtodos, or data base copy of proprietary system's databases across platforms using, eg, dbms copy). a perl script can run with little change on many different typesof platform; communication using ascii files containing state information separately for each task is highlyrobust.the script shown in figure 3 was written quickly, in part to exemplify the effectiveness of wide use ofintegration tools. perl itself is eclectic, so no attempt was made for programming elegance. the script is run withseveral arguments, as in j s 4 a b c, where 4 is the power of the machine, and a, b, and c denote the tasks.state information is saved in files named a, b, c, automatically created if not present at task invocation, in thedirectory keeptrack/. each task comprises a sequence of states, tailored using a set of three associative arraysincluded in the perl script. different types of task are accommodated using different initial states. a fourthassociate array gives the size of the subtask from each state to the next.any number of j s jobs can be started at the same or different times, without explicit reference to other jsjobs that are already running. each job first places a lock on a free task by appending the string '.locked' to thestate specified in the task's state file in keeptrack/. the job proceeds through the states of the task, updating thetask's state file, until either the task is complete, or the current subtask is too large for the size of the machine, orthe system call returns an error status. the job then removes the lock and moves to the next task. completedtasks are removed from the array of tasks; the script exits when that array is empty, or when the count ofunsuccessful attempts exceeds a preset threshold (when this number exceeds a smaller threshold, a interval is setbetween attempts).environments such as sas and s provide very powerful tools for statistical analysis. they may use parallelarchitecture, but they do not offer this kindmassive data sets: guidelines and practical experience from health care66massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.of simple support for distributed computing. the perl script given here, which could be implemented in sas orin s, is an example of a software device that allows computational resources to be fully used with almost noadditional effort. in the divide and conquer strategy employed in the analysis of health policy data, such tools arecritical.the script is easily modified and tuned to special situations. for example, in an environment where onemachine is more powerful than the others, it may be useful to have job schedulers on that machine allocate effortto the most intensive subtasks when there is one to be done. or, each task may have different overall size, whichmight be read from an ancillary file. or, an upper limit might be placed on the number of active subtasks(because of storage bandwidth say), and a count may be maintained using a further file. or, a breadth firstscheduler may be required, instead of the depth first algorithm given. or, in a primitive neuralnet like approach,a script might learn which tasks and subtasks are accomplished most efficiently. or, one task may depend on thecompletion of several other tasks, which can be implemented using an initial state and associated command thatchecks the states of the respective tasks in the files, possibly coupled with command line arguments thatincorporate sublists of such tasks.referencesharrell, f.e, lee, k.l., and mark, d.b. (1995). ''multivariate prognostic models: issues in developing models, evaluating assumptions andadequacy, and measuring and reducing errors. statistics in medicine 14 to appear.iezzoni, l.i. (ed.) (1994). risk adjustment for measuring health care outcomes . ann arbor, mi: health administration press.silber, j.h., rosenbaum, p.r., and ross, r.n. (1995). ''comparing the contributions of groups of predictors: which outcomes vary withhospital rather than patient characteristics?" journal of the american statistical association 90718.vaul, j.h. and goodall, c.r. (1995). the guide to benchmarking hospital value. st. anthonys publishing.massive data sets: guidelines and practical experience from health care67massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.massive data sets: guidelines and practical experience from health care68massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.massive data sets in semiconductor manufacturingedmund l. russelladvanced micro devices1 introductionincreasingly in industry and recently in semiconductor manufacturing, partly due to the introduction ofsensorbased data collection, network connectivity, and the availability of cheap data storage devices, we areseeing the automation of data collection becoming so common that one of the questions most frequently asked tostatisticians is: "how do we turn all this data into information?" the consulting statistician in this industry isbeginning to be faced with massive data sets.2 backgroundthe semiconductor manufacturing environment is a high volume manufacturing environment. in a typicalprocessing sequence, there may be over 100 process operations performed before the raw material, crystallinesilicon wafers up to 8 inches in diameter, is converted to wafers carrying up to several thousand unpackagedelectronic circuits, called die, on them. there are perhaps a few dozen additional manufacturing operationsrelated to packaging, in which the die are encapsulated in plastic or ceramic, that occur before the product isready for sale.in the semiconductor manufacturing industry, the competition is generally very aggressive. prices forproducts generally fall throughout any given product's lifecycle. and that product lifecycle can be very short; itis often measured in months. even in this environment of short lifecycles and failing prices, the demands forproduct quality and reliability are extreme. customers are beginning demanding that product be delivered withless than 10 parts per million defective and with reliability such that there are less than 30 failures expected perbillion device hours of operation.these characteristics of falling prices, increasing competition, complex state of the art processes and shortlifecycles combined with the high capital cost of bringing a new manufacturing facility online are creating agreat need to collect an analyze ever greater amounts of data.massive data sets in semiconductor manufacturing69massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.3 data overviewdata taken during manufacturing is available from a variety of sources. some of the data are collected as anormal part of the statistical process control effort. some of the data are collected as a normal part of theelectrical screening tests that ensure product quality. at amd we currently collect and summarize approximately2 gigabytes of such data per day. in order to discover better ways of controlling key process steps, a number ofcompanies are now automatically collecting some data using process state sensors.even in the development stage, the data volume from these sensors is huge. it is now possible to collectover 1 megabyte of sensor data per wafer in a plasma etch step alone. given that there are typically 10 or moresuch steps in a manufacturing process, when one considers that an average wafer fabricationsite produces several thousand wafers per week, the potential data volume for analysis is huge.some of the reasons we wish to collect manufacturing data and perform the analyses include: process andproduct characterization process optimization yield optimization process control design for manufacturingthe question might be raised as to how these needs are different from the same needs in a more typicalmanufacturing environment? the first and foremost reason is that data are available from a large number ofprocess operations š and much of that data can be collected automatically. the second reason is that themanufacturing process involves a large number of steps, some of which are essentially single wafer steps andothers of which are batch processing steps of various batch sizes.in addition, much of the summary data collected at this time are highly correlated due to the nature of theunderlying physics and chemistry of the processing operations. in addition there is an established practice oftaking multiple measures of the same electrical characteristics using test cells of varying sizes and properties. so,many of the apparently "independent" observationsaren't actually independent.there are other sources of data that are less related to direct manufacturing that may be used with themanufacturing data. these sources of data involve the output of process simulators and die design simulators. itis becoming more standard throughout the semiconductor industry to link these simulators together in chains toget a better picture of the expected performance characteristics of processes and semiconductor devices. theseexpectations may then be compared to actual manufacturing experience.manufacturing process data are typically collected in 4 different stages, each of which provides acharacteristic type of data for analysis. these data types are: die fabrication data wafer electrical data sortelectrical data final test data4 die fabrication datadie fabrication data are typically inprocess spc data at this time. although spc data and its uses in themanufacturing environment are fairly well understood, there has been some interest expressed both within amdand in other companies about further leveraging themassive data sets in semiconductor manufacturing70massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.spc data. given that there are often about 100 process operations providing spc data, in a typicalmanufacturing process there would appear to be a good opportunity for mining the spc data for processcharacterization and optimization.however, even when several of lots of wafers have experienced some similar sort of problem earlier in theprocess, it can be quite difficult to determine when all the lots ran through a given piece of equipment in a givenspan of time. this is because the wafer lot processing is not generally serial by lot. this is due to the mix ofproducts in a manufacturing facility and the differences in the process flows among the products.we are also beginning to get processstate data on individual process operations, such as plasma etch, fromprocessstate sensors that are being added to the equipment. these sensors can provide over 10,000measurements per wafer. this type of data is being examined by a number of companies for the potential toprovide modelbased process control for runtorun, or wafertowafer, modification of the processing parameters.because many, if not most, of the process steps during semiconductor manufacture are under control ofprocess controllers and are also capable of being fitted with sensors, the future data potential is enormous. forinstance, for the etch step in a typical wafer fabrication site, it would not be unreasonable to expectapproximately 35 gb of data per week to be collected at some point in the future.this processstate sensor data is typically autocorrelated within the process step itself and there isfrequently some correlation from wafer to wafer, both in singlewafer steps and in batchprocess steps. it isobvious that the observed autocorrelation patterns in the data change over time within the processing of a singlewafer š and the autocorrelation patterns may even change in response to changes in the processing environment.both spc and sensor based die fabrication data are now being explored in a variety of ways to assist withthe optimization of individual process steps with respect to product quality, reliability, yield and throughput. acomplication in this is that it is not known at this time what a "signal" is for a good process or even if the presentprocessstate sensors are the correct ones to use.for instance, one of the hidden complications of sensor based data is that the sensors have responsefunctions. that is, the signal that is observed can be distorted or even completely disguised by the response ofthe sensor itself and how it interacts with its measurement environment. there are few statistical tools availabletoday, and precious little training for statisticians, on identifying and dealing with sensor response functions.another hidden complication is that most of these process operations are under the active control of aprocess controller, often a pid controller. so we must also deal with the presence in the data of the apparentsignal induced by the process controller itself. here again, the statisticians are often not very conversant withprocess controllers and may not even be aware that at times the "signal" from a process may be mostly due to thecontroller trying to "settle."5 wafer electrical datathis type of data is typically taken towards the end of the manufacturing of the semiconductor circuits, butbefore the die on the wafer are individually separated and packaged. themassive data sets in semiconductor manufacturing71massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.data represents parametric measurements taken from special test structures located near the individual die on thewafers. generally these test structures are neither a test die nor a part of the circuit itself. they are intended toprovide information to the manufacturing site's engineers about the basic health of the manufacturing process.the typical set of wafer electrical tests is comprised of about 100 to 200 or more electrical tests onfundamental components or building blocks of the electronic circuits. most of the values retained in the databases are processed data and not the raw data taken directly from the test structures. the choices of the reportedvalues are typically made so as to be most informative about particular fabrication process operations. this leadsto many of the test values being highly correlated with each other.in addition, for any given test, consistent patterns across the individual silicon wafers or across waferswithin the same wafer lot may be evident to the analyst. these patterns are often due to the underlying physics orchemistry of the process steps and so are expected to some extent.so with wafer electrical test data, we have data which can be both spatially and temporarily correlated bytest site as well as highly autocorrelated within test site. this type of data represents some of the potentially mostuseful data gathered during the manufacturing of the wafer.it is generally believed by the engineers and managers that there are strong relationships between the waferelectrical tests and the individual processing operations. indeed, there have been numerous occasions in theindustry where misprocessed product was identifiable at wafer electrical test. if this is the case in the moregeneral setting, then it would be highly desirable to produce models relating inprocess data to wafer electricaldata for "normal" product so that corrections can be made to the process to achieve optimum performance for theprocess.6 sort electrical test datathis data is generated by an electrical prescreen, usually 100 prescreen, of each of the individual die on allwafers in a wafer lot. these tests are often functional tests, however some tests my be performed on criticalelectrical parameters, such as product speed. as in wafer electrical test data, there is a potential for hundreds ofdata values collected per individual die on a wafer. and as in wafer electrical tests, there are often patternsdiscernible in the test results across the wafers and from wafer to wafer.sort electrical test represents the first time the product itself is actually electrically tested. the primary useof this data is to prescreen the product so that resources are not wasted in the remainder of the manufacturingprocess by embedding nonfunctional die in plastic or ceramic packages.another possible use of this data, however one that is not frequently pursued, is to relate the sort results tothe wafer electrical test data and thus the wafer fabrication process itself. it is worth noting that in such a case theanalyst would have hundreds of dependent variables and independent variables simultaneously.massive data sets in semiconductor manufacturing72massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.7 final electrical test datathis delta is electrical test data taken on the finished product. there can be literally thousands of highlycorrelated tests performed each packaged die. many of these tests are parametric in nature however only pass/faildata may be available for analysis unless special data collection routines are used. most of the tests are relateddirectly to product specifications and so are tests of the product's fitness for use. since there can be considerableinteraction between the package and the silicon die, some electrical tests are possible for the first time at thisstage of manufacture.as with the sort electrical test data, we would like to relate these test results to the die fabrication process.in addition we would also like to relate them to the package assembly process. possible additional uses of thisdata include the identification of defective subpopulations and marginal product, product characterization andprocess characterization.at this stage of the manufacturing process, there can literally be thousands of highly correlatedmeasurements taken on literally millions of packaged die. even with relatively sparse sampling, the size of thedata sets that can accumulate for characterization and analysis can be enormous. because of the sizes of the databases and the large number of measurements taken on each die, the available statistical methods for analyzingtend to be highly inadequate. even seemingly trivial tasks such as validating and reformatting the data becomemajor problems!8 statistical issuesthe types of data sets that we are beginning to see in the semiconductor manufacturing industry presentmajor challenges to the applied statistician. in this industry there are simultaneously, high "n," high "p," and high''n and p" problems to be dealt with.because of the turn around time required for these types of analyses, and the potential for rewarding results,many managers and engineers are trying to use other methods, such as neural nets and cart like methods, toanalyze these types of data sets. most are unaware that these methods are also statistical in nature and haveparticular strengths and weaknesses.by and large the best manner to proceed in attacking these types of problems is not really known. we oftenhave great difficulty just dealing with the database issues, much less the analytical issues. data volume issueseven with simple analyses are a significant stumbling block for many software packages. sematech hasinvestigated a limited number of statistical packages and evaluated their performance in doing simple analyseswith merely "large" data sets and found that most gave up or failed to work with reasonable performance.once one begins analyzing a "small" problem in say 70 main effects with multiple responses and moderatecorrelation of some explanatory variables, and you are told that interactions are expected, the analyst quicklydiscovers the limitations of the common statistical methods at his disposal. common concepts of even suchroutine ideas as goodness of fit are less intuitive. even describing the domain of the explanatory variables can bea challenge!massive data sets in semiconductor manufacturing73massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.9 conclusionshow do we cope? the short answer is: "not well." statistical resources are generally somewhat scarce inmany industries š and those resources are often directed towards supporting ongoing spc and simple design ofexperiments efforts. in addition, many applied statisticians are at best, somewhat hobbled by a number of otherfactors: a statistical education that was too focused on the simpler core techniques, a lack of capable and efficientanalytical tools, and a lack of ability to rapidly validate and reformat data. an applied industrial statistician thatis lacking in any of these three areas on any given project may be in serious trouble.the more adventuresome analysts explore the use of nonstandard (or nonstandard uses of) methodologiessuch as aid, cart, neural nets, geneticalgorithms, kriging, pls, pcr, ridge regression, factor analysis, cluster analysis, projection pursuit,kalman filtering, and latin hypercube sampling to mention only a few. less adventuresome analysts utilizemore commonly taught procedures such as stepwise regression, logistic modeling, fractional factorial andresponse surface experiments, and arima modeling. howeverfar too many applied statisticians are unable to apply more than a couple of these techniques. virtually allthe meaningful training is "on the job."even very highly experienced analysts fall prey to the special problems found with sensor based data andprocesses that use automated controllers. these problems include, but are definitely not limited to modeling:processes that have more than one type of regime (such as turbulent flow and laminar flow), nonstationary timeseries that change order suddenly for brief periods of time, sensor response functions rather than process signal,data from differing stoichiometries in the same experiments, pid controller settling time as noise, the responsesof pid controllers rather than the process itself.failure to deal appropriately with these sorts of problems in the design of the collection of the data canjeopardize the results of any analysis, no matter how numerically sophisticated the basic techniques.10 recommendationsto adequately address these problems, we need a permanent forum focusing specifically on the issuesinherent in massive industrial data sets, possibly including an annual conference and a journal. issues that iwould like to see addressed include:improving the education of applied statisticians. the ability to use a standard statistics package, performsimple tests of hypothesis, design and analyze standard fractional factorial experiments, and set up spcprograms does not represent the needs for the future.developing an understanding of which types of advanced modeling techniques provide leverage againstwhich types of data for different analytical goals. developing an understanding of the problems related tosensors in particular is critical. it is no longer sufficient to fit simple linear models and simple time series to data.developing more graphical aids and more knowledge on how to use existing graphics to assist in analysis.graphical aids can assist with analysis and can help explain findings tomassive data sets in semiconductor manufacturing74massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.engineers and management.developing data handling and reformatting/parsing techniques so that throughput can be increased. a largeportion of the time spent in a typical analysis is often in data handling and parsing.making new algorithms more readily available and providing a source for training in the usage of the newalgorithms.encouraging software vendors to provide analytical tools that can handle large quantities of data, either inthe number of observations or in the number of variables, for identified high leverage techniques.this research is supported by arpa/rome laboratory under contract #f30602930100, and by the dept.of the army, army research office under contract #daah049510466. the u.s. government is authorized toreproduce and distribute reprints for governmental purposes not withstanding any copyright notation hereon. theviews and conclusions contained herein are those of the authors and should not be interpreted as necessarilyrepresenting the official policies or endorsements either expressed or implied, of the advanced research projectsagency, rome laboratory, or the u.s. government.references[1] paul r. cohen, michael l. greenberg, david m. hart, and adele e. howe. trial by fire: understanding the design requirements foragents in complex environments. ai magazine, 10(3):3248, fall 1989.[2] john d. emerson and michal a. stoto. transforming data. in david c. hoaglin, frederick mosteller, and john w. tukey, editors,understanding robust and exploratory data analysis. wiley, 1983.[3] usama fayyad, nicholas weir, and s. djorgovski. skicat: a machine learning system for automated cataloging of large scale skysurveys. in proceedings of the tenth international conference on machine learning , pages 112119. morgan kaufmann, 1993.[4] michael p. georgeff and amy l. lansky. procedural knowledge. proceedings of the ieee special issue on knowledge representation ,74(10):13831398, 1986.[5] peter j. huber. data analysis implications for command language design. in k. hopper and i. a. newman, editors, foundation forhumancomputer communication. elsevier science publishers, 1986.[6] amy l. lansky and andrew g. philpot. aibased planning for data analysis tasks . ieee expert, winter 1993.[7] stuart russell and peter norvig. artificial intelligence: a modern approach. prentice hall, 1995.[8] robert st. amant and paul r. cohen. toward the integration of exploration and modeling in a planning framework. in proceedings of theaaai94 workshop in knowledge discovery in databases, 1994.massive data sets in semiconductor manufacturing75massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.[9] robert st. amant and paul r. cohen. a case study in planning for exploratory data analysis. in advances in intelligent data analysis ,pages 15, 1995.[10] robert st. amant and paul r. cohen. control representation in an eda assistant. in douglas fisher and hans lenz, editors, learning from data: ai and statistics v. springer, 1995. to appear.massive data sets in semiconductor manufacturing76massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.management issues in the analysis of largescale crimedata setscharles r. kindermannmarshall m. deberry, jr.bureau of justice statistics u.s. department of justice1 the information glutthe bureau of justice statistics (bjs), a component agency in the department of justice. has theresponsibility for collecting, analyzing, publishing and disseminating information on crime. criminal offenders,victims of crime. and the operation of justice systems at all levels of government. two very large data setsš thenational incidentbased reporting system (nibrs) and the national crime victimization survey (ncvs)arepart of the analytic activities of the bureau. a brief overview of the two programs is presented below.2 nibrsnibrs. which will eventually replace the traditional uniform crime reporting (ucr)1 program as thesource of official fbi counts of crimes reported to law enforcement agencies. is designed to go far beyond thesummarybased u cr in terms of information about crime. this summarybased reporting program countsincidents and arrests. with some expanded data on incidents of murder and nonnegligent manslaughter.in incidents where more than one offense occurs, the traditional ucr counts only the most serious of theoffenses. nibrs includes information about each of the different offenses (up to a maximum of ten) that mayoccur within a single incident. as a result. the nibrs data can be used to study how often and under whatcircumstances certain offenses. such as burglary and rape, occur together.the ability to link information about many aspects of a crime to the crime incident marks the mostimportant difference between nibrs and the traditional ucr. these various aspects of the crime incident arerepresented in nibrs by a series of more than fifty data elements. the nibrs data elements are categorizedinto six segments: administrative. offenses, property, victim, offender. and arrestee. nibrs enables analysts tostudy how these data elements relate to each other for each type of offense.1the uniform crime reporting (ucr) program is a nationwide, cooperative statistical effort of approximately 16.000 city.county, and state law enforcement agencies voluntarily reporting data on crimes brought to their attention. the federalbureau of investigation (fbi) has administered this program since 1930.management issues in the analysis of largescale crime data sets77massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.3 ncvsthe bureau of justice statistics also sponsors and analyzes the national crime victimization survey(ncvs). and ongoing national household survey that was begun in 1972 to collect data on personal andhousehold victimization experiences. all persons 12 years of age and older are interviewed in approximately50.000 households every six months throughout the nation. there are approximately 650 variables on the ncvsdata file. ranging from the type of crime committed, the time and place of occurrence. and whether or not thecrime was reported to law enforcement authorities. the average size of the data file for all crimes reported for aparticular calendar year is 120 megabytes.the ncvs utilizes a hierarchical file structure for its data records. in the ncvs there are four types ofrecords: a household link record, followed by the household, personal. and incident records. the householdrecord contains information about the household as reported by the respondent and characteristics of thesurrounding area as computed by the bureau of the census. the person record contains information about eachhousehold member 12 years of age and older as reported by that person or proxy. with one record for eachqualifying individual. finally, the incident record contains information drawn from the incident report,completed for each household or person incident mentioned during the interview. the ncvs is a somewhatsmaller data set than nibrs. but may be considered analytically more complex because 1) there is moreinformation available for each incident and 2) it is a panel design, i.e., the persons in each housing unit areinterviewed every six months for a period of three years, thereby allowing for some degree of limitedlongitudinal comparison of households over time.4 data utilizationan example of how those interested in the study of crime can tap the potentially rich source of newinformation represented by nibrs is seen in the current supplementary homicide reports data publishedannually by the fbi in its crime inthe united states series. crosstabulations of various incidentbased data elements are presented, includingthe age, sex, and race of victims and offenders, the types of weapon(s) used, the relationship of the victim to theoffender, and the circumstances surrounding the incident (for example, whether the murder resulted from arobbery, rape, or argument). the nibrs data will offer a variable set similar in scope.currently, portions of eight states are reporting nibrs data to the fbi. in 1991. three small states reported500,000 crime incidents that required approximately one gigabyte of storage. if current nibrs storage demandswere extrapolated to full nationwide participation. 40 gigabytes of storage would be needed each year.although full nationwide participation in nibrs is not a realistic shortterm expectation. it is realistic toexpect that a fourth of the u.s. could be represented in nibrs within the next several years. the correspondingvolume of data, 10 gigabytes each year. could still be problematic for storage and analysis.certain strategies may be chosen to reduce the size of the corresponding nibrs data files. for example,most users of nibrs data may not need or desire a data file that containsmanagement issues in the analysis of largescale crime data sets78massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.all twentytwo types of group a offences. which contains crimes such as sports tampering. impersonation. andgambling equipment violations. if a user is interested in much smaller file, only the more common offenses, suchas aggravated assault, motor vehicle theft, burglary, or larceny/theft, could be included in the data set. anotherarea in which data reduction can be achieved is in the actual nibrs record layout. although the multiplerecordformat may aid law enforcement agencies in the inputting of the data, it can create difficulties in analyzing thefiles. for example, in the current nibrs format, each record, regardless of type, begins with 300 bytes reservedfor originating agency identifier (ori) information. currently, nearly a third of each ori header is filler spacereserved for future use. moreover, the records for the different incident types have been padded with filler so asto be stored as fixed length records instead of variable length records. this wasted space occupied by multipleori headers and filler can be eliminated by restructuring and reorganizing the current file structure into a moresuitable format that current statistical software packages can utilize.even with the restructuring of the current record formats, the annual collection of nibrs data will stillresult in a large volume of data to be organized, stored, and analyzed. one strategy bjs is considering is tosample the nibrs data in order to better manage the volume of data expected. since the nibrs program can beviewed as a potentially complete enumeration of incidents obtained by law enforcement agencies, simple randomsampling could be employed, thereby avoiding the complications of developing a complex sample designstrategy and facilitating the use of'' off the shelf" statistical software packages.using the sample design of the ncvs, bjs has produced a 100 megabyte longitudinal file of householdunits that covers a period of four and one half years. this file contains information on both interviewed andnoninterviewed households in a selected portion of the sample over the seven interviews. the ncvslongitudinal file can facilitate the examination of patterns of victimization over time, the response of the police tovictimizations. the effect of life events on the likelihood of victimization, and the long term effects of criminalvictimization on victims and the criminal justice system. however, current analysis of this particular data file hasbeen hampered by issues relating to the sample design and utilizing popular statistical software packages. sincethe ncvs utilizes a complex sample design. standard statistical techniques that assume a simple random samplecannot be utilized, although there are software packages that can deal with complex sample designs, the ncvsdata are collected by the bureau of the census under title 13 of the u.s. code. as a result. selected informationthat would identify primary sampling units and clusters is suppressed to preserve confidentiality. researchers,therefore, cannot compute variances and standard errors for their analyses on this particular data sets. bjs iscurrently working with the bureau of the census to facilitate the computation of modified sample information tobe included on future public use tapes that will facilitate the computation of the appropriate sample variances.most of the current statistical software packages are geared to processing data on a case by case basis. thencvs longitudinal file is structured in a nested hierarchical manner. when trying to examine events over aselected time period, it becomes difficult to rearrange the data in a way that will facilitate understanding the timeor longitudinal aspects of the data. for example, the concept of what constitutes a "case record" depends on theperspective of the current question. is a case all households that remain in sample over all seven interviews,management issues in the analysis of largescale crime data sets79massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.or is it those households that are replaced at every interview period? moving the appropriate incident data fromthe lower levels of the nested file to the upper level of the household can complicate obtaining a "true" count ofthe number of households experiencing a victimization event, since many statistical software packages duplicateinformation at the upper level of the file structure down to the lower level.5 future issueslocal law enforcement agencies will be participating on a voluntary basis. nibrs data collection andaggregation at the agencylevel will be far more labor and resourceintensive than the current ucr system. whatare the implications for coverage and data accuracy?criminal justice data have a short shelf life, because detection of current trends is important for planningand interdiction effectiveness. can new methods be found to process massive data files and produce informationin a time frame that is useful to the criminal justice community? numerous offenses such as sports tampering arenot of great national interest. a subset of the full nibrs file based on scientific sampling procedures couldfacilitate many types of analyses.how easy is it to integrate change into such a data system, as evaluations of nib rs identify newinformation needs that it will be required to address? does the sheer volume of data and reporting agencies makethis need any more difficult than for smaller ongoing data collections? as data storage technology continues toevolve, it is important to weigh both cost and future compatibility needs, particularly in regards to distributingthe data to law enforcement agencies and the public. bjs will continue to monitor these technological changes sothat we will be able to utilize such advances in order to enhance our analytic capabilities with these large scaledatasets.management issues in the analysis of largescale crime data sets80massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.analyzing telephone network dataallen a. mcintosh*bellcoreabstractas our reliance on computer networks grows, it becomes increasingly important that we understand thebehavior of the traffic that they carry. because of the speeds and sizes involved, working with traffic collectedfrom a computer network usually means working with large datasets. in this paper, i will describe our experiencewith traffic collected from the common channel signaling (ccs) network. i will briefly describe the data andhow they are collected and discuss similarities with and differences from other large datasets. next, i willdescribe our analysis tools and outline the reasons that they have been found useful. finally, i will discuss thechallenges facing us as we strive for a better understanding of more data from faster networks. while myemphasis in this paper is on the ccs network, it has been my experience that both the problems and thesolutions generalize to data from other networks.as our reliance on computer networks grows, it becomes increasingly important that we understand thebehavior of all aspects of the traffic that they carry. one such network, used by millions of people every day, isthe common channel signaling (ccs) network. the ccs network is a packetswitched network that carriessignaling information for the telephone network. it carries messages for a number of applications, includingmessages to start and stop calls, to determine the routing of toll free (area code 800) calls and to verify telephonecredit card usage. network failures, while rare, can affect hundreds of thousands of telephone subscribers. as aresult, telephone companies are very interested in the health of their portion of the ccs network. at bellcore,we help network providers keep their part of the ccs network running smoothly by testing vendor equipmentand by monitoring live networks. to test equipment, we build small test networks and subject them to extremestress and catastrophic failure, such as might be caused by a fire at a switch or a backhoe cutting a cable during amass callin. we usually collect all the* senior research scientist, statistics and data analysis research group, bellcore, 445 south street, morristown, nj07960.analyzing telephone network data81massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.traffic on the test network, though selective collection may be used if a test involves subjecting the network to aheavy load. to monitor live networks, we collect all the traffic from a small subnetwork. collecting all the datafrom a network provider's live network would require monitoring several thousand communication links, and isbeyond the capacity of our monitoring equipment.in north america, a protocol called signaling system number 7 (ss7, ansi (1987)) governs the format ofdata carried by the ccs network. 1 the major components of the ccs network are telephone switches (ssp's),database servers (scp's), and ss7 packet switches (stp's). stp's are responsible for routing traffic, while ssp'sand scp's can only send and receive. stp's are deployed in pairs for redundancy. each ssp and scp isconnected to at least one pair of stp's. the (digital) communications links between nodes run at a maximumspeed of 56000 bits per second. when extra communications capacity is required, parallel links are used. 56000bits per second is relatively slow, but there are many links in a ccs network, and many seconds in a day.anyone trying to collect and analyze ss7 data from a ccs network soon must deal with large datasets.to date, our main datacollection tool for both live and test networks has been a network services testsystem (nsts). the nsts is about the size of a household washing machine. each nsts can monitor 16 bidirectional communication links, and store a maximum of 128 megabytes of data. this represents two to fourmillion ss7 messages, which can be anywhere from one hour of ss7 traffic to and from a large access tandem toapproximately four days of traffic to and from a small end office. we usually collect data by placing one nstsat each stp of a pair. thus, our datasets from live networks are usually 256 megabytes in size. datasets fromtest networks tend to be smaller, depending on the length of the test. along with every message that it saves, thensts saves a header containing a millisecond timestamp, the number of the link that carried the message, andsome other status information. the timestamps are synchronized with a central time source, and so arecomparable between monitoring sites.our ss7 datasets have many of the "standard" features of the large datasets discussed in this volume.inhomogeneity and nonstationarity in time are the rule. for example, figure 1 shows the call arrival process ona communication link to a small switch from 22:45 wednesday to 14:15 sunday. there are 31,500 points in thisplot, joined by line segments. each point represents the number of calls received during a ten second interval,expressed in units of calls per second. evidently there is a timeofday effect, and the effect is different on theweekend. this dataset isnote: this paper is reprinted by permission from bellcore. copyright 1996 by bellcore.1 telecommunications practice varies between countries. the ccs network considered in this paper is the north americannetwork.analyzing telephone network data82massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.discussed in more detail in duffy et al. (1993). they show that the call arrival process in figure 1 is welldescribed by a timeinhomogeneous poisson process. the overall packet arrival process shows signs of longrange dependence, and is not well modeled by a timeinhomogeneous poisson process. plots such as this alsosuggest that it is virtually impossible to find a stationary busy hour, the typical engineering period in traditionaltelephone networks.figure 1: call arrival processthe inhomogeneity can be subtle. for example, figure 2 illustrates the number of calls in progress for thesame switch as a function of time. since most telephone calls are short, one would expect figures 1 and 2 to besimilar. however, figure 2 contains an extra peak on thursday evening which is not present in figure 1. thissuggests that evening calls (from residential customers) tend to be longer than daytime calls (from businesscustomers).2 similar signs are visible friday and saturday evenings, but they are not as pronounced. figure 3 is aplot of the number of calls in progress over a one week period, taken from a different dataset. now we can seethat monday through thursday evenings are similar, and the remaining evenings are very different. thisabundance of features is typical of our data.some other features of our ss7 data make analysis more difficult. first, there are many different kinds ofmessage, and most of them have variable length. this makes it difficult to use conventional database tools,which usually insist on fixed length records. second, ss7 messages are binary, and several fields may be packed2 duffy et al. (1994) show that the distribution of call homing times of nighttime calls has very heavy tails.analyzing telephone network data83massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.figure 2: calls in progressfigure 3: calls in progressanalyzing telephone network data84massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.into a single byte to save space. this renders them unsuitable for processing by most statistical packagesand other commonly used tools (unix utilities such as awk and perl, in our case) which are asciibased. in apilot study, we tried converting our data to an ascii representation and processing it in this form. this resultedin a tenfold increase in the size of our data. the ascii tools that we used for processing, being very general,were not very efficient to begin with, and the data bloat exacerbated the problem.in the end, we decided to keep our data in the original binary file format written by nsts. we wrote a suiteof tools (described below) to process the data in this format. we still needed tools to produce ascii output, butthese were usually used in the final steps of an analysis.in designing our data processing tools, we followed an old and well tested paradigm. kernighan and plauger(1976) wrote:a surprising number of programs have one input, one output, and perform a useful transformation on data as itpasses through. ... a careful selection of filters that work together can be a toolkit for quite complicated processing.because our network datasets are sequential over time, it is natural to view them as sequences of messages.our tools for analyzing binary ss7 data are thus message filters. conceptually, they read one or more binary filesone message at a time, perform some operation on each message, and write some or all of the messages outagain. some filters translate to ascii instead of writing in binary. our filters perform a number of functions,including subsetting. huber (1996) mentions the importance of building subsets, an observation we share. most ofthe subsetting operations on our ss7 data are performed by one filter program. this filter appears(usually multiple times) in any analysis i do. its user interface is easily the most complex of any of ourmessage filters. there are more than two dozen different fields that can be referenced. we have built agui (graphical user interface) to make this filter easier to use. fortunately, the complexity of theexternal interface is not matched by either internal complexity or long running times. sampling. content and protocol checking. we can check for malformed ss7 messages, and can do some checkingof the protocol for managing calls. sort/merge. the raw data files produced by nsts need to be sorted into time order, and sorted filesfrom multiple nsts's need to be merged.analyzing telephone network data85massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved. call assembly. it takes several messages to set up and tear down a call. they appear at different timesand may appear on different links. [the best analogy i can think of is to imagine pointofsale data for astore where keystrokes for odd numbered keys are recorded on one tape and keystrokes for evennumbered keys are recorded on another tape.] significant processing is required to gather these togetherif one wants to analyze calls rather than packets. checking for wiring errors. we learned the hard way that this was necessary. drawing plots of link load in postscript3 counting, by message type, priority, destination, and so on. translating to ascii, with varying levels of detailit is instructive to examine how figures 2 and 3 were produced. the data gathered by nsts were stored ondisk in time sorted order. to produce the plots, the data were run through the following filters, in sequence:1. a filter to trim off end effects that arise because the two nsts's do not start and stop simultaneously.2. a filter to select only messages related to call setup and termination.3. a filter to organize messages into groups where each group contains the messages from a singletelephone call.4. a filter to print out (in ascii) the timestamps of the start and end of every call, along with a +1(start) and a 1 (end).5. an ascii sort filter, since the output of the previous step is no longer in time sorted order.6. a filter to do a cumulative sum of the result of the previous step and print out the value of the sum ata reasonable number of equally spaced times. the output from this step was handed to plottingsoftware.the filters in steps one through four are message filters, and the filters in steps five and six are ascii filters.aside from the soundness of the underlying concept, there are a number of reasons why this approachsucceeded. first, the message filters we have written are3 postscript is a registered trademark of adobe systems incorporatedanalyzing telephone network data86massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.efficient. a filter that copies every message individually takes 77 seconds elapsed time to read a 256 megabytefile on my workstation. a standard system i/o utility takes 73 seconds for the same task. the bottleneck in bothtasks is the i/o bandwidth. most simple filters are i/o bound like this. an exploratory analysis can thus proceedby saving only a few key intermediate results (perhaps from steps requiring more extensive processing), andrepeating other intermediate operations when required.second, we have made message filters easy to write. to build a simple filter, it is only necessary to write a c++ function that examines a message and returns a value indicating whether it is to be output or discarded.argument processing, a ''standard i/o" library, and a support library are provided for the more adventurous. iwas both amused and disturbed that in some ways this puts us where we were over 20 years ago with the analysisof small and mediumsized datasets, namely writing oneofakind programs with the help of a large subroutinelibrary. fortunately, programming tools have improved over the last 20 years, and the oneofakind programscan be connected together.third, message filters are easily connected to perform complex processing tasks. this is easily done bycombining filter programs, as in the example above. it is also easy to manipulate filters as objects in c++ codewhen considerations of convenience or efficiency dictate. filters are implemented as c++ classes derived from asingle base class. the interface inherited from the base class is small, and defaults are provided for allfunctionality. support is also provided for lists of filters, and for simple boolean operations like inversion anddisjunction.the analyses we perform on our data are intended for a wide audience whose members have different needsand very different views of the network. they include:1. network planners, who are interested in traffic trends over months or years. the network nodesthemselves provide gross traffic data. network monitoring can provide more detail, such as trafficcounts for sourcedestination pairs.2. network operations personnel, who are interested in anomalies such as might be found in a set ofdaily sourcedestination pair counts. (see duffy et al. (1993) section 3.3)3. traffic engineers, who are interested in building models for traffic. the intent here is to evaluatebuffer management policies and other issues affected by the randomness of traffic. data can helpvalidate models, though there are some pitfalls, as we shall see.4. analysts interested in anomalies in subscriber calling patterns.analyzing telephone network data87massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.figure 4: figure 1 with data reduction5. testers, who are interested in protocol errors, and do not care about the data from the 99.99% of thetime that the test network worked correctly.to satisfy these needs, we must provide tools to explore both trends and exceptions in our data at differinglevels of detail. our filter building blocks allow us to design software tools to fit each application. because theapplications share a common infrastructure, the programming effort required is not large.a 256 megabyte dataset contains the details of more telephone calls than there are pixels on my workstationscreen, and nearly as many messages as there are dots on an 8.5 by 11 piece of paper at 300 dots per inch. goodvisualization tools must therefore be able to avoid burying important details in a blob of ink. even drawing theblob can be prohibitively expensive. for example, figure i contains 31,500 line segments, and seems to takeforever to draw on an x terminal running over a modem. figure 4 plots the same data as figure 1, but reduces itby plotting the minimum and maximum 10 second load in each 10 minute period. peaks and valleys have beenpreserved, thus preserving the overall appearance of the plot, but the number of points plotted has been reducedby a factor of 30.on the other hand, smoothing to reduce data is not a good idea, as figure 5 illustrates. the upper and lowerjagged lines are respectively the maximum and minimum 5second loads in each one minute period. the smoothline in the middle misses the main feature of the plot completely.to date, we have not had much opportunity for indepth modeling of our data. the efforts described induffy et al. (1993) and duffy et al. (1994) barely scratch theanalyzing telephone network data88massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.surface. the complexity of figure 3 suggests that building a comprehensive model is a daunting task. traditionaltechniques can be inappropriate for even simple models. for example, a 256 megabyte file contains roughly sixmillion messages. in a certain context, one can view these as bernoulli trials with p = 0.5. since adherence to thebernoulli model is not perfect, a sample this size is more than sufficient to reject the hypothesis that p = 0.5 infavor of p = 0.5 +  (and indeed in favor of something that is not bernoulli at all). unfortunately, the hypothesistests don't answer the real question: does it matter?figure 5: the dangers of oversmoothingthere are some classes of analysis, such as resampling methods, that are more practical with large datasets.an adequate model for a large dataset may be so complex that one would be better off resampling. are thereadequate tools for doing this? one potential pitfall that comes to mind immediately: a 32 bit random numbergenerator may not provide enough bits to sample individual records.the strategy described here has worked well for our 256 megabyte datasets. we have a proven set of toolsthat enable us to do flexible, efficient analyses. however, there are monsters in our future! we are testing asecond generation of ss7 data collection equipment that can store roughly 55 gigabytes of data from 96(bidirectional) communication links at a single stp site without manual intervention. this is roughly a week ofdata from a mediumsized local access and transport area (lata). if someone is available to change tapesperiodically, the amount of data that can be collected is unlimited.we are still trying to come to grips with datasets of this size. just reading 55analyzing telephone network data89massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.gigabytes of data off tape and putting it onto a disk takes two and a half days. storing the data on disk isproblematic, because my workstation doesn't allow files bigger than two gigabytes. it is likely that some sort ofparallelism will be required for both storage and processing. splitting the data across the right dimension is adifficult problem. the wrong choice can force an analysis to be serial rather than parallel, and it is not clear thatthe same choice cam work for all questions of interest.a parallel effort at bellcore has involved the collection and analysis of ethernet traffic (leland et al. (1994),willinger et al. (1995), leland et al. (1995)). datasets here have been larger, in part because of the highertransmission speed involved, but there has been significantly less detail available because only the ip headerswere saved.we are now turning our attention to traffic on emerging networks, such as atm and frame relay. thesenetworks are used to carry data from other protocols (ip, ss7). as a consequence, we expect to encounter manyof the challenges, and the opportunities, that were present in our ss7 and ethernet datasets.acknowledgementsdiane duffy, kevin fowler, deborah swayne, walter willinger and many others contributed to the workdescribed here. the opinions axe mine.referencesansi (1987). american national standard for telecommunicationssignalling system number 7. american national standards institute,inc., new york. standards t1.110114.duffy, d. e., mcintosh, a. a., rosenstein, m., and willinger, w. (1993). analyzing telecommunications traffic data from workingcommon channel signaling subnetworks. in tarter, m. e. and lock, m. d., editors, computing science and statistics:proceedings of the 25th symposium on the interface, pages 156165. interface foundation of north america.duffy, d. e., mcintosh, a. a., rosenstein, m., and willinger, w. (1994). statistical analysis of ccsn/ss7 traffic data from working ccssubnetworks. ieee journal on selected areas in communications, 12(3):544551.huber, p. j. (1996). need to get the title of this article. in kettenring, j. and pregibon, d., editors, massive data sets, pages 111999.analyzing telephone network data90massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.kernighan, b. w. and plauger, p. l. (1976). software tools. addisonwesley, reading, mass.leland, w. e., taqqu, m. s., willinger, w., and wilson, d. v. (1994). on the selfsimilar nature of ethernet traffic (extended version).ieee/acm transactions on networking, 2:115.leland, w. e., taqqu, m. s., willinger, w., and wilson, d. v. (1995). selfsimilarity in highspeed packet traffic: analysis and modelingof ethernet traffic measurements. statistical science, 10:6785.willinger, w., taqqu, m. s., sherman, r., and wilson, d. v. (1995). selfsimilarity through highvariability: statistical analysis ofethernet lan traffic at the source level. computer communications review, 25:100113. proceedings of the acm/sigcomm'95, boston, august 1995.analyzing telephone network data91massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.analyzing telephone network data92massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.massive data assimilation/fusion in atmospheric models andanalysis: statistical, physical, and computational challengesgad levyoregon state universitycarlton puoregon graduate institute of science and technologypaul d. sampsonuniversity of washingtonabstractthe goals and procedures of the most data intensive operations in atmospheric sciencesdata assimilationand fusionare introduced. we explore specific problems which result due to the expansion in observing systemsfrom conventional to satellite borne and the corresponding transition from small, medium, and large data sets tomassive data sets. the satellite data, their volumes, heterogeneity, and structure are described in two specificexamples. we illustrated that the atmospheric data assimilation procedure and the satellite data pose uniqueproblems that do not exist in other applications and are not easily addressed by existing methods and tools.existing solutions are presented and their performance with massive data sets is critically evaluated. weconclude that since the problems are interdisciplinary, a comprehensive solution must be interdisciplinary aswell. we note that components of such a solution already exist in statistics, atmospheric, and computationalsciences, but that in isolation they often fail to scale up to the massive data challenge. the prospects ofsynthesizing an interdisciplinary solution which will scale up to the massive data challenge are thus promising.1 introductionthe purpose of data assimilation is to combine atmospheric measurements and observations with ourknowledge of atmospheric behavior in physical atmospheric models, thus producing a best estimate of thecurrent state of the atmosphere. the similar but distinct purpose of data fusion is to extract the best informationfrom a multitude of heterogeneous data sources, thus devising an optimal exploitation of the synergy of thesedata. the resulting analyses (a.k.a. 'initialization fields') have great diagnostic value, and are the basis formassive data assimilation/fusion in atmospheric models and analysis: statistical, physical, andcomputational challenges93massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.model prediction. this procedure of analysis and model initialization has seen an exponential growth in thevolume of observational geophysical data. the main purpose of this paper is to (1) critically evaluate howexisting methods and tools scale up to the massive data challenge: and (2) explore new ideas/methods/toolsappropriate for massive data sets problems in atmospheric science. our interest and focus is in the jointexploration of the different facets of what we consider some of the weakest components of current dataassimilation/fusion schemes in atmospheric and climate models as they attempt to process massive data sets. wefirmly believe that since the problems are interdisciplinary, a comprehensive solution must bring togetherstatisticians, atmospheric and computational scientists to explore general methodology towards the design of anefficient, truly open (i.e., standard interface), widely available system to answer this challenge. recognizing thatthe greatest proliferation in data volume is due to satellite data, we discuss two specific problems that arise in theanalysis of such data.in a perfect assimilation scheme, the processing must allow merging of satellite and conventional data,interpolated in time and space, and for model validation, error estimation and error update. even if the input andoutput data formats are compatible, and the physical model is reasonably well understood, the integration ishampered by several factors. these roadblocks include: the different assumptions made by each of the modelcomponents about the important physics and dynamics, error margins and covariance structure, uncertainty,inconsistent and missing data, different observing patterns of different sensors, and aliasing (zeng and levy,1995).the earth observing system and other satellites are expected to download massive amounts of data, andthe proliferation of climate and general circulation models (gcm) will also make the integrated models morecomplex (e.g., review by levy, 1992). inconsistency and error limits in both the data and the modeling should becarefully studied. since much of the data are new, methods must be developed which deal with the roadblocksjust noted, and the transformation of the (mostly indirect) measured signal into a geophysical parameter.the problems mentioned above are exacerbated by the fact that very little interdisciplinary communicationbetween experts in the relevant complementary fields takes place. as a consequence, solutions developed in onefield may not be applied to problems encountered in a different discipline, efforts are duplicated, wheels reinvented, and data are inefficiently processed. the sequoia 2000 project (stonebraker et al., 1993) is an exampleof successful collaboration between global change researchers and computer scientists working on databases.their team includes computer scientists at uc berkeley, atmospheric scientists at ucla, and oceanographers atuc santa barbara. data collected and processed include effects of ozone depletion on ocean organisms andlandsat thematic mapper data. however, much of the data management and statistical methodology inmeteorology are still being developed 'in house' and are carried out by atmospheric scientists rather than incollaborative efforts. meanwhile, many statisticians do not use available and powerful physical constraints andmodels and are thus faced with the formidable task of fitting to data statistical models of perhaps unmanageabledimensionality.as a preamble, we describe in the next section the satellite data: volumes, heterogeneity, and structure,along with some special problems such data pose. we then describe some existing methods and tools in sectionthree and critically evaluate their performance with massive data sets. we conclude with some thoughts andideas of how methods can bemassive data assimilation/fusion in atmospheric models and analysis: statistical, physical, andcomputational challenges94massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.improved and developed to scale up to the massive data sets challenge.2 the satellite datain atmospheric studies, as in many other fields of science, researchers are increasingly relying upon on theavailability of global data sets to support their investigations. problems arise when the data volumes are huge andthe data are imprecise or undersampled in space and time as is often the case with satellite sampling. numericalweather prediction models have traditionally accepted observations at given time intervals (synoptic times) froma network of reporting stations, rawindsondes, island stations, buoys, weather ships, ships of opportunity,aircrafts and airports, treating the state variables in a gridded fashion. this has set the norm for the acceptabledata format in these studies, dictating the need for most observations to be eventually brought to 'level 3'(gridded) form. it has also contributed to the development of the statistical and meteorological field known asobjective analysis. however, volumes and sampling patterns of satellite data often lead to bottlenecks and to thefailure of traditional objective analysis schemes in properly processing asynoptic satellite data to level 3geophysical records as the examples in the next paragraphs demonstrate.figure 1 presents a small (stormsize) scale schematic illustration of data structure, volume, andheterogeneity. in it, data from three different satellite instruments (two wind speed products from differentchannels of the special sensor microwave imager (ssm/i) on the defense meteorological satellite program(dmsp) space craft, and one wind vector product from the active microwave instrument (ami) on board thefirst european remote sensing (ers1) satellite) are combined with the european centre for mediumrangeweather forecasts (ecmwf) model thermodynamic output to create composite analysis fields (upper panels).each of the data products has been sampled at different times and locations and has already undergone somequality control and data reduction procedures. the compositing procedure of eighteen looks at a single stormsuch as the one illustrated in fig. 1 required the reduction of approximately 3 gb of input data to 290 mb in thefinal product. operational weather prediction centers need to process similar products four times daily, globally,at 20 vertical levels, and with additional conventional and satellite data. the imperative of having fast algorithmsand fast data flow is clear in this example.the monthly mean (climate scale level 3 product) stratospheric water vapor from the microwave limbsounder (mls) on board the upper atmosphere research satellite (uars) for january 1992 is shown infigure 2. spatial structure which is related to the satellite orbital tracks is apparent in this figure. carefulinspection of the maps provided by halpern et al. (1994) reveals similar structures in the ers1/ami monthlyand annual mean for 1992, as well as in the pathfinder ssm/i monthly mean wind speeds maps. thecorresponding monthly means created from the ecmwf daily synoptic maps (also presented in halpern et al.,1994) do not show this structure. these observations strongly imply that the structure is caused by the samplingrather than by an instrument error. zeng and levy (1995, hereafter, zl95) designed an experiment to confirmthat the structure is indeed a result of the satellite sampling. the ecmwf surface wind analyses were sampledwith the ers1 temporalspatial sampling pattern to form a simulated data set which exhibited the same structureas in figure 2. in their experiment, the application of a bicubic spline filter to the monthlymassive data assimilation/fusion in atmospheric models and analysis: statistical, physical, andcomputational challenges95massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.figure 1: schematic of compositing processmassive data assimilation/fusion in atmospheric models and analysis: statistical, physical, andcomputational challenges96massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.figure 2: uars mls 215hpa h2o (ppmv)  january 1992massive data assimilation/fusion in atmospheric models and analysis: statistical, physical, andcomputational challenges97massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.mean simulated data resulted in a field that was usually free of the structure. however, large biases of up to3 m s1 remain in several areas, and there was no significant reduction in the variance added to the control set bythe aliasing (spatial structure) caused by the satellite sampling in these areas. the scatter plot of the smoothedfield versus the control set presented by zl95 demonstrates that the smoothed field has less variance but is stillseriously biased from the control set (0.5 m s1 globally). problems with spectral analysis of both the simulatedand simulatedsmoothed fields were also reported by zl95.the examples above underscore two serious problems with the analysis and assimilation of satellite data inatmospheric studies. the first example demonstrates the special need for the construction of efficient,inexpensive, maintainable, and modular software tools for application in synoptic scale atmospheric models. thesecond example and the analysis in zl95, clearly show how irregular sampling and undersampling at higherfrequencies by polarorbiting satellite instruments can lead to aliasing at scales which are of interest to climatestudies requiring monthly means. it was pointed out by halpern (1988) and ramage (1984) that an error of 0.5 ms1 in surface wind in the tropics may lead to an uncertainty of about 12 w m2 in surface heat flux. this amountof uncertainty was associated with a 4k increase in global sea surface temperature in model sensitivity testsreported by randall et al. (1992).3 existing methods and tools: prospects and limitationsin this section we critically evaluate some disciplinary and interdisciplinary methods that we haveexperimented with while trying to address the specific needs described in section two. we also discuss potentialimprovement and available products that may make these methods scale up to massive data.part of the special needs demonstrated by the first example in the previous section is for efficient datastorage and retrieval tools. the main goal of database researchers in the sequoia 2000 project is to provide anefficient storage and retrieval mechanism for the scientific data being collected, characterized by massive size(100 terabytes in four sites), complex data types as mentioned above, and sophisticated searching for scientificresearch. the project has been quite successful in the creation of software for the management of tertiary storage(tapes and cartridges), since the amount of data still exceeds the current economic capacity of magnetic disks.however, the sequoia 2000 benchmark, created as a model for testing and evaluating databases for earthscience users, does not address the critical issues mentioned previously in this paper. concretely, the sequoia2000 benchmark consists primarily of four kinds of data: raster data, point data, polygon data, and directed graphdata. while these benchmarks are very useful, they do not touch data assimilation problems that are the realbottleneck in the system.in general, the lack of attention to data assimilation has been the situation with computer vendors as well.many vendors currently offer hierarchical storage managers capable of storing and handling petabytes of data,typically using a combination of magnetic disks, optical juke boxes, and tapes/cartridges managed by robots. thesystem software knowsmassive data assimilation/fusion in atmospheric models and analysis: statistical, physical, andcomputational challenges98massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.how to handle and process bits, possibly arranged in columns as in relational database management systems, butthe handling of missing data and irregularly taken samples is beyond the state of art of current systems software.more recently, new objectrelational database management systems such as illustra have created support fornew access methods particularly suitable for special applications such as earth sciences. however, these newaccess methods are still concerned primarily with bit movement and comparisons. most commercial databasesand software, including illustra, simply assume that data are consistent and clean. in fact, discussions on dataassimilation are absent from almost all of sequoia 2000 papers on data management, including stonebraker et al.(1993). this is another manifestation of the difficulties of crossfertilization among disciplines.as noted above, most database management systems provide support only for precise data, though in thephysical world data are often imprecise. in these cases, the scientist is left with the unpleasant decision ofwhether to ignore the imprecise information altogether and store some approximation to the precise value, or toforgo the use of a standard database management system and manage the data directly. the latter is the decisionmost commonly taken. however, with ever increasing volumes of data and with real time processing demands(e.g., first example in section 2), such a decision can no longer be afforded. in many of the situations whereprecise data are not available, information much more useful than ''value unknown'' or "predicate is possiblytrue" is available, even though imprecise. one of the most common and useful forms of information available toa scientist is the ability to bound the amount of imprecision and estimate the error associated with the data. wethink that it could prove valuable if the database management system were able to represent and store valueswith bounded error, along with error covariance information, thus supporting the direct manipulation ofimprecise data in a consistent and useful manner according to physical models. in this context, the mostimmediate problems we need to address are: (1) can we represent imprecise and error information, (2) can wedevelop a data model for imprecise information, and (3) is it feasible to manipulate imprecise data in a consistentmanner? the value representation will be designed explicitly to deal with imprecise data with known errorbounds. our preliminary (disciplinary) work includes an algebra for interval relations that uses methods frominterval arithmetic as operators (barga and pu, 1993). algorithms designed for epsilon serializability (pu andleff, 1991) provide us with the means for bounding the amount of the imprecision introduced into the data.in atmospheric modeling, generally a forecast is combined with data in a manner that takes account ofcorrelation structure between the various sources of data. certain kinds of errors or imprecision have acomplicated correlation structure. putting an interval about such numbers and propagating these intervals byinterval methods do not capture common kinds of error structure that occur in meteorological observational databases. similarly, the often nonlinear physical model operations require different methods of propagatingimprecision. although interval methods are applicable in other sciences, more sophisticated error handling mustbe included to be useful in atmospheric modeling. we are interested in utilizing error covariance information andtesting the feasibility of building software tools that facilitate such integration. a toolkit approach is necessary tolower the cost of incorporating these error handling techniques into atmospheric models, so the model predictionproduced can achieve a quality inaccessible to naive models.massive data assimilation/fusion in atmospheric models and analysis: statistical, physical, andcomputational challenges99massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.it is clear from the previous section that the spatialtemporal sampling requires new methods for theinterpolation and extrapolation of the gridded data in a manner that will provide accurate estimates in gap areasand times. most applications of spatial estimation (i.e., objective analysis) have used models for the spatialtemporal covariance structure that are (1) separable in time and spaceši.e., that factor into separate models forthe spatial and temporal correlation structure, and (2) stationary in time and space. some do accommodatenonstationarity in the mean, but do not accurately reflect nonstationarity (heterogeneity) in the spatial covariancestructure. there is little reason to expect spatial covariance structures to be homogeneous over the spatial scalesof interest in our application. in our attempts to perform analyses based primarily on scatterometer data (e.g.,levy and brown, 1991: levy, 1994) we have experimented with common methods of interpolation andextrapolation of the original data. most (e.g., the bspline interpolation and smoothing in section 2) are incapableof handling the unique satellite nonsynoptic sampling pattern even on the much larger (monthly) scale (e.g., thesecond example in section 2).zl95 have designed a threedimensional spatialtemporal interpolator. it makes use of both the temporaland spatial sampling pattern of the satellite, substituting temporal information where spatial information ismissing, and vice versa. since there are usually nonmissing values around a missing value when both the timeand space dimensions are considered, a missing value at a point in space and time can be estimated as a linear(weighted) combination of the n nonmissing values found within a prescribed space and time 'neighborhood'.there are several shortcomings to the zl interpolator which need to be addressed if it is to be generalized.since the interpolator does not use any spatial or temporal correlation structure information the weights itemploys may be suboptimal. establishing a systematic approach to determine the optimal weight function forspecific satellite instruments or study goals would make the interpolator more robust. unfortunately, since onlysimulated data were used in zl95, there was no control set to verify the results in an absolute sense or to testwhether the interpolator weights are optimal for the true field. the ecmwf field (control set) in zl95 does notcontain high frequency oscillations with temporal scale shorter than the 6hour ecmwf analysis interval orspatial scale smaller than the grid spacing, which may appear in real satellite data. the rest of this sectionoutlines ideas for methods that may address these shortcomings and scale up to the massive satellite data.sampson and guttorp, 1992 (hereafter sg92) developed a modeling and estimation procedure forheterogeneous spatial correlation that utilizes the fact that much environmental monitoring data are taken overtime at fixed monitoring sites, and thus provide a sequence of replicates from which to compute spatialcovariances. their technique uses multidimensional scaling to transform the geographic coordinates into a spacewhere the correlation structure is isotropic and homogeneous so standard correlation estimation techniquesapply. when these methods are applied to polar orbiting satellite data rather than to reporting stations one isfaced again with the unique problem related to the sampling frequency: waiting for the satellite to return to thesame spatial location may result in an irrelevantly long temporal lag. additionally, with massive data andincreased resolution, the dimensionality of the problem gets to be unmanageably large. therefore the adaptationof the sg92 model to the massive satellite data sets requires different data processing and estimation procedures.we propose to start by simultaneously reducing the data into representative summaries andmassive data assimilation/fusion in atmospheric models and analysis: statistical, physical,and computational challenges100massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.modeling the spacetime correlation error structure. one can then directly carry out the spacetime estimation.relying on good physical models ensures that the statistical model is now needed merely to describe an errorfield which is usually much smaller in magnitude than the observed or predicted field. this approach derivesfrom an analysis and modeling of temporal autocorrelation functions and spacetime crosscorrelation functionsbound by strong physical constraints. it incorporates statistical error information into the zl95 idea of properdata reduction and substituting temporal information for missing spatial information, while relaxing some of theassumptions implicit in the zl method (e.g., stationarity, isotropy).4 summary and concluding remarksthe greatest proliferation in data volumes in atmospheric studies is due to satellite data. the importance ofthese data for monitoring the earth atmosphere and climate cannot be underestimated. however, the uniqueperspective from space that polar orbiting satellites have is accompanied by massiveness of data and a uniquesampling pattern which pose special problems to the traditional data assimilation and management procedures.for most applications these data need to be transformed into 'level 3' (gridded) form. we have presented twospecific examples of two different processes to illustrate the problems and special needs involved in properlygenerating the level 3 data. without devising proper tools for error assessment and correction, many of the level3 global data sets may lead to serious misinterpretation of the observations which can be further propagated intoatmospheric models.we have identified some disciplinary partial solutions, along with their limitations. new objectrelationaldatabase management systems offer some needed support for new access methods and for the management andstorage of massive data sets, but do not handle imprecise data. interval methods attempt to handle imprecise databut do not propagate observational error information properly. the zl95 method properly interpolates andreduces data on some scales, but may be suboptimal for some sensors. it is scale dependent, and does notincorporate statistical error information. the sg92 estimation technique handles heterogeneous spatialcorrelation for small and medium data sets, but does not scale up to massive data sets as its dimensionalityincreases unmanageably with increased data volume and resolution.a comprehensive solution is possible by a synergistic combination of the partial disciplinary solutions. wehave outlined an idea for a general methodology to incorporate the statistical error structure information with thephysical and dynamical constraints and with proper data reduction into representative summaries. a betterstatistical, physical, and numerical understanding of the error structure and growth may then lead to softwaresolutions that will properly propagate imprecision.acknowledgmentsthe authors are grateful to suzanne dickinson for generating the figures and commenting on themanuscript. this work was jointly supported by the divisions of mathematical sciences and atmosphericsciences at the national science foundation under grant dms9418904.massive data assimilation/fusion in atmospheric models and analysis: statistical, physical,and computational challenges101massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.references[1] barga r., and c. pu. accessing imprecise data: an interval approach . ieee data engineering bulletin, 16, 1215, 1993.[2] halpern, d., on the accuracy of monthly mean wind speeds over the equatorial pacific j. atmos. oceanic technol., 5, 362367, 1988.[3] halpern, d., o. brown, m. freilich, and f. wentz, an atlas of monthly mean distributions of ssmi surface wind speed, argos buoydrift. avhrr/2 sea surface temperature. ami surface wind components, and ecmwf surface wind components during 1992. jplpubli, 944, 143 pp., 1994.[4] levy, g., southern hemisphere low level wind circulation statistics from the seasat scatterometer. ann. geophys., 12, 6579, 1994.[5] levy, g., trends in satellite remote sensing of the planetary boundary layer, 1993. (review chapter), in trends in atmospheric sci., 1(1992), 337347. research trends pub.[6] levy, g., and r. a. brown, southern hemisphere synoptic weather from a satellite scatterometer. mon. weather rev., 119, 28032813,1991.[7] pu c., and a. leff, replica control in distributed systems: an asynchronous approach. in proceedings of the 1991 acm sigmodinternational conference on management of data, 377386, denver, may 1991.[8] ramage, c.s., can shipboard measurements reveal secular changes in tropical airsea heat flux? j. clim. appl. meteorol., 23, 187193,1984.[9] randall, d.a., et al., intercomparison and interpretation of surface energy fluxes in atmospheric general circulation models. j. geophys.res., 97, 37113724, 1992[10] sampson p.d., and p. guttorp, nonparametric estimation of nonstationary spatial covariance structure. journal of the americanstatistical association 87, 108119, 1992.[11] stonebraker, m., frew, j., gardels, k., and j. meredith, the sequoia 2000 storage benchmark, in proceedings of the 1993 acmsigmod international conference on management of data, washington, d.c., 1993.[12] zeng l. and g. levy, space and time aliasing structure in mean polarorbiting satellite data. journal of geophysical research,atmospheres, 100, d3, pp 51335142, 1995.massive data assimilation/fusion in atmospheric models and analysis: statistical, physical,and computational challenges102massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.part iiiadditional invited papers103massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.104massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.massive data sets and artificial intelligence planningrobert st. amant and paul r. cohenuniversity of massachusettsabstractgenerating a description of a massive dataset involves searching through an enormous space ofpossibilities. artificial intelligence (ai) may help to alleviate the problem. ai researchers are familiar with largesearch problems and have developed a variety of techniques to handle them. one area in particular, ai planning,offers some useful guidance about how the exploration of massive datasets might be approached. we describe aplanning system we have implemented for exploring small datasets, and discuss its potential application tomassive datasets.1 the information glutit has been said that the sunday new york times contains more unique assertions than an average personencountered during a lifetime in the renaissance. after a week of email and telephone calls, reading papers,listening to the news, skimming directmail catalogs and channelhopping on tv, we are given little respite bythe times. researchers in artificial intelligence (ai) have developed methods to both manage and exploit thisinformation glut, so that we might all enjoy a more contemplative life. we provide a quick tour of aiapplications for processing large amounts of data, and then focus on what ai can contribute directly to dataanalysis.a variety of ai applications have been developed to take advantage of the the availability of huge quantitiesof information in electronic form. examples include personal assistants that filter news stories, schedulemeetings, and "crawl the web" in search of information at remote internet sites. many of these applications areeasy to build and require little intelligence; the effort of a few hours is enough, for example, to write a filteringprogram to find calls for papers in the welter of internet traffic. more intelligence is required if an assistant is totake account of human goals and preferences. one's personal email assistant should learn that broadcastmessages about cars with lights on in the parking lot are irrelevant unless the car might be one's own. similarly,an information retrieval assistant shouldmassive data sets and artificial intelligence planning105massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.understand languagešno mean featšincluding indirect speech acts. if a student were to tell a hypotheticallibrarian assistant that she's looking for "a book by tukey," she means that she doesn't know which book shewants, she'd like to see some titles, and then she'd like the assistant to send her one.thomas, the frontend to congress's information system, relies on sophisticated information retrievalalgorithms in responding to user requests [ref]. natural language processing systems exploit online databases ofnewswire stories to learn rules that resolve wordsense ambiguity [ref]. machine learning techniques canautonomously generate classifications of stellar objects in large scale sky surveys [3]. planning systems can helporganize access to enormous quantities of satellite image data [6]. researchers in information retrieval, machinelearning, knowledge discovery in databases, and natural language understanding are finding that the informationglut is actually helpful, providing an inexhaustible supply of training data, test cases, and opportunities todevelop new techniques. their results are case studies in how massive datasets in different domains can behandled.our own research interest lies in exploratory data analysis (eda). we have developed an assistant forintelligent data exploration, called aide, to help users with the task. aide is a knowledgebased planningsystem that incrementally explores a dataset, guided by user directives and its own evaluation of indications inthe data [10]. the system is mixedinitiative: the user can let aide run unattended, searching for its ownconception of interesting structure, or can use aide as an interactive statistics package, or can combine the twoprocesses, letting aide perform some of the tasks of exploration, but under human guidance. while aide is nota system for exploration of massive datasets, we believe that the issues addressed by aide are relevant to anysystem that would explore data semiautonomously.2 an planning perspective on data analysiseda is a search problem, not unlike medical diagnosis or chess. at every juncture, you can take one ofseveral actionsštransform a variable, order a cat scan, sacrifice a bishop. after each action, your state ofknowledge changes. because actions are generally not free, you look into the future to evaluate potentialoutcomes, so the action you eventually select is by some local criterion "best." if local criteria guaranteed globalsuccess, chess wouldn't be a challenge. nor would diagnosis or eda. but even if local criteria are heuristicallygood, the search space in the vicinity of a decision can be enormous. the average branching factor of a search isthe average number of actions feasible at each decision. chess has an average branching factor of 40, orthereabouts, and because games last roughly 50 moves, the search space for chess contains 4050 positions. edahas a much larger branching factor. any statistics package permits dozens of operations on dozens or hundredsof subsets of variables. an analyst might run any of hundreds of operations at each juncture, from simpletransformations, to descriptive statistics, to model specification and analyses of residuals, and so on. the searchspace for eda, even for a small dataset, is effectively infinite.ai tames intractable search spaces two ways, by reformulating a problem to have a smaller search space,and by using expert knowledge to focus attention on small pieces of the space. our approach to data explorationrelies on both of these techniques. in order to explain further, we need to take a short detour for somebackground information about ai planning.massive data sets and artificial intelligence planning106massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.planning is a form of search, in which the task is to construct a sequence of steps that lead from a set ofinitial conditions to some desired conclusion. planners formulate the search problem in terms of states, goals,and sequences of actions to achieve goals. states are partial descriptions of the world, in the internalrepresentation of the system. in a data exploration system, the state includes the data under consideration, modelsof the behavior of the data, observations of unusual characteristics, and so forth. goals specify desired states. agoal might be, for example, to find a set of predictors for some specific variable. actions take the system fromone state to another. examples of eda actions include generating a linear fit for a relationship, powertransforming a variable, and other such operations. actions are defined by the preconditions or requirements fortheir application and the effects of their application.planners rely on task decomposition to solve complex problems, using knowledge about states, actions, andgoals to structure the search at different levels of abstraction [7]. the simplest planners work by backwardchaining. given a goal state, a planner begins by examining those actions that achieve the goal state. by treatingthe preconditions of these actions as goals to be satisfied in turn, and taking note of potential interactionsbetween actions, the planner recursively generates a sequence of appropriate actions.traditional ai planners construct plans from primitive actions, stating from scratch for each new problem.partial hierarchical planning takes a different approach. if a planning system repeatedly encounters similarproblems, or subproblems, it becomes wasteful to generate similar solutions from scratch each time. instead, apartial hierarchical planner can rely on a library of partial plans that apply to common cases. rather thanchoosing an appropriate action at some point during the planning process, the planner can choose a partial planin which many or all of the actions have already been decided on. if the library has sufficient coverage, planninglargely reduces to a matter of choosing which plan to apply at what time.problem reformulation and expert knowledge apply in the aide planning framework as follows. problemreformulation corresponds to capitalizing on knowledge of abstraction. it's clear that statisticians do not think ofeda as sequences of independent operations selected from menus of statistics packages. the fact that menushave names (e.g., transform data) suggests that elementary operations can be grouped into equivalence classes.data analysis plans can be formulated in terms of these classes of operations, not the individual operations.reformulating eda in terms of abstract operations (such as transform, decompose, fit, etc.) reduces thebranching factor of the search because there are fewer abstract actions to consider at each juncture. acombinatorially intractable search is replaced by two simpler searches: one to generate an abstract plan, the otherfor operations to instantiate the plan. for example, a useful abstract plan involves partitioning one or morevariables, performing an operation on each partition, then presenting the results in an organized way. histogramsdo this for one variable, contingency tables for two or more variables; tables of means in analysis of variance doit, too.aide takes advantage of expert knowledge by storing abstract plans, instead of trying to generate them denovo. we have developed a simple but powerful language in which users can express plans for eda. plans aretriggered by indications (which aide discovers automatically). for example, if a variable has "gaps" in itsrange, abstract plans are triggered to to search for another variable to explain the gaps and produce a contingencytable, tomassive data sets and artificial intelligence planning107massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.examine the separate partitions more closely, and so on. the number of options at any juncture is still largebecause aide generally finds many indications and can apply many plans with different parameterizations, butit is better structured and thus more manageable.3 an planning system for analysis of small datasetsto make the discussion above concrete, we present aide's planning representation. while the datahandling aspects of the language are novel and powerful, we make no claim that they will scale to massivedatasets. the plan language, on the other hand, implements strategic aspects of the exploration, which we believeare comparable for datasets of all sizes.3.1 representationaide's processing is built around a planning representation. in a functional language, procedures call oneanother directly. a statisticalsummary procedure, for example, might call a mean procedure during itsprocessing. in a planning language, control moves between plans indirectly, by the establishment and satisfactionof goals. rather than specifying that a mean should be computed, a plan might establish a goal (computelocation ?x). this goal could be satisfied by a mean, or median, or trimmedmean procedure, depending on thecharacteristics of ?x. the advantage gained is flexibility. instead of relying on functions that must be heavilyparameterized to produce appropriate behavior, the planning language distinguishes between two separatecontrol issues: what should be done, and how it should be done.an example of a plan specification is given below. a plan has a name, a goal that the plan can potentiallysatisfy, constraints on its bindings, and a body. the body of a plan is a control schema of subgoal specifications,subgoals which must be satisfied for the plan to complete successfully. an action specification is similar to aplan specification, except that its body contains arbitrary code, rather than a control schema.(defineplan histogram:goal (generatedescription :histogramtype ?batch ?histogram):body (:sequence(:subgoal (decompose (function uniquevalues) ?batch ?bins))(:map/transform (?bin ?bins ?histogram)(:subgoal (reduce (function count) ?bin))))):goal (generatedescription :histogramtype ?batch ? histogram):body (:sequence(:subgoal (decompose(function uniquevalues) ?batch ?bins))(:map/transform (?bin ?bins ?histogram)(:subgoal (reduce (function count) ?bin)))))this plan implements a simple procedure for generating a histogram of a discretevalued variable. in words:divide the range of the variable into its unique values; break the variable into subsets, one subset per value; countthe number of elements in each subset. the resulting counts are the bar heights of the histogram. in other words,we decompose the variable, which generates a new relation with a single attribute that contains the subsets. wethen apply a transformation, with an embedded reduction, which maps each subset relation to a single value, the"count" statistic of the subset.let's now consider a contingency table generated for a relationship between categorical variables <x,y>.the procedure is as follows. we divide the relationship into subsets, onemassive data sets and artificial intelligence planning108massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.corresponding to each unique combination of x and y values. each subset is structurally identical to the originalrelationship. we now record the number of observations in each subset. the resulting values are the cell countsfor the contingency table. a contingency table can thus be viewed as a twodimensional analog of a histogram.not surprisingly, the two procedures are identical in form:(defineplan contingencytable:goal (generatedescription :contingencytable ?xyrelationship ?table):body (:sequence(:subgoal (decompose (function uniquevalues) ?xyrelation ?cells))(:map/transform (?cell ?cells ?table)(:subgoal (reduce (function count) ?cell)))))in fact, we might use a single plan for both procedures, relying on subordinate plans to specializeappropriately on the type of the input data structure. these combinations are simple, hardly different from theway they would appear in a functional representation. the planning representation offers more flexibility thanshown here. in addition to sequencing and functional composition of primitives, the body of a plan can specifythat subgoals should be satisfied or actions executed in parallel, iteratively, conditionally, or in other ways.further, we can control how we evaluate whether a plan has completed successfully or not; we might wish toiterate over all possible ways to satisfy a subgoal, or simply accept the first plan that succeeds.a histogram/contingency table plan is a simple example of how procedures may generalize. a morecomplex example is shown below. this plan was initially designed to iteratively fit a resistant line to arelationship [2]. the resistant line generated by some methods is initially only an approximation, in the sensethat executing the same procedure on the residuals may give a line with nonzero slope. when this happens, thefit is reapplied to the residuals and the line parameters updated appropriately. when the magnitude of theincremental changes falls below some heuristic threshold, the iteration stops. this plan captures the processinginvolved in iterating over the residuals. (the subgoals have been elided for the sake of presentation.)(defineplan iterativefitplan:goal (describe :iterativefit ?operation ?relationship ?description):body (:sequence(:subgoal generatefit)(:action generateiterationrecord)(:while (not (null ?continueiterationp))(:sequence(:subgoal extractresidualrelationshipsubgoal)(:subgoal residualfitsubgoal)(:action updateiterationrecord)))(:subgoal evaluatefitsubgoal)))this kind of heuristic improvement is also part of other procedures, lowess being the most familiar. thesame plan can be used for both procedures. the fit subgoal is satisfied by different subordinate plans, theirselection depending on context. this example is againmassive data sets and artificial intelligence planning109massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.typical of the way complex procedures may be generated dynamically through plan combination. most plans donot specify behavior down to the level of primitive operations, but let the selection of appropriate subordinateplans depend on the context of intermediate results generated.finally, we have an example of a simple incremental modeling plan, instantiated in the exploration of adataset. it generates an initial model of the dataset of an appropriate type. it then establishes the goal ofelaborating the model. with the plans in the aide library, elaboration will involve adding relationships, one at atime, to the model. one of the plans that matches the elaboratemodel subgoal recursively establishes an identicalsubgoal, with ?model bound to the incrementally extended model.(defineplan explorebyincrementalmodeling ():goal (exploreby :modeling ?modeltype ?description ?structure ?model):constraint ((?structure ((:datasettype dataset)))):body (:sequence(:when (null ?model)(:subgoal initializesubgoal(generateinitialmodel ?description ?structure?modeltype ?model)))(:subgoal elaboratemodel(elaboratemodel ?modeltype ?activity?description ?structure ?model))))aide's plan library currently contains over 100 plans. fewer than a dozen plans are usually applicable atany point during exploration, and each plan constrains the set of subordinate plans applicable at later points.many of the plans are simple, on the order of univariate transformations and residual generation. others are morecomplex descriptive and modelbuilding procedures. some, like the last plan presented, implement controlstrategies applicable in a variety of different situations. with the plan representation we have found littledifficulty in implementing most familiar exploratory procedures. in addition to descriptive plans for resistantlines, various box plot procedures, and smoothing procedures, we have implemented forward selectionalgorithms for cluster analysis and regression analysis [9], a causal modeling algorithm [8], and a set of weakeropportunistic modeling procedures.3.2 controlling plan executionunless we can ensure that only a single procedure is applicable at any time, and that we know the exactsubset of the data we should concentrate onšan improbable scenariošwe must address a set of questionsconcerning control: which data structure (variable, relationship, model, etc.) should be considered? which plan (or plan type, in the case of higher level decisions) is appropriate? given a set of comparable but different results, produced by similar procedures (e.g., a leastsquares or aresistant line; four clusters or five), which result is best?massive data sets and artificial intelligence planning110massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.in aide, rules are used to make these decisions. the relevance of a rule is determined by structure andcontext constraints. by structure constraints, we mean that the feature and indication values specified in a rulemust be present in the structure under consideration. rules can thus constrain plans to act, for example, only onvariables with indications of clustering, or on relationships between continuous variables. context constraintsapply to the history of plan and subgoal activations that have led up to the current decision point. using contextconstraints a rule can, for example, prevent clusters detected in the residuals of a linear fit from being explored ifthe clustering plan has not yet been applied at the level of the original relationship. these constraints can pruneand order related decisions. these rules control the ordering and selection of data to explore, the plans to explorethem, and the results they produce.aide can thus run without assistance from the user. its rules let it evaluate candidate data structures toexplore; its library provides the plans that perform the exploration. not surprisingly, however, aide's lack ofcontextual knowledge will take the exploration in directions an informed human analyst would not need or wantto go. if a user is interested in gender issues, for example, partitions of data by sex will form a significant part ofthe exploration. aide cannot know that one user is interested in gender, another studies time series of climatechange, and so on. for this reason, aide is an analyst's assistant, not an autonomous package. it hunts forindications and suggests plans to explore them, but the analyst guides the whole process.the process is mixedinitiative in the following sense. aide explores a dataset by elaborating andexecuting a hierarchy of plans. decision points in the hierarchy are made available for the user's inspection. theuser guides exploration by changing the ordering of plans, by selecting new plans, or by shifting to differentareas in the data. this guidance includes selecting appropriate dataset variables and subsets to be explored aswell as applying any available statistical manipulations to the selected data. the arrangement gives the user mostof the flexibility of the usual statistical interface, while still remaining within the planning framework.each decision point gives the user the opportunity to view the choicesšand the dataš currently underconsideration. the user can step through the process, letting the planner execute only until another decision isreached. the user can let planner execution continue until some descriptive result has been generated, at whichpoint another decision can be made using the new information. at any time the user can override aide'sdecisions about the operation to execute or data to examine. aide keeps the user constantly informed withdescriptions of the data being explored, justifications for candidate operations at a decision point, orderingconstraints on choices, and descriptions of the plans under consideration.4 discussionaide represents a compromise between two eventually unworkable approaches to data analysis. on onehand we have statistics packages, which are driven by users, and treat actions as independent. the search spacefor eda is intractable when elementary actions are independent selections from statistics package menus. on theother hand, we have autonomous ''black boxes'' from machine learning and statistics, algorithms like stepwisemultiple regresmassive data sets and artificial intelligence planning111massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.sion, cart, and c4. these require nothing but data from the user, but nor does the user have much opportunityto control how they work, and the results are never explained. your choice is to relinquish all control, or controlevery detail, of analyses. aide offers an alternative, involving the user in the strategic aspect of data analysis,but reducing the tactical burden.to manage the process aide relies on the techniques of ai planning. we have encountered some strongsimilarities between planning problems and the types of problems found in eda. these similarities yield somesome potentially useful implications for the processing of massive datasets, implications that we have onlybegun to study.peter huber observes that data analysis differs from other computersupported tasks, such as programmingand text preparation. in the latter tasks the final version is everything; there is no need to know the path by whichit was reached. in data analysis the correctness of the end product cannot be checked without inspecting the pathleading to it [5]. huber argues that existing paradigms are thus inappropriate for data analysis tasks. if a massivedataset is to be analyzed without the direct supervision of a human user, then a representation of the processcarried out is a necessary component of the result. this is one of the key distinctions between planning and otherforms of searchšthe aim is to generate a sequence (or more complex combination) of operations, not simply theend result.one of the bestknown observations to arise in the planning literature is that problem decomposition is mosteffective when the subproblems that are generated are largely independent of one another. a recent ai textbookcontains an example in which a search problem, if solved directly, requires a search of 1030 states. in contrast, ahierarchical decomposition brings the search space down to 600 states [7]. by imposing structure on the searchspace we can sometimes transform an intractable problem into a relatively simple one. using an analogydeveloped during the massive datasets workshop, we observe that the effectiveness of exploration of a massivedataset will depend on our knowledge of its geography: we need to know either where to look for patterns orwhat kinds of patterns to look for. in the absence of this knowledge we have no way to carve up the search spaceinto smaller, more manageable blocks. a hierarchical approach to exploring a massive dataset can be effective tothe extent that the decomposition follows the geography of the dataset.a planner can derive great benefit from compiled knowledge in the form of a plan library. much of theknowledge we bring to bear in solving problems takes the form of procedures or sequences of actions forachieving particular goals [4]. planning problems in some complex environments are too difficult to solvewithout the user of sophisticated plan libraries [1]. similarly, the exploration of massive datasets poses a problemthat may only be solved with both knowledge of individual data analysis operations and knowledge about howthey can be combined.our longterm objectives for this research include fully automated modelbuilding and discoverymechanisms driven by an opportunistic control strategy. we expect to develop the automated strategies from ourexperience with the system as a manual analysis decision aid, letting human analysts provide much of the initialreasoning control strategy. although we are developing this system for our own use in modeling complexprogram behavior, we believe it to be potentially useful for any scientific modeling problem, particularly those inwhich the process of data collection is itself automated and the resulting volume of data overwhelming forhuman analysts (e.g., astronomical surveys or remote sensing of terrestrialmassive data sets and artificial intelligence planning112massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.features).in sum, ai researchers are responding to the challenges and opportunities afforded by massive amounts ofelectronic data. capitalizing on the opportunities involves statistical reasoning, although not all ai researchersrecognize it as such. statistics provides a foundation for analysis and design of ai programs. reciprocally, someai programs facilitate the work of applied statisticians, not only in eda, but also in model induction and testing.acknowledgmentsthis research is supported by arpa/rome laboratory under contract #f30602930100, and by the dept.of the army, army research office under contract #daah049510466. the u.s. government is authorized toreproduce and distribute reprints for governmental purposes not withstanding any copyright notation hereon. theviews and conclusions contained herein are those of the authors and should not be interpreted as necessarilyrepresenting the official policies or endorsements either expressed or implied, of the advanced research projectsagency, rome laboratory, or the u.s. government.references[1] paul r. cohen, michael l. greenberg, david m. hart, and adele e. howe. trial by fire: understanding the design requirements foragents in complex environments. ai magazine, 10(3):3248, fall 1989.[2] john d. emerson and michal a. stoto. transforming data. in david c. hoaglin, frederick mosteller, and john w. tukey, editors,understanding robust and exploratory data analysis. wiley, 1983.[3] usama fayyad, nicholas weir, and s. djorgovski. skicat: a machine learning system for automated cataloging of large scale skysurveys . in proceedings of the tenth international conference on machine learning , pages 112119. morgan kaufmann, 1993.[4] michael p. georgeff and amy l. lansky. procedural knowledge. proceedings of the ieee special issue on knowledge representation ,74(10):13831398, 1986.[5] peter j. huber. data analysis implications for command language design. in k. hopper and i. a. newman, editors, foundation forhumancomputer communication. elsevier science publishers, 1986.[6] amy l. lansky and andrew g. philpot. aibased planning for data analysis tasks. ieee expert, winter 1993.[7] stuart russell and peter norvig. artificial intelligence: a modem approach. prentice hall, 1995.massive data sets and artificial intelligence planning113massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.[8] robert st. amant and paul r. cohen. toward the integration of exploration and modeling in a planning framework. in proceedings of theaaai94 workshop in knowledge discovery in databases, 1994.[9] robert st. amant and paul r. cohen. a case study in planning for exploratory data analysis. in advances in intelligent data analysis ,pages 15, 1995.[10] robert st. amant and paul r. cohen. control representation in an eda assistant. in douglas fisher and hans lenz, editors, learning from data: ai and statistics v. springer, 1995. to appear.massive data sets and artificial intelligence planning114massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.massive data sets: problems and possibilities, withapplication to environmental monitoringnoel cressieiowa state universityanthony olsenu.s. environmental protection agencydianne cookiowa state universityabstractmassive data sets are not unlike small to large data sets in at least one respect, namely it is essential to knowtheir context before one starts an analysis. that context will almost certainly dictate the types of analysesattempted. however, the sheer size of a massive data set may challenge and, ultimately, defeat a statisticalmethodology that was designed for smaller data sets. this paper discusses the resulting problems andpossibilities generally and, more specifically, considers applications to environmental monitoring data.introductionmassive data sets are measured in gigabytes (109 bytes) and terabytes (1012 bytes). we can think aboutthem, talk about them, access them, and analyze them because data storage capabilities have evolved over thelast 10,000 years (especially so over the last 20 years) from human memory, to stone, to wood, bark, and paper,and to various technologies associated with digital computers. with so much data coming on line andimprovements in query languages for data base management, data analysis capabilities are struggling to keep up.the principal goal of data analysis is to turn data into information. it should be the statistician's domain butthe technology will not wait for a community whose evolutionary time scales are of the order of 510 years. as aconsequence, scientists working with massive data sets will commission analyses by people with good computertraining but often minimal statistics training. this scenario is not new but it is exacerbated by massivedatariches (e.g., in environmental investigations, an area familiar to us).we would argue that statisticians do a better job of data analysis because they are trained to understand thenature of variability and its various sources. non statisticians often think of statistics as relevant only for dealingwith measurement error, which may be the least important of the sources of variability.massive data sets: problems and possibilities, with application to environmental monitoring115massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.types of massive data setsalthough "massive data sets" is the theme of this workshop, it would be a mistake to think, necessarily, thatwe are all talking about the same thing. a typology of the origin of massive data sets is relevant to theunderstanding of their analyses. amongst others, consider: observational records and surveys (health care,census, environment); process studies (manufacturing control); science experiments (particle physics). another"factor" to consider might be the questions asked of the data and whether the questions posed were explicitly partof the reason the data were acquired.statistical data analysisas a preamble to the following remarks, we would like to state our belief that good data analysis, even in itsmost exploratory mode, is based on some more or less vague statistical model. it is curious, but we haveobserved that as data sets go from "small" to "medium," the statistical analysis and models used tend to becomemore complicated, but in going from "medium" to "large,'' the level of complication may even decrease! thatwould seem to suggest that as a data set becomes "massive,'' the statistical methodology might once again bevery simple (e.g., look for a central tendency, a measure of variability, measures of pairwise association betweena number of variables). there are two reasons for this. first, it is often the simpler tools (and the models thatimply them) that continue to work. second, there is less temptation with large and massive data sets to "chasenoise." think of a study of forest health, where there are 2 × 106 observations (say) in vermont: a statisticiancould keep him(her)self and a few graduate students busy for quite some time, looking for complex structure inthe data. instead, suppose the study has a national perspective and that the 2 × 106 observations are part of amuch larger data base of 5 × 108 (say) observations. one now wishes to make statements about forest health atboth the national and regional level but for all regions. but the resources to carry out the bigger study are not 250times more. the data analyst no longer has the luxury of looking for various nuances in the data and so declaresthem to be noise. thus, what could be signal in a study involving vermont only, becomes noise in a studyinvolving vermont and all other forested regions in the country.the massiveness of the data can be overwhelming and may reduce the non statistician to asking oversimplified questions. but the statistician will almost certainly think of stratifying (subsetting), allowing for abetweenstrata component of variance. within strata, the analysis may proceed along classical lines that looks forreplication in errors. or, using spatiotemporal analysis, one may invoke the principle that nearby (in space andtime) data or objects tend to be more alike than those that are far apart, implying redundancies in the data.another important consideration is dimension reduction when the number of variables is large. dimensionreduction is more than linear projections to lower dimensions such as with principal components. nonlineardimension reduction techniques are needed that can extract lowerdimensional structure present in a massive dataset. these new dimensionreduction techniques in turn imply new methods of clustering. where space and/ortime coordinates are available, these data should be included with the original observations.massive data sets: problems and possibilities, with application to environmental monitoring116massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.at the very least they could simply be concatenated together to make a slightly higher dimensional(massive) data set. however, the special nature of the spatial and temporal coordinates is apparent in problemswhere the goal is spacetime clustering for dimension reduction.an issue that arises naturally from the discussion above is how one might compare two "useful" models.we believe that statistical model evaluation is a very important topic for consideration and that models should bejudged both on their predictive ability and on their simplicity.one has to realize that interactive data analysis on all but small subsets of the data may be impossible. itwould seem sensible then that "intelligent" programs, that seek structure and pattern in the data, might be letloose on the data base at times when processing units might otherwise be idle (e.g., carr, 1991; openshaw,1992). the results might be displayed graphically and animation could help compress long time sequences ofinvestigation. graphics, with its ability to show several dimensions of a study on a single screen, should beincorporated whenever possible. for example, mcdonald (1992) has used a combination of realtime andanimation to do rotations on as many as a million data points.it may not be necessary to access the whole data set to obtain a measure of central tendency (say). statisticaltheory might be used to provide sampling schemes to estimate the desired value; finite population samplingtheory is tailormade for this task. sampling (e.g., systematic, adaptive) is particularly appropriate when there areredundancies present of the type described above for spatiotemporal data.it is not suggested that the unsampled data should be discarded but, rather, that it should be held for futureanalyses where further questions are asked and answered. in the case of longterm monitoring of theenvironment, think of an analogy to medicine where all sorts of data on a patient are recorded but often neverused in an analysis. some are, of course, but those that are not are always available for retrospective studies orfor providing a baseline from which unusual future departures are measured. in environmental studies, thetendency has been to put a lot of resources into data collection (i.e., when in doubt, collect more data).sampling offers a way to analyze massive data sets with some statistical tools we currently have. data thatexhibit statistical dependence donot need to be looked at in their entirety for many purposes because there is much redundancy.application to the environmental sciencesenvironmental studies, whether they are involved in longterm monitoring or shortterm wastesitecharacterization and restoration, are beginning to face the problems of dealing with massive data sets. most ofthe studies are observational rather than designed and so scientists are scarcely able to establish much more thanassociation between independent variables (e.g., presence/absence of pollutant) and response (e.g., degradationof an ecosystem). national longterm monitoring, such as is carried out in the u.s. environmental protectionagency (epa)'s environmental monitoring and assessment program (emap), attempts to deal with this byestablishing baseline measures of mean and variance from which future departuresmassive data sets: problems and possibilities, with application to environmental monitoring117massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.might be judged (e.g., messer, linthurst, and overton, 1991). national or large regional programs will typicallydeal with massive data sets. for example, sample information (e.g., obtained from field studies) from a limitednumber of sites (1,000 to 80,000) will be linked with concomitant information in order to improve national orregional estimation of the state of forests (say). thematic mapper (1010 observations), avhrr (8 × 106observations), digital elevation (8 × 106 observations), soils, and so forth, coverage information is relatively easyand cheap to obtain. where do we stop? can statistical design play a role here to limit the "factors"? moreover,once the variables are chosen, does one lose anything by aggregating the variables spatially (and temporally)?the scientist always asks for the highest resolution possible (so increasing the massiveness of the data set)because of the well known ecological fallacy that shows a relationship between two variables at an aggregatedlevel may be due simply to the aggregation rather than to any real link. (simpson's paradox is a similarphenomenon that is more familiar to the statistics community.) one vexing problem here is that the variousspatial coverages referred to above are rarely acquired with the same resolution. statistical analyses mustaccommodate our inability to match spatially all cases in the coverages.environmental studies often need more specialized statistical analyses that incorporate the spatiotemporalcomponent. these four extra dimensions suggest powerful and obvious ways to subset (massive) environmentaldata sets. also, biological processes that exhibit spatiotemporal smoothness can be described and predicted withparsimonious statistical models, even though we may not understand the etiologies of the phenomena. (theatmospheric sciences have taken this "prediction" approach even further, now giving "data" at every latitudelongitude node throughout the globe. clearly these predicted data have vastly different statistical properties thanthe original monitoring data.)common georeferencing of data bases makes all this possible. recent advances in computing technologyhave led to geographic information systems (giss), a collection of hardware and software tools that facilitate,through georeferencing, the integration of spatial, nonspatial, qualitative, and quantitative data into a data basethat can be managed under one system environment. much of the research in gis has been in computer scienceand associated mathematical areas; only recently have giss begun to incorporate modelbased spatial statisticalanalysis into their informationprocessing subsystems. an important addition is to incorporate exploratory dataanalysis tools including graphics into giss, if necessary by linking a gis with existing statistical software(majure, cook, cressie, kaiser, lahiri, and symanzik, 1995).conclusionsstatisticians have something to contribute to the analysis of massive data sets. their involvement isoverdue. we expect that new statistical tools will arise as a consequence of their involvement but, equally, webelieve in the importance of adapting existing tools (e.g., hierarchical models, components of variance,clustering, sampling). environmental and spatiotemporal data sets can be massive and represent important areasof application.massive data sets: problems and possibilities, with application to environmental monitoring118massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.acknowledgmentthis research was supported by the environmental protection agency under assistance agreement no.cr822919010.referencescarr, d. b. (1991). looking at large data sets using binned data plots, in a. buja and p. a. tukey, eds. computing and graphics in statistics,springer verlag, new york, 739.mcdonald, j. a. (1992). personal demonstration of software produced at university of washington, seattle, wa.majure, j. j., cook, d., cressie, n., kaiser, m., lahiri, s., and symanzik, j. (1995). spatial cdf estimation and visualization withapplications to forest health monitoring. computing science and statistics , forthcoming.messer, j. j., linthurst, r. a., and overton, w. s. (1991). an epa program for monitoring ecological status and trends . environmental monitoring and assessment, 17, 6778.openshaw, s. (1992). some suggestions concerning the development of ai tools for spatial analysis and modeling in gis. annals ofregional science, 26, 3551.massive data sets: problems and possibilities, with application to environmental monitoring119massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.massive data sets: problems and possibilities, with application to environmental monitoring120massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.visualizing large data setsstephen g. eick*bell laboratories (a division of lucent technologies)abstractvisualization is a key technology for understanding large datasets. it is useful throughout the analysisprocess, for exploratory descriptive analysis, to aid in model building; and for presenting the analysis results.our approach to visualizing abstract, nongeometric data involves domainspecific representations, multiplelinked views, color, and a highlyinteractive user interface using filtering and focusing to reduce visual clutter.we have developed a software infrastructure embodying our design principles for producing novel, highqualityvisualizations of corporate datasets.1 introductionjust as spreadsheets revolutionized our ability to understand small amounts of data, visualization willrevolutionize the way we understand large datasets. our research focuses on extracting the information latent inlarge databases using visual techniques. the difficulty in extracting this information lies in understanding thecomplexity of the databases. to aid in this task, we have created many novel, highly interactive visualizations oflarge datasets. this involved developing the techniques, software tools, and infrastructure to mine knowledgefrom corporate databases so that it can be put to competitive and commercial advantage.* at&t bell laboratoriesrm 1g351, 1000 east warrenville road, naperville, il 60566, email: eick@research.att.comvisualizing large data sets121massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.2 domainspecific representationa key component of an effective visualization involves the visual representation of the data. therepresentation determines how the items in the dataset are rendered on the computer display. the bestrepresentations are often domainspecific: scatterplots for statistical data, maps for spatial data, and node andlink diagrams for network data, for example. inventing a representation for a new domain is a difficult, creative,and iterative process.1 the representation should take full advantage of perceptual cues such as size. positions.color, depth, and may even use motion and sound.3 high information densityour representations are often compact, colorcoded glyphs positioned spatially. by using compact glyphsthat overplot gracefully we can pack a lot of information into an image and thereby display a large dataset. ahighresolution 1280×1024 workstation monitor has over 1,300,000 pixels. our goal is to use every pixel todisplay data, thereby maximizing the information content. in the image.in some cases is is possible to display an entire dataset on a single screen, thereby eliminating the difficultnavigation problems associated with panning and zooming interfaces that focus on small portions of the database.4 interactive filtersoften informationdense displays become overly cluttered with too much detail. one approach to solvingthe display clutter problem involves interactive filters that reduce the amount of information shown on thedisplay. humans have sophisticated pattern recognition capabilities, perhaps due to our evolution, and are veryefficient at manipulating interactive controls to reduce visual clutter. we exploit this to effortlessly solve thecomplex computational problems involved with determining when a display is too busy for an easyinterpretation. our approach is to leverage people's natural abilities by designing user interface controls thatparameterize the display complexity.1 see the figures for examples from some domains that we have considered.visualizing large data sets122massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.5 multiple linked viewsthe power of our representations is magnified through the use of interaction and linked views. each view,whether custom or standard (color keys, bar charts, box plots, histograms, scatter plots, etc.), functions both as adisplay and a control panel. selecting and filtering data in one view instantly propagates to the other views,thereby providing additional insights. linking multiple views interactively provides an integrated visualizationfar more powerful than the sum of the individual views.6 systemsour systems have been used to successfully analyze and present software version control information. filesystem sizes. budgets, network traffic patterns, consumer shopping patterns, relational database integrityconstraints, resource usage on a compute server, etc. the amount of information that our systems present on asingle screen is between 10,000 and 1,000,000 records. some of the more interesting systems we have builtinclude:1. seesofttmlines of text in files [eic94] (figure 1)2. seesliceprogram slices and code coverage [be94] (figure 2)3. seelogtimestamped log reports [el95] (figure 3)4. seedatarelational data [aep95] (figure 4)5. seenetgeographic networks data [bew95] (figures 5 and 6)6. nicheworkstmabstract networks [ew93] (figure 7)7. seedifftmfile system differences8. seelibbibliographic databases [ejw94] (figure 9)9. seesyshierarchical software modules [be95] (figure 10)10. seesalestmretail sales inventory and forecasts (figure 11)11. seetreehierarchical datavisualizing large data sets123massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.visualizing large data sets124figure 1. lines of code colored by agefigure 2. forward program slicefigure 3. log file viewfigure 4. relational database viewfigure 5. christmas morning longdistance trafficfigure 6. world wide internet trafficmassive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.visualizing large data sets125figure 7. market basket analysisfigure 8. demographic informationfigure 9. document retrievalfigure 10. hierarchical system viewfigure 11. sales by week and eventfigure 12. organization productivity by weekmassive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.12. seefraudnetwork calling fraud.since the needs of each user are unique, the best visualizations are taskoriented. the most successfulvisualizations help frame interesting questions as well as answer them. our visualizations: make use of existing data. in many cases large databases of vital importance to an organization alreadyexist. our visualizations extract meaningful information from this data. are directed toward real problems with targeted users. our efforts are motivated by business needs andaddress real problems. focus on understanding and insight. results are more important than any particular technique. are used throughout the analysis process including the initial data exploration, intermediate modelformulation, and final result presentation.7 software and technologyunderlying all of our visualizations is a common infrastructure embodied in a c++ library that handlesinteraction, graphics, and view linking. this c++ visualization library helps us to: minimize our development time, encapsulate expertise and design principles, build crossplatform systems (unix/x11, open gl, and pc/windows), and keep visualization application code small.visualizing large data sets126massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.8 conclusionvisualization is a key technology that can help users understand the complexity in industrialsized systems.we have exploited this technology to investigate a variety of large and complex data sets. interactive datavisualization is complementary to other analytic, modelbased approaches and will become a widely used toolfor extracting the information contained in large complex datasets.acknowledgmentsthe research presented here represents the joint efforts of jackie antis. dave atkins, tom ball, brianjohnson, ken cox, nate dean, paul lucas, john pyrce, and graham wills.references[aep95] jacqueline m. antis, stephen g. eick, and john d. pyrce. visualizing the structure of relational databases. ieee software,accepted for publication 1995.[be94] thomas ball and stephen g. eick. visualizing program slices. in 1994 ieee symposium on visual languages, pages 288295, st.louis, missouri, 4 october 1994.[be95] marla j. baker and stephen g. eick. spacefilling software displays. journal of visual languages and computing, 6(2), june 1995.[bew95] richard a. becker, stephen g. eick, and allan r. wilks. visualizing network data. ieee transactions on visualization andgraphics, 1(1):1628, march 1995.[eic94] stephen g. eick. graphically displaying text. journal of computational and graphical statistics, 3(2):127142. june 1994.visualizing large data sets127massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.[ejw94] stephen g. eick, eric e. sumner jr., and graham j. wills. visualizing bibliographic databases. in john p. lee and georges g.grinstein, editors. database issues for data visualization, pages 186193. springerverlag, october 1994. lecture notes incomputer science.[el95] stephen g. eick and paul j. lucas. displaying trace files. software practice and experienced, accepted for publication 1995.[ew93] stephen g. eick and graham j. wills. navigating large networks with hierarchies. in visualization '93 conference proceedings,pages 204210. san jose, california, 2529 october 1993.visualizing large data sets128massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.from massive data sets to science catalogs: applications andchallengesusama fayyadmicrosoft researchpadhraic smythuniversity of california, irvineabstractwith hardware advances in scientific instruments and data gathering techniques comes the inevitable floodof data that can render traditional approaches to science data analysis severely inadequate. the traditionalapproach of manual and exhaustive analysis of a data set is no longer feasible for many tasks ranging fromremote sensing, astronomy, and atmospherics to medicine, molecular biology, and biochemistry. in this paper wepresent our views as practitioners engaged in building computational systems to help scientists deal with largedata sets. we focus on what we view as challenges and shortcomings of the current stateoftheart in dataanalysis in view of the massive data sets that are still awaiting analysis. the presentation is grounded inapplications in astronomy, planetary sciences, solar physics, and atmospherics that are currently driving much ofour work at jpl.keywords: science data analysis, limitations of current methods, challenges for massive data sets,classification learning, clustering.note: both authors are affiliated with machine learning systems group, jet propulsion laboratory, m/s 5253660,california institute of technology, pasadena, ca 91109, http://wwwaig.jpl.nasa.gov/mls/.from massive data sets to science catalogs: applications and challenges129massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.1 introductionthe traditional approach of a scientist manually examining a data set, and exhaustively cataloging andcharacterizing all objects of interest, is often no longer feasible for many tasks in fields such as geology,astronomy, ecology, atmospheric and ocean sciences, medicine, molecular biology, and biochemistry.the problem of dealing with huge volumes of data accumulated from a variety of sources is now largelyrecognized across many scientific disciplines. database sizes are already being measured in terabytes (1012bytes), and this size problem will only become more acute with the advent of new sensors and instruments [9,34]. there exists a critical need for information processing technologies and methodologies to manage the dataavalanche. the future of scientific information processing hinges upon the development of algorithms andsoftware that enable scientists to interact effectively with large scientific data sets.1.1 backgroundthis paper reviews several ongoing automated science cataloging projects at the jet propulsion laboratory(sponsored by nasa) and discusses some general implications for analysis of massive data sets in this context.since much of nasa's data is remotelysensed image data the cataloging projects have focused mainly onspatial databases, which are essentially large collections of spatiallygridded sensor measurements of the sky,planetary surfaces and earth where the sensors are operating within some particular frequency band (optical,infrared, microwave, etc). it is important to keep in mind that the scientific investigator is primarily interested inusing the image data to investigate hypotheses about the physical properties of the target being imaged and he orshe is not directly interested in the image data per se. hence, the image data merely serve as an intermediaterepresentation that facilitates the scientific process of inferring a conclusion from the available evidence.in particular, scientists often wish to work with derived image products, such as catalogs of objects ofinterest. for example, in planetary geology, the scientific process involves examination of images (and otherdata) from planetary bodies such as venus and mars, the conversion of these images into catalogs of geologicobjects of interest (such as craters, volcanoes, etc.), and the use of these catalogs to support, refute, or originatetheories about the geologic evolution and current state of the planet. typically these catalogs contain informationabout the location, size, shape, and general context of the object of interest and are published and made generallyavailable to the planetary science community [21]. there is currently a significant shift to computeraidedvisualization of planetary data, a shift which is driven by the public availability of many planetary data sets indigital form on cdroms [25].in the past, both in planetary science and astronomy, images were painstakingly analyzed by hand and muchinvestigative work was carried out using hardcopy photographs or photographic plates. however, the image datasets that are currently being acquired are so large that simple manual cataloging is no longer practical, especiallyif any significant fraction of the available data is to be utilized. this paper briefly discusses nasarelatedprojects where automated cataloging is essential, including the second palomar observatory sky survey (possii) and the magellansar (synthetic aperture radar) imagery of venus returned by the magellan spacecraft.both of these image databases are too large for manual visual analysis and provide excellent examples of theneed for automated analysis tools.the possii application demonstrates the benefits of using a trainable classification approach in a contextwhere the transformation from pixel space to feature space is wellunderstood. scientistsfrom massive data sets to science catalogs: applications and challenges130massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.often find it easier to define features of objects of interest than to produce recognition models for these objects.possii illustrates the effective use of prior knowledge for feature definition: for this application, the primarytechnical challenges were in developing a classification model in the resulting (relatively) highdimensionalfeature space.in the magellansar data set the basic image processing is not wellunderstood and the domain experts areunable to provide much information beyond labeling objects in noisy images. in this case, the significantchallenges in developing a cataloging system lie in the feature extraction stage: moving from a pixelrepresentation to a relatively invariant feature representation.1.2 developing science catalogs from datain a typical science data cataloging problem, there are several important steps:1. decide what phenomena are to be studied, or what hypotheses are to be evaluated.2. collect the observations. if the data are already in existence then decide which subsets of the dataare of interest and what transformations or preprocessing are necessary.3. find all events of interest in the data and create a catalog of these with relevant measurements ofproperties.4. use the catalog to evaluate current hypotheses or formulate new hypotheses of underlyingphenomena.it is typically the case that most of the work, especially in the context of massive data sets is in step 3, thecataloging task. it is this task that is most tedious and typically prohibitive since it requires whoever is doing thesearching, be it a person or a machine, to sift throughout the entire data set. note that the most significant''creative'' analysis work is typically carried out in the other steps (particularly 1 and 4).in a typical cataloging operation, the recognition task can in principle be carried out by a human, i.e., atrained scientist can recognize the target when they come across it in the data (modulo fatigue, boredom, andother human factors). however, if the scientist were asked to write a procedure, or computer program, to performthe recognition, this would typically be very difficult to do. translating human recognition and decisionmakingprocedures into algorithmic constraints that operate on raw data is in many cases impractical. one possiblesolution is the pattern recognition or "trainingbyexample" approach: a user trains the system by identifyingobjects of interest and the system automatically builds a recognition model rather than having the user directlyspecifying the model. in a sense, this trainingbyexample approach is a type of exploratory data analysis (eda)where the scientist knows what to look for, but does not know how to specify the search procedure in analgorithmic manner. the key issue is often the effective and appropriate use of prior knowledge. for pixellevelrecognition tasks, prior information about spatial constraints, invariance information, sensor noise models, andso forth can be invaluable.2 science cataloging applications at jpl2.1 the skicat projectthe sky image cataloging and analysis tool (skicat, pronounced "skycat") has been developed for useon the images resulting from the possii conducted by caltech. the photographic platesfrom massive data sets to science catalogs: applications and challenges131massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.are digitized via highresolution scanners resulting in about 3,000 digital images of 23,040 × 23, 040 pixels each,16 bits/pixel, totaling over three terabytes of data. when complete, the survey will cover the entire northern skyin three colors, detecting virtually every sky object down to a b magnitude of 22 (a normalized measure of objectbrightness). this is at least one magnitude fainter than previous comparable photographic surveys. it is estimatedthat at least 5 × 107 galaxies and 2 × 109 stellar objects (including over 105 quasars) will be detected. this dataset will be the most comprehensive largescale imaging survey produced to date and will not be surpassed inscope until the completion of a fully digital allsky survey.the purpose of skicat is to facilitate the extraction of meaningful information from such a large data setin an efficient and timely manner. the first step in analyzing the results of a sky survey is to identify, measure,and catalog the detected objects in the image into their respective classes. once the objects have been classified,further scientific analysis can proceed. for example, the resulting catalog may be used to test models of theformation of largescale structure in the universe, probe galactic structure from star counts, perform automaticidentifications of radio or infrared sources, and so forth [32, 33, 8]. reducing the images to catalog entries is anoverwhelming manual task. skicat automates this process, providing a consistent and uniform methodologyfor reducing the data sets.2.1.1 classifying sky objectseach of the 3,000 digital images is subdivided into a set of partially overlapping frames. lowlevel imageprocessing and object separation is performed by a modified version of the focas image processing software[20]. features are then measured based on this segmentation. the total number of features measured for eachobject by skicat is 40, including magnitudes, areas, sky brightness, peak values, and intensity weighted andunweighted pixel moments. some of these features are generic in the sense that they are typically used inanalyses of astronomical image data [31]: other features such as normalized and nonlinear combinations arederived from the generic set.once all the features are measured for each object, final classification is performed on the catalog. the goalis to classify objects into four categories, following the original scheme in focas: star, star with fuzz, galaxy,and artifact (an artifact represents anything that is not a sky object, e.g. satellite or airplane trace, filmaberrations, and so forth).2.1.2 classifying faint sky objectsin addition to the scanned photographic plates, we have access to ccd images that span several smallregions in some of the plates. the main advantage of a ccd image is higher spatial resolution and higher signaltonoise ratio. hence, many of the objects that are too faint to be classified by inspection on a photographic plateare easily classifiable in a ccd image. in addition to using these images for photometric calibration of thephotographic plates, the ccd images are used for two purposes during training of the classification algorithm:(i) they enable manual identification of class labels for training on faint objects in the original (lower resolution)photographic plates, and (ii) they provide a basis for accurate assessment of human and algorithmic classificationperformance on the lower resolution plates. hence, if one can successfully' build a model that can classify faintobjects based on training data from the plates that overlap with the limited highresolution ccd images, thenthat model could in principle classify objects too faint for visual classification by astronomers or traditionalcomputational methods used in astronomy. faint objects constitute the majority of objects in any image.from massive data sets to science catalogs: applications and challenges132massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.the classification learning algorithms used are decision tree based, as in [3, 24]. the particular algorithmsused in skicat are covered in [12, 14, 11]. the basic idea is to use greedy tree growing algorithms to find aclassifier in the high dimensional feature space. a system called ruler [12] is then used to optimize rules froma multitude of decision trees trained using random sampling and cross validation. ruler applies pruningtechniques to rules rather than trees as in cart [3, 24]. a rule is a single path from a decision tree's root to oneleaf.2.1.3 skicat classification resultsstable test classification error rates of about 94% were obtained using ruler, compared to the originaltrees which had an accuracy of about 90%. note that such high classification accuracy results could only beobtained after expending significant effort on defining more robust features that captured sufficient invariancesbetween various plates. when the same experiments were conducted using only the generic features measured bythe standard schemes, the results were significantly worse. the skicat classifier correctly classified themajority of faint objects (using only the original lower resolution plates) which even the astronomers cannotclassify without looking at the special ccd plates: these objects are at least one magnitude fainter than objectscataloged in previous surveys. this results in a 200% increase in the number of classified sky objects availablefor scientific analysis in the resulting sky survey catalog database.a consequence of the skicat work is a fundamental change in the notion of a sky catalog from theclassical static entity "in print," to a dynamic online database. the catalog generated by skicat will eventuallycontain about a billion entries representing hundreds of millions of sky objects. skicat is 'part of thedevelopment of a new generation of intelligent scientific analysis tools [33, 8]. without the availability of thesetools for the first survey (possi) conducted over four decades ago, no objective and comprehensive analysis ofthe data was possible. consequently only a small fraction of the possi data was ever analyzed.2.1.4 why was skicat successful?it is important to point out why a decisiontree based approach was effective in solving a problem that wasvery difficult for astronomers to solve. indeed there were numerous attempts by astronomers to handcode aclassifier that would separate stars from galaxies at the faintest levels, without much success. this lack ofsuccess was likely due to the dimensionality of the feature space and the nonlinearity of the underlying decisionboundaries. historically, efforts involving principal component analysis, or "manual" classifier construction, byprojecting the data down to 2 or 3 dimensions and then searching for decision boundaries, did not lead to goodresults.based on the data it appears that accurate classification of the faint objects requires at least 8 dimensions.projections to 24 dimensions lose critical information. on the other hand, human visualization and design skillscannot go beyond 35 dimensions. this classification problem is an excellent example of a problem whereexperts knew what features to measure, but not how to use them for classification. from the 40dimensionalfeaturespace, the decision tree and rule algorithms were able to extract the relevant discriminative information(the typical set of rules derived by ruler by optimizing over many decision trees references only 8 attributes).one can conclude that the combination of scientistsupplied features (encoding prior knowledge), and automatedidentification of relevant features for discriminative rules were both critical factors in the success of the skicatproject.from massive data sets to science catalogs: applications and challenges133massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.2.2 cataloging volcanoes in magellansar images2.2.1 backgroundon may 4th 1989 the magellan spacecraft was launched from earth on a mapping mission to venus.magellan entered an elliptical orbit around venus in august 1990 and subsequently transmitted back to earthmore data than that from all past planetary missions combined [26]. in particular, a set of approximately 30,000,1024 × 1024 pixel, synthetic aperture radar (sar), 75m/pixel resolution images of the planer's surface weretransmitted resulting in a high resolution map of 97% of the surface of venus. the total combined volume of premagellan venus image data available from various past us and ussr spacecraft and groundbased observationsrepresents only a tiny fraction of the magellan data set. thus, the magellan mission has provided planetaryscientists with an unprecedented data set for venus science analysis. it is anticipated that the study of themagellan data set will continue well into the next century [21,27, 5].the study of volcanic processes is essential to an understanding of the geologic evolution of the planet [26],and volcanoes are by far the single most visible geologic feature in the magellan data set. in fact, there areestimated to be on the order of 106 visible volcanoes scattered throughout the 30,000 images [1]. central to anyvolcanic study is a catalog identifying the location, size, and characteristics of each volcano. such a catalogwould enable scientists to use the data to support various scientific theories and analyses. for example, thevolcanic spatial clustering patterns could be correlated with other known and mapped geologic features such asmean planetary radius to provide evidence for (or against) particular theories of planetary history. however, ithas been estimated that manually producing such a catalog of volcanoes would require 10 manyears of aplanetary geologist's time. thus, geologists are manually cataloging small portions of the data set and inferringwhat they can from these data [10].2.2.2 automated detection of volcanoesat jpl we have developed a pattern recognition system, called the jpl adaptive recognition tool(jartool), for volcano classification based on matched filtering, principal component analysis, and quadraticdiscriminants. over certain regions of the planet the system is roughly as accurate as geologists in terms ofclassification accuracy [4]. on a more global scale, the system is not currently competitive with humanclassification performance due to the wide variability in the visual appearance of the volcanoes and the relativelylow signaltonoise ratio of the images.for this problem the technical challenges lie in the detection and feature extraction parts of the problem.unlike the stars and galaxies in the skicat data, volcanoes are surrounded by a large amount of backgroundclutter (such as linear and small nonvolcano circular features) which renders the detection problem quitedifficult. locating candidate local pixel regions and then extracting descriptive features from these regions isnontrivial to do in an effective manner. particular challenges include the fact that in a complex multistagedetection system, it is difficult to jointly optimize the parameters of each individual component algorithm. afurther source of difficulty has been the subjective interpretation problem: scientists are not completelyconsistent among themselves in terms of manual volcano detection and so there is no absolute ground truth: thisadds an extra level of complexity to model training and performance evaluation. thus, in the general scheme ofscience cataloging applications at jpl, the volcano project has turned out to be one of the more difficult.from massive data sets to science catalogs: applications and challenges134massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.2.3 other science cataloging projects at jplthere are several other ongoing automated cataloging projects currently underway at jplšgiven the datarates for current and planned jpl and nasa observation missions (including the recentlylaunched sohosatellite) there will continue to be many such applications. for example, there is currently a project underway tocatalog plages (bright objects in the ultraviolet sun; somewhat analogous to sunspots) from fulldisk solarimages taken daily from terrestrial observatories. the data in one spectral band from one observatory is asequence of 104, roughly 2k × 2k pixel images taken since the mid1960s. of interest here is the fact that thereis considerable prior knowledge (going back to the time of galileo) about the spatial and temporal evolution offeatures on the surface of the sun: how to incorporate this prior information effectively in an automatedcataloging system is a nontrivial technical issue.another ongoing project involves the detection of atmospheric patterns (such as cyclones) in simulatedglobal climate model data sets [29]. the models generate simulations of the earth's climate at different spatiotemporal resolutions and can produce up to 30 terabytes of output per run. the vast majority of the simulateddata set is not interesting to the scientist: of interest are specific anomalous patterns such as cyclones. datasummarization (description) and outlier detection techniques (for spatiotemporal patterns) are the criticaltechnical aspects of this project.3 general implications for the analysis of massive data sets3.1 complexity issues for classification problemsdue to their size, massive data sets can quickly impose limitations on the algorithmic complexity of dataanalysis. let n be the total available number of data points in the data set. for large n, linear or sublinearcomplexity in n is highly desirable. algorithms with complexity as low as o(n2) can be impractical. this wouldseem to rule out the use of many data analysis algorithms; for example, many types of clustering.however, in reality, one does not necessarily need to use all of the data in one pass of an algorithm. indeveloping automated cataloging systems as described above, two typical cases seem to arise:type sm problems for which the statistical models can be built from a very small subset of the data, andthen the models are used to segment the massive larger population (small work setmassive application, hence sm)type mm problems for which one must have access to the entire data set for a meaningful model to beconstructed. see section 3.1.2 for an example of this class of problem.3.1.1 supervised classification can be tractablea supervised classification problem is typically of type sm. since the training data needs to be manuallylabeled by humans, the size of this labeled portion of the data set is usually a vanishingly small fraction of theoverall data set. for example, in skicat, only a few thousand examples were used as a training set while theclassifiers are applied to up to a billion records in the catalog database. for jartool, on the order of 100 imageshave been labeled for volcano content (with considerable time and effort), or about 0.3% of the overall imageset. thus, relative to the overall size of the data set, the data available for model construction can be quite small,and hence complexity (within the bounds of reason) may not to be a significant issue in model construction.from massive data sets to science catalogs: applications and challenges135massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.once the model is constructed, however, prediction (classification) is typically performed on the entiremassive data set: for example, on the other 99.7% of unlabelled magellansar images. this is typically not aproblem since prediction is linear in the number of datapoints to be predicted, assuming that the classifieroperates in a spatially local manner (certainly true for detection of small, spatially bounded objects such as smallvolcanoes or stars and galaxies). even algorithms based on nearest neighbor prediction, which require thetraining set be kept online can be practical provided the training set size n is small enough.3.1.2 unsupervised classification can be intractablea clustering problem (unsupervised learning) on the other hand, can easily be a type mm problem. astraightforward solution would seem to be to randomly sample the data set and build models from these randomsamples. this would only work if random sampling is acceptable. in many cases, however, a stratified sample isrequired. in the skicat application, for example, uniform random sampling would simply defeat the entirepurpose of clustering. current work on skicat focuses on exploring the utility of clustering techniques to aidin scientific discovery. the basic idea is to search for clusters in the large data sets (millions to billions of entriesin the sky survey catalog database). a new class of sky objects could potentially show up as a strong cluster thatdiffers from known objects: stars and galaxies. the astronomers would then follow up with high resolutionobservations to see whether indeed the objects in the suspect cluster constitute a new class of what one hopes arepreviously unknown objects. the basic idea is that the clustering algorithms serve to focus the attention ofastronomers on potential new discoveries.the problem, however, is that new classes are likely to have very low prior probability of occurrence in thedata. for example, we have been involved in searching for new highredshift quasars in the universe. theseoccur with a frequency of about 100 per 107 objects. using mostly classification, we have been able to helpdiscover 10 new quasars in the universe with an order of magnitude less observation time as compared to effortsby other teams [23].however, when one is searching for new classes, it is clear that random sampling is exactly what should beavoided. clearly, members of a minority class could completely disappear from any small (or not so small)sample. one approach that can be adopted here is an iterative sampling scheme1 which exploits the fact thatusing a constructed model to classify the data scales linearly with the number of data points to be classified. theprocedure goes as follows:1. generate a random sample s from the data set d.2. construct a model ms based on s (based on probabilistic clustering or density estimation).3. apply the model to the entire set d, classifying items in d in the clusters with probabilities assignedby the model ms.4. accumulate all the residual data points (members of d that do not fit in any of the clusters of ms withhigh probability). remove all data points that fit in ms with high probability.5. if a sample of residuals of acceptable size and properties is collected, go to step 7, else go to 6.6. let s be the set of residuals from step 4 mixed with a small sample (uniform) from d, return to step 2.1 initially suggested by p. cheeseman of nasaames in a discussion with u. fayyad on the complexity of bayesianclustering with the autoclass system, may 1995.from massive data sets to science catalogs: applications and challenges136massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.7. perform clustering on the accumulated set of residuals, look for tight clusters as candidate newdiscoveries of minority classes in the data.other schemes for iteratively constructing a useful small sample via multiple efficient passes on the data arealso possible [22]. the main idea is that sampling is not a straightforward matter.3.2 human factors: the interactive process of data analysisthere is a strong tendency in the artificial intelligence and pattern recognition communities to buildautomated data analysis systems. in reality, fitting models to data tends to be an interactive, iterative, humancentered process for most largescale problems of interest [2]. certainly in the possii and magellansarprojects a large fraction of time was spent on understanding the problem domains, finding clever ways to preprocess the data, and interpreting the scientific significance of the results. relatively little time was spent ondeveloping and applying the actual algorithms which carry out the modelfitting. in a recent paper, hand [17]discusses this issue at length: traditional statistical methodology focuses on solving precise mathematicalquestions, whereas the art of data analysis in practical situations demands considerable skill in the formulation ofthe appropriate questions in the first place. this issue of statistical strategy is even more relevant for massivedata sets where the number of data points and the potentially highdimensional representation of the data, offerhuge numbers of possibilities in terms of statistical strategies.useful statistical solutions (algorithms and procedures) can not be developed in complete isolation fromtheir intended use. methods which offer parsimony and insight will tend to be preferred over more complexmethods which offer slight performance gains but at a substantial loss of interpretability.3.3 subjective human annotation of data sets for classification purposesfor scientific data, performance evaluation is often subjective in nature since there is frequently no "goldstandard." as an example consider the volcano detection problem: there is no way at present to independentlyverify if any of the objects which appear to look like volcanoes in the magellansar imagery truly representvolcanic edifices on the surface of the planet. the best one can do is harness the collective opinion of the expertplanetary geologists on subsets of the data. one of the more surprising aspects of this project was the realizationthat image interpretation (for this problem at least) is highly subjective. this fundamentally limits the amount ofinformation one can extract. this degree of subjectivity is not unique to volcanocounting: as part of thepreviously mentioned project involving automated analysis of sunspots in daily images of the sun, there appearsalso to be a high level of subjectivity and variation between scientists in terms of their agreement. while somestatistical methodologies exist for handling subjective opinions of multiple experts [30][28]: there appears to beroom for much more work in this area.3.4 effective use of prior knowledgea popular (and currently resurgent) approach to handling prior information in statistics is the bayesianinference philosophy: provided one can express one's knowledge in the form of suitable prior densities, andgiven a likelihood model, one then can proceed directly to obtain the posterior (whether by analytic orapproximate means). however, in practice, the bayesian approach can be difficult to implement effectivelyparticularly in complex problems. in particular, the approach is difficult for nonspecialists in bayesian statistics.for example, while there is a wealth of knowledge available concerning the expected size, shape, appearance,etc., of venusian volcanoes, it is quitefrom massive data sets to science catalogs: applications and challenges137massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.difficult to translate this highlevel information into precise quantitative models for at the pixellevel. in effectthere is a gap between the language used by the scientist (which concerns the morphology of volcanoes) and thepixellevel representation of the data. there is certainly a need for interactive, "interviewing" tools which couldelicit prior information from the user and automatically construct "translators" between the user's language andthe data representation. this is clearly related to the earlier point on modelling statistical strategy as a whole,rather than focusing only on algorithmic details. some promising approaches for building bayesian (graphical)models from data are beginning to appear (see [19] for a survey).3.5 dealing with high dimensionalitymassiveness has two aspects to it: the number of data points and their dimensionality. most traditionalapproaches in statistics and pattern recognition do not deal well with high dimensionality. from a classificationviewpoint the key is effective feature extraction and dimensionality reduction. the skicat application is anexample of manual feature extraction followed by greedy automated feature selection. the venus applicationrelies entirely on a reduction from highdimensional pixel space to a low dimensional principal componentbasedfeature space. however, finding useful lowdimensional representations of highdimensional data is stillsomething of an art, since any particular dimension reduction algorithm inevitably performs well on certain datasets and poorly on others. a related problem is that frequently the goals of a dimension reduction step are notaligned with the overall goals of the analysis, e.g., principal components analysis is a descriptive technique but itdoes not necessarily help with classification or cluster identification.3.6 how does the data grow?one of the recurring notions during the workshop2 is whether massive data sets were more complex at somefundamental level than familiar "smaller" datasets. with some massive data sets, it seems that as the size of thedata set increases, so do the sizes of the models required to accurately model it. this can be due to many factors.for example, inappropriateness of the assumed model class would result in failure to capture the underlyingphenomena generating the data. another cause could be the fact that the underlying phenomena are changingover time, and the "mix" gets intractable as one collects more data without properly segmenting it into thedifferent regimes. the problem could be related to dimensionality, which is typically larger for massive data sets.we would like to point out that this "problematic growth" phenomenon is not true in many important cases,especially in science data analysis. in case of surveys, for example, one is careful about the design of datacollection and the basic data processing. hence, many of the important data sets are by design intended to be oftype sm. this is certainly true of the two applications presented earlier. hence growth of data set size is notalways necessarily problematic.4 conclusionthe proliferation of large scientific data sets within nasa has accelerated the need for more sophisticateddata analysis procedures for science applications. in particular, this paper has briefly discussed several recentprojects at jpl involving automated cataloging of large image data sets. the issues of complexity, statisticalstrategy, subjective annotation, prior knowledge, and high dimensionality were discussed in the general contextof data analysis for massive data sets. the2 massive data sets workshop (july, 1995, nrc), raised by p. huber and other participants.from massive data sets to science catalogs: applications and challenges138massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.subjective human role in the overall dataanalysis process is seen to be absolutely critical. thus, the developmentof interactive, processoriented, interpretable statistical procedures for fitting models to massive data sets appearsa worthwhile direction for future research.in our view, serious challenges exist to current approaches to statistical data analysis. addressing thesechallenges and limitations, even partially, could go a long way in getting more tools in the hands of users anddesigners of computational systems for data analysis. issues to be addressed include: developing techniques that deal with structured and complex data (i.e., attributes that have hierarchicalstructure, functional relations between variables, data beyond the flat featurevector that may containmultimodal data including pixels, timeseries signals, etc.) developing summary statistics beyond means, covariance matrixes, and boxplots to help humans bettervisualize highdimensional data content. developing measures of data complexity to help decide which modelling techniques are appropriate toapply in which situations item addressing new regimes for assessing overfit since massive data sets willby definition admit much more complex models. developing statistical techniques to deal with high dimensional problems.in conclusion, we point out that although our focus has been on sciencerelated applications, massive datasets are rapidly becoming commonplace in a wide spectrum of activities including healthcare, marketing,finance, banking, engineering and diagnostics, retail, and many others. a new area of research, bringing togethertechniques and people from a variety of fields including statistics, machine learning, pattern recognition, anddatabases, is emerging under the name: knowledge discovery in databases (kdd) [16, 15]. how to scalestatistical inference and evaluation techniques up to very large databases is one of the core problems in kdd.acknowledgementsthe skicat work is a collaboration between fayyad (jpl), n. weir and s. djorgovski (caltechastronomy). the work on magellansar is a collaboration between fayyad and smyth (jpl), m.c. burl and p.perona (caltech e.e.) and the domain scientists: j. aubele and l. crumpler, department of geological sciences,brown university. major funding for both projects has been provided by nasa's office of space access andtechnology (code x). the work described in this paper was carried out in part by the jet propulsion laboratory,california institute of technology, under a contract with the national aeronautics and space administration.references[1] aubele, j. c. and slyuta, e. n. 1990. small domes on venus: characteristics and origins. earth, moon and planets, 50/51, 493532.[2] brachman, r. and anand, t. 1996. the process of knowledge discovery in databases: a human centered approach, pp. 3758,advances in knowledge discovery and data mining, u. fayyad, g. piatetskyshapiro, p. smyth, & r. uthurusamy (eds.), boston:mit press.[3] breiman, l., friedman, j.h., olshen, r.a. and stone, c.j. 1984. classification and regression trees. monterey, ca: wadsworth &brooks.from massive data sets to science catalogs: applications and challenges139massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.[4] burl, m. c., fayyad, u. m., perona, p., smyth, p. and burl, m. p. 1994. automating the hunt for volcanoes on venus. in proceedings ofthe 1994 computer vision and pattern recognition conference, cvpr94 , los alamitos, ca: ieee computer society press,pp.302309.[5] cattermole, p. 1994. venus: the geological story, baltimore, md: johns hopkins university press.[6] cheeseman, p. and stutz, j. 1996. bayesian classification (autoclass): theory and results. in advances in knowledge discovery anddata mining , u. fayyad, g. piatetskyshapiro, p. smyth, & r. uthurusamy (eds.), boston: mit press, pp.153180.[7] dasarathy, b.v. 1991. nearest neighbor norms: nn pattern classification techniques. ieee computer society press, los alamitos, ca.[8] djorgovski, s.g., weir, n., and fayyad, u. m. 1994. processing and analysis of the palomarstsci digital sky survey using a novelsoftware technology. in d. crabtree, r. hanisch, and j. barnes (eds.), astronomical data analysis software and systems iii,a.s.p. conf. ser. 61, 195.[9] fasman, k. h., cuticchia, a.j., and kingsbury, d. t. 1994. the gdb human genome database anno 1994. nucl. acid. res., 22(17),34623469.[10] guest, j. e. et al. 1992. small volcanic edifices and volcanism in the plains of venus. journal of geophysical research, vol.97, no.e10,pp.1594966.[11] fayyad, u.m. and irani, k.b. 1993. multiinterval discretization of continuousvalued attributes for classification learning. in proc. of the thirteenth inter. joint conf. on artificial intelligence, chambery, france: ijcai11.[12] fayyad, u.m., djorgovski, s.g. and weir, n. 1996. automating analysis and cataloging of sky surveys. in advances in knowledge discovery and data mining, u. fayyad, g. piatetskyshapiro, p. smyth, & r. uthurusamy (eds.), boston: mit press, pp.471494.[13] fayyad, u.m. 1994. branching on attribute values in decision tree generation. in proc. of the twelfth .national conference on artificial intelligence aaai94, pages 601606, cambridge, ma, 1994. mit press.[14] fayyad, u.m. 1995. on attribute selection measures for greedy decision tree generation. submitted to artificial intelligence[15] fayyad, u. and uthurusamy, r. (eds.) 1995. proceedings of the first international conference on knowledge discovery and datamining (kdd95). aaai press.[16] fayyad, u., piatetskyshapiro, g. and smyth p. 1996. from data mining to knowledge discovery: an overview. advances inknowledge discovery and data mining, u. fayyad, g. piatetskyshapiro, p. smyth, & r. uthurusamy (eds.), boston: mit press,pp.136.[17] hand, d. j. 1994. deconstructing statistical questions. j. r. statist. soc. a, 157(3), pp.317356.[18] j. w. head et al. 1991. venus volcanic centers and their environmental settings: recent data from magellan. american geophysicalunion spring meeting abstracts, eos 72:175.[19] heckerman, d. 1996. bayesian networks for knowledge discovery, pp. 273306, advances in knowledge discovery and data mining,u. fayyad, g. piatetskyshapiro, p. smyth, & r. uthurusamy (eds.), boston: mit press.[20] jarvis, j., and tyson, a. 1981. focas: faint object classification and analysis system. astronomical journal 86, 476.[21] magellan at venus: special issue of the journal of geophysical research, american geophysical union, 1992.from massive data sets to science catalogs: applications and challenges140massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.[22] kaufman, l. and rousseeuw, p. j. 1990. finding groups in data: an introduction to cluster analysis, new york: wiley.[23] kennefick. j.d., de carvalho, r.r., djorgovski, s.g., wilber, m.m., dickson, e.s., weir, n., fayyad, u.m. and roden, j. 1995. thediscovery of five quasars at z>4 using the second palomar sky survey. astronomical journal (in press).[24] quinlan, j. r. 1986. the induction of decision trees. machine learning, 1(1).[25] nssdc news, vol.10, no.1, spring 1994, available from request@nssdc.gsfc.nasa.gov.[26] saunders, r. s. et al. 1992. magellan mission summary. journal of geophysical research, vol.97, no. e8, pp.1306713090.[27] science, special issue on magellan data, april 12, 1991.[28] smyth, p., 1995. bounds on the mean classification error rate of multiple experts, pattern recognition letters, in press.[29] stolorz, p. et al. 1995. fast spatiotemporal data mining of large geophysical datasets. in proceedings of the first international conference on knowledge discovery and data mining, pp.300305, u. m. fayyad and r. uthurusamy (eds.), aaai press.[30] uebersax, j. s., 1993. statistical modeling of expert ratings on medical treatment appropriateness. j. amer. statist. assoc., vol.88,no.422, pp.421427.[31] valdes, f. 1982. the resolution classifier. in instrumentation in astronomy iv, volume 331:465, bellingham, wa, spie.[32] weir, n., fayyad, u.m., and djorgovski, s.g. 1995. automated star/galaxy classification for digitized possii. the astronomical journal, 1096:24012412.[33] weir, n., djorgovski, s.g., and fayyad, u.m. 1995. initial galaxy counts from digitized possii. astronomical journal, 1101:120.[34] wilson, g. s., and backlund, p. w. 1992. mission to planet earth. photo. eng. rein. sens., 58(8), 11331135.from massive data sets to science catalogs: applications and challenges141massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.from massive data sets to science catalogs: applications and challenges142massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.information retrieval and the statistics of large data setsdavid d. lewis&t bell laboratoriesabstractproviding contentbased access to large quantities of text is a difficult task, given our poor understanding ofthe formal semantics of human language. the most successful approaches to retrieval, routing, andcategorization of documents have relied heavily on statistical techniques. we briefly review some of thosetechniques and point out where better statistical insight could lead to further advances.1 ir and statistics todayinformation retrieval (ir) is concerned with providing access to data for which we do not have strongsemantic models. text is the most notable example, though voice, images, and video are of interest as well.examples of ir tasks include retrieving documents from a large database in response to immediate user needs,routing or filtering documents of interest from an ongoing stream over a period of time, and categorizingdocuments according to their content (e.g. assigning dewey decimal numbers to abstracts of articles).statistical approaches have been widely applied to these systems because of the poor fit of text to datamodels based on formal logics (e.g. relational databases) [8]. rather than requiring that users anticipate exactlythe words and combinations of words that will appear in documents of interest, statistical ir approaches let userssimply list words that are likely to appear in relevant documents. the system then takes into account thefrequency of these words in a collection of text, and in individual documents, to determine which words arelikely to be the best clues of relevance. a score is computed for each document based on the words it contains,and the highest scoring documents are retrieved, routed, categorized, etc.there are several variations on this approach [5, 17, 18, 19]. vector space models treat the words suggestedby the user as specifying an ideal relevant document in a high dimensional space. the distance of actualdocuments to this point is used as a measure of relevance. probabilistic models attempt to estimate, for instance,the conditional probability of seeing particular words in relevant and nonrelevant documents. these estimates arecombined under independence assumptions and documents are scored for probability of membership in the classof relevant documents. inferenceinformation retrieval and the statistics of large data sets143massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.net models are a subclass of probabilistic models which use network representations of the distribution of words.a variety of other formal and ad hoc statistical methods, including ones based on neural nets and fuzzy logichave been tried as well.in ir systems documents are often represented as vectors of binary or numeric values correspondingdirectly or indirectly to the words of the document. several properties of language, such as synonymy,ambiguity, and sheer variety make these representation far from ideal (but also hard to improve on [13]). avariety of unsupervised learning methods have been applied to ir, with the hope of finding structure in largebodies of text that would improve on straightforward representations. these include clustering of words ordocuments [10, 20], factor analytic decompositions of term by document matrices [1], and various termweighting methods [16].similarly, the retrieval query, routing profile, or category description provided by an ir system user is oftenfar from ideal as well. supervised learning techniques, where user feedback on relevant documents is used toimprove the original user input, have been widely used [6, 15]. both parametric and nonparametric (e.g. neuralnets, decision trees, nearest neighbor classifiers) have been used. supervised learning is particularly effective inrouting (where a user can supply ongoing feedback as the system is used) [7] and in text categorization (where alarge body of manually indexed text may be available) [12, 14].2 the futurethese are exciting times for ir. statistical ir methods developed over the past 30 years are suddenly beingwidely applied in everything from shrinkwrapped personal computer software, up to large online databases(dialog, lexis/nexis, and west publishing all fielded their first statistical ir systems in the past three years) andsearch tools for the internet.until recently, ir researchers dealt mostly with relatively small and homogeneous collections of shortdocuments (often titles and abstracts). comparisons of over 30 ir. methods in the recent nist/arpa textretrieval conferences (trec), have resulted in a number of modifications to these methods to deal with large(one million documents or more) collections of diverse full text documents [2, 3, 4]. much of this tuning hasbeen ad hoc and heavily empirical. little is known about the relationship between properties of a text base andthe best ir methods to use with it. this is an undesirable situation, given the increasing variety of applications iris applied to, and is perhaps the most important area where better statistical insight would be helpful.four observations from the trec conferences give a sense of the range of problems where better statisticalinsight is needed:1. term weighting in long documents: several traditional approaches give a document credit formatching a query word proportional to the number of times the word occurs in a document.performance on trec is improved if the logarithm of the number of occurrences of the word isused instead. better models of the distribution of word occurrences in documents might provide lessad hoc approaches to this weighting.2. feedback from top ranked documents: supervised learning methods have worked well in trec,with some results suggesting that direct user input becomes of relatively little valueinformation retrieval and the statistics of large data sets144massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.when large number of training instances are available. more surprisingly, applying supervisedlearning methods to the top ranked documents from an initial retrieval run, as if they were known tobe relevant, has been found to be somewhat useful. this strategy had failed in all attempts prior totrec. is the size of the trec collection the key to success? can this idea be better understood andimproved on (perhaps using em methods)?3. massive query expansion: supervised learning approaches to ir often augment the original set ofwords suggested by a user with words from documents they have judged to be relevant. forprobabilistic ir methods, adding only a few words from relevant documents has been found to workbest. however, for vector space methods, massive expansion (adding most or all words from knownrelevant documents) seems to be optimal. reconciling this with the usually omnipresent curse ofdimensionality is an interesting issue.4. the difficulty of evaluating ir systems: trec has reminded researchers that we do not have a goodunderstanding of how to decide if one ir system is significantly better than another, much less howto predict in advance the level of effectiveness that an ir system can deliver to a user. indeed, it isoften unclear what reasonable measures of effectiveness are. these issues are of more interest thanever, given the large number of potential new users of ir technology.trec reveals just a few of the ir problems where better statistical insight is crucial. others include dealingwith timevarying streams of documents (and timevarying user needs), drawing conclusions from databases thatmix text and formatted data, and choosing what information sources to search in the first place. on the tools side,a range of powerful techniques from statistics have seen relatively little application in ir, including crossvalidation, model averaging, graphical models, hierarchical models, and many others. curiously, highlycomputational methods have seen particularly little use. the author has been particularly interested in methodsfor actively selecting training data (ala statistical design of experiments) for supervised learning [9, 11]. sincevast quantities of text are now cheap, while human time is expensive, these methods are of considerable interest.in summary, the opportunities for and need of more statistical work in ir is as vast as the flood of onlinetext engulfing the world!references[1] scott deerwester, susan t. dumais, george w. furnas, thomas k. landauer, and richard harshman. indexing by latent semanticindexing. journal of the american society for information science, 41(6):391407, september 1990.[2] d. k. harman, editor. the first text retrieval conference (trec1 ), gaithersburg, md 20899, 1993. national institute of standardsand technology. special publication 500207.[3] d. k. harman, editor. the second text retrieval conference (trec2) , gaithersburg, md 20899, 1994. national institute of standardsand technology. special publication 500215.information retrieval and the statistics of large data sets145massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.[4] d. k. harman, editor. overview of the third text retrieval conference (trec3), gaithersburg, md 208990001, 1995. nationalinstitute of standards and technology. special publication 500225.[5] donna harman. ranking algorithms. in william b. frakes and ricardo baezayates, editors, information retrieval: data structures and algorithms, pages 363392. prentice hall, englewood cliffs, nj, 1992.[6] donna harman. relevance feedback and other query modification techniques. in william b. frakes and ricardo baezayates, editors,information retrieval: data structures and algorithms, pages 241263. prentice hall, englewood cliffs, nj, 1992.[7] donna harman. overview of the third text retrieval conference (trec3). in d. k. harman, editor, overview of the third textretrieval conference (trec3), pages 127, gaithersburg, md, 1995. u. s. dept. of commerce, national institute of standardsand technology.[8] david d. lewis. learning in intelligent information retrieval. in eighth international workshop on machine learning, pages 235239,1991.[9] david d. lewis and jason catlett. heterogeneous uncertainty sampling for supervised learning. in william w. cohen and haym hirsh,editors, machine learning: proceedings of the eleventh international conference on machine learning, pages 148156, sanfrancisco, ca, 1994. morgan kaufmann.[10] david d. lewis and w. bruce croft. term clustering of syntactic phrases. in thirteenth annual international a cm sigir conference on research and development in information retrieval, pages 385404, 1990.[11] david d. lewis and william a. gale. a sequential algorithm for training text classifiers. in w. bruce croft and c. j. van rijsbergen,editors, sigir 94: proceedings of the seventeenth annual international a cmsigir conference on research and development ininformation retrieval, pages 312, london, 1994. springerverlag.[12] david d. lewis and philip j. hayes. guest editorial. acm transactions on information systems, 12(3):231, july 1994.[13] david d. lewis and karen sparck jones. natural language processing for information retrieval. communications of the acm, 1996. toappear.[14] david d. lewis and marc ringuette. a comparison of two learning algorithms for text categorization. in third annual symposium on document analysis and information retrieval, pages 8193, las vegas, nv, april 1113 1994. isri; univ. of nevada, las vegas.[15] gerard salton and chris buckley. improving retrieval performance by relevance feedback. journal of the american society forinformation science, 41(4):288297, 1990.information retrieval and the statistics of large data sets146massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.[16] gerard salton and christopher buckley. termweighting approaches in automatic text retrieval. information processing andmanagement , 24(5):513523, 1988.[17] gerard salton and michael j. mcgill. introduction to modern information retrieval. mcgrawhill book company, new york, 1983.[18] h. r. turtle and w. b. croft. a comparison of text retrieval models. the computer journal, 35(3):279290, 1992.[19] c. j. van rijsbergen. information retrieval. butterworths, london, second edition, 1979.[20] peter willett. recent trends in hierarchic document clustering: a critical review. information processing and management, 24(5):577598, 1988.information retrieval and the statistics of large data sets147massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.information retrieval and the statistics of large data sets148massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.some ideas about the exploratory spatial analysistechnology required for massive databasesstart openshawleeds universityabstractthe paper describes an emerging problem of an explosion in spatially referenced information at a timewhilst there is only essentially legacy geographical analysis technology. it offers some ideas about what isrequired and outlines some of the computationally intensive methods that would seem to offer considerablepotential in this area of applications.1 a global spatial data explosionthe geographic information systems revolution of the mid 1980's in conjunction with other historicallyunprecedented developments in information technology are creating an extremely spatial data rich world.suddenly, most map and map related databases have increasingly fine resolution geography on them. the papermap using industries throughout the world have gone digital. in many countries this ranges from the traditionalam/fm areas of gis (viz utilities), it includes the large scale mapping agencies (viz in the uk the ordnancesurvey, the geological survey, etc), it includes all manner of remotely sensed data and just as exciting it extendsto virtually all items of fixed infrastructure and certainly all people related databases (that is any and all postaladdress data, property, road, and land information systems). at a world level there is increasing interest in globalenvironmental databases (mounsey and tomlinson, 1988). there is now a vast wealth of digital backclothmaterial (viz the digital maps) and related attribute data covering most aspects of human life, ranging from thecradle to the grave and an increasing proportion of behaviour related events in between. most governmental andcommercial sector statistical and client information systems are well on the way to becoming geostatistical. infact nearly all computer information can be regarded as being broadly geographical in the sense that there areusually some kind of spatial coordinates on it or implicit in it. this may range from the traditionalsome ideas about the exploratory spatial analysis technology required for massive databases149massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.geographic map related data to nontraditional sources such as 2 and 3 dimensional images of virtually any kind.indeed, it is worth observing that several existing gis's already contain sufficient numeric resolution to workdown to nanometre scales! but lets stick with the more traditional geographic data. the technology needed tocreate, store, and manipulate these land and people related databases exists, it well developed, and is fairlymature. what is almost entirely missing are the geographical analysis and modelling technologies able to dealwith the many new potential opportunities that these data rich environments now make possible.it is not unusual for governments and commercial organisations to spend vast sums of money on buildingdatabases relevant to their activities. many have huge amounts of capital tied up in their databases and conceptssuch as data warehouses are providing the necessary it infrastructure. they know that their future depends onbuilding and using these information resources. most businesses and governmental agencies are becominginformation industries but currently there seems to be an almost universal neglect of investment in the analysistools needed to make the most of the databases.2 a global data swampmaybe the problem is that the data holders and potential users are becoming ''data swamped'' and can nolonger appreciate the opportunities that exist. often their ideas for analysis on these massive databases seems tomainly relate to an earlier period in history when data was scarce, the numbers of observation were small and thecapabilities of the computer hardware limited. as a result there are seemingly increasingly large numbers ofimportant, sometimes life critical and sometimes highly commercial, databases that are not being fully analysed;if indeed they are being spatially analysed at all. openshaw (1994a) refers to this neglect as a type of spatialanalysis crime. for some data there is already an overwhelming public imperative for analysis once the dataexist in a suitable form for analysis. is it not a crime against society if critical databases of considerablecontemporary importance to the public good are not being adequately and thoroughly analysed. this appliesespecially when there might well be a public expectation that such analysis already occurs or when there is astrong moral imperative on the professions involved to use the information for the betterment of people's lives.examples in the uk would include the nonanalysis of much spatially referenced information: examples includemost types of cancer data, mortality data, realtime crime event data, pollution data, data needed for realtimelocal weather forecasting, climatic change information, and personal information about people who are to betargeted for special assistance because they exist in various states of deprivation. there are many examples tooinvolving the spatial nonanalysis of major central and local government databases: for example, tax, socialsecurity payments, education performance and housing benefits. in the commercial sector also, it not unusual forlarge financial sector organisations to create massive customer databases, often containing longitudinal profilesof behaviour, increasingly being updated in realtime; and then do virtually nothing clever when it comes toanalysis. yet every single business in the it age knows that their long term future viability depends onthemselves making good use of their information resources. some of the opportunities involve spatial analysis;for example, customer targeting strategies, spatial planning and resome ideas about the exploratory spatial analysis technology required for massive databases150massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.organisation of business networks. economic well being and growth may become increasingly dependent onmaking more sophisticated use of the available data resources. of course there are dangers of intrusion intoprivate lives but all too often the confidentiality problems are grossly exaggerated as a convenient excuse fordoing nothing (openshaw, 1994b).to summarise, there are an increasing number of increasingly large and complex databases containingpotentially useful information that for various reasons are not currently being adequately analysed from a spatialstatistical geographic perspective. in many countries, if you ask the question who is responsible for monitoringkey indicators of health and wellbeing for unacceptable or preventable local abnormalities, then the answer willusually be 'nonone' because the technology needed simply does not exist. in the uk the most famous suspectedleukaemia cancer cluster was uncovered by journalists in a pub rather than by any detailed spatial statisticalanalysis or data base monitoring system. spatial analysis is a branch of statistics that is concerned with thequantitative analysis (and modelling) of the geographic arrangement, patterns, and relationships found in andamongst map referenced data of one form or another. the gis revolution has greatly increased the supply ofspatially referenced data without any similar provision of new spatial analysis tools able to cope with even thebasic analysis needs of users with very large spatial databases. in the last couple of years high parallel computingsystems such as the cray t3d provide memory spaces just about big enough to handle all but the very largest ofdatabases. the need now is for new data exploratory tools that can handle the largest databases and producesuggestions of geographical patterns or relationships; identify, geographically: localised anomalies if any exist:and provide a basis for a subsequent more focused analysis and action. note also that the need is for analysis butnot necessarily because there are expectations of discovering anything. analysis has a major reassurance aspectto it. in modern societies it is surely not an unreasonable expectation that benign big brother surveillancesystems should be continually watching for the unusual and unexpected.3 new tools are requiredin a spatial database context there is an urgent need for exploratory tools that will continually sieve andscreen massive amounts of information for evidence of patterns and other potentially interesting events (if anyexist) but without being told, in advance and with a high degree of precision, where to look in space, whento look in time. and what to look for in terms of the attributes that are of interest other than in the broadestpossible ways. traditionally, spatial analysts either start with a priori theory which they then attempt to test orelse they use interactive graphics procedures, perhaps linked to map displays, to explore spatial data. however,as the level of complexity and the amount of data increases this manual graphics based approach becomeinadequate. equally, in the spatial data rich 1990's there are not many genuine and applicable a priori hypothesesthat can be tested; usually, we just do not know what to expect or what might be found. exploration is seen as ameans of being creative and insightful in applications where current knowledge is deficient.the combination of a lack of prior knowledge and the absence spatial analysis tools sufficiently powerful tohandle the task of exploratory spatial data analysis especially insome ideas about the exploratory spatial analysis technology required for massive databases151massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.highly multivariate situations with large numbers of observations, has resulted in a situation where very littleinvestigative and applied analysis actually occurs. the technical problems should not be underestimated asspatial data is often characterised by the following features which serve to complicate the analysis tasks:1. nonnormal frequency distributions, 2. nonlinearity of relationships, 3. spatially autocorrelatedobservations, 4. spatially structured variations in degrees of precision, noise, and error levels, 5. large andincreasing data volumes, 6. large numbers of potential variables of interest (i.e. highly multivariate), 7. data ofvarying degrees of accuracy (i.e. can be variable specific), 8. often misplaced confidentiality concerns (i.e. itmight identify someone!), 9. nonideal information (i.e. many surrogates), 10. mixtures of measurement scales,11. modifiable areal unit or study region effects, 12. patterns and relationships that are localised and not global,and 13. presence of database errors.traditionally, people have coped by being highly selective whilst working with very few variables andrelatively small numbers of observations. however, this is increasingly hard to achieve. many spatialepidemiological studies decide in advance on the selection of disease, the coding of continuous time into discretebands, the recoding of the data, the geographical aggregation to be used, and the form of standardisation to beapplied. then they expect to "let what is left of the data to speak for itself" via exploratory analysis, after havingfirst strangled it by the study protocols imposed on it in order to perform the analysis. this is crazy! heavenalone knows what damage his may have done to the unseen patterns and structure lurking in the database orindeed, what artificial patterns might have been accidentally created. no wonder exploratory spatialepidemiology has had so few successes. it is useful, therefore, to briefly review some of the developments thatappear to be needed to handle the geographical analysis of massive spatial databases.3.1 automated map pattern detectorsone strategy is to use a brute force approach and simply look everywhere for evidence of localised patterns.the geographical analysis machines (gam) of openshaw et al (1987) is of this type, as is the geographicalcorrelates exploration machine (gcem) of openshaw et al (1990). the search requires a supercomputer but isexplicitly parallel and thus well suited for the current parallel supercomputers. the problems here are thedimensionally restricted nature of the search process, being limited to geographic space; and the statisticaldifficulties caused by testing millions of hypotheses (even if only applied in a descriptive sense). nevertheless,empirical tests have indicated that the gam can be an extremely useful spatial pattern detector that will explorethe largest available data sets for evidence of localised geographic patterning. the strength of the technologyresults from its comprehensive search strategy, the lack of any prior knowledge about the scales and nature of thepatterns to expect, and its ability to handle uncertain data.3.2 autonomous database exploreropenshaw (1994c, 1995) outlines a further development based on a different search strategy. borrowingideas from artificial life, an autonomous pattern hunting creature can be used to search for patterns by movingaround the spatial database in whatever dimensions aresome ideas about the exploratory spatial analysis technology required for massive databases152massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.relevant. it operates in trispace defined by geographic map coordinates, time coordinates. and also attributecoordinates (a dissimilarity space). in the prototype the creature is a set of hyperspheres that try to capturepatterns in the data by enveloping them. the dimensions and locations of the spheres is determined by a geneticalgorithm and performance is assessed by a sequential monte carlo significance test. the characteristics of thehyperspheres indicate the nature of the patterns being found. this technology is being developed further andbroadened to include a search for spatial relationships and also linked to computer animation to make theanalysis process visible and thus capable of being more readily understood by endusers (openshaw and perrie.1995).3.3 geographic object recognitionanother strategy is that described in openshaw (1994d) which viewers the problem as being one of patternrecognition. many spatial databases are now so detailed that it is becoming increasingly difficult to abstract andidentify generalisable and recurrent patterns. as the detail and resolution have increased dramatically,geographers have lost the ability to stand back and generalise or even notice recurrent patterns. it is ironic that ingeography the discovery of many of the principal spatial patterns and associated theoretical speculations thatexist predate the computer. in the early computer and data starved era geographers tested many of these patternhypotheses using statistical methods, and looked forward to better resolution data so that new and more refinedspatial patterns might be found. now that there is such an incredible wealth of spatial detail, it is clear that heretoo there is no good ideas of what to do with it! beautiful, multicoloured maps accurate to an historicallyunprecedented degree shown so much detail that pattern abstraction by manual means is now virtuallyimpossible and, because this is a new state, the technology needed to aid this process still needs to be developed.here, as in some other areas, finer resolution and more precision has not helped but hindered.if you look at a sample of towns or cities you can easily observe broad pattern regularities in the location ofgoodbadaverage areas etc. each town is unique but the structure tends to repeat especially at an abstract level.now examine the same towns using the best available data and there is nothing that can be usefully abstracted orgeneralised, just a mass of data with highly complex patterns of almost infinite variation. computer visiontechnology could in principle be used to search for scale and rotationally invariant two or three dimensionalgeographic pattern objects, with a view to creating libraries of recurrent generalisations. it is thought, thatknowledge of the results stored in these libraries might well contribute to the advancement of theory andconcepts relating to spatial phenomenon.3.4 very large spatial data classification toolsclassification is a very useful data reduction device able to reduce the number of cases/observations fromvirtually any very large number to something quite manageable such as 50 clusters (or groups) of cases/observations that share common properties. this is not a new technology, however, there is now some need to beable to efficiently and effectively classify several millions (or more) cases. scaling up conventional classifiers isnot a difficult task; openshaw et al (1985) reported the results of a multivariate classification of 22 million italiansome ideas about the exploratory spatial analysis technology required for massive databases153massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.households. however, much spatial data is imprecise, nonrandom, and of variable accuracy. spationeuralnetwork methods based on modified kohonen selforganising nets provide an interesting approach that can betterthe problems (openshaw, 1994e). this version uses a modified training method that biases the net towards themost reliable data. another variant also handles variable specific data uncertainty and has been put into a dataparallel form for the cray t3d.another form of classification is provided by spatial data aggregation. it is not unusual for n individualrecords describing people or small area aggregations of them to be aggregated to m statistical reporting areas,typically, m is much less than n. traditionally, statistical output area definitions have been fixed, by a mix oftradition, historical, accident, and fossilised by inertia and outmoded thinking. however, gis removes thetyranny of users having to use fixed areas defined by others that they cannot change. user controlled spatialaggregation of very large databases is potentially very useful because: (1) it reduces data volumes dramaticallybut in a user controlled or application specific way: (2) it provides a mechanism for designing analytically usefulzones that meet confidentiality restrictions and yield highest possible levels of data resolution: and (3) it isbecoming necessary purely as a spatial data management tool. however, if users are to be allowed to design orengineer their own zoning systems then they need special computer systems to help them a start has been madebut much remains to be done, openshaw and rao (1995).3.5 neurofuzzy and hybrid spatial modelling systems for very large data basesrecent developments in ai, in neural networks and fuzzy logic modelling have created very powerful toolsthat can be used in geographical analysis. the problem is applying them to very large spatial databases. the sizeand relational complexity of large data bases increasingly precludes simply downloading the data in a fiat fileform for conventional workstation processing. sometimes this will be possible but not always, so how do youdeal with a 10 or 40 gigabyte database containing many hundred hierarchically organised table? there are twopossible approaches: method 1 is to copy it all in a decomposed fiat file form onto a highly parallel system withsufficient memory to hold all the data and sufficient speed to do something useful with it. the cray t3d with2.56 processors provides a 16 gigabyte memory space albeit distributed. however, there is clearly the beginningsof large highly parallel systems with sufficient power and memory to at least load the data. the question of whatthen still however needs to be resolved.method two is much more subtle. why not leave the database alone, assuming that it is located on a suitablyfast parallel database engine of some kind. the problem is doing something analytical with it using only sqlcommands. one approach is to rewrite whatever analysis technology is considered useful so that it can be runover a network and communicates with the database via sql instructions. the analysis machines starts off witha burst of sql commands. it waits patiently for the answers: when they are complete, it uses the information togenerate a fresh burst of sql instructions: etc. now it is probably not too difficult to rewrite most relevantanalysis procedures for this type of approach.some ideas about the exploratory spatial analysis technology required for massive databases154massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.4 discovery how to exploit the geocyberspaceit is increasingly urgent that new tools are developed and made available that are able to analyse and modelthe increasing number of very large spatial databases that exist. it is not data management or manipulation orhardware that is now so critical but analysis. it is not a matter of merely scaling existing methods designed longago because usually the underlying technology is so fundamentally inappropriate. nor is it a matter of beingmore precise about what we want to do: in fact there is often just too much data for that. instead, there is anincreasing imperative to develop the new highly computation and intelligent data analysis technologies that areneeded to cope with incredibly data rich environments. in geography the world of the 21st century is perceivedto be that of the geocyberspace, the world of computer information (openshaw. 1994f). it is here where thegreatest opportunities lie but it is also here where an extensive legacy of old fashioned, often early computer,philosophically inspired constraints still dominate thinking about what we can and should not do. maybe it isdifferent elsewhere.references[1] mounsey, h., tomlinson, r., 1988 building databases for global science. taylor and francis, london[2] openshaw, s., sforzi, f., wymer, c., 1985 national classifications of individual and area census data: methodology, comparisons, andgeographical significance. sistemi urbani 3, 283312[3] openshaw, s., charlton, m., wymer, c., craft, a., 1987 a mark i geographical analysis · machine for the automated analysis of pointdata sets. int j. of gis 1. 33.5358[4] openshaw, s., cross, a., charlton, m., 1990 building a prototype geographical correlates exploration machine. int j of gis 3, 297312[5] openshaw, s., 1994a gis crime and spatial analysis in proceedings of gis and public policy conference. ulster business school 2234[6] openshaw, s., 1994b social costs and benefits of the census. proceedings of xvth international conference of the data protection andprivacy commissioners manchester. 8997[7] openshaw, s., 1994c two exploratory spacetime attribute pattern analysers relevant to gis. in fotheringham, s., and rogerson, p.,(eds) gis and spatial analysis taylor and francis. london 83104[8] openshaw, s., 1994d a concepts rich approach to spatial analysis. theory generation and scientific discovery in gis using massivelyparallel computing, in worboys, m.f. (ed) innovations in gis taylor and francis, london 123138some ideas about the exploratory spatial analysis technology required for massive databases155massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.[9] openshaw, s., 1994e neuroclassification of spatial data, in hewitson, b.c., and crane, r.g., (eds) neural nets: applications ingeography kluwer publishers, boston 5370[10] openshaw, s., 1994f computational human geography; exploring the geocyberspace leeds review 37, 201220[11] openshaw, s., 1995 developing automated and smart spatial pattern exploration tools for geographical information systems. thestatistician 44, 316[12] openshaw, s., rao, l., 1995 algorithms for reengineering 1991 census geography. environment and planning a 27. 425446some ideas about the exploratory spatial analysis technology required for massive databases156massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.massive data sets in navy problemsj.l. solka, w.l. poston, and d.j. marchettenaval surface warfare centere.j. wegmangeorge mason universityi. abstractthere are many problems of interest to the u. s. navy that, by their very nature, fall within the realm ofmassive data sets. our working definition of this regime is somewhat fuzzy although there is general agreementamong our group of researchers that such data sets are characterized by large cardinality (>106), highdimensionality (>5), or some combination of moderate levels of these two which leads to an overall large levelof complexity. we discuss some of the difficulties that we have encountered working on massive data setproblems. these problems help point the way to new needed research initiatives. as will be seen from ourdiscussions one of our applications of interest is characterized by both huge cardinality and large dimensionalitywhile the other is characterized by huge cardinality in conjunction with moderate dimensionality. thisapplication is complicated by the need to ultimately field the system on components of minimal computationalcapabilities.ii. introductionwe chose to limit the focus of this paper to two application areas. the first application area involves theclassification of acoustic signals arising from military vehicles. each of these acoustic signals takes the form of(possibly multiple) time series of varying duration. some of the vehicles whose acoustic signatures we maydesire to classify include helicopters, tanks, and armored personal carriers. in figures la and b we provide anexample plot of signals collected from two different rotary wing vehicle sources. these signals are variable inlength and can range anywhere from tens of thousands of measurements to billions of measurements dependingon the sampling rate and data collection interval. part of our research with these signals involves theidentification of advantageous feature sets and the development of classifiers which utilize them. as will bediscussed below this process in itself is fraught with associated massive data set problems. an additional layer ofdifficulty comes from the fact that we are interested in ultimatelymassive data sets in navy problems157massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.fielding an instrument which will be capable of classifying acoustic signatures under battlefield conditions in asnear to a realtime manner as possible. this burden places severe restrictions on the complexity and robustnessof the system that is ultimately fielded.figure 1 time series plots for two helicopter models.figure 2 grayscale images of a tank and mammogram.our second application focuses on the automatic segmentation of images into regions of homogeneouscontent, with an ultimate goal of classifying the different regions. our research has focused on a wide variety ofimage modalities and desired classification systems. they include the detection of combat vehicles in naturalterrain (solka, priebe, and rogers, 1992), the detection of manmade regions in aerial images (priebe, solka, androgers, 1993 and hayes et al., 1995), and the characterization of mammographic parenchymal patterns (priebeet al., 1995). in figure 2, we provide representative vehicular and mammographic images. the homogeneitycriterion in this case is based on functionality of the region. for example, we wish tomassive data sets in navy problems158massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.have the capability to identify a given region as containing a tank or a bush, normal parenchymal tissue or tumor.this problem is more difficult then the previous one with regard to the size of the data sets, the spatialcorrelation of the extracted features, and the dimensionality of the extracted features.in the following sections we discuss at length the specific difficulties that we have encountered analyzingthe data sets associated with these two applications. there are associated problems with both the exploratory dataanalysis of potential features, the development of density estimates of these features, and the ultimate testing andfielding of classification systems based on these features. after presenting these problems, we next suggest acourse of potential research initiatives which will help alleviate these difficulties. this is followed by a briefsummary of our main points.iii. discussionswe first discuss the simpler of these problems. in the acoustic signal processing application, one is initiallyconcerned with identifying features in the signals that will be useful in distinguishing the various classes. fourierbased features have been useful in our work with rotary wing vehicles because the acoustic signals that we havestudied to date have been relatively stationary in their frequency content. depending on the type of signal whichone is interested in classifying, one typically keeps a small number of the strongest frequency components. wecurrently employ a harmogramic hypothesis testing procedure to decide which and how many of the spectralcomponents are significant (poston, holland, and nichols, 1992). for some of our work this has led to twosignificant spectral components. in the case of rotary wing aircraft, these components correspond to the main andtail rotor frequencies (see figure 3). given a signal for each class of interest one extracts these features fromeach signal based on overlapping windows. in our application this easily leads to approximately 106 pointsresiding in r2 for each of 5 classes.however, once we extend the problem to the classification of moving ground vehicles, these simple featuresare no longer adequate. in fact, it is not surprising that the periodograms from moving ground vehicles exhibitnonstationary characteristics. in figure 4, we present a periodogram for one of the vehicle types. this problemrequires that one must perform exploratory data analysis on the possible tilings of regions of the timefrequencyplane in order to discover their utility as classification features. this increases the dimensionality of our problemfrom 2 to between 16 and 100 depending on our tiling granularity. so we are faced with the task of performingdimensionality reduction using millions of observations in tens of dimensions. even if one utilizes automatedtechniques such as projection pursuit for the determination of the needed projection, there can be problems withlong projection search times and overplotting in the lowerdimensional projected views.massive data sets in navy problems159massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.figure 3 power spectral density of one of the helicopter signals.figure 4 periodogram of a moving vehicle.massive data sets in navy problems160massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.in order to cast the classification problem within a hypothesis testing framework, probability densityestimates are built for each of the classes. performing quality probability density estimation in a moderately lowdimensional space (<4) using millions of features can be difficult. many density estimation procedures areanalogous to clustering and as shown by wegman (1995), clustering procedures are computationally costly. withan ultimate cost of between o(n3/2) and o(n2) depending on the nature of the algorithm, extremely longexecution times can be expected for the analysis of these large data sets.these probability density estimators are ultimately deployed as part of the fielded system. in addition it isdesirable for the system to have some limited capability to either refine the existing estimates for a given class orto add in an estimate of a new class's probability density function as part of its field operation. although we havedeveloped and do utilize schemes for density estimation that are recursive in nature (priebe and marchette, 1991and priebe, 1994), given a new signal of limited duration one may need to have stored the features for a period oftime in order to obtain a good maximum likelihood density estimate for the signal features. in addition the natureof this problem dictates our interest in density estimation and dimensionality reduction procedures in thepresence of partial classification information.the second problem is much more daunting in extent. to begin with it is not uncommon to deal withimages that are on the order of 4,000 × 4,000 pixels in size, with anywhere from 1 to 7 spectral bands per pixel.typical data sets can range from tens to thousands of images. unlike some industrial applications where there istight control on the background and possible content of the image, these images can have a very wide range ofsubjects. a typical example is to detect and identify the ground vehicles within an image. the range of possiblebackgrounds is enormous: desert, farmlands, forest, jungle, and city streets. each background will requiredifferent kinds of processing to eliminate clutter. further, one is often forced to determine the background fromthe image without a priori knowledge (for example, for an automated system on a satellite).in order to solve this problem, features must be computed from the image. these are then used for theclassification task. typically the problem is partitioned into subproblems. for example, one might firstdetermine the environment (forest, desert, etc.). this would then dictate the types of algorithms to be used for thedetection of regions of interest (roi), the detection of objects, and finally the classification of the objects into thedesired categories. each of these problems will require (potentially) different features to be extracted andprocessed.massive data sets in navy problems161massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.given the nature of the image classification problem there are many different types of features that can beextracted at each pixel or within each region of the image. even if one limits their attention to ''low level''features (for roi detection, for example), it is very easy to find 30 features of interest, each of which must becomputed for each pixel or small region of pixels. assuming 20 images are used to identify good combinationsof features for classification this still leads to roughly 9.6 billion observations in r30. this may have to berepeated for each different environment or class of potential targets. at this level of complexity even the mostrudimentary exploratory data analysis (eda) tasks are inadequate.the curse of dimensionality (scott, 1992) often requires a projection to a lower dimensional space beforedensity estimation or other classification techniques can be performed but this still reduces the complexity of theproblem only by an order of magnitude. the added complexity from the selection of appropriate projectionsmore than makes up for this reduction. rampant problems with overplotting and slow plotting times are thenorm for this kind of data, making interactive eda extremely difficult. again, wegman (1995) suggests thatdirect visualization of data sets over about 106 data points is virtually impossible. in fact, the unwieldy nature ofthese images makes it difficult for human operators to obtain ground truth on them. in addition, one oftenproduces a number of "interesting" projections or other derived data associated with a given set of features. itbecomes a very difficult task, just in the data structures area, to keep track of all of this associated data.even if one first segments the image into regions and extracts features based on regions rather thanindividual pixels, there are still serious problems. one still must deal with the entire image in order to do thesegmentation and so the original requirement to deal with approximately 16 million pixels per image still holds.in addition, different features may require different segmentation algorithms and so the apparent reduction innumber of feature vectors is easily matched by an increase in computational complexity.once a "best" combination of features is found, one must build probability density estimates for each of theclasses in the image. in this case the huge data sets that are associated with each class can lead to extremely slowconvergence of recursive density estimation procedures. this phenomena has been observed by us in the case oflikelihood maximization of over determined mixture models under the expectation maximization (em)algorithm (solka, 1995). in these cases careful monitoring of the parameter estimation process is essential(solka, poston, wegman 1995). if one attempts to mitigate this problem through the use of iterative proceduresone is faced with the overhead associated with the storage of all of the observations.massive data sets in navy problems162massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.the problem becomes more pronounced when one realizes that 20 images is by no means sufficient forvalidation of the algorithms. many hundreds or thousands of images must be processed in order to get goodestimates of performance. these images are then typically used as part of a leaveoneimageout cross validationprocedure. furthermore, these numbers may increase dramatically as the number of target types increases.a final problem, alluded to above, is that often truth values are known only for a very small subset of theimages. imagine trying to build a system to detect scud missiles in iraq. this obviously would require images ofthese missiles in the terrain in which they were likely to be deployed. however, in order to obtain these imagesone must either be lucky, spend a lot of time examining images, or already have a method for finding scudmissiles in iraq. while it is sometimes possible to use images of similar targets, or construct mock targets, thereis always the danger of building a system which finds mock targets (as opposed to real ones). as a result of thisproblem, detection and classification algorithms must be able to handle partially categorized and uncategorizeddata.fielded systems that will be utilized to build classifiers "on the fly" are required to deal with hundreds orthousands of images even in the training phase of the procedure. after an initialization phase these classifierswill be expected to continue to update their density estimates for the existing "target" classes and also build newestimates for any new target classes that might appear. as we discussed with regard to the acoustic signalprocessing work the use of recursive estimation procedures becomes essential.iv. proposed needsgiven this daunting list of problems what are some of the possible solutions? we will discuss our "wishlist" of solutions in the order into which we decompose the classification problem. first we will discuss the needsof feature extraction, followed by exploratory data analysis, then density estimation, and finally classification.we hope that these observations will help point the way to some fruitful research areas.first and foremost the problems associated with the design of classification systems must be managed bysome sort of database management system. there has been some recent press by such developers as statsciregarding database utilities which will be included in their new releases. there are also some packages providedby third party developers such as dbms copy. these packages are aimed at providing a seamless interfacebetween statistical analysis software and database management systems. that being said, there are yetfundamental research issues associated with the seamless integration of statistical software and databasemanagement systems. there are numerous types of information that occur routinely in the classification processthat these systems could be used to manage. we have focused on situations in which elemassive data sets in navy problems163massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.ments of the database may be multispectral images. how, for example, can one index and browse suchmultispectral image databases? low resolution, thumbnail sketches, for example, may essentially obscure oreliminate texture which often is the very feature for which we may be browsing. to our knowledge, no databasemanagement system has proven to be adequate for the problems addressed above, especially those inherent inmanaging a large image library with associated features and class models.more to the point, statisticians as a group tend not to think in terms of massive data files. the implication ofthis is that flat file structures tend to be adequate for the purposes of most small scale data and statisticalanalysis. there is no perceived need to understand more complex database structures and how to exploit thesestructures for analysis purposes. very few academic statistics programs would even consider a course in datastructures and databases as a legitimate course for an advanced degree program. one interesting consequence ofthis is the newfound interest in the computer science community for the discipline they now call data mining.while principally focused on exploratory analysis of massive financial transaction databases, tools that involvenot only statistical and graphical methods, but also neural network and artificial intelligence methods are beingdeveloped and commercialized with little input from the statistics community. two urls which give someinsight into the commercial knowledge discovery and data mining community are http://info.get.com/~kdd/ andhttp://www.ibi.com/. in short, we believe that a fundamental cultural shift among statistical researchers isneeded.next we turn our attention to the eda portion of the task. some of the needs here include techniques fordimensionality reduction in the presence of partial classification information and recursive dimensionalityreduction procedures. since the dimensionality reduction technique ultimately is part of any fielded system, itseems that recursive techniques for updating these transformations based on newly collected observations areneeded. techniques to deal with the problem of overplotting are also required. density estimation has beensuggested by scott (1992) as a technique to improve on the overplotting problem. the inclusion of binningprocedures such as the hexagonal procedure of carr et al. (1992) as part of the existing statistical analysispackages would be helpful. another approach to this problem is the use of sampling techniques to thin the databefore plotting. these techniques have the requirement of efficient methods for outlier identification (poston et.al., 1995 and poston, 1995). techniques for the visual assessment of clusters in higher dimensional space arealso needed. we have previously developed a system for the visualization of data structures in higher dimensionsbased on parallel coordinates (wegman, 1990), but much work remains to be done on this and new approaches tohigher dimensional cluster visualization.there remain many technical issues associated with probability density estimation for massive data sets inmoderately highdimensional spaces. efficient procedures for estimating these densities and performing clusteranalysis on them are needed. these procedures must be highly efficient in order to combat the large cardinalityof these data sets. further study into recurmassive data sets in navy problems164massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.sive estimation procedures is needed. recursive or some sort of hybrid iterative/recursive procedure becomesessential when the data sets become too massive for full storage. continued study into overdetermined mixturesmodels is needed. in particular additional work into the visualization of these models and the derivation of theirtheoretical properties is needed.last we turn our attention to some needs in the area of discriminant analysis. image classificationprocedures based on the use of markov random field models continues to be a fruitful area of research. inaddition research into the use of pseudometrics such as kullback leibler for comparisons of probability densityfunctions as an alternative to the standard likelihood ratio hypothesis testing deserves continued research. finallyprocedures that attempt to combine some of the modem density estimation techniques with the markov randomfield approach have merit. on very large images, markov random field techniques may be inappropriate due tocomputation time. fast versions of these techniques need to be developed.v. conclusionswe have examined some of the problems that we have encountered in building systems which performautomatic classification on acoustic signals and images. the design of these systems is a multifaceted problemthat spawns a large number of associated massive data set subproblems. we have not only indicated some of theproblems that we have encountered but we have also pointed the way to fruitful problems whose solutions arerequired to produce the tools which researchers in this area need.a final point is the use of video over single images, which is becoming more important as the technologyadvances. the use of video not only vastly decreases the time allowed to process each image (or frame) but alsogreatly increases the number of images. with image frame rates of 30 fps and higher this gives a potential newmeaning to the word massive data set. since these images are highly correlated in time and space, this raisespotentially a whole new conceptual framework. we conceive, for example, an animation or movie as a sequenceof two dimensional images. alternatively we could conceive of an animation or movie as a function f(x, y, t)which lives in r4. this change of perspective raises the need for filtering, processing, and analysis techniques forthree dimensional "images" as opposed to the two dimensional images we have been primarily discussing here.of course, at each "voxel" location it is possible to have a vectorvalued function (e.g. a multispectral vector).but this discussion raises many new issues, and is beyond the scope of this paper.vi. acknowledgmentsthe authors (ils, wlp and dim) would like to acknowledge the support of the nswcdd independentresearch program through the office of naval research. in addition, the work of ejw was supported by thearmy research office under contract number daah0494g0267, by themassive data sets in navy problems165massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.office of naval research under contract number n0001492j1303.vii. referencescarr, d.b., olsen, a.r. and white, d. (1992), "hexagon mosaic maps for display of univariate and bivariate geographical data,"cartography and geographic information systems, 19(4), 228236 and 271.poston, w.l., holland, o.t., and nichols, k.r. (1992) "enhanced harmogram analysis techniques for extraction of principal frequencycomponents," tr92/313poston, w.l., wegrnan, e.j., priebe, c.e., and solka, j.l. (1995) "a recursive deterministic method for robust estimation of multivariatelocation and shape," accepted pending revision to the journal of computational and graphical statistics.poston, w.l. (1995) optimal subset selection methods, ph.d. dissertation, george mason university: fairfax, va.priebe, c.e., (1994), "adaptive mixtures," journal of the american statistical association, 89, 796806.priebe, c.e., solka, j.l., lorey, r.a., rogers, g.w., poston, w.l., kallergi, m., qian, w., clarke, l.p., and clark, r.a. (1994), "theapplication of fractal analysis to mammographic tissue classification," cancer letters, 77, 183189.priebe, c. e., solka, j. l., and rogers, g. w. (1993), "discriminant analysis in aerial images using fractal based features," adaptive andlearning systems ii, f. a. sadjadi, ed., proc. spie 1962, 196208.priebe, c.e. and marchette, d.j. (1991) "adaptive mixtures: recursive nonparametric pattern recognition," pattern recognition, 24(12), pp.11961209.scott, d. w. (1992), multivariate density estimation, john wiley and sons: new york.solka, j.l., poston, w.l., and wegman, e.j. (1995) "a visualization technique for studying the iterative estimation of mixture densities,"journal of computational and graphical statistics, 4(3), 180198.solka, j.l. (1995) matching model information content to data information , ph.d. dissertation, george mason university: fairfax, va.solka, j. l., priebe, c. e., and rogers, g. w. (1992), "an initial assessment of discriminant surmassive data sets in navy problems166massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.face complexity for power law features," simulation, 58(5), 311318.wegman, e.j. (1990) "hyperdimensional data analysis using parallel coordinates," journal of the american statistical association, 85,664675.wegman, e.j. (1995) "huge data sets and the frontiers of computational feasibility," to appear journal of computational and graphicalstatistics , december, 1995.massive data sets in navy problems167massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.massive data sets in navy problems168massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.massive data sets workshop: the morning afterpeter j. huberuniversität bayreuthabstract.some issues crucial for the analysis of massive data sets are identified: computational complexity, datamanagement questions, heterogeneity of data, customized systems, and some suggestions are offered on how toconfront the challenges inherent in those issues.1 introduction.this paper collects some of my observations at, reactions to, and conclusions from, the workshop onmassive data sets in washingtond.c., july 78, 1995.at the workshop, we had not gotten as far as i had hoped. we had discussed long wishlists, but had notwinnowed them down to a list of challenges. while some position papers had discussed specific bottlenecks, orhad recounted actual experiences with methods that worked, and things one would have liked to do but couldn't,those examples had not been elaborated upon and inserted into a coherent framework. in particular, thediscussions in the small groups barely had scratched the implications of the fact that massive sets differ fromsmaller ones not only by size. maybe an additional day, providing more time for thinking and for informalcontacts and discussions, would have been beneficial.i shall try to continue the discussion of some of the points we left unfinished and connect some of the openends.2 disclosure: personal experiences.clearly, the personal viewpoints of the workshop participants were heavily influenced by the data sets theyhad worked with. we somehow resembled the proverbial group of blind men confronted with an elephant. thismakes it mandatory to disclose the data that have shaped one's views. in my case these were: children's growthdata, census data, air traffic radar data, environmental data, hospital data, marketing research data, road qualitydata, agricultural and meteorological data, with sizes ranging from 3 mbytes to 2 gbytes. most data sets wereobservational, a few were opportunistic; there were no imaging data. the census data were an outlier in severalrespects. i shall later cite specific examples for illustrative purposes. perhaps the most important thing i havelearned from thesemassive data sets workshop: the morning after169massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.experiences was: even though the data sources and the analysis goals at first blush seemed disparate, the analysesalmost invariably converged or are expected to converge toward a sometimes rudimentary, sometimes elaborate,customized data analysis system adapted to a particular data set. the reason of course is that in the case of largedata sets many people will have to work for an extended period of time with the same or similar data.3 what is massive? a classification of size.a thing is massive, if it is too heavy to be moved easily. we may call a data set massive, if its mere sizecauses aggravation. of course, any such a characterization is subjective and depends on the task, one's skills, andon the available computing resources.in my position paper (huber 1994b), i had proposed a crude objective classification of data by size, fromtiny (102 bytes), small (104), medium (106), large (108), huge (1010) to monster (1012). the step size 100 is largeenough to turn quantitative differences into qualitative ones: specific tasks begin to hurt at well defined steps ofthe ladder. whether monster sets should be regarded as legitimate objects of data analysis is debatable (at first, ihad deliberately omitted the "monster" category, then ed wegman added it under the name "ridiculous"). ralphkahn's description of the earth observing system however furnishes a good argument in favor of planning fordata analysis (rather than mere data processing) of monster sets.data analysis goes beyond data processing and ranges from data analysis in the strict sense (nonautomated,requiring human judgment based on information contained in the data, and therefore done in interactive mode, iffeasible) to mere data processing (automated, not requiring such judgment). the boundary line is blurred, partsof the judgmental analysis may later be turned into unsupervised preparation of the data for analysis, that is, intodata processing. for example, most of the tasks described by bill eddy in connection with fnmr imaging mustbe classified as data processing. with regard to visualization, one runs into problems just above medium sets. with regard to data analysis, a definite frontier of aggravation is located just above large sets, whereinteractive work breaks down, and where there are too many subsets to step through for exhaustivevisualization. with regard to mere data processing, the frontiers of aggravation are less well defined, processing timesin batch mode are much more elastic than in interactive mode. some simple standard data basemanagement tasks with computational complexity o(n) or o(n log(n)) remain feasible beyond terabytemonster sets, while others (e.g. clustering) blow up already near large sets.massive data sets workshop: the morning after170massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.4 obstacles to scaling.by now, we have very considerable experience with data analysis of small and medium sets. presentdaypcs are excellently matched to the requirements of interactive analysis of medium setsif one tries to go beyond,one hits several bottlenecks all at once. the problem is to scale our tools up to larger sets. some hard obstacles toscaling are caused by human limitations, by computational complexity or by technological limits. others arefinancial (e.g. memory costs) or lack of software (for massive parallelism).4.1 human limitations: visualization.direct visualization of a whole data set through scatterplots and scatterplot matrices is feasible withoutsignificant information loss up to about medium size. for larger sets one must either step through subsets orshow summaries (e.g. density estimates). this limitation has more to do with the resolution of the human visualsystem than with display hardware. ed wegman's position paper elaborates on this.this limitation holds across the board, also for imaging data: an efficiently coded highresolution picturecomprises at most a few megabytes.4.2 humanmachine interactions.necessary prerequisites for interactivity are: the task is such that a sequence of reasonably straightforwarddecisions have to be made in relatively quick succession, each of them based on the results of the preceding step.all three prerequisites can be violated for large sets: the decisions may be not straightforward because of datacomplexity, the response may be too slow (the human side of the feedback loop is broken if response timeexceeds the order of human think time, with the latter depending on the task under consideration), and it may bedifficult to provide a rational basis for the next decision if one cannot visualize the preceding results.in interactive work, the timing requirements are stringent: for highinteraction graphics the response timemust be a fraction of a second, for most other tasks of interactive data analysis it can be a few seconds, but itmay exceed 1020 seconds only very rarely.4.3 storage requirements.according to experiences with medium to large sets on pcs and workstations, disk size nowadays rarely isa problem, but memory size frequently is a bottleneck preventing full use of fast processors.backup storage (disk) must be large enough to hold the raw data plus several derived sets. for comfortablework it ought to provide space for the equivalent of about 10 copies of the raw data set.massive data sets workshop: the morning after171massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.on a singleprocessor machine with fiat highspeed memory, the latter must be large enough to hold 4copies of the largest array one intends to work with (otherwise one runs into severe swapping problems with twoargument array operations such as matrix multiplications); for comfortable work, it ought to be at least twice aslarge. thus, in order to run the processor at full speed, one sometimes may need almost as much free memory asfree disk space.4.4 computational complexity.processor speed does not scale well, since computational complexity tends to increase faster than linearlywith data size.the position papers of huber (1994b) and wegman contain extensive discussions of computationalcomplexity issues. so it suffices to sketch the salient points.at present, batch tasks involving a total of 1012 floating point operations (flops) are easy to handle on pcs,and 1015 flops are just about feasible on supercomputers (this corresponds to 1 gflop per second, sustained fortwo weeks). if performance continues to double every two years as in the past, we may reach 1018 operations in20 years or so. this argument ignores all questions of data flow, whose justintime management may become amuch worse bottleneck, considering that light moves a mere 0.3 mm in 1012 seconds.even so, it follows that we can safely forget about socalled computerintensive methods. for large sets andbeyond, tasks with a computational complexity of o(n2) are not feasible as batch jobs on any machine now. thepractical limit is around o(n3/2) operationsup to huge sets now, up to monster sets in 20 years.a simple orderofmagnitude calculation shows that computerintensive operations on the whole of a hugesets are infeasible even when the data has a coarsegrained structure and the intensive operations are restricted toone grain at a time, unless those grains are small. for the sake of the argument let us regard a total of n3/2operations as feasible in the huge to monster range. if there are n1" grains with a size m = n! each, and if acalculation with m3 operations is to be performed on each grain, we reach the feasible total of n3/2 operationswith " = 0.25. for n = 1010 and n = 1012, this corresponds to grain sizes of merely 316 and 1000, respectively.if the data is arranged in the form of a matrix with r rows and c columns, n = rc, with , thenthe same kind of argument shows that tasks with complexity o(nc), such as multiple regression, singular valuedecomposition and multiplication with a cbyc matrix on the right, all are feasible, while clustering algorithms,with complexity o(nr), are out.5 on the structure of large data sets.larger data sets tend to be structurally different from smaller ones. they are not just more of the same, theyare larger because they have to be larger. in particular, they are, as a rule, much more heterogeneous.massive data sets workshop: the morning after172massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.5.1 types of data.data can be experimental (from a designed experiment), observational (with little or no control of theprocess generating the data), or opportunistic (the data have been collected for an unrelated purpose). massivedata sets rarely belong to the first category, since by a clever design the data flow often can be reduced alreadybefore it is recorded (of course there are exceptions, e.g., computational fluid dynamics). but they often belongto the third category for plain reasons of economy.sometimes, data sets are massive because their collection is mandated by law (e.g. census and certain healthdata), or because they are collected anyway for other purposes (e.g. financial data). often, however, they have tobe massive because smaller sets will not do, and the predominant reason why they will not do is that the data inquestion are intrinsically heterogeneous. in particular, there may be many observers and many observed objects,both being located in space and time (e.g. aircraft traffic radar data).i wonder whether the onslaught of massive data sets will finally force us to acknowledge and heed somestudiously ignored. but longstanding admonitions going back to deming (1940), and reiterated by tukey(1962), to wit: the statistical protession as a whole is paying much too little attention to the need for dealingwith heterogeneous data and with data that arise from conditions not in statistical control (randomness).5.2 how do data sets grow?if we think in terms of a hierarchical data organization, data sets may grow by acquiring more hierarchical layers, or more branches, or bigger leaves,or all of the above. for example, some data sets are extremely large because each single leaf is an imagecomprising several megabytes.it must be stressed that actual data sets very often either do not possess a tree structure, or else severalconflicting ones. instead of ''leaf'', the more neutral terms "case" or "grain" might therefore be more appropriate.5.3 on data organization.statistical data bases often have a tree structure imposed on them through sampling or data collection (e.g.census districtshousing blockshouseholdspersons). but there may be several simultaneous conflicting treestructures (e.g. households and employers). different priority orderings of categorical variables define differenttree structures. for very large sets, a clean tree structure is rather the exception than the rule. in particular, thosesets often are composed during the analysis from several, originally unrelated sources (for example health dataand environmental data, collected independently for different purposes), that are linked as an afterthoughtthrough common external (here: geographical) references. inmassive data sets workshop: the morning after173massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.our work with some opportunistic data sets, we found that this kind of associative joining of originally unrelateddata sets was one of the most important operations. moreover, the larger the data set is, the more important aresubsetting operations, and also these cut across hierarchies or establish new ones.no single logical structure fits all purposes. in our experience, the flat format traditional in statistics usuallyturned out to be most expedient: the data are organized as a family of loosely linked matrices, each rowcorresponding to a "case", with a fixed number of columns, each column or "variable" being of a homogeneoustype.sometimes, an even simpler linear organization is preferable: a very long unstructured sequence of items,each item consisting of a single observation together with the circumstances under which it was made (whoobserved whom, which variable, when, where, and the like). from that basis, the interesting parts are extractedas required and restructured into matrix form.how such a logical organization should be implemented physically is of course an entirely differentquestion. the problem with massive data is to distribute not only the data, but also ancillary materials andretrieval tools over a hierarchy of storage devices, so that the ad hoc retrieval and reorganization tasks to beencountered in the course of a data analysis can be performed efficiently. for example, when should one workwith pointers, when with copies?5.4 derived data sets.it has been said that data analysis is a progress through sequences of derived data sets. we can distinguishbetween at least four levels of derived data sets: raw data set: rarely accessed, never modified, base data set: frequently accessed, rarely modified, low level derived sets: semipermanent, high level derived sets: transient.the base set is a cleaned and reorganized version of the raw set, streamlined for fast access and easyhandling. the base set and low level derived sets ordinarily must be maintained on some mass storage device forreasons of space. their preparation may involve sophisticated and complex, timeconsuming data processing.the highest level derived sets almost by definition must fit into high speed memory for reasons of computationalefficiency. the actual sizes and details of organization clearly will be governed by the available hardware andsoftware. to fix the idea: on presently available superworkstations (100 mflops, 1 gbyte memory) with almostpresentday software one may just be able to handle a huge raw data set (10 gbytes). in this case, high levelderived sets might comprise about 100 mbytes of data each for nongraphical tasks, but at most a few mbytes fortasks involving highly interactive visualization. with massively parallel hardware some of these figures can bepushed higher, but adequate software does not yet exist. here we have a definite software challenge: to designand implement a pilot system for general purpose data analysis of massive data sets on massively parallelhardware. more comments on this are contained in section 10.massive data sets workshop: the morning after174massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.derived sets can be formed in various ways. in our experience, low level derived sets mostly are created byapplication specific preprocessing, or by subsetting (more about this in sections 8 and 9). summaries areproblematic with large setsone ought not to group or summarize across heterogeneityand splitting intohomogeneous parts may be an overly expensive clustering problem. thus, one will be restricted in practice tosplitting based on external a priori information or, if data based, to cartlike singlevariable methods.afterwards, summaries of the homogeneous parts then may be recombined into new derived sets.6 data base management and related issues.with larger data sets, data base management operations become ever more important.6.1 data base management and data analysis systems.in view of the importance and central role of data base operations, it has been suggested that future dataanalysis (da) systems should be built around a data base management (dbm) kernel. but paradoxically, all theusual dbm systems do a very poor job with large statistical data bases. for an explanation why this is so, seefrench (1995), who confronts the design goals of the ordinary dbm systems with those of decision supportsystems (dss). data analysis needs all facilities of a dss, but more flexibility, in particular readwrite symmetryto assist with the creation and manipulation of derived sets. as a consequence, the designer of a da system mustperforce also design and implement his or her own dbm subsystem.6.2 problems and challenges in the data base area.data base type operations get both harder and more important with larger sets, and they are used morefrequently. with small and medium sets, where everything fits into high speed memory with room to spare andwhere all tasks are easily handled by a single processor, one does not even realize when one is performing a database operation on the side. larger sets may have to be spread over three or four hierarchical storage levels, eachlevel possibly being split into several branches. parallel processors and distributed memory create additionalcomplications.with large sets, processing time problems have to do more with storage access than with processor speed.to counteract that, one will have to produce small, possibly distributed, derived sets that selectively contain therequired information and can be accessed quickly, rather than to work with pointers to the original, larger sets,even if this increases the total required storage space and creates tricky problems with keeping data integrity (e.g.with carrying back and expanding to a superset some changes one has made in a subset).massive data sets workshop: the morning after175massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.to get a good grip on those problems, we must identify, categorize and rank the tasks we actually performnow with moderately sized sets. we then must identify specific tasks that become harder, or more important, orboth, with massive data sets, or with distributed processors and memory. in any case, one will need general,efficient subset operations that can operate on potentially very large base sets sitting on relatively slow storagedevices.7 the stages of an data analysis.most data analysis is done by nonstatisticians, and there is much commonality hidden behind a diversity oflanguages. rather than to try to squeeze the analysis into a too narrow view of what statistics is all about,statisticians ought to take advantage of the situation, get involved interdisciplinarily, learn from the experience,expand their own mind, and thereby their field, and act as catalysts for the dissemination of insights andmethodologies. moreover, the larger the data sets are, the more important the general science aspects of theanalysis seem to become relative to the "statistical" aspects.i believe that some of the discussions at the workshop have become derailed precisely because they weretoo much concerned with categories defined in terms of classical statistical concepts. in retrospect, it seems tome that it might have been more profitable to structure the discussions according to stages common to most dataanalyses and to watch out for problems that become more pronounced with more massive data.at the risk of belaboring the obvious, i am providing a kind of commented checklist on steps to be watched.7.1 planning the data collection.very often, the data is already there, and one cannot influence its collection and its documentation any more.the planning of a large scale data collection runs into problems known from big science projects: manydifferent people are involved over several years in a kind of relay race. by the time the data are ready to beanalyzed, the original designers of the experiment have left or are no longer interested, and the original goalsmay have been modified beyond recognition.the obvious conclusion is that big scale data collection must be planned with am open mind for unforeseenmodes of use. beware of obsolescence. the documentation must be complete and selfsufficient 10 years later,technical specifications of the measuring equipment may be lost, and names of geographical locations and thescope of zip code numbers may have changed. even the equipment to read the original media may be gone.if one is planning to collect massive data, one should never forget to reserve a certain percentage of the totalbudget for data analysis and for data presentation.massive data sets workshop: the morning after176massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.7.2 actual collection.it is not possible to plan and specify correctly all details ahead. in particular, minor but crucial changes inthe coding of the data often remain undocumented and must afterwards be reconstructed through painstakingdetective work. whoever is responsible for collecting the data must also be held responsible for documentingchanges to the code book and keeping it uptodate.everybody seems to be aware of the need for quality control, in particular with regard to instrument driftand the need for continuous calibration. there is much less awareness that also the quality of hardware, softwareand firmware of the recording system must be closely watched. i have personally encountered at least twounrelated instances where leading bits were lost due to integer overflow, in one case because the subject matterscientist had underestimated the range of a variable, in the other case because a programmer had overlooked thatshort integers do not suffice to count the seconds in a day. i also remember a case of unusable data summariescalculated online by the recording apparatus (we noticed the programming error only because the maximumoccasionally fell below the average).7.3 data access.as data analysts we need tools to read raw data in arbitrary and possibly weird binary formats. an examplewas given by allen mcintosh in connection with telephone network data. not only the actual reading must beefficient, but also the ad hoc programming of data input must be straightforward and easy; we have repeatedlyrun into similar problems and have found that very often we were hunting a moving target of changing dataformats.in addition, we must be able to write data in similarly weird formats, in order that we can forceheterogeneous sets into a homogeneous form.7.4 initial data checking.usually, the problem is viewed as one of legality and plausibility controls. what is outside of the plausiblerange is turned into missing values by the checking routine. this is a welltested, successful recipe foroverlooking obvious, unexpected features, such as the ozone hole.the real problem of data checking has to do with finding systematic errors in the data collection. and this ismuch harder! for example. how does one find accidental omissions or duplications of entire batches of data? the"linear" data organization mentioned in section 5.3 facilitates such checks.massive data sets workshop: the morning after177massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.7.5 data analysis proper.a person analyzing data alternates in no particular sequence between the following five types of activities: inspection, modification, comparison, modelling, interpretation.with massive data sets, both the inspection and the comparison parts run into problems with visualization.interpretation is thinking in models. models are the domain of subject matter specialists, not of statisticians; notall models are stochastic! therefore, modelling is one of the areas least amenable to a unified treatment and thusposes some special challenges with regard to its integration into general purpose data analysis software throughexport and import of derived sets.7.6 the final product: presentation of arguments and conclusions.with massive data, also the results of an analysis are likely to be massive. jim hodges takes the finalproduct of an analysis to be an argument. i like this idea, but regard it a gross oversimplification: in the case ofmassive data we are dealing not with a single argument, but with a massive plural of arguments. for examplewith marketing data, a few hundred persons may be interested in specific arguments about their own part of theworld, and once they become interested also in comparisons ("how is my product x doing in comparison toproduct y of my competitor?"), complexity grows out of hand. however, there is a distinction between potentialand actual: from a near infinity of potential arguments, only a much smaller, but unpredictable, selection willever be actually used.with massive data, the number of potential arguments is too large for the traditional precannedpresentation in the form of a report. one rather must prepare a true decision support system, that is a customized,specialpurpose data analysis system sitting on top of a suitable derived data set that is able to produce andpresent those arguments that the end user will need as a basis for his or her conclusions and decisions. if such asystem does a significantly better job than, say, a 1 000page report, everybody will be happy; this is a modestgoal.8 examples and some thoughts on strategy.by now, we have ample experience with the analysis of medium size data sets (data in the low megabyterange), and we begin to feel reasonably comfortable with large sets (108 bytes, or 100 megabytes), even thoughdirect visualization of larger than medium sets in their entirety is an unsolved (and possibly unsolvable) problem.let us postulate for the sake of the argumentsomewhat optimisticallythat we know how to deal with large sets.assume you are confronted with a huge data set (1010 bytes, or 10 gigabytes).massive data sets workshop: the morning after178massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.if a meaningful analysis is possible with a 1% random subsample, the problem is solvedwe are back tolarge sets. except for validation and confirmation, we might not even need the other 99%.assume therefore that random samples do not work for the problem under consideration. they may notwork for one of several possible reasons: either because the data are very inhomogeneous, or because they arehighly structured, or because one is looking for rare events, or any combination of the above. density estimatesor summary statistics then will not work either.example: air traffic radar data. a typical situation is: some 6 radar stations observe several hundred aircraft,producing a 64byte record per radar per aircraft per antenna turn, approximately 50 megabytes per hour. if one isto investigate a near collision, one extracts a subset, defined a by window in space and time surrounding the criticalevent. if one is to investigate reliability and accuracy of radars under reallife air traffic conditions, one mustdifferentiate between gross errors and random measurement errors. outlier detection and interpretation is highlynontrivial to begin with. essentially, one must first connect thousands of dots to individual flight paths(technically, this amounts to tricky prediction and identification problems). the remaining dots are outliers, whichthen must be sorted out and identified according to their likely causes (a swarm of birds, a misrecorded rangemeasurement, etc. etc.). in order to assess the measurement accuracy, one must compare individual measurementsof single radars to flight paths determined from all radars, interpolated for that particular moment of time. summarystatistics do not enter at all, except at the very end, when the results are summarized for presentation.i believe this example is typical: the analysis of large sets either begins with task and subject matterspecific, complex preprocessing, or by extracting systematic subsets on the basis of a priori considerations, or acombination of the two. summaries enter only later. often, the subsets will be defined by windows in space andtime. even more often, the selection has two stages: locate remarkable features by searching for exceptionalvalues of certain variables, then extract all data in the immediate neighborhoods of such features. for a nontrivial example of preprocessing, compare ralph kahn's description of the earth observing system and theconstruction of several layers of derived data sets. for one beginning with subsetting, see eric lander'sdescription of how a geneticist will find the genes responsible for a particular disease: in a first step, the locationin the human genome (which is a huge data set, 3 × 109 base pairs) is narrowed down by a factor 1 000 by atechnique called genetic mapping (lander 1995).after the preparatory steps, one may want to look up additional information in other data bases, possiblyfrom informal external sources:example: environmental data. we found (through eda of a large environmental data set) that very high radonlevels were tightly localized and occurred in houses sitting on the locations of old mine shafts.massive data sets workshop: the morning after179massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.in this example, indiscriminate grouping would have hidden the problem and would have made itimpossible to investigate causes and necessary remedies. the issue here is one of "data mining", not one oflooking, like a traditional statistician, "for a central tendency, a measure of variability, measures of pairwiseassociation between a number of variables". random samples would have been useless, too: either one wouldhave missed the exceptional values altogether, or one would have thrown them out as outliers.data analysis is detective work. the metaphor is trite but accurate. a careful distinction between tasksrequiring the acumen of a first rate sleuth and tasks involving mere routine work is required. after perusing someof the literature on data mining, i have begun to wonder: too much emphasis is put on futile attempts to automatenonroutine tasks, and not enough effort is spent on facilitating routine work.in particular, everybody would like to identify noteworthy, but otherwise unspecified features by machine.from my experience with projection pursuit on small to medium sets i think this is a hopeless search for the holygrail (computational complexity grows too fast with dimensionality). pattern discovery is intrinsically harderthan pattern recognition. a less ambitious, still hard task is the approximate matchup problem: find all structuresin data set a that are approximately similar to some structure in data set b, where a and b are large. it is not atall clear whether even such problems can be solved within the desired o(n3/2)complexity.9 volume reduction.volume reduction through data compression (with information loss) sometimes is advocated as a kind ofpanacea against data glut: "keep only what is exceptional, and summarize the rest". at least in the case ofobservational, as against experimental data, i think this is a daydream, possibly running counter to several of thereasons why the data are being collected in the first place! it is only a slight exaggeration to claim thatobservational data deserve to be saved either completely or else not at all. for example, a survey of the sky mustbe preserved completely if one later wants to check the early stages of supernovae.but prior to specific analyses, targeted volume reduction usually can be performed on a data matrix eitherby reducing the number of rows (cases) or the number of columns (variables), or both. this might be done basedon a priori interest. datadriven reduction might be done for example by aggregation (i.e. by combining similarrows) or by summarizing (e.g. by forming means and variances over a homogeneous set of rows). reducing thenumber of variables is usually called "dimension reduction" and can be done for example by variable selection(pick one of several highly correlated columns), or by forming (linear or nonlinear) combinations of variables.but it is difficult to put the general notion of dimension reduction on a sound theoretical basis; exploratoryprojection pursuit comes closest, but as its computational complexity increases exponentially with dimension, itis not well suited to massive data sets.massive data sets workshop: the morning after180massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.the next best general approach is dimension reduction through principal component or correspondenceanalysis (i.e. the truncated singular value decomposition): remember that the k leading singular values yield thebest approximation (in the square norm sense) to the data matrix by a matrix of rank k. according to sue dumaisthis was surprisingly successful in the context of information retrieval even with quite large data matrices.however, the most powerful type of dimension reduction is through fitting local models:example: children's growth data. several hundred children were observed periodically from birth to adulthood.among other things. for each child, 36 measurements of body length were available. it was possible to reducedimension from 36 to 6, by fitting a 6parameter growth curve to each child. the functional form of that curve hadseveral components and had been found by estimating some 30 global parameters from the total availablepopulation of children. most of the 6 parameters specific to a particular child had an intuitive interpretation (age atpuberty, duration and intensity of pubertal growth spurt, etc.); the residual error of the fit was only slightly largerthan the intrinsic variability of the measurements.local model fitting is computationally expensive, but typically, it seems to stay within the critical o(n3/2)limit.10 supercomputers and software challenges.on presently available superworkstations (100 mflops, 1 gbyte memory) one can certainly handle largesets with almostpresentday software, and one may just barely be able to handle huge sets (10 gbytes). to pushthose figures higher, one would have to invest in massively parallel supercomputers and novel software. is suchan investment worthwhile? what are its chances to succeed? i do not have final answers, but would like to offersome food for thought.10.1 when do we need a concorde?i believe the perceived (or claimed) need for supercomputers exceeds the real need.the problem is that of the concorde: flying the concorde is a status symbol, but if too much time is spenton the ground, the fast flight is not worth the money. response times in fractions of a second are neither needednor appreciated, if it takes several minutes, and possibly hours, to think up a question and to digest the answer.we must learn to identify and differentiate between situations where supercomputers are necessary or atleast truly advantageous, and situations where they are not.massive data sets workshop: the morning after181massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.we have encountered several examples with raw data sets in the 10900 mbyte range, where it had beenclaimed that a large mainframe or supercomputer plus several months of customized programming were neededin order to accomplish certain data analytic tasks. in all those cases we found that a well endowed pc, plus a afew weeks of customized programming in a highlevel interpreted data analysis language (isp) would beadequate, with comparable or even shorter execution times, at total costs that were orders of magnitude lower.10.2 general purpose data analysis and supercomputers.can general purpose data analysis take advantage of supercomputers? dongarra's famous benchmarkcomparison (i have the version of april 13, 1995 in front of me) highlights the crux of the problem: withoutspecial handtuning efforts to improve and distribute the data flow, the fastest multiprocessor supercomputersbeat the fastest singleprocessor superworkstations merely by a factor 4. which is not worth the money. tuningmay yield another factor 20 or so. we need to discuss strategies for recovering that factor 20. ad hoc codetweaking on a case by case basis is so labor intensive and errorprone that ordinarily it will be out of thequestion. that is, we have a very serious software challenge.our experiences with isp suggest how a speedup could be done in a general purpose system. isp is asmall, general purpose, array oriented, interpretive data analysis language somewhat similar to s. it contains acore of 100200 building blocks (commands or functions). explicit, interpreted looping is slow, but it is rarelyneeded. efficiency relative to compiled code increases with data size because of array orientedness. i believe thatmany of the building blocks can be beefed up for parallelism, but there may be snags. for example,reorganization and redistribution of data between subtasks might be more expensive than anticipated.the proof of the pudding is in the eating, i.e. we need to conduct the experiment suggested by huber(1994b, at the end of section 8) and build a working pilot system. to prove the point we should aim for a small,relatively unspecific, but universal system. for a basis, i would choose isp over s precisely because it has thesethree properties. but we should take the opportunity to build a new, better system from scratch, rather than tryingto port an existing system. from past experiences i estimate that it will take three years before the pilot systemfor such an experiment attains sufficient maturity for beta testing. we better begin soon.the next section summarizes the issues and experiences i consider important for such an undertaking.massive data sets workshop: the morning after182massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.10.3 languages, programming environments and databased prototyping.here is an account of some things we have learned from our isp experience (see also huber (1994a)). thetraditional programming languages (fortran, c, pascal, ...) are too lowlevel for the purposes of data analysis.we learned that already back in the 1970's: there is much reprogramming and reuse under just slightly differentcircumstances, and for that, these languages are too clumsy and too errorprone (cf. also the comments by allenmcintosh). subroutine libraries like linpack help, but are not enough. we need a highlevel arrayorientedlanguage on top, with a simple syntax and safe semantics, whose units or building blocks must be very carefullyselected for universal use. the language must be userextensible through combination of those units into newbuilding blocks with the same syntax. the core building blocks must be highly optimized.in the 1980's we became aware of the need for programming environments in the style of smalltalk andlisp machines. in data analysis, you never hit it right the first, or the second, or even the third time around, andit must be possible to play interactively with modifications, but without having to start everything from scratch.rather than building a system on top of smalltalk or lisp, we decided to augment our data analysis languageisp so that it acquired its own programming environment.around 1990, we realized that we had to go even further into what we call databased prototyping: build acustomized data analysis system while actually doing production work with the data. the basic problem is thatthe user (whether it is a customer or we ourselves) never is able to specify the requirements in advance. oursolution is to mock up a roughandready working prototype, and let the user work with it on his or her actualdata. without involving the actual user early and actively in the use and redesign of the system, in particular inissues of presentation of results (what to show and how), it is extremely hard to arrive at a satisfactory solution.some comments made by schmitz and schoenherr in the context of marketing databases nicely illustrate thedifficulty. a highlevel language and a good programming environment are indispensable prerequisites for databased prototyping.none of the existing languages and systems is entirely satisfactory. after seeing carr's list of preferences insection 4 of his position paper, it seems to me that our own software (isp) comes closer to an ideal, universalsystem than i ever would have suspected. it does a job superior to matlab in the area of visualization; we use itinstead of gis because the latter systems have difficulties with discontinuous values that are attached to arbitrarypoints rather than to grid points or political districts; after becoming dissatisfied with all available general database software, we began to improvise our own approaches in isp; we prefer it to sas and s for data analysis,especially with large sets. carr's comment on the ''diminished influence of the statistical community upon mywork'' is reflected by the fact that in isp we never felt compelled to go beyond a frugal minimum of statisticalfunctions.massive data sets workshop: the morning after183massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.11 summary of conclusions. with the analysis of massive data sets, one has to expect extensive, applicationand taskspecificpreprocessing. we need tools for efficient ad hoc programming. it is necessary to provide a highlevel data analysis language, a programming environment and facilitiesfor databased prototyping. subset manipulation and other data base operations, in particular the linking of originally unrelated datasets, are very important. we need a data base management system with characteristics rather differentfrom those of a traditional dbms. the need for summaries arises not at the beginning, but toward the end of the analysis. individual massive data sets require customized data analysis systems tailored specifically toward them,first for the analysis, and then for the presentation of results. pay attention to heterogeneity in the data. pay attention to computational complexity; keep it below o(n3/2), or forget about the algorithm. the main software challenge: we should build a pilot data analysis system working according to theabove principles on massively parallel machines.12 references.deming, w. e. (1940). discussion of professor hotelling's paper. ann. math. statist. 11 470471.french, c. d. (1995). "one size fits all" database architectures do not work for dss. sigmod record, vol. 24, june 1995.proceedings of the 1995 acm sigmod international conference on management of data. acm press.huber, p. j. (1994a). languages for statistics and data analysis. in: computational statistics, p. dirschedl and r. ostermann (eds.), physicaverlag, heidelberg.huber, p. j. (1994b). huge data sets. in: proceedings of the 1994 compstat meeting, r. dutter and w. grossmann (eds.), physicaverlag, heidelberg.lander, e. s. (1995). mapping heredity: using probabilistic models and algorithms to map genes and genomes, notices of the ams, july1995, 747753. adapted from: calculating the secrets of life. national academy press, washington, d.c. 1995.tukey, j. w. (1962). the future of data analysis. ann. math. statist. 33 167.massive data sets workshop: the morning after184massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.part ivfundamental issues and grand challenges185massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.186massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.panel discussionmoderator: james hodgesuniversity of minnesotamy name is jim hodges. first, i would like to introduce our panelists: art dempster, harvard university;usama fayyad, jet propulsion laboratory; dan carr, george mason university; peter huber, universitätbayreuth; david madigan, university of washington; mike jordan, massachusetts institute of technology; andluke tierney, university of minnesota. last night at dinner we test marketed a group of questions and we had acontention meter at the table. the four questions with the highest scores are the four questions we are going todiscuss, so you will see an argument. i am going to start off by giving the four questions.the first question: what is a theory, and what are theories good for? one of the things that a theory does isto carve the world into categories. and statistical theory, at least as it is taught in schools and as it is propoundedin journals, carves the statistical world into categories labeled estimation and hypothesis testing, and, to a lesserextent, model selection and prediction.this is not a very good characterization of what we do. for one thing, where is model identification? we doit all the time, but we do not really know how to interpret statistical inference in the presence of modelidentification. where in this list of categories is predictive validation?i would also say that this standard litany of hypothesis testing, model selection, and so on is not a helpfulcharacterization of what we need to do in particular problems. it does not characterize problems in terms of theburden of proof in any given problem. i am responsible for the line on group b's slide about how a statistician'sproduct is an argument, and there are qualitatively different arguments which i don't want to get into now. buteach comes with its distinctive burdens of proof. i would argue that that would be a start on a more usefulcategorization of statistical problems. but i will pose this question to the panel: what is a more usefulcategorization than estimation and hypothesis testing and so on, for statistical problems?i think we also need a second new categorization, which is categorizing problems in terms of data structuresthat are peculiar to them. i have this queer faith that there really is only a short list of truly different kinds of datastructures out there in the world, five or six, maybe, and if we could figure out what those are, we would be along way toward developing fairly generic software.the second question changes direction somewhat. before fisher, statistics was very databased. welooked at tables; we looked at a lot of pictures. there was not a lot of sophisticated machinery. fisher is probablyresponsible, more than anyone else, for making statistics a modelbased endeavor, perhaps too much so. onemight get the impression from the workshop talks that the advent of massive data sets means that we are going tobe coming back around again to being a more datadriven discipline. but you might alternatively say that wehave just identified the need for richer classes of models, or a more flexible vocabulary of structures. or as peterhuber put it last night, how does your data grow?the third question: a lot of computerintensive techniques like monte carlo markov chain [mcmc],bootstrap, or jackknife have been developed for mediumsized data sets. can those methods coexist with massivedata sets?the fourth question: in the usual sort of statistical theory, the idea of sufficiency makes life simple,because it reduces the amount of data that you have to worry about. so in a standardpanel discussion187massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.sort of an i.i.d. normal model, all you have to worry about are a couple of sufficient statistics. but sufficiency isdependent on the model in the usual sort of theory. or you can ask the question, can we define a notion or ananalog to sufficiency that does not depend on a specific model as a means of compressing data without loss ofinformation?those are the four questions. we will discuss question 1 first.question 1: what are some alternatives for classifying statisticalproblems?peter huber: i think question 1 is not connected with massive data sets. it is much older. as ademonstration of this, i would like to put up a slide from the annals of mathematical statistics, 1940. it is aboutthe teaching of statistics, a very short discussion of hotelling by deming. deming essentially says in slightlydifferent words what was said as question 1, namely, that the statisticians coming out of the schools somehow tryto fit all the problems into modem statistical techniques, the socalled model series of estimation, and so on. sofirst of all, this is not exactly new.james hodges: but perhaps it is no less compelling.huber: i think if you look at it carefully, deming pointed out the sample defect in the hotelling scheme forthe teaching of statistics.usama fayyad: here is my view of what you have to do to find models of data, and this is probably notnew to any of you. there are three basic things you have to do. first is representation of your models. that is thelanguage, how much complexity you allow, how many degrees of freedom, and so forth. second is modelevaluation or estimation. this is where statistics comes in bigtime on only some of the aspects, not all of them.finally, there is model search, which is where i think statistics has done very little. in other words, statisticiansdo not really do search in the sense of optimization. they go after closed forms or linear solutions and so forth,which we can achieve, and there is a lot to be said for that from a practical point of view. but that can only buyyou so much.in the middle, in the estimation stage, i see a high value for statistics in measurement of fit and so forth, andparameter selection for your models, once you have fixed the model. there are these notions of novelty or beinginteresting, accounting for utility. it has not been addressed. in practical situations, that is probably where mostof your "bang for the buck" is going to come from, if you can somehow account for those or go after those, andyour methods should have models of those dimensions.finally, on model representation, i think statistics has stuck to fairly simple, fairly wellanalyzed andunderstood models. with that, they brushed away the problem of search. they do not search over a large set ofmodels to select models to begin with. computers now, even with massive data sets, allow you to do things liketry out many more alternatives than are typically tried. if you have to take one pass at the data, take one pass, butyou can evaluate many things in that pass. that is my basic response to how i view question 1, and i am not surewhat the answers are.arthur dempster: i agree that the textbooks have the categories wrong. i have a set of categories that itend to use all the time. i do not have a slide, but imagine that there are five of them. they are a set ofinterlocking technologies which i think are the basis of applied statistics.panel discussion188massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.you can go back and forth among them, but you generally start with data structures in some broad sense,meaning the logical way the traits are connected in time and space and so on. i would also include in that themeta data, that is, the tying of the concepts to the science or to the realworld phenomenon. that is the first set ofthings; you have to learn how to set all of that up, not just for the data, but for the problem. there has to be aproblem that is behind it, or a set of problems.the second category is what is sometimes called survey design, experimental designšdata selection andmanipulation type issues, what choices have been made there, and things of that sort. if you can control them somuch the better.the third category is exploratory data analysis, summarizing, describing, digging in, and just getting a senseof whatever data you have.the fourth category is modeling. i am mostly concerned with stochastic modeling, although that includesdeterministic models in some special cases.the fifth category is inference, which i think is primarily the bayesian inference, at least in my way ofthinking, that is to say, reasoning with subjective probabilities for assumptions about what may be going on.but i think of all of these as forms of reasoning. they are opportunities for reasoning about the specificphenomenon. the idea of the specific phenomenon is the key, because one of the major complaints aboutstatistical theory as formulated and taught in the textbooks is that it is theory about procedures. it is divorcedfrom the specific phenomenon. that is part of the reason that statisticians have so much trouble getting back tothe real problems.there is a second part of the question which asks, do we need canonical data structures? yes, of course,and a lot of canonical models, as well. i'll quit there.daniel carr: i actually like that description very much. the only thing i would do is emphasize that weneed training to interact with people in other disciplines. there is a big change that has gone on from smallscience to big science. if we want to be part of big science, that means we must be part of teams, and we need tolearn how to work on teams, learn to understand and talk with people from other disciplines.i have seen lots of models being put out about how big science is done. a model might have a computerscientist, application specialist, and engineer. i do not even see the statistician listed in the models. that concernsme. we have seen some nice examples of teamwork here. i really like bill eddy's example, where there is agroup of people. that is bigtime science, and it is great.david madigan: the one comment i would add is, it might also be useful to think of the purposes to whichwe put our models as being of some relevance.sometimes we use models purely as devices to get us from the data to perhaps a predictive distribution or aclustered distribution, or whatever it is we do, in which case the model itself is not of much interest. sometimeswe build models to estimate parameters. we are interested in the size of an effect, and we build models toestimate the size of the effect, with primary interest focused on one parameter.then finally, sometimes we build models as models of the process that generated the data. but in myexperience, we as statisticians do not do as much of that as we should. in the first case, where the model is purelya means to an end, when dealing with massive data sets we should choose models that are computationallyefficient. there is a lot of scope for, in particular, graphic models in that context, because you can do thecomputations very efficiently.but at the other end of the scale where you are trying to build models of the process that generated the data,i am not sure. i haven't a clue how we are going to build those kinds of modelspanel discussion189massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.using massive data. the causative modeling folks are very excited about the possibilities of inferring causationfrom data sets, and the bigger they are, the better. but in practice, i do not know if we are going to be able to doit or not.michael jordan: my experience has been in the neural net community. the size of data sets that areusually studied there are what we used to think of as large. you had to have at least 10,000 data points, and100,000 was quite impressive. that is nothing here, but on the other hand, there has been some successši do notwant to overemphasize itšin that community with dealing with that size data set, and there are a few lessonsthat can be learned.one of them was that the neural net people immediately started to rediscover a great deal of statistics andbrought it on board very quickly. that has now happened completely. if you go to a neural net conference youwill feel quite at home. all the same terminology, all the way up to the most advanced bayesian terminology, isthere.in fact, it was clear quite early on to many of us who had some statistical training that a lot of these ideashad to be closely related. you end up using logistic functions, for example, in neural nets quite a bit. it has got tohave something to do with logistic regression. you have lots of hidden latent variables; it has got to havesomething to do with latent variable factor analysis and structure, and ridge regression; all of these techniquesare there.in fact, when people started to analyze their networks after they fit them on data, it was interesting, becausethe tools for analyzing the networks were right out of multivariate statistics books. you would cluster or youwould do canonical correlations on variables, and so it is a closely related set of ideas.this has happened increasingly. some of the ideas we are working on are to put layered networks into amore statistical framework, or to see them as generalizations of things like factor analysis, layers of factoranalysis of various kinds. we also rediscovered mixture models, but in a much broader framework than they arecommonly used in statistics. in fact, one of the pieces of work we did a few years ago was to recast the wholeidea of fitting a decision tree as a mixture model problem. you can convert a decision tree probabilistically into amixture model. therefore, all the usual apparatus like the em algorithm and so on are applicable directly todecision trees.as these ideas have started to be promulgated in the literature, the issues have become whether you cancompute the probabilistic quantities that you need. in the neural network literature, the problems are alwaysnonlinear. the fitting algorithms always have to be nonlinear. no question of (x' x)1 is ever even conceived inneural nets. one of the standard ideas that has become important is that you fit data incrementally as you go. youget a new data point, you make some changes to your model, but you do not ever have one model; you usuallyhave several models, and you make changes to the models. in fact, you do not make changes to all the modelsuniformly; you calculate something like a posterior probability of a model, given a data point, and you updatethose models that are most likely to have generated the data.if i were approaching one of the data sets that i have heard about in the last couple of days, this is the kindof methodology i would most want to use, to look at data points one after another, and start building sets ofmodels, start verifying them with each new data point that comes in, and start allowing myself to throw awaydata points that way. it is like smooth bootstrap, which is an example i gave in our discussion groupša lovelytechnique for allowing yourself to use an important technique, the bootstrap, which you could never use on agigabyte of data, but which you could use on a smoothed version of a gigabyte of data, conceptually.panel discussion190massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.in neural nets, this has been by far the most successful technique. in those small problems where you cancalculate an (x'x)1 type batch version of a fitting algorithm, it is usually much less successful. you have to domany more iterations in terms of real computer time than you do with a simple gradient or more general onlineincremental schemes. it has to do with the redundancy of the dam sets.there is a lot to be learned from this literature that works for mediumsized dam sets that i would tend towant to carry over.another way to think about this is that general clustering sorts of methodology are not just a simpletechnique; it is a way of life; it is a philosophy. models can be thought of as the elements of clusters. they do nothave to be simple centroids in some kind of euclidian space. think of each new dam point coming in asbelonging to one of the models, the posterior probability model giving the dam point, say, as your measure. thenas you are going through dam sets and making models, you are effectively doing a generalized kind of clustering,and you can think of these as mixture models, if you like that framework.i would like to have a lot of parallel processors, each one containing the developing model fitting the damsets, and by the time i got through with my massive dam set, i would have obtained a much smaller set ofparameterized models that i would then want to go back through again and validate, and so on.we tend to use a hodgepodge of techniques. sometimes it may look ad hoc, but the problem is that no onetechnique ever works on any of the dam sets we look at. so we always find ourselves using bits and pieces of ahierarchical this and a mixture of this and a bootstrap of this. i think that has to carry over to these morecomplicated situations.luke tierney: i do not have a whole lot more to add, just a few little things. on the second point aboutcanonical dam structure, to reemphasize something i have said, this is an area where i am sure the computerscience database community can offer us some help that is worth looking at and that we should be sure not toignore.on the other aspect, i cannot add much to what deming said. it is clear that these standard classifications inmathematical statistics not being directly related to statistical practice is a problem that has been around forever,and it is not connected to the largeness or smallness of the dam. that does not mean that these ideas areirrelevant. if you think of them as useful tools for giving you guidance about, for example, what is worthcomputing if you have to compute one thing, that is a useful piece of information to have. not the only piece, butit is useful, even in small dam situations, though there are many things for which we have no such theory thathelps us.with these larger data sets, if we find ourselves faced with situations where there are so many questions thatwe cannot possibly answer all of them, then we are going to have to start thinking in terms of the output on ananalysis of a large dam set that is not going to be necessarily a static report, not one number, not even onepicture, maybe not even a smile report, but a system, a software system that allows us to interactively askquestions. how does one decide that one such system is better than another? mean squared error is not going tohelp a lot.huber: can i add two remarks? one is, i strongly endorse luke tierney's last remark. essentially, amassive dam set requires massive answers, and massive answers you cannot do except with some softwaresystem. the other is on what would be a more useful categorization of statistical problems. i wonder whether oneshould rephrase it into a classification of statistical activities rather than problems. my five favorite categoriesare, first, inspection, and then modification, comparison, modeling, and interpretation. the inspection cancomprise almostpanel discussion191massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.anything, such as inspection of residuals from a model. modification may mean modification of the model, ormodification of the data by eliminating certain things that do not belong. comparison is allimportant.hodges: i would like to take usama fayyad's comment and then throw it open to the floor.fayyad: i just wanted to reinforce what michael jordan said in terms of the clustering being a way of life.one analogy i would like to draw is to this room here. if you wanted to study it in every detail, it is a hugemassive data set. think of what your visual system is doing. you encode this wood panel here as a panel. youcluster it away, and you do not spend your life staring at every fine little detail of variation in it. that is how youmove yourself through this huge data set and navigate through the room. so it seems to me that partitions of thedata, clusters, are the key to dealing with large data sets.that brings us headon with the big problem of how you can cluster if you need to order n2, where n is verylarge. but there are clever solutions for that. i don't believe data varies so much or grows in such hideous waysthat we cannot deal with it.jerome sacks: i have only one quick comment about all of the discussion here about what is importantabout statistics. everybody has left out the idea of statistical design.daniel relies: i had forgotten what art dempster said, too. there was an impressive list of five things,which did not include computing. i thought at one point you were trying to get the kinds of techniques that youwant to teach, and computing was not one of them.i remember what the dictionary said about statistics. it says that we are about the collection, manipulation,analysis, and presentation of masses of numerical data. numerical, i think we are all ready to shed; that is a littletoo restrictive.but when i try to tell people what i do, telling them i do inference, or telling them i do modeling, is a littletoo lowlevel for them to understand. those are subcategories of the bigger picture. i frankly find myself doingmostly collection and manipulation and relatively little of the analysis and presentation.i guess my feeling is that, if you do the collection and manipulation right, then you do not have to do muchanalysis.dempster: it all depends on the problem. as for computing, i am trying to put more emphasis on thinkingabout what you want to compute, rather than the problems of computing per se, which have dominated thisworkshop.hodges: there are five categories for problems. five seems to be the magic number today. the first kind ofproblem is causal problems, either inferential, where you want to determine whether a caused b in the past, orpredictive, whether a will cause b.the second kind of problem is noncausal predictor problems. i thought susan dumais' example wasperfect. you want to predict the performance of a research procedure and the causality involved, and why it hasthat performance is of no consequence whatsoever; you just want to know what it is going to do.the third class of problems i would consider is description, where, for example, you have a massive data setand you just want to summarize that data in a useful way, where there is no sampling or uncertainty involved.the fourth category is existence arguments. they are very popular in medicine, the area i work in, wherebasically a case series allows you to say, yes, it is possible to do coronary artery bypass grafts on people withclass iv congestive heart failure without killing threequarters of them.panel discussion192massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.the last class of problems i would describe as hypothesis generation, where the thing you produce is anargument that "here is a hypothesis that has not got any uninteresting explanation like a selection bias, and thatwe should therefore go expend resources on has not."where do i get these categorizations? if you look at each of these arguments, the burden of proof in eachargument is qualitatively distinct, and the activities that you have to do to make those arguments are qualitativelydistinct.for example, in susan dumais' case, the burden of proof is to show that, in fact, you do get the kind ofperformance from a search tool that you say you will get. that is what the argument has to establish. now, thereis a lot of statistical ingenuity that goes into building a tool like that, but the crux of the argument is to show that,yes, we have applied it to this group of data sets, we have measured its performance in this way, and we havethese reasons to believe that it will generalize that way.i think that if you focus on the problem, then that tells you what kind of activities you have to do to supportthe argument you need to make in that problem, and that in turn is going to drive things like what kind of datastructures you want to have, so you can do those activities.peter olsen: i would like to follow up on what usama fayyad said, in two senses. the first thing is, i thinkwe need to be able to take better advantage of the bandwidth we have on our own internal sensors to understandthe data that we are going to deal with. i suspect that we can process well in excess of t1 rates, in terms of oursenses, but we cannot present that data to our senses in that way. i am aware as i stand here of the sights, thesounds, the temperatures, the feel of the floor, all these things. but when i go to look at data, i look at data withmy computer. i have a 13inch diagonal screen.the second thing is that we are not born with this ability. if you look at a baby, he or she does not get theconcept of points, planes, lines. it takes a while for small children to come to grips with and be able to extract theworld around them. we seem to have become very good at being able to recognize very complex data structures.i can already tell the difference between you by sight and associate names with your faces, which is far beyondmy ability to compute on a computer. perhaps we ought to give some more thought to how people do thesethings, because we seem to be very successful at it.question 2: should statistics be databased or modelbased?hodges: does the advent of massive data sets mean that we are becoming more datadriven and less modeldriven, or does it just identify the need for richer classes of models? do the models in fact stay the same as ngrows?tierney: in the discussion groups that i have been in, less modeling is not what we have been talking about.there has been a lot of talk about different kinds of models, and about hierarchical models in a variety ofdifferent senses.i think to some degree one can say that hierarchical modeling is a divideandconquer strategy. one thingthat you do with hierarchical modeling, whether it is by partitioning or viewed in the most general sense, is bringa massive data set problem down to a situation where in some local sense, you have maybe a hundred data pointsper parameter, just as you have many leaves of a tree [model] at which there is one parameter with a hundreddata points sitting underneath them.panel discussion193massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.we have talked a lot about more modeling or more complex models. it looks like that is a way to makeprogress. whether that is necessarily the right way, or it is just a matter of trying to bring what we think weunderstand, which is small data ideas, to bear in the larger thing, is hard to tell.fayyad: i voiced my opinion in the group, but i will voice it formally on the recordši am very skeptical ofthis attitude: ''here is a large data set and it is cracking my back; i can't carry it, and it is very complicated.''i have dealt mostly with science data sets and a little bit with manufacturing data sets, not as massive as thescience ones. for those, i can understand why there is some degree of homogeneity in the data, so that yourmodels do not have to explode with the data as it grows.but in general, i am not convinced that you need to go after a significant jump in sophistication in modelingto grapple with massive data sets. i have not seen evidence to that effect.dempster: climate models are pretty complicated. i do not know how you get away from that.fayyad: is the complication an inherent property of whether the data set is small or not? i agree that somedata sets are complex and some data sets are totally random or not compressible. i am not disagreeing with thatfact. the question is, about these massive data sets that we collect nowadays, does their massiveness imply theircomplexity? that is the part that i disagree with a bit.dempster: i think it is the phenomenon that is complex, and it drags the data in with it.fayyad: by the way, peter huber made an excellent statement in one of the groups, and i filed it away as aquotation to keep. i agree with his point a lot, even though he disagrees with my thinking. if data werehomogeneous, one would not need to collect lots of it.jordan: the issue that always hits us is not n but always p [the number of parameters]. if p is 50, which isnot unusual in a lot of the situations we are talking about, especially if you get somewhat heterogeneous data setswhere the p are not commensurate and you have to treat them somewhat separately, then k to the p, where k is asmall integer like 2, is much too large, and is going to overwhelm any data set you are possibly going to collectin your whole lifetime.so these are not large data sets, in some sense. all of these issues that arise in small data sets of modelsbeing wrong, models being local, uncertainty, et cetera, are still there. i do not think these are massive data setsin any sense.carr: i had some thoughts about modeling. i tried to write down four classes of models. the first class ithought about was differential equations. i think of these as being very high compression models. they aretypically simple equations, and i can almost hold them in my memory.then we have a slightly more expansive set of models that i call heuristic functional fitting. the number ofparameters is a lot more. i do not know exactly how to interpret those. i might have standard errors for them, buti really do not know exactly what they mean, but they fit the data pretty well.then we can have a database model, where we have domain variables and predictor variables. if we coulduse nearestneighbor methods, it is almost like a table lookup. i latch onto the domain, find something that isclose, and then use the predictor variables as my prediction. so the database almost is a model itselfor more generally, we have tables. that is a model. this is way beyond my human memory, and so i usethe computer to actually handle the models.panel discussion194massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.there are also rulebased methods and probabilistic variance of that, where we try to encapsulate ourwisdom, our experience. so we have these different classes. it seems like the computer has made the differencein that our models can in some sense be massive, or way beyond our own memories.huber: i should add a remark on the structure and growth of data. i think the situation we have is roughlythis: in a small data set, we have one label or two labels, and with larger sets we have more labels, morebranches and bigger leaves. a single leaf may be a pixel.what is even worse, it is often not a tree. we impose the tree structure on it as a matter of convenience. so ihave begun to wonder whether the oldfashioned way of representing data as a family of matrices might not betechnically more convenient. the logical representation is not necessarily the internal representation.the other point was in connection with the advent of massive data sets, that the data will again becomemore prominent relative to the model. i am not so sure whether this is a correct description of the situation. ithink it is more like a situation in which we are evolving along a spiral. in the 17th century, statistics wasbasically tables. then in the 18th century, it was a little bit more incidenceoriented. then the 19th century wasthe century of graphics, population statistics, and so on. then it mined over again to mathematics in statistics. ithink we are now again in the other side of the spiral, where developments are on a larger time scale and theyhave relatively little to do with massive data sets.of course, the computer is the driving force behind it, and one does not forget what happened before.sometimes i am worried that the proponents of one particular branch tend to forget all the other branches, andthat the others are all wrong.eddy: could we get peter huber to give us an example of a model that is not a tree?huber: it is a question of subsetting. if you have a classical matrix structure, then a zeroone variablecorresponds to subsets. say you have males and females, you have unemployed and employedšof course, it ispossible to push the stuff into a tree if you subdivide into four branches, or you can do it by two. you can do it asyou want.i think the facility to rearrange data by subsetting is very important. once you have cast the whole thing intoa tree structure, rearrangement is conceptually complicated. that is one of the things that i think is mostimportant and gets more important with larger setsšthe ability to do subsetting in an efficient manner.eddy: i would like to agree, but it seems to me you can also think about it as multiple trees.huber: of course.madigan: i just have one point in response to that question, that massive data sets for modeling might goaway or something. nobody said that, but it was implied by the question.i think that inferring models of underlying physical processes is of fundamental importance and a majorcontribution of statistics. i think that massive data sets offer opportunities to build better models.dempster: history has been raised, and i thought i would talk about it briefly. there was a comment beforethat fisher statistics were very databased, and that was echoed in peter huber's comment. i do not see that at all,and so i am challenging the notion of oscillation a little bit.the 19th century was full of very modemsounding statistical inference. edgeworth in the 1885 50thanniversary publication of the royal statistical society was doing the kind of data analysis with small data setsthat people would have done in the 1950s and 1960s. there are manypanel discussion195massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.examples of galton and his correlation and regression and of gauss very concerned with theoretically justifyingleast squares and linear models and so on.so there has always been a balance. i would have thought that in the 19th century, the capabilities for doingdata analysis empirically were limited, until there was almost none of it being done. so i would have thought theother way around, but i am willing to see that there is a balance.i do think that the current situation, for technical reasons, because of computing, is a whole different bailgame, almost. i think, though, that the null hypothesis should still be that there will be this balance. it may in factbešmaybe i am echoing david madiganšthat you are more likely to get drowned if you do empirical dataanalysis on the huge thing, unless you are guided by the science somehow.so the role of models as simplifiers is likely to be stronger, i would think.jordan: when you pick up the telephone nowadays and say "collect" or "operator," what is happening isthat there are about a hundred hmms [hidden markov models] sitting back there analyzing those words, andthey are divided into different ones, one for each word.the way they train these things is very revealing. they know good and well that hmms are not a goodmodel of the speech production process, but they have also found that they are a good flexible model with agoodfitting algorithm, very efficient. so they can run 200 million words through the thing and adjust theparameters. therefore, it is a data reduction scheme that gets you quite a ways.after you fit these models with maximum likelihood, you try to classify with them, and you get pretty poorresults. but nonetheless, it is still within a clean statistical framework, and they do a lot of model validation andmerging and parameter tests and so on.they do not use that, though. they go another stop beyond that, which is typically called discriminanttraining; it is a maximum mutual information type procedure. what that does is move around the boundariesaround the models. you do not just train a model on its data; you also move it away from all the other data thatcorrespond to the other models.at the end of that procedure, you get a bunch of solutions that are by no means maximum likelihood, butthey will classify and predict better, often quite a bit better. it is very computer intensive, but this is how theysolve the problem nowadays.i think that is very revealing. it just shows that here is a case, a very flexible model that you can fit, and youdo not trust, but nonetheless it can be very useful for prediction purposes, and you use it in these different ways.for model validation, you still stay within maximum likelihood. but for classification prediction, you useanother set of techniques.edward george: i like usama fayyad's metaphor of thinking about the room. it also keeps gettingreinforced; i am seeing everything now through hierarchical models.another representation for thinking about that is multiresolution analysis. consider wavelets and what ishappening with wavelets right now: in some sense, an arbitrary function is a massive data set. so how do weorganize our model of it? we come up with something very smooth at the top level, and then a little less smooth,and we just keep going down like that.it is probably also the way we think about the room. for that panel, we have our gestalt, and that is waydown at the bottom, but we have top things as well. that is probably the fight way to organize hierarchicalinformation for simplicity.relies: i think we need to become more databased. that is where the action is. the complexity of the dataimply that modeldam interactions could very well come up to bite you. with a small data set, we would have theluxury of taking the information, spending a day or two orpanel discussion196massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.a week and boning up on it, and then fitting our models and publishing our journal articles. i do not think we cando that. we are going to make mistakes if we try to do that now.i think that the message has to be that dataoriented activities have to become recognized as a legitimateway for a statistician to spend a career. that gets back to something i said yesterday. i think that the journals areterrible about wanting sophisticated academic numerical techniques in an article in order to publish it. how i fixmy data set seems like a boring topic, but it is the foundation on which everything else rests. if we as statisticiansdo not recognize that and seize it, then our situation is going to be like what happened to the horse when theautomobile came along.all the excitement i have had in my career as a statistician has been derived from the data and thecomplexity that i have had to deal with there, and not from integrating out the fivedimensional multivariatenormal distribution, which is what i learned in school.david lewis: i absolutely agree with that. from the standpoint of a consumer of statistics, i am much moreinterested in reading about what you just described than reading about fivedimensional integration to do somefancy thing. but there is this huge unspoken set of things that applied statisticians do that we urgently need toknow about, because a lot of us who are not statisticians are thrown into the same data analysis game right now.carr: we are talking about these complex hierarchical models. yes, maybe that is the only thing that isgoing to work on some problems, but i would like to think of models as running the range from a scalpel to aclub. the differential equations are more like a scalpel, and some of these huge models that are only stored in thecomputer are like a club. i am not content until i get it down to something that i can understand.some problems may be impossible to get to that level, but i would like to seek understanding. yes, i haveall this processing going on in this room when i look at it, but if it does not have meaning to me, i am going toignore it, block it out. a lot of these complicated models work, but they are hard to grapple with if i don'tunderstand what all these thousands of coefficients mean, or all these tables mean.dempster: in case there is misunderstanding, i agree with dan relies that it is certainly databased. thequestion is, where is it data driven, meaning very empirical, or are model structures a part of it? the truth is thatit is an interaction between the two things. the danger in not paying attention to the interaction is i think a keything, as dan pointed out.tierney: usama fayyad made a comment early on about not wanting too complex models. i am not ahundred percent sure from what he said whether a hierarchical model would be complex or not, in his view.one of the points of it is that hierarchical models tend to be simple things put together in simple ways tobuild something larger. so in one sense they are simple, and in other senses they are more complex. but there aredifferent views that you can take of them. they can be a model. you can also think, as many people do, that onceyou fit one, it is a descriptive statistic; it is a model but it is used for the data, and it is something you need tounderstand. it is a dimensionality reduction usually, even if it is a binary tree type of thing. you are going downby a factor of two, the next level up by another factor of two. it simplifies. it is still complicated, and it is stillsomething we have to understand maybe, but we can bring new parts of it as data, use data analytic tools to lookat the coefficients of these models. we have to understand the small pieces and use our tools to understand thebigger pieces. i do not see the dichotomy necessarily. things flow together and work well. they need to be madeto work better.panel discussion197massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.fritz scheuren: for me there are questions and data and models, and models are in between questions anddata, and there is a back and forth. i think the interactive nature of the process needs to be emphasized. i think inthis world of fast computing and data that flows over you, you can focus more on the question than on themodels, which we used to do in the various versions of history, and i think the questioner is important, and weneed to talk about the questioners, with the models as a data reduction tool.fayyad: about the model complexity, just a quick response. if your data size is n, as n grows, is theresome constant somewhere, m, that is significantly smaller than n, after which if you continue growing n yourmodels do not need to go beyond that m? or does your decision tree or whatever it is, hierarchical model, growwith log n or whatever it is? that is the distinction. i find it hard to believe. there are so many degrees offreedom, at least in these redundant data sets that we collect in the real world, that that must be the case.i do not consider those beyond understanding. they imply a partition as a tool that lets you look at verysmall parts of the data and try to analyze them. it does not give you a global picture, though. but i do not thinkthat humans will be able to understand large data sets. i think we just have to face up to that, that we understandparts of them, and that is life. we may never get the global picture.question 3: can computerintensive methods coexist with massivedata sets?hodges: now we are going to change gears quite radically, although it is not quite a change of gears if wehave decided that hierarchical models are nirvana. then we have to come to the issue of how we compute them.in some sense, monte carlo markov chains and hierarchical models go together quite beautifully.madigan: can they coexist with massive data sets? i think that they have to coexist with massive data sets,and that is that. we must build these hierarchical models. the massive data sets will enable us to do it, and wehave to build the mcmc schemes to enable us to do it.jordan: the way they are built now, they do not do it at all. this is an important research question. theway the mcmc schemes are built now, they are batchoriented. there is a whole data set for every sample of astochastic sample, and that is hopeless. so there is a big gulf between use and theory.those schemes historically came out of statistical physics, developed in the 1940s for studying gasses. thestatistical physicists still use them a great deal and have developed them quite a bit since then. i gave a reference,a lead article on some of the recent work, and i guess ed george would know a lot more about this, too.but it rams out that there is also a second class of techniques that the physicists use even more, looselycalled renormalization group methods, or mean field type methods. these are just as interesting, in fact, moreinteresting in many cases. physicists like them because they get analytical results out of them. but if you look atthe equations that come out of these things, you can immediately turn them into interactive kinds of algorithmsto fit data. i think that for the researchoriented academicians in the room, this is a very important area. justtaking that one method from physics did not exhaust the set of tools available.fayyad: what i don't understand is what the bootstrap would have to do with the massive data sets. whatrole would it play?panel discussion198massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.dempster: i was going to comment on the bootstrap. the sampling theory of bootstrap was invented 20years ago, when the chemists were eating up all the computer time, and the statisticians wanted something thatwould do an equivalent kind of thing. it generated a lot of wonderful theory, and still is generating it. but i donot think it is especially relevant with massive data sets. at least, i don't see how.hodges: why not? in highdimensional spaces, you have still got to figure out what your uncertainty is. itcould be very large, and you would like to know that. the massive data set is not large n.dempster: i was just displaying my bayesian prejudices. i agree with david madigan. whether it ismcmc or some other magic method, those are the ones we need.huber: for massive data sets, you rarely operate in any particular instance from the whole data set. theproblem is to narrow it down to the path of interest. on that part, you can use anything.jordan: another way of saying it is, if you can do clustering first, then you can do bootstrap. the technicalterm for that is smooth bootstrap. clustering is density estimation. then if you sample from that, you are doingsmooth bootstrapping. that is a very useful way of thinking for large data sets.tierney: the bootstrap is a technical device. you can do a lot of things with it, but one of the things it getsat is variability of an estimate. i think a real question is, when is variability of an estimate relevant? sometimesit is. it is not when a dominating problem is model error: no matter how i can compute the standard error of theestimate, it is going to be small compared to the things that really matter. but at times it will be important, and ifit is important, then whatever means i need to compute it, whether it is the bootstrap, the jackknife, or anythingelse, i should go for it.another comment is that design of experiments is something i think we have neglected. there has got to besome way of using design ideas to help us with a large data set to decide what parts are worth looking at. i couldsay more than that, but i think that is something that must be worth pursuing.hodges: i know you could say a little more on that. would you, please?tierney: i wish i could. it is something we need to think about. we have had design talked about as one ofthe things we should do in statistics. we have not talked about it very much in the groups i have been in. therehas got to be potential there, to help make markov chain monte carlo work in problems where you cannot loopover every data point. there must be some interesting ideas from design that we can leverage.hodges: you mean selecting data points or selecting dimensions in a highdimensional problem?tierney: those might be possible. i do not claim to have answers.dempster: luke tierney raised the issue of model error. other people said there is no true model. so ifmodel error is the difference between the used model and the true model, then there is no model error, either. socould somebody elaborate on that concept?hodges: this is partly in response to luke tierney. if you hearken back to a little paper in the americanstatistician by efron and gong in 1983, and to some work that david freedman did in the census undercountcontroversy, you can use bootstrapping to simulate the outcome of model selection applied to a particular dataset. given the models you select on the different bootstrap samples, you may be able to get some handle on thebetweenmodel variability or the betweenmodel predictive uncertainty.panel discussion199massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.i am not enthusiastic about it myself, but that is one sense in which, to disagree with luke, something likethe bootstrap could be brought to bear.madigan: i think the bayesian solution is a complete solution to the problem that the bootstrap fails utterlyto solve. the basic problem is, if you are going to do some sort of heuristic model selection, you are likely toselect the same model every time. you will not explore models, and therefore you will not reflect true modeluncertainties.hodges: for the people who are not that familiar with what he is talking about, the bayesian idea he isreferring to is this: you entertain a large class of models and take the predictive distributions from those models,and combine them using some weighting such as the posterior probabilities of the model giving the data, so thatyou do not pick any models; you smoothly choose models.carr: one simple observation on the bootstrap is that if your sample is biased, your massive data set isn'treally adequately representing the phenomenon of interest, and then the bootstrap is not going to tell you thatmuch. you may model the database very well, and still not model the phenomenon of interest.george: a short comment. why are you talking about the bootstrap and not crossvalidation? this is ahighly rich environment when we are talking about massive data sets. we cannot do crossvalidation when youhave small data sets, but here we can. the bootstrap is in some sense a substitute for that when you do not haveenough data.john cozzens: would somebody enlighten me and please give me a definition of a massive data set? a lotof the things i have heard are, as far as i can tell, much ado over nothing. certainly in many scientificcommunities, i can give you examples of data sets that would overwhelm or dwarf anything you have describedand yet, they can be handled very effectively, and nobody would be very concerned.so what i would like to know is, particularly when you are talking about these things, where is the problem?i think to understand that, i would like a definition of what we really mean by a massive data set.daryl pregibon: i think that you are asking a valid question. i think that some of the applications we heardabout are examples. maybe we can just draw on allen mcintosh's talk in the workshop, where we are dealingwith packets of information on a network and accumulate gigabytes very quickly. there are different levels ofanalyses, different levels of aggregation, and there are important questions to be answered.i do not think a statistician or a runofthemill engineer can grapple with these issues very easily using thecurrent repertoire of tools, whether they be statistical or computational. i do not think we have the vocabulary todescribe massiveness and things like that; these are things beyond our reach. maybe it is complexity, and clearlythere is a size component. but i do not know how to deal with the problem that allen is dealing with. if youknow how, i think you should talk to allen, because he would love to talk to you.lewis: one simple answer to that is that the people in physics write their own software to handle theirgigantic data sets. the question is, are we going to require that everybody go out and write their own software ifthey have a large data set, or are we going to produce software and analytic tools that let people do that withoutbecoming computer scientists and programmers, which would seem like a real waste of time?panel discussion200massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.william eddy: i think there is an issue that john cozzens is missing here about inhomogeneity of the data.we are talking about data sets that are vastly inhomogeneous, and that is why we are worrying about clusteringand modeling and hierarchy and all of this staff.i think that john is saying, "i can process terabytes of data with no problem." yes, you can process it, but ifit consists of large numbers of inhomogeneous subsets, that processing is not going to lead to anything useful.cozzens: now you are beginning to put a definition on this idea of a massive data set.dempster: my operational definition is very simple. if i think very hard about what it is i want to computein terms of modeling and using the data and so on, and then i suddenly find that i have very difficult computingproblems and nobody can help me and it is going to take me months, then i am dealing with a massive data set.huber: i once tried to categorize data sets according to size, from tiny to small, medium, large, huge, andmonster. huge would be large enough so that its size would create aggravation. i was specifically not concernedwith mere data processing, that is, grinding the data sequentially through some meat grinder, but with dataanalysis.i think it is fairly clear that with today's equipment, the aggravation starts around 108 bytes. it depends onthe equipment we have now.sacks: i think the definition i prefer for massive data sets is the one the supreme court applies topornography: i don't know what it is, but i know it when i see it.question 4: is there an modelfree notion of sufficiency?hodges: can we define a modelfree notion or analog for sufficiency? the interest here is as a means ofcompressing data without loss of information.tierney: sufficiency in some ways has always struck me as a little bit of a peculiar concept, looking for afree lunch, being able to compress with no loss. most nonstatisticians who use the term data reduction do notexpect a free lunch. they expect to lose a little bit, but not too much. i think that is a reasonable thing to look at.i have a gut feeling i need to confirm. if you look at some of the ways people talk about efficiency ofestimators, early on, for example in rao's book, it is not from some of the fancy points of view that we often see,but more from the point of view of wanting to do a data reduction, and losing a little bit, but not losing too much.another way of putting this is, can we think about useful ideas of data compression? somebody raised anissue yesterday with the motion picture industry being able to produce a scene. fractal ideas help a lot. you canvery simply represent a scene in terms of a fractal model that does not reproduce the original exactly, but in acertain sense gives you the texture of a landscape, or something similar. you have lost information, but maybenot important information. you have to allow for some loss.huber: i have pretty definite ideas about it. sufficiency requires some sort of model. otherwise, you cannottalk about sufficiency. but maybe you may use approximate sufficiency in the way that le cam used it manyyears ago. the model may mean separating the staff into some structure plus noise.panel discussion201massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.dempster: we did have a discussion of this, and my thought was that sufficiency is a way to get rid ofthings if they are independent of anything you are interested in from an inferential point of view.hodges: i can see defining a notion of sufficiency that is determined not by the model but by the questionthat you are interested in. i think it is impossible to throw away some of the data forever, even by some fractalkind of idea, for example, because for some questions, the data at the most detailed level is exactly what youneed. you may be able to throw away higherlevel data because they are irrelevant to the issue you are interestedin. so it is the question that you are answering that determines sufficiency in that sense. perhaps we have cometo confuse the question with models because from the first 15 minutes of my first statistical class, for example,we were already being given models of parameters, and being told that our job was to answer questions aboutparameters.george: luke tierney talked about sufficiency and efficiency. i think another major issue for massive datasets is economies of scale. that turns a lot of the tradeoffs that we use for small data sets on their head, likecomputing cost versus statistical efficiency and summarization. for example, if you want to compute the mostefficient estimate for a gaussian random field for a huge data set, it would take you centuries. but you haveenough data to be able to blithely set estimates equal to some appropriate fixed values, and you can do it in asecond. it is inefficient, but it is the smart thing to do.ed russell: i do not see how you can possibly compress data without having some sort of model to tell youhow to compress it.huber: maybe i should ask ralph kahn to repeat a remark he made in one of the small sessions. just thinkof databases in astronomy, surveys of the sky, which are just sitting there, and when a supernova occurs, youlook up at what was sitting in the place of the supernova in previous years.if you think about data compression, sufficiency, i doubt that you could invent something reasonable thatwould cover all possible such questions that you can solve only on the basis of a historical database. you do notknow what you will use. but you might use any particular part to high accuracy.pregibon: i am partly responsible for this question. i am interested in it for two reasons. one is, i think inthe theory of modem statistics, we do have a language, we do have some concepts that we teach our students andwe have learned ourselves, such as sufficiency and other concepts. so this question was a way to prompt adiscussion of whether these things are relevant for the development of modem statistics. are they relevant forapplication to massive amounts of data?when we do significance testing, we know what is going to happen when n grows. all of our models aregoing to be shot down, because there is enough data to cast each in sufficient doubt. do we have a language, orcan we generalize or somehow extend the notions that we have grown up with to apply to large amounts of data,and maybe have them degrade smoothly rather than roughly?the other point about sufficiency is, there is always a loss of information. a sufficient statistic is not goingto lose any information relative to the parameters that are captured by the sufficient statistic, but you are going tolose the ability to understand what you have assumed, that is, to do goodnessoffit on the model that theparameters are derived from. so you are willing to sacrifice information on one piece of the analysis, that is,model validation, to get the whole or the relevant information on the parameters that you truly believe in.panel discussion202massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.items for ongoing considerationdata preparation elevation of status of data preparation and data quality stages in professional societies clear articulation of what is meant by a massive data set development of rigorous, theorybased methods for reduction of dimensionality systematic study of how, when, and why methods used with small and mediumsized data sets breakdown with large size data sets; understanding of how far current methods, both statistical andcomputational, can be pushed; articulation of the variety of models that might be useful development of methods for integration of tools and techniques development of specialized tools in general "packages" for nonstandard (e.g., sensorbased) data establishment of better links between statistics and computer science exploration of the use of "infinite" data sets to stimulate methods for massive data sets creation of richer language for describing structure in data educational opportunitiesšfor nonstatisticians who use some statistical techniques and for statisticians,to broaden the knowledge base and provide better links to computer sciencemodels and data presentation research issues discovery and comparison of homogeneous groups communication and display of variability and bias in models better design of hierarchical visual display new modeling metaphors and richer class of presentation approaches methods to help "generalize" and "match" local models (e.g., automated agents) robust or multiple models; sequential and dynamic modelsitems for ongoing consideration203massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved. alternatives to internal crossvalidation for model verification retooling of computing environment for modeling massive data sets simple presentation of ''massive'' complex data analysesitems for ongoing consideration204massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.closing remarksjon kettenringbellcorevery briefly, i would like to thank all the people who have made the last two days an interesting time. onbehalf of the national research council and the committee on applied and theoretical statistics, i hope thatthis is a beginning of a lot of excitement in the area of massive data sets. i think we have struggled with some ofthe very difficult aspects. i hope that we have a little momentum going. i hope that we have a useful network,and that you have had a chance to make some new connections here that will be valuable to you. we will followup in some ways that we know about now, and perhaps some others that some of you will take as a leadon toother efforts.again, thank you all very much.closing remarks205massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.closing remarks206massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.appendixworkshop participantsjack alexander, national research councilalbert f. anderson, public data queries, inc.thomas e. ball, mckinsey & company, inc.fred bannon, national security agencykenneth s. cantwell, national security agencydaniel b. cart, george mason universitycarolyn carroll, stat techmichael cohen, committee on national statisticsdianne cook, iowa state universityjohn cozzens, national science foundationkeith crank, national science foundationnoel cressie, iowa state universitymarshall m. deberry, jr., bureau of justice statistics, u.s. department of justicearthur dempster, harvard universitysusan t. dumais, bellcorecheryl eavy, national science foundationwilliam f. eddy, carnegie mellon universitystephen g. eick, bell laboratories (a division of lucent technologies)usama fayyad, jet propulsion laboratorymark fitzgerald, carnegie mellon universityedward george, university of texascolin r. goodall, health process management, pennsylvania state universityjames hodges, university of minnesotapeter j. huber, universität bayreuthmichael i. jordan, massachusetts institute of technologyralph kahn, jet propulsion laboratoryjon r. kettenring, bellcorecharles r. kindermann, bureau of justice statistics, u.s. department of justicer. brad kummer, lucent technologiesgad levy, oregon state university and university of washingtondavid d. lewis, at&t bell laboratoriesjames maar, national security agencydavid madigan, university of washingtonfred mann, department of defensed.j. marchette, naval surface warfare centeralien a. mcintosh, bellcoreaudris mockus, bell laboratories (a division of lucent technologies)ruth e. o'brien, national research councilpeter olsen, national security agencystan openshaw, leeds universityw.l. poston, naval surface warfare centerappendix207massive data sets: proceedings of a workshopcopyright national academy of sciences. all rights reserved.daryl pregibon, at&t laboratoriescarey priebe, johns hopkins universitydaniel relies, rand corporationedmund l. russell, advanced micro devicesjerome sacks, national institute of statistical sciencesfritz scheuren, george washington universityjohn schmitz, information resources, inc.david w. scott, rice universitysteven scott, harvard universityj.l. solka, naval surface warfare centerpadhraic smyth, university of california, irvinerobert st. amant, university of massachusetts, amherstwilliam f. szewczyk, national security agencyluke tierney, university of minnesotajohn r. tucker, national research councillyle ungar, university of pennsylvaniae.j. wegman, george mason universitylixin zeng, university of washingtonappendix208
detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/11865putting people on the map: protecting confidentiality withlinked socialspatial data176 pages | 6 x 9 | paperbackisbn 9780309104142 | doi 10.17226/11865myron p. gutmann and paul c. stern, editors; panel on confidentiality issuesarising from the integration of remotely sensed and selfidentifying data;committee on the human dimensions of global change; division of behavioral andsocial sciences and education; national research councilputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.panel on confidentiality issues arising from the integration ofremotely sensed and selfidentifying datamyron p. gutmann and paul c. stern, editorscommittee on the human dimensions of global changedivision of behavioral and social sciences and educationthe national academies presswashington, d.c.www.nap.eduputting people on the mapprotecting confidentiality with linked socialspatial dataputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved. the national academies press ¥ 500 fifth street, n.w. ¥ washington, dc 20001notice: the project that is the subject of this report was approved by the governing boardof the national research council, whose members are drawn from the councils of the national academy of sciences, the national academy of engineering, and the institute of medicine. the members of the committee responsible for the report were chosen for their specialcompetences and with regard for appropriate balance.this study was supported by contract/grant nos. bcs0431863, nnh04pr35p, and n01od42139, to 131 between the national academy of sciences and the u.s. national sciencefoundation, the u.s. national aeronautics and space administration, and the u.s. department of health and human services, respectively. any opinions, findings, conclusions, orrecommendations expressed in this publication are those of the author(s) and do not necessarily reflect the views of the organizations or agencies that provided support for the project.library of congress cataloginginpublication dataputting people on the map : protecting confidentiality with linked socialspatial data / panel onconfidentiality issues arising from the integration of remotely sensed and selfidentifyingdata, committee on the human dimensions of global change, division of behavioral andsocial sciences and education. p. cm. ònational research council.ó includes bibliographical references. isbn 9780309104142 (pbk.) ñ isbn 9780309668316 (pdf) 1. social sciencesñresearchñmoral and ethical aspects. 2. confidential communicationsñsocial surveys. 3.spatial analysis (statistics) 4. privacy, right ofñunited states. 5. public recordsñaccesscontrolñunited states. i. national research council (u.s.). panel on confidentiality issuesarising from the integration of remotely sensed and selfidentifying data. ii. title: protecting confidentiality with linked socialspatial data.h62.p953 2007174'.93ñdc22 2006103005additional copies of this report are available from the national academies press, 500 fifthstreet, n.w., lockbox 285, washington, dc 20055; (800) 6246242 or (202) 3343313 (inthe washington metropolitan area); internet http://www.nap.edu.printed in the united states of america.cover image: tallinn, the capital city and main seaport of estonia, is located on estoniaõsnorth coast to the gulf of finland. acquired on june 18, 2006, this scene covers an area of35.6 × 37.5 km and is located at 59.5 degrees north latitude and 25 degrees east longitude.the red dots are arbitrarily selected and do not correspond to the locations of actual researchparticipants.cover credit: nasa/gsfc/meti/ersdac/jaros and u.s./japan aster science team.suggested citation: national research council. (2007). putting people on the map: protecting confidentiality with linked socialspatial data. panel on confidentiality issues arisingfrom the integration of remotely sensed and selfidentifying data. m.p. gutmann and p.c.stern, eds. committee on the human dimensions of global change. division of behavioraland social sciences and education. washington, dc: the national academies press.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.the national academy of sciences is a private, nonprofit, selfperpetuating societyof distinguished scholars engaged in scientific and engineering research, dedicatedto the furtherance of science and technology and to their use for the general welfare.upon the authority of the charter granted to it by the congress in 1863, the academy has a mandate that requires it to advise the federal government on scientific andtechnical matters. dr. ralph j. cicerone is president of the national academy ofsciences.the national academy of engineering was established in 1964, under the charter ofthe national academy of sciences, as a parallel organization of outstanding engineers. it is autonomous in its administration and in the selection of its members,sharing with the national academy of sciences the responsibility for advising thefederal government. the national academy of engineering also sponsors engineering programs aimed at meeting national needs, encourages education and research,and recognizes the superior achievements of engineers. dr. wm. a. wulf is president of the national academy of engineering.the institute of medicine was established in 1970 by the national academy ofsciences to secure the services of eminent members of appropriate professions in theexamination of policy matters pertaining to the health of the public. the instituteacts under the responsibility given to the national academy of sciences by itscongressional charter to be an adviser to the federal government and, upon its owninitiative, to identify issues of medical care, research, and education. dr. harvey v.fineberg is president of the institute of medicine.the national research council was organized by the national academy of sciencesin 1916 to associate the broad community of science and technology with theacademyõs purposes of furthering knowledge and advising the federal government.functioning in accordance with general policies determined by the academy, thecouncil has become the principal operating agency of both the national academyof sciences and the national academy of engineering in providing services to thegovernment, the public, and the scientific and engineering communities. the council is administered jointly by both academies and the institute of medicine. dr.ralph j. cicerone and dr. wm. a. wulf are chair and vice chair, respectively, of thenational research council.www.nationalacademies.orgputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.vpanel on confidentiality issues arising fromthe integration of remotely sensed andselfidentifying datamyron p. gutmann, chair, interuniversity consortium forpolitical and social research, university of michigan, ann arbor marc p. armstrong, department of geography, university of iowa deborah balk, school of public affairs, baruch college, cityuniversity of new york kathleen oõneill green, alta vista company, berkeley, ca felice j. levine, american educational research association,washington, dc harlan j. onsrud, department of spatial information science andengineering, university of maine jerome p. reiter, institute of statistics and decision science, dukeuniversity ronald r. rindfuss, department of sociology and the carolinapopulation center, university of north carolina at chapel hillpaul c. stern, study directorlinda depugh, administrative assistantputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.prefaceviithe main themes of this reportñprotecting the confidentiality of human research subjects in social science research and simultaneously ensuring that research data are used as widely and as frequently as possibleñhave been the subject of a number of national research council (nrc)publications over a considerable span of time. beginning with sharing research data (1985) and continuing with private lives and public policies:confidentiality and accessibility of government statistics (1993), protecting participants and facilitating behavioral and social science research(2003), and, most recently, expanding access to research data: reconciling risks and opportunities (2005), a series of reports has emphasized thevalue of expanded sharing and use of social science data while simultaneously protecting the interests (and especially the confidentiality) of human research subjects. this report draws from those earlier evaluationsand analyzes the role played by a type of data infrequently discussed inthose publications: data that explicitly identify a location associated with aresearch subjectñhome, work, school, doctorõs office, or somewhere else.the increased availability of spatial information, the increasing knowledge of how to perform sophisticated scientific analyses using it, and thegrowth of a body of science that makes use of these data and analyses tostudy important social, economic, environmental, spatial, and public healthproblems has led to an increase in the collection and preservation of thesedata and in the linkage of spatial and nonspatial information about thesame research subjects. at the same time, questions have been raised aboutthe best ways to increase the use of such data while preserving respondentputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.viiiprefaceconfidentiality. the latter is important because analyses that make the mostproductive use of spatial information often require great accuracy andprecision in that information: for example, if you want to know the routesomeone takes from home to the doctorõs office, imprecision in one or theother degrades the analysis. yet precise information about spatial locationis almost perfectly identifying: if one knows where someone lives, one islikely to know the personõs identity. that tension between the need forprecision and the need to protect the confidentiality of research subjects iswhat motivates this study.in this report, the panel on confidentiality issues arising from theintegration of remotely sensed and selfidentifying data recommends waysto find a successful balance between needs for precision and the protectionof confidentiality. it considers both institutional and technical solutionsand draws conclusions about each. in general, we find that institutionalsolutions are the most promising for the short term, though they needfurther development, while technical solutions have promise in the longerterm and require further research.as the report explains, the members of the panel chose in one significant way to broaden their mandate beyond the explicit target of òremotelysensed and selfidentifyingó data because working within the limitation ofremotely sensed data restricted the problem domain in a way at odds withthe world. from the perspective of confidentiality protection, when socialscience research data are linked with spatial information, it does not matterwhether the geospatial locations are derived from remotely sensed imageryor from other means of determining location (gps devices, for example).the issues raised by linking remotely sensed information are a special casewithin the larger category of spatially precise and accurate information.for that reason, the study considers all forms of spatial information as partof its mandate.in framing the response to its charge, the panel drew heavily on existingreports, on published material, and on best practices in the field. the panelalso commissioned papers and reports from experts; they were presented ata workshop held in december 2005 at the national academies. two of thepapers are included as appendixes to this report. biographical sketches ofpanel members and staff are also included at the end of this report.this report could not have been completed successfully without thehard work of members of the nrc staff. paul stern served as study directorfor the panel and brought his usual skills in planning, organization, consensus building, and writing. moreover, from a panel chairõs perspective, he isa superb partner and collaborator. we also thank the members of thecommittee on the human dimensions of global change, under whoseauspices the panel was constituted, for their support.the panel members and i also thank the participants in the workshopputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.prefaceixon confidentiality issues in linking geographically explicit andselfidentifying data. their papers and presentations provided the members of the panel with a valuable body of information and interpretations,which contributed substantially to our formulation of both problems andsolutions.rebecca clark of the demographic and behavioral sciences branch ofthe national institute of child health and human development has been atireless supporter of many of the intellectual issues addressed by this study,both those that encourage the sharing of data and those that encourage theprotection of confidentiality; and it was in good part her energy that led tothe studyõs initiation. we gratefully acknowledge her efforts and the financial support of the national institute of child health and human development, a part of the national institutes of healthof thedepartment ofhealth and human services; the national science foundation; and thenational aeronautics and space administration.finally, i thank the members of the panel for their hard work and activeengagement in the process of preparing this report. they are a lively groupwith a wide diversity of backgrounds and approaches to the use of spatialand social science data, who all brought a genuine concern for enhancingresearch, sharing data, and protecting confidentiality to the task that confronted us. national research council panels are expected to be interdisciplinary: thatõs the goal of constituting them to prepare reports such as thisone. this particular panel was made up of individuals who were themselvesinterdisciplinary, and the breadth of their individual and group expertisemade the process of completing the report especially rewarding. the panelõsdiscussions aimed to find balance and consensus among these diverse individuals and their diverse perspectives. writing the report was a group effortto which everyone contributed. iõm grateful for the hard work.this report has been reviewed in draft form by individuals chosen fortheir diverse perspectives and technical expertise, in accordance with procedures approved by the report review committee of the national researchcouncil. the purpose of this independent review is to provide candid andcritical comments that assist the institution in making the published reportas sound as possible and ensure that the report meets institutional standards for objectivity, evidence, and responsiveness to the study charge. thereview comments and draft manuscript remain confidential to protect theintegrity of the deliberative process.we thank the following individuals for their participation in the reviewof the report: joe s. cecil, division of research, federal judicial center,washington, dc; lawrence h. cox, research and methodology, nationalcenter for health statistics, centers for disease control and prevention,hyattsville, md; glenn d. deane, department of sociology, university atalbany; jerome e. dobson, department of geography, university of kanputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.xprefacesas; george t. duncan, heinz school of public policy and management,carnegie mellon university; lawrence gostin, research and academicprograms, georgetown university law center, washington, dc; josephc. kvedar, directorõs office, partners telemedicine, boston, ma; w.christopher lenhardt, socioeconomic data and applications center, columbia university, palisades, ny; jeanbernard minster, scripps institutionof oceanography, university of california, la jolla, ca; and gerardrushton, department of geography, the university of iowa.although the reviewers listed above provided many constructive comments and suggestions, they were not asked to endorse the conclusions orrecommendations nor did they see the final draft of the report before itsrelease. the review of this report was overseen by richard kulka, abtassociates, durham, nc. appointed by the national research council,he was responsible for making certain that an independent examination ofthis report was carried out in accordance with institutional procedures andthat all review comments were carefully considered. responsibility for thefinal content of this report rests entirely with the authoring panel and theinstitutions.myron p. gutmann, chairpanel on confidentiality issues arising from theintegration of remotely sensed and selfidentifying dataputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.xicontentsexecutive summary11linked socialspatial data: promises and challenges72legal, ethical, and statistical issues in protecting confidentiality263meeting the challenges424the tradeoff: confidentiality versus access59references71appendixesaprivacy for research data81robert gellmanbethical issues related to linked socialspatial data123felice j. levine and joan e. sieberbiographical sketches for panel members and staff160putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.1executive summaryprecise, accurate spatial data are contributing to a revolution in somefields of social science. improved access to such data about individuals,groups, and organizations makes it possible for researchers to examinequestions they could not otherwise explore, gain better understanding ofhuman behavior in its physical and environmental contexts, and createbenefits for society from the knowledge flows from new types of scientificresearch. however, to the extent that data are spatially precise, there is acorresponding increase in the risk of identification of the people or organizations to which the data apply. with identification comes a risk of variouskinds of harm to those identified and the compromise of promises of confidentiality made to gain access to the data.this report focuses on the opportunities and challenges that arise whenaccurate and precise spatial data on research participants, such as the locations of their homes or workplaces, are linked to personal information theyhave provided under promises of confidentiality. the availability of thesedata makes it possible to do valuable new kinds of research that linksinformation about the external environment to the behavior and values ofindividuals. among many possible examples, such research can explorehow decisions about health care are made, how young people develophealthy lifestyles, and how resourcedependent families in poorer countriesspend their time obtaining the energy and food that they need to survive.the linkage of spatial and social information, like the growing linkage ofsocioeconomic characteristics with biomarkers (biological data on indiputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.2putting people on the mapviduals), has the potential to revolutionize social science and to significantlyadvance policy making.while the availability of linked socialspatial data has great promise forresearch, the locational information makes it possible for a secondary user ofthe linked data to identify the participant and thus break the promise ofconfidentiality made when the social data were collected. such a user couldalso discover additional information about the research participant, withoutasking for it, by linking to geographically coded information from other sources.open public access to linked social and highresolution spatial datagreatly increases the risk of breaches of confidentiality. at the same time,highly restrictive forms of data management and dissemination carry veryhigh costs: by making it prohibitively difficult for researchers to gain accessto data or by restricting or altering the data so much that they are no longeruseful for answering many types of important scientific questions.conclusionsconclusion 1: recent advances in the availability of socialspatialdata and the development of geographic information systems (gis) andrelated techniques to manage and analyze those data give researchersimportant new ways to study important social, environmental, economic, and health policy issues and are worth further development.conclusion 2: the increasing use of linked socialspatial data hascreated significant uncertainties about the ability to protect the confidentiality promised to research participants. knowledge is as yet inadequate concerning the conditions under which and the extent to whichthe availability of spatially explicit data about participants increasesthe risk of confidentiality breaches.various new technical procedures involving transforming data or creating synthetic datasets show promise for limiting the risk of identificationwhile providing broader access and maintaining most of the scientific valueof the data. however, these procedures have not been sufficiently studied torealistically determine their usefulness.conclusion 3: recent research on technical approaches for reducing the risk of identification and breach of confidentiality has demonstrated promise for future success. at this time, however, no knowntechnical strategy or combination of technical strategies for managinglinked spatialsocial data adequately resolves conflicts among the objectives of data linkage, open access, data quality, and confidentialityprotection across datasets and data uses.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.executive summary3conclusion 4: because technical strategies will be not be sufficientin the foreseeable future for resolving the conflicting demands for dataaccess, data quality, and confidentiality, institutional approaches willbe required to balance those demands.institutional solutions involve establishing tiers of risk and access anddeveloping datasharing protocols that match the level of access to the risksand benefits of the planned research. such protocols will require that theauthority to decide about data access be allocated appropriately amongprimary researchers, data stewards, data users, institutional review boards(irbs), and research sponsors and that those actors are very well informedabout the benefits and risks of the data access policies they may be asked toapprove.we generally endorse the recommendations of the 2004 national research council report, protecting participants and facilitating social andbehavioral sciences research, and the 2005 report, expanding access toresearch data: reconciling risks and opportunities, regarding restrictedaccess to confidential data and unrestricted access to publicuse data thathave been modified so as to protect confidentiality, expanded data access(remotely and through licensing agreements), increased research on ways toaddress the competing claims of access and confidentiality, and relatedmatters. those reports, however, have not dealt in detail with the risks andtradeoffs that arise with data that link the information in social scienceresearch with spatial locations. consequently, we offer eight recommendations to address those data.recommendationsrecommendation 1: technical and institutional researchfederal agencies and other organizations that sponsor the collectionand analysis of linked socialspatial datañor that support data thatcould provide added benefits with such linkageñshould sponsor research into techniques and procedures for disseminating such data whileprotecting confidentiality and maintaining the usefulness of the datafor socialspatial analysis. this research should include studies to adaptexisting techniques from other fields, to understand how the publication of linked socialspatial data might increase disclosure risk, and toexplore institutional mechanisms for disseminating linked data whileprotecting confidentiality and maintaining the usefulness of the data.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.4putting people on the maprecommendation 2: education and trainingfaculty, researchers, and organizations involved in the continuing professional development of researchers should engage in the education ofresearchers in the ethical use of spatial data. professional associationsshould participate by establishing and inculcating strong norms for theethical use and sharing of linked socialspatial data.recommendation 3: training in ethical issuestraining in ethical considerations needs to accompany all methodological training in the acquisition and use of data that include geographically explicit information on research participants.recommendation 4: outreach by professional societies and other organizationsresearch societies and other research organizations that use linkedsocialspatial data and that have established traditions of protection ofthe confidentiality of human research participants should engage inoutreach to other research societies and organizations less conversantin research with issues of human participant protection to increaseattention to these issues in the context of the use of personal, identifiable data.recommendation 5: research designprimary researchers who intend to collect and use spatially explicit datashould design their studies in ways that not only take into account theobligation to share data and the disclosure risks posed, but also provideconfidentiality protection for human participants in the primary research as well as in secondary research use of the data. although thereconciliation of these objectives is difficult, primary researchers shouldnevertheless assume a significant part of this burden.recommendation 6: institutional review boardsinstitutional review boards and their organizational sponsors shoulddevelop the expertise needed to make wellinformed decisions that balance the objectives of data access, confidentiality, and quality in research projects that will collect or analyze linked socialspatial data.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.executive summary5recommendation 7: data enclavesdata enclaves deserve further development as a way to provide wideraccess to highquality data while preserving confidentiality. this development should focus on the establishment of expanded placebasedenclaves, òvirtual enclaves,ó and meaningful penalties for misuse ofenclaved data.recommendation 8: licensingdata stewards should develop licensing agreements to provide increasedaccess to linked socialspatial datasets that include confidential information.the promise of gaining important scientific knowledge through theavailability of linked socialspatial data can only be fulfilled with carefulattention by primary researchers, data stewards, data users, irbs, and research sponsors to balancing the needs for data access, data quality, andconfidentiality. until technical solutions are available, that balancing mustcome through institutional mechanisms.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.71linked socialspatial data:promises and challengesprecise, accurate spatial data are contributing to a revolution in somefields of social science. improved access to such data, combined with improved methods of analysis, is making possible deeper understanding of therelationships between people and their physical and social environments.researchers are no longer limited to analyzing data provided by researchparticipants about their personal characteristics and their views of theworld; rather, it has become possible to link personal information to theexact locations of homes, workplaces, daily activities, and characteristics ofthe environment (e.g., water supplies). those links allow researchers tounderstand much more about individual behavior and social interactionsthan previously, just as linking biomedical data (on genes, proteins, bloodchemistry) to social data has helped researchers understand the progress ofillness and health in relation to aspects of peopleõs behavior. the potentialfor improved understanding of human activities at the individual, group,and higher levels by incorporating spatial information is only beginning tobe unlocked.yet even as researchers are learning from new opportunities offered byprecise spatial information, these data raise new challenges because theyallow research participants to be identified and therefore threaten the promise of confidentiality made when collecting the social data to which spatialdata are linked. although the difficulties of ensuring access to data whilepreserving confidentiality have been addressed by previous national research council reports (1993, 2000, 2003, 2005a), those did not considerin detail the risks posed by data that link the information in social scienceputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.8putting people on the mapresearch with spatial locations. this report directly addresses the tradeoffsbetween providing greater access to data and protecting research participants from breaches of confidentiality in the context of the unique capacityof spatial data to lead to the identification of individuals.the new world of locational datathe development of new data, approaches, spatial analysis tools,and data collection methods over the past several decades has revolutionized how researchers approach many questions. the availability of highresolution satellite images of earth, collected repeatedly over time, and ofsoftware for converting those images into digital information about specific locations, has made new methods of analysis possible. along withmore and improved satellite images, there are aerial images, global positioning systems (gps) and other types of sensorsñespecially radiofrequency identification (rfid) tags that can be used to track peopleworldwideñthat allow the possibility of ubiquitous tracking of individuals and groups. the same technologies also permit enhanced research aboutbusiness enterprises, for example, by providing tracking information forcommercial vehicles or shipments of goods.with the advent of gps, the goal of realtime, continuous global coverage with an accuracy finer than 1 meter has been achieved, though somecaveats, such as difficulty with indoor coverage, apply. triangulation basedon cellular telephone signal strength can be used to establish location on theorder of 100 meters in many locations, and researchers are now developingtechniques for mapping mobile locations at much higher resolutions(borriello et al., 2005). satellite remote sensing instruments have improvedby more than an order of magnitude during the past two decades in severaldimensions of resolution. commercial remote sensing firms provide datawith a submeter ground resolution. with the increasing availability ofhyperspectral sensor systems (those that sense in hundreds of discrete spectral bands along the electromagnetic spectrum), the amount of geographicinformation being collected from satellites has increased at a staggeringpace.terrestrial sensing systems are also increasing in quantity and capability. lowcost solidstate imagers with gps control are now widely deployedby private companies and scientific investigators. in addition, fixed sensorarrays (e.g., closed circuit television) are now used routinely in many locations to provide continuous coverage of events in their field of view. ascomputers continue to decrease in size and power consumption while alsoincreasing in computing and storage capacity, inexpensive in situ sensornetworks are able to record information that is transmitted over peertopeer networks and other types of radio communication technologies (culler,putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.linked socialspatial data9estrin and srivastava, 2004; martinez, hart, and ong, 2004). these devices are now rather primitive, often sensing single types of informationsuch as temperature or pressure, but their capabilities are increasing rapidly. moreover, their space requirements are decreasing; some researchersnow describe nanoscale computing and sensing devices (geer, 2006).these emerging technologies are being integrated with other developing streams of technologyñsuch as rfid tags (want, 2006) and wearablecomputers (smailagic and siewiorek, 2002)ñthat are location and contextaware. indeed, the ubiquity of these devices has caused some to assert thattraditional sensing and processing systems will, in essence, disappear (streitzand nixon, 2005; weiser, 1991). these technologies are creating significant concerns about threats to privacy, although few, if any, of these concerns relate to research uses of the technologies. nevertheless, emergingtechnological capabilities are an important part of the context for the research use of locational data.as these new tools and methods have become more widely available,researchers have begun to pursue a variety of studies that were previouslydifficult to accomplish. for example, analysis of health services once focused on access as a function of age, sex, race, income, occupation, education, and employment. it is now possible to examine how access and itseffects on health are influenced by distances from home and work to healthcare providers, as well as the quality of the available transportation routesand modes (williams, 1983; entwisle et al., 1997; parker, 1998; kwan,2003; balk et al., 2004). improved understanding of how these spatialphenomena interact with social ones can give a much clearer picture of thenature of access to health care than was previously possible.critical to research linking social and spatial data are the developmentand use of geographical information systems (gis) that make it possible totie data from different sources to points on the surface of the earth. thisconnection has great importance because geographic coordinates are aunique and unchanging identification system. with gis, data collected fromparticipants in a social survey can be linked to the location of the respondentsõ residences, workplaces, or land holdings and thus can be analyzed inconnection with data from other sources, such as satellite observations oradministrative records that are tied to the same physical location. such datalinkage can reveal more information about research participants than canbe known from either source alone. such revelations can increase the fundof human knowledge, but they can also be seen by the individuals whosedata are linked as an invasion of privacy or a violation of a pledge ofconfidentiality.increasingly sophisticated tools for spatial analysis involving, but going far beyond, the simple digitized maps of the early geographical information systems have also contributed to this revolution. not only hasputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.10putting people on the mapcommercial software made spatial data processing, visualization, and integration relatively accessible, but several packages (including freeware; e.g.,anselin, 2005; anselin et al., 2006; bivand, 2006; also see http://www.rproject.org/) also make multivariate spatial regression analysis mucheasier (e.g., fotheringham et al., 2002). moreover, standard statisticalsoftware packages, such as stata and matlab, now have much greaterfunctionality to accommodate spatial analytic models, and sas (anothersoftware package) and stata have increased flexibility to accommodatecomplex design effects often associated with spatially linked data.scope of workin response to such challenges of providing wider access to data usedfor socialspatial analysis while maintaining confidentiality, the sponsors ofthis study asked the national academies to address the scientific value oflinking remotely sensed and òselfidentifyingó social science data that areoften collected in social surveys, that is, data that allow specific individualsand their attributes to be identified. the academies were further asked todiscuss and evaluate tradeoffs involving data accessibility, confidentiality,and data quality; consider the legal issues raised by releasing remotelysensed data in forms linked to selfidentifying data; assess the costs andbenefits of different methods for addressing confidentiality in the dissemination of such data; and suggest appropriate models for addressing theissues raised by the combined needs for confidentiality and data access.in carrying out our study, it became clear that limiting the study toremotely sensed data unnecessarily restricted the problem domain. whensocial science research data are linked with spatially precise and accurateinformation, it does not matter in terms of confidentiality issues whetherthe geospatial locations are derived from remotely sensed imagery or fromother means of determining location, such as gps devices or addressmatching using gis technology. the issues raised by linking remotelysensed information are a special case within the larger category of spatiallyprecise and accurate information. for that reason, the committee considered as part of its mandate all forms of spatial information. we alsoconsidered all forms of data collected from research participants that mightallow them to be identified, including personal information about individuals, which may or may not be sensitive if revealed to others, andinformation about specific businesses enterprises. for purposes of simplicity we call all this personal and enterprise information used for the research considered here òsocial data,ó and their merger with spatial information òsocialspatial data.óputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.linked socialspatial data11this report focuses mainly on microdata, specifically, informationabout individuals, households, or businesses that participate in researchstudies or supply data for administrative records that have the potential tobe shared with researchers outside the original group that produced thedata. this focus is the result of the fact that such individual, household, orenterpriselevel data are easily associated with precise locations. microdataare especially important because spatial data can compromise confidentiality both by identifying respondents directly and by providing sensitive information that creates risk of harm if linked to identifying data. in addition,spatially precise information may sometimes be associated with small aggregates of individuals or businesses; and care is always needed when sharing data that have exact locations, for example, a cluster of persons orfamilies living near each other.this report provides guidance to agencies that sponsor data collectionand research, to academic and nonacademic institutions and their institutional review boards (irbs), to researchers who are collecting data, toinstitutions and individuals involved in the research enterprise (such asfirms that contract to conduct surveys), and to those organizations chargedwith the longterm stewardship of data. it discusses the challenges they facein preserving confidentiality for linked social and spatial data, as well asways that they can simultaneously honor their commitment to share theirwealth of data and their commitment to preserve participant confidentiality. although all these individuals and organizations involved in the research enterprise have somewhat different roles to play and somewhatdifferent interests and concerns, we refer to them throughout this report asdata stewards. this focus on the responsibilities of those who share data foranalysis does not absolve others who have responsibility for the collectedinformation from thinking about the risks associated with spatially explicitdata. the report therefore also speaks to those who use linked socialspatialdata, including researchers who analyze the data and editors who publishmaps or other spatially explicit information that may reveal informationthat is problematic from a privacy perspective (e.g., monmonnier, 2002;armstrong and ruggles, 2005; rushton et al., 2006).this study follows and builds on a series of previous national researchcouncil reports that address closely related issues, including: issues of dataaccess (1985); the challenges of protecting privacy and reducing disclosurerisk while maximizing access to quality, detailed data for informed analyses(1993, 2000, 2003, 2004b); and ethical considerations in using microleveldata, including linked data (2005a). the conclusions and recommendationsof several of these earlier studies inform this report. these earlier reportsand other studies (e.g., national research council, 1998; jabine 1993;melichar et al., 2002), have generally developed two themes, one emphasizing the need for datañespecially microdatañto be shared among researchputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.12putting people on the mapers, and the other the need to protect research participants. while the themeof expanding access to data has included data produced by both individualresearchers and government agencies, it has generally emphasized the latter.in the closely related area of environmental data, the national researchcouncil (2001) emphasizes that publicly funded data are a public good andthat the public is entitled to full and open access.the consensus of this work is that secondary use of data for replicationand new research is valuable and that both privately and publicly produceddata should be shared. the most recent report on the subject (nationalresearch council, 2005a) presents a concise set of recommendations thatencourage increased access to publicly produced data. at the same time,these reports and studies have also insisted on the protection of researchparticipants, mostly in the broader context of protecting all human researchsubjects.this report supports the conclusions of the prior work while exploringnew ground. none of the earlier reports considered the potential forbreaches of confidentiality posed by the increase in research using linkedsocialspatial data. the analyses and recommendations included in thisreport strive to expand the field to the new world of locational data.the concerns addressed in this report are raised in the context of abroader recognition that vast amounts of data are available about mostresidents of the united states, that these data have been collected andcollated without the explicit permission of their subjects, and that invasionsof privacy take place frequently (oõharrow 2005; dobson and fisher 2003;goss 1995; fisher and dobson 2003; sui 2005; electronic privacy information center [http://www.epic.org/pivacy/census], 2003). huge commercialdatabases of financial transactions, court records, telephone records, healthinformation, and other personal information have been established, in manycases without any meaningful request to the relevant individuals for releaseof that information. these databases are often linked and the results madeavailable for a fee to purchasers in a system that has greatly diminishedindividualsõ and businessesõ control over information about themselves.these invasions or perceived invasions of privacy, however, are not a subject of this report. all datasets that include personal information, includingthose created for commercial as well as research purposes, whether or notthey have spatial information and those that do not, are in need of comprehensive care to prevent breaches of confidentiality and invasions of privacy.neither this report nor earlier reports deal with the kinds of informationtechnology security required to prevent breaches or invasions, in the case ofthis report because there is nothing special for spatial data about the needfor that security.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.linked socialspatial data13privacy, confidentiality, identification, and harmto understand the dimensions of the confidentiality problem, it is important first to distinguish the concepts of privacy, confidentiality, identification, and harm (see box 11). privacy concerns the ability of individualsto control personal information that is not knowable from their publicpresentations of themselves (see appendix a for a more detailed discussionof privacy and u.s. privacy law). when someone willingly provides information about himself or herself, it is not an invasion of privacy, especiallyif the person has been informed that it is acceptable to terminate the disclosure at any time. an invasion of privacy occurs when an agent obtains suchinformation about a person without that personõs agreement. an invasionof privacy is especially egregious when the person does not want the agentto have the information. an example is the acquisition and sale of themobile telephone records of individuals without their permission (newyork times, 2006).confidentiality involves a promise given by an agentña researcher inthe cases of interest in this reportñin exchange for information. before aresearch activity begins, the researcher explains the purposes of the project,describes the benefits and harms that may affect the research participantand society more broadly, and obtains the consent of the participant tobox 11brief definitions of some key termsprivacy concerns the ability of individuals to control personal information this is notknowable from their public presentations of themselves. an invasion of privacyoccurs when an agent obtains such information about a person without that personõs agreement.confidentiality in the research context involves an agreement in which a researchparticipant makes personal information available to a researcher in an exchange fora promise to use that information only for specified purposes and not to reveal theparticipantõs identity or any identifiable information to unauthorized third parties.identification of an individual in a database occurs when a third party learns theidentity of the person whose attributes are described there. identification disclosure risk is the likelihood of identification.harm is a negative consequence that affects a research participant because of abreach of confidentiality.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.14putting people on the mapcontinue. this process is called òinformed consentó (see national researchcouncil, 1993). the researcher then collects the informationñthrough interview, behavioral observation, physical examination, collection of biological sample specimens, or requests for the information from a thirdparty, such as a hospital or a government agency. in exchange, the researcher promises to use that information only for specified purposes (oftenlimited to statistical analysis) and not to reveal the participantõs identity orany identifiable information to unauthorized third parties. if promises ofconfidentiality are kept, a participantõs privacy is protected in relation tothe information given to the researchers. in academic and other researchorganizations, the process of obtaining informed consent and making confidentiality promises is part of normal research protocol: institutional review boards have guidelines that require agreements and protection ofconfidentiality and the ethical standards of research communities providefurther support for confidentiality.identification is a key element in confidentiality promises. confidentiality means that when researchers release any informationñanalyses, descriptions of the project, or databases that might be used by third partiesñthey promise that the identity of the participants will not be publiclyrevealed and cannot be inferred. identification of an individual in a database occurs when a third party learns the identity of the person whoseattributes are described there. identification obviously increases the risk ofbreaches of confidentiality. identification disclosure risk is sometimes quantified in terms of the likelihood of identification. in the context of thisstudy, precise spatial information increases the risk of disclosure and thusthe likelihood of identification.it is important to note that it is not so much the information that isbeing protected, but the link of the information to the individual. forexample, it is acceptable to describe a personõs survey answers or characteristics so long as the identity of the participant is not revealed. the dangerinherent in a breach of confidentiality is not only that private informationabout an individual might be revealed, but also that the successful conductof research requires that there be no breaches of confidentiality: any suchbreach may significantly endanger future research by making potential research participants wary of sharing personal information. including spatialdata in a dataset with social data greatly increases the possibility of identification while at the same time being necessary for certain kinds of analysis.harm is a negative consequence that affects a survey respondent orother research participant, in the instances of interest in this study, becauseof a breach of confidentiality. social science research can cause variouskinds of harm (for example, legal, reputational, financial, or psychological)because information is revealed about a person that she or he does not wishothers to know, such as financial liabilities or a criminal record. in excepputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.linked socialspatial data15tional cases, identification of a participant in social science research couldput the person at risk of physical harm from a third party. in linking socialand spatial data, the need to prevent breaches of confidentiality remainsserious, even if no discernable harm is done to respondents, because evenapparently harmless breaches violate the expectations of a trusting relationship and can also damage the reputation of the research enterprise.1thus, the challenge to the research community is to preserve confidentiality (and also to protect private information to the extent possible). thismeans that research participants must be protected from identification especially, but not only, when identification can harm them. though thechance of a confidentiality breach is never zero, the risk of disclosure depends on the nature of the data. the separate risk of harm also depends onthe nature of the data. in some instances, confidentiality is difficult toprotect but the risk of harm to respondents is low (e.g., when the datainclude only information that is publicly available); in others, confidentiality may be easy to protect (e.g., because the data include few characteristicsthat might be used to identify someone), but the risk of harm may be highif identification occurs (because some of the recorded characteristics could,if known, endanger the wellbeing of the respondent). when preciselocational data are included in or can be determined from a dataset, researchers face tougher challenges of protecting confidentiality and preventing identification.opportunities and challenges for researchersin response to the growing opportunities for knowledge about relationships between social and spatial phenomena on the part of researchers andpolicy makers, research fundersñespecially the national institute of childhealth and human development[national institutes of health], the national science foundation, and the national aeronautics and space administrationñthe sponsors of this study, have contributed substantial resourcesto the creation of linked socialspatial datasets (see box 12). such datasetscover parts of the united states (arizona state university, 2006; universityof michigan, 2005a), brazil (moran, brondizio, and vanwey, 2005; indiana university, 2006), ecuador (university of north carolina, 2005), thailand (walsh et al., 2005; university of north carolina, 2006), nepal (university of michigan, 2005b), and other countries. one outstanding exampleis research on the relationship among population, land use, and environment in the nang rang district of thailand, described in figure 11.1for more on the distinction between risk and harm, see the risk and harm report of thesocial and behavioral sciences working group on human research protections (http://www.aera.net/aera.old/humansubjects/riskharm.pdf, accessed january 2007).putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.16putting people on the mapbox 12an example of socialspatial dataa good example of a socialspatial dataset comes from the nang rong study,begun in 1984. this project covers 51 villages in nang rong district, northeastthailand, an agricultural setting in the countryõs poorest region. the researcherswho work on this project have collected data from all households in each village,including precise locations of dwelling units and agricultural parcels. social network data link households along lines of kinship as well as economic assistanceñwho helps whom with agricultural tasks. the project team also follows migrants outto their most common destinations, including bangkok and the countryõs easternfigure 11confidentiality in nang rong, thailand. the image is an aerial photo with simulated households identified and linked to their farm plots. at this resolution, it is impossible toprevent identification of households.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.linked socialspatial data17figure 12 confidentiality issues in bangkok, thailand. the background is an ikonos satellite image, with simulated household data overlaid. the figure shows that migrants from thesame village cluster at their destination, forming a village enclave (upper insert) or cluster withmigrants from other nang rong villages forming a nang rong cluster (lower insert). releasedin this fashion, the data can give away the identity of the migrants (unless circles are enlargedto cover more area in which case the quality of the data is degraded).seaboardña governmentsponsored development zone. the projectõs social datahave been merged with the locations of homes, fields, and migration destinations,and then linked to a variety of other types of geographic information includingsatellite data, aerial photographs, elevation data, road networks, and hydrologicalfeatures. these linked data have been used for many types of analysis (see university of north carolina, 2006). figures 11 and 12 are simulated data of the typecreated for the nang rong project. they show just how clearly individuals andhouseholds can be located in these data and therefore how easy it would be foranyone who has the spatial information for actual respondents to identify them.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.18putting people on the maplinking social data that are collected from individuals and householdswith spatial data about them, collected in place or by remote sensing,creates potential for improved understanding of a variety of social phenomena (see butz and torrey, 2006). much has already been learned about theeffects of context on social outcomes by analyzing social data at relativelyimprecise geographic levels, such as census blocks and tracts or other primary sampling units (e.g., gephart,1997; smith and waitzman 1997; leclere et al. 1998; ross et al., 2000; sampson et al., 2002). advances ingeographic information science and in remote sensing make it possible toconnect individuals and households to their geographic and biophysicalenvironmentsñand changes in themñat much finer scales.because concerns about confidentiality have limited the use of linkedsocial and finescale spatial data, the potential for advancing knowledgethrough such linkages is only beginning to be explored. there are someearly hints of exciting work, and we can speculate about future progress.some of the progress involves studies of human interactions with the natural environment, a field that has been supported by the agencies that haverequested the present study (e.g., national research council, 1998, 2005b).researchers have combined household surveys with remotely sensed dataon changes in land use to gain deeper understanding of the processes driving those changes and their economic consequences (e.g., conversion ofagricultural land to urban uses, seto, 2005; changes in cropping patterns,walsh et al., 2005; changes in forest cover, foster, 2005; moran et al.,2005).another area of research and opportunity involves global populationpatterns. global gridded population data demonstrates that people tend tolive at low elevation and near sea coasts and rivers (small and cohen, 2004;small and nicholls, 2003) and that people living in coastal regions aredisproportionately residents of urban areas. moreover, coastal regions,whether urban or rural, are much more densely populated than other typesof ecosystems (mcgranahan et al., 2005). about one of every ten people onearth lives in a low elevation coastal zone at risk of storm surges associatedwith expected increases in sea levels (mcgranahan et al., 2006)interesting examples come from health research. for example, the availability of exercise options near where people live, including features assimple as a sidewalk, affects peopleõs health and physical fitness (gordonlarsen et al., 2006). other research shows how migration responds to localenvironmental conditions, with recurrent droughts perhaps providing thebest example (deane and gutmann, 2003; gutmann et al., 2006). thereare opportunities for improving estimates of vulnerability to famine bycombining data on food availability with data on household coping capabilities and strategies (hutchinson, 1998). in one example, combining demographic survey data with environmental variables showed that houseputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.linked socialspatial data19hold factors (composition, size, assets), maternal education, and soil fertility were all significant determinants of child hunger in africa (balk et al,2005). the future of health research offers myriad opportunities. for example, environmental factors (e.g., air and water quality) have been linkedto peopleõs health: as social and biophysical datasets become better integrated at finer scales, it will be possible to examine a variety of environmental factors and link them to peopleõs health with greater precision and sodevelop better understanding of those environmental factors.another example of the future of research concerns understandingtravel behavior by linking personal data with finescale spatial informationon actual travel patterns. researchers could evaluate simultaneously theindividual attributes of the research participants, the environmental attributes of the places they live, work, or otherwise frequent, and the detailed travel patterns that lead from one to another. beyond knowingwhether a route to school has a sidewalk and whether a child walks toschool, one can ask whether that route also has a candy store or a community exercise facility and whether the actual trip to school allows the childto stop there. yet combining all that informationñlocation of home andschool, route taken, and attributes of child and familyñand publishing itwould reveal the actual identities of research participants and so breach thepromise of confidentiality made when data were collected from them.as research combining spatial data with social data collected fromindividuals has expanded, both researchers and their sponsors have beenforced to confront questions about the release of the massive amounts ofdata they have accumulated. the opportunities for research offer the potential for great benefits, but there is also some risk of harm. moreover, bothprofessional ethics and agency policies require that researchers share theirdata with others.2 at the same time, researchers who collect social andbehavioral data customarily promise the participants who provide the dataconfidentiality, and the same professional ethics and agency policies thatrequire data sharing also require that pledges of confidentiality be honored.these requirements combine to produce the central dilemma that this report addresses.2see, for example, the codes of ethics of the urban and regional information systemsassociation (http://www.urisa.org/about/ethics); the american society of photogrammetry andremote sensing (http://www.asprs.org/membership/certification/appendixa.html); the american sociological association (http://www.asanet.org/galleries/defaultfile/code%20of%20ethics.pdf) and the association of american geographers (http://www.aag.org/publications/ethicsstatement.html). also see, for example, the policies of the national institutes of health(http://grants1.nih.gov/grants/guide/noticefiles/notod03032.html) and the national science foundation (article 36 at http://nsf.gov/pubs/2001/gc101/gc101rev1.pdf). [all abovecited web pages accessed january 2007.]putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.20putting people on the mapin order to understand the challenges and opportunities, consider arecent finding and two hypothetical examples. the finding concerns therapidly growing use of maps in medical research. brownstein and colleagues (2006) identified 19 articles in five major medical journals in 2004and 2005 that plotted the addresses of patients as dots or symbols on maps.to determine how easy it might be to identify individual patients from thesemaps, they created a simulated map of 550 geographically coded addressesof patients in boston, using the minimum figure resolution required forpublication in the new england journal of medicine, and attempted toreidentify the addresses using standard gis technology. they precisely identified 79 percent of the addresses from the map, and came within 14 metersof precision with the rest. the authorsõ point was that improved ability tovisualize disease patterns in space comes at a cost to patientsõ privacy.the first hypothetical example concerns a researcher who (expandingon the insights in gordonlarsen et al., 2006) undertakes a project thatincludes a survey of adolescent behavior, including exercise and eatinghabits, in order to understand the causes of obesity in the teenage population. in addition to asking about how the research subjects get to schooland the availability of places to walk and exercise, the researcher takes gpsreadings of their homes and schools, and asks them to wear a device thattracks their location during waking hours for 1 week. because of the complexity of the problem, the researcher asks about drug and alcohol consumption in addition to food consumption. finally, the information obtained from the participants is merged with detailed maps of thecommunities in which they live in order to know the location of specifickinds of places and the routes between them. in the second example, aresearcher interested in the effects of family size on land use and resourceconsumption in south asia conducts a survey that asks each family abouttheir reproductive and health history, as well as detailed questions aboutthe ways that they obtain food and fuel. then, walking in the communitywith family representatives, the researcher takes gps readings of the locations of the familiesõ farm plots and the areas where they gather wood forheat and cooking. finally, the researcher spends a day with the women andchildren in the families as they go about gathering fuelwood, wearing agpsbased tracking device so that the location and timing of their activitiescan be recorded. some of these locations are outside the sanctioned areas inwhich the family is legally permitted to gather fuel.in both hypothetical examples, the linking of the social data gatheredfrom the participants and the spatial data will permit identification of someor all of the participants. yet the researchers have made promises of confidentiality, which state that the data will only be analyzed by qualified researchers and that the participants will never be identified in any publicationor presentation. yet both the sponsor of the research and the research ethicsrequire that the researchers make their data available to other researchers forputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.linked socialspatial data21replication and for new research. in both surveys, there are questions aboutactivities that are outside officially sanctioned behavior, which if linked to anindividual respondent might cause them harm if revealed.in both hypothetical examples, the locational information is essentialto the value of the data, so the researchers may not simply discard ormodify data items that could lead to identification. rather, they face achoice between honoring the requirement to share data and the commitment to protect confidentiality, or somehow finding a way to do both.sharing data is not by itself automatically harmful to research participants.responsible researchers regularly analyze data that include confidentialinformation, and do so without compromising the promises that were madewhen the data were collected. the challenge arises when the data are sharedwith secondary researchers, who must either guarantee that they will adhere to the promise of confidentiality made by the original researcher, orreceive data that are stripped of useful identifying information. the goal isto make sure that responsible secondary users do not reveal respondentidentities, and do not share the data to others who might do so. butlocational information may also make it possible for a secondary researcherto identify research participants by linking to data from other sources,without requesting permission for that information.some recent research suggests that it is possible to gauge social, demographic, and economic characteristics from remote sensing data alone(cowen and jensen 1998; cowen et al 1993; weeks, larson, and fugate,2005), but this suggestive idea is unproven and would require considerablesupporting research to overcome the challenge that the data are of limitedvalue and have a high likelihood of error. identifying social attributes fromearthobserving satellites is not easy, but satellite data, particularly fromhighresolution satellites (launched since the late 1990s) make the identification of particular anthropogenic featuresñroads, buildings, infrastructure, vehiclesñmuch easier than previously.3 other forms of spatial data,such as aerial photographs, especially historic ones, are much less likely tobe accurately georeferenced (if georeferenced at all) for finescale matchingwith other attributes, but may nevertheless foster identification.spatial data create the possibility that confidentiality may be compromised indirectly by secondary data users in ways that identify individualparticipants.4 those ways relate to the spatial context of observations andthe spatial covariance that exists among variables. spatial covariance refers3a review of satellites, their spatial and temporal resolutions and coverage, and detectablefeatures can be found at http//sedac.ciesin.columbia.edu/tg/guideframe.jsp?rd=rs&ds=1 [accessed january 2007].4confidentiality issues rarely, if ever, arise for spatial data when unlinked to social data.much spatial data are in the public domain, and the supreme court has ruled that privacyrights do not exist for observations made from publicly navigable airspace (see appendix a).putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.22putting people on the mapto the tendency of the magnitude of variables to be arranged systematicallyacross space. for example, the locations of high values of one variable areoften associated systematically with high values (or with low values) ofanother variable. thus, if the spatial covariance structure between variablesis known, and the value for one variable is also known, an estimate of theother variable can be made, along with an estimate of error. this knowledge can be applied in several ways, such as interpolation and contextualanalyses associated with process models.interpolation methods can be placed into two classes: exact and approximate (lam, 1983). exact methods enforce the condition that the interpolated surface will pass through the observations. approximate methodsuse the data points to fit a surface that may pass above or below the actualobservations. kriging is a widely used exact method in which the linkbetween location (x,y) and value of the observation (z) is preserved. kriging,therefore, threatens confidentiality because it exactly reproduces data values for each sample point: if the spatial location of sample data points isknown, the linked values of other variables can be revealed (cox, 2004).kriging also provides the analyst with an assessment of the error at eachpoint.contextual data are sometimes used to facilitate analysis when detailedexact data are either too sensitive for release or unavailable. however,contextual data can themselves be identifying; for example, a sequence ofdaily air quality monitoring readings from the nearest monitor provide acomplete òsignatureó for each monitor, revealing fairly precise locationsfor individuals whose data are linked to such air quality readings. knowledge about context can also be used to infer locations when deterministicspatial process models are used. studies of the human effects of air pollution may use such models to study atmospheric dispersion of harmful substances. given a model and a set of input parameters, such as wind speed,direction, temperature, and humidity, results are reported in the form of aplume òfootprintó of dispersion (see, e.g., chakraborty and armstrong,2001). if the location of a pollution source is known, along with the modeland its parameters, a result from the model can be used to reveal thelocations of participants in the dataset, who can then be identified, alongwith the confidential information they provided for the dataset.data quality, access, and confidentiality: tradeoffsmore precise and accurate data are generally more useful for analysis.for analysis of social and spatial relationships, accuracy and precision inthe spatial data are often crucial. however, having such data increases thechances that research participants can be identified, thus breaking researchersõ promises of confidentiality. in general, as data with detailed locationalputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.linked socialspatial data23information about participants becomes more widely accessible, the risk ofa confidentiality breach increases. the problem of tradeoffs involving dataquality, access, and confidentiality is becoming more urgent because of tworecent trends. one is increased demands from research funders, particularlyfederal agencies, for improving data access so as to increase the scientificbenefit derived from a relatively fixed investment in data collection. theother is the continuing improvement in computer technologies generally,and especially techniques for mining datasetsñtechniques that can be usednot only to provide more detailed understanding of social phenomena, butalso to identify research participants despite researchersõ promises of confidentiality. the current context and a consideration of the ethical, legal, andstatistical issues are discussed in chapter 2.this report also addresses ways to solve the problem of increasing thevalue of linked socialspatial data, both to the original researchers and topotential secondary users, while at the same time keeping promises ofconfidentiality to research participants. chapter 3 examines several methods available for dealing with the problem. they can be roughly classifiedas technical and institutional, and each has significant limitations.both technical and institutional approaches limit the amount of dataavailable, the usefulness of the data for research, or the ways that researchers can access those data in return for increased protection of pledges ofconfidentiality. most researchers believe that those restrictions have had anegative effect on the amount and value of research that has been done, butthere is relatively little solid evidence about the quantity of research notperformed for this cause. it is not surprising that such negative evidencedoes not exist, and its absence does not prevent us from recommendingimprovements. at the workshop organized by the panel we heard testimonyfrom users of data enclaves about the ways that the arduous rules of thoseinstitutions limited research. in addition, there was interesting testimonysubmitted at the time of the preparation of the 2000 u.s. census thatdocumented research that could not be conducted because of variables andvalues that the census bureau proposed to remove from the public usemicrodata samples in order to reduce the risk of identification (minnesotapopulation center, 2000). the lack of readily accessible data about anything smaller than quite large areas does limit research. research is notbeing done on certain topics that require knowledge of locations becausethe data are not available or access is difficult.some of the technical approaches involve changing data in variousways to protect confidentiality. one is to mask locations by shifting themrandomly. this approach helps protect against identification, but makesthe data less useful for understanding the spatial phenomena that justifiedcreating the linked dataset in the first placeñthe significance of location ofplaces (such as home and work) for the social conditions of interest. reputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.24putting people on the mapsearchers and data stewards need to be sensitive to linkages of data that aremasked in order to avoid conclusions based on an overestimation of theaccuracy of data that have been changed in some way.institutional approaches include restrictions in access to the data. thenotion of tiers of access to data means that there is a gradient of accessibility: data that create the greatest risk of identification are least available andthose with the lowest risk are the most available. at the same time, manyanalyses will only be possible with data that have the highest risk of disclosure and harm and therefore will be the least available.the seriousness of these tradeoffs, in terms of the likelihood of identification or disclosure and of the potential for harm to research participants,depends on attributes of the research population, the information in thedataset, the contexts of inadvertent disclosure, and the motives of secondary users who may act as òdata spiesó (armstrong et al., 1999) in relationto the dataset, as well as on the strategy used to protect confidentiality.most of these factors apply regardless of whether the data include spatialinformation, but the availability of spatial characteristics of the researchpopulation can affect the seriousness of the tradeoffs. for example, a highlyclustered sample of schoolage students (with school as the primary sampling unit and with geographic identifiers) is more identifiable and moreopen to risk of harm than a nationally scattered sample of adults, especiallyif the data collected include information about social networks.5 manynonspatial factors can also affect disclosure risk. for example, questionsabout individualsõ attitudes (what do you think about òxó) are less likely toincrease disclosure risk than questions about easily known characteristicsof family or occupation (age, number of children, occupation, distance toplace of employment).at the same time, some questions, if identification occurs, are morelikely to be harmful than others, with a question about drug use more likelyto cause harm than a question about retirement planning. finally, the seriousness of the tradeoffs may depend on the identities and motives of secondary users. at present, little is known about such users, what they mightwant, the conditions under which they might seek what they want from aconfidential dataset, the extent to which what they want would lead toidentification of research participants and their attributes, or the techniquesthat they might use (see, e.g., duncan and lambert, 1986b; armstrong etal., 1999).it is possible for the linkage of social and spatial data to create signifi5because social networks locate individuals within a social space, releasing social networkdata involve analogous risks to the risks related to spatial network data discussed in thisreport. for discussions of ethical issues in social network research, see borgatti and molina(2003), breiger (2005), kadushin (2005), and klovdahl (2005).putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.linked socialspatial data25cant risks of harm to research participants. for example, it has been claimedthat the nazis used maps and tabulations of òjews and mixed breedsó toround up people for concentration camps (cox, 1996) and that the u.s.government used special tabulations of 1940 census data to locate japaneseamericans for internment (anderson and fienberg, 1997). improvementsin the precision of spatial data and advances in geocoding are likely tolower the costs of identifying people for such purposes. we note, however,that risks of identification and harm by governments or other organizationswith strong capabilities for tracking people and mining datasets exist evenif social data are not being collected under promises of confidentiality. thekey issue for this study concerns the incremental risks of linking confidential social data to precise spatial information about research participants.among secondary users who might seek information about particularindividuals, those who know that another person is likely or certain to beincluded in a database (e.g., a parent knowing that a child was studied orone spouse knowing about another) have a much easier time identifying arespondent than someone who starts without that knowledge. experts suspect that although those who know which participant they are looking formay be interested in harming that individual, they are unlikely to be interested in harming the entire class of participants or the research processitself. the benefitrisk tradeoffs created by socialspatial is a major challenge for research policy.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.262legal, ethical, and statistical issues inprotecting confidentialitypast and current practicethere is a long tradition in government agencies and research institutions of maintaining the confidentiality of human research participants(e.g., de wolf, 2003; national research council, 1993, 2000, 2005a).most u.s. research organizations, whether in universities, commercial firms,or government agencies, have internal safeguards to help guide data collectors and data users in ethical and legal research practices. some also haveguidelines for the organizations responsible for preserving and disseminating data, data tables, or other compilations.government data stewardship agencies use a suite of tools to constructpublicuse datasets (micro and aggregates) and are guided by federal standards (doyle et al., 2002; confidentiality and data access committee, 2000,2002). for example, current practices that guide the u.s. census bureaurequire that geographic regions must contain at least 100,000 persons formicro data about them to be released (national center for health statisticsand centers for disease control and prevention, 2003). most federal agencies that produce data for public use maintain disclosure review boards thatare charged with the task of ensuring that the data made available to thepublic have minimal risk of identification and disclosure. federal guidelinesfor data collected under the health insurance portability and accountabilityact of 1996 (hipaa) are less stringent: they prohibit release of data forregions with fewer than 20,000 persons. table 21 shows the approaches ofvarious federal agencies that regularly collect social data to maintaining confidentiality, including cell size restrictions and various procedural methods.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.legal, ethical, and statistical issues27fewer guidelines exist for nongovernmental data stewardship organizations. many large organizations have their own internal standards andprocedures for ensuring that confidentiality is not breached. those procedures are designed to ensure that staff members are well trained to avoiddisclosure risk and that data in their possession are subject to appropriatehandling at every stage in the research, preservation, and disseminationcycle. the interuniversity consortium for political and social research(icpsr) requires staff to certify annually that they will preserve confidentiality. it also has a continual process of reviewing and enhancing the trainingthat its staff receives. moreover, icpsr requires that all data it acquires besubject to a careful examination that measures and, if necessary, reducesdisclosure risk. icpsr also stipulates that data that cannot be distributedpublicly over the internet be made available using a restricted approach (seechapter 3). other nongovernmental data stewardship organizations, suchas the roper center (university of connecticut), the odum institute (university of north carolina), the center for international earth science information network (ciesin, at columbia university), and the murray research archive (harvard university), have their own training and disclosureanalysis procedures, which over time have been very effective; there havebeen no publicly acknowledged breaches of confidentiality involving thedata handled by these organizations, and in private discussions with archivemanagers, we have learned of none that led to any known harm to researchparticipants or legal action against data stewards.universities and other organizations that handle social data have guidelines and procedures for collecting and using data that are intended toprotect confidentiality. institutional review boards (irbs) are central inspecifying these rules. they can be effective partners with data stewardshiporganizations in creating approaches that reduce the likelihood of confidentiality breaches. the main activities of irbs in the consideration of researchoccur before the research is conducted, to ensure that it follows ethical andlegal standards. although irbs are mandated to do periodic continuingreview of ongoing research, they generally get involved in any major wayonly reactively, when transgressions occur and are reported. few irbs areactively involved in questions about data sharing over the life of a researchproject, and fewer still have expertise in the new areas of linked socialspatial data discussed in this report.although not all research is explicitly subject to the regulations thatrequire irb review, most academic institutions now require irb review forall human subjects research undertaken by their students, faculty, and staff.in the few cases for which irb review is not required for research that linkslocation to other human characteristics and survey responses, researchersundertaking such studies are still subject to standard codes of researchethics. in addition, many institutions require that their researchers, regardputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.28putting people on the mapless of their funding sources, undergo general human subjects protectiontraining when such issues are pertinent to their work or their supervisoryroles. irbs are also taking a more public role; for example, making resources available for investigators and study subjects.1 educating irbs andtable 21agencyspecific features of data use agreements andlicensesmechanisms for data approval*irbsecurityapprovalinstitutionalpledgesreportagencyrequiredconcurrenceall usersdisclosuresnational center forxxxeducation statisticsnational science foundationxxxdepartment of justicexxxhealth care financingadministrationsocial securityxxxxadministrationhealth care financingadministrationnationalcancer institutebureau of labor statisticsxxnational longitudinalsurvey of youthbureau of labor statisticsxxcensus of fataloccupational injuriesnational institute of childxxxhealth and humandevelopmentnational heart, lung, andxblood institutenational institute of mentalxxhealthnational institute on drugxxabusenational institute on alcoholxabuse and alcoholism*the agreement mechanisms for data use range from those believed to be most stringent(irb approval) on the left to the least stringent (notification of reports) on the right. inpractice, policies for human subjects protection often comprise several mechanisms orfacets of them. irb approval and òinstitutional concurrenceó are similar, though thelatter often encompasses financial and legal requirements of grants not generally coveredby irbs.1for example, see the website for columbia universityõs irb: http://www.columbia.edu/cu/irb/ [accessed april 2006].putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.legal, ethical, and statistical issues29securitysecuritycell sizepriorapprovalnotification ofplaninspectionsrestrictionsreportsreportsxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxnote: security plans may be quite broad, including safeguards on the computing environment as well as the physical security of computers on which confidential data arestored. security inspections are randomly timed inspections to assess compliance of thesecurity plan.source: seastrom (2002:290).having irbs do more to educate investigators may be important to increased awareness of confidentiality issues, but education alone does notaddress two challenges presented by the analysis of linked spatial and socialdata.one of these challenges is that major sources of finegrained spatialdata, such as commercial firms and such government agencies as the national aeronautics and space administration (nasa) and national oceputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.30putting people on the mapanic and atmospheric administration (noaa), do not have the same history and tradition of the protection of human research subjects that arecommon in social science, social policy, and health agencies, particularly inrelation to spatial data. as a result, they may be less sensitive than thenational institutes of health (nih) or the national science foundation(nsf) to the risks to research participants associated with spatial data andidentification. neither nasa nor noaa has largescale grant or researchprograms in the social sciences, where confidentiality issues usually arise.however, nasa and noaa policies do comply with the u.s. privacy actof 1974, and in some research activities that involve human specimens orindividuals (e.g., biomedical research in space flight) or establishments (suchas research on the productivity of fisheries).2 nasa and noaa also provide clear guidance to their investigators on the protection of human subjects, including seeking irb approval, obtaining consent from study subjects, and principal investigator education. for example, nasaõs policydirective on the protection of human research subjects offers useful guidance for producers and users of linked spatialsocial data, although it isclearly targeted at biomedical research associated with space flight.3the difference in traditions between nasa and noaa and other research agencies may be due in part to the fact that spatial data in and ofthemselves are not usually considered private. although aerial photographycan reveal potentially identifiable features of individuals and lead to harm,legal privacy protections do not apply to observations from navigable airspace (see appendix a). thus, agencies have not generally established human subjects protection policies for remotely sensed data. privacy andconfidentiality issues arise with these data mainly when they are linked tosocial data, a kind of linkage that has been regularly done only recently.these linkages, combined with dramatic increases in the resolution of images from earthobserving satellites and airborne cameras in the past decade, now make privacy and confidentiality serious issues for remote dataproviders. thus, it is not surprising that nasa and noaa are absent fromthe list of agencies in table 21 that have been engaged in specifying datause agreements and licensesñanother context in which confidentiality issues may arise. agencies that already have such procedures established forsocial databases may be better prepared to adopt such procedures for spatial data than agencies that do not have established procedures for humansubjects protection.the other challenge is that, absent the availability of other information2for details, see http://www.pifsc.noaa.gov/wpacfin/confident.php [accessed january 2007].3see http://nodis3.gsfc.nasa.gov/npgimg/npd7100008d/npd7100008dmain.pdf [accessed january 2007].putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.legal, ethical, and statistical issues31or expertise, irbs have, for the most part, treated spatially linked or spatially explicit data no differently from other selfidentifying data. there areno current standards or guidelines for methods to perturb or aggregatespatially explicit data other than those that exist for other types of selfidentifying data. current practice primarily includes techniques such asdata aggregation, adding random noise to alter precise locations, and restricting data access. without specialized approaches and specialized knowledge provided to irbs, they can either be overly cautious and preventvaluable data from being made available for secondary use or insufficientlycautious and allow identifiable data to be released. neither option addresses the fundamental issues.the need for effective training in confidentialityrelated research andethics issues goes beyond the irbs and investigators, and extends to datacollectors, stewards, and users. many professional organizations in thesocial sciences have ethics statements and programs (see chapter 1 andappendix b), and these statements generally require that students be trainedexplicitly in ethical research methods. training programs funded by thenih also require ethics components, but it is not at all certain that thecoverage provided or required by these programs goes beyond general ethical issues to deeper consideration of ethics in social science research, letalone in the context of socialspatial linkages.4 professional data collectionand stewardship organizations, as noted above, typically have mandatorystandards and training. nonetheless, there is no evidence that any of theseorganizations are systematically considering the issue of spatial data linkedto survey or other social survey data in their training and certificationprocesses. we offer some recommendations for improving this situation inchapter 4.legal issuesresearchers in college or university settings or supported by federalagencies are subject to the rules of those institutions, in particular, theirfederalwide assurances (fwas) for the protection of human subjects andthe institutional review boards (irbs) designated under their assurances.4for example, the program announcement for national research service award institutional research grants (t32) specifies: although the nih does not establish specific curricula or formal requirements, all programs are encouraged to consider instruction in thefollowing areas: conflict of interest, responsible authorship, policies for handling misconduct, data management, data sharing, and policies regarding the use of human and animalsubjects. within the context of training in scientific integrity, it is also beneficial to discussthe relationship and the specific responsibilities of the institution and the graduate studentsor postdoctorates appointed to the program (see http://grants1.nih.gov/grants/guide/pafiles/pa02109.html [accessed april 2006]).putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.32putting people on the mapalso, researchers may find guidance in the federal statutes and codes thatgovern research confidentiality for various agencies.5 rules may also bedefined legally through employeremployee or sponsorrecipient contracts.obligations to follow irb rules, policies, and procedures may be incorporated in the terms of such contracts in addition to any explicit language thatmay refer to the protection of human subjects.researchers who are not working in a college or university or who arenot supported with federal funds may be bound, from a practical legalperspective, only by the privacy and confidentiality laws that are generallyapplicable in society. such researchers in the united states usually includethose working for private companies or consortia. in an international context, research may be done using human subjects data gathered in nationswhere different legal obligations apply to protecting privacy and confidentiality and where the social, legal, and institutional contexts are quite different. as a general rule, u.s. researchers are obligated to adhere to the laws ofcountries in which the data are collected, as well as those of the unitedstates.the notion of confidentiality is not highly developed in u.s. law.6privacy, in contrast with confidentiality, is partly protected both by tort lawconcepts and by specific legislative protections. appendix a provides adetailed review of u.s. privacy law as it applies to issues of privacy, confidentiality, and harm in relation to human research subjects. the appendixsummarizes when information is sufficiently identifiable so that privacyrules apply, when the collection of personal information does and does notfall under privacy regulations, and what legal rules govern the disclosure ofpersonal information. as appendix a shows, the legal status of confidentiality is less well defined than that of privacy.u.s. law provides little guidance for researchers and the holders ofdatasets except for the rules imposed by universities and research sponsorsregarding methods by which researchers may gain access to enhanced anddetailed social data linked to location data in ways that both meet theirresearch needs and protect the rights of human subjects. neither does current u.s. privacy law significantly proscribe or limit methods that might beused for data access or data mining. the most detailed provisions are in theconfidential information protection and statistical efficiency act of 2002(cipsea).7 this situation makes it possible for researchers and organiza5an illustrative compendium of federal confidentiality statutes and codes can be found athttp://www.hhs.gov/ohrp/nhrpac/documentsnhrpac15.pdf [accessed april 2006].6for some references to federal laws on confidentiality, see http://www.hhs.gov/ohrp/nhrpac/documents/nhrpac15.pdf [accessed january 2007].7egovernment act of 2002, pub. l. 107347, dec. 17, 2002, 116 stat. 2899, 44 u.s.c. ¤3501 note ¤ 502(4).putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.legal, ethical, and statistical issues33tions that are unconstrained by the rules and requirements of universitiesand federal agencies to legally access vast depositories of commercial dataon the everyday happenings, transactions, and movements of individualsand to use increasingly sophisticated data mining technology to conductdetailed analyses on millions of individuals and households without theirknowledge or explicit consent.these privacy issues are not directly relevant to the conduct of socialscience research under conventional guarantees of confidentiality. however, they may become linked in the future, either because researchers maybegin to use these data sources or because privacy concerns raised by usesof large commercial databases may lead to pressures to constrain researchuses of linked social and spatial data. solutions to the tradeoffs among dataquality, access, and confidentiality must be considered in the context of thelegal vagueness surrounding the confidentiality concept and the effects itmay have on individualsõ willingness to provide information to researchersunder promises of confidentiality.ethical issuesthe topics of study, the populations being examined, and the methodor methods involved in an inquiry interact to shape ethical considerationsin the conduct of all research involving human participants (levine andskedsvold, 2006). linked socialspatial research raises many of the typicalissues of sound science and sound ethics, for which the basic ethical principles have been well articulated in the codes of ethics of scientific societies,8 in research and writing on research ethics, in the evolution of the codeof federal regulations for the protection of human subjects (45 cfr 46)and the literature that surrounds it, and in a succession of important reportsand recommendations typically led by the research community (see appendix b). much useful ethical guidance can also be extrapolated from pastnational research council reports (e.g., 1985, 1993, 2004b, 2005a).in addition, as noted above, linked social and spatial data raise particularly challenging ethical issues because the very spatial precision of thesedata is their virtue, and, thus, aggregating or masking spatial identifiers toprotect confidentiality can greatly reduce their scientific value and utility.therefore, if precise spatial coordinates are to be used as research data,primary researchers and data stewards need to address how ethically tostore, use, analyze, and share those data. appendix b provides a detaileddiscussion of ethical issues at each stage of the research process, fromprimary data collection to secondary use.8for example, see those of the american statistical association, at http://www.amstat.org/profession/index.cfm?fuseaction=ethicalstatistics [accessed january 2007].putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.34putting people on the mapthe process of linking microlevel social and spatial data is usuallyconsidered to fall in the domain of human subjects research because itinvolves interaction or intervention with individuals or the use of identifiable private information.9 typically, such research is held to ethical guidelines and review processes associated with irbs at colleges, universities, andother research institutions. this is the case whether or not the research isfunded by one of the federal agencies that are signatories to the federalregulations on human subjects research.10 thus, generic legal and ethicalprinciples for data collection and access apply. also, secondary analysts ofdata, including those engaged in data linking, have the ethical obligation tohonor agreements made to research participants as part of the initial datacollection. however, the practices of irbs for reviewing proposed secondary data analyses vary across institutions, which may require review ofproposals for secondary data analysis or defer authority to thirdparty dataproviders that have protocols for approving use.11 data stewardshipñthepractices of providing or restricting the access of secondary analysts tooriginal or transformed datañentails similar ethical obligations.planning for ethically responsible research is a matter of professionalobligation for researchers and other professionals, covered in part by irbsunder the framework of a national regulatory regime. this regime providesfor a distributed human subjects protection system that allows each irb totailor its work with considerable discretion to meet the needs of researchersand the research communities in which the work is taking place. the linking of social and spatial data raises new and difficult issues for researchersand irbs to consider: because the uses of linked data are to some extentunpredictable, decisions about data access are rarely guided by an explicitset of instructions.the national commission for the protection of human subjects ofbiomedical and behavioral research (1979) concisely conveyed the essential ethical principles for research:9these are the elements of human subject research as defined in the code of federalregulations at 45 cfr 46.102(f).10academic and research institutions typically have in place federally approved federalwide assurances that extend the federal regulations for the protection of human subjects toall human subjects research undertaken at the institution, not just to research funded by the17 agencies that have adopted the federal regulations.11irbs even vary in whether research using publicuse data files is reviewed, althoughincreasingly the use of such data, if not linked to or supplemented by other data, is viewedas exempt once vetted for public use). see http://www.hhs.gov/ohrp/nhrpac/documents/dataltr.pdf for general guidelines and http://info.gradsch.wisc.edu/research/compliance/humansubjects/7.existingdata.htm for a specific example. [web pages accessed january2007].putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.legal, ethical, and statistical issues35beneficenceñmaximizing good outcomes for society, science, and individual research participants while avoiding or minimizing unnecessaryrisk or harm;respect for personsñprotecting the autonomy of research participantsthrough voluntary, informed consent and by assuring privacy and confidentiality); andjusticeñensuring reasonable, carefully considered procedures and afair distribution of costs and benefits.these three principles together provide a framework for both facilitating social and spatial research and doing so in an ethically responsible andsensitive way.for primary researchers, secondary analysts, and data stewards, themajor ethical issues concern the sensitivity of the topics of research; maintaining confidentiality and obtaining informed consent; considerations ofbenefits to society and to research participants; and risk and risk reduction,particularly the obligation to reduce disclosure risk. linking spatial data tosocial data does not alter ethical obligations, but it may pose additionalchallenges.data collectors, stewards, and analysts have a high burden with regardto linked social and spatially precise data to ensure that the probability ofdisclosure approaches zero and that the data are very securely protected.they also need to avoid inadvertent disclosure through the ways findingsare presented, discussed, or displayed. to meet this burden, they need toconsider all available technical methods and data management strategies.we examine these methods and strategies in chapter 3 in relation to theirability to meet the serious challenges of data protection for linked socialspatial data.statistical issuesall policies about access to linked socialspatial data implicitly involvetradeoffs between the costs and benefits of providing some form of accessto the data, or modified versions of the data, by secondary data users. therisk of disclosures of sensitive information constitutes the primary cost, andthe knowledge generated from the data represents the primary benefit. atone extreme, data can be released as is, with identifiers such as precisegeocodes intact. this policy offers maximum benefit at a maximum cost(i.e., minimum confidentiality protection). at the other extreme, data canbe completely restricted for secondary use, a policy that provides minimalbenefit and minimal cost (i.e., maximum confidentiality protection). mostcurrent methods of data release, such as altering or restricting access to theoriginal data, have costs and benefits between these two extremes.wellinformed data access policies reflect wise decisions about theputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.36putting people on the maptradeoffs, such as whether the data usefulness is high enough for the disclosure risks associated with a particular policy. however, most data stewardsdo not directly measure the inputs to these costbenefit analyses. this is notnegligence on the part of data stewards; indeed, the broader research community has not yet developed the tools needed to make such assessments. yet,data stewards could quantify some aspects of the costbenefit tradeoff,namely, disclosure risks and data quality. evaluating these measures canenable data stewards to choose policies with better riskquality profiles (e.g.,between two policies with the same disclosure risk, to select the one withhigher data quality). there have been a few efforts to formalize the task ofassessing data quality and disclosure risk together for the purpose of evaluating the tradeoffs (duncan et al., 2001; gomatam et al., 2005). this sectionbriefly reviews statistical approaches to gauging the riskquality tradeoffsboth generally and for spatial data. (for further discussion about the costbenefit approach to data dissemination, see abowd and lane, 2003).most data stewards seeking to protect data confidentiality are concernedwith two types of disclosures. one is identity disclosure, which occurs whena user of the data correctly identifies individual records using the releaseddata. the other is attribute disclosure, which occurs when a data user learnsthe values of sensitive variables for individual records in the dataset. attribute disclosures typically require identification disclosures (duncan andlambert, 1986a). other types of disclosures include perceived identity disclosure, which occurs when a data user incorrectly identifies individual recordsin the database, and inferential disclosure, which occurs when a data user canaccurately predict sensitive attributes in the dataset using the released datathat may have been alteredñfor example, by adding statistical noiseñtoprevent disclosure. (for introductions to disclosure risks, see federal committee on statistical methodology, 1994; duncan and lambert, 1986a,1986b; lambert, 1993: willenborg and de waal, 1996, 2001.)efforts to quantify identity disclosure risk generally fall in two broadcategories: (1) estimating the number of records in the released data that areunique records in the population and can therefore be at high risk ofidentification, and (2) estimating the probabilities that users of the releaseddata can determine the identities of the records in the released data by usingthe information in those data. although these approaches are appropriatefor many varieties of data, in cases where there are exact spatial identifiers,virtually every individual is unique, so the disclosure risk is very great.quantifying disclosure risksmethods of estimating the risk of identification disclosure involve estimating population uniqueness and probabilities of identification. estimatesof attribute disclosures involve measuring the difference between estimatesputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.legal, ethical, and statistical issues37of sensitive attributes made by secondary data users and the actual values.this section describes methods that are generally applicable for geographicidentification at scales larger than that characterized by exact latitude andlongitude (such as census blocks or tracts, minor civil divisions, or counties). in many cases, exact latitude and longitude uniquely identifies respondents, although there are exceptions (e.g., when spatial identifiers locate aresidence in a large, highrise apartment building).population uniqueness is relevant for identity disclosures becauseunique records are at higher risk of identification than nonunique records.for any unperturbed, released record that is unique in the population, asecondary user who knows that target recordõs correct identifying variablescan identify it with probability 1.0. for any unperturbed released population nonunique target record, secondary users who know its correct identifying variables can identify that record only with probability 1/k, where kis the number of records in the population whose characteristics match thetarget record. for purposes of disclosure risk assessment, population uniqueness is not a fixed quality; it depends on what released information isknown by the secondary data user. for example, most individuals areuniquely identified in populations by the combination of their age, sex, andstreet address. when a data user knows these identifying variables and theyare released on a file, most records are population unique records. however, when the secondary user knows only age, sex, and state of residence,most records will not be unique records. hence, all methods based onpopulation uniqueness depend on assumptions about what information isavailable to secondary data users. the number of population unique recordsin a sample typically is not known and must be estimated by the datadisseminator. methods for making such estimates have been reported byseveral researchers (see, e.g., bethlehem et al., 1990; greenberg and zayatz,1992; skinner, 1992; skinner et al., 1994; chen and kellermcnulty, 1998;fienberg and makov, 1998; samuels, 1998; pannekoek, 1999; dale andelliot, 2001.) these methods involve sophisticated statistical modeling.probabilities of identification are readily interpreted as measures ofidentity disclosure risk: the larger the probability, the greater the risk. datadisseminators determine their own thresholds for probabilities consideredunsafe. there are two main approaches to estimating these probabilities.the first is to match records in the file being considered for release withrecords from external databases that a secondary user plausibly would useto attempt an identification (paass, 1988; blien et al., 1992; federal committee on statistical methodology, 1994; yancey et al., 2002). the matching is done using record linkage software, which (1) searches for the recordsin the external data file that look as similar as possible to the records in thefile being considered for release; (2) computes the probabilities that thesematching records correspond to records in the file being considered forputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.38putting people on the maprelease, based on the degrees of similarity between the matches and theirtargets; and (3) declares the matches with probabilities exceeding a specified threshold as identifications.the second approach is to match records in a file being considered forrelease with the records from the original, unperturbed data file (spruill,1982; duncan and lambert 1986a, 1986b; lambert, 1993; fienberg et al.1997; skinner and elliot, 2002; reiter, 2005a). this approach can be easierand less expensive to implement than obtaining external data files andrecord linkage software. it allows a data disseminator to evaluate the identification risks when a secondary user knows the identities of some or all ofthe sampled records but does not know the location of those records in thefile being considered for release. this approach can be modified to workunder the assumption that the secondary user does not know the identitiesof the sampled records.many data disseminators focus on identity disclosures and pay lessattention to attribute disclosures. in part, this is because attribute disclosures are usually preceded by identity disclosures. for example, when original values of attributes are released, a secondary data user who correctlyidentifies a record learns the attribute values. many data disseminatorstherefore fold the quantification of attribute disclosure risks into the measurement of identification disclosure risks. when attribute values are altered before release, attribute risks change to inferential disclosure risks.there are no standard approaches to quantifying inferential disclosure risks.lambert (1993) provides a useful framework that involves specifying asecondary userõs estimator(s) of the unknown attribute valuesñsuch as anaverage of plausible matchesõ released attribute valuesñand a loss functionfor incorrect guesses, such as the euclidean or statistical distance betweenthe estimate and the true value of the attribute. a data disseminator canthen evaluate whether the overall value of the loss functionñthe distancebetween the secondary userõs proposed estimates and the actual valuesñislarge enough to be deemed safe. (for examples of the assessment of attribute and inferential disclosure risks, see gomatam et al., 2005; reiter,2005d.)the lossfunction approach extends to quantifying overall potentialharm in a data release (lambert, 1993). specifically, data disseminators canspecify cost functions for all types of disclosures, including perceived identification and inferential disclosures, and combine them with the appropriate probabilities of each to determine the expected cost of releasing thedata. when coupled with measurements of data quality, this approachprovides a decisiontheoretic framework for selecting disclosure limitationpolicies. lambertõs total harm model is primarily theoretical and has notbeen implemented in practice.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.legal, ethical, and statistical issues39quantifying data qualitycompared with the effort that has gone into developing measures ofdisclosure risks, there has been less work on developing measures of dataquality. existing quality measures are of two types: (1) comparisons ofbroad differences between the original and released data, and (2) comparisons of differences in specific models between the original and releaseddata. the former measures suffer from not being tied to how users analyzethe data; the latter measures suffer from capturing only certain dimensionsof data quality.broad difference measures essentially quantify differences between thedistributions of the data values on the original and released files. as thedifferences between the distributions grow, the overall quality of the released data drops. computing differences in distributions is a nontrivialstatistical problem, particularly when there are many variables and recordswith unknown distributional shapes. most approaches are therefore adhoc. for example, some researchers suggest computing a weighted averageof the differences in the means, variances, and correlations in the originaland released data, where the weights indicate the relative importance thatthose quantities are similar in the released and observed files (domingoferrer and torra, 2001; yancey et al., 2002). such ad hoc methods are onlytangentially tied to the statistical analyses being done by data users. forexample, a user interested in analyzing incomes may not care that meansare preserved when the tails of the distribution are distorted, because theresearcherõs question concerns only the extremely rich. in environmentalresearch, the main concern may be with the few people with the greatestexposure to an environmental hazard. these measures also have limitedinterpretability and little theoretical basis.comparison of specific models is often done informally. for example,data disseminators look at the similarity of point estimates and standarderrors of regression coefficients after fitting the same regression on theoriginal data and on the data proposed for release. if the results are considered closeñfor example, the confidence intervals for the coefficients obtained from the models largely overlapñthe released data have high qualityfor that particular analysis. such measures are closely tied to how the dataare used, but they only reflect certain dimensions of the overall quality ofthe released data. it is prudent to examine models that represent the widerange of expected uses of the released data, even though unexpected usesmay arise for the conclusions of such models that do not apply.a significant issue for assessing data quality with linked spatialsocialdata is the need at times to simultaneously preserve several characteristicsor spatial relationships. consider, for example, a collection of observationsrepresented as points that define nodes in a transportation network, when aputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.40putting people on the mapnode is defined as a street intersection. although it is possible to create asynthetic or transformed network that has the same mean (global) linklength as the original one, it is difficult to maintain, in addition, actualvariation in the local topology of links (the number of links that connect ata node), as well as the geographical variability in link lengths that might bepresent in the original data. consequently, some types of analyses donewith transformed or synthetic data may yield results similar to those thatwould result with the original data, while others may create substantialrisks of inferential error. the results may include both type i errors, inwhich a null hypothesis is incorrectly rejected, and type ii errors, when anull hypothesis is incorrectly accepted. data users may be tempted to treattransformed data as equal quality to the original data unless they are informed otherwise.effects of spatial identifiersthe presence of precise spatial identifiers can have large effects on theriskquality tradeoffs. releasing these identifiers can raise the risks of identification to extremely high levels. to reduce these risks, data stewards mayperturb the spatial identifiers if they plan to release some version of theoriginal data for open accessñbut doing this can very seriously degrade thequality of the data for analyses that use the spatial information, and particularly for analyses that depend on patterns of spatial covariance, such asdistances or topological relationships between research participants and locations important to the analysis (armstrong et al., 1999). for example, someanalyses may be impossible to do with coarsened identifiers, and others mayproduce misleading results due to altered relationships between the attributesand spatial variables. furthermore, if spatial identifiers are used as matchingvariables for linking datasets, altering them can lead to matching errors,which, when numerous, may seriously degrade analyses.perturbing the spatial information may not reduce disclosure risks sufficiently to maintain confidentiality, especially when the released data includeother information that is known by a secondary data user. for example, theremay be only one person of a certain sex, age, race, and marital status in aparticular county, and this information may be readily available for thecounty, so that coarsening geographies to the county level would provide nogreater protection for that person than releasing the exact address.identity disclosure risks are complicated to measure when the data areset up to be meaningfully linked to other datasets for research purposes.altering spatial identifiers will reduce disclosure risks in the set of dataoriginally collected, but the risks may increase when this dataset is linked todatasets with other attributes. for example, unaltered attributes in file amay be insufficient to identify individuals if the spatial identifiers are alputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.legal, ethical, and statistical issues41tered, but when file a is linked to file b, the combined attributes andaltered spatial identifiers may uniquely identify many individuals. the complication arises because the steward of the collected data may not knowwhich attributes are in the files to be linked to those data, so that it isdifficult to evaluate the degree of disclosure risk.even when safeguards have been established for data sharing, publication of research papersusing linked socialspatial data may pose otherproblems such as those associated with the visualization of data. vanweyet al. (2005) present a means for evaluating the risks associated with displaying data through maps that may be presented orally or in writing tocommunicate research results. the method involves identifying data with aspatial area of a radius sufficient to include, on average, enough researchparticipants to reduce the identity disclosure risk to a target value. methodsfor limiting disclosure risk from maps are only beginning to be developed.no guidelines currently exist forvisualizing linked socialspatial data, inpublished papers or even presentations; butfuture standards for trainingandpublication contextsshouldbe based on systematic assessment ofsuchrisks.in principle, policies for access to data that include spatial identifierscan be improved by evaluating the tradeoff between disclosure risks anddata quality. in practice, though, such an evaluation will be challenging formany data stewards and for irbs that are considering proposals to uselinked data. existing approaches to quantifying risk and quality are technically demanding and may be beyond the capabilities of some data stewards.lowcost, readily available methods for estimating risks and quality do notyet exist, whether or not the data include spatial identifiers. and existingtechniques do not account for the additional risks associated with linkeddatasets. this challenge would be significantly lessened, and data dissemination practice improved, if data stewards had access to reliable, valid, offtheshelf software and protocols for assessing the tradeoffs between disclosure risk and data quality and for undertaking broad costbenefit analyses.the next chapter addresses the issue of evaluating and addressing thetradeoffs involving disclosure risk and data quality.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.423meeting the challengesalthough the challenges described in chapter 2 are substantial, a numberof possible approaches exist for preserving respondent confidentiality whenlinks to geospatial information could engender breaches. they fall in twomain categories: institutional approaches, which involve restricting access tosensitive data; and technical and statistical approaches, which involve transforming the data in various ways to enhance the protection of confidentiality.this chapter describes these two broad categories of approaches.institutional approachesinstitutions that have responsibility for preserving the confidentiality ofrespondents employ a number of strategies. these strategies are very important for protecting respondent confidentiality in survey data under all circumstances, and especially when there is a high risk of identification due tothe existence of precise geospatial attributes. at their heart, many of thesestrategies protect confidentiality by restricting access to the data, either bylimiting access to those data users who explicitly guarantee not to revealrespondent identities or attributes or by requiring that data users work in arestricted environment so they cannot remove information that might reveal identities or attributes. restricting data access is a strategy that can beused with original data or with data that have been deidentified, buffered,or synthesized.in addition to restricting access, institutional approaches require thatresearchersñstudents and faculty or staff at universities or other institutionsñbe educated in appropriate and ethical use of data. many data stewputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.meeting the challenges43ards provide guidelines about how data should be used, what the risks ofdisclosure are, and why researchers should be attentive to disclosure riskand its limitation.1user education at a more fundamental levelñin the general training ofactive and future researchersñshould be based on sound theoretical principles and empirical research. such studies, however, are few: there are onlya few examples of good materials or curricula for ensuring education inproper data use that minimizes the risk of confidentiality breaches. forinstance, the disclosure limitation program project, òhuman subject protection and disclosure risk analysis,ó at the interuniversity consortiumfor political and social research (icpsr) has resources available for teaching about its findings and the best practices it has developed (see http://www.icpsr.umich.edu/hsp [april 2006]). icpsr is also working on a set ofeducation and certification materials on handling restricted data for its ownstaff, which will probably evolve into formal training materials. the carolina population center also has a set of practices for training students whowork on its national longitudinal study of adolescent health (add health:see http://www.cpc.unc.edu/projects/addhealth [april 2006]) and otherprojects, and for teaching its demography trainees about ethics (see http://www.cpc.unc.edu/training/meth.html [april 2006]). however, few othertraining programs have equivalent practices.fundamental to most institutional approaches is the idea that thegreater the risk of disclosure or harm, the more restricted access should be.for every tier of disclosure risk, there is an equivalent tier of access restriction. the tiers of risk are partly a function of the ability of the data distributor to make use of identity masking techniques to limit the risk of disclosure. on a low tier of risk are data with few identifiable variables, such asthe publicuse microdata sets from the u.s. census bureau and many smallsample surveys. because there is little or no geographic detail in these data,when they are anonymized there is very little risk of disclosure, although ifa secondary user knows that an individual is a respondent in a survey (e.g.,because it is a family member), identification is much easier. disseminationof these data over the web has not been problematic from the standpoint ofconfidentiality breaches. on the highest tier of risk are absolutely identifiable data, such as surveys of business establishments and data that includethe exact locations of respondentsõ homes or workplaces.2 the use of thesedata must be tightly restricted to preserve confidentiality. methods and1for example, see the interuniversity consortium for political and social research (icpsr),2005; also http://www.icpsr.umich.edu/access/deposit/index.html [accessed april 2006].2business establishments are generally considered to be among the most easily identifiablebecause data about them are frequently unique: in any given market, there are usually only asmall number of business establishments engaged in any given area of activity, and each hasunique characteristics such as relative size or specialization.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.44putting people on the mapprocedures for restricted data access are well described in a national research council report (2005a:2834).the number of tiers of access can vary from one study to another andfrom one data archive to another. a simple model might have four levels ofaccess: full public access, limited licensing, strong licensing, and data enclaves.3full public accessfull access is provided through webbased publicuse files that areavailable to the general public or to a limited public (for example those whosubscribe to a data service, such as icpsr). access is available to all userswho accept a data use agreement through a webbased form that requiresthem to avoid disclosure. this tier of access is typically reserved for datafiles with little risk of disclosure and harm, such as those that include verysmall subsamples of a larger sample, that represent a tiny fraction of thepopulation in a geographic area, that contain little or no geographic information, or that do not include any sensitive information. we are unawareof any cases for which this form of public access is allowed to files thatcombine social data with highly specific locational data, such as addressesor exact latitude and longitude.public use, fullaccess datasets may include some locational data, such asneighborhood or census tract, if it is believed that such units are too broad toallow identification of particular individuals. however, when datasets arelinked, it is often possible to identify individuals with high probability evenwhen the linked data provide only neighborhoodlevel information. becauseof this probability, the u.s. census bureau uses data swapping and othertechniques in their fullaccess publicuse data files (see http://factfinder.census.gov/jsp/saff/saffinfo.jsp?pageid=su5confidentiality).full public access is extremely popular with data users, for whom itprovides a very high level of flexibility and opportunity. their main complaint is that the datasets made available by this mechanism often includefewer cases and variables than they would like, so that certain types ofanalysis are impossible. although data stewards appear generally satisfied3for other models, see the practices of the carolina population center at the university ofnorth carolina at chapel hill for use of data from the national survey of adolescent health(http://www.cpc.unc.edu/projects/addhealth/data[accessed april 2006]) and the nang rongstudy of social and environmental change, among others. as part of icpsrõs data sharing fordemographic research project (http://www.icpsr.umich.edu/dsdr [accessed april 2006]), researchers there have published a detailed review of contract terms used in restricted useagreements, with recommendations about how to construct such agreements. those documents are available at http://www.icpsr.umich.edu/dsdr/rduc [accessed april 2006].putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.meeting the challenges45with this form of data distribution, they have in recent years begun toexpress concern about whether data can be shared this way without risk ofdisclosure, and so have increasingly restricted the number of data collections available in this format. for example, the national center for healthstatistics (nchs) linked the national health interview survey to the national death index and made the first two releases of the linked file availablepublicly. the third release, which follows both respondents from the earliersurvey years and adds new survey years, is no longer available publicly; it isavailable for restricted use in the nchs research data center.limited licensinglimited licensing provides a second tier of access for data that presentsome risk of disclosure or harm, but for which the risk is limited becausethere is little geographic precisionñthe geographic information has beensystematically masked (armstrong et al., 1999) or sensitive variables havebeen deleted or heavily masked. limited licensing allows data to be distributed to responsible scientific data users (generally those affiliated withknown academic institutions) under terms of a license that requires the datauser and his or her employer to certify that the data will be used responsibly. data stewards release data in this fashion when they believe that thereis little risk of identification and that responsible professionals are able tounderstand the risk and prevent it in their research activities. for example,the demographic and health surveys (dhs) (see http://www.measuredhs.com/[april 2006]) distributes its large representative survey data collectionunder a limited licensing model. it makes geocoded data available under amore restricted type of limited licensing arrangement.the dhs collects the geographic coordinates of its survey cluster, orenumerator areas, but the boundaries or areas of those regions are notmade available. these geocodes can be attached to individual or householdrecords in the survey data, for which identifying information has beenremoved. when particularly sensitive information has been collected in thesurvey (e.g., hiv testing), the current policy is to introduce error into thedata, destroy the original data, and release only the data that have beentransformed.data users consider it a burden to obtain limited licensing agreements,but both data stewards and users generally perceive them as successfulbecause they combine an obligation for education and certification withrelatively flexible access for datasets that present little risk of disclosure orharm. nevertheless, the limitations on the utility of data that may be altered(see armstrong et al., 1999) for release in this manner are still largelyunknown, in part because careful tests with the original data cannot beconducted after the data have been transformed.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.46putting people on the mapstrong licensingstrong licensing is a third tier of data access used for data that presenta substantial risk of disclosure and for which the data steward decides thatthis risk cannot be protected within the framework of responsible researchpractice. datasets are typically placed at this tier if they present a substantial risk of disclosure but are not fully identified or if they include attributedata that are highly sensitive if disclosed, such as responses about sexualpractices, drug use, or criminal activity. most often, these data are sharedthrough a license that requires special handling: for example, they may bekept on a single computer not connected to a network, with specific technical requirements. virtually all strong licenses require that the data userobtain institutional review board (irb) approval at his or her home institution. many of these strong licenses also include physical monitoring, suchas unannounced visits from the data stewardõs staff to make sure thatconditions are followed. these licenses may also require very strong institutional assurances from the researcherõs employer, or may provide for sanctions if not followed. for example, the license to use data from the healthand retirement survey of the national institutes of health (nih) includeslanguage that says the data user may be prevented from obtaining nihgrants in the future if he or she does not adhere to the restrictions. somedata stewards also require the payment of a fee, usually in the range of$500 to $1,000, to support the expenses associated with document preparation and review and the cost of site visits.although some researchers and universities are wary of these agreements, in recent years they have been seen as successful by most data users.data distributors, however, continue to be fearful that their rules aboutdata access are not being followed sufficiently closely or that sensitive dataare under inadequate control.data enclavesfor data that present the greatest risk of disclosure or harm, or thosethat are collected under tight legal restrictionsñsuch as geospatial data thatare absolutely identifiableñaccess is usually limited to use within a research enclave. for example, this will be the case when the fully geocodednang rong data are made available at the data enclave at icpsr. the mostvisible example of this practice in the united states today is the network ofnine research data centers (rdcs) created by the bureau of the censusñwashington, dc; durham, nc; new york city and ithaca, ny; boston,ma; ann arbor, mi; chicago; and los angeles and berkeley, ca.4 the4see http://webserver01.ces.census.gov/index.php/ces/1.00/researchlocations [accessed april2006].putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.meeting the challenges47bureau makes its most restricted data, including the full count of the census of population and the census of business enterprises, available only inthese centers.the principle behind data enclaves is that researchers are not able toleave the premises with any information that might identify an individual.in practice, a trained professional reviews all the materials that each researcher prints. for data analyses, researchers are typically allowed to remove only coefficients from regressiontype analyses and tabulations thathave a large cell size (because small cell sizes may lead to identification).although many data stewards limit users to working within a single, supervised room described as a data center or enclave, alternatives also exist. forexample, in addition to its data enclaves nchs also maintains a systemthat allows data users to submit data analytic programs from a remotelocation, have them run against the data in the enclave, and then receive theresults by email. this procedure is sometimes performed with an automated disclosure review and sometimes with a manual, expert review.there are considerable barriers of inconvenience and cost to use of thedata centers, which means that they are not used as much as they might orshould be. most centers only hold data from a single data provider (forexample, the census, nchs data, or add health), and the division ofwork leads to inefficiencies that might be overcome if a single center helddata from more than one data provider. for the use of its data, the censusbureau centers require a lengthy approval process that can take a full yearfrom the time a researcher is ready to begin work, as well as a òbenefitstatementó on the part of the researcher that demonstrates the work undertaken in the rdc will not only contribute to science, but will also deliver abenefit to the census bureauñsomething required by the bureauõs statutory authority. although other data centers and enclaves do not requiresuch lengthy approval processes, many require a substantial financial payment from the researcher (often calculated as a per day or per month cost ofresearch center use), in addition to travel and lodging costs. personal scheduling to enable a researcher to travel to a remote site competes with teaching, institutional service, and personal obligations and can be a seriousbarrier to use of data in enclaves. the challenge of scheduling becomes evenmore severe in the context of the large, interdisciplinary teams often involved in the analysis of spatial social science data and the need to usespecialized technology and software. in addition to the cost passed on tousers, the data stewards who maintain data enclaves bear considerable costand space requirements.in sum, data enclaves are effective but inefficient and inequitable. social science research is faced with the prospect of full and equal access todata when risk is low, but highly differential and unequal access when risksare high. considerable improvements in data access regimes will be reputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.48putting people on the mapquired so that price will not be the mediating factor that determines whohas access to linked social science and geospatial data.technical approachesdata stewards and statistical researchers have developed a variety oftechniques for limiting disclosure risks (for a summary of many of them, seenational research council, 2005a). this section briefly reviews some ofthese methods and discusses their strengths and weaknesses in the contextof spatial data. generally, we classify the solutions as data limitation (releasing only some of the data), data alteration (releasing perturbed versionsof the data), and data simulation (releasing data that were not collectedfrom respondents but that are intended to perform as the original datawhen analyzed). the approaches described here are designed to preserve asmuch spatial information as possible because that information is necessaryfor important research questions. in this way, they represent advances overolder approaches to creating publicuse social science data, in which thenearuniversal strategy was to delete all precise spatial information fromthe data, usually through aggregation to large areas.data limitationdata limitation involves manipulations that restrict the number of variables, the number of values for responses, or the number of cases that aremade available to researchers. the purpose of data limitation is to reducethe number of unique values in a dataset (reducing the risk of identification)or to reduce the certainty of identification of a specific respondent by asecondary user. a very simple approach sometimes taken with publicusedata is to release only a small fraction of the data originally collected,effectively deleting half or more of all cases. this approach makes it difficult, even impossible, for a secondary user who knows that an individual isin the sample to be sure that she or he has identified the right person: thetarget individual may have been among those deleted from the publicdataset.for tabular data, as well as some microdata, one data limitation approach is cell suppression. the data steward essentially blanks out cellswith small counts in tabular data or blanks out the values of identifiers orsensitive attributes in microdata. the definition of òsmall countsó is selected by the data steward. frequently, cells in tables are not released unlessthey have at least three members. when marginal totals are preserved, as isoften planned in tabular data, other values besides those at risk may need tobe suppressed; otherwise, the data analyst can subtract the sum of theavailable values from the total to obtain the value of the suppressed data.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.meeting the challenges49complementary cells are selected to optimize (at least approximately) various mathematical criteria. (for discussions of cell suppression, see cox,1980, 1995; willenborg and de waal, 1996, 2001.)cell suppression has drawbacks. it creates missing data, which complicates analyses because the suppressed cells are chosen for their values andare not randomly distributed throughout the dataset. when there are manyrecords at risk, as is likely to be the case for spatial data with identifiers,data disseminators may need to suppress so many values to achieve satisfactory levels of protection that the released data have limited analytical utility. cell suppression is not necessarily helpful for preserving confidentialityin survey data that include precise geospatial locations. it is possible, even ifsome or many cells are suppressed, for confidentiality to be breached iflocational data remain. cell suppression also does not guarantee protectionin tabular data: it may be possible to determine accurate bounds for valuesof the suppressed cells using statistical techniques (cox, 2004; fienberg andslavkovic, 2004, 2005). an alternative to cell suppression in tabular data iscontrolled tabular adjustment, which adds noise to cell counts in ways thatpreserve certain analyses (cox et al., 2004).data can also be limited by aggregation. for tabular data, aggregationcorresponds to collapsing levels of categorical variables to increase the cellsize for each level. for microdata, aggregation corresponds to coarseningvariables; for example, releasing ages in 5year intervals or locations at thestate level in the united states. aggregation reduces disclosure risks byturning unique records into replicated records. it preserves analyses at thelevel of aggregation but creates ecological inference problems for lowerlevels of aggregation.for spatial data, stewards can aggregate spatial identifiers or attributevalues or both, but the aggregation of spatial identifiers is especially important. aggregating spatial attributes puts more than one respondent into asingle spatial location, which may be a point (latitudelongitude), a line(e.g., along a highway), or an area of various shapes (e.g., a census tract orother geographic division or a geometrically defined area, such as a circle).this aggregation has the effect of eliminating unique cases within the datasetor eliminating the possibility that a location in the data refers to only asingle individual in some other data source, such as a map or list of addresses. in essence, this approach coarsens the geographic data.some disclosure limitation policies prohibit the release of information atany level of aggregation smaller than a county. use of a fixed level of geography, however, introduces variability in the degree of masking provided. manyrural counties in the united states contain very small total populations, onthe order of 1 thousand, while urban counties may contain more than1 million people. the same problem arises with geographic areas defined byspatial coverage: 1 urban square kilometer holds many more people thanputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.50putting people on the map1 rural square kilometer. the more social identifiers, such as gender, race, orage, are provided for an area, the greater the risk of disclosure.the use of aggregation to guard against accidental release of confidential information introduces side effects into analyses. when point data areaggregated to areas that are sufficiently large to maintain confidentiality,the ability of researchers to analyze data for spatial patterns is attenuated.clusters of disease that may be visually evident or statistically significant atthe individual level, for example, will often become undetectable at thecounty level of aggregation. other effects arise as a consequence of the wellknown relationship between variance and aggregation: variance tends todecrease as the size of aggregated units increase (see robinson, 1950; clarkand avery, 1976). the suppression of variance with increasing levels ofaggregation introduces uncertainty (sometimes called the ecological inference problem) into the process of making inferences based on statisticalanalyses and is a component of the more general modifiable areal unitproblem in spatial data analysis (see openshaw and taylor, 1979).for tabular data, another data limitation alternative is to release aselection of subtables or collapsed tables of marginal totals for some properties to ensure that the cells for the full joint table are large (fienberg andslavkovic, 2004, 2005). this approach preserves the possibility of analysiswhen counts from the released subtables are sufficient for the analysis. forspatial data, this approach could be used with aggregated spatial identifiers,perhaps enabling smaller amounts of aggregation. this approach iscomputationally expensive, especially for highdimensional tables, and requires additional research before a more complete assessment can be madeof its effectiveness.data alterationspatial attributes are useful in linked socialspatial data because theyprecisely record where an aspect of a respondentõs life takes place. sometimes these spatial data are collected at the moment that the original socialsurvey data are collected. in the nang rong (see box 11) and other similarstudies, researchers use a portable global positioning system (gps) device torecord the latitude and longitude of the location of the interview or ofmultiple locations (farm fields, daily itineraries) during the interview process. it is also possible for researchers to follow the daily itineraries of studyparticipants by use of gps devices or rfid (radio frequency identification)tags.in the united states and other developed countries, however, locationsare frequently collected not as latitude and longitude from a gps device,but by asking an individual to supply a street address. street addressesrequire some transformation (e.g., to latitude and longitude) to be madeputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.meeting the challenges51specific and comparable. this transformation, called geocoding, consists ofthe processes through which physical locations are added to records. thereare several types of geocoding that vary in their level of specificity; eachapproach uses different materials to support the assignment of coordinatesto records (see box 31).areal geocoding can reduce the likelihood of identification, but mostother forms of geocoding have the potential to maintain or increase the riskof disclosure because they provide the data user with one or more preciselocations (identifiers) for a survey respondent. the improvements in accuracy associated with new data sources and new technologies, such as parcelgeocoding, only heighten the risk. as a consequence, a new set of techniques has been devised to distort locations, and hence to inhibit disclosure.two of the general methods available are swapping and masking.swappingit is sometimes possible to limit disclosure risk by swappingdata. for example, a data steward can swap the attributes of a person inone area for those of a person in another area, especially if some of thoseattributes are the same (such as two 50yearold white males with differentresponses on other questions), in order to reduce a secondary userõs confidence in correctly identifying an individual. swapping can be done onspatial identifiers or nonspatial attributes, and it can be done within oracross defined geographic locations. swapping small fractions of data generally attenuates associations between the swapped and unswapped variables, and swapping large fractions of data can completely destroy thoseassociations. swapping data will make spatial analyses meaningless unlessthe spatial relationships have been carried into the swapped data. it isgenerally difficult for analysts of swapped data to know how much theswapping affects the quality of analyses.when data stewards swap cases from different locations but leave(genuine) exact spatial identifiers on the file, the identity of participantsmay be disclosed, even if attributes cannot be meaningfully linked to theparticipant. for example, if the data reveal that a respondent lived at aparticular address, even if that personõs data are swapped with someoneelseõs data, a secondary user would still know that a person living at thataddress was included in the study. swapping spatial identifiers thus isbetter suited for limiting disclosures of respondentsõ attributes than theiridentities. swapping may not reduceñand probably increasesñthe risk ofmistaken attribute disclosures from incorrect identifications.swapping may be more successful at protecting participantsõ identitieswhen locations are aggregated. however, swapping may not provide muchadditional protection beyond the aggregation of locations, and it may decrease data quality relative to analyzing the unswapped aggregated data.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.52putting people on the mapbox 31geocoding methodsareal geocodingareal geocoding assigns observations to geographic areas. ifall a researcher needs is to assign a respondent to a political jurisdiction, censusunit, or administrative or other areas in order to match attributes of those largerareas to the individual and perform hierarchical analyses, areal geocoding resolution is a valuable tool. areal geocoding can be implemented when the databasehas either addresses or latitude and longitude data, either through the use of a listof addresses that are contained in an area or through the use of an algorithm thatdetermines whether a point is contained within a particular polygon in space. in thelatter case, a digital file of polygon geometry is needed to support the areal geocoding process.interpolated geocodinginterpolated geocoding estimates the precise locationof an address along a street segment, typically defined between street intersections, on a proportional basis. this approach relies on the use of a geographicbase file (gbf) that contains street centerline descriptions and address ranges foreach side of each street segment in the coverage area. an example is the u.s.census bureauõs tiger (topologically integrated geographic encoding and referencing) files. for any specific address, an algorithm assigns coordinates to recordsby finding the street segment (typically, one side of a block along a street) thatcontains the address and interpolating. thus, the address 1225 maple street wouldbe placed onequarter of the way along the block that contains the oddnumberedaddresses 12011299 and assigned the latitude and longitude appropriate to thatprecise point.interpolated geocoding can produce digital artifacts, such as addresses placedin the middle of a curving street, or errors, such as can occur if, for example, 1225is the last house on the 1201 block of maple street. some of these problems canmaskingmasking involves perturbations or transformations of somedata. observations, in some cases, may be represented as points, but havetheir locations altered in such a way to minimize accurate recovery ofpersonallevel information. among the easiest masking approaches toimplement involves the addition of a stochastic component to each observation, which can be visualized as moving the point by a fixed or randomamount so that the information about a respondent is associated not withthat personõs true location but with another location (see chakrabortyand armstrong, 2001). that is, one can replace an accurately locatedpoint with another point derived from a uniform distribution of radius rcentered on that location. the radius parameter may be constant or allowed to vary as a function of density or some other factor important toa particular application. if density is used, r will be large in lowdensityareas (rural) and would be adjusted downward in areas with higherdensities.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.meeting the challenges53be minimized with software (e.g., by setting houses back from streets). the extentto which such data transformations change the results of data analyses from whatthey would have been with untransformed data has not been carefully studied.this approach may reduce disclosure risks.parcel geocodingparcel geocoding makes use of new cadastral informationsystems that have been implemented in many communities. when this approachis used, coordinates are often transferred from registered digital orthophotographs(images that have been processed to remove distortion that arises as a consequence of sensor geometry and variability in local elevation, for example). thesecoordinates typically represent such features as street curbs and centerlines, sidewalks, and most importantly for geocoding, the locations of parcel and buildingfootprint polygons and either parcel centroids or building footprint centroids. thus,a onetoone correspondence between each address and an accurate coordinate(representing the building or parcel centroid) can be established during geocoding.this approach typically yields more accurate positional information than interpolated geocoding methods.gnssbased geocodingthe low cost and widespread availability of devicesused to measure location based on signals provided by global navigation satellitesystems (gnss), such as the global positioning system deployed by the u.s.department of defense, glonass (russia), and galileo (european union), hasencouraged some practitioners to record coordinate locations for residence locations through field observations. as in the parcel approach, a onetoone correspondence can be established between each residence and an associated coordinate. though this process is somewhat labor intensive, the results are typicallyaccurate since trained field workers can make policydriven judgments about howto record particular kinds of information.though masking can be performed easily, it has a negative side effect:the displaced points can be assigned to locations that contain real observations, thus creating the possibility of false identification and harm to individuals who may not even be respondents in the research. moreover, research on spatial data transformation that involve moving the location ofdata points (armstrong et al., 1999; rushton et al., 2006) shows that thesetransformations may have a significant deleterious effect on the analysis ofdata. not only is there still risk of false identification, but sometimes thepoints are placed in locations where they cannot beñresidences in lakesthat do not permit houseboats, for example. moreover, no single transformation process provides data that are valuable for every possible form ofanalysis. these limitations have major consequences both for successfulanalysis and for reduction of the disclosure risk.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.54putting people on the mapadding noise generally inflates uncertainties in data analyses. for someattributes being estimated, the effect is to increase the width of confidenceintervals. adding noise can also attenuate associations: in a simple linearregression model, for example, the estimated regression coefficients getcloser to zero when the predictors have extra noise. there are techniquesfor accounting for the extra noise, called measurement error models (e.g.,fuller, 1993), but they are not easy to use except in such standard analysesas regressions. some research by computer scientists and cryptographersunder the rubric of òprivacypreserving data miningó (e.g., agrawal andsrikant, 2000; chawla et al., 2005) also follows the strategy of addingspecially constructed random noise to the data, either to individual valuesor to the results of the computations desired by the analyst. privacypreserving data mining approaches have been developed for regressionanalysis, for clustering algorithms, for discrimination, and for associationrules. like other approaches that add noise, these approaches generallysacrifice data quality for protection against disclosure. the nature of thattradeoff has not been thoroughly evaluated for social and spatial data.secure accessan emerging set of techniques aims to provide users with the results ofcomputations on data without allowing them to see individual data values.some of these are based on variants of secure summation (benaloh, 1987),which allows different data stewards to compute the exact values of sumswithout sharing their values. one variant, used at the national center foreducational statistics, provides public data on a diskette or cdrom thatis encoded to allow users to construct special tabulations while preventingthem from seeing the individuallevel data or for calculating totals whenthere are fewer than 30 respondents in a cell. secure summation variantsentail no sacrifice in data quality for analyses based on sums. they provideexcellent confidentiality protection, as long as the database stewards followspecified protocols. this approach is computationally intensive and challenging to set up (for a review of these methods, see karr et al., 2005).another approach involves remote access model servers, to which userssubmit requests for analyses and, in return, receive only the results ofstatistical analyses, such as estimated model parameters and standard errors. confidentiality can be protected because the remote server never allows users to see the actual data (see boulos et al., 2006). remote accessservers do not protect perfectly, however, as the user may be able to learnidentities or sensitive attributes through judicious queries of the system (forexamples, see gomatam et al., 2005). computer scientists also have developed methods for secure record linkage, which enable two or more datastewards to determine which records in their databases have the sameputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.meeting the challenges55values of unique identifiers without revealing the values of identifiers forthe other records in their databases (churches and christen, 2004; oõkeefeet al., 2004).secure access approaches have not generally been used by stewards ofsocial science data, and the risks and benefits for spatialsocial data dissemination and sharing are largely unevaluated. however, the concept underpinning these techniquesñto allow users to perform computations withthe data without actually seeing the datañmay point to solutions for sharing social and spatial data.data simulationdata providers may also release synthetic (i.e., simulated) data thathave similar characteristics as the genuine data as a way to preserve bothconfidentiality and the possibility of meaningful data analysis, an approachfirst proposed by rubin (1993) in the statistical literature. the basic idea isto fit probability models to the original data, then simulate and release newdata that fit the same models. because the data are simulated, the releasedrecords do not correspond to individuals from the original file and cannotbe directly linked to records in other datasets. these features greatly reduceidentity and attribute disclosure risks. however, synthetic data are subjectto inferential disclosure risk when the models used to generate data are tooaccurate. for example, when data are simulated from a regression modelwith a very small mean square error, analysts can use the model to estimateoutcomes precisely and can infer the identities of respondents with highaccuracy.when the probability models closely approximate the true joint probability distributions of the actual data, the synthetic data should have similar characteristics, on average. the òon averageó caveat is important: parameter estimates from any one synthetic dataset are unlikely to equalexactly those from the actual data. the synthetic parameter estimates aresubject to variation from sampling the collected data and from simulatingnew values. it is not possible to estimate all sources of variation from onlyone synthetic dataset, because an analyst cannot measure the amount ofvariability from the synthesis. rubinõs (1993) suggestion is to simulate andrelease multiple, independent synthetic data sets from the same originaldata. an analyst can then estimate parameters and their variances in each ofthe synthetic datasets and combine the results with simple formulas (seedescription by raghunathan et al., 2003).synthetic datasets can have many positive data utility features (seerubin, 1993; raghunathan et al., 2003; reiter, 2002, 2004, 2005b). whenthe data generation models are accurate, valid inferences can be obtainedfrom multiple synthetic datasets by combining standard likelihoodbased orputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.56putting people on the mapsurveyweighted estimates. an analyst need not learn new statistical methods or software programs to unwind the effects of the disclosure limitationmethod. synthetic datasets can be generated as simple random samples, sothat analysts can ignore the original complex sampling design for inferences. the data generation models can adjust for nonsampling errors andcan borrow strength from other data sources, thereby making highqualityinferences possible. finally, because all units are simulated, geographicidentifiers can be included in synthetic datasets.synthetic data reflect only those relationships included in the modelsused to generate them. when the models fail to reflect certain relationships,analystsõ inferences also do not reflect those relationships. for example, ifthe data generation model for an attribute does not take into accountrelationships between location and that attribute, the synthetic data willcontain zero association between the spatial data and that attribute. similarly, incorrect distributional assumptions built into the models are passedon to the usersõ analyses. for example, if the data generation model for anattribute is a normal distribution when the actual distribution is skewed,the synthetic data will fail to reflect the shape of the actual distribution. ifa model does fail to include such relationships, it is a potentially seriouslimitation to releasing fully synthetic data. practically, it means that someanalyses cannot be performed accurately and that data disseminators needto release information that helps analysts decide whether or not the synthetic data are reliable for their analyses.to reduce dependency on data generation models, little (1993) suggests a variant of the fully synthetic data approach called partially syntheticdata. imagine a data set with three kinds of information: information that,when combined, is a potential indirect identifier of the respondent (age, sex,race, occupation, and spatial location); information that is potentially highlysensitive (responses about antisocial or criminal behavior, for example);and a residual body of information that is less sensitive and less likely tolead to identification (responses about personal values or nonsensitive behaviors). partially synthetic data might synthesize the first two categories ofdata, while retaining the actual data of the third category. for example, theu.s. federal reserve board protects data in the u.s. survey of consumerfinances by replacing monetary values at high disclosure risk with multipleimputations, releasing a mixture of these imputed values and the unreplaced,actual values (kennickell, 1997). the u.s. bureau of the census protectsdata in longitudinal linked data sets by replacing all values of some sensitivevariables with multiple imputations and leaving other variables at theiractual values (abowd and woodcock, 2001). partially synthetic approachespromise to maintain the primary benefits of fully synthetic datañprotecting confidentiality while allowing users to make inferences without learningputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.meeting the challenges57complicated statistical methods or softwareñwith decreased sensitivity tothe specification of the data generation models (reiter, 2003).the protection afforded by partially synthetic data depends on thenature of the synthesis. replacing key identifiers with imputations obscuresthe original values of those identifiers, which reduces the chance of identifications. replacing values of sensitive variables obscures the exact values ofthose variables, which can prevent attribute disclosures. partially syntheticdatasets present greater disclosure risks than fully synthetic ones: the originally sampled units remain in the released files, albeit with some valueschanged, leaving values that analysts can use for record linkages.currently, for either fully or partially synthetic data, there are no semiautomatic data synthesizers. data generation models are tailored to individual variables, using sequential regression modeling strategies(raghunathan et al., 2001) and modifications of bootstrapping, amongothers. substantial modeling expertise is required to develop valid synthesizers, as well as to evaluate the disclosure risks and data utility of theresulting datasets. modeling poses an operational challenge to generatingsynthetic datasets. a few evaluations of the disclosure risk and data utilityissues have been done with social surveys, but none with linked spatialsocial data.for spatially identifiable data, a fully synthetic approach simulates allspatial identifiers and all attributes. such an approach can be achievedeither by first generating new values of spatial identifiers, (for example,sampling addresses randomly from the population list, and then simulatingattribute values tied to those new values of identifiers) or by first generatingnew attribute values and then simulating new spatial identifiers tied tothose new attribute values. in generating new identifiers, however, careshould be taken to avoid implausible or impossible results (e.g., privateproperty on public lands, residences in uninhabitable areas). either way,the synthesis requires models relating the geographic identifiers to the attributes. contextual variables can provide information for modeling. theimplications of these methods for data utility, and particularly for thevalidity of inferences drawn from linked socialspatial data synthesized bydifferent methods, have not yet been studied empirically.fully synthetic records cannot be directly linked to records in otherdatasets, which reduces data utility when linkage is desired. one possibilityfor linkage is to make linkages informed by statistical analyses that attemptto match synthetic records in one dataset with appropriate nonsynthesizedrecords in another dataset. research has not been conducted to determinehow well such matching preserves data utility.partially synthetic approaches can be used to simulate spatial identifiersor attributes. simulating only the identifiers reduces disclosure risks without distorting relationships among the attribute variables. its effect on theputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.58putting people on the maprelationships between spatial and nonspatial variables depends on the quality of the synthesis model. at present, not much is known about the utilityof this approach.linking datasets on synthetic identifiers or on attributes creates matching errors, and relationships between spatial identifiers and the linked variables may be attenuated. analyses involving the synthetic identifiers reflectthe assumptions in the model used to generate new identifier values on thebasis of attribute values. this approach introduces error into matches obtained by linking the partially synthetic records to records in other datasets.alternatively, simulating selected attributes reduces attribute risks withoutdisturbing the identifiers: this enables linking, but it does not prevent identity disclosures. relationships between the synthetic attributes and thelinked attributes are attenuatedñalthough to an as yet unknown degreeñwhen the synthesizing models are not conditional on the linked attributes.this limitation also holds true when linking to fully synthetic data.the release of partially synthetic data can be combined with otherdisclosure limitation methods. for example, the census bureau has anapplication, on the map (http://lehdmap.dsd.census.gov/), that combinessynthetic data and the addition of noise. details of the procedure, whichcoarsens some workplace characteristics and generates synthetic travel origins conditional on travel destinations and workplace characteristics, havenot yet been published.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.594the tradeoff:confidentiality versus accessthe previous three chapters describe the challenge of preserving confidentiality while facilitating research in an era of increasingly detailed andavailable data about research participants and their geographic locations.this chapter presents the committeeõs conclusions about what canñandcannotñbe done to achieve two goals: ensure that both explicit and implied pledges of confidentiality are kept when social data are made spatiallyexplicit and provide access to important research data for analysts workingon significant basic and policy research questions. following our conclusions, we offer recommendations for data stewards, researchers, and research funders.conclusionstradeoffs of benefits and risksrecognition of the benefits and risksmaking social data spatially explicitcreates benefits and risks that must be considered in ethical guidelines andresearch policy. spatially precise and accurate data about individuals,groups, or organizations, added to data records through processes ofgeocoding, make it possible for researchers to examine questions they couldnot otherwise explore and gain better understanding of human actors intheir physical and environmental contexts, and they create benefits forsociety in terms of the knowledge that can flow from that research.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.60putting people on the mapconclusion 1: recent advances in the availability of social andspatial data and the development of geographic information systems(gis) and related techniques to manage and analyze those data giveresearchers important new ways to study important social, environmental, economic, and health policy issues and are worth furtherdevelopment.sharing of linked socialspatial data among researchers is imperative toget the most from the time, effort, and money that goes into obtaining thedata. however, to the extent that data are spatially precise and accurate,the risk increases that the people or organizations that are the subject of thedata can be identified. promises of confidentiality that are normally provided for research participants and that can be kept when data are notlinked could be jeopardized as a result of the data linkage, increasing therisk of disclosure and possibly also of harm, particularly when linked dataare made available to secondary data users who may, for example, combinethe linked data with other spatially explicit information about respondentsthat enables new kinds of analysis and, potentially, new kinds of harm.these risks affect not only research participants, but also the scientificenterprise that depends on participantsõ confidence in promises of confidentiality.researcherõs obligationsresearchers who collect or undertake secondaryanalysis of linked socialspatial data and organizations that support research or provide access to such data have an ethical obligation to maximize the benefits of the research and minimize the risk of breaches ofconfidentiality to research participants. this obligation exists even if legalobligations are not clearly defined. those who collect, analyze, or provideaccess to such data need to articulate strong data protection plans, stipulateconditions of access, and safeguard against possible breaches of confidentiality through all phases of the researchñfrom data collection through dissemination. protecting against any breach of confidentiality is a priority forresearchers, in light of the need to honor confidentiality agreements between research participants and researchers, and to support public confidence in the integrity of the research.the tradeoff of confidentiality and accessrestricting data access affordsthe highest protection to the confidentiality of linked socialspatial datathat include exact locations. however, the costs to science are high. ifconfidentiality has been promised, common publicuse forms of data distribution create unacceptable risks to confidentiality. consequently, only morerestrictive forms of data management and dissemination are appropriate,including extensive data reduction, strong licenses, and data center (enputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.the tradeoff: confidentiality versus access61clave) access. when the precise data are available only in data enclaves,many researchers simply do not use the datasets, so research that could bedone is not undertaken. improved methods for providing remote access toenclave data require research and development efforts.conclusion 2: the increasing use of linked socialspatial data hascreated significant uncertainties about the ability to protect the confidentiality promised to research participants. knowledge is as yet inadequate concerning the conditions under which and the extent to whichthe availability of spatially explicit data about participants increasesthe risk of confidentiality breaches.the risks created by the availability and publication of such information increases the betterknown risks associated with other publicationrelated breaches of confidentiality, such as the publication of the names orlocations of primary sampling units or of specific tabular cell sizes. forexample, cartographic materials are often used in publications to illustratepoints or findings that do not lend themselves as easily to tabular or textexplication: what is not yet understood are the conditions under which theyalso increase the ability to identify a research participant.technical strategies for reducing riskcell suppression, data swapping, and aggregation cell suppression anddata swapping techniques can protect confidentiality, but they seriouslydegrade the value of data for analyses in which spatial information isessential. aggregation can provide adequate protection and preserves analysis at a level of aggregation, but it renders data useless when exact locationsare required. hence, aggregation has merit for data that have low levels ofrisk and are slated for publicuse dissemination, but not for data that willbe used for analyses that require exact spatial information.when analyses require exact locations, essentially all observations arethe equivalent of small cells in a statistical table: cell suppression wouldtherefore be tantamount to destroying the spatial component of the data.suppressing nonspatial attributes leaves so much missing information thatthe data are difficult to analyze. swapping exact locations may not preventidentifications and can create serious distortions in analysis when a locationor a topological relationship is a critical variable. swapping nonspatialattributes to limit attribute disclosure risk may need to be done at so high arate that the associations in the data are badly attenuated. suppression orswapping can be used to preserve confidentiality when analyses requireinexact levels of geography, but aggregation is a superior approach in thesecases because it preserves analyses at those levels. aggregation makes itputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.62putting people on the mapimpossible to perform many types of analyses, and when it is used it canlead to ecological inference problems.data alterationdata alteration methods, such as geographic masking oradding noise to sensitive nonspatial attributes, may improve confidentialityprotection but at the expense of data quality. altering data to mask precisespatial locations impedes the ability of researchers to calculate accuratespatial relationships, such as distances, directions, and inclusion of locations within an enumeration unit (e.g., a census tract). there is a tradeoffbetween the magnitude of any masking displacement and the corresponding utility of an observation for a particular use. decisions about thistradeoff affect the risk of a breach of confidentiality. a mask may also beapplied to nonspatial attributes associated with known locations: this mightbe done when knowledge about the magnitude of an attribute, along withknowledge about a generating process (such as a deterministic model oftoxic emissions), could enable the recovery of a location that could then belinked to other information.synthetic datasynthetic data approaches may have the potential to provide access to data with exact spatial identifiers while preserving confidentiality. there is insufficient evidence at present to determine how well thisapproach preserves the socialspatial relationships of interest to researchers. in addition, with current technologies, it is very difficult for data stewards to create analytically valid synthetic datasets. the goal of syntheticdata approaches is to protect confidentiality while preserving certain relationships in the data. this approach depends on data simulation modelsthat capture the relationships among the spatial and nonspatial variables.the effectiveness of such models has not been fully demonstrated across awide range of analyses and datasets. for example, it is not known how wellthese models can preserve distance and topological relationships. it is alsonot known whether and how the various synthetic data approaches can beapplied when linking datasets.secure accesstechniques for providing secure access to linked data, suchas sharing sums but not individual values or conducting data analyses onrequest and returning the results but not the data may have the potential toprovide results from spatial analyses without revealing data values. theseapproaches are not yet extensively used by stewards of spatial data, andtheir feasibility for social and spatial data is unproven. they are computationally intensive and require expertise that is not available to manydata stewards. the value of some of these methods is limited by restrictions on the total number of queries that can be performed before queriescould be combined to identify elements in the original data.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.the tradeoff: confidentiality versus access63conclusion 3: recent research on technical approaches for reducing the risk of identification and breach of confidentiality has demonstrated promise for future success. at this time, however, no knowntechnical strategy or combination of technical strategies for managinglinked socialspatial data adequately resolves conflicts among the objectives of data linkage, open access, data quality, and confidentialityprotection across datasets and data uses.in our judgment, it will remain difficult to reconcile these conflictingobjectives by technical strategies alone, though efforts to identify effectivemethods and procedures should continue. it is likely that different methodsand procedures will be optimal for different applications and that the bestapproaches will evolve with the data and with techniques for protectingconfidentiality and for identifying respondents.institutional approachesconclusion 4: because technical strategies will be not be sufficientin the foreseeable future for resolving the conflicting demands for dataaccess, data quality, and confidentiality, institutional approaches willbe required to balance those demands.institutional approaches involve establishing tiers of risk and accessand producing datasharing solutions that match levels of access to the risksand benefits of the planned research. institutional approaches must addressissues of shared responsibility for the production, control, and use of dataamong primary data producers, secondary producers who link additionalinformation, data users of all kinds, research sponsors, irbs, governmentagencies, and data stewards. it is essential that the power to decide aboutdata access and use be allocated appropriately among these responsibleactors and that those with the greatest power to decide are highly informedabout the issues and about the benefits and risks of the data access policiesthey may be asked to approve. it is also essential that users of the data bearthe burden of confidentiality protection for the data they use.recommendationswe generally endorse the recommendations of two reports, protectingparticipants and facilitating social and behavioral sciences research (national research council, 2003) and expanding access to research data:reconciling risks and opportunities (national research council, 2005a)regarding general issues of confidentiality and data access. it is important tonote that the recommendations in those reports address only data collectedputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.64putting people on the mapand held by federal agencies, and they do not deal with the special issuesthat arise when social and spatial data are linked. this report extends thoserecommendations to include the large body of data that are collected byindividual researchers and academic and research organizations and held atuniversities and other public research entities. it also addresses the need forresearch sponsors, research organizations such as universities, and researchers to pay special attention to data that record exact locations.in particular, we support several key recommendations of these reports:¥access to data should be provided òthrough a variety of modes,including various modes of restricted access to confidential data and unrestricted access to publicuse data altered in a variety of ways to maintainconfidentialityó (national research council, 2005a:68).¥organizations that sponsor data collection should òconduct or sponsor research on techniques for providing useful, innovative publicuse datathat minimize the risk of disclosureó (national research council, 2005a:72)and continue efforts to òdevelop and implement stateoftheart disclosureprotection practices and methods (national research council, 2003:4).¥organizations that sponsor data collection òshould conduct or sponsor research on costeffective means of providing secure access to confidential data by means of a remote access mechanism, consistent with theirconfidentiality assurance protocolsó (national research council,2005a:78).¥data stewardship organizations that use licensing agreements shouldòexpand the files for which a license may be obtained [and] work with datausers to develop flexible, consistent standards for licensing agreements andimplementation procedures for access to confidential dataó (national research council, 2005a:79).¥professional associations should develop strong codes of ethical conduct and should provide training in ethical issues for òall those involved inthe design, collection, distribution, and use of data collected under pledgesof confidentialityó (national research council, 2005a:84).some of these recommendations will not be straightforward to implement for datasets that link social and spatially explicit data. we thereforeelaborate on those recommendations for the special issues and tradeoffsraised by linking social and spatial data.technical and institutional researchrecommendation 1: federal agencies and other organizationsthat sponsor the collection and analysis of linked socialspatial datañputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.the tradeoff: confidentiality versus access65or that support data that could provide added benefits with such linkageñshould sponsor research into techniques and procedures for disseminating such data while protecting confidentiality and maintaining the usefulness of the data for social and spatial analysis. thisresearch should include studies to adapt existing techniques from otherfields, to understand how the publication of linked socialspatial datamight increase disclosure risk, and to explore institutional mechanismsfor disseminating linked data while protecting confidentiality and maintaining the usefulness of the data.this research should include three elements. first, it should includestudies that focus on both adapting existing techniques and developing newapproaches in social science, computer science, geographical science, andstatistical science that have the potential to deal effectively with the problems of linked socialspatial data. the research should include assessmentsof the disclosure risk, data quality, and implementation feasibility associated with the techniques, as well as seeking to identify ways for data stewards to make these assessments for their data.this line of research should include work on techniques that enabledata analysts to understand what analyses can be reliably done with shareddata. it should also include research on analytical methods that correct orat least account for the effects of data alteration. finally, the researchshould be done through collaborations among data stewards, data users,and researchers in the appropriate sciences. among the most promisingtechniques are spatial aggregation, geographic masking, fully and partiallysynthetic data and remote access model servers and other emerging methods of secure access and secure record linkage.second, the research should include work to understand how the publication of spatially explicit material using linked socialspatial data mightincrease disclosure risk and thus to increase sensitivity to this issue. theresearch would include assessments of disclosure risk associated with cartographic displays. it should involve researchers from the social, spatial, andstatistical sciences and would aim to better understand how the publicpresentation of cartographic and other spatially explicit information couldaffect the risk of confidentiality breaches. the education should involveresearchers, data stewards, reviewers and journal editors.third, the research should work on institutional mechanisms for disseminating linked socialspatial data while protecting confidentiality andmaintaining the usefulness of the data for social and spatial analysis. thisresearch should include studies of modifications to traditional data enclaveinstitutions, such as expanded and virtual enclaves, and of modified licensing arrangements for secondary data use. direct data stewards, whether ingovernment agencies, academic institutions, or private organizations, shouldputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.66putting people on the mapparticipate in such research, which should seek to identify and examine theeffects of various institutional mechanisms and associated enforcement systems on data access, data use, data quality, and disclosure risk.education and trainingrecommendation 2: faculty, researchers, and organizations involved in the continuing professional development of researchers shouldengage in the education of researchers in the ethical use of spatial data.professional associations should participate by establishing and inculcating strong norms for the ethical use and sharing of linked socialspatial data.education is an essential tool for ensuring that linked socialspatialdata are organized and used in ways that balance the benefits of the data fordeveloping knowledge, the value of wide access to the data, and the need toprotect the confidentiality of research participants. education and training,both for students and as part of continuing education, require materialsthat extrapolate from general ethical principles for data collection, maintenance, dissemination, and access. these materials should include the ethicalissues raised by linked socialspatial data and, to the extent they are identified and accepted, best practices in the handling of these forms of data.organizations and programs involved in training members of institutionalreview boards (irbs) should incorporate attention to the benefits, uses, andpotential risks of linked socialspatial data.training in ethical issuesrecommendation 3: training in ethical considerations needs toaccompany all methodological training in the acquisition and use ofdata that include geographically explicit information on research participants. education about how to collect, analyze, and maintain linked socialspatial data, how to disseminate results without compromising the identities of individuals involved in the research, and how to share such dataconsonant with confidentiality protections is essential for ensuring thatscientific gains from the capacity to obtain such information can be maximized. graduatelevel courses and professional workshops addressed toethical considerations in the conduct of research need to include attentionto social and spatial data; to enhance awareness of the ethical issues relatedto consent, confidentiality, and benefits as well as risks of harm; and toidentify the best practices available to maximize the benefits from suchputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.the tradeoff: confidentiality versus access67research while minimizing any added risks associated with explicit spatialdata. similarly, institutes, courses, and programs focusing on spatial methods and their use need to incorporate substantive consideration of ethicalissues, in particular those related to confidentiality. education needs toextend to primary and secondary researchers, staffs of organizations engaged in data dissemination, and institutional review boards (irbs) thatconsider research protocols that include linked socialspatial data.outreach by professional societies and other organizationsrecommendation 4: research societies and other research organizations that use linked socialspatial data and that have establishedtraditions of protection of the confidentiality of human research participants should engage in outreach to other research societies andorganizations less conversant in research with issues of human participant protection to increase their attention to these issues in the contextof the use of personal, identifiable data.expertise on outreach is not uniformly distributed across research disciplines and fields. given the likely increased interest in using explicit spatial data linked to other social data, funding agencies, scientific societies,and related research organizations should take steps to ensure that expertise in the conduct of research with human participants is broadly accessibleand shared. an outreach priority should be to develop targeted materials,workshops, and shortcourse training institutes for researchers in fields orsubfields that have had little or no tradition of safeguarding personal,identifiable information.research designrecommendation 5: primary researchers who intend to collectand use spatially explicit data should design their studies in ways thatnot only take into account the obligation to share data and the disclosure risks posed, but also provide confidentiality protection for humanparticipants in the primary research as well as in secondary research useof the data. although the reconciliation of these objectives is difficult,primary researchers should nevertheless assume a significant part ofthis burden.researchers need to consider the tradeoffs between data utility andconfidentiality at the very start of their research programs, when they aremaking commitments to sponsors, designing procedures to obtain informedconsent, and presenting their plans to their irbs. they should be mindful ofputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.68putting people on the mapboth potential benefits and potential harm and plan accordingly. everyoneinvolved needs to understand that achieving a balance between benefits andharms may turn out to be difficult, and at the very least it will requireinnovative thinking, compromise, and partnership with others. it is imperative to recognize that it may take a generation to find norms for sharing thenew kind of data and an equally long effort to ensure the safety of humanresearch subjects. if, for example, irbs need to be continuously involved inmonitoring projects, they (and the researchers) should accept that role. ifresearchers must turn their data over to more experienced stewards forsafekeeping, that, too, will need to be acknowledged and accepted. finally,secondary researchers need to understand that access to confidential datamay involve difficulties, and plan their work accordingly.institutional review boardsrecommendation 6: institutional review boards and their organizational sponsors should develop the expertise needed to make wellinformed decisions that balance the objectives of data access, confidentiality, and quality in research projects that will collect or analyzelinked socialspatial data.given the rapidity with which advances are being made in collectingand linking social and spatial data, maintaining appropriate expertise willbe an ongoing task. irbs need to learn what they do not know and developplans to consult with experts when appropriate. traditionally, irbs haveconcerned themselves more with the collection of data than its dissemination, but the heightened risks to confidentiality that arise from linkingsocial data to spatial data requires increased attention to data dissemination. government agencies that sponsor research that requires the application of the common rule, the human subjects research subcommittee ofthe executive branch committee on research, and the association for theaccreditation of human research protection programs (aahrpp) shouldwork together to convene an expert working group to address the issue ofsocial and spatial data and make recommendations for best practices.data enclavesrecommendation 7: data enclaves deserve further developmentas a way to provide wider access to highquality data while preservingconfidentiality. this development should focus on the establishment ofexpanded placebased enclaves, òvirtual enclaves,ó and meaningful penalties for misuse of enclaved data.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.the tradeoff: confidentiality versus access69three elements are critical to this development. first, data producers,data stewards, and academic and other research organizations should consider expanding placebased (as opposed to virtual) data enclaves to holdmore extensive collections of social and spatial data. currently, many suchdata enclaves are maintained by a data producer (such as the u.s. bureauof the census) and contain only the data produced by that organization oragency. the panelõs recommendation proposes alternative models in whichorganizations that store the research they produce also house social andspatial datasets produced elsewhere or in which institutions that managemultiple enclaves combine them into a single entity. this recommendationmay require that some agencies (e.g., the census bureau) obtain regulatoryor legislative approval in order to broaden their ability to manage restricted data. this approach could make such data more accessible andcosteffective for secondary researchers while also increasing the capacityand sustainability of data enclaves. the main challenge is to work outadequate confidentiality protection arrangements between data producersand the stewards of expanded enclaves.second, òvirtual enclaves,ó in which data are housed in a remote location but accessed in a secure setting by researchers at their own institutionunder agreed rules, deserve further development. virtual archives at academic institutions should be managed by their libraries, which have expertise in maintaining the security of valuable information resources, such asrare books and institutional archives. the census bureau has demonstratedthe effectiveness of such remote archives with the technology used for itsresearch data centers, and statistics canada has created a system that isrelatively more accessible (relative to the number of canadian researchers)through its research data centre program (see http://www.statcan.ca/english/rdc/index.htm). the extension of these approaches will reduce thecost of access to research data if researchers and their home institutionsinvest in construction and staffing and if principles of operation can beagreed on. one key issue in the management of virtual or remote enclaves isthe location of the òwatchful eyeó that ensures that the behavior of restricted data users follows established rules. in some cases, the observer willbe a remote computer or operator, while in others it will be a personworking at the location where the data user is working, for example, in acollege or university library.third, access to restricted data through virtual or placebased enclavesshould be restricted to those who agree to abide by the confidentialityprotections governing such data, and meaningful penalties should be enforced for willful misuse of the linked socialspatial data. highqualityscience depends on sound ethical practices. ethical standards in all fields ofscience require honoring agreements made as a condition of undertakingprofessional workñwhether those agreements are between primary reputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.70putting people on the mapsearchers and research participants or between researchers and researchrepository in the case of secondary use. appropriate penalties might includepublication of reports of willful misuse, disbarment from future researchusing restrictedaccess data, reduced access to federal research funding, andmechanisms that would provide incentives to institutions that employ researchers who willfully or carelessly misuse enclaved data so that theyenforce agreements to which they are party.licensingrecommendation 8: data stewards should develop licensingagreements to provide increased access to linked socialspatial datasetsthat include confidential information.licensing agreements place the burden of confidentiality protection onthe data user. several aspects of licensing deserve further development.first, nontransferable, timelimited licenses require the data user only toensure that his or her own use does not make respondents identifiable toothers or cause them harm and to return or destroy all copies of the data aspromised. however, to be effective, such agreements require strong incentives for users to protect the confidentiality of the research participants.second, strong licensing, which requires data users to take specialprecautions to protect the shared data, can make sensitive data more widelyavailable than has been the case to date. data stewards who are responsiblefor managing data enclaves or other restricted data centers, as well asresearch sponsors who support research that can only be disseminatedunder tight restrictions, should make these kinds of data as accessible aspossible. strong licensing agreements provide an appropriate mechanismfor providing increased access in many situations.third, research planning should include mechanisms to facilitate datause under license. sponsors of primary research should ensure that plansare developed at the outset, with sufficient resources provided (e.g., time todo research, funds to pay for access) to prepare datasets that facilitateanalysis by secondary data users. data sponsors and data stewards shouldensure that the plans for data access are carried through.fourth, explicit enforcement language should be included in contractsand license agreements with secondary users setting forth penalties forbreaches of confidentiality and other willful misuse of the linked geospatialand social data. funding agencies and research societies with codes of ethicsshould scrutinize confidentiality breaches that occur and take actions appropriate to their roles and responsibilities.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.71referencesabowd, j.m., and j. lane2003the economics of data confidentiality. cornell university and the urban institute, washington, dc. available: http://www7.nationalacademies.org/cnstat/abowdlane.pdf [accessed april 2006].abowd, j.m., and s.d. woodcock2001disclosure limitation in longitudinal linked data. pp. 216277 in p. doyle, j.lane, l. zayatz, and j. theeuwes, eds., confidentiality, disclosure, and dataaccess: theory and practical applications for statistical agencies. amsterdam,netherlands: northholland elsevier.agrawal, r., and r. srikant2000privacypreserving data mining. pp. 439450 in proceedings of the 2000 acmsigmod on management of data. new york: acm press.anderson, m., and fienberg, s.f.1997who counts? the politics of census taking. society 34(3):1926.anselin, l.2005exploring spatial data with geodaª: a workbook. center for spatially integrated social science, university of illinois, urbanachampaign. available: http://www.geoda.uiuc.edu/pdf/geodaworkbook.pdf [accessed april 2006].anselin, l., i. syabri, i., and y. kho2006geoda: an introduction to spatial data analysis. geographical analysis 38(1):522.arizona state university2006central arizonaphoenix longterm ecological research project, global institute of sustainability. available: http://caplter.asu.edu/home/data/index.jsp [accessed november 2006].armstrong, m.p., and a. ruggles2005geographic information technologies and personal privacy. cartographica40(4):6373.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.72putting people on the maparmstrong, m.p., g. rushton, and d.l. zimmerman1999geographically masking health data to preserve confidentiality. statistics in medicine 18:497525.balk, d., t. pullum, a. storeygard, f. greenwell, and m. neuman2004a spatial analysis of childhood mortality in west africa. population, space andplace 10:175216.balk, d., a. storeygard, m. levy, j. gaskell, m. sharma, and r. flor2005child hunger in the developing world: an analysis of environmental and socialcorrelates. food policy 30(56):584611.benaloh, j.1987secret sharing homomorphisms: keeping shares of a secret secret. pp. 251260 ina.m. odlyzko, ed., crypto86. lecture notes in computer science no. 263.berlin, germany: springerverlag.bethlehem, j.g., w.j. keller, and j. pannekoek1990disclosure control of microdata. journal of the american statistical association85:3845.bivand, r.2006implementing spatial data analysis software tools in r. geographical analysis38(1):2340.blien, u., h. wirth, and m. muller1992disclosure risk for microdata stemming from official statistics. statisticaneerlandica 46:6982.borgatti, s.p., and j.l. molina2003ethical and strategic issues in organizational social network analysis. the journal of applied behavioral science 39(30):337349.borriello, g., m. chalmers, a. lamarca, and p. nixon2005delivering realworld ubiquitous location systems. communications of the association for computing machinery 48(3):3641.boulos, m.n.k., q. cai, j.a. padget, and g. rushton2006using software agents to preserve individual health data confidentiality in microscale geographical analyses. journal of biomedical informatics 39(2):160170.breiger, r.l.2005introduction to special issue: ethical dilemmas in social network research. socialnetworks 27(2):8993.brownstein, j.s., c.a. cassa, and k.d. mandi2006no place to hideñreverse identification of patients from published maps. newengland journal of medicine 355(16):17411742.butz, w., and b.b. torrey2006some frontiers in social science. science 312:18981900.chakraborty, j., and m.p. armstrong2001assessing the impact of airborne toxic releases on populations with special needs.the professional geographer 53:119131.chawla, s., c. dwork, f. mcsherry, a. smith, and h. wee2005towards privacy in public databases. pp. 363385 in j. kilian, ed., theory ofcryptography conference proceedings. lecture notes in computer science 3378.berlin, germany: springerverlag.chen, g., and s. kellermcnulty1998estimation of identification disclosure risk in microdata. journal of official statistics14:7995.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.references73churches, t., and p. christen2004some methods for blindfolded record linkage. bmc medical informatics anddecision making 4(9). available: http://www.pubmedcentral.nih.gov/tocrender.fcgi?iid=10563 [accessed april 2006].clark, w.a.v., and k. avery1976the effects of data aggregation in statistical analysis. geographical analysis8:428438.confidentiality and data access committee2000panel on disclosure review boards of federal agencies: characteristics, defining qualities, and generalizability. 2000 joint statitistic meetings, indianapolis,in. available: http://www.fcsm.gov/committees/cdac/drbpanel.doc [accessedapril 2006].2002indentifiability in microdata files. available: http://www.fcsm.gov/committees/cdac/cdacra9.doc [accessed april 2006].cowen, d., and j.r. jensen1998extraction and modeling of urban attributes using remote sensing. pp. 164188in national research council, people and pixels: linking remote sensing andsocial science. committee on the human dimensions of global change, d.liverman, e.f. moran, r.r. rindfuss, and p.c. stern, eds. washington, dc:national academy press.cowen, d., j. jensen, j. halls, m. king, and s. narumalani1993estimating housing density with cams remotely sensed data. proceedings,american congress on surveying and mapping/american society for photogrammetry and remote sensing (acrm/asprs):3543.cox, l.h.1980suppression methodology and statistical disclosure control. journal of the american statistical association 75:377385.1995network models for complementary cell suppression. journal of the americanstatistical association 90:14531462.1996protecting confidentiality in small population health and environmental statistics. statistics in medicine 15:18951905.2004inference control problems in statistical disclosure query systems. pp. 113 inresearch directions in data and applications security, c. farkas and p.samarati, eds. boston, ma: kluwer.cox, l.h., j.p. kelly, and r. patil2004balancing quality and confidentiality for multivariate tabular data. pp. 8798 inj. domingoferrer and v. torra, eds., privacy in statistical databases. lecturenotes in computer science, vol. 3050. berlin, germany: springerverlag.culler, d., d. estrin, and m. srivastava2004overview of sensor networks. computer 37(8):4149.dale, a., and m. elliot2001proposals for 2001 samples of anonymized records: an assessment of disclosurerisk. journal of the royal statistical society, series a 164:427447.deane, g.d., and m.p. gutmann2003blowinõ down the road: investigating bilateral causality between dust storms andpopulation change in the great plains. population research and policy review22:297331.de wolf, v.2003issues in accessing and sharing confidential survey and social science data. datascience journal 2 (17 feb):6674.dobson, j.e., and p.f. fisher2003geoslavery. technology and society 22(1):4752.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.74putting people on the mapdomingoferrer, j., and v. torra2001a quantitative comparison of disclosure control methods for microdata. pp.111133 in p. doyle, j. lane, l. zayatz, and j. theeuwes, eds., disclosure, anddata access: theory and practical applications. amsterdam, netherlands:northholland elsevier.doyle p., j. i. lane, j.j.m. theeuwe, and l.v. zayatz, eds.2002confidentiality, disclosure and data access: theory and practical applicationsfor statistical agencies. amsterdam, netherlands: northholland elsevier.duncan, g.t., and d. lambert1986adisclosurelimited data dissemination. journal of the american statistical association 81:1028.1986bthe risk of disclosure for microdata. journal of business and economic statistics7:207217.duncan, g.t., s.a. kellermcnulty, and s.l. stokes2001disclosure risk vs. data utility: the ru confidentiality map. technical report, u.s. national institute of statistical sciences, research triangle park, nc.electronic privacy information center2003the census and privacy. available: http://www.epic.org/privacy/census/ [accessed july 2006].entwisle, b., r.r. rindfuss, s.j. walsh, t.p. evans, and s.r. curran1997geographic information systems, spatial network analysis, and contraceptivechoice. demography 34:171187.federal committee on statistical methodology1994statistical policy working paper 22: report on statistical disclosure limitationmethodology. subcommittee on disclosure limitation methodology, office ofmanagement and budget, executive office of the president. washington, dc:u.s. general printing office.fienberg, s.e., and u.e. makov1998confidentiality, uniqueness, and disclosure limitation for categorical data. journal of official statistics 14:361372.fienberg, s.e., u.e. makov, and a.p. sanil1997a bayesian approach to data disclosure: optimal intruder behavior for continuous data. journal of official statistics 13:7589.fienberg, s.e., and a.b. slavkovic2004making the release of confidential data from multiway tables count. chance17(3):510.2005preserving the confidentiality of categorical statistical data bases when releasinginformation for association rules. data mining and knowledge discovery 11:155180.fisher, p., and j. dobson2003who knows where you are, and who should, in the era of mobile geography?geography 88:331337.foster, a.2005a review of ten years of work on economic growth and population change inrural india. pp. 287308 in national research council, population, land use,and environment: research directions. panel on new research on populationand the environment, b. entwisle and p.c. stern, eds. washington, dc: thenational academies press.fotheringham, a.s., c. brunsdon, and m. charlton2002geographically weighted regression. hoboken, nj: john wiley & sons.fuller, w.a.1993masking procedures for microdata disclosure limitation. journal of official statistics 9:383406.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.references75geer, d.2006nanotechnology: the growing impact of shrinking computers. pervasive computing 5:711.gephart, m.a.1997neighborhoods and communities as contexts for development. pp. 143 in j.brooksgunn, g.j. duncan, and j.l. aber, eds., neighborhood poverty: context and consequences for children. new york: russell sage.gomatam, s., a.f. karr, j.p reiter, and a.p. sanil2005data dissemination and disclosure limitation in a world without microdata: ariskutility framework for remote access servers. statistical science 20:163177.gordonlarsen, p., m.c. nelson, p. page, and b.m. popkin2006inequality in the built environment underlies key health disparities in physicalactivity and obesity. pediatrics 117:417424.goss, j.1995we know who you are and we know where you live: the instrumental rationality of geodemographic systems. economic geography 71:171198.greenberg, b., and l.v. zayatz1992strategies for measuring risk in public use microdata files. statistica neerlandica46:3348.gutmann, m.p., g.d. deane, n. lauster, and a. peri2006heat, elevation, and migration: two populationenvironment regimes in thegreat plains of the united states, 19301990. population and environment27(2):191225. available: http://www.springerlink.com/content/f51434802p621653/ [accessed december 2006].hutchinson, c.f.1998social science and remote sensing in famine early warning. pp. 189196 in national research council, people and pixels: linking remote sensing and socialscience. committee on the human dimensions of global change, d. liverman,f.f. moran, r.r. rindfuss, and p.c. stern, eds. washington, dc: nationalacademy press.indiana university2006anthropological center for training and research on global environmentalchange. available: http://www.indiana.edu/%7eact/research.htm [accessed november 2006].interuniversity consortium for political and social research2005guide to social science data preparation and archiving. third edition. available: http://www.icpsr.umich.edu/access/dataprep.pdf [accessed april 2006].jabine, t.b.1993statistical disclosure limitation practices of united states statistical agencies. journal of official statistics 9:427454.kadushin, c.2005who benefits from network analysis: ethics of social network research. socialnetworks 27(2):139153.karr, a.f., x. lin, a.p. sanil, and j.p. reiter2005secure regressions on distributed databases. journal of computational andgraphical statistics 14:263279.kennickell, a.b.1997multiple imputation and disclosure protection: the case of the 1995 survey ofconsumer finances. pp. 248267 in national research council, record linkagetechniques. committee on applied and theoretical statistics. washington, dc:national academy press.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.76putting people on the mapklovdahl, a.s.2005social network research and human subjects protection: towards more effectiveinfectious disease control. social networks 27(2):119137.kwan, m.p.2003individual accessibility revisited: implications for geographical analysis in thetwentyfirst century. geographical analysis 35(4):341353.lahlou, s., m. langheinrich, and c. rıcker2005privacy and trust issues with invisible computers. communications of the association for computing machinery 48(3):5960.lambert, d.1993measures of disclosure risk and harm. journal of official statistics 9:313331.leclere, f.b., r.g. rogers, and k.d. peters1998neighborhood social context and racial differences in womenõs heart diseasemortality. journal of health and social behavior 39:91107.levine, f., and p.r. skedsvold2006behavioral and social science research. in e.j. emanuel, r.a. crouch, c. grady,r. lie, f. miller, and d. wendler, eds., the oxford textbook of clinical research ethics. oxford, england: oxford university press.little, r.j.a.1993statistical analysis of masked data. journal of official statistics 9:407426.martinez, k., j.k. hart, and r. ong2004environmental sensor networks. computer 37(8):5056.mcgrahanan, g., p. marcotullio, x. bai, d. balk, t. braga, i. douglas, t. elmqvist, w. rees,d. satterthwaite, j. songsore, and h. zlotnik2005urban systems. chapter 22 in conditions and trends assessment of the millennium ecosystem assessment. chicago, il: island press.mcgranahan, g., d. balk, and b. anderson2006low coastal zone settlements. tiempo: a bulletin on climate and development59(april):2326.melichar, l., j. evans, and c. bachrach2002data access and archiving: options for the demographic and behavioral sciences branch. deliberations and recommendations of dbsb workshop. available: http://www.nichd.nih.gov/publications/pubs/upload/dataaccess.pdf [accessed november 2006].minnesota population center, university of minnesota, and interuniversity consortium forpolitical and social research census 2000 advisory committee2000the public use microdata samples of the u.s. census: research applicationsand privacy issues. census. 2000 usersõ conference on pums, alexandria, va.available: http://www.pop.umn.edu/~census2000/indexolder.html [accessedjuly 2006].monmonier, m.2002spying with maps: surveillance technologies and the future of privacy. chicago, il: the university of chicago press.moran, e.f., e.s. brondizio, and l.k. vanwey2005population and environment in amazonia: landscape and household dynamics.pp. 106134 in national research council, population, land use, and environment: research directions. panel on new research on population and the environment, b. entwisle and p.c. stern, eds. washington, dc: the national academies press.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.references77national center for health statistics and centers for disease control and prevention2003zip code tabulation area and confidentiality. joint ece/eurostat work session on statistical data confidentiality, conference of european statisticians,luxembourg, 79 april 2003, sponsored by united nations statistical commission and economic commission for europe and european commission, statistical office of the european communities (eurostat). available: http://www.unece.org/stats/documents/2003/04/confidentiality/wp.34.e.pdf [accessed april2006].national commission for the protection of human subjects of biomedical and behavioralresearch1979belmont report: ethical principles and guidelines for the protection of humansubjects of research. gpo 887809. washington. dc: u.s. government printing office.national research council1985sharing research data. committee on national statistics, s. fienberg, l. martin, and m. straf, eds. washington, dc: national academy press.1993private lives and public policies: confidentiality and accesssibility of government statistics. panel on confidentiality and data access, committee on national statistics, g.t. duncan, t.b. jabine, and v.a. dewolf, eds. washington,dc: national academy press.1998people and pixels: linking remote sensing and social science. committee onthe human dimensions of global change, d. liverman, f.f. moran, r.r.rindfuss, and p.c. stern, eds. washington, dc: national academy press.2000improving access to and confidentiality of research data: report of a workshop. committee on national statistics, c. mackie and n. bradburn, eds. washington, dc: national academy press.2001resolving conflicts arising from the privatization of environmental data. committee on geophysical and environmental data, board on earth sciences andresources, division on earth and life studies. washington, dc: national academy press.2003access to research data in the 21st century: an ongoing dialogue amonginterested parties. science, technology, and law panel, division of policy andglobal affairs. washington, dc: the national academies press.2004alicensing geographic data and services. committee on licensing geographicdata and services, board on earth sciences and resources. washington, dc:the national academies press.2004bprotecting participants and facilitating social and behavioral science research.panel on institutional review boards, surveys, and social science research, c.f.citro, d.r. ilgen, and c.b. marrett, eds., committee on national statistics andboard on behavioral, cognitive and sensory sciences. washington, dc: thenational academies press.2005aexpanding access to research data: reconciling risks and opportunities. panelon data access for research purposes, committee on national statistics: washington, dc: the national academies press.2005bpopulation, land use, and environment: research directions. panel on newresearch on population and the environment, b. entwisle and p.c. stern, eds.washington, dc: the national academies press.new york times2006house panel to press cellphone industry on improving protection of customerrecords. february 1:c3 (late edition). available: http://select.nytimes.com/search/restricted/article?res=f70f11ff345b0c728cddab0894de404482 [accessedapril 2006].putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.78putting people on the mapoõharrow, r.2005no place to hide. new york: free press.oõkeefe, c.m., m. yung, l. gu, and r. baxter2004privacypreserving data linkage protocols. pp. 94102 in v. atluri, p. syverson,and s. de capitani di vimercati, eds. proceedings of the 2004 acm workshopon privacy in the electronic society, wpes 2004. washington, dc: acm press.openshaw, s., and p.j. taylor1979a million or so correlation coefficients: three experiments on the modifiableareal unit problem. pp. 127144 in n. wrigley, ed., statistical applications inthe spatial sciences. london, england: pion.paass, g.1988disclosure risk and disclosure avoidance for microdata. journal of business andeconomic statistics 6:487500.pannekoek, j.1999statistical methods for some simple disclosure limitation rules. statisticaneerlandica 53:5567.parker, e.b.1998measuring access to primary medical care: some examples of the use of geographical information systems. health and place 4(2):183193.raghunathan, t.e., j.m. lepkowski, j. van hoewyk, and p. solenberger2001a multivariate technique for multiply imputing missing values using a series ofregression models. survey methodology 27:8596.raghunathan, t.e., j.p. reiter, and d.b. rubin2003multiple imputation for statistical disclosure limitation. journal of official statistics 19:116.reiter, j.p.2002satisfying disclosure restrictions with synthetic data sets. journal of officialstatistics 18:531544.2003inference for partially synthetic, public use microdata sets. survey methodology29:181189.2004simultaneous use of multiple imputation for missing data and disclosure limitation. survey methodology 30:235242.2005aestimating risks of identification disclosure in microdata. journal of the american statistical association 100:11031113.2005breleasing multiplyimputed, synthetic public use microdata: an illustration andempirical study. journal of the royal statistical society, series a:168, 185205.2005csignificance tests for multicomponent estimands from multiplyimputed, synthetic microdata. journal of statistical planning and inference 131:365377.2005dusing cart to generate partially synthetic, public use microdata. journal ofofficial statistics 21:441462.robinson, w.s.1950ecological correlation and the behavior of individuals. american sociologicalreview 15:351357.ross, c.e., j.r. reynolds, and k.j. geis2000the contingent meaning of neighborhood stability for residentsõ psychologicalwellbeing. american sociological review 65:581595.rubin, d.b.1993discussion: statistical disclosure limitation. journal of official statistics 9:462468.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.references79rushton, g., m.p. armstrong, j. gittler, b. greene, c.e. pavlik, m. west, m. and d.zimmerman2006geocoding in cancer research: a review. american journal of preventive medicine 30(2, suppl. 1):s16s24.sampson, r.j., j.d. morenoff, and t. gannonrowley2002assessing ôneighborhood effectsõ: social processes and new directions in research.annual review of sociology 28:443478.samuels, s.m.1998a bayesian speciessamplinginspired approach to the unique problems inmicrodata. journal of official statistics 14:373384.seastrom, m.m.2002licensing. pp. 279296 in confidentiality, disclosure and data access: theoryand practical applications for statistical agencies, p. doyle, j. lane, j.j.m.theeuwes, and l. zayatz, eds. amsterdam, netherlands: northholland elsevier.seto, k.c.2005economies, societies, and landscapes in transition: examples from the pearl riverdelta, china, and the red river delta, vietnam. pp. 193216 in national research council, population, land use, and environment: research directions.panel on new research on population and the environment, b. entwisle andp.c. stern, eds. washington, dc: the national academies press.skinner, c.j.1992on identification disclosure and prediction disclosure for microdata. statisticaneerlandica 46:2132.skinner, c.j., and m.j. elliot2002a measure of disclosure risk for microdata. journal of the royal statistical society, series b 64:855867.skinner, c., c. marsh, s. openshaw, and c. wymer1994disclosure control for census microdata. journal of official statistics 10:3151.smailagic, a., and d.p. siewiorek2002application design for wearable and contextaware computers. pervasive computing 1(4):2029.small, c., and j.e. cohen2004continental physiography, climate, and the global distribution of human population. current anthropology 45(2):269277.small, c., and r.j. nicholls2003a global analysis of human settlement in coastal zones. journal of coastal research 19(3):584599.smith, k.r., and n.j. waitzman1997effects of marital status on the risk of mortality in poor and nonpoor neighborhoods. annals of epidemiology 7:343349.spruill, n.l.1982measures of confidentiality. pp. 260265 in proceedings of the section on surveyresearch methods of the american statistical association. alexandria, va:american statistical association.streitz, n., and p. nixon2005the disappearing computer. communications of the association for computingmachinery 48(3):3335.sui, d.2005will ubicomp make gis invisible? computers, environment and urban systems29:361367.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.80putting people on the mapuniversity of michigan2005aproject sluce: spatial land use change and ecological effects at the ruralurban interface: agentbased modeling and evaluation of alternative policiesand interventions. available: http://www.cscs.umich.edu/sluce [accessed april2006].2005breciprocal relations between population and environment project, populationstudies center. available: http://www.psc.isr.umich.edu/research/projectdetail.html?id=51 [accessed april 2006].university of north carolina2005ecuador projects, carolina population center. available: http://www.cpc.unc.edu/projects/ecuador [accessed november 2006].2006nang rong projects, carolina population center: available: http://www.cpc.unc.edu/projects/nangrong [accessed november 2006].vanwey, l.k., rindfuss, r.r., gutmann, m.p., entwisle, b.e., and balk, d.l.2005confidentiality and spatially explicit data: concerns and challenges. proceedingsof the national academy of sciences 102:1533715342.walsh, s.j., r.r. rindfuss, p. prasartkul, b. entwisle, and a. chamratrithirong2005population change and landscape dynamics: the nang rong, thailand studies.pp. 135162 in national research council, population, land use, and environment: research directions. panel on new research on population and the environment, b. entwisle and p.c. stern, eds. washington, dc: the national academies press.want, r.2006an introduction to rfid technology. pervasive computing 5(1):2533.weeks, j.r., d.p. larson, and d.l. fugate2005patterns of urban land use as assessed by satellite imagery: an application tocairo, egypt. pp. 265286 in national research council population, land use,and environment: research directions. panel on new research on populationand the environment, b. entwisle and p.c. stern, eds. washington, dc: thenational academies press.weiser, m.1991the computer for the 21st century. scientific american 265(3):94104.willenborg, l., and t. de waal1996statistical disclosure control in practice. new york: springerverlag.2001elements of statistical disclosure control. new york: springerverlag.williams, a.p.1983how many miles to the doctor? the new england journal of medicine309(16):958963.wood, c.h., and d. skole1998linking satellite, census, and survey data to study deforestation in the brazilianamazon. pp. 7093 in national research council, people and pixels: linkingremote sensing and social science. committee on the human dimensions ofglobal change, d. liverman, f.f. moran, r.r. rindfuss, and p.c. stern, eds.washington, dc: national academy press.yancey, w.e., w.e. winkler, and r.h. creecy2002disclosure risk assessment in perturbative microdata protection. in j. domingoferrer, ed., inference control in statistical databases. berlin, germany: springerverlag.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.81apppendix aprivacy for research datarobert gellmanintroductionscope and purposethe purpose of this paper is to describe privacy rules in the three mostimportant areas relevant to research uses of information involving remotelysensed and selfidentifying data. the three issues are (1) when is information sufficiently identifiable so that privacy rules apply or privacy concernsattach? (2) when does the collection of personal information fall underregulation? and (3) what rules govern the disclosure of personal information? in addition, a short discussion of liability for improper use or disclosure is included. the goal is to provide sufficient information to illustratewhere linesñalbeit vague, inconsistent, and incompleteñhave been drawn.spatial information can have a variety of relationships with personaldata. a home address is spatial information that is likely to be personallyidentifiable and will typically be included within the scope of statutoryprivacy protections along with name, number, and other personal data.even in the absence of a statute, spatial data that are identifiable raise overtprivacy issues. in other contexts, spatial information linked with otherwisenonidentifiable personal data (e.g., from an anonymous survey) may produce data that are personally identifiable or that may be potentially personally identifiable. spatial information is not unique in being either identifiable or linkable. however, the manner in which spatial information canbecome linked with identifiable data or may create identifiable data differsin practice from that for other types of data in both overt and subtle ways.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.82appendix ain general, data about individuals are growing more identifiable asmore information is collected, maintained, and available for public andprivate uses. technological developments also contribute to the increasingidentifiability of data that do not have overt identifiers. spatial informationhas both of these characteristics, more data and better technology. linkingspatial information to research data can affect promises of confidentialitythat were made at the time of data collection and in ways that were notforeseeable at that time. these are some of the challenges presented by theuse of spatial information.two preliminary observations about the complexity of privacy regulation are in order. first, privacy regulation can be highly variable and unpredictable in application. in the united states, privacy standards establishedby statute may differ depending on the extent to which the information isidentifiable, the type of information, the identity of the record keeper, theidentity of the user, the purpose for which the information was collected oris being used, the type of technology employed, and other elements. forsome information activities, such as surveillance, additional factors may berelevant, including the manner in which information is stored or transmitted, the location being surveilled, the place from which the surveillance isdone, and the nationality of the target. this list of factors is not exhaustive.second, american privacy regulation is often nonexistent. privacy statutes are often responsive to widely reported horror stories, and there arehuge gaps in statutory protections for privacy. for many types of personalinformation, many categories of record keepers, and many types of information collection and disclosure activities, no privacy rules apply. furthermore, where regulation exists, information can sometimes be transferredfrom a regulated to a nonregulated environment. a person in possession ofinformation regulated for privacy may be able to disclose the informationto a third party who is beyond the regulatory scheme. common law standards may apply at times, but they rarely provide clear guidance.the paper begins by discussing terminology, particularly distinctionsbetween privacy and confidentiality, and considers privacy as it is addressedin legislation, administrative process, professional standards, and litigationin the united states. major legal and policy issues considered are identifiability of personal data, data collection limitations, disclosure rules, andliability for misuse of data.a note on terminologyprivacy and confidentiality are troublesome terms because neither has auniversally recognized definition. while broad definitions can be found,none is enlightening because definitions are at too high a level of abstraction and never offer operational guidance applicable in all contexts. neverputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.privacy for research data83theless, because the terms are impossible to avoid, some clarification isappropriate.privacy is generally an attribute of individuals. while some foreignlaws grant privacy rights to legal persons (e.g., corporations) as well asindividuals, american usage usually ties privacy interests to individuals.that usage will be followed in this paper.the scope of privacy interests that should be recognized is much debated. philosophers, sociologists, economists, physicians, lawyers, and others have different views on the goals and meaning of privacy protection. forpresent purposes, however, the focus is primarily on the privacy of personalinformation. europeans and others refer to this aspect of privacy as dataprotection.the most universally recognized statement of information privacypolicy comes from a 1980 document about fair information practices (fips)from the organisation for economic cooperation and development.1while this statement is not free from controversy, fips provide a highlyuseful framework for discussing and evaluating information privacy matters. fips are useful because the principles define the elements of privacy insome detail, and the details are crucial. the implementation of fips in anycontext will vary because the principles are broad and not selfexecuting.applying fips is as much art as science.confidentiality is an attribute that can apply to individuals and to legalpersons. both personal information and business information may be confidential. however, the precise meaning of that designation is often unclear.statutes often designate information with the singleword descriptor ofconfidential. however, these laws routinely fail to define the scope of confidentiality, the obligations of record keepers, or the rights of record subjects or third parties. those who maintain statutorily designated confidential records may have to decide on their own if they can disclose informationto contractors, to police, to researchers, when required by other statutes, inresponse to a subpoena, when requested by the data subject, or otherwise.standards for data collection, security, data quality, accountability, or access and correction rights are typically wholly unaddressed.statutes that protect business information from disclosure suffer fromthe same lack of specificity. the federal freedom of information act (foia)allows agencies to withhold òtrade secrets and commercial or financialinformation obtained from a person and privileged or confidential.ó2 eachof the terms in this phrase has been the subject of litigation, and differentcourts have reached significantly different interpretations of what constitutes confidential business information.categories of data held by government agencies sometimes have a designation suggesting or imposing a degree of secrecy. under the executiveorder on security classification,3 confidential is one of three terms with aputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.84appendix adefined scope and process for designation of information that requiresprotection in the interests of national defense and foreign policy. the otherterms are secret and top secret. however, many other terms used by federalagencies (e.g., òfor official use onlyó or òsensitive but unclassifiedó) tocategorize information as having some degree of confidentiality have nodefined standards.the term confidential is much harder to encircle with a definition,whether in whole or in part. it retains a useful meaning as broadly descriptive of information of any type that may not be appropriate for unrestrictedpublic disclosure. unadorned, however, a confidential designation cannotbe taken as a useful descriptor of rights and responsibilities. it offers asentiment and not a standard.the terms privacy and confidentiality will not, by themselves, informanyone of the proper way to process information or balance the interests ofthe parties to information collection, maintenance, use, or disclosure. inany context, the propriety and legality of any type of information processing must be judged by legal standards when applicable or by other standards, be they ethical, social, or local.local standards may arise from promises made by those who collectand use personal data. standards may be found, for example, in websiteprivacy policies or in promises made by researchers as part of the informedconsent process. in nearly all cases, broad promises of confidentiality maycreate expectations that record keepers may not be able to fulfill. the lawsthat may allow or require disclosure of records to third partiesñand particularly the federal governmentñcreate a reality that cannot be hiddenbehind a general promise of confidentiality. other aspects of privacy (i.e.,fips) may also require careful delineation. the vagueness of commonlyused terminology increases the need for clarity and specificity.identifiability and privacyinformation privacy laws protect personal privacy interests by regulating the collection, maintenance, use, and disclosure of personal information. the protection of identifiable individuals is a principal goal of theselaws.4 usually, it is apparent when information relates to an identifiableindividual because it includes a name, address, identification number, orother overt identifier associated with a specific individual. personal information that cannot be linked to a specific individual typically falls outsidethe scope of privacy regulation. however, the line between the regulatedand the unregulated is not always clear.removing overt identifiers does not ensure that the remaining information is no longer identifiable. data not expressly associated with a specificindividual may nevertheless be linked to that individual under some condiputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.privacy for research data85tions. it may not always be easy to predict in advance when deidentified5data can be linked. factors that affect the identifiability of informationabout individuals include unique or unusual data elements; the number ofavailable nonunique data elements about the data subject; specific knowledge about the data subject already in the possession of an observer; the sizeof the population that includes the data subject; the amount of time andeffort that an observer is willing to devote to the identification effort; andthe volume of identifiable information about the population that includesthe subject of the data.in recent decades, the volume of generally available information aboutindividuals has expanded greatly. partly because of an absence of generalprivacy laws, the united states is the world leader in the commercial collection, compilation, and exploitation of personal data. american marketersand data brokers routinely combine identifiable public records (e.g., voterregisters, occupational licenses, property ownership and tax records, courtrecords), identifiable commercial data (e.g., transaction information), andnonidentifiable data (e.g., census data). they use the data to create fornearly every individual and household a profile that includes name, address, telephone number, educational level, homeownership, mail buyingpropensity, credit card usage, income level, marital status, age, children,and lifestyle indicators that show whether an individual is a gardener,reader, golfer, etc.6 records used for credit purposes are regulated by thefair credit reporting act,7 but other consumer data compilations aremostly unregulated for privacy. as the amount of available personal dataincreases, it becomes less likely that nonidentifiable data will remainnonidentifiable. latanya sweeney, a noted expert on identifiability, hassaid: òi can never guarantee that any release of data is anonymous, eventhough for a particular user it may very well be anonymous.ó8for the statistician or researcher, identifiability of personal data israrely a black and white concept. whether a set of data is identifiable candepend on the characteristics of the set itself, on factors wholly external tothe set, or on the identity of the observer. data that cannot be identified byone person may be identifiable by another, perhaps because of differentskills or because of access to different information sources. furthermore,identifiability is not a static characteristic. data not identifiable today maybe identifiable tomorrow because of developments remote from the originalsource of the data or the current holder of the data. as the availability ofgeospatial and other information increases, the ability to link whollynonidentifiable data or deidentified data with specific individuals will alsoincrease.from a legislative perspective, however, identifiability is more likely tobe a black and white concept. privacy legislation tends to provide expressregulation for identifiable data and nonregulation for nonidentifiable data,putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.86appendix awithout any recognition of a middle ground. however, statutes do not yetgenerally reflect a sophisticated understanding of the issues. until recently,policy makers outside the statistical community paid relatively little attention to the possibility of reidentification. nevertheless, a selective review oflaws and rules illustrates the range of policy choices to date.u.s. legislative standardsthe privacy act of 1974,9 a u.s. law applicable mostly to federalagencies, defines record to mean a grouping of information about an individual that contains òhis name, or the identifying number, symbol, or otheridentifying particular assigned to the individual, such as a finger or voiceprint or a photograph.ó10 an identifier is an essential part of a record. theability to infer identity or to reidentify a record is not sufficient or relevant.a location may or may not be an identifier under the privacy act. ahome address associated with a name is unquestionably an identifier. ahome address without any other data element could be an identifier if onlyone individual lives at the address, but it might not be if more than oneindividual lives there. as data elements are added to the address, the context may affect whether the information is an identifier and whether the actapplies. if the information associated with the address is about the property(ò2,000 square feetó), then the information is probably not identifyinginformation about an individual. if the information is about the resident(òleaves for work every day at 8:00 a.m.ó), it is more likely to be found tobe identifying information. part of the uncertainty here is that there is asplit in the courts about how to interpret the actõs concept of what ispersonal information. the difference does not relate specifically to locationinformation, and the details are not enlightening.however, the question of when a location qualifies as an identifier is anissue that could arise outside the narrow and somewhat loosely draftedprivacy act of 1974.11 if a location is unassociated with an individual, thenit is less likely to raise a privacy issue. however, it may be possible toassociate location information with an individual, so that the addition oflocation data to other nonidentifiable data elements may make it easier toidentify a specific individual.other federal laws are generally unenlightening on identifiability questions. neither the driverõs privacy protection act12 nor the video privacyprotection act13 addresses identifiability in any useful way. the cablecommunications policy act excludes from its definition of personally identifiable information òany record of aggregate data which does not identifyparticular persons.ó14 this exclusion, which probably addressed a politicalissue rather than a statistical one, raises as many questions as it answers.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.privacy for research data87congress took a more sophisticated approach to identifiability in the confidential information protection and statistical efficiency act of 2002(cipsea).15 the law defines identifiable form to mean òany representation ofinformation that permits the identity of the respondent to whom the information applies to be reasonably inferred by either direct or indirect means.ó thislanguage is probably the result of the involvement of the statistical communityin the development of the legislation. the standard is a reasonableness standard, and some international examples of reasonableness standards will bedescribed shortly. cipseaõs definition recognizes the possibility of using indirect inferences to permit identification, but it does not indicate the scope ofeffort that is necessary to render deidentified data identifiable. that may besubsumed within the overall concept of reasonableness.no standardnational privacy laws elsewhere do not always include guidance aboutidentifiability. canadaõs personal information protection and electronicdocuments act (pipeda) defines personal information as òinformationabout an identifiable individual.ó16 the act includes no standard for determining identifiability or anonymity, and it does not address the issue ofreidentification. a treatise on the act suggests that òcaution should beexercised in determining what is truly ôanonymousõ information since theavailability of external information in automated format may facilitate thereidentification of information that has been made anonymous.ó17strict standardthe 1978 french data protection law defines information as ònominativeó if in any way it directly or indirectly permits the identification of anatural person.18 according to an independent analysis, òthe french lawmakes no distinction between information that can easily be linked to anindividual and information that can only be linked with extraordinary meansor with the cooperation of third parties.ó19 the french approach does notappear to recognize any intermediate possibility between identifiable andanonymous. unless personal data in france are wholly nonidentifiable, theyappear to remain fully subject to privacy rules. this approach may providegreater clarity, but the results could be harsh in practice if data only theoretically identifiable fall under the regulatory scheme for personal data. however, the french data protection law includes several provisions that appearto ameliorate the potentially harsh results.20putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.88appendix areasonableness standardsthe definition of personal data in the european union (eu) data protection directive refers to an identifiable natural person as òan individualperson . . . who can be identified, directly or indirectly.ó21 on the surface,the eu definition appears to be similar to the strict standard in french law.however, the directiveõs introductory recital 26 suggests a softer intentwhen it states that privacy rules will not apply to òdata rendered anonymous in such a way that the data subject is no longer identifiable.ó it alsoprovides that òto determine whether a person is identifiable, account shouldbe taken of all the means likely reasonably to be used either by the controller or by any other person to identify the said person.ó22 thus, the directiveoffers a reasonableness standard for determining whether data have beenadequately deidentified.variations on a reasonableness standard can be found elsewhere. thecouncil of europeõs recommendations on medical data privacy providethat an individual is not identifiable òif identification requires an unreasonable amount of time and manpower.ó23 an accompanying explanatorymemorandum says that costs are no longer a reliable criterion for determining identifiability because of developments in computer technology.24 however, it is unclear why òtime and manpoweró are not just a proxy for costs.the australian privacy act defines personal information to mean òinformation . . . about an individual whose identity is apparent, or canreasonably be ascertained, from the information.ó25 it appears on the surface that a decision about identifiability is limited to determinations fromthe information itself and not from other sources. this language highlightsthe general question of just what activities and persons are included withinthe scope of a reasonableness determination inquiry. under the eu directive, it is clear that identification action taken by any person is relevant. thecouncil of europe uses a time and manpower measure, but without defining who might make the identification effort. the australian law appears tolimit the question to inferences from the information itself. the extent towhich these differences are significantly different in application or intent isnot clear.the british data protection actõs definition of personal data coversdata about an individual who can be identified thereby or through òotherinformation which is in the possession of, or is likely to come into thepossession of, the data controller.ó26 the british standard does not expressly rely on reasonableness or on the effort required to reidentify data. itbases an identifiability determination more narrowly by focusing on information that a data controller has or is likely to acquire. this appears to beonly a step removed from an express reasonableness test.the canadian institutes of health research (cihr) proposed a clarifiputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.privacy for research data89cation of the definition of personal information from pipeda that mayoffer the most specific example of a reasonableness standard.27 the cihrlanguage refers to òa reasonably foreseeable methodó of identification orlinking of data with a specific individual. it also refers to anonymizedinformation òpermanently strippedó of all identifiers such that the information has òno reasonable potential for any organization to make an identification.ó in addition, the cihr proposal provides that reasonably foreseeability shall òbe assessed with regard to the circumstances prevailing at thetime of the proposed collection, use or disclosure.óadministrative processthe alberta health information act takes a different approach. itdefines individually identifying to mean when a data subject òcan be readilyascertained from the information,ó28 and it defines nonidentifying to meanthat the identity of the data subject òcannot be readily ascertained from theinformation.ó29 this appears to limit the identifiability inquiry to the information itself.albertaõs innovation comes in its regulation of data matching,30 whichis the creation of individually identifying health information by combiningindividually identifying or nonidentifying health information or other information from two or more electronic databases without the consent ofthe data subjects. the data matching requirements, which attach to anyoneattempting to reidentify nonidentifying health information, include submission of a privacy impact assessment to the commissioner for review andcomment.31the alberta law is different because it expressly addressesreidentification activities by anyone (at least, anyone using any electronicdatabases). in place of a fixed standard for determining whether identifiable information is at stake, the act substitutes an administrative process.32the law regulates conduct more than information, thereby evading thedefinitional problem for information that is neither clearly identifiable norwholly nonidentifiable.data elements and professional judgment standardsin the united states, general federal health privacy standards derivefrom a rule33 issued by the department of health and human servicesunder the authority of the health insurance portability and accountabilityact34 (hipaa). the rule defines individually identifiable health information to include health information for which there is a reasonable basis tobelieve that the information can be used to identify an individual.35 this isan example of a reasonableness standard that by itself provides little interputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.90appendix apretative guidance. hipaaõs approach to identifiability does not end withthis definition, however. hipaa offers what may be the most sophisticatedapproach to identifiability found in any privacy law.the rule offers two independent methods to turn identifiable (regulated) data into deidentified (unregulated) data. the first method requiresremoval of 18 specific categories of data elements.36 with these elementsremoved, any risk of reidentification is deemed too small to be a concern.the hipaa rule no longer applies to the stripped data, which can then beused and disclosed free of hipaa obligations. the only condition is thatthe covered entity does not have actual knowledge that the informationcould be used, either on its own or in combination with other data, toidentify an individual.37 the advantage of this socalled safe harbor methodis that mechanical application of the rule produces data that can nearlyalways be treated as wholly nonidentifiable. some critics claim that theresulting data are useless for many purposes.the second way to create deidentified (unregulated) health data requires a determination by òa person with appropriate knowledge of andexperience with generally accepted statistical and scientific principles andmethods for rendering information not individually identifiable.ó38 therequired determination must be that òthe risk is very small that the information could be used, alone or in combination with other reasonably available information, by an anticipated recipient to identify an individual whois a subject of the information.ó39 the person making the determinationmust document the methods used and the results of the analysis on whichthe determination is based.40hipaa includes another procedure for disclosure of a limited datasetthat does not include overt identifiers but that has more data elements thanthe safe harbor method. in order to receive a limited dataset, the recipientmust agree to a data use agreement that establishes how the data may beused and disclosed, requires appropriate safeguards, and sets other termsfor processing.41 disclosures under the limited dataset procedure can bemade only for activities related to research, public health, and health careoperations. a recipient under this procedure is not by virtue of the receiptsubject to hipaa or accountable to the secretary of health and humanservices, but the agreement might be enforced by the covered entity thatdisclosed the data or, perhaps, by a data subject.litigationidentifiability issues have arisen in a few court cases.¥one u.s. case involved a commercial dispute between two largehealth data processing companies. webmd purchased a company (envoy)putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.privacy for research data91from quintiles in 2000. as part of the acquisition, webmd agreed tosupply quintiles with nonidentifiable patient claims data processed by envoy. quintiles processes large volumes of data to assess the usage of prescription drugs. quintiles sells the resulting information in nonidentifiableform primarily to pharmaceutical manufacturers. the litigation arose because of concerns by webmd that the combination of its data with identifiable data otherwise in the possession of quintiles would allowreidentification.42 the resolution of this dispute did not involve a ruling onthe identifiability issues raised, but it may be a precursor to other similarbattles.¥a united kingdom case43 involving identifiability began with apolicy document issued by the british department of health. the documentexpressly stated that stripping of identifiers from patient information before disclosure to private data companies seeking information on the habitsof physicians is not sufficient to avoid a breach of the physicianõs duty ofconfidentiality. even the disclosure of aggregated data would be a violationof confidentiality. a company that obtains prescription data identifiable tophysicians and not patients sued to overturn the policy. the lower courtfound that disclosure of patient information was a breach of confidencenotwithstanding the anonymization. however, an appellate court foundthe reverse and overturned the department policy. both courts proceededon the theory that either personal data were identifiable, or they were not.neither opinion recognized or discussed any middle ground.¥an illinois case arose under the state freedom of information actwhen a newspaper requested information from the illinois cancer registryby type of cancer, zip code, and date of diagnosis.44 the registry denied therequest because another statute prohibits the public disclosure of any groupof facts that tends to lead to the identity of any person in the registry. thecourt reversed and ordered the data disclosed. although an expert witnesswas able to identify most of the records involved, the court was not convinced. the court held that the òevidence does not concretely and conclusively demonstrate that a threat exists that other individuals, even thosewith skills approaching those of dr. sweeney, likewise would be able toidentify the subjects or what the magnitude of such a threat would be, if itexisted.ó the illinois supreme court upheld the decision in 2006.45¥litigation over the constitutionality of a federal law prohibiting socalled partial birth abortions produced a noteworthy decision on identifiability.46 the specific dispute was over disclosure during discovery of patient records maintained by physicians testifying as expert witnesses. therecords were to be deidentified before disclosure so that a patientõs identitycould not reasonably be ascertained. the case was decided in part ongrounds that there is still a privacy interest even if there were no possibilitythat the patientõs identity could be determined.47 arguments that whollyputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.92appendix anonidentifiable records retain a privacy interest are unusual, and the conclusion is all the more remarkable because the judge (richard posner) is awellknown critic of privacy.conclusionexisting statutes and rules that address deidentification matters can becategorized roughly into three groups. one category establishes standardsfor determining whether data are sufficiently or potentially identifiable towarrant regulation. the standards can (a) be inwardlooking (consideringonly the data themselves); (b) be outwardlooking (considering other dataactually or potentially available elsewhere as well as the capabilities forreidentification generally available to individuals or experts); (c) requireprofessional statistical judgment; or (d) consider the time, effort, or costrequired for reidentification. this is not an exhaustive list, and multiplestandards may apply at the same time.the second category involves an administrative process. the albertalaw requires an administrative review for privacy of some plannedreidentification activities. an administrative process could also reviewdeidentification efforts. other forms of notice, review, and even approvalare possible as well, but the alberta law is the only known example to date.the third category is a mechanical rule requiring the removal of specified data elements. while the first two categories are not exclusiveñit ispossible to have a standard and a process together, for exampleña mechanical rule could be a complete alternative for a standard or a process, ashipaa illustrates.statutes, both domestic and international, are all over the lot. the significance of the differences among the various legislative provisions on identifiability is uncertain. it is not clear how much attention legislators paid toidentifiability standards, and the statutes may simply offer alternate wordformulas produced without much consideration. better legislative standardson identifiability do not appear to be on anyoneõs agenda at present.the few court decisions in the area are no better than the statutes. theabortion records case and the illinois cancer registry decision reach conclusions that are hard to reconcile. one case found a privacy interest in whollynonidentifiable data, and the other found no privacy interest in supposedlydeidentified records that an expert proved were identifiable. it may be sometime before the courts understand the basic issues or produce any meaningful standards on identifiability.finally, none of the statutes or court cases expressly addresses locationinformation. location information is just another data element that maycontribute to the identifiability of personal data.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.privacy for research data93collectiona second major privacy concern arises with the collection of personalinformation. in the united states, what personal information may be collected depends on who is doing the collection and what methods are beingused. however, much actual and potential personal data collection is unregulated, especially for private parties. for example, many merchants collect transaction and other information from data subjects and from a largeindustry of data brokers, mailing list purveyors, and other commercialfirms. even the collection of information from web users through spywarewas not clearly or expressly illegal anywhere a few years ago, althoughsome spyware may violate unfair and deceptive trade practices laws. inmany other countries, however, general standards for collection exist aspart of broadly applicable data protection laws, and the collection standards apply generally to all public and private record keepers.video surveillancevideo (and visual) surveillance is of particular interest because it hasthe capability of recording location in addition to other data elements.except for surveillance by the government for law enforcement purposes,however, there is little law on video surveillance or the data produced byvideo surveillance. the lengthy description here is intended to describestandards for personal information collection for arguably public data elements that might apply when statutes are rare or nonexistent.u.s. laws and policies for all types of surveillance lack clarity, coherence, consistency, compactness, and currency.48 the rules governing surveillance vary depending on numerous factors. general surveillance jurisprudence in the united states is extensive for criminal matters, and thefourth amendment provides important standards for government actions.surveillance by private parties (other than wiretapping49) is only occasionally statutorily regulated, but it maybe actionable through a privacy tort.for all types of visual surveillance, the most important factors are whetherit takes place in a public or private place and whether there is a reasonableexpectation of privacy. a general rule of thumb (with some exceptions) isthat visual surveillance in public space is not restricted.supreme court decisionsin katz v. united states,50 the main issue was whether to allow evidence of a telephone conversation overheard by government agents whoattached an electronic device to a public telephone booth made of glass.the supreme court decided that the surveillance was subject to fourthputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.94appendix aamendment protection, meaning that the surveillance needed a court order.importantly, the court held that the fourth amendment protects peopleand not places. still, the court said that ò[w]hat a person knowingly exposes to the public, even in his own home or office, is not a subject offourth amendment protection.ó51 this statement suggests almost directlythat the fourth amendment does not protect surveillance in public places.however, the court did not decide that issue expressly.in a concurring opinion, justice john m. harlan offered a test nowwidely used to assess when privacy should fall under the protections of thefourth amendment. under the test, a reasonable expectation of privacyexists if (1) a person has exhibited an actual (subjective) expectation ofprivacy and (2) that expectation is one that society is prepared to recognizeas reasonable.52 when this test is satisfied, a government search or surveillance activity that violates the reasonable expectation of privacy falls underthe fourth amendment. a wellrecognized problem with the reasonableexpectation of privacy test is the òsilent ability of technology to erode ourexpectations of privacy.ó53in united states v. knotts,54 the government surreptitiously attachedan electronic beeper to an item purchased by a suspect and transported inhis car. the court held that òa person traveling in an automobile on publicthoroughfares has no reasonable expectation of privacy in his movementsfrom one place to another.ó55 knotts implies that virtually any type ofvisual surveillance in a public place is free of fourth amendment constraints. aware that its decision might be read to allow unrestricted publicplace surveillance, the court said that òdragnettype law enforcement practicesó will be considered when they arise.56in california v. ciraolo,57 police officers in a private airplane flew overa house at an altitude of 1,000 feet and saw marijuana growing in the yard.the issue for the supreme court was whether the warrantless aerial observation of a fenced yard adjacent to a home violated the fourth amendment.privacy in a home receives the highest degree of fourth amendment protection. however, the court concluded that observation of the yard frompublicly navigable airspace was not unreasonable and that there was nofourth amendment protection.dow chemical company v. united states58 involved government aerialobservation of a large chemical complex with security that barred groundlevel public views and limited scrutiny from the air. the supreme courtheld that the complex fell under the doctrine of open fields, so aerial photographs from navigable airspace are not a fourth amendment search. thecourt suggested (but did not decide) that use of òhighly sophisticated surveillance equipment not generally available to the public, such as satellitetechnology, might be constitutionally proscribed absent a warrant.ó59 thisdecision came in 1986, long before satellite photos were available to everyputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.privacy for research data95internet user. both this case and the preceding case (ciraolo) were decidedby 5 to 4 majorities.60video surveillance statutes generallystatutes on video surveillance by private parties are rare but increasing.recent years have seen a wave of legislation prohibiting video voyeurism.washington state provides an example. prior to a 2003 amendment, astatute defined the crime of voyeurism as viewing, photographing, or filming another person without that personõs knowledge or consent, while theperson is in a place where he or she would have a reasonable expectation ofprivacy.61 the law defined a place where an individual would have areasonable expectation of privacy as being (1) a place where a reasonableperson could disrobe in privacy without being concerned about being photographed or (2) a place where a person may reasonably expect to be safefrom casual or hostile intrusion or surveillance.62the law had to be changed when the state supreme court overturnedthe conviction of defendants who filmed in public places using a groundlevel camera to take photographs up the skirts of women. the socalledupskirt photography took place in public, where there was no expectationof privacy. the state legislature quickly amended the statute, making it acrime to view, photograph, or film the intimate areas of another personwithout that personõs knowledge and consent under circumstances in whichthe person has a reasonable expectation of privacy, whether in a public orprivate place.63 a roughly comparable arizona law, however, has an exception for use of a child monitoring device,64 sometimes called a nannycam.other state laws regulate videotaping in particular circumstances. aconnecticut law prohibits employers from operating electronic surveillancedevices in employee restrooms, locker rooms, or lounges.65 texas passed asocalled granny cam law in 2001 that allows a nursing home resident òtoplace in the residentõs room an electronic monitoring device that is ownedand operated by the resident or provided by the residentõs guardian.ó66some laws regulate cameras to catch red light running and cameras forracial profiling oversight.privacy tortsvideo surveillance can constitute an invasion of privacy that is actionable through a private lawsuit under state laws, but state laws can varyconsiderably. many states have adopted some policies from the restatement of torts (second). the restatement defines four types of privacyinvasions, of which unreasonable intrusion upon the seclusion of another isputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.96appendix athe most important for surveillance purposes.67 this tort does not dependon any publicity given to the person whose interest is invaded.68 for theother privacy torts, actionable activities derive from the use to which aname, image, or information is put. under the intrusion tort, mere surveillance can, under the right circumstances, give rise to a cause of action.the restatement is clear that the intrusion must occur in a private placeor must otherwise invade a private seclusion that an individual has established for his or her person or affairs. the restatement expressly excludesthe possibility of liability for taking a photograph while an individual iswalking on a public highway. even in public, however, some matters aboutan individual ònot exhibited to the public gazeó can be actionable. forexample, photographing someoneõs underwear or lack of it could be invasive and actionable as a tort, regardless of a criminal statute.69the public/private distinction so important to fourth amendment jurisprudence is equally important to the tort of intrusion upon seclusion.surveillance of a public place, house, yard, car parked in a public place, atan airport counter, and at similar places would not give rise to liability.surveillance in a private area, such as a dressing room or bathroom, couldcreate liability.tort law recognizes some limits, however. several precedents find liability for invasion of privacy even though the surveillance took placeentirely in public space. thus, unreasonable or intrusive surveillance ofpersonal injury defendants will give rise to a claim for invasion of privacy.consumer advocate ralph nader successfully sued general motors forsurveilling him and invading his privacy while in public.70 jacquelinekennedy onassis sued a paparazzo who aggressively followed and photographed her and her children.71 finding that the photographer insinuatedhimself into the very fabric of mrs. onassisõs life, the court issued a detailedinjunction limiting the photographer from approaching her. extrapolatingfrom the nader and onassis cases is difficult, however.even regular surveillance of a particular individual may not alwayssupport an actionable invasion of privacy. in personal injury cases, forexample, it has become common for an insurance company to hire a privateinvestigator to determine the extent of a victimõs injuries through surveillance. this type of surveillance is not always invasive, and the courts recognize it as a consequence of filing injury claims.the use of tort law in response to unreasonable surveillance activities,even in public space, has a firm basis. however, the border between reasonable and unreasonable activities remains uncertain, depending on the factsof each case and the intent of the person conducting the surveillance.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.privacy for research data97conclusionthe first issue in assessing the legality of surveillance is whether thesurveillance is being done by the government or by a private actor. rulesregulating government surveillance are exquisitely complex, and rules governing private surveillance are mostly nonexistent. for both types of surveillance, however, the two most important factors in distinguishing permissible from impermissible visual surveillance are whether the area beingsurveilled is public or private and whether there is a reasonable expectationof privacy. however, many questions about the legitimate use of visualsurveillance remain unanswered because courts and legislatures often trailtechnological developments. for the most part, however, there is almost nolaw that regulates visual surveillance in general or in public places. theimplication in knotts that virtually any type of visual surveillance in apublic place is free of fourth amendment constraints is not an assurancethat anything goes for the government, but that may well be the result, atleast when an exotic technology is not employed. for private activity, alawsuit over visual surveillance in public places is always possible, but itmight be difficult for a plaintiff to win in the absence of a lewd intent orother showing of bad faith.the extent to which physical or camera surveillance of an individual isdifferent from the association of location information with an individual isnot clear. there is a qualitative difference between being followed or filmed,on one hand, and being tracked electronically with locations recorded(whether continuously or otherwise), on the other.whether the association of geocoding with other types of personal datawould create any legally recognized violations of privacy is impossible tosay. none of the existing precedents is directly on point, and much woulddepend on facts, intent, methods, locations (public or private), expectations, and uses. consider the possibility that compiled information wouldcreate evidence of a crime, produce a record that would break up a marriage, something that would embarrass a public figure, disclose sensitivemedical information (e.g., entering a drug abuse clinic), or constitutegrounds for losing a job.a collection of information that violated an agreement or understanding reached with a research subject might be actionable under several different legal theories, including contract law and tort law. the tort for intrusion upon seclusion is most relevant because it is not dependent on publicity(i.e., use of the information) given to the person whose interest is invaded.the mere collection of information could be enough to sustain a lawsuit.however, proving damages in privacy cases is often challenging, and itcould present a significant barrier to recovery in an intrusion. recoveringdamages from a researcher would be difficult in many foreseeable factualputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.98appendix acircumstances. however, ultimate success in litigation might provide limited comfort to a researcher obliged to pay for and live through a lawsuit.technology constantly changes the nature of surveillance and blurs thedistinctions between traditional categories. cell phones may (or may not)provide an example of a form of surveillance that is similar to but not quitethe same as visual surveillance. physically following an individual in publicspace is visual surveillance. tracking an automobile with a beeper inside onpublic highways is also visual surveillance. tracking an individual in publicspace by means of a cell phone may be different, especially if the phone isnot in plain sight. this distinction between visually following an individualand using a cell phone as a tracking device may be important in a criminalcontext, and the courts are beginning to pay attention.72 however, criminal jurisprudence is not likely to be of great relevance to researchers.commercial tracking of cell phone locations73 may produce locationinformation, but the availability of tracking information for secondarypurposes is unknown and likely to be controlled by service contracts. theredo not appear to be any statutes expressly regulating the use of cell phonelocation information for private purposes. it is common for private repositories of personal information to exist without any statutory regulation.marketers have voracious appetites for personal data, and they may be amarket for using or acquiring location information.eu data protection directivemost national privacy laws implement internationally recognized fairinformation practice principles. the principle for collection limitation statesòthat there should be limits to the collection of personal data, that datashould be collected by lawful and fair means, and that data should becollected, where appropriate, with the knowledge or consent of the subject.ó74the eu data protection directive75 implements this policy throughseveral provisions.76 article 6(1)(b) requires member states to provide thatpersonal data must becollected for specified, explicit and legitimate purposes and not furtherprocessed in a way incompatible with those purposes. further processingof data for historical, statistical or scientific purposes shall not be considered as incompatible provided that member states provide appropriatesafeguards.this policy is far removed from the anythinggoes approach to personalinformation collection usually found in the united states in the absence ofa statute that provides otherwise. in europe, the purposes for collection andputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.privacy for research data99processing must be specific, explicit, and legitimate. that means, amongother things, that a data controller must define purposes in advance. secondary uses may not be incompatible with the stated purposes, and that isa weaker test than an affirmative requirement that secondary uses be compatible.this provision of the directive provides that processing for historical,statistical, or scientific purposes does not violate the compatibility standardwith additional safeguards. that allows disclosures of personal informationto researchers and others, but it does not exempt the recipients from complying with data protection standards for the data they are processing.the directive also requires as a condition of personal data processing(including collection and disclosure) that the data subject has given consentunambiguously. exceptions to consent include if processing is necessary forthe performance of a contract, to comply with a legal obligation, to protectthe vital interests of the data subject, to carry out a task in the publicinterest, or for the purposes of the legitimate interests pursued by the controller.77 there are more terms and conditions to these exceptions.european organizations cannot make unrestricted decisions about whatto collect. in particular, the last justification for processingñfor the purposes of the legitimate interests pursued by the controllerñis worthy ofadditional discussion. it applies except when the data controllerõs interestsare overridden by the interests or fundamental rights and freedoms of thedata subject. the specific balance between the use of information for legitimate ordinary business activities (including but not limited to marketing) issomething left to member states to decide. the policy allows considerableflexibility in implementation. for example, great britain implements theprinciple by giving individuals a limited right to prevent processing likely tocause damage or distress and an absolute right to prevent processing forpurposes of direct marketing.78 in the united states, by contrast, there is nogeneral right to optout of collection, marketing, or other types of processing. some specific statutes grant limited rights to prevent some uses. somecompanies have adopted privacy policies that grant greater rights to datasubjects.one distinction that is important when comparing statutory standardsacross jurisdictions is the breadth of privacy laws. in countries with omnibus privacy laws, all data controllers are likely to be subject to privacyregulation for identifiable data. thus, a person who takes deidentified dataand reidentifies them is likely to fall under the privacy regulatory schemegenerally applicable to all record keepers immediately upon completion ofthe reidentification. the effect is that a european researcher who may haveescaped data protection regulation because of the absence of identifiabledata may become subject to regulation by linking that data with additionalgeographical or other data.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.100appendix ain the united states, however, unless a law directly regulates an entityõsinformation processing activities, it is unlikely that any privacy restrictionswill apply. the u.s. health privacy rule known as hipaa offers an illustration.79 the rule regulates the use of individually identifiable health information only by covered entities, which are most health care providers andall health plans (insurers) and health care clearinghouses. others who obtain and use health data and who are not operating as covered entities (or astheir business associates) are not affected by the rule in their processingactivities. thus, a researcher, public health department, or court may obtain regulated health data (under specified standards/procedures) withoutbecoming subject to the hipaa rule.selected u.s. statutes limiting collection of personal informationnot all existing u.s. privacy statutes limit the collection of personalinformation. a few examples of collection restrictions illustrate the diversity that exists among the laws.privacy act of 1974the privacy act of 1974,80 a law that applies only to federal government agencies and to a few government contractors (but no grantees),regulates collection in several ways. first, it allows agencies to maintainonly information about an individual as is relevant and necessary to accomplish an agency purpose. second, it requires agencies to collect informationto the greatest extent practicable directly from the data subject if an adversedetermination may result. third, it prohibits the maintenance of information describing how an individual exercises any right guaranteed by thefirst amendment, unless authorized by statute or pertinent to an authorized law enforcement activity.81 for a researcher working for a federalagency who collects and links geographic data, the first two restrictions arenot likely to be meaningful, and the third would be relevant only in narrowinstances (such as tracking individuals at a political demonstration).health insurance portability and accountability actthe federal health privacy rule, issued by the u.s. department of healthand human services under the authority of the health insurance portability and accountability act (hipaa), reflects generally recognized fair information practices,82 except that information collection is barely mentioned.the apparent policy is to avoid dictating to health care providers whatinformation they can and cannot collect when treating patients. the onlylimited exception comes with the application of the hipaa privacy ruleõsputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.privacy for research data101minimum necessary standard.83 in general, the rule seeks to limit uses of,disclosures of, and requests for personal health information to the minimum necessary to accomplish the purpose of the use, disclosure, or request.the minimum necessary rule has several exceptions, including a broad onefor treatment activities. the rule directs a covered entity requesting personal health information from another covered entity to make reasonableefforts to limit the information requested to the minimum necessary toaccomplish the intended purpose of the request. data collection from a datasubject or from any source other than another covered entity is not restricted by the minimum necessary rule.childrenõs online privacy protection actthe childrenõs online privacy protection act (coppa)84 makes itunlawful for a website operator to collect personal information from achild under the age of 13 without obtaining verifiable parental consent.personal information includes a physical address. the law appears to applyto website operators located anywhere in the world. the law does notrestrict collection of information by phone, fax, or other means or fromolder children.cable communications policy actcable television operators may not use their cable system to collectpersonally identifiable information concerning a subscriber without consent.85 exceptions cover service and theft detection activities. the law doesnot otherwise restrict collection, but it does restrict disclosure.conclusionno general statute regulates personal information collection in theunited states. a few u.s. laws restrict the collection of personal information in narrow contexts. the collection of personal informationñincludinginformation from public sources, from private companies, by direct observation, or by linking of data from disparate sourcesñis only rarely thesubject of overt regulation. legal challenges to the mere collection of information are likely to be hard to mount in the absence of legislation, butchallenges are not impossible. when collected information is used or disclosed, however, different standards are likely to apply than apply to themere collection of data. use and disclosure regulations, while still rare, arefound more frequently. no known federal law expressly addresses the collection of location information.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.102appendix adisclosurea third major privacy concern is the disclosure of personal information. in the united states, disclosure is only sometimes subject to regulation. for many record keepers, the only limits on disclosure come fromcontracts with data subjects, the possibility of tort lawsuits, or marketpressure. many commercial and other institutions collect and disclose personal information without the knowledge or consent of the data subjects.some record keepers are subject to privacy or other laws with disclosure restrictions. researchers are typically subject to human subject protection rules and to oversight by institutional review boards. in some instances, laws protect narrow classes of research records or statistical datafrom disclosure. laws that mandate disclosureñopen government or public record lawsñmay apply to government record keepers and to someothers who receive grants from or do business with governments. examplesof all of these laws are discussed below.any record may become the subject of a search warrant, court order,subpoena, or other type of compulsory process. some laws protect recordsfrom some types of compulsory process, and these are reviewed here. general laws, rules, and policies about compulsory process will not be examined here, with one exception. a general statute providing for courtordered disclosures that has received considerable attention is the usapatriot act. 86 section 215 of the act allows the director of the federalbureau of investigation (fbi) to seek a court order requiring the productionof òany tangible things (including books, records, papers, documents, andother items) for an investigation to protect against international terrorismor clandestine intelligence activities.ó87 the technical procedure is not ofimmediate interest, but the law requires a judge to issue an order if therequest meets the statuteõs standards. the law also prevents the recipient ofan order from disclosing that it provided materials to the fbi. the authorityof this section makes virtually every record in the united states accessible tothe fbi. it is unclear whether the usa patriot act was intended to overridelaws that protect research records against legal process. there may bedifferent answers under different research protection laws.the standards for disclosure under the eu data protection directive (areasonable proxy for most international data protection laws) are mostlythe same as the standards described above for collection. the directivegenerally regulates processing of personal information, and processing includes collection and disclosure. as with collection, a data controller in theeu needs to have authority to make a disclosure (consent, legitimate interest, and others). international standards are not considered further in thissection.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.privacy for research data103laws restricting disclosure by record keepersmost privacy laws restrict the disclosure of personal information bydefined record keepers. a brief description of the restrictions from a sampleof these laws follows.privacy act of 1974the privacy act of 1974,88 a law that applies only to federal government agencies and to a few government contractors (but no grantees),regulates disclosure of personal information maintained in a system ofrecords in several ways. generally, an agency can disclose a record onlywhen the act allows the disclosure or with the consent of the subject of therecord. the act describes 12 conditions of disclosure, which generally coverroutine disclosures that might be appropriate for any government record(within the agency, for statistical uses, to the u.s. government accountability office, to congress, for law enforcement, pursuant to court order,etc.). one of the conditions of disclosure is for a routine use, or a disclosurethat an agency can essentially establish by regulation.89 each system ofrecords can have its own routine uses determined by the agency to beappropriate for the system. as a practical matter, the privacy act imposes aclear procedural barrier (publication in the federal register) to disclosure,but the substantive barriers are low.fair credit reporting actenacted in 1970, the fair credit reporting act (fcra) was the firstmodern information privacy law. the act tells consumer reporting agencies(credit bureaus) that they can disclose credit reports on individuals only fora permissible purpose. the main allowable purposes are for credit transactions or assessments, employment, insurance, eligibility for a governmentlicense, or for a legitimate business need in connection with a transactioninitiated by a consumer. some other governmental, law enforcement, andnational security purposes also qualify.90health insurance portability and accountability actthe privacy rule issued under the authority of the health insuranceportability and accountability act controls all disclosures of protectedhealth information by covered entities (health care providers, health plans,and clearinghouses).91 however, the rule allows numerous disclosures without consent of the data subject. disclosures for research purposes are permitted if an institutional review board or a privacy board approved waiverputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.104appendix aof individual authorization.92 once disclosed to a researcher, protectedhealth information is no longer subject to regulation under hipaa (unlessthe researcher is otherwise a covered entity). however, the researcher willstill be subject to the institutional review board that approved the project,which may seek to oversee or enforce the conditions of the disclosure,including restrictions on redisclosure. whether institutional review boardshave adequate oversight or enforcement capabilities is an open question.confidential information protection and statistical efficiency actthe confidential information protection and statistical efficiency actof 2002 provides generally that data acquired by a federal agency under apledge of confidentiality and for exclusively statistical purposes must beused by officers, employees, or agents of the agency exclusively for statistical purposes.93 information acquired under a pledge of confidentiality forexclusively statistical purposes cannot be disclosed in identifiable form forany use other than an exclusively statistical purpose, except with consent.the law essentially seeks to provide for functional separation of records,which is ensuring that data collected for a research or statistical purposecannot be used for an administrative purpose.94 some other statisticalconfidentiality laws (see below) offer express protections against subpoenas, but cipsea does not directly address legal process. the lawõs definition of nonstatistical purpose can be read to exclude disclosures for legalprocess, but any exclusion is not express, and the law has not been tested.95driverõs privacy protection actin 1994, congress passed a law that prevents the states from disclosingmotor vehicle and driversõ license records. as later amended, the driverõsprivacy protection act requires affirmative consent before those recordscan be disclosed.96 the law allows disclosures for permissible purposes,and one of purposes is for use in research activities and in producingstatistical reports.97 any personal information so used cannot be published, redisclosed, or used to contact individuals.highway toll recordsat least one state has a strict law protecting the confidentiality ofelectronic toll collection system (ez pass) records that excludes all secondary uses, apparently including law enforcement and research. new hampshire law provides thatputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.privacy for research data105all information received by the department that could serve to identifyvehicles, vehicle owners, vehicle occupants, or account holders in anyelectronic toll collection system in use in this state shall be for the exclusive use of the department for the sole purpose of administering the electronic toll collection system, and shall not be open to any other organization or person, nor be used in any court in any action or proceeding,unless the action or proceeding relates to the imposition of or indemnification for liability pursuant to this subdivision. the department may makesuch information available to another organization or person in the courseof its administrative duties, only on the condition that the organization orperson receiving such information is subject to the limitations set forth inthis section. for the purposes of this section, administration or administrative duties shall not include marketing, soliciting existing account holders to participate in additional services, taking polls, or engaging in othersimilar activities for any purpose.98no search was undertaken to locate comparable state laws.laws protecting research or statistical recordsseveral laws provide stronger protection for research or statisticalrecords, sometimes shielding the records from legal process. these lawsvary, sometimes significantly, from agency to agency. it is not clear whetherthe differences are intentional or are the result of legislative happenstance.census bureaufor records of the census bureau, the law prohibits the use, publication, or disclosure of identifiable data (with limited statistical/administrative exceptions). it even provides that a copy of a census submission retained by the data subject is immune from legal process and is not admissibleinto evidence in court. this may be the most comprehensive statutoryprotection against judicial use in any law.health agenciesa law applicable to activities undertaken or supported by the agencyfor healthcare research and quality protects identifiable information frombeing used for another purpose without consent and prohibits publicationor release without consent.99 a similar law applies to the national centerfor health statistics.100 neither law expressly addresses protection againstlegal process, but the u.s. department of health and human servicesreportedly believes that both laws can be used to defeat subpoenas.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.106appendix ajustice agenciesa law protects identifiable research and statistical records of recipientsof assistance from the office of justice programs, the national institute ofjustice, and the bureau of justice assistance.101 the law prohibits secondary uses and makes records immune from legal process or admission intoevidence without the consent of the data subject. while the protectionappears to be broad, the law yields to uses and disclosures òprovided byófederal law. thus, it appears that any statute or regulation calling for a useor disclosure (including the usa patriot act) would be effective.controlled substances actthrough the controlled substances act, the attorney general can give agrant of confidentiality that authorizes a researcher to withhold identifiersof research subjects.102 disclosure of identifiers of research subjects maynot be compelled in any federal, state, or local civil, criminal, administrative, legislative, or other proceeding. the scope of protection against compelled disclosure is impressive and more detailed than some other laws.institute of education sciencesa law applicable to the recently established institute of education sciences at the u.s. department of education severely restricts the disclosureof individually identifiable information and includes immunity from legalprocess.103 however, this strong protection has a significant limitationadded by the usa patriot act. that act makes the records available for theinvestigation and prosecution of terrorism.104 a court order is required,but the court is obliged to issue the order if the government certifies thatthere are specific and articulable facts giving reason to believe that theinformation is relevant to a terrorism investigation or prosecution.the change in confidentiality protection previously afforded to education records is significant and potentially chilling. first, it illustrates howcongress can easily amend statutory protections afforded to statistical orresearch records. second, the change appears to be retroactive, meaningthat all records previously obtained under the older, more complete confidentiality regime are no longer protected against terrorism uses. third, theavailability of the records for terrorism eliminates the functional separationpreviously provided by law.public health service actthe public health service act105 authorizes the secretary of health andhuman services to provide a certificate of confidentiality to persons engagedputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.privacy for research data107in biomedical, behavioral, clinical, or other research. the certificate protects a researcher from being compelled in any federal, state, or local civil,criminal, administrative, legislative, or other proceedings to identify datasubjects.106 certificates are not limited to federally supported research. aconfidentiality certificate does not protect against voluntary or consensualdisclosure by the researcher or the data subject. it is not certain that acertificate protects data if the data subjectõs participation in the research isotherwise known.laws that may require disclosureopen records lawsvirtually all government agencies are subject to either federal or stateopen records laws. the federal freedom of information act107 permits anyperson to request any record from a federal agency. the lawõs personalprivacy exemption covers most identifiable information about individuals.the exemption would be likely to protect any personal data contained inresearch records maintained by government researchers. while many stateopen records laws are similar to the federal law, some are significantlydifferent. for example, some state open records laws do not provide aprivacy exemption at all. in those states, research records might be protected under other exemptions, other state laws, by constitutional limitations, or, conceivably, not at all.in a 1999 appropriations law, congress directed the u.s. office ofmanagement and budget (omb) to require federal awarding agencies toensure that all data produced under a grant be made available to the publicthrough the procedures established under the freedom of information act.the purpose was to provide for the public access to governmentfundedresearch data. the extension of the foia to government grantees wasunprecedented. omb circular a110 contains the implementing rules.108the circular defines research data to exclude personal information thatwould be exempt from disclosure under the foiaõs personal privacy exemption òsuch as information that could be used to identify a particularperson in a research study.ó109 the possibility for disclosure of identifiableresearch data is remote, but the omb standard hereñòcould be used toidentify a particular personóñis not derived expressly from the foia itself.it is not clear how the phrase should be interpreted. see the discussion ofidentifiability standards above.public recordspublic records is a term that loosely refers to government records thatcontain personal information about individuals and that are available forputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.108appendix apublic inspection or copying either in whole or in part.110 state and localgovernments, rather than the federal government, maintain most publicrecords. examples include property ownership records, property taxrecords, occupational licenses, voting registration records, court records,ethics filings, and many more. many states disclosed publicly driversõ license data before the federal driverõs privacy protection act restricted suchdisclosures. 111 some public records are available only to some users or forsome purposes.public records are relevant for several reasons. first, they are oftensource material for commercial or other data activities. many details of anindividualõs life, activities, and personal characteristics can be found inpublic files of government agencies. regular review of public records maynot only reveal current information about individuals but will also permitthe compilation of a history of former addresses, roommates, jobs, andother activities. commercial data companies heavily exploit public recordsto build personal and household profiles. second, the records typicallycontain address information. third, the continued public availability ofpublic records has become controversial in some states because of privacyand identity theft concerns. legislatures are reviewing decisions about disclosure of the records.conclusionsome privacy laws include provisions regulating the disclosure of personal information. other laws regulate the disclosure of narrowly definedcategories of records used for statistical or research purposes. still otherlaws define the terms under which public records (largely maintained bystate and local governments) are publicly available. open records lawsmake all government records subject to disclosure procedures, but recordscontaining personal information are often exempt from mandated disclosure. many records in private hands are unregulated at all for disclosure.there is no overarching theme or policy to be found in the law for disclosure of personal information, and it may require diligent research to determine when or if personal information in public or private hands is subjectto disclosure obligations or restrictions.liabilityliability for misuse of personal data is a complex issue, and it can beaddressed here only briefly. a full treatment would result in a legal treatiseof significant length that would not provide significant enlightenment.112some privacy laws expressly include criminal or civil penalties that mayapply to record keepers or to record users. other laws or policies may applyputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.privacy for research data109to specific record keepers. physicians, for example, have an ethical obligation to protect the confidentiality of patient information, and they could besued or sanctioned under a variety of laws and theories for breaching thatobligation. credit bureaus are subject to the rules of the fair credit reporting act, and the law provides for administrative enforcement, civil liability,and criminal penalties.113 credit bureaus also have qualified immunity thatprovides limited protection against lawsuits from data subjects.114 somepenalties apply to those who misuse credit reports. most merchants arelikely to have neither a statutory nor an ethical obligation to protect clientdata, but some may have customer agreements or formal privacy policies.violations of those agreements or policies could give rise to liability undertort or contract law and perhaps under other theories as well.officers and employees of statistical agencies are subject to criminalpenalties for wrongful disclosure of records.115 the confidential information protection and statistical efficiency act of 2002 expanded the class ofindividuals who may be subject to criminal penalties for wrongful disclosure.116 cipsea penalties cover officers and employees of a statisticalagency, along with agents. an agent is a broadly defined category thatappears to include anyone who a statistical agency allows to perform astatistical activity that involves access to restricted statistical information.117an agent must agree in writing to comply with agency rules.cipsea does not include any provision that would expressly authorizea data subject to sue over a wrongful disclosure. however, other laws,including the privacy act of 1974, might provide a basis for a lawsuit for anindividual against a federal agency that wrongfully used or disclosed personal data. it is unlikely that the courts would conclude that cipsea creates a private right of action for an aggrieved data subject against an agencyemployee or agent who improperly used or disclosed statistical information, but state law might provide a tort or other remedy. the creativity ofplaintiffõs lawyers in finding a basis for a cause of action for cases withattractive facts should not be discounted. winning a lawsuit and receivingdamages, however, are harder to accomplish.because of the patchwork quilt of privacy statutes and legal principles,the administrative, civil, and criminal liability of each record keeper andeach record user must be analyzed separately. in the absence of statutes orregulations, the analysis would begin by identifying any duty that a recordkeeper may have to a data subject. in many instances, there will be no clearduty.in at least some circumstances, however, it may be possible for a datasubject to have a legal remedy against a wholly unrelated third party,regardless of the source of the data used by the third party. the tort forintrusion upon seclusion and the tort for publicity given to private lifepermit a lawsuit to be filed against another person who has no relationshipputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.110appendix awith the data subject and no defined contractual or statutory duty of confidentiality.118 the availability of these torts varies from state to state.an unexplored area of potential liability involves recipients ofdeidentified data who then reidentify the data subjects. in some instances,exploration of liability begins with a regulation. for example, under thefederal health privacy rules issued under the authority of hipaa, disclosureof a limited data set is permitted for some activities (including research)subject to conditions that include a prohibition against identifying the information. if the recipient is not a covered entity under the rule, then thereis no administrative enforcement against the recipient.119 other enforcement possibilities may be available regardless of an underlying law.when a recipient obtains information from an entity that has a confidentiality duty to data subjects, liability over reidentification could arise inseveral ways. the reidentification activity might violate the agreement under which the data were transferred. the data supplier might be able to suethe recipient for breach of contract. assuming that any privacy obligationfalls directly on the data supplier only and not on the recipient, it is possiblethat the supplier could be sanctioned administratively for failing to properly control further use of the information.if a recipient reidentifies data contrary to a contract or a law, it ispossible that an aggrieved data subject could sue either the data supplier orthe recipient. for the supplier, the principal question would be whether abreach of a duty of confidentiality resulted from an imprudent transfer ofdeidentified data.for a lawsuit against the recipient by an aggrieved data subject, thelegal preliminaries are more complex. a tort or contract lawsuit may bepossible, but a data subject may be unable to sue the recipient relying on thecontract because the data subject is not a party to the contract between thedata supplier and the recipient. the data subject lacks privityñan adequatelegal relationshipñto the contract to be able to use the contract to enforcean interest. in general, the requirement for privity can be a major obstacleto enforcement of privacy rights for data subjects.120however, the lack of privity can be trumped in some jurisdictions bythe doctrine of thirdparty beneficiaries. under current contract law principles, a contract with privacy clauses benefiting a data subject who is not aparty to the contract may still be enforceable by the data subject. theconditions are that the parties to the contract (i.e., the supplier and therecipient) intended the data subject to benefit and that enforcement by thedata subject is appropriate to achieve the intent of the parties.121 in otherwords, a data subject may be able to sue to enforce data protection provisions of a contract despite the lack of privity.122 the law on thirdpartybeneficiaries varies from jurisdiction to jurisdiction, so different results arepossible in different states.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.privacy for research data111now consider the class of data recipients that reidentifies data afterobtaining the data from public or other sources. these recipients may haveno duty to the data subject and no direct relationship or obligation to thedata suppliers. for example, latanya sweeney demonstrated that u.s. hospital discharge datañpublicly available with all overt identifiers removedñcan nevertheless be linked to individual patients.123 in one example, sheidentified the hospital record of the governor of massachusetts from recordsthat had been deidentified before public release.124 a public disclosure ofthis type of information could support a lawsuit against a researcher, although a public figure might have a more difficult case, especially if anewspaper published the reidentified data. whether the agency that released the discharge data could also be sued is uncertain.a federal government agency might conceivably be sued for disclosingpotentially identifiable personal information in violation of the privacy actof 1974.125 however, the act also allows agencies to justify disclosures thatare compatible with the purpose for which the information was collected.an agency that took steps to allow a disclosure of deidentified data mighthave a complete defense.126 in any event, the act may not cover deidentifieddata at all, and the agency might not be responsible for its subsequentreidentification by another party.in all of these possible lawsuits, much would depend on the facts. if areidentified record were used for a research purpose, a data subject mighthave difficulty convincing a jury that harm resulted. however, if the datawere used to deny an insurance policy or to charge a higher price for amortgage, proof of harm would be enhanced, as would the jury appeal ofthe case.because there are so many institutions, interests, and potential legalstandards, no broad conclusion about legal liability for data disclosures canbe offered. some statutes include clear sanctions, but much is uncertainotherwise. a data subject might have a remedy with respect to the disclosure or use of deidentified data that are later reidentified. the type ofremedy and the likelihood of success would vary depending on the sourceof the data, the institutions involved, their relationship with the data subject, and other facts. no known case or statute clearly addresses the possibility of a lawsuit by a data subject over reidentification of personal data.it is noteworthy, however, that remedies for the misuse and disclosureof identifiable personal information are often weak or absent. it seemsunlikely that protections for deidentified data would be easier to achievethrough the courts in the absence of clear statutes or other standards.however, the creativity of the plaintiffõs bar and the courts should not bediscounted should a shocking misuse of data occur.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.112appendix aconcluding observationsthe law surrounding the collection, maintenance, use, and disclosureof personal information by researchers and others is typically vague, incomplete, or entirely absent. the possibility of civil liability to a data subject forcollection, use, or disclosure of personal information exists, but lawsuitsare not frequent, successes are few, and cases are highly dependent on facts.however, the research community faces other risks. for example, if anaggressive researcher or tabloid newspaper acquires deidentified researchdata and reidentifies information about politicians, celebrities, or sportsheroes, the story is likely to be frontpage news everywhere. the resultingpublic outcry could result in a major change in data availability or theimposition of direct restrictions on researchers. many privacy laws originated with horror stories that attracted press attention. when a reporterobtained the video rental records of a u.s. supreme court nominee, nervous members of congress quickly passed a privacy law restricting the useand disclosure of video rental records.127 the driverõs privacy protectionact also had its origins with a horror story.the demise of human resources development canadaõs longitudinallabour force file in the summer of 2000 offers an example of how privacyfears and publicity can affect a research activity. the file was the largestrepository of personal information on canadian citizens, with identifiableinformation from federal departments and private sources. the databaseoperated with familiar controls for statistical records, including exclusiveuse for research, evaluation, and policy and program analysis. the publicdid not know about the database until the federal privacy commissionerraised questions about the òinvisible citizen profile.ó128 the database wasstaunchly defended, but the public objections were too strong, and canadadismantled the database. the case for the database was not helped by itsmedia designation as the òbig brother database.ó129methods for collecting and using data while protecting privacy interests exist, but how effective they are, how much they compromise researchresults, and how much they are actually used is unclear. it appears thatthere is room for improvement using existing policies, methodologies, andpractices. however, there may be some natural limits to what can be accomplished. the availability of personal data and the technological capabilities for reidentification seem to increase routinely over time as the resultof factors largely beyond control.basic transparency rules (for both privacy and human subjects protection) require that respondents be told of the risks and consequences ofsupplying data. for data collected voluntarily from respondents, it is possible that cooperation will vary inversely with the length of a privacy notice.even when data activities (research or otherwise) include real privacy proputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.privacy for research data113tections, people may still see threats regardless of the legal, contractual, ortechnical measures promised. reports of security and other privacy breachesare commonplace.complex privacy problems will not be solved easily because of themany players and interests involved. those who need data for legitimatepurposes have incentives for reducing the risks that data collection anddisclosure entail, but data users are often more focused on obtaining andusing data and less on remote possibilities of bad publicity, lawsuits, andlegislation. the risk to a data subject is a loss of privacy. the risks to datasuppliers and users include legal liability for the misuse of data and thepossibility of additional regulation. the risk to researchers, statisticians,and their clients is the loss of data sources. the risk to society is the loss ofresearch that serves important social purposes. these risks should encourage all to work toward better rules governing the use and disclosure ofsensitive personal information. risks can be minimized, but most cannot beeliminated altogether.selfrestraint and professional discipline may limit actions that threatenthe user community, but controls may not be effective against all membersof the community and they will not be effective against outsiders. industrystandards may be one useful way to minimize risks, maximize data usefulness, and prevent harsher responses from elsewhere. if standards do notcome from elsewhere, however, then the courts and the legislatures mayeventually take action. judicial and legislative actions always follow technological and other developments, and any changes imposed could be harshand widereaching, especially if the issue is raised as a result of a crisis.privacy legislation often begins with a wellreported horror story.notes1.collection limitation principle: there should be limits to the collection of personaldata and any such data should be obtained by lawful and fair means and, whereappropriate, with the knowledge or consent of the data subject.data quality principle: personal data should be relevant to the purposes for whichthey are to be used and, to the extent necessary for those purposes, should be accurate, complete, and kept uptodate.purpose specification principle: the purposes for which personal data are collectedshould be specified not later than at the time of data collection, and the subsequentuse limited to the fulfillment of those purposes or such others as are not incompatiblewith those purposes, and as are specified on each occasion of change of purpose.use limitation principle: personal data should not be disclosed, made available orotherwise used for purposes other than those specified in accordance with the purpose specification principle except (a) with the consent of the data subject, or (b) bythe authority of law.security safeguards principle: personal data should be protected by reasonable security safeguards against such risks as loss or unauthorized access, destruction, use,modification or disclosure of data.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.114appendix aopenness principle: there should be a general policy of openness about developments,practices and policies with respect to personal data. means should be readily availableof establishing the existence and nature of personal data, and the main purposes oftheir use, as well as the identity and usual residence of the data controller.individual participation principle: an individual should have the right (a) to obtainfrom a data controller, or otherwise, confirmation of whether or not the data controller has data relating to him; (b) to have communicated to him data relating tohim within a reasonable time; at a charge, if any, that is not excessive; in a reasonable manner; and in a form that is readily intelligible to him; (c) to be given reasons ifa request made under subparagraphs (a) and (b) is denied, and to be able to challengesuch denial; and (d) to challenge data relating to him and, if the challenge is successful to have the data erased, rectified, completed, or amended.accountability principle: a data controller should be accountable for complyingwith measures, which give effect to the principles stated above.organisation for economic cooperation and development (1980).2.5 u.s.c. ¤ 552(b)(4).3.executive order 12958.4.laws in other countries sometimes extend privacy protections to legal persons. corporate confidentiality interests (whether arising under privacy laws, through statistical surveys that promise protection against identification, or otherwise) can raisesimilar issues of identification and reidentification as with individuals. corporateconfidentiality interests are beyond the scope of this paper.another set of related issues is group privacy. groups can be defined in manyways, but race, ethnicity, and geography are familiar examples. if the disclosure ofmicrodata can be accomplished in a way that protects individual privacy interests,the data may still support conclusions about identifiable racial, ethic, or neighborhood groups that may be troubling to group members. group privacy has receivedmore attention in health care than in other policy arenas. see alpert (2000).5.the term deidentified is used here to refer to data without overt identifiers but thatmay still, even if only theoretically, be reidentified. data that cannot be reidentifiedare referred to as wholly nonidentifiable data.6.see generally gellman (2001). for more on the growth in information collection andavailability, see sweeney (2001).7.15 u.s.c. ¤ 1681 et seq.8.national committee on vital and health statistics, subcommittee on privacy andconfidentiality (1998a).9.5 u.s.c. ¤ 552a.10.5 u.s.c. ¤ 552a(a)(4). the value of a fingerprint as an identifier is uncertain. without access to a database of fingerprints and the ability to match fingerprints, a singlefingerprint can rarely be associated with an individual. the same is true for a photograph. for example, a photograph of a fouryearold taken sometime in the last 50years is not likely to be identifiable to anyone other than a family member.11.just to make matters even more complex, the federal freedom of information act (5u.s.c. ¤ 552) has a standard for privacy that is not the same as the privacy act. inforest guardians v. u.s. fema (10th cir. 2005) available: http://www.kscourts.org/ca10/cases/2005/06/042056.htm, the court denied a request for òelectronic gis files. . . for the 27 communities that have a flood hazard designated by fema . . .showing all of the geocoded flood insurance policy data (with names and addressesremoved) including the location of structures relative to the floodplain and whetherthe structure insured was constructed before or after the community participated inthe nfip.ó the court found that disclosure would constitute an unwarranted invaputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.privacy for research data115sion of privacy, the privacy standard under the foia. the court reached this conclusion even though virtually identical information had been released in a paper file.the case turned mostly on the courtõs conclusion that there was a lack of publicinterest in disclosure, a relevant standard for foia privacy determinations. in striking a balance, the court found that any privacy interest, no matter how small, outweighed no public disclosure interest.12.personal information means information that identifies òan individual, including anindividualõs photograph, social security number, driver identification number, name,address (but not the 5digit zip code), telephone number, and medical or disabilityinformation, but does not include information on vehicular accidents, driving violations, and driverõs status.ó 18 u.s.c. ¤ 2725(3).13.personally identifiable information òincludes information which identifies a personas having requested or obtained specific video materials or services from a video tapeservice provider.ó 18 u.s.c. ¤ 2710 (a)(3).14.47 u.s.c. ¤ 551(a)(2)(a).15.egovernment act of 2002, pub. l. 107347, dec. 17, 2002, 116 stat. 2899, 44u.s.c. ¤ 3501 note ¤502(4).16.s.c. 2000, c. 5, ¤ 2(1), available: http://www.privcom.gc.ca/legislation/02060101e.asp.17.perrin, black, flaherty, and rankin (2001).18.loi no. 7817 du 6 janvier 1978 at article 4, available: http://www.bild.net/dataprfr.htm. a 2004 amendment added these words: òin order to determine whethera person is identifiable, all the means that the data controller or any other personuses or may have access to should be taken into consideration.ó act of 6 august2004 at article 2, available: http://www.cnil.fr/fileadmin/documents/uk/7817va.pdf.the amendment does not appear to have changed the strict concept of identifiabilityor to have added any reasonableness standard.19.joel r. reidenberg and paul m. schwartz, data protection law and online services:regulatory responses (1998) (european commission), available: http://ec.europa.eu/justicehome/fsj/privacy/docs/studies/regulen.pdf.20.see loi no. 7817 du 6 janvier 1978 (as amended) at article 32 (iv) (allowing thefrench data protection authority to approve anonymization schemes), article 54(allowing the french data protection authority to approve methodologies for healthresearch that do not allow the direct identification of data subjects), and article 55(allowing exceptions to a requirement for coding personal in some medical researchactivities), available: http://www.cnil.fr/fileadmin/documents/uk/7817va.pdf.21.directive on the protection of individuals with regard to the processing of personaldata and on the free movement of such data, council directive 95/46/ec, 1995o.j. (l 281) 31, at article 2(a), available: http://europa.eu.int/comm/internalmarket/en/dataprot/law/index.htm.22.id. at recital 26.23.council of europe, recommendation no. r (97) 5 of the committee of ministers tomember states on the protection of medical data ¤1 (1997), available: http://www.cm.coe.int/ta/rec/1997/word/97r5.doc.24.council of europe, explanatory memorandum to recommendation no. r (97) 5 ofthe committee of ministers to member states on the protection of medical data ¤36 (1997), available: http://www.cm.coe.int/ta/rec/1997/exprec(97)5.htm.25.privacy act 1988 ¤ 6 (2001), available: http://www.privacy.gov.au/publications/privacy88.pdf.26.uk data protection act 1998 ¤ 1(1) (1998), available: http://www.legislation.hmso.gov.uk/acts/acts1998/19980029.htm.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.116appendix a27.canadian institutes of health research, recommendations for the interpretationand application of the personal information protection and electronic documentsact (s.c.2000, c.5) in the health research context 6 (nov. 30, 2001), available:http://www.cihr.ca/aboutcihr/ethics/recommendationse.pdf.1(a) for greater certainty, ôinformation about an identifiable individualõ, within themeaning of personal information as defined by the act, shall include only that information that can:(i)identify, either directly or indirectly, a specific individual; or,(ii)be manipulated by a reasonably foreseeable method to identify a specific individual; or(iii)be linked with other accessible information by a reasonably foreseeable method to identify a specific individual.1(b) notwithstanding subsection 1(a), ôinformation about an identifiable individualõshall not include:(i)anonymized information which has been permanently stripped of all identifiers or aggregate information which has been grouped and averaged, such thatthe information has no reasonable potential for any organization to identify aspecific individual; or(ii)unlinked information that, to the actual knowledge of the disclosing organization, the receiving organization cannot link with other accessible informationby any reasonably foreseeable method, to identify a specific individual.(c) whether or not a method is reasonably foreseeable under subsections 1(a) and1(b) shall be assessed with regard to the circumstances prevailing at the time of theproposed collection, use or disclosure.28.alberta health information act ¤ 1(p) (1999), available: http://www.qp.gov.ab.ca/documents/acts/h05.cfm.29.id. at ¤ 1(r).30.id. at ¤ 1(g).31.id. at ¤ 6872.32.nonstatutory administrative reviews of data disclosure may be commonplace. forexample, the national center for health statistics in the department of health andhuman services uses an administrative review process with a disclosure reviewboard to assess the risk of disclosure for the release of microdata files for statisticalresearch. national center for health statistics, staff manual on confidentiality(2004), http://www.cdc.gov/nchs/data/misc/staffmanual2004.pdf.33.u.s. department of health and human services, òstandards for privacy of individually identifiable health information,ó 65 federal register 8246282829 (dec. 28,2000) (codified at 45 c.f.r. parts 160 & 164).34.public law no. 104191, 110 stat. 1936 (1996).35.45 c.f.r. ¤ 160.103.36.id. at ¤ 164.514(b)(2). the complete list of data elements includes ò(a) names; (b)all geographic subdivisions smaller than a state, including street address, city, county,precinct, zip code, and their equivalent geocodes, except for the initial three digits ofa zip code if, according to the current publicly available data from the bureau of thecensus: (1) the geographic unit formed by combining all zip codes with the samethree initial digits contains more than 20,000 people; and (2) the initial three digitsof a zip code for all such geographic units containing 20,000 or fewer people ischanged to 000; (c) all elements of dates (except year) for dates directly related toan individual, including birth date, admission date, discharge date, date of death; andall ages over 89 and all elements of dates (including year) indicative of such age,except that such ages and elements may be aggregated into a single category of age90 or older; (d) telephone numbers; (e) fax numbers; (f) electronic mail addresses;putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.privacy for research data117(g) social security numbers; (h) medical record numbers; (i) health plan beneficiarynumbers; (j) account numbers; (k) certificate/license numbers; (l) vehicle identifiers and serial numbers, including license plate numbers; (m) device identifiers andserial numbers; (n) web universal resource locators (urls); (o) internet protocol(ip) address numbers; (p) biometric identifiers, including finger and voice prints; (q)full face photographic images and any comparable images; and (r) any other uniqueidentifying number, characteristic, or code.ó37.id. at. ¤ 164.514(b)(2)(ii).38.45 c.f.r. ¤ 164.512(b)(1).39.id. at ¤ 164.512(b)(1)(i). the commentary accompanying the rule includes referencesto published materials offering guidance on assessing risk, and it recognizes thatthere will be a need to update the guidance over time. those materials are federalcommittee on statistical methodology, statistical policy working paper 22, reporton statistical disclosure limitation methodology (1994), available: http://www.fcsm.gov/workingpapers/wp22.html; òchecklist on disclosure potential of proposed datareleases,ó 65 federal register 82709 (dec. 28, 2000), available: http://www.fcsm.gov/docs/checklist799.doc.40.45 c.f.r. ¤ 164.512(b)(1)(ii).41.45 c.f.r. ¤ 164.514(e).42.quintiles transnational corp. v. webmd corp., no. 5:01cv180bo(3), (e.d.n.c. mar. 21, 2002).43.r. v. dept of health ex parte source informatics ltd., 1 all e.r. 786, 79697 (c.a.2000), reversing 4 all e.r. 185 (q.b. 1999).44.the southern illinoisan v. illinois department of public health, 812 n.e.2d 27(ill.app. ct. 2004), available: http://www.state.il.us/court/opinions/appellatecourt/2004/5thdistrict/june/html/5020836.htm.45.the courtõs opinion focused in significant part on the expert abilities of sweeney andfound a lack of evidence demonstrating whether other individuals could identifyindividuals in the same fashion. available: http://www.state.il.us/court/opinions/supremecourt/2006/february/opinions/html/98712.htm. the opinion suggests thata different result might be obtained with a better factual showing that identifiabilitycapabilities were more widespread among the population. just how difficult it wouldbe for others to reidentify the records is not entirely clear. however, both courtsignored the possibility that a recipient of data could hire someone with sweeneyõsskills and learn the names of patients. the courtõs basis for decision does not seem tobe sustainable in the long run.46.northwestern memorial hospital v. ashcroft, 362 f.3d 923 (7th cir. 2004), available: http://www.ca7.uscourts.gov/tmp/i110h5xz.pdf.47.two quotes from the decision are worth reproducing:some of these women will be afraid that when their redacted records aremade a part of the trial record in new york, persons of their acquaintance,or skillful ògooglers,ó sifting the information contained in the medicalrecords concerning each patientõs medical and sex history, will put twoand two together, òoutó the 45 women, and thereby expose them tothreats, humiliation, and obloquy.* * *even if there were no possibility that a patientõs identity might be learnedfrom a redacted medical record, there would be an invasion of privacy.imagine if nude pictures of a woman, uploaded to the internet without herconsent though without identifying her by name, were downloaded in aforeign country by people who will never meet her. she would still feel thatputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.118appendix aher privacy had been invaded. the revelation of the intimate details contained in the record of a lateterm abortion may inflict a similar wound.48.see generally, gellman (2005).49.extensive rules and laws govern surveillance by wire, whether by government actorsor private parties.50.389 u.s. 347 (1967).51.389 u.s. at 351.52.389 u.s. at 361.53.see schwartz (1995).54.460 u.s. 276 (1983).55.460 u.s. at 281.56.id. at 284.57.476 u.s. 207 (1986).58.476 u.s. 227 (1986).59.id.60.in kyllo v. united states, 533 u.s. 27 (2001), the supreme court found that policeuse of heat imaging technology to search the interior of a private home from theoutside was a fourth amendment search that required a warrant. the case turned inpart on the use by the government of òa device that is not in general public use, toexplore the details of the home that would previously have been unknowable without physical intrusion.ó id. at 40. the broader implications of the courtõs standardfor technology not in general public use are not entirely clear.61.wash. rev. code ¤ 9a44115.62.wash. rev. code ¤ 9a44115(1)(c).63.2003 wash. laws ¤ 213 (amending wash. rev. code ¤ 9a44115).64.ariz. rev. stat. ¤ 133019(c)(4).65.conn. gen. stat. ¤ 3148b(b).66.tex. health & safety code ¤ 242.501(a)(5).67.the other torts are for appropriation of a name or likeness, publicity given to privatelife, and publicity placing a person in a false light. 3 restatement (second) of torts ¤652a et seq. (1977)68.id. at ¤ 652b.69.id. at comment c.70.nader v. general motors corp., 255 n.e.2d 765 (ny 1970), 1970 n.y. lexis1618.71.galella v. onassis, 487 f.2d 986 (2d cir. 1973).72.see, e.g., in the matter of an application of the united states for an order (1)authorizing the use of a pen register and a trap and trace device and (2) authorizing release of subscriber information and/or cell site information, magistrateõsdocket no. 051093 (jo), available: www.eff.org/legal/cases/usavpenregister/celltrackingdenial.pdf; brief for amicus electronic frontier foundation at 7, available: http://www.eff.org/legal/cases/usavpenregister/celltrackingeffbrief.pdf(òthe prospective collection of cell site data will therefore reveal the cell phoneõslocation even when that information could not have been derived from visual surveillance, but only from a physical searchó [footnote omitted]).73.note, harvard journal of law and technology (fall, 2004).given current database and storage capacities, the door is open for anorwellian scenario whereby law enforcement agents could monitor notjust criminals, but anyone with a cell phone. if it sounds improbable,consider that commercial tracking services already provide realtime location information for families and businesses. (p. 316)putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.privacy for research data11974.organisation for economic cooperation and development, council recommendations concerning guidelines governing the protection of privacy and transborderflows of personal data, 20 i.l.m. 422 (1981), o.e.c.d. doc. c (80) 58 (final)(oct. 1, 1980), available: http://www.oecd.org/document/18/0,2340,en26493425518151861111,00.html .75.council directive 95/46, art. 28, on the protection of individuals with regard to theprocessing of personal data and on the free movement of such data, 1995 o.j.(l281/47), available: http://europa.eu.int/comm/justicehome/fsj/privacy/law/indexen.htm.76.additional rules govern the processing of special categories of data (racial or ethnicorigin, political opinions, religious or philosophical beliefs, trade union membership,and data concerning health or sex life). generally, explicit consent is necessary forcollection of these special categories, with some exceptions.77.article 7.78.uk data protection act 1998 ¤¤ 10, 11 (1998), available: http://www.legislation.hmso.gov.uk/acts/acts1998/19980029.htm.79.u.s. department of health and human services, òstandards for privacy of individually identifiable health information,ó 65 federal register 8246282829 (dec. 28,2000) (codified at 45 c.f.r. parts 160 & 164).80.5 u.s.c. ¤ 552a.81.id. at ¤¤ 552a(e)(1), (2), & (7).82.u.s. department of health and human services, òstandards for privacy of individually identifiable health information,ó 65 federal register 82462 82464 (dec. 28,2000).83.45 c.f.r. ¤164.502(b).84.15 u.s.c. ¤ 6502.85.47 u.s.c. ¤ 551(b).86.uniting and strengthening america by providing appropriate tools required tointercept and obstruct terrorism (usa patriot act) act of 2001, public law no.107056, 115 stat. 272, available: http://frwebgate.access.gpo.gov/cgibin/getdoc.cgi?dbname=107congpubliclaws&docid=f:publ056.107.87.50 u.s.c. ¤ 1861.88.5 u.s.c. ¤ 552a.89.the conditions of disclosure are at 5 u.s.c. ¤ 552a(b), with the routine use authorityat (b)(2). the definition of routine use is at 5 u.s.c. ¤ 552a(a)(7).90.15 u.s.c. ¤ 1681b.91.45 c.f.r. ¤ 164.512.92.id. at ¤ 164.512(i).93.44 usc ¤ 3501 note, ¤ 512(a). an exception allows disclosure to a law enforcementagency for the prosecution of submissions of false statistical information under statutes imposing civil or criminal penalties. id. at ¤ 504(g).94.see privacy protection study commission, personal privacy in an information society 573 (1977), available: http://www.epic.org/privacy/ppsc1977report/. see alsonational research council and the social science research council (1993:3435).95.44 usc ¤ 3501 note, ¤ 502(5).96.18 u.s.c. ¤ 2721.97id. at ¤ 2721(b)(5).98.n.h. rev. stat. online ¤ 237:16e (2004), available: http://www.gencourt.state.nh.us/rsa/html/xx/237/23716e.htm.99.42 u.s.c. ¤ 934 (formerly 42 u.s.c. ¤ 299c3(c)).100.42 u.s.c. ¤ 242m(d).putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.120appendix a101.42 u.s.c. ¤ 3789g(a).102.21 u.s.c. ¤ 872(c).103.20 u.s.c. ¤ 9573. the law formerly applied only to the national center for education statistics.104.usa patriot act of 2001 at ¤ 508 (amending 20 u.s.c. ¤ 9007), public law no.107056, 115 stat. 272, available: http://frwebgate.access.gpo.gov/cgibin/getdoc.cgi?dbname=107congpubliclaws&docid=f:publ056.107.105.42 u.s.c. ¤ 241(d).106.the national institutes of health encourages investigators working on sensitive biomedical, behavioral, clinical, or other types of research to obtain certificates.107.5 u.s.c. ¤ 552.108.u.s. office of management and budget, circular a110 (uniform administrativerequirements for grants and agreements with institutions of higher education,hospitals, and other nonprofit organizations) (9/30/99), available: http://www.whitehouse.gov/omb/circulars/a110/a110.html.109.id. at .36(d)(2)(i)(a).110.see generally, gellman (1995).111.18 u.s.c. ¤ 2721.112.more on this general subject can be found in perritt (2003).113.15 u.s.c. ¤ 1681 et seq.114.id. at ¤ 1681s2.115.see, e.g., 13 u.s.c. ¤ 214 (census bureau employees).116.44 u.s.c. ¤ 3501 note ¤ 513. interestingly, while cipsea regulates both use anddisclosure of statistical information, id. at ¤ 512, only wrong disclosure is subject tocriminal penalties.117.44 u.s.c. ¤ 3501 note ¤ 502 (òthe term ôôagentõõ means an individualñ(a)(i) who is an employee of a private organization or a researcher affiliated withan institution of higher learning (including a person granted special sworn status bythe bureau of the census under section 23(c) of title 13, united states code), andwith whom a contract or other agreement is executed, on a temporary basis, by anexecutive agency to perform exclusively statistical activities under the control andsupervision of an officer or employee of that agency;(ii) who is working under the authority of a government entity with which acontract or other agreement is executed by an executive agency to performexclusively statistical activities under the control of an officer or employee ofthat agency;(iii) who is a selfemployed researcher, a consultant, a contractor, or an employeeof a contractor, and with whom a contract or other agreement is executed byan executive agency to perform a statistical activity under the control of anofficer or employee of that agency; or(iv)who is a contractor or an employee of a contractor, and who is engaged bythe agency to design or maintain the systems for handling or storage of datareceived under this title; and(b) who agrees in writing to comply with all provisions of law that affect information acquired by that agency.ó)118.3 restatement (second) of torts ¤¤ 652b, 652d (1977).119.the hipaa criminal penalties may not apply, either. see u.s. department of justice,office of legal counsel, scope of criminal enforcement under 42 u.s.c. ¤ 1320d6(june 1, 2005), available: http://www.usdoj.gov/olc/hipaafinal.htm.120.see, e.g., reidenberg (1992).121.restatement (second) of contracts ¤¤ 302, 303 (1981).putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.privacy for research data121122.the original draft hipaa privacy rule required business partner agreements to statethat the agreements intended to create thirdparty beneficiary rights. in the final rule,the thirdparty beneficiary language was removed. the commentary stated that theruleõs intent was to leave the law in this area where it was. the discussion in the finalrule shows that there were strongly divergent views on the issue. see 65 federalregister 82641 (dec. 28, 2000).123.considerable amounts of patientlevel information are available. for example, thehealthcare cost and utilization project distributes four databases for health servicesresearch, with data dating back to 1988. this joint federalstate partnership is sponsored by the agency for healthcare research and quality, a part of the federaldepartment of health and human services. the databases contain patientlevel information for either inpatient or ambulatory surgery stays in a uniform format òwhileprotecting patient privacy.ó healthcare cost and utilization project, description ofhealthcare cost and utilization project (undated), available: http://www.ahcpr.gov/downloads/pub/hcup/appkitv15b.pdf. whether the privacy protections are adequateto protect against reidentification under all conditions is uncertain. numerous othermedical data sets are available from other sources.124.see national committee on vital and health statistics, subcommittee on privacyand confidentiality (1998b).125.5 u.s.c. ¤ 552a.126.5 u.s.c. ¤ 552a(b)(3) allows agencies to define a routine use to justify a disclosure.127.video privacy protection act (òbork lawó), 18 u.s.c. ¤ 2710.128.privacy commissioner (canada), annual report 19992000 available: http://www.privcom.gc.ca/information/ar/020409e.asp.129.mccarthy (2000).referencesalpert, s.2000privacy and the analysis of stored tissues. pp. a1ða36 in research involvinghuman biological materials: ethical issues and policy guidance (volume iicommissioned papers). rockville, md: national bioethics advisory commission. available: http://bioethics.georgetown.edu/nbac/hbmii.pdf. [accessed december 2006].gellman, r.1995public records: access, privacy, and public policy. government informationquarterly 12:391426.2001public record usage in the united states. paper presented at the 23rd international conference of data protection commissioners, september 25, paris,france. available: http://www.personaldataconference.com/eng/contribution/gellmancontrib.html [accessed december 2006].2005a general survey of video surveillance law in the united states. in s. nouwt,b.r. de vries, and c. prins, eds., reasonable expectations of privacy? elevencountry reports on camera surveillance and workplace privacy. hague, netherlands: t.m.c. asser press.harvard journal of law and technology2004who knows where youõve been? privacy concerns regarding the use of cellularphones as personal locators. harvard journal of law and technology 18(1):307,316 (fall).putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.122appendix amccarthy, s.2000ottawa pulls plug on big brother database, canadians promised safeguards ondata. globe and mail, may 30.national committee on vital and health statistics, subcommittee on privacy and confidentiality1998aproceedings of roundtable discussion: identifiability of data. hubert humphreybuilding, january 28, washington, dc. transcript available:http://ncvhs.hhs.gov/980128tr.htm [accessed december 2006].1998broundtable discussion: identifiability of data. available:http://ncvhs.hhs.gov/980128tr.htm [accessed january 2007].national research council and the social science research council1993private lives and public policies. g.t. duncan, t.b. jabine, and v.a.. de wolf,eds. panel on confidentiality and data access. committee on national statistics, commission on behavioral and social sciences and education.washington,dc: national academy press.organisation for economic cooperation and development1980council recommendations concerning guidelines governing the protection ofprivacy and transborder flows of personal data. o.e.c.d. doc. c (80) 58(final). available: http://www.oecd.org/document/18/0,2340,en26493425518151861111,00.html [accessed december 2006].perrin, s., h.h. black, d.h. flaherty, and t.m. rankin2001the personal information protection and electronic documents act: an annotated guide. toronto, canada: irwin law.perritt, h.h., jr.2003protecting confidentiality of research data through law. paper prepared forcommittee on national statistics, national research council data confidentiality and access workshop, washington, dc. available: http://www7.nationalacademies.org/cnstat/perrittpaper.pdf [accessed january 2007].reidenberg, j.r.1992the privacy obstacle course: hurdling barriers to transnational financial services. fordham law review 60:s137, s175.reidenberg, j.r., and p.m. schwartz1998data protection law and online services: regulatory responsescommissioned from arete by directorate general xv of the commission ofthe european communities. available: http://ec.europa.eu/justicehome/fsj/privacy/docs/studies/regulen.pdf [accessed december 2006].schwartz, p.1995privacy and participation: personal information and public sector regulation inthe united states. iowa law review 80:553, 573.sweeney, l.2001information explosion. chapter 3 in p. doyle, j. lane, j. theeuwes, and l.zayatz, eds., confidentiality, disclosure, and data access: theory and practicalapplications for statistical agencies. new york: northholland elsevier.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.123apppendix bethical issues related tolinked socialspatial datafelice j. levine and joan e. sieberthe ethics of research related to linking geographically explicit spatialdata1 and individual, household, or grouplevel social data is an issue ofscientific and social significance. the capacity to measure location andcontext over time and with exact precision offers substantial opportunitiesto comprehend human, social, biological, and environment activities, interactions, and transformations at a level of sophistication that could not havebeen anticipated just a decade ago. the mesh of technological advances,computational capacity, multilevel statistical models, spatial analysis software, and robust data mining and management techniques makes it a ripetime for new explorations and applications to come to the fore using veryprecise locational information.2 along with these improved measurementsand analytic methods come ethical issues regarding how best to use thesenew capabilities consonant with protecting the interests of research participants involved in such studies.the most immediate ethical issue raised by linking different datasets orresources of any form is whether the integration of such information encroaches on the privacy of research subjects or compromises the confidentiality of information that otherwise is secure. attention to issues of privacyof persons and confidentiality of data has increased over recent years.3there is growing awareness of the scientific value of sharing data, thegreater contributions made possible with microlevel data, and the potentialuses from linking different datasets. yet there is also mindfulness of thepotential risks of confidentiality breaches due to intentional or inadvertentdisclosure. in this current context, not unexpectedly, opportunities for linkputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.124appendix bing social and spatial data have also been accompanied by serious discussion of the confidentiality issues and policies involved in doing so (see, e.g.,rindfuss and stern, 1998; vanwey et al., 2005; golden, downs, and davispackard, 2005; gutmann et al., 2005).whether in the biomedical or the socialbehavioral sciences, new methodological capabilities or work at the frontiers of discovery invariably requires fresh consideration of ethical issues as an integral part of research.especially in nascent areas of science in which practical experience is limited, grappling with ethical issues needs to go handinhand with confronting theoretical, methodological, and operational considerations.4 thus, itis notable that those attracted to or engaged in linking spatial and socialdata have already initiated the process of thinking reflectively and constructively about matters of confidentiality and reduction of the risk of information disclosure. the establishment of a national research council panel onconfidentiality issues arising from the integration of remotely sensed andselfidentifying data, with funding from the national institutes of health,the national science foundation, and the national aeronautics and spaceadministration, to address such confidentiality issues is a strong indicatorof the salience of this topic to data producers, users, archivists, databasemanagers, and those who review and support such work.the purpose of this paper is to consider the ethical issues that come intoplay in research that links social and spatial data. our aim is to present anoverview of the ethical issues regarding the protection of human subjects,for researchers engaged in primary collection of social and spatial data, andfor those engaged in secondary use of such data. first, we briefly highlightthe ethical guidance available for researchers or research teams as theyconsider how best to undertake research on these data or provide such datato others. second, we elaborate on and recommend as guidance the framework of ethical principles enunciated in the now classic 1979 belmontreport, ethical principles and guidelines for the protection of humansubjects of research (national commission for the protection of humansubjects of biomedical and behavioral research, 1979). third, we considerthe range of ways ethical issues can manifest themselves in the course ofcollecting, providing, or using linked socialspatial data and how researchers might best advance ethically sound research and approach review by aninstitutional review board (irb). fourth, we examine such issues as consent, privacy and confidentiality, benefits and harm, and assessments of riskof harm and how to address them in research that either links or uses linkedsocialspatial data. fifth, we specifically discuss the ethics of data dissemination, sharing, and accessñemphasizing issues important to socialspatialresearch. finally, we consider ethics education and training for those whocollect, prepare, provide access to, use, or review research that links socialand spatial data.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.ethical issues related to linked socialspatial data125this focus on ethical considerations in social and spatial research isdistinct from an analysis of the legal requirements that could apply depending on the data that are to be obtained. use of extant information sourcesmay be protected by privacy laws. some of the most promising social andspatial research is addressed to issues in which privacy regulations aregermane. health research, for example, is a key area of inquiry in whichaccess to confidential records, including precise locational information,could have tremendous scientific value and benefits to society. the healthinsurance portability and accountability act of 1996 (hipaa)5 protectsindividual privacy but allows for the use of health records for researchwithout individual authorization. such research needs to be evaluated as nomore than minimal risk and needs to conform with a set of procedures andalternative methods to avert disclosure (e.g., meeting 18 specified criteriafor deidentification, having a qualified expert determine what needs to bedone to prepare the data for release).6 while researchers, data providers,and research analysts need to be mindful of legal requirements in planningtheir research, our purpose is directed to the ethical considerations thatshould guide collecting, gaining access to, analyzing, disseminating, or sharing such data irrespective of whether certain standards of privacy andconfidentiality are required by law.in emphasizing ethical considerations in research linking social andspatial data, we also do not intend to sidestep attention to the humanresearch protection programs in place at academic or research institutionsor the centrality of their irbs for approval and oversight of research. nordo we intend to minimize the challenge that can be involved in raisingcomplex ethical issues to irbs in areas in which the decisionmaking procedures are not yet developed. we do discuss the irb review process directly.our purpose in taking a broader approach to ethical decision making withsocial and spatial data is to focus attention on the research enterprise itselfand how best to weigh factors in planning and executing research or inusing or making accessible linked socialspatial data. we consider interaction with irbs to be a key step in that process. while irbs have directinstitutional responsibility for the review of protocols and determinationsabout human research protection as stipulated in the code of federal regulations for the protection of human subjects (45 cfr 46),7 we see thisinteraction between researcher (producer/user) and irb, and how to navigate it, as a part of the process of ethical decision making in human research, not as constituting that process in and of itself. furthermore, manydecisions having ethical implications are identifiable to the researcher notonly prior to interacting with the irb but also afterward; we regard theselatter decision points to be integral to the overall process of ethical conduct.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.126appendix bethical guidance and human research protectiongiven the social and behavioral science backgrounds of many of thoseengaged in social and spatial research, it might be expected that ethicalnorms would primarily derive from frameworks in these and adjacent fields.although there are variations among codes of conduct (e.g., whether or nota code of ethics explicitly encourages data sharing), general standards in thesocial sciences have much in common regarding such issues as informedconsent, intrusions on privacy, confidentiality and its limits, and benefitsand harm. whether the codes were promulgated in detail by the americanpsychological association (2003) or the american sociological association(1997) or in more summary fashion by the american anthropological association (1998), the association of american geographers (1998), theamerican political science association (1998), the american statistical association (1999), or the american association for public opinion research(2003), there is on balance considerable consistency in their guidance.one visible marker of specific interest in ethical considerations relatedto spatial data is the approval in 2003 of a geographic information systems(gis) code of ethics by the urban and regional information systems association (2003). by design, the code builds on a study of several dozen othercodes. it states, among other guidance, that the gis professional will protect individual privacy, especially about sensitive information; will encourage individual autonomy, including allowing individuals to withhold consent from being in a database, correct information, or remove themselvesfrom a database; and will avoid undue intrusions into the lives of individuals (urban and regional information systems association, 2003).exposure to research with human participants and related codes ofconduct is by no means uniform among scientists and other specialistsengaged in social and spatial research. experts in remote sensing and othersophisticated locational measurements are typically not from the social andbehavioral sciences or the health sciences, in which individuals or groupsare the focus of inquiry and in which ethical guidance emphasizes theprotection of human participants in research. thus, in addition to thescientific richness of this interdisciplinary arena of study, there is also thechallenge of fostering a deep appreciation among diverse researchers andresearch communities of the ethical issues at stake at each stage of theresearch process, from primary data collection through secondary use.a second challenge flows from the fact that there is very limited researchbased evidence about how ethical issues related to human researchprotection play out in the context of the collection or use of social andspatial research. in general, empirical study of ethical issues is far too scantacross even wellestablished domains of inquiry, let alone new areas ofresearch.8 the small body of literature addressed to linking social andputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.ethical issues related to linked socialspatial data127spatial data evidences an appreciation that this research area is a dynamicand fluid one and that expert knowledge can help produce research approaches that maximize advancing science consonant with human researchprotection principles. for example, armstrong, rushton, and zimmerman(1999) do so by examining alternative methods of masking individuallevelhealth data, testing the security of each approach for preserving confidentiality while permitting important uses. similarly, kwan, casas, and schmitz(2004) test three geographic masks with different perturbation radii toidentify the optimum tradeoff between data confidentiality and accuracy ofanalytic results. these forms of empirical examination hold promise ofproducing useful guidance. less directly, but also germane, kwan and lee(2004), using threedimensional geovisualization methods and activityðtravel diary data, found gender differences in time use, mobility, and travelpatterns, but at the same time they cautioned that òindividuallevel activityðtravel data geocoded to street addresses, given their reasonable degreeof positional accuracy, may lead to considerable risk of privacy violationó(p. 63).9the belmont principles as an ethical frameworkin addition to drawing on ethics codes, recent national commissions,and relevant national research council panels, contemporary discussionsof ethical considerations with social and spatial data (largely directed toissues of confidentiality) are taking place in the context of more than a 30year history of ongoing attention to these issues in research and writing.10more visible than any other, the belmont report articulated three overarching ethical principles that continue to offer a framework for responsible research conduct as well as form the basis of the code of federalregulations for the protection of human subjects (45 cfr 46). this report, issued by the national commission for the protection of humansubjects of biomedical and behavioral research, states the purpose of theseprinciples as follows (p. 3):three principles, or general prescriptive judgments, that are relevant toresearch involving human subjects are identified in this statement.other principles may also be relevant. these three are comprehensive,however, and are stated at a level of generalization that should assistscientists, subjects, reviewers and interested citizens to understand theethical issues inherent in research involving human subjects. these principles cannot always be applied so as to resolve beyond dispute particularethical problems. the objective is to provide an analytical framework thatwill guide the resolution of ethical problems arising from research involving human subjects.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.128appendix bthus, in setting forth these principles, the commission sought not to dictatebut to create a culture of ethical decision making that could effectively serveresearchers and irbs alike.the three ethical principles that are the foundation of the belmontreport are respect for persons, beneficence, and justice. depending on thecomplexities of a situation, the belmont report emphasizes that ethicaldecision making canñand often doesñrequire balancing competing claimsin order to accomplish the overall goals of the principles themselves. brieflyput, the principles are defined as:1.respect for personsñrespect for persons incorporates at least twoethical convictions: first, that individuals should be treated as autonomousagents, and second, that persons with diminished autonomy are entitled toprotection. . . . in most cases of research involving human subjects, respectfor persons demands that subjects enter into the research voluntarily andwith adequate information. . . .2.beneficenceñpersons are treated in an ethical manner not only byrespecting their decisions and protecting them from harm, but also bymaking efforts to secure their wellbeing. . . . the obligations of beneficenceaffect both individual investigators and society at large, because they extend both to particular research projects and to the entire enterprise ofresearch. . . .3.justiceñwho ought to receive the benefits of research and bear itsburdens? this is a question of justice, in the sense of òfairness in distributionó or òwhat is deserved.ó an injustice occurs when some benefit towhich a person is entitled is denied without good reason or when someburden is imposed unduly. . . .it is the application of the principles of the belmont report that leadsto considerations of informed consent, riskbenefit assessment, and theselection of subjects for research. as specified in the belmont report,respect for persons requires informed consent of research participantsñmeaning the provision of adequate information, participantsõ comprehension of that information, and their voluntariness to be part of the research. assessment of risk and benefits of research is closely related tobeneficenceñincluding an assessment of the probability of experiencing aharm, the magnitude of that harm (whether physical, psychological, legal,social, or economic), and the benefits that might derive to research participants or society from that research. the importance of risk reductionis also a concept emphasized in the belmont ethical guidance. the thirdbelmont principleñjusticeñis embodied in the requirement that the selection of subjects needs to be appropriate to the research and ought notputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.ethical issues related to linked socialspatial data129to place an undue burden on certain populations or disadvantage themthrough omission as research participants.11privacy and confidentiality are not explicitly mentioned in the belmontreport, although they follow from the principles of respect for persons andbeneficence and both are made explicit in 45 cfr 46.12 privacy refers tothe interest that persons have in controlling othersõ access to them andprivate information about them. individuals can vary in what they considerintrusive about themselves. in a research context, as long as human subjectswillingly agree to participate in the research, can freely decide against providing certain forms of information, and can end their participation at anypoint, they have preserved their privacy right to control their information.confidentiality refers to how data will be handled by researchers, otherdata producers, and ultimately secondary analysts consonant with agreements with human subjects regarding private information.13 a corollary toparticipantsõ providing access to information in this trusting relationship isthat researchers have the ethical responsibility to avoid intrusion on participantsõ privacy and to minimize the likelihood of harm from the disclosureof private information (both identity and attribute disclosure14). this commitment takes the form of a confidentiality agreement that provides assurances to research participants about what will be done with identifiable andprivate information about them. except when data are collected anonymously (i.e., without identifying information) or the researcher is collectingonly public information, the belmont principles of respect for persons andbeneficence lead researchers to consider confidentiality as part of the consent process and put into place data protection plans to reduce the likelihood of personal identification.like privacy and confidentiality, ethical guidance on data sharing canbe deduced from the belmont report, but data sharing is not explicitlyaddressed in either this document or in 45 cfr 46. much of ethical guidance in human research has focused on the intervention, interaction, andinformation acquisition processes. there has been far less attention to dissemination of results, access to data, or subsequent data use.15 the belmontprinciple of beneficence emphasizes the value of addressing benefits thatcan accrue to participants, similarly situated others, and the larger societyas well as to the entire research enterprise. broad in its scope, this principleis particularly applicable to weighing gains that can come from data sharingñincluding the verification of results, consideration of competing hypotheses, and examination of new questions.overall the belmont principles and derivative applications providedesiderata to help inform the ethical conduct of social and spatial research.since the belmont principles were developed primarily by physicians, theydo reflect a conception of harm and benefit more appropriate to biomedicalresearch than to social and behavioral science research. this emphasis isputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.130appendix bproblematic when the primary ethical concern is a possible invasion ofprivacy or a confidentiality breach due to using analytically precise coordinate data rather than when the concern is direct risk of physical harm.similarly, the notion of autonomy set forth in the belmont principles andoperationalized via informed consent is much harder to understand whenthe choice is whether to participate in a survey linked to a complex set oflocational measurements rather than when the choice is whether to participate in a treatment program that involves specific physical risks and benefits to the individual. nevertheless, although the belmont principles leaveroom for debate and uncertainty when applied to social and behavioralphenomena, the basic concerns of the principles and their emphasis onnuanced ethical decision making commend their use.by design, the principles offer not answers, but expectations for balancing important considerations in undertaking ethically responsible research.the belmont principles undergird the federal regulations for the protection of human subjects and are also pervasively used across fields of human research. their strength, however, lies in comprehending the flexibilitythat they were intended to foster, not in invoking them in a formulaicfashion. no ethical principles taken off the shelf can resolve dilemmas.thus, in using the belmont principles, researchers, data providers, andsecondary analysts need to extrapolate from them to think through howthey apply to social and spatial research.ethical considerations, the research context, andresearch planning in social and spatial researchethical considerationsin general, the collection, use, and analysis of linked socialspatial dataraise ethical issues that parallel those involved generally in handling identifiable, largescale data sets on individuals or groups, whether the data areacquired directly or indirectly, and specifically when research involves linkages among microlevel data. although not as powerful an individual identifier as dna or other genetic material used in genetic studies, precisecoordinate data in the social sciences is at once an identifier and a compelling social indicator that rivals most other forms of contextual measurement because it is locationspecific and can be collected repeatedly, inmultiple sites, and on a very large scale. it is rare, perhaps even unique, tohave a single measure or indicator essentially serve as an exact identifier,either alone or in combination with only a few other variables.the ethical principles and applications enunciated in the belmont report provide a framework for unraveling some of the complexities of socialspatial research. the ethical issues are at one level familiar ones: grapplingputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.ethical issues related to linked socialspatial data131with how best to honor confidentiality agreements with research participants, minimize risk of disclosure of private information and potentialharm, and maximize the benefits that can flow from research and access tothe data. the potential identifiability of individuals and groups in studiesinvolving linked socialspatial data makes it important for researchers toconsider informed consent and the situations in which it can be waived; thenature of confidentiality agreements and protections; the risk of breaches ofconfidentiality and steps to ameliorate that risk; the magnitude of anypotential harm from disclosure; and the benefits that can accrue to participants, their communities, or the larger society.attending to these considerations does not per se distinguish social andspatial research from other inquiries that cannot be undertaken anonymously or that involve identifiable and potentially sensitive personal information. with precise spatial data, the threshold for identifiability may belower than in research in which analytic measures are not also personalidentifiers, but the ethical principles shaping researchersõ responsibilitiesare the same. technological advances that can aid research can also contribute to increasing the probability of identification. for example, researchusing video recordings to study behavior in public places or that haveresearch participants use wearable computers to monitor movement andinteractions in work or social groups has considerable scientific potential,but it can also increase the risk of identifiability, even if the consequentharm is quite minimal. similarly, spatial measurements are sufficiently precise in that they are at once invaluable to research and yet could makedifficult protecting the identities of individuals and information about themfrom inadvertent or intrusive disclosure.the very complexity of undertaking research of this genre does notmean that the work inherently involves more than minimal risk in terms ofthe type of harm or the likelihood of its occurrence. also, research procedures can be put into place to reduce or ameliorate risk to a minimal level.responsible conduct in research commends the use of advanced measurements and technologies to maximize scientific progress and the benefits ofresearch while ensuring that any risk of harm for participants remains low.contexts of researchin research involving the linkage of social and spatial data, there are alarge number of persons who collect, use, or otherwise make decisionsabout how to maintain, preserve, and make such information available.depending on the context, different individuals connected with the researchmay take on various roles in the development of a particular human research protection plan or the articulation of a strategy that will engenderconfidence in data sharing and use. the basic principles underlying ethicalputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.132appendix bdecision making, whether by data producers or users, are no different fromthose in similarly complex, largescale studies about people and their livesin which there can be data from multiple sites, multiple sources, and multiple time points. in all such research, there is an interest in and commitment to enhancing access and use in order to maximize addressing important issues while ensuring that confidentiality agreements are honored andthe risk of personal identification is minimal.linkages between spatial and social data are being made by researchersat every point in the research enterprise, from primary to secondary use.for example, investigators are specifying designs that incorporate precisecoordinate data in the research (e.g., home, workplace, school, recreationcenter; more than one location) or link to extant databases that provideprecise coordinates. secondary analysts, too, are examining individual,household, or grouplevel behaviors by using data that have those links orby enhancing those data through integrating additional resources. even inthe absence of precise spatial data, the merger of two deidentified databasesor one set of public records and one or two deidentified databases raises thepossibility of the reidentification of research participants. identification iseven more likely when highly refined locational data are in the mix and areintended to be used as analytic variables.the data producer and user face particularly challenging circumstances when they generate new data or pursue data integration, analysis,dissemination of results, and sharing or transferring of these data to others. the archivist and the database manager also have responsibilities forhow such data are to be preserved, stored, and potentially used.16 finallythe secondary analyst has the ethical responsibility to honor agreementsfor access, which include those agreements made with research participants as to use.purposive planningfrom the vantage of human research protection and review of researchby an institutional review board, there are some immediate ethical questions for primary researchers and secondary users to consider. it is optimal,for example, to determine in advance whether data collection or linkedanalyses will be individually identifiable only by virtue of obtaining andusing locational data; whether or not the consent of research participantswill be obtained and, if so, in what form and with what assurances; andwhether the likely benefits and the potential harms can be specified, and, inthe case of potential harms, whether steps can be taken to ensure that theyare low (e.g., embarrassment versus legal liability) and the risks of theiroccurrence are minimal (through strong data protection or access plans). aprimary data producer and user can consider most of these issues in adputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.ethical issues related to linked socialspatial data133vance of initiating research or can specify them for followup review, up toand including strategies for data dissemination and sharing.the secondary analyst does not create the data or the conditions for theresearch; nevertheless, she or he needs to develop a research plan consonantwith confidentiality protections and needs to seek irb review to the extentthat the new work contemplates the integration of heretofore unlinkeddatasets or spatial measures.17 in the case of secondary data, the dataarchivist, data collector, or initial researcher can require licensing or othercontractual arrangements with the secondary user or her or his institution,or the secondary user may need to work in a data enclave or otherrestrictedaccess environment in order to use the data. each of these stepsadds a level of review as a condition of access, controls the nature of thataccess, and includes the force of law to enhance confidentiality protections(see national research council, 2000, 2005).18 the extent to which suchsteps are necessary or appropriate depends on whether there is more than aminimal risk of disclosure and the probability of harm that any disclosurecould entail.ethically responsible conduct in the collection or use of social andspatial data is sufficiently complex that it requires a planned, deliberativeprocess. one useful way to think about the preparation of a protocol forreview by an irb, as well as the review process itself, is as a structuredopportunity for primary researchers or secondary analysts to present to agroup of peer scientists and community members a human research protection plan and approaches for undertaking sound and ethically responsiblework. because of the challenging issues involved in human research protection with social and spatial data, there are core ethical questions that needto be addressed: is this human subjects research? does the use of precisecoordinate data add value to the topic under study? what is the process forgaining consent or the rationale underlying a request for a waiver of consent? how are issues of confidentiality to be addressed? what are thebenefits of the research, and what are the risks of harm and strategies foramelioration? each of these issues is considered in the next section.the belmont principles and questions to guideethical decision makingthe principles and standards specified in the belmont report provide auseful tool for the responsible planning and implementing of social andspatial research. for example, they can guide in assessing whether exactspatial data affect determinations of what constitutes human subjects research; judging the risks and benefits of certain research topics; and sortingout issues of confidentiality, data access, and data sharing. fundamental toweighing how research can be done, how research data can be secured, andputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.134appendix bhow access to data can be provided are considerations of respect for theautonomy of human subjects, appreciation of their voluntariness, and assessment of the benefits for subjects and the research enterprise while assessing the risk of harm, the justness of inquiries, and the equitable distribution of benefits and burdens. these ethical principles help to frame questionsthat inform responsible decision making.human subjects researchsocial and spatial research that otherwise involves no interaction orintervention can become human subjects research as defined in the federalregulations for the protection of human subjects because precise coordinate data allows for personal identification. the belmont principles aredirected to the conduct of research with human subjects, and these principles shape the boundaries of what constitutes human subjects research inthe federal regulations. there is considerable research in the social sciencesusing public records or other information that is publicly available or observable that is not human subjects research, even though it meets theresearch standard of contributing to generalizable knowledge.19 information gathered without intervention or interaction with individuals or without identifiable private information20 is considered to be outside the scopeof human subjects research. also, identifiable information about individuals that is publicly available is not identifiable private information, andhence it is also outside the scope of human subjects research.highly refined coordinate data can shift otherwise public informationto the category of private identifiable information and thus human subjectsresearch. for example, anonymous data on peopleõs personal health habitsbecomes identifiable when linked to spatial data describing, with considerable accuracy, where a person lives. such precise spatial data, coupled withother demographic descriptors of individuals, may enable an intruder todeductively identify individuals. this is a changed circumstance producedby major advances in observation technology and the capacity to recordand store such information. until recently, locational mapping, aerial photography, and other mechanisms to depict spatial relations were not sophisticated enough to yield private identifiable information and thus were outside the definition of human subjects research. the same transformationhas occurred in the context of individual observation in public places wherenotetaking has been replaced by audio or video recording, and the potential identifiability of recorded data in public places can make researchpreviously considered outside the definition fall under the scope of humansubjects research.it might be expected that the capacity to make refined measurementswould lead data providers and secondary users to seek to have access toputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.ethical issues related to linked socialspatial data135these data. secondary use of datasets that are not identifiable and areavailable for public use have less scientific potential but do not create thesame concerns about disclosure risk. secondary analysts need to weighwhat forms of data meet their needs and what benefits to research may belost without the use of more precise locational information.21many of the sophisticated techniques that have been employed to preserve and optimize the analytical value of data to secondary users can begeneralized to social and spatial data. data releases can vary depending onthe needs of the secondary users. for highly qualified secondary users, theuse of enclaves, licensing, and other related mechanisms, as described byrodgers and nolte (2006), can enable the secondary user to enjoy the samerichness and usefulness that was available to the primary data user. alternatively, judicious decisions by a disclosure review committee may result inthe use of techniques, such as data swapping and suppression of geographicdetail, and render the data appropriate for broader dissemination to secondary users (see, for example, oõrourke et al., 2006).ethical responsibilities follow for researchers engaged in data collectionor the analysis of data in which information is identifiable. ethical researchwith known human subjects requires that they be aware of and informedabout the research, that they agree to participate in it, that their information be treated in confidence, and that there be benefits to the work thatoutweigh the risks of harm. with known persons, researchers have fiduciary obligations to these individuals as part of the compact of their participation. if secondary analysts are studying data that are similarly identifiable, they also have the same obligation to honor agreements that havebeen previously made.topics of researchtopics of inquiry vary in sensitivity and the likelihood that researchparticipants may believe that they are sharing information that is highlypersonal and private. there are individual differences among participantsas to their boundaries of privacy and what they are willing to share withresearchers. these differences are exacerbated when it is not only the primary researcher but also others, later, who may gain access to individualswho are seeking to keep private their status, condition, or personal information. individual differences in peopleõs desire to control who has accessto them and to information about them are likely to arise in some of thekinds of research that include spatial linkages to social data. for example,research on domestic violence, crime, stigmatized diseases, and naturaldisasters would be enhanced by geographic display of incidence data. manypersons in these circumstances are quite willing to participate in researchand view quite favorably the opportunity to speak to a researcher or be partputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.136appendix bof a larger enterprise. others, however, who have been traumatized, perhaps repeatedly, are likely be highly sensitive to and fearful of invasion oftheir privacy and any consequent, remotely possible intrusion on their person or their social circumstances.22attention to the justness principle helps to assess whether the needs ofcertain populations commend the use of spatial data because of increasedbenefits that can derive from the research (e.g., vulnerability to toxicwaste), or whether certain populations may be more vulnerable to beingstudied and to researchers seeking access to personal information (i.e.,inequitable burden). linked socialspatial data could add to knowledgeon very personal, yet highly important topics (e.g., studying the relationship between health risks and access to health resources) that researchparticipants and the larger society would value. alternatively, such datacould increase the vulnerability of already vulnerable populations tostigma or other forms of harm (e.g., studying drug use patterns proximalto highcrime òhot spotsó). the key ethical questions include: to whatextent does linking social and spatial data add to the importance of theresearch? to what extent does it add to the risk of disclosing personalinformation? how will the researcher or secondary analyst explain thebenefits of the study and the value of social and spatial links to researchparticipants and to the larger society?23consent and confidentiality agreementsinformed consent of research participants is the standard ethical requirement for human subjects research. researchers have an ethical responsibility to show respect for persons and earn their trust based on the assumption that people have agreed to participate on a voluntary basis andwith sufficient information and understanding to make a decision. as specified in the belmont principles, the standard is one of subjectsõ having òsufficientó information. the principles allow for incomplete information incertain circumstances to ensure the validity of the research as long as therisk is minimal and the information being withheld does not pertain to risk.typically, as part of the compact between researcher and research participant, consent to participate also includes an agreement to treat informationas confidential and to ensure that no personal identifiers would discloseeither subjectsõ participation in the research or information about them.24the addition of finegrained spatial data makes implementing this promisean additional challenge.25in making determinations about consent, the nature of consent, andwhether to seek a waiver of consent under the federal regulations,26 researchers and others collecting highly identifiable spatial data need to assesshow they will approach the process of obtaining consent and whether andputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.ethical issues related to linked socialspatial data137under what circumstances they would seek a waiver of consent. for example, could a waiver be appropriate in a study of atrisk driving patternsusing court records to identify driversõ license suspensions and home coordinate data to study car use, but in which no direct interaction with subjectsis anticipated? given the importance of ensuring personsõ autonomy toparticipate in research and also of maximizing public trust in research,circumstances justifying waivers of consent require close scrutiny.when spatial data are collected along with social data, it is importantfor researchers as well as irbs to consider how the need for and use ofcoordinate data should be described in obtaining informed consent, whatagreements of confidentiality should be made, and how explicit researchersshould be about secondary or unanticipated use.27 without some explanation, it is not reasonable to expect that research subjects would understandeither the potential risks or the benefits of social and spatial data. there isgood general guidance in ethics codes and in recent reports on informingresearch participants about future data use that is equally applicable toprimary researchers and data producers engaged in social and spatial research (see, e.g., national research council, 2005, recommendation 14,pp. 8081). nevertheless, when new media and their conceivable risks areexplained, it is all too easy for the researcher to assume that the potentialresearch participant understands the terminology used to explain the technology and its risks, and it is likewise too easy for subjects to pretend tounderstand rather than appear uninformed. moreover, such problems ofmiscommunication are likely to vary across different sectors of the subjectpopulation. such techniques as cognitive interviewing can be usefully employed both to develop informed consent language that is understandableto the various relevant sectors of the population and as probes to evaluatecomprehension by consenting individuals (willis, 2006).there are instances in which the consent of research subjects may notbe possible for obtaining or using linked socialspatial data. such instancesare most likely to arise in the contexts of unanticipated or secondary use.secondary analysts may seek to use social and spatial data for which therewas no previous agreement about multiple research use during the originaldata collection. also, primary or secondary researchers may identify a subsequent use for linked data for which recontacting research subjects toobtain consent may not be feasibleñeffectively making the research impossible if consent was required. in determining whether to seek waivers ofconsent, researchers need to weigh obligations to research subjects and tothe scientific enterprise, as the belmont principle of beneficence specifies.under such circumstances, salient ethical questions include: is the researchof minimal risk and sufficient potential benefit to commend being pursuedwithout consent? will the researcher operate consistent with any priorconfidentiality agreements, extrapolating to this circumstance? can theputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.138appendix bidentity of research participants and information about them be protectedin light of the privacy needs that people typically have?benefitsin general, the benefits of social science research typically accrue tosociety or to people with similar conditions or circumstances to the researchsubjects, rather than to individual subjects. consonant again with the principle of beneficence, research participants should be made aware of this,and also of the fact that benefits derive from the accumulation of scientificknowledge based on information that they provide or make accessible andfrom having that information enhanced by linking to other information.researchers can also communicate to participants the benefits that canderive from making the information they provide available to other qualified researchers who can reexamine findings or ask new questions using thesame information.28the benefits of precise spatial measurements can best be understood inthis context. more extensive measurement of contextual variables, such aslocation permits identifying and explaining patterns and differences on agroup, community, or societal scale. emphasizing these benefits does notmean that individuals do not themselves reap personal benefits commensurate with their time and engagement. typically an aspect of ethically responsible research is to provide some tangible benefit to participants. in thecase of a heath survey with precise coordinate data, it could, for example,be a handout of proximal health clinics and routes of public transportation.with unsavory or undesirable human subjects, benefits may not accrue, butneither should direct harm due to their willingness to participate, assumingthey are aware of ethical and legal limits.29overall, in assessing and communicating the benefits of research, thesalient ethical questions for researchers include: are the research participants or their communities likely to benefit from more geographically explicit research? are they likely to receive far fewer benefits if the use ofgeospatial data is severely restricted? can researchers provide research participants or their communities with added benefit by adding the geographically specific information? can the potential benefits of linking social andspatial data be reaped without research participants being exposed to undue risk of harm or disclosure? can the researcher set forth the benefits ofsuch data and not overpromise?risk, harm, risk reduction and confidentiality protectionas with assessing benefits, the assessment of risk in social and spatialresearch needs to identify both general risks associated with the researchputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.ethical issues related to linked socialspatial data139and any increased harm or risk of harm due to the use of precise locationalinformation. for all studies, there are potential risks at each stage of theresearch enterprise, including in the identification or specification of thesample, data collection, database construction, data analysis, disseminationof results, and data sharing. along with the benefits that derive from usingexact coordinate data is some greater risk of disclosure made possiblethrough the use of a readily identifiable variable.the belmont principles appropriately emphasize the distinction between risk of harm and severity or magnitude of harm and that the òbenefits [of research] are properly contrasted with harms rather than the risk ofharm.ó except at the extremes, determinations of level of risk and types ofharm are frequently confused.30 social and spatial research on highly sensitive topics for which physical, psychological, legal, or economic harms areconceivable (e.g., a study of mobility patterns and selfprotective behaviorof abused spouses) place a higher burden on ensuring that preserved datahave a very low (approaching zero) risk of disclosure and are protected bya very secure data protection plan.31 in some instances, researchers maywish to obtain certificates of confidentiality from federal agencies to protect, to the extent possible, some forms of data from forced disclosure.32much research of importance in social science is not on highly sensitivetopics, and the primary risk of harm may take the form of transitoryembarrassment, stress, or discomfort. even under such circumstances, however, efforts to reduce the risk of disclosure remain important because ofthe ethical value placed on honoring agreements with research participantsand the ethical principle of making information on participants accessibleonly if essential to addressing research issues. with linked socialspatial data, there is an incremental risk of breaching confidentiality and the potential for disclosure due to the value ofpreserving and using precise locational information. there is an ethicalobligation to minimize disclosure risk generallyñeven when it remainsminimal. precise coordinate data may continue to have analytic meaningfor many years, but risks associated with its use may reduce over time asmigration and other life course changes alter the identifiability of thesedata. nevertheless, in implementing ethically responsible research and planning for access, issues for consideration include: what technical approachescan be used, and to what extent should they be used to reduce the identifiability of social and spatial data while still retaining their scientific andanalytic value? what do researchers and others who produce, manage, ordisseminate data need to know to minimize risk of disclosure? what formsof data protection plans and models of restricted access are most promisingto maximize the use of data and to minimize dual use (that is, unanticipatedand adverse use by an intruder)33 or inadvertent disclosure? to what extentare different strategies or guidelines needed at different stages of researchputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.140appendix b(e.g., strengthened certificates of confidentiality,34 guidelines for researchreporting) in order to minimize disclosure risk?planning and implementing ethically responsible research, whether asprimary researchers or secondary users, involve addressing this spectrum ofissues. these questions serve to illustrate what needs to be asked by researchers, by data providers, by funders, and ultimately by review groupslike irbs in undertaking social and spatial research. the process is muchmore nuanced than a simple determination of how and at what level precisecoordinate data must be masked to maximally reduce identifiability andpotential breaches of confidentiality. if the data can be adequately protected from intruders, if inadvertent disclosure can be sufficiently reduced,if the risk of exposure is low, and if the harms from any exposure are onlyof minimal or transitory impact, then the core considerations to allow forethical use have been met. thus, the emphasis on strong data protectionplans and conditions of responsible use is as important as masking databeyond a point at which its value would be substantially compromised.ethics of dissemination, sharing, access, and theconfidentiality nexusas the foregoing discussion has emphasized, ethical decision makingprominently includes attention to issues of confidentiality, but ethical considerations are larger and more comprehensive than confidentiality alone.35because of the considerable scientific value of using precise coordinate databy primary researchers and secondary analysts, there is an inevitable tension between data dissemination or sharing and doing so consonant withthe promises made to research participants not to disclose their identities oridentifiable personal information about them. what are at once soundethical standardsñmaximizing scientific gains from available informationand ensuring that promises of confidentiality are kept to research participantsñcan conflict if the advancement of one compromises the other.disseminationethical decision making in human subjects research typically focuses onissues that relate to identifying research populations and informing themabout the study, gaining their agreement to participate, and minimizing theprobability of any harm or risk of harm that might occur during the conduct of the research or with information gathered through it. ethical responsibility as it relates to other steps in the research process, in particularresearch reporting and dissemination, is far rarer in discussion and decisionmaking related to human research protection. beyond confidentiality guarantees and cautions with respect to personal identification or the identifiputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.ethical issues related to linked socialspatial data141ability of personal information, much is implied but little is elaborated onin the federal regulations. the belmont principles and related standardsprovide useful guidance for greater attention to the dissemination phase.first, the belmont principles are explicit about the need to contribute tothe larger body of knowledge. also, they imply that fair treatment of thosewho participate in research includes the dissemination of results; beneficence depends in part on the dissemination of valid new knowledge. inaddition, ethical standards related to reporting on research require that thedata underlying results need to be presented in sufficient detail to permitreaders to follow the logic of inquiry and assess the warrants underlyinginferences.36 these objectives need to be considered in the context of howinformation is publicly presented with linked socialspatial data.the presentation of precise locational information can enhance contributions to knowledge, but, with locational data, the form of presentation ofresearch results may require special measures or procedures to be as transparent as possible without risking disclosure of the identity of researchparticipants. how will the data be presented or displayed to avert thelikelihood of identifying research participants or the potential misuse offindings? how will the research methodology and design be described toallow for maximum transparency and the accumulation of knowledge butwithout risking inadvertent disclosure?depending on the precision of the locational data and the rarity of thesocial data that are linked, even a map display could reveal the identity ofspecific individuals without mention of any specific names. a study of drugusers and their dispersion and density in a community may add immeasurably to knowledge of how social networks contribute to atrisk behaviors,but also published maps by household could be tantamount to publishedaddress books in certain neighborhoods. thus, it may be necessary in presentations or published work to coarsen the displays, swap data, or extrapolate to similarly situated geographic spaces in the same or an equivalent neighborhood, or take other steps that allow for the reporting of resultswhile preserving the confidentiality of linked socialspatial data.37sharinggiven work of the scope, size, and significance of social and spatialresearch, the ethics of inquiry commends data sharing on the part of primary researchers and data collectors. like the dissemination of results, datasharing also contributes to the important belmont principle of contributingto the accumulation of knowledge. the belmont report emphasizes, as anelement of beneficence, the improvement of knowledge and the benefitsthat can accrue to society in the form of knowledge to be gained fromresearch. data sharing in science can be seen as a means to that end:putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.142appendix bfundamental to science is a commitment to openness of inquiry that enablesthe selfcorrecting and cumulative advancement of knowledge.38prior to about 1975, openness regarding human subjects data wasmanifested by the way research methods were described and data werepresented in publications; replication of results established the validity andgeneralizability of results.39 in 1985, the national research council, underthe leadership of its committee on national statistics, published the reportsharing research data, which was influential in its reach (national research council, 1985).40 almost immediately, the division of social andeconomic science at the national science foundation (nsf) took up therecommendations in that report and established a data archiving policy(national science foundation, division of social and economic science,1986). by the late1980s, some federal funding agencies, most notably nsf,began to encourage more formal sharing of documented data and materialsin all areas of science.41 by the time the national institutes of health (nih)fully elaborated its policy in 2003,42 the ethical underpinning and normative value of data sharing were quite evident both in official policy andrelated educative materials.43ethical conduct in research involves not only attention to the value ofdata sharing but also doing so consonant with confidentiality agreements.researchers and data producers need to plan for data sharing and the formsthat data sharing can take. especially with social and spatial data and otherforms of information that may be readily identifiable, primary researchersand data producers need to ensure that research subjects are sufficientlyinformed about potential use of the data and to develop data sharing plansthat can reasonably be expected to protect the identities of human subjectsand personal information about them. as noted earlier, even with researchin which potential harms are minimal, the broader commitment of honoring confidentiality agreements with research participants looms large evenif the consequences of disclosure of personal identifiable information aresmall. also, the reputation of human research with the general public willgreatly influence the willingness of individuals to participate in research inthe future.from the vantage of ethically responsible research, the articulation ofdata sharing and data protection plans appropriate to the research go handinhand. with largescale social and spatial data (including that collected atmany sites or over longperiods of time), there is the potential for considerable future use. thus, gaining the consent of research participants couldreadily include noting that other researchers will have an opportunity toanalyze the information. if a study is on quite personal or sensitive topics,primary researchers either could explain that information provided to others would be altered in such a way that identification would be virtuallyimpossible, or they could indicate that other researchers can have access toputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.ethical issues related to linked socialspatial data143identifiable information only under restricted conditions in which they commit to honoring confidentiality agreements. if the researcher plans to sharedata through a thirdparty provider (e.g., a public archive), both the researcher and the provider need to anticipate whether the level and richnessof the linked coordinate and social data allow for public use data files(typically limiting the detail that can be provided) or alternatively whetherrestricted access arrangements need to be made (e.g., licensing agreementsor access at controlled sites).there is good general guidance for investigators and for institutionalreview boards on specific ways to protect the privacy of human subjectsand the confidentiality of the data.44 there is need to develop and testapproaches for providing access to precise coordinate data that can maximize the analytic potential of these measurements without risking the disclosure of identifiable information in primary or secondary use. our purpose here is not to elaborate on the methodologies, the processes for sharingdata (e.g., under the direct auspices of researchers, through a data archiveor enclave), or even the timing of data sharing (e.g., released in waves forlongitudinal study). our aim is to underscore the ethical basis for datasharing and that data sharing and data protection can best be addressedtogether by researchers and by irbs.accesssecondary users of publicuse data or restricted data files have an ethical obligation to contribute to the advancement of knowledge in accordance with the agreements made to produce these resources. the ethicalobligations of primary researchers extend to secondary analysts. secondaryanalysts are reliant on the trust provided by research participants in theresearch enterprise, and thus the obligation of secondary analysts is notaltered by the fact that they were not themselves party to any promise withthe human subjects of research.public archives like the interuniversity consortium for political andsocial research (icpsr) explicitly set forth the obligations of secondaryanalysts for responsible use.45 with data that are either publicly availableor available through limited or restricted forms of access, typically researchers have an ethical requirement to use data in their current form,without the integration of additional data or enhancements of other information, unless they take additional steps to assess the ethical issues relatedto an expansion or change. except for data that are publicly available, thisobligation also includes not otherwise sharing data with tertiary users. insocial and spatial research, this guidance on secondary use is particularlyimportant. secondary analysts who seek to add precise coordinate information need to examine the ethical aspects as well as the feasibility of doing soputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.144appendix bresponsibly, and they need to consider issues of consent. in such instances,researchers may request waivers of consent, as noted previously, in seekingirb approval. an irbõs ethical responsibility is to assess the reasonablenessof waiver requests in light of the potential risk of harm and steps that willbe taken to ameliorate that risk.contributing to the advancement of knowledge also obligates secondary analysts to acknowledge the sources of data as part of disseminatingtheir results. to the extent that beneficence includes both an obligation tocontribute to the wellbeing of research participants and the larger publicgood, acknowledgment of the connections between the new research andthe initial research helps to ensure cumulative benefits. most ethical guidance includes secondary analysts also acknowledging any assistance thatthey have received from primary researchers in gaining an understanding ofor access to such data.ethics education and training forsocial and spatial researchthe scientific potential of linked socialspatial data and the complexissues involved in responsible social and spatial research raise questionsabout how best to prepare researchers, data managers, data stewards, andsecondary analysts, among others, to engage in such work. typically preparation for research of such complexity and sophistication focuses on issuesof methods and measurement: at the data collection stage, research preparation tends to emphasize what information to collect and preserve andhow best to ensure that different forms of data at different units of analysiscan be meaningfully gathered and linked. primary data collection includesobtaining the consent of research participants, but practices may vary widelyas to whether data sharing or future use is noted as part of that process.46at the data management, analysis, and dissemination stages, research preparation focuses on how to store or provide access to data at varying levels ofdisclosure risk or turns to technical and statistical questions about how toretain scientific value without jeopardizing confidentiality agreements.these are all important issues for those engaged in producing or usinglinked socialspatial data, but, in these contexts, guidance is aimed at beingmore instructive about the requirements for use than educative about them.attention to confidentiality, inadvertent disclosure, requirements with respect to any data enhancements or linkage are considered part of the process of providing access to datañwith any heightened sensitivity to ethicalissues at this stage being a secondary benefit.47ethics education and training are not an explicit component of mostgraduate education programs. ethical considerations across social and behavioral science fields and specialties are generally addressed sporadicallyputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.ethical issues related to linked socialspatial data145in courses or in the supervised training and mentoring that more experienced researchers provide. scant materials are available for course or classunless one enters the specialized literature on human research ethics,48contributing to facultyõs giving limited attention to ethical considerations asan aspect of methodology except among those with specialized interests orexpertise in this subject. added to this, the rapidly changing circumstancesrelated to confidentiality issues due to advances in computer technologyand finegrained identifiable measurements (in areas like coordinate data orvideo recording) make for more questions than answersña factor likely tofurther discourage curriculum development by the nonexpert. in this context, social and spatial research is no exception. 49in recent years, irbs are explicitly encouraging researchers to takecourses50 (typically available on the internet requiring approximately onehour). in the case of research to be funded by nih, since 2000, education isa requirement of receiving nih support.51 irb members are also required toundertake training to serve in this role. further underscoring the importanceof education and training, since 2005,52 as part of their assurance of compliance with the public health serviceõs policies on research misconduct, institutions have a general responsibility to foster a research environment thatpromotes the responsible conduct of research and research training (withtraining responsibilities covering human subjects and data acquisition, management, sharing, and ownership among other issues).53 the office of research integrity54 promotes educational activities and has oversight of institutional assurances. the current emphasis on responsible research conduct aspart of the regulatory clime could support a shift in attention to ethical issuesif it could be meaningfully encouraged by federal agencies and meaningfullyimplemented by researchers and their institutions.research societies in the social and behavioral sciences have sought tofocus greater attention on human research ethics among their members andin departments that train in their fields (see, e.g., iutcovich, kennedy, andlevine, 2003; levine and iutcovich, 2003). sessions at annual meetings,courses, and workshops are not uncommonñalthough attendance is variable. over recent years, the american sociological association, the american anthropological association, the american educational research association, and the american historical association, among others, haveincluded human research protection issues on the agenda of the meetings ofdepartment chairs, directors of graduate programs, or, in the case of education research, graduate school deans. the american statistical associationhas a portion of its website dedicated to information and resources onconfidentiality and privacy.55 the social and behavioral science workinggroup on human research protection, supported under a contractualagreement with the nih office of social and behavioral science research,has issued educational documents, prepared course material, and convenedputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.146appendix bcourses at research society meetings since 2002.56 directed to issues ofcentral concern to the social and behavioral sciences, these offerings focuson such core topics as consent, confidentiality, privacy, benefits and risks,and the use of public and restricted data files. although these efforts are notspecifically focused on social and spatial research or issues involved in thecollection, management, and analysis of linked data by primary researchersand secondary analysts, they provide some basis for further targeted work.as noted earlier, the goal of balancing access with the protection ofconfidentiality is set forth in written materials by data stewards to informpotential users.57 data providers (e.g., public archives or research teamsproviding their own direct access) also note their responsibility to trainthose engaged in data preparation, database management, and the reviewof requests for data access to avert inadvertent disclosure. collaborativeefforts across research societies and stewardship organizations could verywell provide a framework for both offering highquality education andfurther encouraging graduate departments to do so as an integral part oftheir training programs. outreach should also include efforts directed tofields of science engaged in social and spatial research but with less experience in human research and related ethical issues.that ethical requirements can be seen as hurdles by researchers andusers is understandable given that what is required can be mechanistic inmany instances or oblique as to its goals and intent. attention is rarelyfocused on sensitizing researchers, database managers, or users to ethicalconsiderations or how to weigh them in undertaking or being a part ofsocial and spatial research. this situation is by no means unique to socialand spatial research, however. despite the expanded requirement that researchers take various online courses in human research protection to certify to irbs that they are prepared to undertake research, there is littleformal preparation in the undergraduate or graduate curriculum directed tothe ethics of research and responsible research conduct.ethics education is often conceived as a topdown activity in whichirbs and irb specialists educate irb members, researchers, and students. amajor deficiency of this approach is that it tends to present generalities andto overlook the commonly observed fact that the devil is in the details.58 inthe case of social and spatial research, this problem is accentuated by thefact that certain issues, such as the fineness/coarseness of the data, are atechnical matter, as are various ways of intruding on the data set or protecting the data set from intrusion. hence, it is particularly important thatsocialspatial data specialists are prominent in the development of ethicseducation in this realm, via textbook chapters, national and regional workshops, and journal articles.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.ethical issues related to linked socialspatial data147final thoughtsthis paper has sought to provide ethical guidance to those involved inproducing, using, preserving, managing, and analyzing spatially linked data.our aim was to present an overview of the ethical issues that come into playin the research process, from design through data sharing and dissemination. it is only recently that the capacity to collect precise coordinate dataover many locations and points in time and to link them to social data hasdeveloped to a point that raises human subjects issues. using the belmontprinciples as a base and extrapolating from them, we have sought to examine ethical considerations and how they might be weighed here.this paper seeks to raise issues, not only for those involved in socialand spatial research, but also for those engaged in the review of it. inrelation to irbs, we recommend a highly proactive approach, since irbswill be largely unaware of this complex new situation, and either naivelyoverlook serious risks or, in the absence of good communication and a onestepatatime approach, could introduce barriers that could unnecessarilylimit such research.the opportunities for linking to important forms ofdata should not be avoided, for example, because they were not anticipatedin advance when approval was initially sought or because the risks of harmcould not be sufficiently assessed at a prior point. the irb process allowsfor continuing review with provisional approval. thus, under certain circumstances, researchers may want to provide a broad map of their workand follow up with subsequent review when it becomes germane. documentation would grow with each new addition or use of the data, but theresearcher would not need to anticipate all uses too far in advance.59data producers and users who intend to undertake research involvinglinked socialspatial data will need to take time early in the planning stagesto begin conversations with appropriate members of their irbs.of course,risks will emerge that are unforeseen; hence, the conversation must includesome discussion of this possibility.there must be an understanding thatthese risks will be discussed openly and immediately with the irb andincorporated into the data documentation. this rapport could deter naiverisk taking by researchers or riskaverse actions by the irb.as implied by the observations above, research protection programs atacademic or research institutions need to support and encourage irbs tofunction as ethically effective decision makers. institutional programs canbe established and approved that allow irbs to avoid mechanistic application of rules and to use the flexibility accorded to them. openness to thecoordination of multisite review or to preapproval for certain types of timesensitive data collection are just two procedures that irbs could introduceto facilitate review of social and spatial research consonant with humanresearch protection. an emphasis on confidentiality and data protectionputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.148appendix bplans and a willingness to use waivers of consent could similarly facilitatediscoveries related to geographic location while strictly adhering to minimal risk standards.in social and spatial research and in human research more generally,irbs and researchers, data producers, and secondary analysts would servethe advancement of knowledge and the ethical conduct of science throughtaking an unusually collaborative and collegial approach. part of doing sorequires essential expertise on irbs or irbs involving expert consultantsrelated to the technical and ethical issues involved in social and spatialresearch. representation on the irb of scientists knowledgeable in spatialmeasurement, in data disclosure methods, and in approaches that can ameliorate risk would be optimal when there are sufficient numbers of relevantprotocols. the promise of social and spatial research is so significant that itis incumbent on those who propose research and those reviewing it toproceed cognizant of the contribution of research participants and committed to benefits for all.notes1.as used here the term embraces all of the mechanisms that permit the identificationof a location through latitude and longitude coordinates. the magnitude and speedof obtaining such information due to advances in remote sensing (from satelliteimages to highresolution aerial photography) and global positioning systems (gps),coupled with the growing sophistication of geographic information systems (gis) tostore and manipulate such data, have accelerated interest in research use and applications.2.for an excellent overview of this rapidly emerging field, see national researchcouncil (1998).3.to date, much of the attention on balancing data access and considerations ofconfidentiality has focused on federal statistical data collections, administrativerecords, and other public resources (see, for example, national research council,2005; lane, 2003; de wolf, 2003). for earlier consideration of these issues, seeduncan (1993), national research council (1993), and u.s. general accountingoffice (2001).4.two important examples relate to research ethics in complex humanitarian emergencies (see national research council, 2002) and with victims of disasters (seecollogan, tuma, dolansewell, borja, and fleischman, 2004; collogan, tuma, andfleischman, 2004). for a general consideration of challenging research circumstances, see national bioethics advisory commission (2001).5.public law no. 104191, 110 stat. 1936 (1996).6.for useful guidance, see de wolf, sieber, steel, and zarate (2006).7.the federal regulations for the protection of human subjects were adopted in1991 and subpart a (known as the common rule) was accepted by 17 federalagencies as policy. only research funded by these agencies needs to be considered byan irb at the relevant institution, but institutions under their assurance of compliance with the federal regulations (filed with the office of human research protections; available: http://www.hhs.gov/ohrp/assurances/assurancesindex.html) generputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.ethical issues related to linked socialspatial data149ally require that all human research receive irb consideration, whether or not thework is extramurally funded, or whether it is funded by federal agencies (beyond the17 signatories) or private foundations.8.calls for empirical research on human research ethics have increased in recent years.there is general awareness that human research considerations are shaped by toomany assumptions about research participants (see, e.g., levine and skedsvold,2007). recent reports from the national research council addressed to issues ofdata access are strong in their calls for research (see, e.g., national research council, 2005, 2003a). in 2006, the journal of empirical research on human researchethics, published by the university of california press, was established to serve as aforum for empirical research on such issues.9.research, for example, that graphically displayed individuallevel activity patternsñleaving from home to work but stopping to have coffee with friends rather than toarrive promptly for business meetingsñcould encroach on personal privacy and runemployment risks if confidentiality were breached.10.for brief recent histories relating to the social and behavioral sciences generally, seenational research council (2003a); also see the section on emergence of ethicalconsiderations and related cites in levine and skedsvold (2007).11.respect for persons, riskbenefit, and justice are key considerations as they relate tothe autonomy of subject populations to participate in research and to ensure thattheir doing so is equitable in terms of inclusion as well as exclusion. for an important example of attention to ethical considerations in the conduct of research involving prisoners, see institute of medicine (2007). the committee undertaking thisreport sought to reexamine and address such important issues as what constitutesprisoner populations, whether review of research should shift from categories ofresearch to a riskbenefit approach, and how justice might best be understood in thecontext of an ethical framework.12.private information is one of the defining characteristics of research involving human subjects at 45 cfr 46.102(f); that is, information obtained in a context inwhich an individual might reasonably expect that no observation or recording istaking place or information that a person would reasonably expect will not be madepublic and is individually identifiable by the researcher. subsequently, in settingforth the criteria for irb approval of research at 45 cfr 46.111(a)(7), the need forprovisions to protect the privacy of subjects and the confidentiality of data is emphasized. confidentiality is also explicitly mentioned in the federal regulations at46.116(a)(7) as an element of informed consentñthat is, the need for informedconsent to address the extent to which the confidentiality of records identifyingresearch participants will be maintained.13.privacy and confidentiality are distinct from anonymity, which generally refers toresearchers retaining no record of the identity of research participants, either because unique identifiers are unknown to the researcher or they are not included aspart of the data. for an accessible discussion of the distinction between privacy,confidentiality, and anonymity, see sieber (1992:4445). some researchers and secondary analysts use the term òanonymizationó to refer to the removal or alterationof identifiable informationñalthough deidentification tends to be the preferred termto refer to eliminating or masking data to reduce the likelihood of potential disclosure (see national research council, 1993).14.gutmann et al. (2005:2) made this useful distinction between the identity of subjectsand information about them in the context of providing spatial data for secondaryanalysis. for a general discussion of identity disclosure and attribute disclosure, seenational research council (2003:2324, 143144).putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.150appendix b15.ethical considerations in biomedical and behavioral research evolved first in thecontext of experimental research, including in clinical medicine, which put greateremphasis on subject recruitment, consent to participate, and benefits or risks ofharm due to participation than on other phases of researchñincluding data preservation, dissemination, access, or subsequent use. the national research councilreports (1985, 1993, 2000, 2005) on data sharing and on access to research datañin particular public data and administrative filesñare an exception to the dominantattention to the data collection stage.16.excellent suggestions are outlined in gutmann et al. (2005).17.irbs at some institutions want to review research on extant data resources thatinclude identifiable information even if the data are made available by thirdpartyproviders who have protocols and procedures in place for approving use. if additional data are to be linked by the secondary analyst, then irb review is requiredbecause the additional data integration (whether or not there is new primary datacollection) changes the conditions of research and potentially raises new ethicalconsiderations in relation to research participants that need to be addressed.18.for a recent description of ways in which data enclaves and other forms of limitedaccess data sharing can be employed to permit qualified secondary users to analyzedata with strict safeguards against disclosure of confidential information, seerodgers and nolte (2006).19.the scope of this paper is directed to social and spatial research directed to producing and adding to generalizable knowledge. the definition of what constitutes research covered by the code of federal regulations for the protection of humansubjects is set forth in 45 cfr 46.102(d), òresearch means a systematic investigation, including research development, testing and evaluation, designed to develop orcontribute to generalizable knowledge. . . .ó20.according to 45 cfr 46.102(f), òprivate information must be individually identifiable (i.e., the identity of the subject is or may readily be ascertained by the investigator or associated with the information) in order for obtaining the information toconstitute research involving human subject.ó21.increasingly irbs at institutions are not doing additional review of protocols forresearch on public use files. for an excellent example, see the website of the university of wisconsin, madison, irb at http://www.grad.wisc.edu/research/compliance/humansubjects/7.existingdata.htm. more generally, see the recommendation of thenational human research protections advisory committee on public use data filesat http://www.hhs.gov/ohrp/nhrpac/documents/dataltr.pdf). two nrc reports (national research council, 2003, recommendations 5.2 and 5.3; 2005, recommendation 6) urge the exemption of secondary analysis of public use files from additional irb review based on certification of confidentiality protection from a dataprovider, including federal statistical agencies. the federal regulations at 45 cfr46.101(b)(4) define as exempt òresearch involving the collection or study of existingdata, documents, records . . . , if these sources are publicly available or if theinformation is recorded by the investigator in such a manner that subjects cannot beidentified, directly or through identifiers linked to the subjects.ó22.empirical research on the complexity of undertaking research in traumatic circumstances or on traumatized populations is reviewed in newman and kaloupek (2004)and newman, risch, and kassamadams (2006); see also, griffin, resick, waldrop,and mechanic (2003).23.with certain topics of research or subject populations, researchers need to takespecial care to conceive of the research cognizant of the perceptions of humansubjects about the study and the research procedures being used. there are manyputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.ethical issues related to linked socialspatial data151good examples related to ethic and cultural populations and communities, includingimmigrant and refugee groups, in trimble and fisher (2006).24.since spatial data with precise coordinates by definition locate persons and theircommunities, community consultation about the consent process and informing communities about the research and its purposes may help to work out agreements (see,e.g., melton et al., 1988; marshall and rotimi, 2001).25.because precise spatial data are the equivalent of personal identifiers or close proxies for them, social and spatial research that includes such measures would typicallyrequire research participant consent. unlike the completion and return of a survey,for example, that is completed online or received in the mail, for which executingthe task can be presumed to be consent, collecting coordinate data at a personõshome, workplace, or health clinic and recording or linking it to survey or social datawould ethically require the knowledge and agreement of the persons potentiallyunder study.26.the criteria for waivers of informed consent are set forth in 45 cfr 46.116(d).27.an irb is likely to expect researchers to address what information will be conveyedto research participants about spatially explicit data and how they would be combined with other information collected in the study. an irb is most likely to expectdiscussion of this linkage and any risk of disclosure when locational data are beingobtained as part of a primary data collection, along with survey or other social data.the actual wording of such an informed consent process and how it is understoodby potential subjects would, in accordance with ethical principles, be specified bythe researcher, with explanation to the irb as to why the information and theassurances are being presented in that format, the data protection plan to be put inplace, and the level of risk of harm. survey researchers know that some wordings ofwarnings raise undue alarm, erode willingness to participate in research, can skewthe research sample, or may be misunderstood or not even be recognized, as whenresearch participants sign a consent form without reading it.28.there is some evidence that people want their data shared if it is likely to benefitsociety and if risk to the research participant is minimal (see, e.g., willison, 2003).29.the òidealized typeó of human subject is a person of value in terms of communitynorms of decency and trustworthiness. like other areas of inquiry, social and spatialresearch may focus on undesirable or unsavory persons (for example, a study ofdiffusion of fraudulent medical practices among physicians). the ethical obligationto be respectful of research participants and not to increase their vulnerability is partof the consent agreement. there are limitations to agreements relating in someinstances to a duty to report (e.g., learning about identifiable child abuse) that needto be made clear to human subjects as part of gaining their informed consent (seethe discussion of research populations in levine and skedsvold, 2007).30.òriskó and òharmó are terms that are often conflated (see the risk and harmreport of the social and behavioral sciences working group on human researchprotections at http://www.aera.net/aera.old/humansubjects/riskharm.pdf). òharmórefers to potential adverse consequences and òriskó refers to the likelihood of theiroccurrences. there are standards for minimal risk implied in codes of ethics andenunciated explicated in 45 cfr 46.102(i) that set forth that the òprobability andmagnitude of harm or discomfort anticipated in the research are not greater in andof themselves than those ordinarily encountered in daily life or during the performance of routine physical or psychological examinations or tests.ó while this definition offers rules of thumb, in no area does it provide the empirical clarity that wouldbe useful (see also wendler et al., 2005).putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.152appendix b31.for useful general recommendations on confidentiality and research data protections, see the national human research protections advisory committee (2002).32.certificates of confidentiality are issued by designated federal agencies to protect theprivacy of research subjects by protecting investigators and institutions from beingcompelled to release information that could be used to identify subjects with aresearch project. they allow the investigator and others who have access to researchrecords to refuse to disclose identifying information in any civil, criminal, administrative, legislative, or other proceeding, whether at the federal, state, or local level(see, e.g., the national institutes of health web site at http://grants1.nih.gov/grants/policy/coc/background.htm). for a compilation of federal research confidentialitystatutes and codes prepared by the social and behavioral sciences working groupfor the national human research protections advisory committee, see http://www.aera.net/aera.old/humansubjects/nhrpacfinalconftable.pdf.33.dualuse research is of major concern in the biological sciences. as defined in thenational security advisory board for biosecurity charter, dual use refers to òbiological research with legitimate scientific purpose that may be misused to pose abiologic threat to public health and/or national securityó (shea, 2006:. crs2).34.certificates of confidentiality vary in their reach and protection, and the need tostrengthen or align them across federal agencies is generally recognized (see national human research protections advisory committee, 2002).35.fienberg (2004) makes the point that protecting confidentiality is not synonymouswith ethical behavior.36.see section 7 on ethics in reporting in american educational research association,(2006).37.ethical decision making can require consulting with expert peers to ensure that stepsare taken in publications or presentation that do not compromise research participants but do so with a presumption that openness in research dissemination isoptimal for transparent and wellwarranted reporting. other areas of science alsoface the challenge of how to maximize openness in research reporting while remaining sensitive to potential risks of harm. some of the current discussion in the lifesciences about the reporting of results consonant with concerns about security issuesis a new domain deeply engaged in trying to understand how best to balance bothethical considerations (see, e.g., vest, 2003; somerville and atlas, 2005).38.for one of the earliest and most profound statements of the norms guiding science(originally published in 1942), see merton (1973).39.there was some early attention in the 1970s to issues of access to government dataand the conditions for dissemination of microdata sets (including attention to linkages to survey data) in a report of the american statistical association (1977). seealso the bellagio principles, which were developed in 1977 at a conference of academic and government representatives from five countries (canada, the unitedstates, the federal republic of germany, sweden, and the united kingdom) convened to consider privacy, confidentiality, and the use of government microdata forresearch and statistical purposes. the principles call for expanded access to theresearch and statistical community and also addressed issues of data linkage consonant with confidentiality protections (see flaherty, 1978).40.also for an overview of the emergence of data sharing as a practice integral to theopenness of science, see sieber (1991). in recent years, the biological sciences havealso been grappling with the principles underlying the sharing of data and softwareas well as materials related to publication. based on discussion at a workshop, thenational research council committee on responsibilities of authorship in the biological sciences articulated recommendations for sharing publicationrelated prodputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.ethical issues related to linked socialspatial data153ucts that are familiar in tone and substance to those specified in the social andbehavioral sciences (see national research council, 2003b).41.the national science foundation first specified a data sharing requirement agencywide in april 1989. the current statement of nsf policy on dissemination andsharing of research results (section 734) is in the grant policy manual at http://www.nsf.gov/pubs/manuals/gpm05131/gpm05131.pdf.42.nih issued data sharing policy and implementation guidelines for grants of$500,000 or more annually in direct costs, which is available: http://grants.nih.gov/grants/policy/datasharing/datasharingguidance.htm.43.see, e.g., frequently asked questions at http://grants1.nih.gov/grants/policy/datasharing/datasharingfaqs.htm; data sharing workbook at http://grants1.nih.gov/grants/policy/datasharing/datasharingworkbook.pdf; data sharing regulations/policy/guidance chart for nih awards at http://grants1.nih.gov/grants/policy/datasharing/datasharingchart%20.doc; data sharing brochure at http://grants1.nih.gov/grants/policy/datasharing/datasharingbrochure.pdf.44.helpful guidance is provided in duncan (2003); see also oõrourke et al. (2006). inaddition, expanding access to research data: reconciling risks and opportunities(national research council, 2005) specifically addresses a range of approaches toallowing greater access to federally collected data while strengthening confidentiality protections. the nih documents also provide useful elaboration on considerations that can guide the development of data access and data sharing plans.45.see, e.g., the icpsr responsible use statement at http://www.icpsr.umich.edu/org/policies/respuse.html.46.practices are changing as federal funding agencies like nih are more explicit aboutdata sharing and the need to address data sharing or future use as part of theprocess of obtaining informed consent. see the national institutes of health datasharing policy and implementation guidelines at http://grants.nih.gov/grants/policy/datasharing/datasharingguidance.htm.47.the national longitudinal study of adolescent health (add health) is a goodexample of a major nationally representative longitudinal study that provides potential users with straightforward information on available publicuse data sets andrestricteduse data sets, with spatial analysis data being available through restricteduse. access to restricted use data requires an irbapproved security plan and agreement to a datause contract (requirements for access to restricteduse contractualdata are described at http://www.cpc.unc.edu/projects/addhealth/data/restricteduse.)educative guidance of steps to avert deductive disclosure is provided on the addhealth website at http://www.cpc.unc.edu/projects/addhealth/data/dedisclosure. theproject on human development in chicago neighborhoods, also a major longitudinal, multimethod study, has publicuse files and restricted data available throughthe interuniversity consortium for political and social research. precise locationaldata are considered sensitive information and obtainable through icpsrõs restricteduse agreement or secure data enclave (see http://www.icpsr.umich.edu/phdcn/about.html).48.exceptions include national research council (2003, 2005), which could beadopted in course and class. also, for useful background texts, see sieber (1992)and fisher (2003).49.the center for spatially integrated social science (csiss) at the university of california, santa barbara undertakes a valuable range of activities to foster capacitybuilding in researchers, including workshops, extensive bibliographic references, coursesyllabi, information on best practices, and so forth (see http://www.csiss.org/). thesyllabi included on the website for courses taught on spatial analysis at differentputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.154appendix binstitutions make no mention of ethical considerations. the csiss also produced avery informative best practices volume in 2004 (goodchild and janelle, 2004). thisbook is directed to the potential value of thinking spatially and sets forth examplesof spatial analysis, but there was no attention to ethical considerations for potentialresearchers or data analysts.50.the university of chicago social and behavioral sciences irb emphasizes educationand provides useful educational resources (see http://humansubjects.uchicago.edu/sbsirb/education.html). also, its irb & investigator manual (see http://humansubjects.uchicago.edu/sbsirb/manual/sbsirbmanual.pdf) is a very helpful documentfor both those preparing research and reviewing protocols.51.effective october 2000, nih requires education on the protection of human researchparticipants for all investigators submitting applications for research involving humansubjects under contracts or awards. see required education in the protection of human research participants at http://grants.nih.gov/grants/guide/noticefiles/notod00039.html; also see frequently asked questions for the requirement for educationon the protection of human subjects at http://grants.nih.gov/grants/policy/hseducfaq.htm. although a good deal of information is offered on the website, the numberand range of opportunities for training are quite limited, in particular for researchgrounded in the social and behavioral sciences.52.see public health service policies on research misconduct, 42cfr parts 50 and 93,at http://ori.dhhs.gov/documents/42cfrparts50and932005.pdf.53.training in the responsible conduct of research was an element of national research service award (nrsa) institutional research training grants (t32) prior to2005, but attention to research conduct as part of institutional assurances heightened attention to this component: òevery predoctoral and postdoctoral nrsatrainee supported by an institutional research training grant must receive instructionin the responsible conduct of research. (for more information on this provision, seethe nih guide for grants and contracts, volume 21, number 43, november 27,1992, available: http://grants.nih.gov/grants/guide/noticefiles/not92236.html.) applications must include a description of a program to provide formal or informalinstruction in scientific integrity or the responsible conduct of research. . . .ó54.the mission of the office of research integrity is to monitor institutionsõ investigations of research misconduct and promote the responsible conduct of researchthrough education, prevention, and regulatory activities (see http://ori.dhhs.gov/).55.this portion of the website is operated by the committee on privacy and confidentiality of the american statistical association; see http://www.amstat.org/comm/cmtepc/index.cfm?fuseaction=main.56.for further information on the working group and its educational activities, seehttp://www.aera.net/default.aspx?id=669.57.organizations that serve as archives for data resources and stewards providing access for their use offer materials that serve to educate and inform researchers andsecondary analysts about the ethical as well as technical issues involved in sharingand gaining access to data (see, e.g., icpsr responsible use statement at http://www.icpsr.umich.edu/org/policies/respuse.html). also, the henry a. murray research archive of the harvardmit data center is the repository for qualitative andquantitative research data at the institute for quantitative social science. it hasmaterials on data archiving that offer brief guidance, from data collection throughtransfer to an archive, and on steps to facilitate data sharing (see http://murray.harvard.edu/mra/service.jsp?id=55&bct=ddata%252bpreservation.p5.s55) or application for data use (see http://www.murray.harvard.edu/mra/showcontent.jsp?key=dataapplicationform). the guidance sets forth conditions for use of various forms of data, including video and audio recordings.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.ethical issues related to linked socialspatial data15558.one useful example of an accessible educative document is u.s. general accountingoffice (2001).59.a similar approach was discussed by f.j. levine regarding natural and humanitarian disasters and strategies for ongoing flexible review processes (see national research council, 2002).referencesamerican anthropological association1998code of ethics. arlington, va: american anthropological association.american association for public opinion research2003protection of human participants in survey research: a source document forinstitutional review boards. available:http://www.aapor.org/default.asp?page=newsandissues/aaporstatementforirb. [accessed november 25, 2005].2005code of professional ethics and practices. available: http://www.aapor.org/pdfs/aaporcode2005.pdf [accessed november 25, 2005].american educational research association2006standards for reporting on empirical social science research in aera publications. educational researcher 35(6):3340.american political science association1998a guide to professional ethics in political science. washington, dc: americanpolitical science association.american psychological association2003ethical principles of psychologists and code of conduct. american psychologist57:10601073.american sociological association1997code of ethics. washington, dc: american sociological association.american statistical association1977report of ad hoc committee on privacy and confidentiality. the american statistician 31(2):5978.1999 ethical guidelines for statistical practice. alexandria, va: author.armstrong, m.p., g. rushton, and d.l. zimmerman1999geographic masking health data to preserve confidentiality. statistics in medicine 18:497525.association of american geographers1998statement on professional ethics. available: http://www.aag.org/publications/other%20pubs/ethicsstatement.html [accessed november 25, 2005].collogan, l.k., f. tuma, r. dolansewell, s. borja, and a.r. fleischman2004ethical issues pertaining to research in the aftermath of disaster. journal oftraumatic stress 17:363372.collogan, l.k., f.k. tuma, and a.r. fleischman2004research with victims of disaster: institutional review board considerations. irb:ethics & human research 26(julyaugust):911.de wolf, v.a.2003issues in accessing and sharing confidential survey and social science data. datascience journal 2:6674.de wolf, v.a., j.e. sieber, p.m. steel, and a.o. zarate2006part ii: hipaa and disclosure risk. irb: ethics & human research,28(januaryfebruary), 611.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.156appendix bduncan, g.t.1993special issue on confidentiality and data access. journal of official statistics93:269607.2003confidentiality and data access issues for institutional review boards. pp. 235247 in national research council, protecting participants and facilitating social and behavioral sciences research. c.f. citro, d.r. ilgen, and c. b. marrett,eds. panel on institutional review boards, surveys, and social science research.washington, dc: the national academies press.fienberg, s.e.2004confidentiality in geospatiallylinked data: how robust are the solutions?presentation at the workshop on confidentiality issues in linking geographically explicit and selfidentifying data, the national academies, washington,dc, december 910.fisher, c.b.2003decoding the ethics code: a practical guide for psychologists. thousand oaks,ca: sage publications.flaherty, d.h.1978report of the bellagio conference. journal of the royal statistical society. seriesa (general) 141:401405.golden, m.l., r.r. downs, and k. davispackard2005confidentiality issues and policies related to the utilization and disseminationof geospatial data for public health applications. a report to the public healthapplications of earth science program, national aeronautics and space administration, science mission directorate, applied sciences program. prepared bythe socioeconomic data and applications center, center for international earthscience information network, columbia university, march 2005. available:http://www.ciesin.columbia.edu/pdf/sedacconfidentialityreport.pdf [accessednovember 25, 2005].goodchild, m.f., and d.g. janelle, eds.2004spatially integrated social science. new york: oxford university press.griffin, m.g., p.a. resick, a.e. waldrop, and m.b. mechanic2003participation in trauma research: is there evidence of harm? journal of traumatic stress 16:221227.gutmann, m., k. witkowski, c. colyer, j.m. oõrourke, and j. mcnally2005providing spatial data for secondary analysis: issues and current practices relating to confidentiality. unpublished manuscript, interuniversity consortiumfor political and social research, university of michigan (available from myrongutmann).institute of medicine2007ethical considerations for research involving prisoners. committee on ethicalconsiderations for revisions to dhhs regulations for protection of prisonersinvolved in research, l.o. gostin, c. vanchieri, and a. pope eds. washington,dc: the national academies press.iutcovich, j.m., j.m. kennedy, and f.j. levine2003establishing an ethical climate in support of research integrity: efforts and activities of the american sociological association. science and engineering ethics9:201205.kwan, m.p., i. casas, and b.c. schmitz2004protection of geoprivacy and accuracy of spatial information: how effective aregeographical masks? cartographica 39:1528.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.ethical issues related to linked socialspatial data157kwan, m.r., and j. lee2004geovisualization of human activity patterns using 3d gis. pp. 1866 in m.f.goodchild and d.g. janelle, eds., spatially integrated social science. new york:oxford university press.lane, j.2003key issues in confidentiality research: results of an nsf workshop. available:http://www.nsf.gov/sbe/ses/mms/nsfworkshopsummary1.pdf [accessed november 25, 2005].levine, f.j., and j.m. iutcovich2003challenges in studying the effects of scientific societies on research integrity.science and engineering ethics 9:257268.levine, f.j., and p.r. skedsvold2007behavioral and social science research. in e.j. emmanuel, r.a. crouch, c. grady,r. lie, f. miller, and d. wendler (eds.) the oxford textbook of clinical research ethics. oxford, england: oxford university press.marshall, p.a., and c. rotimi2001ethical challenges in communitybased research. american journal of the medical sciences 322(5):241245.melton, g.b., r.j. levine, g.p., kocher, r rosenthal, and w.c. thompson1988community consultation in socially sensitive research: lessons from clinical trials on treatments for aids.american psychologist 43:573581.merton, r.k.1973the normative structure of science. in r.k. merton, ed., the sociology of science: theoretical and empirical investigations. chicago, il: university of chicago press.national bioethics advisory commission2001ethical and policy issues in research involving human participants: vols i, ii.bethesda, md: national bioethics advisory commission.national commission for the protection of human subjects of biomedical and behavioralresearch1979belmont report: ethical principles and guidelines for the protection of humansubjects of research. (gpo no. 887809). washington, dc: u.s. governmentprinting office. also available: http://ohsr.od.nih.gov/guidelines/belmont.html[accessed november 13, 2005].national human research protections advisory committee2002recommendations on confidentiality and research data protections. available:http://www.aera.net/aera.old/humansubjects/nhrpacfinalconfidentiality.pdf.national research council1985sharing research data. committee on national statistics, s.e. fienberg, m.m.martin, and m.l. straf, eds. washington, dc: national academy press.1993private lives and public policies: confidentiality and accessibility of government statistics. committee on national statistics, g.t. duncan, t.b. jabine, andv.a. de wolf, eds. washington, dc: national academy press.1998people and pixels: linking remote sensing and social science. committee onthe human dimensions of global change, d. liverman, e.f. moran, r.r.rindfuss, and p.c. stern, eds. washington, dc: national academy press.2000improving access to and confidentiality of research data: report of a workshop. committee on national statistics, c. mackie and n. bradburn, eds. washington, dc: national academy press.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.158appendix b2002research ethics in complex humanitarian emergencies: summary of a workshop. h, reed, rapporteur, roundtable on the demography of forced migration, committee on population. division of behavioral and social sciences andeducation. washington, dc: the national academies press.2003aprotecting participants and facilitating social and behavioral sciences research.panel on institutional review boards, surveys, and social science research, c.f.citro, d.r. ilgen, and c.b. marrett, eds.. washington, dc: the national academies press.2003bsharing publicationrelated data and materials. committee on responsibilitiesof authorship in the life sciences. washington, dc: the national academiespress.2005 expanding access to research data; reconciling risks and opportunities. panelon data access for research purposes, committee on national statistics. washington, dc: the national academies press.national science foundation, division of social and economic science1986data archiving policy and implementation guidance. materials available fromf.j. levine at flevine@aera.net.newman, e., and d.g. kaloupek2004the risks and benefits of participating in traumafocused research studies. journal of traumatic stress 17:383394.newman, e., e. risch, and n. kassamadams2006ethical issues in traumarelated research: a review. journal of empirical research on human research ethics 1(3):2946.oõrourke, j., s. roehrig, s. heeringa, b. reed, w. birdsall, m. overcashier, m., and k.zidar2006solving problems of disclosure risk while retaining key analytic uses of publiclyreleased microdata. journal of empirical research on human research ethics1(3):6384.rindfuss, r.r., and p.c. stern1998linking remote sensing and social science: the need and the challenges. pp. 127in national research council, people and pixels: linking remote sensing andsocial science. committee on the human dimensions of global change, d.liverman, e.f. moran, r.r. rindfuss and p.c. stern, eds. washington, dc:national academy press.rodgers, w., and m. nolte2006disclosure review procedures in an academic setting: example of the health andretirement study. journal of empirical research on human research ethics1(3):8598.shea, d.a.2006oversight of dualuse biological research: the national science advisoryboard for biosecurity. (crs report no. rl33342). washington, dc: congressional research service.sieber, j.e.1991introduction: sharing social science data. in j.e. sieber (ed.), sharing socialscience data: advantages and challenges. newbury park, ca: sage publications.1992planning ethically responsible research: a guide for students and internalreview boards. newbury park, ca: sage publications.somerville, m.a., and r.m. atlas2005ethics: a weapon to counter bioterrorism. science 307:18811882.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.ethical issues related to linked socialspatial data159trimble, j.e., and c.b. fisher2006the handbook of ethical research with ethnocultural populations and communities, thousand oaks, ca: sage publications.urban and regional information systems association2003a gis code of ethics. available: http://www.urisa.org/ethics/codeofethics.htm[accessed august 9, 2005].u.s. general accounting office2001record linkage and privacy: issues in creating new federal research and statistical information. (gao01126sp). washington, dc: u.s. general accounting office.vanwey, l.k., r.r. rindfuss, m.p. gutmann, b.e. entwisle, and d.l. balk2005confidentiality and spatially explicit data: concerns and challenges. proceedingsof the national academy of sciences 102:1533715342.vest, c.m.2003balancing security and openness in research and education. academe. available:http://www.aaup.org/publications/academe/2003/03so/03sowest.htm [accessedjanuary 7, 2006].wendler, d., l. belsky, k.m. thompson, and e.j. emanuel2005quantifying the federal minimal risk standard: implications for pediatric research without a prospect of direct benefit. journal of the american medicalassociation 294:826832.willis, g.2006cognitive interviewing as a tool for improving the informed consent process.journal of empirical research on human research ethics 1(1):923.willison, d.j.2003privacy and the secondary use of data for health research. experience in canada,and suggested directions forward. journal of health services research and policy8(suppl 1):1723.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.160biographical sketches forpanel members and staffmyron p. gutmann (chair) is a professor of history and director of theinteruniversity consortium for political and social research (icpsr) atthe university of michigan. prior to joining the michigan faculty in augustof 2001, he was a professor of history and geography and director of thepopulation research center at the university of texas at austin. his research covers interdisciplinary historical topics, especially in health, population, economy, and the environment, and he has used a variety of approaches to study populationland use interactions. he also does researchand writes on issues relating to data preservation and dissemination andabout confidentiality protection in data used for secondary analysis. he isthe author of war and rural life in the early modern low countries, andtoward the modern economy, early industry in europe, 15001800, aswell as more than 50 articles and chapters. he has been a member of thenational academiesõ committee on the human dimensions of globalchange and its panel on new research in population and environment, aswell as other national advisory committees and editorial boards. gutmannreceived a b.a. from columbia university, an m.a. from princeton university, and a ph.d. in history from princeton university.marc p. armstrong is a professor and chair of the department of geography at the university of iowa, where he also holds an appointment in thegraduate program in applied mathematical and computational sciences.a primary focus of his research is on the use of parallel processing toimprove the performance of analysis methods used in spatial decision supputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.biographical sketches for panel members and staff161port systems. other research interests are in mobile computing, privacyaspects of geospatial technologies, and evolutionary computation. he hasserved as north american editor of the international journal of geographical information science and on the editorial boards of many other journals.he has published more than 100 papers in a wide variety of academicjournals, including the annals of the association of american geographers, statistics and medicine, mathematical geology, and journal of theamerican society for information science. he received a ph.d. from theuniversity of illinois at urbanachampaign.deborah balk is associate professor at the baruch school of public affairsand acting associate director of the institute for demographic research atthe city university of new york. previously, she was a research scientist atthe center for international earth science information network at columbia university, where she was also the lead project scientist for the socioeconomic data and applications center, working on largescale data integration of geographic, survey, and administrative data. among her currentprojects, she is the principal investigator on two studies of urbanization andone on emerging infectious disease. she is a member of the working groupon urbanisation of the international union for the scientific study of population. she received a b.a. and m.a. from the university of michiganannarbor and a ph.d. in demography from the university of california atberkeley.kathleen (kass) oõneill green recently retired from her position as presidentof space imaging solutions, a division of space imaging, llc. while withspace imaging, she directed programs that offered satellite imagery, remotesensing, and gis (global information services) services to clients worldwide.she currently serves as an independent consultant and board member topublic, private, and nonprofit natural resource and geospatial organizations.her background includes 30 years of experience in natural resource policy,economics, gis analysis, and remote sensing. she is the author of numerousarticles on gis and remote sensing and coauthored a book on the practicalaspects of accuracy assessment. she is the recent past president of management association for private photogrammetric surveyors (mapps), an organization of private mapping firms dedicated to advancing the mapping industry. she received a b.s. in forestry and resource management from theuniversity of california at berkeley and an m.s. in resource policy andmanagement from the university of michiganann arbor.felice j. levine is executive director of the american educational researchassociation (aera). previously, she served as executive officer of theamerican sociological association, as a program director at the nationalputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.162putting people on the mapscience foundation, and as senior research social scientist at the americanbar foundation. her work has concentrated on science policy issues, including research ethics, data access and sharing, and peer review; academicand scientific professions; and diversity in higher education. she was amember of the national human research protections advisory committeeof the u.s. department of health and human services and on the advisorycommittee for the decennial census. she is currently on the executive committee of the consortium of social science associations and served as itschair from 1997 to 2000 and is on the board of directors of the council ofprofessional associations on federal statistics. she is a fellow of the american psychological society and of the american association for the advancement of science. she holds a.b., a.m. and ph.d. degrees in sociologyand psychology from the university of chicago.harlan onsrud is a professor in the department of spatial informationscience and engineering at the university of maine. his research focuses onthe analysis of legal, ethical, and institutional issues that affect the creationand use of digital databases and the assessment of the social effects ofspatial technologies. he is president elect of the global spatial data infrastructure association (gsdi), past president of the university consortiumfor geographic information science (ucgis), and past chair of the u.s.national committee (usnc) on data for science and technology(codata) of the national research council. he is a licensed engineer,land surveyor, and attorney. current and past research projects have beenfunded by the national science foundation, the national geospatialintelligence agency, the federal geographic data committee, and the u.s.department of education. he holds b.s. and m.s. degrees in engineeringand a j.d., all from the university of wisconsin.jerome p. reiter is an assistant professor at the institute of statistics anddecision sciences at duke university. his primary research areas includestatistical methods for preserving data confidentiality, handling missingdata, and making casual inference. he works extensively with the u.s.census bureau and the national institute of statistical science on researchin statistical disclosure limitation. he is a member of the committee onprivacy and confidentiality of the american statistical association. heserves on the editorial board of the journal of the american statisticalassociation, the journal of privacy and confidentiality, and survey methodology. he received a b.s. in mathematics from duke university and aph.d. in statistics from harvard university.ronald rindfuss is the robert paul ziff distinguished professor of sociology and a fellow at the carolina population center (cpc) at the universityputting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.biographical sketches for panel members and staff163of north carolinachapel hill and a senior fellow at the eastwest centerin honolulu. he previously served as director of the cpc. as a socialdemographer, his work focuses on the timing and sequencing of cohabitation, marriage, childbearing, divorce, education, migration, and employment. he is currently working on the relationship between populationprocesses and the environment, examining migration and social change inthailand examining the consequences of child care patterns in norway andexamining changes in family processes in japan. he is a past president ofthe population association of america and a fellow of the american association for the advancement of science. he holds a b.a. from fordhamuniversity and a ph.d. from princeton university, both in sociology.paul c. stern (study director) is a senior staff officer at the nationalresearch council and study director of the committee on the humandimensions of global change. his research interests include the determinants of environmentally significant behavior, particularly at the individuallevel; participatory processes for informing environmental decision making;and the governance of environmental resources and risks. he is the coeditorof numerous national research council publications, including population, land use, and environment: research directions (2005), the dramaof the commons (2002), and people and pixels: linking remote sensingand social science (1998). he is a fellow of the american association forthe advancement of science and the american psychological association.he holds a b.a. from amherst college and an m.a. and ph.d. from clarkuniversity, all in psychology.putting people on the map: protecting confidentiality with linked socialspatial datacopyright national academy of sciences. all rights reserved.
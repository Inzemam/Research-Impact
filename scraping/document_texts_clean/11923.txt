detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/11923software for dependable systems: sufficient evidence?148 pages | 6 x 9 | hardbackisbn 9780309384506 | doi 10.17226/11923daniel jackson, martyn thomas, and lynette i. millett, editors; committee oncertifiably dependable software systems; computer science andtelecommunications board; division on engineering and physical sciences;national research councilsoftware for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.daniel jackson, martyn thomas, and lynette i. millett, editorscommittee on certifiably dependable software systemscomputer science and telecommunications boarddivision on engineering and physical sciencessufficient evidence?software for dependable systemssoftware for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.the national academies press 500 fifth street, n.w. washington, dc 20001notice: the project that is the subject of this report was approved by the governing board of the national research council, whose members are drawn from the councils of the national academy of sciences, the national academy of engineering, and the institute of medicine. the members of the committee responsible for the report were chosen for their special competences and with regard for appropriate balance.support for this project was provided by the national science foundation and the national security agency under sponsor award number ccr0236725; the ofce of naval research under sponsor award number n000140310915; and the national science foundation, the national security agency, and the federal aviation administration under sponsor award number cns0342801. any opinions, ndings, or recommendations expressed in this publication are those of the authors and do not necessarily re˚ect the views of the agencies and organizations that provided support for the project.international standard book number13: 9780309103947international standard book number10: 0309103940 additional copies of this report are available fromthe national academies press500 fifth street, n.w., lockbox 285washington, dc 20055(800) 6246242(202) 3343313 (in the washington metropolitan area)http://www.nap.educopyright 2007 by the national academy of sciences. all rights reserved.printed in the united states of americasoftware for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.the national academy of sciences is a private, nonprot, selfperpetuating society of distinguished scholars engaged in scientic and engineering research, dedicated to the furtherance of science and technology and to their use for the general welfare. upon the authority of the charter granted to it by the congress in 1863, the academy has a mandate that requires it to advise the federal government on scientic and technical matters. dr. ralph j. cicerone is president of the national academy of sciences.the national academy of engineering was established in 1964, under the charter of the national academy of sciences, as a parallel organization of outstanding engineers. it is autonomous in its administration and in the selection of its members, sharing with the national academy of sciences the responsibility for advising the federal government. the national academy of engineering also sponsors engineering programs aimed at meeting national needs, encourages education and research, and recognizes the superior achievements of engineers. dr. charles m. vest is president of the national academy of engineering.the institute of medicine was established in 1970 by the national academy of sciences to secure the services of eminent members of appropriate professions in the examination of policy matters pertaining to the health of the public. the institute acts under the responsibility given to the national academy of sciences by its congressional charter to be an adviser to the federal government and, upon its own initiative, to identify issues of medical care, research, and education. dr. harvey v. fineberg is president of the institute of medicine.the national research council was organized by the national academy of sciences in 1916 to associate the broad community of science and technology with the academy™s purposes of furthering knowledge and advising the federal government. functioning in accordance with general policies determined by the academy, the council has become the principal operating agency of both the national academy of sciences and the national academy of engineering in providing services to the government, the public, and the scientic and engineering communities. the council is administered jointly by both academies and the institute of medicine. dr. ralph j. cicerone and dr. charles m. vest are chair and vice chair, respectively, of the national research council.www.nationalacademies.orgsoftware for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.vcommittee on certifiably dependable software systemsdaniel jackson, massachusetts institute of technology, chairjoshua bloch, google inc.michael dewalt, certication systems, inc.reed gardner, university of utah school of medicinepeter lee, carnegie mellon universitysteven b. lipner, microsoft trustworthy computing groupcharles perrow, yale universityjon pincus, microsoft researchjohn rushby, sri internationallui sha, university of illinois at urbanachampaignmartyn thomas, martyn thomas associatesscott wallsten, progress and freedom foundationdavid woods, ohio state universitystafflynette i. millett, study director and senior program ofcerdavid padgham, associate program ofcergloria westbrook, senior program assistant (through november 2006)phil hilliard, research associate (through may 2004)penelope smith, senior program assistant (february 2004july 2004)software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.vicomputer science and telecommunications boardjoseph f. traub, columbia university, chaireric benhamou, benhamou global ventures, llcfrederick r. chang, university of texas, austinwilliam dally, stanford universitymark e. dean, ibm almaden research centerdeborah estrin, university of california, los angelesjoan feigenbaum, yale universitykevin kahn, intel corporationjames kajiya, microsoft corporationmichael katz, university of california, berkeleyrandy h. katz, university of california, berkeleysara kiesler, carnegie mellon universityteresa h. meng, stanford universityprabhakar raghavan, yahoo! researchfred b. schneider, cornell universityalfred z. spector, independent consultant, pelham, new yorkwilliam stead, vanderbilt universityandrew j. viterbi, viterbi group, llcpeter weinberger, google inc.jeannette m. wing, carnegie mellon universitystaffjon eisenberg, directorkristen batch, associate program ofcerradhika chari, administrative coordinatorrenee hawkins, financial associatemargaret marsh huynh, senior program assistantherbert s. lin, senior scientistlynette i. millett, senior program ofcerdavid padgham, associate program ofcerjanice m. sabuda, senior program assistantted schmitt, consultantbrandye williams, program assistantjoan d. winston, program ofcerfor more information on cstb, see its web site at <http://www.cstb.org>, write to cstb, national research council, 500 fifth street, n.w., washington, dc 20001, call (202) 3342605, or email cstb at cstb@nas.edu.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.vii prefacecritical systems are often subject to certication: a formal assurance that the system has met relevant technical standards designed to ensure it will not unduly endanger the public and can be depended upon to deliver its intended service safely and securely. today, certication1 of the dependability of a softwarebased system usually relies more on assessments of the process used to develop the system than on the properties of the system itself. while these assessments can be useful, few would dispute that direct observation of the artifact ought to provide a stronger kind of assurance than the credentials of its production method. yet the complexity of software systems, as well as the discontinuous way they behave, renders them extremely difcult to analyze unless great care has been taken with their structure and maintenance.to further understand these and related issues, the high condence software and systems (hcss) coordinating group (cg) of the national science and technology council™s networking and information technology research and development (nitrd) subcommittee initiated discussions with the computer science and telecommunications board (cstb) of the national research council (nrc). these discussions resulted in a study to assess the current state of certication in dependable systems with the goal of recommending areas for improvement. funding 1 the committee uses the term ﬁcerticationﬂ to refer to the process of assuring that a product or process has certain stated properties, which are then recorded in a certicate. certication usually involves assurance by an independent party, although the term is also used analogously for customer (secondparty) and developer (rstparty) assurance.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.viii software for dependable systemsfor the project was obtained from the following hcss cg agencies: the national science foundation, the national security agency, the ofce of naval research, and the federal aviation administration.a committee was formed consisting of 13 experts from industry and academia specializing in diverse aspects of systems dependability including software engineering, software testing and evaluation, software dependability, embedded systems, humancomputer interaction, systems engineering, systems architecture, accident theory, standards setting, avionics, medicine, economics, security, and regulatory policy (see appendix a for committee and staff biographies).to accomplish its mission, the committee divided the study into two phases: a framing phase and an assessment phase. the framing phase culminated in a public workshop in april 2004, attended by members of industry, government, and academia. the workshop was organized as a series of panel discussions on a variety of topics and was summarized by the committee in a subsequent report.2in the assessment phase of the study, the committee held a series of meetings over a 2year period. each meeting comprised a day of open sessions in which the committee heard opinions and evidence from a variety of experts, and 1 or 2 days of closed sessions in which the committee analyzed the information presented to it and worked to develop a view on the state of software dependability and recommendations for the future. the chair of the committee also conducted a handful of telephone interviews with experts to supplement the material covered during the committee™s meetings. this report adopts a broad perspective on the question of how software might be made dependable in a costeffective manner rather than focusing narrowly on the question of software certication per se. by design, this diverse committee represented a range of views on issues, and with this wider perspective, the committee found itself confronting the perennial dilemmas of software engineering, discussing in a current context many of the same issues that have been debated since a seminal 1968 nato conference.3 in discussions and through the process of writing the report, a number of these issues were explored, including the likelihood of catastrophes caused by software; whether formal methods will scale to large systems; and the extent to which a manufacturer™s disclaim2 national research council, 2004, summary of a workshop on software certication and dependability, the national academies press, washington, d.c. available online at <http://books.nap.edu/catalog/11133.html>.3 see p. naur and b. randell, eds., 1969, ﬁsoftware engineering: report on a conference sponsored by the nato science committee,ﬂ garmisch, germany, october 711, 1968, nato scientic affairs division, brussels, belgium. available online at <http://homepages.cs.ncl.ac.uk/brian.randell/nato/>.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.preface ixing of liability should be seen as undermining any dependability claims it makes. although this study does not attempt to resolve these longstanding issues, the committee believes that the recommendations and approach set forth in this report can be used in the short term to improve the dependability of systems and in the longer term to lay the foundation for new and more powerful software development methods.the committee thanks the many individuals who contributed to its work. the people who briefed the committee at the workshop and in subsequent meetings are listed in appendix b; we appreciated their willingness to address the questions we posed to them and are grateful for their insights. the sponsors of the report have been most supportive and responsive in helping the committee to do its work. the reviewers and the review monitor, listed below, provided thoughtful and detailed critiques that in˚uenced the nal form of the report signicantly. my personal thanks to martyn thomas, who from the start took a leading role in helping to crystallize the committee™s thinking and shared much of the burden of writing and editing the report; to lynette millett, our study director, for her constructive guidance throughout, for keeping us focused, and for her expert editing of the report; to associate program ofcer david padgham for his meticulous help with the preparation of the nal version; to our research associate at the start of the study, phil hilliard, for gathering materials and setting up our infrastructure; to my doctoral student, derek wayside, for conguring and maintaining the committee wiki; to liz fikre of the deps editorial staff for her careful and clarifying assistance with manuscript preparation; to review monitor elsa garmire for her thorough and helpful oversight; and to jon eisenberg, director of the cstb, for the special interest he has taken in this study and the attention and sage advice he has given.daniel jackson, chaircommittee on certiably  dependable software systemssoftware for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.xi acknowledgment of reviewersthis report has been reviewed in draft form by individuals chosen for their diverse perspectives and technical expertise, in accordance with procedures approved by the national research council™s (nrc™s) report review committee. the purpose of this independent review is to provide candid and critical comments that will assist the institution in making its published report as sound as possible and to ensure that the report meets institutional standards for objectivity, evidence, and responsiveness to the study charge. the review comments and draft manuscript remain condential to protect the integrity of the deliberative process. we wish to thank the following individuals for their review of this report:ashish arora, carnegie mellon university,david corman, boeing company,steven fenves, carnegie mellon university,r. john hansman, massachusetts institute of technology,butler lampson, microsoft corporation,jesse h. poore, university of tennessee,tariq samad, honeywell automation and control solutions,alfred z. spector, independent consultant, andwilliam stead, vanderbilt university.although the reviewers listed above have provided many constructive comments and suggestions, they were not asked to endorse the consoftware for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.xii software for dependable systemsclusions or recommendations, nor did they see the nal draft of the report before its release. the review of this report was overseen by elsa garmire of dartmouth university. appointed by the nrc, she was responsible for making certain that an independent examination of this report was carried out in accordance with institutional procedures and that all review comments were carefully considered. responsibility for the nal content of this report rests entirely with the authoring committee and the institution.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.xiiicontentssummary 1 1 assessment: software systems and dependability 16 today cost and schedule challenges in software development, 17 disruptions and accidents due to software, 18 hazardous materials, 20 aviation, 21 medical devices and systems, 23 infrastructure, 26 defense, 27 distribution of energy and goods, 28 voting, 29 problems with existing certication schemes, 29 security certication, 30 avionics certication, 33 medical software certication, 35 opportunities for dependable software, 36 air transportation, 37 medicine, 37 observations, 38 observation 1: lack of evidence, 39 observation 2: not just bugs, 40 observation 3: the cost of strong approaches, 41software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.xiv software for dependable systems observation 4: coupling and complexity, 45 observation 5: safety culture matters, 482 proposed approach 51 explicit dependability claims, 54 what is dependability?, 54 why claims must be explicit, 55 software as a system component, 56 accidental systems and criticality creep, 57 evolution and recertication, 59 what to make explicit, 60 requirements, specications, and domain assumptions, 61 evidence, 66 goalbased versus processbased assurance, 66 the dependability case, 68 the role of domain assumptions, 68 the role of architecture, 69 the role of testing, 70 the role of analysis, 74 rigorous process: preserving the chain of evidence, 75 components and reuse, 76 expertise, 78 simplicity, 79 best practices, 83 feasibility of the overall approach, 873 broader issues 89 transparency, 89 accountability and liability, 91 certication, 92 evidence and openness, 93 security concerns, 94 a repository of software dependability data, 96 education, 97 research, 994 findings and recommendations 103 findings, 103 recommendations, 104 to builders and users of software, 104 to agencies and organizations that support software education and research, 107software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.contents xv5 bibliography 110appendixesa biographies of committee members and staff 119b open session briefers 128c statement of task 130software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.1 summaryhow can software and the systems that rely on it be made dependable in a costeffective manner, and how can one obtain assurance that dependability has been achieved? rather than focusing narrowly on the question of software or system certication per se, this report adopts a broader perspective.a system is dependable when it can be depended on to produce the consequences for which it was designed, and no adverse effects, in its intended environment. this means, rst and foremost, that the term dependability has no useful meaning for a given system until these consequences and the intended environment are made explicit by a clear prioritization of the requirements of the system and an articulation of environmental assumptions. the effects of software are felt in the physical, human, and organizational environment in which it operates, so dependability should be understood in that context and cannot be reduced easily to local properties, such as resilience to crashing or conformance to a protocol. humans who interact with the software should be viewed not as external and beyond the boundary of the software engineer™s concerns but as an integral part of the system. failures involving human operators should not automatically be assumed to be the result of errors of usage; rather, the role of design ˚aws should be considered as well as the role of the human operator. as a consequence, a systems engineering approachšwhich views the software as one engineered artifact in a larger system of many components, some engineered and some given, and the pursuit of software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.2 software for dependable systemsdependability as a balancing of costs and benets and a prioritization of risksšis vital. unfortunately, it is difcult to assess the dependability of software. the eld of software engineering suffers from a pervasive lack of evidence about the incidence and severity of software failures; about the dependability of existing software systems; about the efcacy of existing and proposed development methods; about the benets of certication schemes; and so on. there are many anecdotal reports, whichšalthough often useful for indicating areas of concern or highlighting promising avenues of researchšdo little to establish a sound and complete basis for making policy decisions regarding dependability. moreover, there is sometimes an implicit assumption that adhering to particular process strictures guarantees certain levels of dependability. the committee regards claims of extraordinary dependability that are sometimes made on this basis for the most critical of systems as unsubstantiated, and perhaps irresponsible. this difculty regarding the lack of evidence for system dependability leads to two conclusions, re˚ected in the committee™s ndings and recommendations below: (1) that better evidence is needed, so that approaches aimed at improving the dependability of software can be objectively assessed, and (2) that, for now, the pursuit of dependability in software systems should focus on the construction and evaluation of evidence.the committee thus subscribes to the view that software is ﬁguilty until proven innocent,ﬂ and that the burden of proof falls on the developer to convince the certier or regulator that the software is dependable. this approach is not novel and is becoming standard in the world of systems safety, in which an explicit safety case (and not merely adherence to good practice) is usually required. similarly, a software system should be regarded as dependable only if it has a credible dependability case, the elements of which are described below. meeting the burden of proof for dependability will be challenging. the demand for credible evidence will, in practice, make it infeasible to develop highly dependable systems in a costeffective way without some radical changes in priorities. if very high dependability is to be achieved at reasonable cost, the needs of the dependability case will in˚uence many aspects of the development, including the choice of programming language and the software architecture, and simplicity will be key. for high levels of dependability, the evidence provided by testing alone will rarely sufce and will have to be augmented by analysis. the ability to make independence arguments that allow global properties to be inferred from an analysis of a relatively small part of the system will be essential. rigorous processes will be needed to ensure that the chain of evidence for dependability claims is preserved.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.summary 3the committee also recognized the importance of adopting the practices that are already known and used by the best developers; this summary gives a sample of such practices in more detail below. some of these (such as systematic conguration management and automated regression testing) are relatively easy to adopt; others (such as constructing hazard analyses and threat models, exploiting formal notations when appropriate, and applying static analysis to code) will require new training for many developers. however valuable, though, these practices are in themselves no silver bullet, and new techniques and methods will be required in order to build future software systems to the level of dependability that will be required.assessmentsociety is increasingly dependent on software. software failures can cause or contribute to serious accidents that result in death, injury, signicant environmental damage, or major nancial loss. such accidents have already occurred, and, without intervention, the increasingly pervasive use of softwarešespecially in arenas such as transportation, health care, and the broader infrastructurešmay make them more frequent and more serious. in the future, more pervasive deployment of software in the civic infrastructure could lead to more catastrophic failures unless improvements are made.software, according to a popular view, fails because of bugs: errors in the code that cause a program to fail to meet its specication. in fact, only a tiny proportion of failures can be attributed to bugs. as is well known to software engineers, by far the largest class of problems arises from errors made in the eliciting, recording, and analysis of requirements. a second major class of problems arises from poor human factors design. the two classes are related; bad user interfaces usually re˚ect an inadequate understanding of the user™s domain and the absence of a coherent and wellarticulated conceptual model. security vulnerabilities are to some extent an exception to this observation: the overwhelming majority of security vulnerabilities reported in software productsšand exploited to attack the users of such productsšare at the implementation level. the prevalence of coderelated problems, however, is a direct consequence of higherlevel decisions to use programming languages, design methods, and libraries that admit these problems.in systems where software failure could have signicant human or nancial costs, it is crucial that software be dependablešthat it can be depended upon to function as expected and to not cause or contribute to adverse events in the environment in which it operates. improvements in dependability would allow such systems to be used more widely and software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.4 software for dependable systemswith greater condence for the benet of society. moreover, software itself has great potential to bring improvements in safety in many areas. complete and reliable data about softwarerelated system failures or the efcacy of particular software development approaches are hard to come by, making objective scientic evaluation difcult. moreover, the lack of systematic reporting of softwarerelated system failures is a serious problem that makes it more difcult to evaluate the risks and costs of such failures and to measure the effectiveness of proposed policies or interventions. this lack of evidence has two direct consequences for this report. first, it has informed the key recommendations in this report regarding the need for evidence to be at the core of dependable software system development; for data collection efforts to be established; and for transparency and openness to be encouraged. second, it has tempered the committee™s desire to provide prescriptive guidance: the approach recommended is therefore largely free of endorsements or criticisms of particular development approaches, tools, or techniques. moreover, the report leaves to the developers and procurers of individual systems the question of what level of dependability is appropriate, and what costs are worth incurring to achieve it.nonetheless, the evidence available to the committee did support several qualitative conclusions. first, developing software to meet even existing dependability criteria is difcult and costly. large software projects fail at a high rate, and the cost of projects that do succeed in delivering highly dependable software is often exorbitant. second, the quality of software produced by the industry is extremely variable, and there is inadequate oversight in some critical areas. today™s certication regimes and consensus standards have a mixed record. some are largely ineffective, and some are counterproductive. they share a heavy reliance on testing, which cannot provide sufcient evidence for the high levels of dependability required in many critical applications.a nal observation is that the culture of an organization in which software is produced can have a dramatic effect on its quality and dependability. it seems likely that the excellent record of avionics software is due in large part to a safety culture in that industry that encourages meticulous attention to detail, high aversion to risk, and realistic assessment of software, staff, and process. indeed, much of the benet of standards such as do178b, software considerations in airborne systems and equipment certication, may be due to the safety culture that their strictures induce. software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.summary 5toward certifiably dependable softwarethe focus of this report is a set of fundamental principles that underlie software system dependability and that suggest a different approach to the development and assessment of dependable software. due to a lack of sufcient data to support or contradict any particular approach, a software system may not be declared ﬁdependableﬂ based on the method by which it was constructed. rather, it should be regarded as dependablešcertiably dependablešonly when adequate evidence has been marshaled in support of an argument for dependability that can be independently assessed. the goal of certiably dependable software cannot therefore be achieved by mandating particular processes and approaches, regardless of their effectiveness in certain situations. instead, software developers should marshal evidence to justify an explicit dependability claim that makes clear which properties in the real world the system is intended to establish. such evidence forms a dependability case, and creating a dependability case is the cornerstone of the committee™s approach to developing certiably dependable software systems.explicit claims, evidence, and expertisethe committee™s proposed approach can be summarized in ﬁthe three esﬂšexplicit claims, evidence, and expertise: ł explicit claims. no system can be ﬁdependableﬂ in all respects and under all conditions. so to be useful, a claim of dependability must be explicit. it must articulate precisely the properties the system is expected to exhibit and the assumptions about the system™s environment upon which the claim is contingent. the claim should also indicate explicitly the level of dependability claimed, preferably in quantitative terms. different properties may be assured to different levels of dependability.ł evidence. for a system to be regarded as dependable, concrete evidence must be present that substantiates the dependability claim. this evidence will take the form of a dependability case arguing that the required properties follow from the combination of the properties of the system itself (that is, the implementation) and the environmental assumptions. because testing alone is usually insufcient to establish properties, the case will typically combine evidence from testing with evidence from analysis. in addition, the case will inevitably involve appeals to the process by which the software was developedšfor example, to argue that the software deployed in the eld is the same software that was subjected to analysis or testing.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.6 software for dependable systemsł expertise. expertisešin software development, in the domain under consideration, and in the broader systems context, among other thingsšis necessary to achieve dependable systems. flexibility is an important advantage of the proposed approach; in particular the developer is not required to follow any particular process or use any particular method or technology. this ˚exibility allows experts freedom to employ new techniques and to tailor the approach to the system™s application and domain. but the requirement to produce evidence is highly demanding and likely to stretch today™s best practices to their limit. it will therefore be essential that developers are familiar with best practices and deviate from them only for good reason. these prescriptions shape any particular development approach only in outline and give considerable freedom to developers in their choice of methods, languages, tools, and processes.this approach is not, of course, a silver bullet. there are no easy solutions to the problem of developing dependable software, and there will always be systems that cannot be built to the required level of dependability even using the latest methods. but, the approach recommended is aimed at producing certiably dependable systems today, and the committee believes it holds promise for developing the systems that will be needed in the future.in the overall context of engineering, the basic tenets of the proposed approach are not controversial, so it may be a surprise to some that the approach is not already commonplace. nor are the elements of the approach novel; they have been applied successfully for more than a decade. nevertheless, this approach would require radical changes for most software development organizations and is likely to demand expertise that is currently in short supply.systems engineering approachcomplementing ﬁthe three esﬂ are several systems engineering ideas that provide an essential foundation for the building of dependable software systems:ł systems thinking. engineering elds with long experience in building complex systems (for example, aerospace, chemical, and nuclear engineering) have developed approaches based on ﬁsystems thinking.ﬂ these approaches focus on properties of the system as a whole and on the interactions among its components, especially those interactions (often neglected) between a component being constructed and the components of its environment. as software has come to be deployed inšindeed has software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.summary 7enabledšincreasingly complex systems, the system aspect has come to dominate in questions of software dependability.ł software as a system component. dependability is not an intrinsic property of software. the committee strongly endorses the perspective of systems engineering, which views the software as one engineered artifact in a larger system of many components, some engineered and some given, and views the pursuit of dependability as a balancing of costs and benets and a prioritization of risks. a software component that may be dependable in the context of one system might not be dependable in the context of another.ł humans as components. peoplešthe operators and users (and even the developers and maintainers) of a systemšmay also be viewed as system components. if a system meets its dependability criteria only if people act in certain ways, then those people should be regarded as part of the system, and an estimate of the probability that they will behave as required should be part of the evidence for dependability.ł realworld properties. the properties of interest to the user of a system are typically located in the physical world: that a radiotherapy machine deliver a certain dose, that a telephone transmit a sound wave faithfully, that a printer make appropriate ink marks on paper, and so on. the software, on the other hand, is typically specied in terms of properties at its interfaces, which usually involve phenomena that are not of direct interest to the user: that the radiotherapy machine, telephone, or printer send or receive certain signals at certain ports, with the inputs related to the outputs according to some rules. it is important, therefore, to distinguish the requirements of a software system, which represent these properties in the physical world, from the specication of a software system, which characterizes the behavior of the software system at its interface with the environment. when the software system is itself only one component of a larger system, the other components in the system (including perhaps, as explained above, the people who work with the system) will be viewed as part of the environment. the dependability properties of a software system, therefore, should be expressed as requirements, and the dependability case should demonstrate how these properties follow from the combination of the specication and the environmental assumptions.coping with complexitythe need for evidence of dependability and the difculty of producing such evidence for complex systems have a straightforward but profound implication. any component for which compelling evidence of dependability has been amassed at reasonable cost will likely be small by software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.8 software for dependable systemsthe standards of most modern software systems. every critical specication property, therefore, will have to be assured by one, or at most a few, small components. sometimes it will not be possible to separate concerns so cleanly, and in that case, the dependability case may be less credible or more expensive to produce.as a result, one key to achieving dependability at reasonable cost is a serious and sustained commitment to simplicity, including simplicity of critical functions and simplicity in system interactions. this commitment is often the mark of true expertise. an awareness of the need for simplicity usually comes only with bitter experience and the humility gained from years of practice. there is no alternative to simplicity. advances in technology or development methods will not make simplicity redundant; on the contrary, they will give it greater leverage. to achieve high levels of dependability in the foreseeable future, striving for simplicity is likely to be by far the most costeffective of all interventions. simplicity is not easy or cheap, but its rewards far outweigh its costs.the most important form of simplicity is that produced by independence, in which particular systemlevel properties are guaranteed by individual components much smaller than the system as a whole, which can preserve these properties despite failures in the rest of the system. independence can be established in the overall design of the system, with the support of architectural mechanisms. its effect is to dramatically reduce the cost of constructing a dependability case for a property, since only a relatively small part of the system needs to be considered.appropriate simplicity and independence cannot be accomplished without addressing the challenges of ﬁinteractive complexityﬂ and ﬁtight coupling.ﬂ both interactive complexity, where components may interact in unanticipated ways, and tight coupling, wherein a single fault cannot be isolated but brings about other faults that cascade through the system, are correlated with the likelihood of system failure. softwareintensive systems tend to have both attributes. careful attention should therefore be paid to the risks of interactive complexity and tight coupling and the advantages of modularity, isolation, and redundancy. the interdependences among components of critical software systems should be analyzed to ensure that there is no fault propagation path from less critical components to more critical components, that modes of failure are well understood, and that failures are localized to the greatest extent possible. the reduction of interactive complexity and tight coupling can contribute not only to the improvement of system dependability but also to the development of evidence and analysis in the service of a dependability case.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.summary 9rigorous process and preserving the chain of evidencegenerating a dependability case after the fact, when a development is largely complete, might be possible in theory. but in practice, at least with today™s technology, the costs of doing so would be high, and it will be practical to develop a dependability case only if the system is built with its construction in mind. each step in developing the software needs to preserve the chain of evidence on which will be based the argument that the resulting system is dependable. at the start, the domain and environmental assumptions and the required properties of the system should be made explicit; they should be expressed unambiguously and in a form that permits systematic analysis to ensure that there are no unresolvable con˚icts between the required properties. each subsequent stage of development should preserve the evidence chainšthat these properties have been carried forward without being corruptedšso each form in which the requirements, design, or implementation is expressed should support analysis to permit checking that the required properties have been preserved. what is sufcient will vary with the required dependability, but preserving the evidence chain necessitates that the checks are carried out in a disciplined way, following a documented procedure, and leaving auditable records.the roles of testing, analysis, and formal methodstesting is indispensable, and no software system can be regarded as dependable if it has not been extensively tested, even if its correctness has been proven mathematically. testing may nd ˚aws that elude analysis because it exercises the system in its entirety, whereas analysis must typically make assumptions about the execution platform, the external environment, and operator responses, any of which may turn out to be unwarranted. at the same time, it is important to realize that testing alone is rarely sufcient to establish high levels of dependability. it is erroneous to believe that a rigorous development process, in which testing and code review are the only verication techniques used, justies claims of extraordinarily high levels of dependability. some certication schemes, for example, associate higher safety integrity levels with more burdensome process prescriptions and imply that following the processes recommended for the highest integrity levels will ensure that the failure rate is minuscule. in the absence of a carefully constructed dependability case, such condence is misplaced.because testing alone will not be sufcient for the foreseeable future, the dependability claim will also require evidence produced by analysis. software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.10 software for dependable systemsmoreover, because analysis links the software artifacts directly to the claimed properties, the analysis component of the dependability case will usually contribute condence at a lower cost than testing for the highest levels of dependability. a dependability case will generally require many forms of analysis, including (1) the validation of environmental assumptions, use models, and fault models; (2) the analysis of fault tolerance measures against fault models; (3) schedulability analysis for temporal behaviors; (4) security analysis against attack models; (5) verication of code against module specications; and (6) checking that modules in aggregate achieve appropriate systemlevel effects. these analyses will sometimes involve informal argument that is carefully reviewed; sometimes mechanical inference (as performed, for example, by ﬁtype checkersﬂ that conrm that memory is used in a consistent way and that boundaries between modules are respected); and, sometimes, formal proof. indeed, the dependability case for even a relatively simple system will usually require all of these kinds of analysis, and they will need to be tted together into a coherent whole.traditional software development methods rely on human inspection and testing for validation and verication. formal methods also use testing, but they employ notations and languages that are amenable to rigorous analysis, and they exploit mechanical tools for reasoning about the properties of requirements, specications, designs, and code. practitioners have been skeptical about the practicality of formal methods. increasingly, however, there is evidence that formal methods can yield systems of very high dependability in a costeffective manner, at least for small to mediumsized critical systems. although formal methods are typically more expensive to apply when only low levels of dependability are required, the cost of traditional methods rises rapidly with the level of dependability and often becomes prohibitive. when a highly dependable system is required, therefore, a formal approach may be the most cost effective.certification, transparency, and accountabilitya variety of certication regimes exist for software in particular application domains. for example, the federal aviation authority (faa) itself certies new aircraft (and airtrafc management) systems that include software, and this certication is then relied on by the customers who buy and use the aircraft; the national information assurance partnership (niap) licenses thirdparty laboratories to assess security software products for conformance to the common criteria. some large organizations have their own regimes for certifying that the software products they buy meet the organization™s quality criteria, and many software product software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.summary 11manufacturers have their own criteria that each version of their product must pass before release. few, if any, existing certication regimes encompass the combination of characteristics recommended in this reportšnamely, explicit dependability claims, evidence for those claims, and a rigorous argument that demonstrates that the evidence is sufcient to establish the validity of the claims. to establish that a system is dependable will involve inspection and analysis of the dependability claim and the evidence offered in its support. where the customer for the system is not able to carry out that work itself (for lack of time or lack of expertise) it may need to involve a third party whose judgment it can rely on to be independent of commercial pressures from the vendor. certication can take many forms, from selfcertication by the supplier at one extreme, to independent thirdparty certication by a licensed certication authority at the other. no single certication regime is suitable for all circumstances, so a suitable scheme should be chosen for each circumstance. industry groups and professional societies should consider developing model certication schemes appropriate to their domains, taking account of the detailed recommendations in this report.when choosing suppliers and products, customers and users can make informed judgments only if the claims are credible. such claims are unlikely to be credible if the evidence underlying them is not transparent. economists have established that if consumers cannot reliably observe quality before they buy, sellers may get little economic benet from providing higher quality than their competitors, and overall quality can decline. sellers are concerned about future sales, and ﬁreputation effectsﬂ compel them to strive to maintain a minimum level of quality. if consumers rely heavily on branding, though, it becomes more difcult for new rms to enter the market, and quality innovations spread more slowly.those claiming dependability for their software should therefore make available the details of their claims, criteria, and evidence. to assess the credibility of such details effectively, an evaluator should be able to calibrate not only the technical claims and evidence but also the organization that produced them, because the integrity of the evidence chain is vital and cannot easily be assessed without supporting data. this suggests that in some cases data of a more general nature should be made available, including the qualications of the personnel involved in the development; the track record of the organization in providing dependable software; and the process by which the software was developed. the willingness of a supplier to provide such data, and the clarity and integrity of the data that the supplier provides, will be a strong indication of its attitude to dependability.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.12 software for dependable systemswhere there is a need to deploy software that satises a particular dependability claim, it should always be explicit who is accountable for any failure to achieve it. such accountability can be made explicit in the purchase contract, or as part of certication of the software, or as part of a professional licensing scheme, or in other ways. since no single solution will suit all the circumstances in which certiably dependable software systems are deployed, accountability regimes should be tailored to particular circumstances. at present, it is common for software developers to disclaim, so far as possible, all liability for defects in their products, to a greater extent than customers and society expect from manufacturers in other industries. clearly, no software should be considered dependable if it is supplied with a disclaimer that withholds the manufacturer™s commitment to provide a warranty or other remedies for software that fails to meet its dependability claims. determining the appropriate scale of remedies, however, was beyond the scope of this study and would require a careful analysis of benets and costs, taking into account not only the legal issues but also the state of software engineering, the various submarkets for software, the economic impact, and the effect on innovation.key findings and recommendationspresented below are the committee™s ndings and recommendations, each of which is discussed in more detail in chapter 4.findingsimprovements in software development are needed to keep pace with societal demands for software. avoidable software failures have already been responsible for loss of life and for major economic losses. the quality of software produced by the industry is extremely variable, and there is inadequate oversight in several critical areas. more pervasive deployment of software in the civic infrastructure may lead to catastrophic failures unless improvements are made. software has the potential to bring dramatic benets to society, but it will not be possible to realize these benetsšespecially in critical applicationsšunless software becomes more dependable.more data are needed about software failures and the efcacy of development approaches. assessment of the state of the software industry, the risks posed by software, and progress made is currently hampered by the lack of a coherent source of information about software failures. software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.summary 13recommendations to builders and users of softwaremake the most of effective software development technologies and formal methods. a variety of modern technologiesšin particular, safe programming languages, static analysis (analysis of software and source code done without actually executing the program), and formal methodsšare likely to reduce the cost and difculty of producing dependable software. follow proven principles for software development. the committee™s proposed approach also includes adherence to the following principles: ł take a systems perspective. here the dependability of software is viewed not in terms of intrinsic properties (such as the incidence of bugs in the code) but rather in terms of the system as a whole, including interactions among people, process, and technology.ł exploit simplicity. if dependability is to be achieved at reasonable cost, simplicity should become a key goal, and developers and customers must be willing to accept the compromises it entails. make a dependability case for a given system and context: evidence, explicitness, and expertise. a software system should be regarded as dependable only if sufcient evidence of its explicitly articulated properties is presented to substantiate the dependability claim. this approach gives considerable leeway to developers to use whatever practices are best suited to the problem at hand. in practice the challenges of developing dependable software are sufciently great that developers will need considerable expertise, and they will have to justify any deviations from best practices.demand more transparency, so that customers and users can make more informed judgments about dependability. customers and users can make informed judgments when choosing suppliers and products only if the claims, criteria, and evidence for dependability are transparent. make use of but do not rely solely on process and testing. testing will be an essential component of a dependability case, but will not in general sufce, because even the largest test suites typically used will not exercise enough paths to provide evidence that the software is correct nor will it have sufcient statistical signicance for the levels of condence usually desired. rigorous process is essential for preserving the chain of dependability evidence but is not per se evidence of dependability.base certication on inspection and analysis of the dependability claim and the evidence offered in its support. because testing and process alone are insufcient, the dependability claim will require, in addisoftware for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.14 software for dependable systemstion, evidence produced by other modes of analysis. security certication in particular should go beyond functional testing of the security components of a system and assess the effectiveness of measures the developer took to prevent the introduction of security vulnerabilities.include security considerations in the dependability case. security vulnerabilities can undermine the case made for dependability properties by violating assumptions about how components behave, about their interactions, or about the expected behavior of users. the dependability case must therefore account explicitly for security risks that might compromise its other aspects. it is also important to ensure that security certications give meaningful assurance of resistance to attack. new security certication regimes are needed that can provide condence that most attacks against certied products or systems will fail. such regimes can be built by applying the other ndings and recommendations of this report, with an emphasis on the role of the environmentšin particular, the assumptions made about the potential actions of a hostile attacker and the likelihood that new classes of vulnerabilities will be discovered and new attacks developed to exploit them.demand accountability and make it explicit. where there is a need to deploy certiably dependable software, it should always be made explicit who or what is accountable, professionally and legally, for any failure to achieve the declared dependability. recommendations to agencies and organizations that support software education and researchthe committee was not constituted or charged to recommend budget levels or to assess tradeoffs between software dependability and other priorities. however, it believes that the increasing importance of software to society and the extraordinary challenge currently faced in producing software of adequate dependability provide a strong rationale for investment in education and research initiatives. place greater emphasis on dependabilityšand its fundamental underpinningsšin the high school, undergraduate, and graduate education of software developers. many practitioners do not have an adequate appreciation of the software dependability issues discussed in this report, are not aware of the most effective development practices available today, or are not capable of applying them appropriately. wider implementation of the committee™s recommended approach, which goes beyond today™s state of the practice, implies a need for further education and training activities. software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.summary 15federal agencies that support information technology research and development should give priority to basic research to further softwareenabled system dependability, emphasizing a systems perspective and evidence. in keeping with this report™s approach, such research should emphasize a systems perspective and ﬁthe three esﬂ (explicit claims, evidence, and expertise) and should be informed by a systems view that attaches more importance to those advances that are likely to have an impact in a world of large systems interacting with other systems and operators in a complex physical environment and organizational context. software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.161 assessment: software systems and  dependability today the software industry is, by most measures, a remarkable success. but it would be unwise to be complacent and assume that software is already dependable enough or that its dependability will improve without any special efforts.software dependability is a pressing concern for several reasons:ł developing software to meet existing dependability criteria is notoriously difcult and expensive. large software projects fail at a rate far higher than other engineering projects, and the cost of projects that deliver highly dependable software is often exorbitant.ł software failures have caused serious accidents that resulted in death, injury, and large nancial losses. without intervention, the increasingly pervasive use of software may bring about more frequent and more serious accidents.ł existing certication schemes that are intended to ensure the dependability of software have a mixed record. some are largely ineffective, and some are counterproductive.ł software has great potential to improve safety in many areas. improvements in dependability would allow software to be used more widely and with greater condence for the benet of society.this chapter discusses each of these issues in turn. it then discusses the committee™s ve observations that informed the report™s recommendations and ndings.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.assessment 17cost and schedule challenges in software development for many years, international surveys have consistently reported that less than 30 percent of commercial software development projects are nished on time and within budget and satisfy the business requirements. the exact numbers are hard to discern and subject to much discussion and disagreement, because few surveys publish their denitions, methodologies, or raw data. however, there is widespread agreement that only a small percentage of projects deliver the required functionality, performance, and dependability within the original time and cost estimate.software project failure has been studied quite widely by governments, consultancy companies, academic groups, and learned societies. two such studies are one published by the standish group and another by the british computer society (bcs). the standish group reported that 28 percent of projects succeeded, 23 percent were cancelled, and 49 percent were ﬁchallengedﬂ (that is, overran signicantly or delivered limited functionality).1 the bcs surveyed2 38 members of the bcs, the association of project managers, and the institute of management, covering 1,027 projects in total. of these, only 130, or 12.7 percent, were successful; of the successful projects, 2.3 percent were development projects, 18.2 percent maintenance projects, and 79.5 percent data conversion projectsšyet development projects made up half the total projects surveyed. that means that of the more than 500 development projects included in the survey, only three were judged to have succeeded.the surveys covered typical commercial applications, but applications with signicant dependability demands (ﬁdependable applications,ﬂ for short) show similar high rates of cancellation, overrun, and inservice failure. for example, the u.s. department of transportation™s ofce of the inspector general and the government accountability ofce track the progress of all major faa acquisition projects intended to modernize and add new capabilities to the national airspace system. as of may 2005, of 16 major acquisition projects being tracked, 11 were over budget, with total cost growth greater than $5.6 billion; 9 had experienced schedule delays ranging from 2 to 12 years; and 2 had been deferred.3 software is cited as the primary reason for these problems.1 robert l. glass, 2005, ﬁit failure ratesš70 percent or 1015 percent?ﬂ ieee software 22(3):112.2 andrew taylor, 2001, ﬁit projects sink or swim,ﬂ based on author™s m.b.a. dissertation, bcs review.3 dot, ofce of the inspector general, 2005, ﬁstatus of faa™s major acquisitions: cost growth and schedule delays continue to stall air trafc modernization,ﬂ report number av2005061, may 26.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.18 software for dependable systemsan air force project that has been widely studied and reported illustrates the difculty of developing dependable software using the methods currently employed by industry leaders. the f/a22 aircraft has been under development since 1986. much of the slow pace of development has been attributed to the difculty of making the complex software dependable.4 the instability of the software has often been cited as a cause of schedule delays5,6 and the loss of at least one test aircraft.7 the integrated avionics suite for the f/a22 is reported to have been redesigned as recently as august 2005 to improve stability, among other things.8the similarly low success rates in both typical and dependable applications is unsurprising, because dependable applications are usually developed using methods that do not differ fundamentally from those used commercially. the developers of dependable systems carry out far more reviews, more documentation, and far more testing, but the underlying methods are the same. the evidence is clear: these methods cannot dependably deliver today™s complex applications, let alone tomorrow™s even more complex requirements.it must not be forgotten that creating dependable software systems itself has economic consequences. consider areas such as dynamic routing in air trafc control, where there are not only signicant opportunities to improve efciency and (arguably) safety, but also great risks if automated systems fail. disruptions and accidents due to softwarethe growing pervasiveness and centrality of software in our civic infrastructure is likely to increase the severity and frequency of accidents that can be attributed to software. moreover, the risk of a major catastrophe in which software failure plays a part is increasing, because the growth in complexity and invasiveness of software systems is not being matched by improvements in dependability.software has already been implicated in cases of widespread economic disruption, major losses to large companies, and accidents in which 4 michael a. dornheim, 2005, ﬁcodes gone awry,ﬂ aviation week & space technology, february 28, p. 63.5 robert wall, 2003, ﬁcode red emergency,ﬂ aviation week & space technology, june 9, pp. 3536.6 general accounting ofce, 2003, ﬁtactical aircraft, status of the f/a22 program: statement of allen li, director, acquisition and sourcing management,ﬂ gao33603t, april 2.7 u.s. air force, ﬁaircraft accident investigation,ﬂ f/a22 s/n 004014. available online at <http://www.airforcetimes.com/content/editorial/pdf/af.exsumf22crash060805.pdf>.8 stephen trimble, 2005, ﬁavionics redesign aims to improve f/a22 stability,ﬂ flight international, august 23.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.assessment 19hundreds of people have been killed. accidents usually have multiple causes, and software is rarely the sole cause. but this is no comfort. on the contrary, software can (and should) reduce, rather than increase, the risks of system failures.the economic consequences of security failures in desktop software have been severe to date. several individual viruses and worms have caused events where damage was assessed at over $1 billion eachšcode red was assessed at $2.75 billion worldwide9šand two researchers have estimated that a worstcase worm could cause $50 billion in damage.10 one must also consider the aggregated effect of minor loss and inconvenience in˚icted on large numbers of people. in several incidents in the last few years, databases containing the personal information of thousands of individualsšsuch as credit card datašwere breached. security attacks on personal computers are now so prevalent that according to some estimates, a machine connected to the internet without appropriate protection would be compromised in under 4 minutes,11 less time than it takes to download uptodate security patches. in domains where attackers may nd sufcient motivation, such as the handling of nancial records or the management of critical infrastructures, and with the growing risk and fear of terrorism and the evolution of mass network attacks, security has become an important concern. for example, as noted elsewhere, in the summer of 2005, radiotherapy machines in merseyside, england, and in boston were attacked by computer viruses. it makes little sense to invest effort in ensuring the dependability of a system while ignoring the possibility of security vulnerabilities. a basic level of securityšin the sense that a software system behaves properly even in the presence of hostile inputs from its environmentšshould be required of any software system that is connected to the internet, used to process sensitive or personal data, or used by an organization for its critical business or operational functions.automation tends to reduce the probability of failure while increasing its severity because it is used to control systems when such control is beyond the capabilities of human operators without such assistance.12 9 see computer economics, 2003, ﬁvirus attack costs on the risešagain,ﬂ figure 1. available online at <http://www.computereconomics.com/article.cfm?id=873>.10 nicholas weaver and vern paxson, 2004, ﬁa worstcase worm,ﬂ presented at the third annual workshop on economics and information security (weis04), march 1314. available online at <http://www.dtc.umn.edu/weis2004/weaver.pdf>.11 gregg keizer, 2004, ﬁunprotected pcs fall to hacker bots in just four minutes,ﬂ tech web, november 30. available online at <http://www.techweb.com/wire/security/54201306>. 12 n. sarter, d.d. woods, and c. billings, 1997, ﬁautomation surprises,ﬂ handbook of human factors/ergonomics, 2nd ed., g. salvendy, ed., wiley, new york. (reprinted in n. moray, ed., ergonomics: major writings, taylor & francis, boca raton, fla., 2004.)software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.20 software for dependable systemsaviation, for example, is no exception, and current trendsšsuperairliners, free ˚ight, greater automation, reduced human oversight in airtrafc control, and so onšincrease the potential for less frequent but more serious accidents. high degrees of automation can also reduce the ability of human operators to detect and correct mistakes. in airtrafc control, for example, there is a concern that the failure of a highly automated system that guides aircraft, even if detected before an accident occurs, might leave controllers in a situation beyond their ability to resolve, with more aircraft to consider, and at smaller separations than they can handle. there is also a legitimate concern that a proliferation of safety devices itself creates new risks. the trafc alert and collision avoidance system (tcas), an onboard collision avoidance system now mandatory on all commercial aircraft,13 has been implicated in at least one near miss.14hazardous materialsthe potential for the worst software catastrophes resulting in thousands of deaths lies with systems involving hazardous materials, most notably plants for nuclear power, chemical processing, storing liqueed natural gas, and other related storage and transportation facilities. although software has not been implicated in disasters on the scale of those in chernobyl15 or bhopal,16 the combination of pervasive software and high risk is worrying. software is used pervasively in plants for monitoring and control in distributed control systems (dcs) and supervisory control and data acquisition (scada) systems. according to the epa,17 123 chemical plants in the united states could each expose more than a million people if a chemical release occurred, and a newspaper article reports that a plant in tennessee gave a worstcase estimate of 60,000 people facing death or serious injury from a vapor cloud formed by an 13 for more information on tcas, see the faa™s ﬁtcas home page.ﬂ available online at <http://adsb.tc.faa.gov/tcas.htm>.14 n. sarter, d.d. woods, and c. billings, 1997, ﬁautomation surprises,ﬂ handbook of human factors/ergonomics, 2nd ed., g. salvendy, ed., wiley, new york. (reprinted in n. moray, ed., ergonomics: major writings, taylor & francis, boca raton, fla., 2004.)15 see the web site ﬁchernobyl.info: the international communications platform on the longterm consequences of the chernobyl disasterﬂ at <http://www.chernobyl.info/>.16 see bbc news™ ﬁone night in bhopal.ﬂ available online at <http://www.bbc.co.uk/ bhopal>.17 see u.s. general accounting ofce, 2004, ﬁfederal action needed to address security challenges at chemical facilities,ﬂ statement of john b. stephenson before the subcommittee on national security, emerging threats, and international relations, committee on government reform, house of representatives (gao04482t), p. 3. available online at <http://www.gao.gov/new.items/d04482t.pdf>.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.assessment 21accidental release of sulfur dioxide.18 railways already make extensive use of software for signaling and safety interlocks, and the use of software for some degree of remote control of petrochemical tanker trucks (e.g., remote shutdown in an emergency) is being explored.19aviationsmaller but still major catastrophes involving hundreds rather than thousands of deaths have been a concern primarily in aviation. commercial ˚ight is far safer than other means of travel, and the accident rate per takeoff and landing, or per mile, is extremely small (although accident rates in private and military aviation are higher). increasing density of airspace use and the development of airliners capable of carrying larger numbers of passengers pose greater risks, however.although software has not generally been directly blamed for an aviation disaster, it has been implicated in some accidents and near misses. the 1997 crash of a korean airlines 747 in guam resulted in 200 deaths and would almost certainly have been avoided had a minimum safe altitude warning system been congured correctly.20 several aircraft accidents have been attributed to ﬁmode confusion,ﬂ where the software operated as designed but not as expected by the pilots.21 several incidents in 2005 further illustrate the risks posed by software:ł in february 2005, an airbus a340642 en route from hong kong to london suffered from a failure in a data bus belonging to a computer that monitors and controls fuel levels and ˚ow. one engine lost power and a second began to ˚uctuate; the pilot diverted the aircraft and landed safely in amsterdam. the subsequent investigation noted that although a backup slave computer was available that was working correctly, the failing computer remained selected as the master due to faulty logic in the software. a second report recommended an independent lowfuel warning system and noted the risks of a computerized management system 18 see james v. grimaldi and guy gugliotta, 2001, ﬁchemical plants feared as targets,ﬂ washington post, december 16, p. a01. 19 see ﬁtanker truck shutdown via satellite,ﬂ 2004, gps news, november. available online at <http://www.spacedaily.com/news/gps03zn.html>.20 for more information, see the national transportation safety board™s formal report on the accident. available online at <http://www.ntsb.gov/publictn/2000/aar0001.htm>.21 see nasa™s ﬁfm program: analysis of mode confusion.ﬂ available online at <http://shemesh.larc.nasa.gov/fm/fmnowmodeconfusion.html>; updated august 6, 2001.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.22 software for dependable systemsthat might fail to provide crew with appropriate data, preventing them from taking appropriate actions.22ł in august 2005, a boeing 777200 en route from perth to kuala lumpur presented the pilot with contradictory reports of airspeed: that the aircraft was overspeed and at the same time at risk of stalling. the pilot disconnected the autopilot and attempted to descend, but the autothrottle caused the aircraft to climb 2,000 ft. he was eventually able to return to perth and land the aircraft safely. the incident was attributed to a failed accelerometer. the air data inertial reference unit (adiru) had recorded the failure of the device in its memory, but because of a software ˚aw, the unit failed to recheck the device™s status after power cycling.23ł in october 2005, an airbus a319131 ˚ying from heathrow to budapest suffered a loss of cockpit power that shut down not only avionics systems but even the radio and transponder, preventing the pilot from issuing a mayday call. at the time of writing, the cause has not been determined. an early report in the subsequent investigation noted, however, that an action was available to the pilots that would have restored power, but it was not shown on the user interface due to its position on a list, and a software design that would have required items higher on the list to be manually cleared in order for that available action to be shown.24perhaps the most serious softwarerelated near miss incident to date occurred on september 14, 2004. a software system at the los angeles air route trafc control center in palmdale, california, failed, preventing any voice communication between controllers and aircraft. the center is responsible for aircraft ˚ying above 13,000 ft in a wide area over southern california and adjacent states, and the outage disrupted about 800 ˚ights across the country. according to the new york times, aircraft violated minimum separation distances at least ve times, and it was only due to onboard collision detection systems (i.e., tcas systems) that no collisions actually occurred. the problem was traced to a bug in the software, in which a countdown timer reaching zero shut down the system.25 the 22 see air accidents investigation branch (aaib) bulletin s1/2005œspecial (ref: ew/c2005/02/03). available online at <http://www.aaib.dft.gov.uk/cmsresources/gvatlspecialbulletin1.pdf>.23 see aviation safety investigation reportšinterim factual, occurrence number 200503722. november 2006. available online at <http://www.atsb.gov.au/publications/investigationreports/2005/aair/aair200503722.aspx>. 24 see aaib bulletin s3/2006 special (ref. ew/c2005/10/05). available online at <http://www.aaib.dft.gov.uk/cmsresources/s32006%20geuob.pdf>. 25 l. geppert, 2004, ﬁlost radio contact leaves pilots on their own,ﬂ ieee spectrum 41(11):1617.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.assessment 23presence of the bug was known, and the faa was in the process of distributing a patch. the faa ordered the system to be restarted every 30 days in the interim, but this directive was not followed. worryingly, a backup system that should have taken over also failed within a minute of its activation. this incident, in common with the hospital system failure described in the next section, illustrates the greater risk that is created when services affecting a large area or many people are centralized in a single system, which then becomes a single point of failure.medical devices and systemsmedical devices such as radiation therapy machines and infusion pumps are potentially lethal. implanted devices pose a particular threat, because although a single failure affects only one user, a ˚aw in the software of a device could produce failures across the entire population of users. safety recalls of pacemakers and implantable cardioverterdebrillators due to rmware (that is, software) problems between 1990 and 2000 affected over 200,000 devices, comprising 41 percent of the devices recalled and are increasing in frequency.26 in the 20year period from 1985 to 2005, the fda™s maude database records almost 30,000 deaths and almost 600,000 injuries from device failures.27in a study the fda conducted between 1992 and 1998, 242 out of 3,140 device recalls (7.7 percent) were found to be due to faulty software.28 of these, 192šalmost 80 percentšwere caused by defects introduced during software maintenance.29 the actual incidence of failures in medical devices due to software is probably much higher than these numbers suggest, as evidenced by a gao study30 that found extensive underreporting of medical device failures in general. 26 william h. maisel, michael o. sweeney, william g. stevenson, kristin e. ellison, laurence m. epstein, 2001, ﬁrecalls and safety alerts involving pacemakers and implantable cardioverterdebrillator generators,ﬂ journal of the american medical association 286:793799.27 fda, 2006, ensuring the safety of marketed medical devices: cdrh™s medical device postmarket safety program. january.28 insup lee and george pappas, 2006, report on the highcondence medicaldevice software and systems (hcmdss) workshop. available online at <http://rtg.cis.upenn.edu/hcmdss/hcmdssnalreport060206.pdf>.29 in addition, it should be noted that delays in vendor testing and certication of patches often make devices (and therefore even entire networks) susceptible to worms and other malware.30 gao, 1986, ﬁmedical devices: early warning of problems is hampered by severe underreporting,ﬂ u.s. government printing ofce, washington, d.c., gao publication pemd871. for example, the study noted that of over 1,000 medical device failures surveyed, 9 percent of which caused injury and 37 percent of which had the potential to cause death or serious injury, only 1 percent were reported to the fda.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.24 software for dependable systemsindeed, software failures have been responsible for some notable catastrophic device failures, of which perhaps the best known are failures associated with radiotherapy machines that led to patients receiving massive overdoses. the welldocumented failure of the therac25, which led to more than ve deaths between 1985 and 1987, exposed not only incompetence in software development but also a development culture unaware of safety issues.31 a very similar accident in panama in 200132 suggests that these lessons were not universally applied.33 as software becomes more pervasive in medicine, and reliance is placed not only on the software that controls physical processes but also on the results produced by diagnostic and scanning devices, the opportunity for software failures with lethal consequences will grow. in addition, software used for data management, while often regarded as noncritical, may in fact pose risks to patients that are far more serious than those posed by physical devices. most hospitals are centralizing patient records and moving toward a system in which all records are maintained electronically. the failure of a hospitalwide database brings an entire hospital to a standstill, with catastrophic potential. such failures have already been reported.34 an incident reported by cook and o™connor is indicative of the kinds of risks faced. a software failure in a pharmacy database in a tertiarycare hospital in the chicago area made all medication records inaccessible 31 see nancy leveson and clark s. turner, 1993, ﬁan investigation of the therac25 accidents,ﬂ ieee computer 26(7):1841.32 see international atomic energy agency (iaea), 2001, ﬁinvestigation of an accidental exposure of radiotherapy patients in panama: report of a team of experts,ﬂ international atomic energy agency, vienna, austria. available online at <http://wwwpub.iaea.org/mtcd/publications/pdf/pub1114scr.pdf>.33 a number of studies have investigated challenges related to infusion devices. see r.i. cook, d.d. woods, and m.b. howie, 1992, ﬁunintentional delivery of vasoactive drugs with an electromechanical infusion device,ﬂ journal of cardiothoracic and vascular anesthesia 6:238244; m. nunnally, c.p. nemeth, v. brunetti, and r.i. cook, 2004, ﬁlost in menuspace: user interactions with complex medical devices,ﬂ ieee transactions on systems, man and cyberneticsšpart a: systems and humans 34(6):736742; l. lin, r. isla, k. doniz, h. harkness, k. vicente, and d. doyle, 1998, ﬁapplying human factors to the design of medical equipment: patient controlled analgesia,ﬂ journal of clinical monitoring 14:253263; l. lin, k. vicente, and d.j. doyle, 2001, ﬁpatient safety, potential adverse drug events, and medical device design: a human factors engineering approach,ﬂ journal of biomedical informatics 34(4):274284; r.i. cook, d.d. woods, and c. miller, 1998, a tale of two stories: contrasting views on patient safety, national patient safety foundation, chicago, ill., april. available online at <http://www.npsf.org/exec/report.html>.34 see, for example, peter kilbridge, 2003, ﬁcomputer crash: lessons from a system failure,ﬂ new england journal of medicine 348:881882, march 6; richard cook and michael o™connor, ﬁthinking about accidents and systems,ﬂ forthcoming, in k. thompson and h. manasse, eds., improving medication safety, american society of healthsystem pharmacists, washington, d.c.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.assessment 25for almost a day. the pharmacy relied on this database for selecting and distributing medications throughout the hospital and was only able to continue to function by collecting paper records from nurses™ stations and reentering all the data manually. had the paper records not been available, the result would have been catastrophic. although no patients were injured, cook and o™connor were clear about the signicance of the event: ﬁaccidents are signals sent from deep within the systems about the sorts of vulnerability and potential for disaster that lie within.ﬂ35 in many application areas, effectiveness and safety are clearly distinguished from each other. in medicine, however, the distinction can be harder to make. the accuracy of the data produced by medical information systems is often critical, and failure to act in a timely fashion can be as serious as failure to prevent an accident. moreover, the integration of invasive devices with hospital networks will ultimately erase the gap between devices and databases, so that failures in seemingly unimportant backofce applications might compromise patient safety. networking also makes hospital systems vulnerable to security attacks; in the summer of 2005, radiotherapy machines in merseyside, england36 were attacked by a computer virus. in contrast to the problem described above, this attack affected availability, not the particular treatment delivered.computerized physician order entry (cpoe) systems are widely used and can reduce the incidence of medical errors as well as bring efciency improvements. the ability to take notes by computer rather than by hand and instantly make such information available to others of the medical team can save lives. the ability to record prescriptions the minute they are prescribed, and the automated checking of these prescriptions against others the patient is taking, reduces the likelihood of interactions. the ability to make a tentative diagnosis and instantly receive information on treatment options clearly improves efciency. but one study37 suggests that poorly designed and implemented systems can actually facilitate medication errors. user interfaces may be poorly designed and hard to use, and important functions that once, before computerization, were implemented by other means may be missing. moreover, users can 35 richard cook and michael o™connor, ﬁthinking about accidents and systems,ﬂ forthcoming, in k. thompson and h. manasse, eds., improving medication safety, american society of healthsystem pharmacists, washington, d.c., p. 15. available online at <http://www.ctlab.org/documents/ashpchapter.pdf>.36 bbc news, 2005, ﬁhospital struck by computer virus,ﬂ august 22. available online at <http://news.bbc.co.uk/1/hi/england/merseyside/4174204.stm>.37 ross koppel, joshua p. metlay, abigail cohen, brian abaluck, a. russell localio, stephen e. kimmel, and brian l. strom, 2005, ﬁrole of computerized physician order entry systems in facilitating medication errors,ﬂ journal of the american medical association 293(10):11971203.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.26 software for dependable systemsbecome reliant on the information such systems provide, even to the point of using it for invalid purposes (for example, using doses in the pharmacy database to infer normative ranges).the usability of medical information systems is an important consideration as poor usability may not only lead to accidents but may also reduce or even eliminate efciency gains and lower the quality of care. if an information system is not designed to carefully represent complex traditional procedures in digital form, information may be lost or misrepresented. moreover, avenues for data entry by physicians need to ensure that the physicians are able to pay sufcient attention to the patient and pick up any subtle cues about the illness without being distracted by the computer and data entry process. many of these challenges might stem from organizational control issuesšcentralized and rigid design that fails to recognize the nature of practice,38 central rulemaking designed to limit clinical choices, insurance requirements that bin various forms of a particular condition in a way that fails to individualize treatment, and insufcient assessment after deployment. however, technology plays a role in poorly designed and inefcient user interfaces as well. although the computerization of health care can offer improvements in safety and efciency, care is needed so that computerization does not undermine the safety of existing manual procedures. in the medical device industry, for example, while many of the largest manufacturers have wellestablished safety programs, smaller companies may face challenges with respect to safety, perhaps because they lack the necessary resources and expertise.39 infrastructureby enhancing communication and live data analysis, software offers opportunities for efciency improvements in transportation and other infrastructure. within a decade or two, for example, trafc ˚ow may be controlled by extensive networks of monitors, signals, and trafc advisories sent directly to cars.40 a major, sustained failure of such a system might be catastrophic. for critical functions such as ambulance, re, and police services, any failure has catastrophic potential. the failure of even 38 see kathryn montgomery, 2006, how doctors think, clinical judgment and the practice of medicine, oxford university press, oxford, united kingdom.39 a recent fda report estimates that there are about 15,000 manufacturers of medical devices and notes that ﬁthese small rms may lack the experience to anticipate, recognize, or address manufacturing problems that may pose safety concerns.ﬂ ensuring the safety of marketed medical devices: cdrh™s medical device postmarket safety program, january 2006.40 see ongoing work at <http://www.foresight.gov.uk/intelligentinfrastructuresystems/index.htm>.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.assessment 27one component, such as the dispatch system, can have signicant repercussions; the infamous collapse of the london ambulance system41 demonstrated how vulnerable such a system is just to failures of availability.software is a key enabler for greater fuel efciency; modern cars rely heavily on software for engine control, and in some cars, control is largely by electrical rather than mechanical means. however, software ˚aws might cause a car to fail to respond to commands or even to shut down entirely. whereas mechanical failures are often predictable (through evidence of wear, for example), software failures can be sudden and unexpected and, due to coupling, can have farreaching effects. in 2005, for example, toyota identied a software ˚aw that caused prius hybrid cars to stall or shut down when traveling at high speed; 23,900 vehicles were affected.42 in the realm of communications infrastructure, advances in telecommunications have resulted in lower costs, greater ˚exibility, and huge increases in bandwidth. these improvements have not, however, been accompanied by improvements in robustness. cell phone networks have a differentšnot necessarily improvedšvulnerability posture than conventional landline systems, and even the internet, despite its redundancies, may be susceptible to failure under extreme load.43 the disaster on september 11 and hurricane katrina were both exacerbated by failures of communication systems.44defensethe u.s. military is a large, if not the largest, user of information technology and software. failures in military systems, as one might expect, can have disastrous consequences:a u.s. soldier in afghanistan used a precision lightweight gps receiverša ﬁpluggerﬂšto set coordinates for an air strike. he then saw 41 d. page, p. williams, and d. boyd, 1993, report of the inquiry into the london ambulance service, communications directorate, south west thames regional health authority, london, february. available online at <http://www.cs.ucl.ac.uk/staff/ a.finkelstein/las/lascase0.9.pdf>.42 sholnn freeman, 2005, ﬁtoyota attributes prius shutdowns to software glitch,ﬂ wall street journal, may 16. available online at <http://online.wsj.com/articleprint/sb111619464176634063.html>.43 for a discussion of how the traditional landline phone system and the internet manage congestion and other issues, see national research council, 1999, trust in cyberspace, national academy press, washington, d.c. available online at <http://books.nap.edu/catalog.php?recordid=6161>.44 for more information on communications relating to september 11, 2001, see national research council, 2003, the internet under crisis conditions: learning from september 11, the national academies press, washington, d.c. available online at <http://books.nap.edu/catalog.php?recordid=10569>.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.28 software for dependable systemsthat the ﬁbattery lowﬂ warning light was on. he changed the battery, then pressed ﬁfire.ﬂ the device was designed, on starting or resuming operation after a battery change, to initialize the coordinate variables to its own location.45 it was reported that three soldiers were killed in this incident.46 the error appears to have been the result of failing to consider the larger system when dening the safety properties that guided the design of the software. hazard analysis should have revealed the danger of transmitting the location of the plugger as the destination for a missile strike, and once the hazard had been identied, it would be straightforward to specify a system property that required (for example) that the specied target be more than some specied (safe) distance away, and that this be checked by the software before the target coordinates are transmitted.defense systems with high degrees of automation are inherently risky. the patriot surfacetoair missile, for example, failed with catastrophic effect on several occasions. an iraqi scud missile hit the u.s. barracks in dhahran, saudi arabia, in february 1991, killing 28 soldiers; a government investigation47 found that a patriot battery failed to intercept the missile because of a software error. u.s. patriot missiles downed a british tornado jet and an american f/a18 hornet in the iraq war in 2003.distribution of energy and goodssoftware failures could also interrupt the distribution of goods and services, such as gasoline, food, and electricity. an extended blackout during wintertime in a cold area of the united states would be an emergency. the role of software in the blackout in the northeast in 2003 is complicated, but at the very least it seems clear that had the software monitoring system correctly identied the initial overload, it could have been contained without leading to systemwide failure.48apart from experiencing functional failures or design ˚aws, software is also vulnerable to malicious attacks. the very openness and ubiquity that makes networked systems attractive exposes them to attack by van45 from page 83 in michael jackson, 2004, ﬁseeing more of the world,ﬂ ieee software 21(6):8385. available online at <http://mcs.open.ac.uk/mj665/seemore3.pdf>.46 vernon loeb, 2002, ﬁ‚friendly re™ deaths traced to dead battery: taliban targeted, but us forces killed,ﬂ washington post, march 24, p. a21.47 gao, 1992, patriot missile software problem, report of the information management and technology division. available online at <http://www.fas.org/spp/starwars/gao/im92026.htm>.48 see charles perrow, 2007, the next catastrophe: reducing our vulnerabilities to natural, industrial, and terrorist disasters, princeton university press, princeton, n.j., chapter 7.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.assessment 29dals or criminals. the trend to connecting critical systems to the internet is especially worrying, because it often involves placing in a new and unknown environment a program whose design assumed that it would be running on an isolated computer. in the summer of 2005, two separate incidents were reported wherein radiotherapy systems were taken of˚ine because their computers were infected by viruses after the systems had been connected to the internet.49 numerous studies and signicant research have been carried out in software and network security. this report does not focus on the security aspects of dependability, but analysis of the security aspects of a system should be part of any dependability case (see chapter 2 for a discussion of dependability cases generally, and see chapter 3 for more on security). votingthere have been many reports of failures of software used for electronic voting, although none have been substantiated by careful and objective analysis. but there are few grounds for condence, and some of the most widely used electronic voting software has been found by independent researchers to be insecure and of low quality.50 in the 2006 election in sarasota county, florida, the outcome was decided by a margin of 363 votes, yet over 18,000 ballots cast on electronic voting machines did not register a vote. a lawsuit led to force a revote cites, among other things, the possibility of software malfunction and alleges that the machines were improperly certied.51problems with existing certification schemesevidence for the efcacy of existing certication schemes is hard to come by. what seems certain, however, is that experience with certication varies dramatically across domains, with different communities of users, developers, and certiers having very different perceptions of certication. a variety of certication regimes exist for software in particular application domains. for example, the federal aviation authority (faa) itself certies new aircraft (and airtrafc management) systems that include software, and this certication is then relied on by the cus49 bbc news, 2005, ﬁhospital struck by computer virus,ﬂ august 25. available online at <http://news.bbc.co.uk/1/hi/england/merseyside/4174204.stm>.50 see avi rubin et al., 2004, ﬁanalysis of an electronic voting system,ﬂ ieee symposium on security and privacy, oakland, calif., may. available online at <http://avirubin.com/vote.pdf>. 51 see the full complaint online at <http://www.eff.org/activism/evoting/˚orida/sarasotacomplaint.pdf>.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.30 software for dependable systemstomers who buy and use the aircraft; whereas the national information assurance partnership (niap) licenses thirdparty laboratories to assess security software products for conformance to the common criteria (cc).52 some large organizations have their own regimes for certifying that the software products they buy meet their quality criteria, and many product manufacturers have their own criteria that each version of their product must pass before release. few, if any, existing certication regimes encompass the combination of characteristics recommended in this report: namely, explicit dependability claims, evidence for those claims, and a demand for expertise sufcient to construct a rigorous argument that demonstrates that the evidence is sufcient to establish the validity of the claims. on the one hand, in the domain of avionics software, the certication process (and the culture that surrounds it) is held in high regard and is credited by many for an excellent safety record, with software implicated in only a handful of incidents. on the other hand, in the domain of software security, certication has been a dismal failure: new security vulnerabilities appear daily, and certication schemes are regarded by developers as burdensome and ineffective. security certicationsecurity certication standards for software were developed initially in response to the needs of the military for multilevelsecure products that could protect classied information from disclosure. concern for security in computing is now universal. the most widely recognized security certication standard is the cc. in short, since cc is demanded by some government agencies, it is widely applied; however better criteria would make it more effective and less burdensome.like its predecessors, the cc is a process in which independent governmentaccredited evaluators conduct technical analyses of the security properties ofštypicallyšcommercial offtheshelf (cots) it products and then certify the presence and quality of those properties. the cc model allows end users or government agencies to write a protection prole that species attributes of the security features of a product (such as the granularity of access controls and the level of detail captured in audit 52 cc was nalized in the late 1990s by the national governments that are signatories to the common criteria mutual recognition agreement. it succeeds the u.s. trusted computer systems evaluation criteria (tcsec, or orange book) and the european it security evaluation criteria. the description in the next section is simplied but fundamentally accurate. it is based on presentations to the committee and on discussions of cc at the common criteria users™ forum in washington, d.c., october 2004.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.assessment 31logs) and the level of security assurance of a product, as determined by the quality of its design and implementation.the cc characterizes assurance at one of seven levels, referred to as evaluation assurance levels or eal 1 (the lowest) through 7 (the highest). each higher eal requires more structured design documentation (and presumably more structured design), more detailed documentation, more extensive testing, and better control over the development environment. at the three highest levels of assurance (eals 57), formal specication of system requirements, design, or implementation is mandated.with a handful of exceptions,53 cots products complete evaluations only at the lowest four levels of assurance (eals 14). commercial vendors of widely used software have not committed either to the use of formal methods or to the extensively documented design processes that the higher levels of the cc require. typical vendor practice for completing evaluation is to hire a specialized contractor who reviews whatever documentation the vendor™s process has produced as well as the product source code and then produces the documentation and associated tests that the cc requires. the vendor often has the option of excluding problematic features (and code) from the ﬁevaluated conguration.ﬂ a separate contractor team of evaluators (often another department of the company that produces the evidence) then reads the documentation and reviews the test plans and test results. at eals 14, the assurance levels applied to cots products, the evaluators may conduct a penetration test to search for obvious vulnerabilities or at the enhanced basic level for other ˚aws (both criteria as dened in the cc documents).54 if the evaluators nd that all is in order, they recommend that the responsible government agency grant cc certication to the product as congured. in the united states, the national security agency employs validators who are government employees or consultants with no con˚icts of interest to check the work of the evaluators. because the cc certication process focuses on documentation designed to meet the needs of the evaluators, it is possible for a product to complete cc evaluation even though the evaluators do not have a deep understanding of how the product functions. and because the certication process at economically feasible evaluation levels focuses on the functioning of the product™s security features even while real vulnerabilities can occur in any component or interface, realworld vulnerability 53 the smartcard industry has embraced higher levels of evaluation, and many smartcard products have completed evaluation at eal 5. of more than 400 evaluated products other than smartcards listed at <http://www.commoncriteriaportal.org>, only 7 have completed evaluation at eal 5 or higher.54 see cc evaluation methodology manuals versions 2.3 and 3.1, respectively.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.32 software for dependable systemsdata show that products that have undergone evaluation fare no better (and sometimes worse) than products that have not.55 while the cc evaluation of security features gives users some condence that the features are appropriate and consistent, most users would logically assume that a product that had completed evaluation would have fewer vulnerabilities (cases in which an attacker could defeat the product™s security) than a product that had not been evaluated. sadly, there is no evidence that this is the case.cc evaluation does not necessarily correlate with the observed rate of vulnerability, as the following examples illustrate. the rst example, which a member of the committee participated in at microsoft, considers the relative effectiveness of cc evaluation and other measures in reducing security vulnerabilities in two microsoft operating system versions. microsoft™s windows 2000 was evaluated at the highest evaluation level usually sought by commercial products (eal 4), a process that cost many millions of dollars and went on for roughly 3 years after windows 2000 had been released to customers. however, windows 2000, as elded, experienced a large number of security vulnerabilities both before and after the evaluation was completed. a subsequent windows version, windows server 2003, was subject to an additional series of pragmatic steps such as threat modeling and application of static analysis tools during its development. these steps proved effective, with the result that the (thenunevaluated) windows server 2003 experienced about half the rate of critical vulnerabilities in the eld as its ccevaluated predecessor.56 some 18 months later, a cc evaluation against the same set of requirements as for windows 2000 was completed for windows server 2003. the evaluation was useful insofar as it demonstrated the operating system contained a relatively complete set of security features, however, microsoft™s assessment was that the vulnerability rate of windows server 2003 was better than that of windows 2000 because of a reduced incidence of errors at the coding level, a level well below the level at which it is scrutinized by the cc evaluation. another example is a recent comparison57 of the vulnerability rates of database products, which indicated that a product 55 see, for example, the national vulnerability database online at <http://nvd.nist.gov/> and a list of evaluated products at <http://www.commoncriteriaportal.org/public/consumer/index.php?menu=5>.56 for information on security vulnerabilities and xes, see the microsoft security bulletin web site at <http://www.microsoft.com/technet/security/current.aspx>.57 see david litcheld, 2006, ﬁwhich database is more secure? oracle vs. microsoft,ﬂ an ngs software insight security research (nisr) publication. available online at <http://www.databasesecurity.com/dbsec/comparison.pdf>. the national vulnerability database at <http://nvd.nist.gov/> also provides information on this topic. software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.assessment 33that had completed several cc evaluations actually experienced a higher vulnerability rate than one that had completed none.these data, and comparable data on other classes of products, demonstrate that completion of cc evaluations does not give users condence that evaluated products will show lower vulnerability rates than products that have not been evaluated. while evaluation against a suitable protection prole ensures completeness and consistency of security features, most users would expect (incorrectly) that cc evaluation is an indicator of better security, which they equate with fewer vulnerabilities.the problem with cc goes beyond the certication process itself. its fundamental assumption is that security certication should focus on security componentsšnamely, components that implement security features, such as access control. this is akin to evaluating the security of a building by checking the mechanisms of the door locks. software attackers, like common burglars, more often look for weaknesses in overall securityšfor example, for entry points that are not guarded. in computer security jargon, an evaluation should consider the entire attack surface of the system. the cc community is well aware of these problems and has discussed them at length. unfortunately, the newly released cc version 3 does not show any signicant change of direction.avionics certicationavionics systems are not certied directly but are evaluated as part of the aircraft as a whole. in the united states, when the regulations governing aircraft design were initially developed, avionics systems were implemented in hardware alone and did not incorporate software. the introduction of software into civilian aircraft beginning in the 1970s exposed inadequacies in the regulations relating to avionics: they could not be readily applied to softwarebased systems. in 1980, a special committee (sc145) of the radio technical commission for aeronautics (rtca) was created to develop guidelines for evaluating software used on aircraft. it was composed of representatives of aircraft manufacturers and avionics manufacturers, members of the academic community, aircraft customers, and certiers. the committee released its report, software considerations in airborne systems and equipment certication (rtca do178), in 1982. the document was subsequently revised, and the present 1992 version, do178b, eventually became the de facto standard worldwide for software in civilian aircraft. in europe, it is known as ed12b and is published by the european equivalent of rtca, the european organisation for civil aviation equipment (eurocae).5858 more information on the work of eurocae is available online at <http://www.eurocae.org/>.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.34 software for dependable systemsdo178b classies software using the faa™s ve failure levels to characterize the impact of that particular software™s failure on an aircraft59šranging from level a (catastrophic) to level e (no effect on the operational capability of an aircraft)šand prescribes more stringent criteria at higher levels. do178b tends to focus more on eliminating defects than on preventing their introduction in the rst place. the desire to make do178b widely acceptable also made it imprecise, and evaluations have yielded very different results when conducted by different organizations or government agencies. for example, there are very few detailed requirements for standards and checklists contained within do178b. where one evaluator may be satised to check against a set of criteria in a checklist or standard, another may document numerous deciencies based on his or her own experience, and do178b cannot be used to adjudicate between the two different results.60 a dearth of skilled personnel with a stable body of knowledge and capable of delivering consistent interpretations has exacerbated the situation. evaluators were typically drawn from industry, but despite having good practical experience, they rarely had any formal qualications in software engineering. to reduce variability, additional explanatory guidance and procedures were developed, and certiers were given special training. these steps have led to a more prescriptive approach and have resulted in better standardization.at least in comparison with other domains (such as medical devices), avionics software appears to have fared well inasmuch as major losses of life and severe injuries have been avoided. however, this is not in itself evidence that any or all of the processes prescribed by the do178b standard are necessary or cost effective. to give one example, do178b lays down criteria for structural coverage of the source code during testing depending on the criticality of the component. the unstated purpose is to establish that requirementsbased testing has ensured that all source code has been completely exercised with a rigor commensurate with the hazard associated with the software. without considerable negotiation, no other approaches are allowable. however, in one published study, detailed analysis and comparison of systems that had been certied to levels a or b of do178b showed that there was no discernible differ59 adapted from jim alvesfoss, bob rinker, and carol taylor, undated, ﬁmerging safety and assurance: the process of dual certication for faa and the common criteria.ﬂ available online at <http://www.csds.uidaho.edu/comparison/slides.pdf>.60 this was documented in the following nasa report: k.j. hayhurst, c.a. dorsey, j.c. knight, n.g. leveson, and g.f. mccormick, 1999, ﬁstreamlining software aspects of certication: report on the ssac survey.ﬂ nas/tm1999209519, august, section 3, observations 1, 3, 4, 5 (p. 45). the report is available online at <http://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/199900703141999110914.pdf>, and an overview of the ssac process is available online at <http://shemesh.larc.nasa.gov/ssac/>.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.assessment 35ence between the two levels in the remaining level of anomalies in the software, and that these anomalies included many serious, safetyrelated defects.61 the main difference between level a (software that could lead to a catastrophic failure) and level b (software whose failure would at most be severely hazardous) is that level a calls for requirementsbased testing to be shown to provide mcdc coverage of the software. this suggests that mcdc test coverage (at least as carried out on the software examined in the lessons learned study mentioned above62) does not signicantly increase the probability of detecting any serious defects that remain in the software.medical software certicationmedical software, in contrast to avionics software, is generally not subject to uniform standards and certication. the food and drug administration (fda) evaluates new products in a variety of ways. some are subject to premarket approval (pma), which is ﬁbased on a determination by fda that the pma contains sufcient valid scientic evidence to assure that the device is safe and effective for its intended use(s).ﬂ63 other classes of products are subject to premarket notication, which requires manufacturers to demonstrate that the product is substantially equivalent to, or as safe and effective as, an existing product. the fda™s requirements for this procedure are minimal.64 they center on a collection of guidance documents that outline the kinds of activities expected and suggest consensus standards that might be adopted. the larger manufacturers often voluntarily adopt a standard such as the international electrotechnical commission™s (iec™s) 61508,65 a standard related to the functional safety 61 andy german and gavin mooney, 2001, ﬁair vehicle software static code analysisšlessons learnt,ﬂ proceedings of the ninth safetycritical systems symposium, felix redmill and tom anderson, eds., springerverlag, bristol, united kingdom.62 in the study cited above, few survey respondents found mcdc testing to be effectivešit rarely revealed errors according to 59 percent and never revealed them at all according to 12 percent. that survey (which had a 72 percent response rate) also found that 76 percent of respondents acknowledged inconsistency between approving authorities; only 7 percent said that the guidance provided was ample (with 33 percent deeming it insufcient and 55 percent barely sufcient); and 75 percent found the cost and time for mcdc to be substantial or nearly prohibitive. the committee is not aware of results suggesting signicant changes in the ensuing years. 63 see the fda™s ﬁdevice adviceﬂ on premarket approval. available online at <http://www.fda.gov/cdrh/devadvice/pma/>.64 the fda™s guidance on premarket notication is available online at <http://www.fda.gov/cdrh/devadvice/314.html>.65 for more information on iec 61508, see <http://www.iec.ch/zone/fsafety/pdfsafe/hld.pdf>; for information on isa s84.01, see <http://www.isa.org/>.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.36 software for dependable systemsof electrical/electronic/programmable electronicsafetyrelated systems, or its u.s. equivalent, isa s84.01. the certication process itself typically involves a limited evaluation of the manufacturer™s software process.the consensus standards contain a plethora of good advice and are mostly processbased, recommending a large collection of practices. they emphasize ﬁvericationﬂ repeatedly, but despite the safetycritical nature of many of the devices to which they are applied, they largely equate verication with testing (which, as explained elsewhere in this report, is usually insufcient for establishing high dependability) and envisage no role for analysis beyond traditional reviews. the fda™s guidance document,66 like the iec™s, has a lengthy section on testing techniques and discusses how the level of criticality should determine the level of testing. it recognizes the limitations of testing and suggests the use of other verication techniques to overcome these limitations, but it does not specify what these might be.opportunities for dependable softwareanalyses of the role of software in safetycritical systems often focus on their potential to cause harm. it is important to balance concern about the risks of more pervasive software with a recognition of the enormous value that software brings, not only by improving efciency but also by making systems safer. software can reduce the risk of a system failure by monitoring for warning signs and controlling interventions; it can improve the quality and timeliness of information provided to operators; and it can oversee the activities of errorprone humans. software can also enable a host of new applications, tools, and systems that can contribute to the health and wellbeing of the population.without better methods for developing dependable software, it may not be possible to build the systems we would like to build. when software is introduced into critical settings, the benets must obviously outweigh the risks, and without convincing evidence that the risk of catastrophic failure is sufciently low, society may be reluctant to eld the system whatever the benets may be. in the united states, the threat of litigation may raise the bar even higher, since failing to deploy a new system that improves safety is less likely to result in damage claims than deploying a system that causes injury.to illustrate these issues, we consider the same two domains: air transportation and medicine. 66 fda, 2002, general principles of software validation; final guidance for industry and fda staff, january 11. available online at <http://www.fda.gov/cdrh/comp/guidance/938.html>.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.assessment 37air transportationsoftware already plays a critical role in air transportation, most notably in onboard avionics and in airtrafc management. dependable software will be a linchpin of safe air transport in the coming decades, in two areas in particular.67 first, efforts to enhance aviation functionality, such as plans for (1) new avionics systems that incorporate fullauthority digital engine controllers (fadecs) to manage large engines and monitor their performance and (2) ˚ightdeck and groundbased automation to support free ˚ight, will rely heavily on software.second, there are efforts to improve aviation safety by employing automation in the detection and mitigation of accidents.68 the category of accident responsible for most fatalities involving commercial jetliners is ﬁcontrolled ˚ight into terrainﬂ (cfit), in which the pilot, usually during takeoff or landing, inadvertently ˚ies the aircraft into the ground. collisions between planes during ground operations, takeoff, and landing also merit attention; a runway incursion in the canary islands in 1977 resulted in one of the worst accidents in aviation history, with 583 fatalities. while such accidents are not common, they pose signicant risk.software can help prevent both kinds of accident, withšfor examplešground proximity warning systems and automatic alerts for runway incursions. software can also be used to defend against mechanical failures: the aircraft condition analysis and management system (acams) uses onboard components and groundbased information systems to diagnose weaknesses and communicate them to maintainers.medicinesoftware is crucial to the future of medicine. although computers are already widely used in hospitals and doctors™ ofces, the potential benets of it in patient management have been garnering increased attention of late. the ready availability of information and automated record keeping can have an impact on health care that goes far beyond efciency improvements. each year, an estimated 98,000 patients die from preventable medical errors.69 many of these deaths could be prevented by software. cpoe systems, for example, can dramatically reduce the rate 67 this section is based on information provided in john c. knight, 2002, ﬁsoftware challenges in aviation systems,ﬂ lecture notes in computer science 2434:106112. 68 see, for example, the nasa aviation safety program. available online at <http://www.aerospace.nasa.gov/programsavsp.htm>.69 institute of medicine, 2000, to err is human: building a safer health system, national academy press, washington, d.c. available online at <http://books.nap.edu/catalog.php?recordid=9728>.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.38 software for dependable systemsof medication errors by eliminating transcription errors. although media attention tends to focus on the more exciting and exotic applications of software, a wider and deeper deployment of existing it could have a profound effect on health care.70 computerization alone, however, is not sufcient; a highly dependable system with adequate levels of decision support is needed.71the ability of software to implement complex functionality that cannot be implemented at reasonable cost in hardware makes new kinds of medical devices possible, such as heart and brain implants and new surgical tools and procedures. an exciting example of the potential of software to improve medical treatment is imageguided surgery, in which images produced by less recent technologies such as mri can be synchronized with positioning data, allowing surgeons to see not only the physical surfaces of the area of surgery but also the internal structure revealed by prior imaging. a neurosurgeon removing a tumor aims to remove as much tumor material as possible without causing neurological damage; better tools allow less conservative but safer surgery. obviously, the software supporting such a tool is critical and must be extraordinarily dependable.observationsthis study raised a host of questions that have been asked many times before in the software engineering community and beyond but have still to be satisfactorily answered. how dependable is software today? is dependability getting better or worse? how many accidents can be attributed to software failures? which development methods are most costeffective in delivering dependable software? not surprisingly, this report does not answer these questions in full; answering any one of them comprehensively would require major research. nevertheless, in the course of investigating the current state of software development and formulating its approach, the committee made some observations that inform its recommendations and re˚ect on these questions.70 edward h. shortliffe, 2005, ﬁstrategic action in health information technology: why the obvious has taken so long,ﬂ health affairs 24(5):12221233.71 in one study of a hospital in utah, 52 percent of admitted patients suffered from adverse drug events (ades), of which 9 percent resulted in serious harm, despite the use of a cpoe system intended to prevent them (jonathan r. nebeker, jennifer m. hoffman, charlene r. weir, charles l. bennett, and john f. hurdle, 2005, ﬁhigh rates of adverse drug events in a highly computerized hospital,ﬂ archives of internal medicine 165:11111116).software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.assessment 39observation 1: lack of evidencestudies of this sort do not have the resources to perform their own data collection, so they rely instead on data collected, analyzed, and interpreted by others. early on in this study, it became clear that very little information was available for addressing the most fundamental questions about software dependability. incomplete and unreliable data about software failures and about the efcacy of different approaches to the development of software make objective scientic evaluation difcult if not impossible. when software fails, the failures leave no evidence of fractured spars or metal fatigue to guide accident investigators; execution of software rarely causes changes to the software itself. investigating the role of software in an accident needs a full understanding of the software design documents, the implementation, and the logs of system events recorded during execution, yet this expertise may be available only to the manufacturer, which may have a con˚ict of interest. failures in a complex system often involve fault propagation and complex interactions between hardware components, software components, and human operators. this makes it very difcult to precisely determine the impact of software on a system failure. complex interactions and tight coupling not only make a system less reliable but also make its failures harder to diagnose. there are a number of compendia of anecdotal failure reports, most notably those collected by the risks forum,72 which for many years has been gathering into a single archive a wide variety of reports of softwarerelated problems, mostly from the popular press. the accident databases maintained by federal agencies (for example, the national transportation safety board) include incidents in which software was implicated. but detailed analyses of software failures are few and far between, and those that have been made public are mostly the work of academics and researchers who based their analyses on secondary sources.the lack of systematic reporting of signicant software failures is a serious problem that hinders evaluation of the risks and costs of software failure and measurement of the effectiveness of new policies or interventions. in traditional engineering disciplines, the value of learning from failure is well understood,73 and one could argue that without this feedback loop, software engineering cannot properly claim to be an engineer72 see the risks digest, a forum on risks to the public in computers and related systems moderated by peter g. neumann. available online at <http://catless.ncl.ac.uk/risks>.73 see, for example, henry petroski, 2004, to engineer is human, st. martin™s press, new york; and matthys levy and mario salvadori, 1992, why buildings fall down, w.w. norton & company, new york.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.40 software for dependable systemsing discipline at all. of course, many companies track failures in their own software, but there is little attention paid by the eld as a whole to historic failures and what can be learned from them.this lack of evidence leads to a range of views within the broader community. the essential question is, if mechanisms for certifying software cannot be relied on, should the software be used or not? some believe that absent evidence for dependability and robust certication mechanisms, a great deal of cautionševen resistancešis warranted in deploying and using softwarebased systems, since there are risks that systems will be built that could have a catastrophic effect. others observe that systems are being built, that software is being deployed widely, and that deployment of robust systems could in fact save lives, and they argue that the risk of a catastrophic event is worth taking. from this perspective, effects should focus not so much on deciding what to build, but rather on providing the guidance that is urgently needed by practitioners and users of systems. accordingly, the lack of evidence has two direct consequences for this report. first, it has informed the key notions that evidence be at the core of dependable software development, that data collection efforts are needed, and that transparency and openness be encouraged so that those deploying software in critical applications are aware of the limits of evidence for its dependability and can make fully informed decisions about whether the benets of deployment outweigh the residual risks. second, it has tempered the committee™s desire to provide prescriptive guidancešthat is, the approach recommended by the committee is largely free of endorsements or criticisms of particular development approaches, tools, or techniques. moreover, the report leaves to the developers and procurers of individual systems the question of what level of dependability is appropriate, and what costs are worth incurring in order to obtain it.observation 2: not just bugssoftware, according to a popular view, fails because of bugs: errors in the code that cause the software to fail to meet its specication. in fact, only a tiny proportion of failures due to the mistakes of software developers can be attributed to bugsš3 percent in one study that focused on fatal accidents.74 as is well known to software engineers (but not to the general public), by far the largest class of problems arises from errors made in the eliciting, recording, and analysis of requirements. a second large class of problems arises from poor human factors design. the two classes are 74 donald mackenzie, 2001, mechanizing proof: computing, risk, and trust, mit press, cambridge, mass., chapter 9.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.assessment 41related; bad user interfaces usually re˚ect an inadequate understanding of the user™s domain and the absence of a coherent and wellarticulated conceptual model.security vulnerabilities are to some extent an exception; the overwhelming majority of security vulnerabilities reported in software  productsšand exploited to attack the users of such productsšare at the implementation level. the prevalence of coderelated problems, however, is a direct consequence of higherlevel decisions to use programming languages, design methods, and libraries that admit these problems. in principle, it is relatively easy to prevent implementationlevel attacks but hard to retrot existing programs.one insidious consequence of the focus on coding errors is that developers may be absolved from blame for other kinds of errors. in particular, inadequate specications, misconceptions about requirements, and serious usability ˚aws are often overlooked, and users are unfairly blamed. the therapists who operated the radiotherapy system that failed in panama, for example, were blamed for entering data incorrectly, even though the system had an egregious design ˚aw that permitted the entry of invalid data without generating a warning, and they were later tried in court for criminal negligence.75 in several avionics incidents, pilots were blamed for issuing incorrect commands, even though researchers recognized that the systems themselves were to blame for creating ﬁmode confusion.ﬂ76understanding software failures demands a systems perspective, in which the software is viewed as one component of many, working in concert with other componentsšbe they physical devices, human operators, or other computer systemsšto achieve the desired effect. such a perspective underlies the approach recommended in chapter 3.observation 3: the cost of strong approachesin the last 20 years, new techniques have become available in which software can be specied and designed using precise notations and subsequently subjected to mechanized analysis. these techniques, often referred to as ﬁformal methods,ﬂ are believed by many to incur unreasonable costs. while it may be true that formal methods are not economical when only the lowest levels of dependability are required, there is some evidence that as dependability demands increase, an approach that includes formal specication and analysis becomes the more costeffective 75 see deborah gage and john mccormick, 2004, ﬁwe did nothing wrong,ﬂ baseline, march 4. available online at <http://www.baselinemag.com/article2/0,1540,1543571,00.asp>.76 see nasa, 2001, ﬁfm program: analysis of mode confusion.ﬂ available online at <http://shemesh.larc.nasa.gov/fm/fmnowmodeconfusion.html>.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.42 software for dependable systemsoption. this section presents some data in support of this claim and gives a simple economic analysis showing how the choice between a traditional approach and a strong approach (one that incorporates formal methods) might be made.traditional software development approaches use specication and design notations that do not support rigorous analysis, as well as programming languages that are not fully dened or that defeat automated analysis. traditional approaches depend on human inspection and testing for validation and verication. strong approaches also use testing but employ notations and languages that are amenable to rigorous analysis, and they exploit mechanical tools for reasoning about properties of requirements, specications, designs, and code.traditional approaches are generally less costly than strong methods for obtaining low levels of dependability, and for this reason many practitioners believe that strong methods are not costeffective. the costs of traditional approaches, however, can increase exponentially with increasing levels of dependability. the cost of strong approaches increases more slowly with increasing dependability, meaning that at some level of dependability strong methods can be more costeffective.77 whether software rms and developers will use traditional or strong approaches depends, in part, on consumer demand for dependability. the following exercise discusses the consumerdemanddependent conditions under which rms and developers would choose either the traditional or the strong approach and the conditions under which it would be sensible, from an economics and engineering perspective, to switch back to the traditional approach.77 peter amey, 2002, ﬁcorrectness by construction: better can also be cheaper,ﬂ crosstalk magazine, the journal of defence software engineering, march. available online at <http://www.praxishis.com/pdfs/cbycbettercheaper.pdf>. this paper describes the savings that are repeatedly made by projects that use strong software engineering methods. on p. 27, amey askshow . . . did spark help lockheed reduce its formal faa test costs by 80 percent? the savings arose from avoiding testing repetition by eliminating most errors before testing even began. . . . most highintegrity and safetycritical developments make use of language subsets. unfortunately, these subsets are usually informally designed and consist, in practice, of simply leaving out parts of the language thought to be likely to cause problems. although this shortens the length of rope with which the programmers may hang themselves, it does not bring about any qualitative shift in what is possible. the use of coherent subsets free from ambiguities and insecurities does bring such a shift. crucially it allows analysis to be performed on source code before the expensive test phase is entered. this analysis is both more effective and cheaper than manual methods such as inspections. inspections should still take place but can focus on more protable things like ﬁdoes this code meet its specicationﬂ rather than ﬁis there a possible data˚ow error.ﬂ eliminating all these ﬁnoiseﬂ errors at the engineer™s terminal greatly improves the efciency of the test process because the testing can focus on showing that requirements have been met rather than becoming a ﬁbug hunt.ﬂ software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.assessment 43consumers have some willingness to pay for dependability. like any other good, the more costly dependability is, the less of it consumers, who have limited resources, will purchase. figure 1.1 shows this downwardsloping demand (d0) for dependability: at low prices for dependability, consumers will purchase a lot of it; at high prices, they will purchase less. it is costly, meanwhile, for suppliers to increase dependability. the marginal cost of supplying different levels of dependability using traditional approaches is depicted by the line labeled ﬁmctraditional.ﬂ with perfect competition, the market will reach an equilibrium in which rms supply dependability, dept0, at the price pt0.next, consider the introduction of strong software engineering approaches (figure 1.2). consumers still have the same willingness to pay for dependability, but the costs of supplying any given amount of it now depend on whether the rm uses traditional approaches or strong engineering approaches, with the cost structure of the latter depicted in the gure by the curve labeled ﬁmcstrong.ﬂd0mctraditionaldependabilitypricept0dept0d0traditionalpt0t011figure 1.1 equilibrium price and dependability with perfect competition and traditional software approaches.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.44 software for dependable systemsd0mctraditionaldependabilitypricept0dept0mcstrongps0deps0d0npt0t0mcstrongps012figure 1.2 lower equilibrium price and higher dependability with strong engineering approaches.consumers have the same demand prole for dependability as they had before, but the curve intersects the strong software cost prole at a different point, yielding a new equilibrium at higher dependability (deps0) and lower price (ps0).78 it is a new equilibrium because, in a perfectly competitive market, rms that continue to use traditional approaches would be driven out of business by rms using strong approaches.lower prices and higher dependability are not necessarily the new equilibrium point. the new equilibrium depends crucially on the slopes and location of the demand and cost curves. for some goods, consumers might not be willing to pay as much for a given level of dependability as they might for other goods. figure 1.3 depicts this demand prole as d1. in this scenario, rms will continue to use traditional approaches, with the equilibrium dept1 at a price of pt1. no rational rm would switch to strong approaches if consumer demand did not justify doing so.78 it is assumed here that the costs of switching to the new programming methods are incorporated into the mcstrong curve.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.assessment 45observation 4: coupling and complexityin normal accidents,79 perrow outlines two characteristics of systems that induce failures: interactive complexity, where components may interact in unanticipated ways, perhaps because of failures or just because no designer anticipated the interactions that could occur; and tight coupling, wherein a failure cannot be isolated but brings about other failures that cascade through the system. systems heavy with software tend to have both attributes. the software may operate as designed, and the component it interfaces with may be performing within specications, but the software design did not anticipate unusual, but still permissible, values in the component. (in one incident, avionics software sensed the pilot was performing a touchandgo maneuver; this was because the wet tarmac did not allow the wheels to turn, so they skidded. the pilot was trying to land but the control assumed otherwise and would not let him deceler79 charles perrow, 1999, normal accidents, princeton university press, princeton, n.j.d0mctraditionaldependabilitymcstrongd1dept1pt1ps1deps1d0mctraditionalpricemcstrongd1dept1pt1ps1deps113figure 1.3 consumer demand for dependability is decreased; there is no switch to strong approaches in equilibrium.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.46 software for dependable systemsate.80) or, the component may be used in a way not anticipated by the software specications, or a newer model of the component is introduced without realizing how the software might affect it. (both were true in the case in the ariane 5 rocket failure. it was destroyed by the over˚ow of a horizontal velocity variable in a reused ariane 4 component that was to perform a function not even required by ariane 5.81) complicated software programs interact with other complicated software programs, so many unexpected interactions can occur. trying to nd a single point of failure is often fruitless. the interactive character of software and the components it interfaces with is, quite literally, tightly coupled, so faulty interactions can easily disturb the components linked to it, cascading the disturbance. modularity reduces this tendency and reduces complexity. redundant paths increase reliability; while they increase the number of components and the amount of software, this does not necessarily increase the interactive complexity and certainly not the coupling. the problem of coupling and complexity is exacerbated by the drive for efciency that underlies modern management techniques. it is common to use software systems in an attempt to increase an organization™s efciency by eliminating redundancy and shaving margins. in such circumstances, systems can tend to be drawn inexorably toward the dangerous combination of high complexity and high coupling. cook and rasmussen explain this phenomenon and illustrate its dangers in the context of patient care.82 in one incident they describe, for example, a hospital allowed surgeries to begin on patients expected to need intensive care afterwards on the assumption that space in the intensive care unit would become available; when it did not, the surgery had to be terminated abruptly. in another incident, when a computer upgrade was introduced, the automated drug delivery program of a large hospital was disrupted for more than 2 days, neccesitating the manual rewriting of drug orders for all patients. all backup tapes of medication orders were corrupted ﬁbecause of a complex interlocking process related to the database management software that was used by the pharmacy application. under particular circumstances, tape backups could be incomplete in ways that 80 see main commission accident investigationšpoland, 1994, ﬁreport on the accident to airbus a320211 aircraft in warsaw on 14 september 1993.ﬂ available online at <http://sunnyday.mit.edu/accidents/warsawreport.html>.81 see j.l. lions, 1996, ﬁariane 5: flight 501 failure,ﬂ report by the inquiry board. available online at <http://www.ima.umn.edu/~arnold/disasters/ariane5rep.html>.82 r. cook and j. rasmussen, 2005, ﬁgoing solid: a model of system dynamics and consequences for patient safety,ﬂ quality and safety in health care 14(2):130134.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.assessment 47remained hidden from the operator.ﬂ83 there was no harm to patients, but the disruption and effort required to mitigate it were enormous.it is also well known that the operator interfaces to complex software systems are often so poorly designed that they invite operator error.84 however, there is a more insidious danger that derives from a lack of condence in systems assurancešnamely, systems that might best be largely autonomous are instead dubbed ﬁadvisoryﬂ and placed under human supervision. for a human to monitor an automated system, the automation must generally expose elements of its internal state and operation; these are seldom designed to support an effective mental model, so the human may be left out of the loop and unable to perform effectively.85 such problems occur frequently in systems that operate in different modes, where the operator has to understand which mode the system is in to know its properties. mode confusion contributing to an error is exemplied by the fatal crashes of two airbus 320sšone in warsaw in 1993 and one near bangalore in 1990.86 systems thinking invites consideration of such combinations (sometimes called ﬁmixed initiative systemsﬂ) in which the operator is viewed as a component and the overall system design takes adequate account of human cognitive functions.these concerns do not necessarily militate against the use of software, but they do suggest that careful attention should be paid to the risks of interactive complexity and tight coupling and the advantages of modularity, buffering, and redundancy; that interdependences among components of critical software systems should be analyzed to ensure that modes of 83 richard cook and michael o™connor, forthcoming, ﬁthinking about accidents and systems,ﬂ in improving medication safety, k. thompson and h. manasse, eds., american society of healthsystem pharmacists, washington, d.c.84 see, for example, ross koppel, joshua p. metlay, abigail cohen, brian abaluck, a. russell localio, stephen e. kimmel, and brian l. strom, 2005, ﬁrole of computerized physician order entry systems in facilitating medication errors,ﬂ journal of the american medical association 293(10):11971203.85 one comprehensive study of this phenomenon is p.j. smith, e. mccoy, and c. layton, 1997, ﬁbrittleness in the design of cooperative problemsolving systems: the effects on user performance,ﬂ ieee transactions on systems, man and cybernetics 27:360371. see also c. layton, p.j. smith, and c.e. mccoy, 1994, ﬁdesign of a cooperative problemsolving system for enroute ˚ight planning: an empirical evaluation,ﬂ human factors 36:94119. for an overview of this and related work see d.d. woods and e. hollnagel, 2006, joint cognitive systems: patterns in cognitive systems engineering, taylor & francis, boca raton, fla.86 the report in flight international (may 28, 1990) on the bangalore crash makes very interesting reading. the account of the number of ˚ight modes which the a320 went through in the 2 minutes before the crash and the side effects of each (which seem not to have been understood properly by the pilots) makes operating an a320 appear very different from ˚ying a fully manual airplane. the secondary effects (such as selecting a target altitude that causes the engines to be retarded to idle, and needing several seconds to develop full power again) need to be well understood by the pilots.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.48 software for dependable systemsfailure are well understood; and that failures are localized to the greatest extent possible. developers and procurers of software systems should also keep in mind that there are likely to be tradeoffs of various sorts between the goals of efciency and safety and that achieving appropriate safety margins may exact a cost in reduced efciency and perhaps also in reduced functionality and automation. at the same time, the clarication and simplication that meeting most safety requirements demands may also improve efciency. recent work on engineering resilience suggests ways to dynamically manage the tradeoff and ways to think about when to sacrice efciency for safety.87 observation 5: safety culture mattersthe efcacy of a certication regimen or development process does not necessarily result directly from the technical properties of its constituent practices. the de facto avionics standard, do178b, for example, although it contains much good advice, imposes (as explained above) some elaborate procedures that may not have a direct benecial effect on dependability. and yet avionics software has an excellent record with remarkably few failures, which many in the eld credit to the adoption of do178b.one possible explanation is that the strictures of the standard and the domain in which system engineers and developers are working have collateral effects on the larger cultural framework in which software is developed beyond their immediate technical effects. the developers of avionics software are confronted with the fact that many lives depend directly on the software they are constructing and they pay meticulous attention to detail. a culture tends to evolve that leads developers to act cautiously, to not rely on intuition, and to value the critiques of others. richard feynman, in his analysis of the challenger disaster,88 commented on similar attitudes among software engineers at nasa:the software is checked very carefully in a bottomup fashion. . . . but completely independently there is an independent verication group, that takes an adversary attitude to the software development group, and 87 see, for example, d.d. woods, 2006, ﬁessential characteristics of resilience for organizations,ﬂ in resilience engineering: concepts and precepts, e. hollnagel, d.d. woods, and n. leveson, eds., ashgate, aldershot, united kingdom; d.d. woods, 2005, ﬁcreating foresight: lessons for resilience from columbia,ﬂ in organization at the limit: nasa and the columbia disaster, w.h. starbuck and m. farjoun, eds., blackwell, malden, mass.88 richard p. feynman, 1986, ﬁappendix fšpersonal observations on the reliability of the shuttle,ﬂ in report of the presidential commission on the space shuttle challenger accident, june. available online at <http://science.ksc.nasa.gov/shuttle/missions/51l/docs/rogerscommission/appendixf.txt>.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.assessment 49tests and veries the software as if it were a customer of the delivered product. . . . a discovery of an error during verication testing is considered very serious, and its origin studied very carefully to avoid such mistakes in the future.to summarize then, the computer software checking system and attitude is of the highest quality. there appears to be no process of gradually fooling oneself while degrading standards so characteristic of the solid rocket booster or space shuttle main engine safety systems.an organizational culture that encourages and supports such attitudes is called a ﬁsafety culture,ﬂ and it is widely recognized as an essential ingredient in the engineering of critical systems. at the same time, it is important to recognize that a strong safety culture, while necessary, is not sufcient. as feynman noted in the same analysis: ﬁone might add that the elaborate system could be very much improved by more modern hardware and programming techniques.ﬂ a safety culture and the processes that support it need to be accompanied by the best technical practices in order to achieve desired dependability.89 establishing a good safety culture is not an easy matter and requires a sustained effort. the task is easier in the context of organizations that already have strong safety cultures in their engineering divisions and in industries that have organizational commitments to safety (and pressure from consumers to deliver safe products).90 the airline industry is a good example. the large companies that produce avionics software have a long history of engineering largescale critical systems. there is a rich assemblage of organizations and institutions with an interest in safety; accidents are vigorously investigated; standards are strict; liabilities established; and its customers are in˚uential and resourceful. in his book on accident 89 the safety culture alone may prevent the deployment of dangerous systems, but it may exact an unreasonably high cost. nasa™s avionics software for the space shuttle, for example, is estimated to have cost roughly $1,000 per line of code (dennis jenkins, ﬁadvanced vehicle automation and computers aboard the shuttle.ﬂ available online at <http://history.nasa.gov/sts25th/pages/computer.html>, updated april 5, 2001). using appropriate tools and techniques can help reduce cost (see previous discussion of the cost of strong approaches). studies of some systems developed by praxis, for example, show that software was obtained with defect rates comparable to the software produced by the most exacting standards, but at costs not signicantly higher than for conventional developments (anthony hall, 1996, ﬁusing formal methods to develop an atc information system,ﬂ ieee software 13(2):6676). it is not clear how widely these results could be replicated, but it is clear that conventional methods based on testing and manual review become prohibitively expensive when very high dependability is required.90 for a comprehensive discussion of the role of safety culture in a variety of industries, see charles perrow, 1999, normal accidents, princeton university press, princeton, n.j.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.50 software for dependable systemsinvestigation,91 chris johnson lists a dozen public and nonprot organizations concerned with software reliability in the industry (and notes the lack of incident reporting even there). a strong safety culture has not been as widespread in some other domains. standards and certication regimes can play a major role in establishing and strengthening safety cultures within companies. the processes they mandate contribute directly to the safety culture, but there are important indirect in˚uences also. they raise the standards of professionalism, the abilities they demand leads to the weeding out of lessskilled engineers, and they call for a seriousness of purpose (and a willingness to perform some laborious work whose benet may not be immediately apparent). the need to conform to a standard or obtain certication imposes unavoidable costs on a development organization. one engineer interviewed by the committee explained that in his department (in a large u.s. computer company), the fact that managers were forced to spend money on safety made them more open and willing to consider better practices in general and somewhat counterbalanced the tendency to focus on expanding the feature set of a product and hurrying the product to market.91 c.w. johnson, 2003, failure in safetycritical systems: a handbook of accident and incident reporting, university of glasgow press, glasgow, scotland. available online at <http://www.dcs.gla.ac.uk/~johnson/book/>.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.2 proposed approachthis chapter is the core of the report. it describes an approach to the development of dependable software that the committee believes could be widely adopted, and would be more effective than the approaches that are currently in widespread use.the proposed approach can be summarized in three key pointsšﬁthe three esﬂ:ł explicit claims. no system can be dependable in all respects and under all conditions. so to be useful, a claim of dependability must be explicit. it must articulate precisely the properties the system is expected to exhibit and the assumptions about the system™s environment on which the claim is contingent. the claim should also make explicit the level of dependability claimed, preferably in quantitative terms. different properties may be assured to different levels of dependability.ł evidence. for a system to be regarded as dependable, concrete evidence must be present that substantiates the dependability claim. this evidence will take the form of a ﬁdependability case,ﬂ arguing that the required properties follow from the combination of the properties of the system itself (that is, the implementation) and the environmental assumptions. so that independent parties can evaluate it, the dependability case must be perspicuous and wellstructured; as a rule of thumb, the cost of reviewing the case should be at least an order of magnitude less than the cost of constructing it. because testing alone is usually insufcient to establish properties, the case will typically combine evidence from testing 51software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.52 software for dependable systemswith evidence from analysis. in addition, the case will inevitably involve appeals to the process by which the software was developedšfor example, to argue that the software deployed in the eld is the same software that was subjected to analysis or testing.ł expertise. expertisešin software development, in the domain under consideration, and in the broader systems context, among other thingsšis necessary to achieve dependable systems. flexibility is an important advantage of the proposed approach; in particular the developer is not required to follow any particular process or use any particular method or technology. this ˚exibility provides experts the freedom to employ new techniques and to tailor the approach to their application and domain. however, the requirement to produce evidence is extremely demanding and likely to stretch today™s best practices to their limit. it will therefore be essential that the developers are familiar with best practices and diverge from them only with good reason. expertise and skill will be needed to effectively utilize the ˚exibility the approach provides and discern which best practices are appropriate for the system under consideration and how to apply them. this chapter contains a short catalog of best practices, judged by the committee to be those that are most important for dependability.these notionsšto be explicit, to demand and produce evidence, and to marshall expertisešare, in one sense, entirely traditional and uncontroversial. modern engineering of physical artifacts marshals evidence for product quality by measuring items against explicit criteria, and licensing is often required in an attempt to ensure expertise. applying these notions to software, however, is not straightforward, and many of the assumptions that underlie statistical process control (which has governed the design of production lines since the 1920s) do not hold for software. some of the ways in software systems differ from more traditional engineering projects include the following: ł criteria. the criteria for physical artifacts are often simpler, often comprising no more than a failure or breakage rate for the artifact as a whole. because of the complexity of software and its interdependence on the environment in which it operates, explicit and precise articulation of claims is both more challenging and more important than for traditional engineering.ł feasibility of testing. for physical artifacts, limited testing provides compelling evidence of quality, with the continuity of physical phenomena allowing widespread inferences to be drawn from only a few sample points. in contrast, limited testing of software can rarely provide compelling evidence of behavior under all conditions.ł process/product correlation. the fundamental premise of statistical software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.proposed approach 53quality control is that sampling the product coming out of a process gives a measure of the quality of the process itself, which in turn will determine the quality of items that are not sampled. although better software process can lead to better software, the correlation is not sufciently strong to provide evidence of dependability. unlike physical engineering, in which large classes of identical artifacts are produced, software engineering rarely produces the same artifact twice, so evidence about one software system rarely bears on another such system. and even an organization with the very best process can produce seriously ˚awed software.these differences have profound implications, so that the application of standard engineering principles to software results in an approach that is far from traditional. practitioners are likely to nd the proposed approach radical in three respects. first, the articulation of explicit dependability claims suggests that software systems requirements should be structured differently, with requirements being prioritized (separating the crucial dependability properties from other desirable, but less crucial, ones) and environmental assumptions being elevated to greater prominence. second, the standard of evidence for a system that must meet a high level of dependability cannot generally be achieved using the kind of testing regimen that is accepted by many certication schemes today. instead, it will be necessary to show an explicit connection between the tests performed and the properties claimed; the inevitable gap will likely have to be lled by analysis. third, constructing the dependability case after the implementation is complete will not usually be feasible. instead, considerations of the ease of constructing the case will permeate the development, in˚uencing the choice of features, the architecture, the implementation language, and so on, and the need to preserve the chain of evidence will call for a rigorous process. achieving all of this will demand signicant and broadranging expertise. lest the reader be concerned that the proposed approach is too risky, it should be noted that although widespread adoption of the proposed approach would be a radical change for the software industry, the constituent practices that the approach would require are far from novel and have been used successfully in complex, critical software projects for over a decade. moreover, the underlying sensibility of the approach is consistent with the attitude advocated by the eld of systems engineeringšoften referred to as ﬁsystems thinkingﬂšwhose validity is widely accepted and repeatedly reafrmed by accidents and failures that occur when it is not applied. because the proposed approach is very different from the approach used to build most software today, it will not only require a change in mindset but will also probably demand skills that are in short supply. a radical improvement in software will therefore depend on improvements software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.54 software for dependable systemsin education (e.g., better curricula). furthermore, the high standards imposed by this approach may not always be achievable at reasonable cost. in some cases, this will mean reducing expectations of dependabilityšin other words, limiting the functionality and complexity of the system. if no acceptable tradeoff can be agreed upon, it may not be possible to build the system at all using today™s technology. without major advances brought about by fundamental research, many software systems that society will want or need in the coming decade will probably be impossible to build to appropriate dependability standards. the approach advocated here is technologyneutral, so as technology advances, more effective and economical means of achieving society™s goals will become possible, and systems that cannot be built today may be feasible in the future.explicit dependability claimswhat is dependability?until now, this report has relied on the reader™s informal understanding of the term ﬁdependable.ﬂ this section claries the way in which the term is used in the context of this report.the list of adjectives describing the demands placed on software has grown steadily. software must be reliable and available; usable and ˚exible; maintainable and adaptable; and so on. it would not be helpful simply to add ﬁdependableﬂ to this long list, with the meaning that a ﬁdependableﬂ software system is one on which the user can depend. one could imagine, though, that demanding dependability in this broad sense from a software system is not unreasonable. after all, do not users of all kinds of nonsoftware systems demand, and obtain, dependability from them? since the late 1970s, for example, drivers have come to expect allround dependability from their cars. but large software systems are more complex than most other engineered systems, and while it might make sense to demand dependability from a car in its entirety, it makes less sense to demand the same of a large software system. it is clear what services are expected of a car: if the car fails in deep water, for example, few drivers would think to point to that as a lack of dependability.1 most 1 incidentally, the increasing complexity of automobile electronic systems means that accidental systems may form and the dependability problems experienced in complex software systems may appear in automobiles. a recent example was the discovery of a ﬁsneak circuitﬂ: if the radio was switched on and the brake pedal was depressed at the same time as a rear window was being operated, the air bags deployed. fortunately, this was detected by simulation tools examining the electronic design, and no vehicles had to be recalled. reported by committee member martyn thomas. software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.proposed approach 55large software systems, in contrast, perform a large range of complex functions in a complex and changing environment. users are not typically aware of a system™s inherent limitations, nor can they always even detect changes in the environment that might compromise the system™s reliability. for these reasons, the dependability of a software system cannot be judged by a simple metric. a system is dependable only with respect to particular claimed properties; unless these properties are made explicit, dependability has little meaning. moreover, dependability is not a local property of software that can be determined module by module but has to be articulated and evaluated from a systems perspective that takes into account the context of usage. a system may be dependable even though some of its functions fail repeatedly; conversely, it may be regarded as undependable if it causes unexpected effects in its environment, even if it suffers no obvious failures. these issues are discussed in more detail below (see ﬁsoftware as a system componentﬂ).in addition, dependability does not reside solely within a system but is also re˚ected in the degree of trust that its users are willing to place in it. systems may meet all of their dependability requirements, but if users cannot be convinced that this is so, the systems will not be seen as dependable. that is, dependability is an ﬁability to deliver service that can justiably be trusted,ﬂ2 and for such justication, evidence will be required.why claims must be explicitwith limitless resources, it might be possible to build a system that is highly dependable in all of its properties, but in practicešfor systems of even minimal complexityšthis will not be achievable at a reasonable cost. a key characteristic of a system designed with dependability in mind will therefore be differentiationšthat is, the properties of the system will not be uniform in the condence they warrant but, on the contrary, will be assured to (possibly dramatically) differing degrees of condence.it follows that the users of a system can depend on it only if they know which properties can be relied upon. in other words, the crucial properties should be explicitly articulated and made clear not only to the user, as consumer of the system, but also to the developer, as its producer. currently, consumer software is typically sold with few explicit representations of the properties it offers or its tness for any purpose. apple and adobe, for example, provide software ﬁas isﬂ and ﬁwith all faultsﬂ and 2 a. avizienis, j.c. laprie, b. randell, and c. landwehr, 2004, ﬁbasic concepts and taxonomy of dependable and secure computing,ﬂ ieee transactions on dependable and secure computing 1(1):1133. software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.56 software for dependable systemsdisclaim all warranties. google offers ﬁno warranties whatsoeverﬂ for its services, and microsoft warrants only that the software will ﬁperform substantially in accordance with the accompanying materials for a period of (ninety) 90 days.ﬂ3 software systems that are developed specially for a particular client are typically built to meet preagreed requirements, but these requirements are often a long and undifferentiated list of detailed functions.software as a system componentengineering elds with long experience in building complex systems (for example, aerospace, chemicals, and nuclear engineering) have developed approaches based on systems thinking; these approaches focus on the properties of the system as a whole and on the interactions among its components, especially those (often neglected) between a component being constructed and the components of its environment.systems thinking can have impacts on component design that may surprise those who have not encountered such thinking before. for example, the designer of a component viewed in isolation may think it a good idea to provide graceful degradation in response to perceived error situations. in a systems context, however, this could have negative consequences: for example, another component might be better placed to respond to the error, but its response might be thwarted by the gracefully degraded behavior of the original component, and its own attempts to work around this degraded behavior could have further negative consequences elsewhere. it might have been better for the original component simply to have shut itself down in response to the error.as software has come to be deployed inšindeed has enabledšincreasingly complex systems, the systems aspects have come to dominate in questions of software dependability. dependability is not an intrinsic property of software. software is merely one component of a system and a software component may be dependable in the context of one system but not dependable in another.43 see, for example, warranty and disclaimer information at the following web pages for each of the companies mentioned: <http://www.adobe.com/products/eula/warranty/> (adobe); <http://www.apple.com/legal/sla/macosx.html> (apple); <http://www.microsoft.com/windowsxp/home/eula.mspx> (microsoft); and <http://desktop.google.com/eula.html> (google). 4 the guidance software for the ariane 4 rocket was dependable as part of that system, but when it was reused in the ariane 5, the assumptions about its operating environment were no longer valid, and the system failed catastrophically. j.l. lions, 1996, ﬁariane 5: flight 501 failure,ﬂ report by the inquiry board. available online at <http://www.cs.unibo.it/~laneve/papers/ariane5rep.html>.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.proposed approach 57a system is not simply the sum of its components: a system causes its components to interact in ways that can be positive (producing desirable emergent behavior) or negative (often leading to surprising outcomes, including failures). consequently, the properties of a system may not be related in a simple way to those of its components: it is possible to have a faulty system composed of correct components, and it is possible for a system correctly to achieve certain properties despite egregious ˚aws in its components.generally, what systems components should do is spelled out in their requirements and specication documents. these documents assume, but sometimes do not articulate, a certain environment. when placed in a system context, however, some of these assumptions may be violated. that is, the actual operational prole includes circumstances for which there may be no specied behavior (which means it is unclear what will happen) or for which the specied behavior is actually inappropriate. when these circumstances are encountered, failure often results. these sources of system failure are far more common, and often far more serious, than those due to simple bugs or coding errors.peoplešthe operators, users (and even the developers and maintainers) of a systemšmay also be viewed as system components. if a system meets its dependability criteria only if people act in certain ways, then those people should be regarded as part of the system, and an estimate of the probability of them behaving as required should be part of the evidence for dependability.5 for example, if airline pilots are assumed to behave in a certain way as part of the dependability claim for an aircraft, then their training and the probability of human error become part of the system dependability analysis.accidental systems and criticality creep many enterprises introduce software, or softwareenabled functions, into their organization without realizing that they are constructing a system or modifying an existing system. for example, a hospital may introduce a wireless network to allow physicians to access various databases from handheld pdas and may link databases (for example, patient and pharmacy records) to better monitor for drug interactions. those developing software to perform these integrations often encounter systems issues but may not recognize them as such. for example, they may recognize that network protocols introduce potential vulnerabilities and will consider the security of the wireless connection and the appropriate 5 sw01, the european standard for groundbased air trafc control systems, incorporates this approach.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.58 software for dependable systemscryptography to employ, but they may not recognize the larger systems issues of linking previously separate systems with their own security and access control policies.as another example, emergency care units may have a dozen or more different medical devices connected to the same patient. these devices are designed and developed in isolation, but they form an accidental system (that is, a system constructed without conscious intent) whose components interact through the patient™s physiology and through the cognitive and organizational faculties of the attending physicians and nurses. each device typically attempts to monitor and support the stabilization of some parameter (heart rate, breathing, blood chemistry) but it does so in ignorance of the others even though these parameters are physiologically coupled. the result can be suboptimal wholebody stabilization6 and legitimate concern that faults in a device, or in its operation, may propagate to other devices. because they are designed in isolation, the devices have separate operator interfaces and may present similar information in different ways and require similar operations to be performed in different ways, thereby inviting operator errors.a consequence of accidental system construction is that components may come to be used in contexts for which they were not designed and in which properties (typically internal failures and response to external faults) that were benign in their original context become more serious. an example is the use of desktop software in mission critical systems, as in the case of u.s.s. yorktown, whose propulsion system failed on september 21, 1997, due to a software failure. an engineer with the atlantic fleet technical support center attributed the failure to the integration and conguration of a commodity operating system without providing for adequate separation and process isolation.7a more subtle but pervasive form of criticality creep occurs when the distinction between safetycritical and missioncritical features becomes blurred as users become dependent on features that they previously lived without. an avionics system, for example, might provide a moving map displayšgenerally not ˚ightcriticalšthat produces information for a pilot, on which the pilot might come to depend. the formation of accidental systems may not always be avoidable, but it can be mitigated in two ways. the developer may be able to limit 6 see, for example, a talk given by timothy buchman titled ﬁdevices, data, information, treatment: a bedside perspective from the intensive care unitﬂ at the june 2005 high condence medical device software and systems workshop in philadelphia, pennsylvania. more information can be found online at <http://rtg.cis.upenn.edu/hcmdss/index.php3>. 7 see gregory slabodkin, 1998, ﬁsoftware glitches leave navy smart ship dead in the water,ﬂ government computer news, july 13. available online at <http://www.gcn.com/print/1717/337271.html>.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.proposed approach 59the exposure of the system as a whole to failures in some components, by designing interfaces carefully. for example, if a medical device is to be integrated into a hospitalwide information system, the developer might erect rewalls in the design of the software and hardware to ensure that the critical functions of the device cannot be controlled remotely. if this is not possible, the accidental system effect can be countered by recognizing the scope of the system as a whole and ensuring that the dependability case covers it.evolution and recerticationbecause systems and their operating environments evolve, a system that was dependable at the time it was introduced may become undependable after some time, necessitating a review and perhaps reworking of its dependability case. this review may conclude that the system no longer meets its original dependability criteria in its new environment. if so, the system may need to be modied, replaced, withdrawn from service, or simply accepted as being undependable.when a system has been accepted as t to put into service and it has been in use for some time, two issues may arise. first (and most commonly) something will happenšperhaps a bug x, or the modication of a feature, or a change to an interfacešthat requires that the software be changed. how should the modied system be recertied as t for service? a modied system is a new system, and local changes may affect the behavior of unmodied parts of the system, through interactions with the modied code or even (in many programming languages) as a result of recompilation of unmodied code. the evidence for dependability should therefore be reexamined whenever the system is modied and, if the evidence is no longer compelling, new evidence of dependability should be generated and the dependability case amended to re˚ect the changes. ideally, most of the dependability case will be reusable. it is also important to rerun the system test suite (including additional tests showing that any known faults have indeed been corrected) as software maintenance can subtly violate the assumptions on which the dependability case was originally based.second, inservice experience may show that the dependability case made incorrect assumptions about the environment. for example, a protection system for an industrial process may have the dependability requirement that it fails no more frequently than once in every thousand demands, based on an assumption that the control system would limit the calls on the protection system to no more than 10 each year. after a few months of service, it might be apparent that the protection system is being called far more often than was assumed would happen. in such cases, the software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.60 software for dependable systemssystem should be taken out of service (or protected in some other way that is known to have the necessary dependability) until the dependability case has been reexamined under the new assumptions and shown to be adequate, or until sufcient additional evidence of the dependability of the protection system has been obtained. third, in the security case, if a new class of vulnerability is discovered, software that was understood to be secure might become vulnerable. in such a case new tests, tools, or review processes must be developed and applied, and the system updated as needed to operate in the new threat environment. the level of revision required to make the system™s security acceptable in the face of the new threat will vary depending on the scope and impact of the vulnerability.what to make explicitthe considerations in the previous sections suggest two important principles regarding what should be made explicit. first, it makes no sense to talk about certiable dependability and justiable condence without dening the elements of the service that must be delivered if the system is to be considered dependable. in general, this will be a subset of the complete service provided by the system: some requirements will not be considered important with respect to dependability in the specic context under consideration. nor will it always be necessary to guarantee conformance to these properties to the highest degree. dependability is not necessarily something that must be applied to all aspects of a system, and a system that is certied as dependable need not work perfectly all the time. second, any claim about a service offered by a software component will be contingent on assumptions about the environment, and these assumptions will need to be made explicit.stating the requirements for a particular software component will generally involve three steps:ł the rst step is to be explicit about the desired properties: to articulate the functional dependability properties precisely. these should be requirements properties expressed in terms of the expected impact of the software in its environment rather than specication properties limited to the behavior of the software at its interface with other components of the larger system (see next section).ł the second step is to be explicit about the degree of dependence that will be placed on each property. this may be expressed as the probability of failure on demand (pfd) or per hour (pfh) or as a mean time between failures (mtbf). in general, the dependence on different properties will be different: for example, it might be tolerable for a rail signal to software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.proposed approach 61give an incorrect ﬁstopﬂ command once every 10,000 hours but an incorrect ﬁgoﬂ would be tolerated only once every 100 million hours, because the former would only cause delay, whereas the latter might cause a fatal accident. ł the third step is to be explicit about the environmental assumptions. these assumptions will generally include a characterization of the system or systems within which the software should be dependable and particular assumed properties of those systems. these properties may be arbitrarily complex, but sometimes they may involve little more than ranges of conditions under which the system will be operating. for example, an airborne collisionavoidance system may dependably provide separation for all geometries of two con˚icting aircraft approaching each other at less than mach 1 but become undependable if the approach is at mach 2 (because the alerts could not be given in time for effective action to be taken) or when more than two aircraft are in con˚ict (because resolving the con˚ict between two aircraft might endanger the third).requirements, specications, and domain assumptionsthe properties of interest to the user of a system are typically located in the physical world: that a radiotherapy machine deliver a certain dose, that a telephone transmit a sound wave faithfully, that a printer make appropriate ink marks on paper, and so on. the software, on the other hand, is typically specied in terms of properties at its interfaces, which usually involve phenomena that are not of direct interest to the user: that the radiotherapy machine, telephone, or printer send or receive certain signals at certain ports, with the inputs related to the outputs according to some rules.it is important, therefore, to distinguish the requirements of a software system, which involve properties in the physical world, from the specication of a software system, which characterizes the behavior of the software system at its interface with the environment.8 when the software system is itself only one component of a larger system, the other components in the system (including perhaps, as explained above, the people who work with the system) will be viewed as part of the environment.one fundamental aspect of a systems perspective, as outlined in the early sections of this chapter, is paying attention to this distinction. indeed, many failures of software systems can be attributed exactly to a 8 these denitions of the terms ﬁrequirementsﬂ and ﬁspecicationﬂ come from michael jackson (see footnote 11 below) and are not conventional. in standard usage, the distinction between the two is rather vague, with requirements being used for descriptions that are produced earlier in a development with more involvement of the customer.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.62 software for dependable systemsfailure to recognize this distinction, in which undue emphasis was placed on the specication at the expense of the requirements. the properties that matter to the users of a system are the requirements; the properties the software developer can enforce are represented by the specication; and the gap between the two should be lled by properties of the environment itself. the dependability properties of a software system, therefore, should be expressed as requirements, and the dependability case should demonstrate how these properties follow from the combination of the specication and the environmental assumptions.in some cases, the requirements, specication, and environmental and domain assumptions will talk about the same set of phenomena. more often, though, the phenomena that can be directly controlled or monitored by the software system are not the same phenomena of interest to the user. a key step, therefore, in articulating the dependability properties, is to identify these sets of phenomena and classify them according to whether they lie at the interface or beyond. in large systems involving multiple components, it will be protable to consider all the various interfaces between the components and to determine which phenomena are involved at each interface.this viewpoint is illustrated in figure 2.1. the outermost box represents the collection of phenomena in the world that are relevant to the problem the software is designed to address. the box labeled ﬁmachineﬂ represents the phenomena of the software system being built (and the machine it runs on). the box labeled ﬁenvironmentﬂ represents the phenomena of the components in the environment in which the software operates, including other computer systems, physical devices, and the human operators about whom assumptions are made. the box labeled ﬁuserﬂ represents the phenomena involving the users. the gray borders of the boxes represent shared phenomena. the three spots denote archetypal phenomena. the phenomenon m is internal to the machine and invisible from outside; the instructions that execute inside the computer, for example, are such phenomena. the phenomenon s is a specication phenomenon, at the interface of the machine, shared with its environment. the phenomenon r is a requirements phenomenon, visible to the user and shared with the environment but not with the machine. this view simplies the situation somewhat. it also shows the user as distinct from the environment, in order to emphasize that the phenomena that the user experiences (labeled r) are not generally the same as the phenomena controlled by the software (labeled s). in practice, the sets of specication and requirement phenomena overlap, and the user cannot be cleanly separated from the environment.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.proposed approach 63the dependability case will involve m, s, and r. the argument will have two pieces. first, a correctness argument for the software will show how s follows from m: that is, how the intended properties of the software system at its interface are ensured by its implementation. second, a specicationrequirements argument will show how r follows from s: that is, how the desired requirement as observed by the user is ensured by the behavior of the software system at its interface. the correctness argument can be constructed in terms of the software alone and is entirely formal (in the sense that it does not involve any notions that cannot in principle be perfectly formalized). the specicationrequirements argument, on the other hand, must combine knowledge about the software system with knowledge of the environment. in its general form, it will say that the requirements follow from the combination of the specication and the properties of the environment. this argument cannot usually be entirely figure 2.1 specication and requirements.21software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.64 software for dependable systemsformal, because determining the properties of the environment will in general have to be an informal matter. so whereas the correctness argument is in theory amenable to mechanized checking, the specicationrequirements argument will rest on assumptions about the environment that will need to be conrmed by domain experts. to illustrate the idea, here are some examples:ł trafc lights. the key dependability property of a trafc light system at a particular intersection is to prevent accidents. this is a requirement, and the phenomenon of two cars crashing is an example of an r. the software system interacts with the environment by receiving sensor inputs and generating control signals for the lights; these are the s phenomena. assumptions about the environment include that the sensor and trafc light units satisfy certain specications (for example, that a control signal sent to a trafc light will change the light in a certain way) and that the drivers behave in certain ways (for example, stopping at red lights and not in the middle of the intersection).ł radiotherapy. a key dependability property of a radiotherapy system is to not deliver an overdose to the patient. the phenomena r are those involving the location of the target of the beam, the dosage delivered, the identity of the patient, and so on. the software system interacts with the operator through user interfaces and with the physical devices that control and monitor the beam settings. assumptions include that the physical devices behave in certain ways and obey commands issued to them within certain tolerances, that the patient behaves in a certain way (not moving during irradiation, for example), and that the human organization of the facility obeys certain rules (such as preventing other people from entering the treatment room when the beam is on and ensuring that the correct patient is placed on the bed).ł criminal records. a dependability property of a system for maintaining criminal records may be that no records are permanently lost. the phenomena r involve the records and the means by which they are created and accessed. the phenomena s at the interface of the software system might include these and, in addition, commands sent to a disk drive. the key assumptions, for example, are that the disk drive offers certain reliability guarantees and that unauthorized access to the le system is prevented. the demarcation between a software system and its environment is not always clear and will often be determined as much by economic and organizational issues as by technical ones. for example, if the criminal records system is built on top of an existing service (such as a database or replicated le system), that service will be regarded as part of the environment.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.proposed approach 65the warsaw airport airbus accident in september 1993 has been cited as an example of a failure to distinguish between specications and requirements. an airbus a320211 came in to land in heavy winds. the aircraft aquaplaned for 9 seconds before reverse thrust was enabled, overran the runway, and collided with an embankment, killing 1 pilot and seriously injuring 2 other crew members and 51 passengers. the reverse thrust system was designed to be disabled under software control unless both left and the right landing gears were under compression, indicating contact with the ground. the software met its specication ˚awlessly, but unfortunately the specication did not match the desired dependability property. the crucial property, a requirement, was that reverse thrust should be disabled only when airborne, and this was certainly not satised. had the dependability of the system been expressed and evaluated in terms of this property, attention might have been drawn to the domain assumption that lack of compression always accompanies being airborne, and the construction of a dependability case might have revealed that this assumption was invalid.9the inevitable gap between specication and requirements properties speaks directly to the dependability of a service provision. at a minimum, software developers must appreciate this distinction, and as part of developing a dependable case there should be accountability for ensuring that the specication properties guarantee the requirements properties and for providing evidence (in the form of justiable environmental assumptions) for this connection. a useful analogy may be the role of architects in the design of a new building: the architects capture the extrinsic requirements (accommodation needs, relationships between different rooms, work˚ow, aesthetic considerations); they add the safety requirements and regulatory requirements and, with the help of specialists such as structural engineers, convert the whole into a set of specications that can be implemented by a construction rm. the architects accept responsibility and accountability for the relationship between extrinsic and intrinsic requirements.the idea of distinguishing requirements from specications is not new. in process control, the need to express requirements in terms of observable, extrinsic properties has long been recognized and was codied 9 the ofcial report of this incident has been translated by peter ladkin and can be found online (see below). interestingly, although the report notes in its recommendations the deciencies of the software, it attributes the cause of the accident to the ˚ight crew, blaming them for not aborting the landing. see peter ladkin, transcriber, 1994, ﬁtranscription of report on the accident to airbus a320211 aircraft in warsaw on 14 september 1993,ﬂ main commission, aircraft accident investigation, warsaw. available online at <http://www.rvs.unibielefeld.de/publications/incidents/docs/comandrep/warsaw/warsawreport.html>.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.66 software for dependable systemsin parnas™s fourvariable model.10 it distinguishes the specication phenomenašthe inputs and outputs of the machinešfrom the requirements phenomenašthe monitored and controlled variablesšthus accounting for the imperfections of the monitors and actuators that mediate between the machine and the environment. recent work has extended this to systems of more general structure.11 evidencea user should not depend on a system without some evidence that condence is justied. dependability and evidence of dependability are thus inseparable, and a system whose dependability is unknown cannot be regarded as dependable. in general, it will not be feasible to generate strong evidence for a system™s dependability after it is built (but before deployment); the evidence will need to be produced as part of the development process. betatesting, controlled release, and other eldtesting strategies may provide some evidence that software is acceptably dependable in applications that only require low dependability, but even in these lesscritical applications, the evidence obtained through eldtesting will rarely be sufcient to provide high condence that the software has the required properties.goalbased versus processbased assuranceto date, most approaches to developing dependable software (i.e., traditional approaches) have relied on xed prescriptions, in which particular processes are applied and from which dependability is assumed to follow. the approach recommended in this report might be characterized in contrast as goalbased.even if a system has the same components and design as some other system, it is likely to be unique in its context of use and in the concerns of its stakeholders. for this reason, assurance for systems by reference to some xed prescription is no longer advocated; instead, a goalbased approach is preferred. in a goalbased approach, the stakeholders rst agree on the goals for which assurance is required (for example, ﬁthis device must not harm peopleﬂ); then the developers produce specic claims (for example, ﬁthe radiation delivered by this device will never 10 d.l. parnas and j. madey, 1995, ﬁfunctional documentation for computer systems,ﬂ science of computer programming 25(1):4161.11 the view of requirements and specications in this section is based on the work of  michael jackson (2001, problem frames: analysing and structuring software development problems, addisonwesley, boston, mass.; and 1996, software requirements and specications, addisonwesley and acm press, new york).software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.proposed approach 67exceed so much intensityﬂ) and an argument to justify the claims based on veriable evidence (for example, ﬁthere is a mechanical interlock on the beam intensity and here is evidence, derived from extensive testing, that it worksﬂ). the top levels of the argument will generally employ methods from systems thinking, such as hazard analysis, fault tree analysis, and failure modes and effects analysis, while lower levels will employ more specialized techniques appropriate to the system and technology concerned.processbased assurance will typically mandate (or strongly recommend) the processes that the developers must follow to support a claim for a particular level of dependability. for example, the avionics standard do178 mandates modied condition/decision coverage (mcdc) testingšdescribed in chapter 1šfor the most critical software. this can lead to a culture where software producers follow the standard and then claim that their software has achieved the required dependability without providing any direct evidence that the resulting product actually has the required properties. in contrast, goalbased standards require the developers to state their dependability targets and to justify why these are adequate for the application, and then to choose development and assurance methods and to show how these methods provide sufcient evidence that the dependability targets have been achieved. goalbased assurance will usually provide a far stronger dependability case than processbased assurance.another advantage of goalbased assurance cases over more prescriptive methods for assurance is that they allow expert developers to choose suitable solutions to novel design problems. in addition, goalbased assurance approaches are able to keep pace with technological change and with the attendant changes in system functions and hazards along with the goals of their stakeholders. as noted earlier, the increased ˚exibility demands expertise and judgment in discerning what technological and process approaches are best suited in a given circumstances to meet explicit requirements and develop the evidence needed for an ultimate dependability case. in short, then, as explained above, the developers make explicit claims about the dependability properties of the delivered system. for these claims to be useful to the consumers of the system, the developers present, along with the claims, a dependability case arguing that the system has the claimed properties. such an approach is only useful to the extent that the claims can be substantiated by the dependability case, and that the case is convincing. it therefore requires transparency so that the consumer (broadly construed) can assess the case™s credibility and accountability to discourage misrepresentation.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.68 software for dependable systemsthe dependability casedependability requires justiable condence, which in turn requires that there be adequate evidence to support the claims of dependability, and that this evidence be available to those who have to assess the degree of condence that the evidence supports. claims for certiably dependable software should therefore be not only explicit but also backed by sufcient evidence, and this evidence should be open to inspection and analysis by those assessing the dependability case. what constitutes sufcient evidence for dependability depends on the nature of the claim and the degree of dependability that is required. in general, however, the evidence will constitute a dependability case that takes into account all components of the system as a whole: the software, physical devices with which it interacts, and assumptions about the domain in which it operates (which will usually include both assumptions about the physical environment and assumptions about the behavior of human operators).of course, the construction of those parts of the dependability case that go beyond the software may require skills and knowledge beyond those of the software engineer and may be relegated to domain experts. but it is vital that the software engineer still be responsible for ensuring not only that the part of the case involving software is sound, but also that it is used appropriately in the larger case.the role of domain assumptionsas explained in the preceding section, a dependability claim for a system should be made in terms of requirements that involve the phenomena of the environment; it is to affect these phenomena that the system is introduced in the rst place. the software itself, on the other hand, is judged against a specication that involves only phenomena at the interface of the machine and the environment, which the software is capable of controlling directly.between the requirements and the specication lie domain assumptions. a dependability case will generally involve a statement of domain assumptions, along with their justications, and an argument that the specication of the software and the domain assumptions together imply the requirements. insisting on this tripartite division of responsibilityšchecking the software, checking the domain assumptions, and checking that they have the correct combined effectšis not a pedantry. as the warsaw airbus incident illustrates, the meeting point of these three components of the dependability case is often a system™s achilles™ heel.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.proposed approach 69the role of architecturethe demand for evidence of dependability and the difculty of producing such evidence for complex systems have a straightforward but profound implication. any component for which compelling evidence of dependability has been constructed at reasonable cost will likely be small by the standards of most modern software systems. every critical specication property, therefore, will have to be assured by one, or at most a few, small components. sometimes it will not be possible to separate concerns so cleanly, and in that case, the dependability case will be less credible or more expensive to produce.the case that the system satises a property has three parts:ł an argument that the requirements properties will be satised by the specication of the system, in conjunction with the domain assumptions. as explained above, this requires that the domain assumptions are made explicit and shown to be justied. for example, the specication of a controller that is used to maintain a safe level in a reservoir may depend on assumptions about signals from sensors, the behavior of valves, and the ˚ow rate through out˚ow pipes under a range of operational conditions. these assumptions should be stated and reviewed by domain experts and may need to be tested under operational conditions to achieve the necessary condence that they are correct.ł an independence argument, based on architectural principles, that only certain components are relevant. the independence argument will rely on properties of both the particular architecture and the language and implementation platform on which it stands. the easiest case will be where the components are physically separated, for example by running on separate processors with no shared memory. where the components share memory, unless they use a safe, welldened language and a robust, fully specied platform, such an argument will not be possible. for example, if the language allows arbitrary integers to be used as if they were pointers to variables (as in c), it will not be possible to argue that the regions of memory read and written by distinct modules are disjoint, so even modules implementing functionality unrelated to the property at issue would have to be treated as relevant. these shortcomings might be overcome, but only at considerable cost. for example, memory safety could be established by restricting the code to a subset that disallows certain constructs and then performing a review, preferably with the aid of automated tools, to ensure that the restriction has been obeyed.ł a more detailed argument that the components behave appropriately. this argument is likely to involve analysis of the specication for completeness and consistency, analysis of the design to show conformance with the specication, and analysis of the implemented software to show software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.70 software for dependable systemsconsistency with the design and the absence of unsafe properties (such as memory faults or the use of undened values). the components, subsystems, and system will then usually be tested to provide some endtoend assurance.the degree of coupling between components, in the form of dependences that cause one component to rely on another, is likely to be a good indicator of the effort that will be required to construct a dependability argument. in general, the more dependences and the stronger the dependences, the more components will need to be considered and the more detailed their specications will need to be, even to establish a limited property. the role of testingtesting is indispensable, and no software system can be regarded as dependable if it has not been extensively tested, even if its correctness has been proven mathematically. testing can nd ˚aws that elude analysis because it exercises the system in its entirety, where analysis must typically make assumptions about the execution platform that may turn out to be unwarranted. human observation of an executing system, especially one that interacts heavily with a user, can also reveal serious ˚aws in the user interface, and even in the formulation of the dependability properties themselves.testing plays two distinct roles in software development. in the rst role, testing is an integral component of the software development process. automatic tests, run every time a change is made to the code, have proven to be extremely effective at catching faults unwittingly introduced during maintenance. if code is frequently refactored (that is, if code is modied to simplify its structure without changing its functionality) retesting is especially important. when a fault is found in the code, standard practice requires the construction of a regression test to ensure that the fault is not reintroduced later. having programmers develop unit tests for their own modules encourages them to pay attention to specications and can eliminate faults that would be more expensive to detect after integration. (there is some evidence, however, that unit tests are not particularly effective or necessary if code is developed from a formal specication and is subject to static analysis.12)testing is often an inexpensive way to catch major ˚aws, especially in areas (such as user interfaces) where analysis is awkward. a skillfully 12 s. king, j. hammond, r. chapman, and a. pryor, eds. ﬁis proof more costeffective than testing?ﬂ ieee transactions on software engineering 26(8):675686.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.proposed approach 71constructed test suite can also nd faults that would rarely fail in service but in ways difcult to diagnose; experienced programmers, for example, will insert diagnostics into concurrent code in patterns that are likely to expose data races and deadlocks. ﬁfuzz testing,ﬂ in which a program is subjected to a huge suite of randomly generated test cases, often reveals faults that have escaped detection in other ways. the power of testing can be greatly amplied if formal models, even very partial ones, are available; tests can be generated automatically from state machine models using a technique known as ﬁmodelbased testingﬂ and from invariants or runtime assertions.as dijkstra observed, however, testing can reveal the presence of errors but not their absence.13 the theoretical inadequacies of testing are well known. to test a program exhaustively would involve testing all possible inputs in all possible combinations and, if the program maintains any data from previous executions, all possible sequences of tests. this is clearly not feasible for most programs, and since software lacks the continuity of physical systems that allow inferences to be drawn from one sample execution about neighboring points, testing says little or nothing about the cases that were not exercised. because state space14 coverage is unattainable and hard even to measure, less ambitious forms of coverage have been invented, such as ﬁallstatementsﬂ (in which every statement of the program must be executed at least once), ﬁallbranchesﬂ (in which every branch in the control ˚ow must be taken), and a variety of predicate coverage criteria (in which the aim is to achieve combinations of logical outcomes from the expressions that comprise the condition of each loop or ifstatement). testing researchers established early on that many of the intuitions that a tester might have that give condence in the value of coverage are incorrectšfor example, a coverage criterion that is stricter (in the sense that it rejects a larger set of test suites as inadequate) is not necessarily more effective at nding faults.15 moreover, a recent study showed that even the predicate coverage criterion known as mcdc (used 13 o. dahl, e.w. dijkstra, and c.a. hoare, 1972, structured programming, academic press, new york.14 the ﬁstate spaceﬂ of a system is the set of statesšinternal congurations or conditionsšthat the system can potentially occupy. if a test suite covers the entire state space, then every possible conguration has been tested, and the test is complete. in practice, however, the state space is usually so large that only a small proportion is exercised by a test suite. modelbased testing is an approach that seeks to appropriately abstract and consolidate states in meaningful ways so that more of the state space can be covered. demonstrating the appropriateness of the abstraction and consolidation then becomes another element of the construction of the dependability case. 15 phyllis g. frankl and elaine j. weyuker, 1993, ﬁa formal analysis of the faultdetecting ability of testing methods,ﬂ ieee transactions in software engineering 19(3):202213.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.72 software for dependable systemswidely in avionics and regarded as extremely burdensome) does not ensure the detection of a class of bugs found easily by static analysis.16as hoare has noted,17 testing is, in practice, ﬁmore effective than it has any right to beﬂ in improving the quality and dependability of software. hoare™s explanation is that while the contribution of testing to exposing bugs might only account for low levels of dependability, its contribution to providing feedback on the development process might account for much higher levels. in hoare™s words: ﬁthe real value of tests is not that they detect bugs in the code but that they detect inadequacies in the methods, concentration, and skills of those who design and produce the code.ﬂ the most conscientious development teams indeed use testing in this manner. when a module or subsystem fails too many tests, the developers do not simply attempt to patch the code. instead, they look to the development process to determine where the error was introduced that eventually resulted in the failure, and they make the correction there. this might involve clarifying requirements or specications, reworking a design, recoding one or more modules from scratch, and, in extreme cases, abandoning the entire development and starting afresh.in short, testing is a powerful and indispensable tool, and a development that lacks systematic testing should not be regarded as acceptable in any professional setting, let alone for critical systems. how a software supplier uses testing is important information in assessing the credibility of its dependability claims (see the discussion of transparency in chapter 3). the second role of testing is in providing concrete evidence that can be used in a dependability case. testing is an essential complement to analysis. because the activities of testing differ so markedly from those involved in analysis, testing provides important redundancy and can catch mistakes made during the analysis process, whether by humans or tools. the dependability case for an extrinsic property will often rely on assumptions about a physical device, which will be represented as a formal model for the purpose of analysis. such formal models should obviously be testedšideally before they are used as the basis for development. a patient monitoring system, for example, might assume certain properties of accuracy and responsiveness for the monitoring devices; the case for the system as a whole will require these to be substantiated by extensive and rigorous testing, ideally not only by the suppliers of the 16 andy german and gavin mooney, 2001, ﬁair vehicle software static code analysisšlessons learnt,ﬂ proceedings of the ninth safetycritical systems symposium, felix redmill and tom anderson, eds., springerverlag, bristol, united kingdom. 17 c.a.r. hoare, 1996, ﬁhow did software get so reliable without proof?ﬂ lecture notes in computer science 1051:117.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.proposed approach 73devices but also by the developers of the system that uses them. endtoend tests are especially important to catch interactions and couplings that may not have been predicted. in a radiotherapy system, for example, the beam would be examined with a dosimeter to ensure that the physical dose delivered at the nozzle matches the prescribed dose entered earlier at the therapist™s workstation. at the same time, it is important to realize that testing alone is very rarely sufcient to establish high levels of dependability. testing will be an essential component of a dependability case but will not in general sufce, because even the largest test suites typically used will not exercise enough paths to provide evidence that the software is correct and have little statistical signicance for the levels of condence usually desired. it is erroneous to believe that a rigorous development process in which testing and code review are the only verication techniques would justify claims of extraordinarily high levels of dependability. some certication schemes, for example, associate higher ﬁsafety integrity levelsﬂ with more burdensome process prescriptions and imply that following the processes recommended for the highest integrity levels gives condence that the failure rate will be less than 1 failure per 1 billion hours. such claims have no scientic basis.furthermore, unless a system is very small or has been meticulously developed bearing in mind the construction of a dependability case, credible claims of dependability are usually impossible or impractically expensive to demonstrate after design and development of the system have been completed.18another form of evidence that is widely used in dependability claims for a component or system to be used in a critical setting is its prior extensive use. in fact, the internal state space of a complex software system may be so large that even several years™ worth of execution by millions of users cannot be assumed to achieve complete coverage. a new environment might expose unknown vulnerabilities in a component. components designed for use in commercial, lowcriticality contexts are not suitable for critical settings unless justied by an explicit dependability case that places only appropriate weight on previous successful uses.testing offered as part of a dependability case, like all other components of the dependability case, should be carefully justied. since the purpose of the dependability case is to establish the critical properties of the system, the degree of condence warranted by the testing will vary 18 b. littlewood and l. strigini, 1993, ﬁvalidation of ultrahigh dependability for softwarebased systems,ﬂ communications of the acm 36(11):6980. also see r. butler and g. finelli, 1993, ﬁthe infeasibility of quantifying the reliability of lifecritical realtime software,ﬂ ieee transactions on software engineering 19(1):312.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.74 software for dependable systemsaccording to the strength of the connection between the tests and the properties claimed. at one extreme, if a component can be tested exhaustively for all possible inputs, testing becomes tantamount to proof, giving very high condence. at the other extreme, execution of even a large set of endtoend tests, even if it achieves high levels of code coverage, in itself says little about the dependability of the system as a whole.it cannot be stressed too much that for testing to be a credible component of a dependability case, the relationship between testing and the properties claimed will need to be explicitly justied. the tester may appeal to known properties of the internals of the system or to a statistical analysis involving the system™s operational prole. in many cases, the justication will necessarily involve an argument based on experiencešfor example, that attaining a certain coverage level has in the past led to certain measured failure rates. that experience should be carefully evaluated. sometimes, the test suite itself may be treated as direct evidence for dependability. for a standard test suite (such as the java compatibility kit used for testing implementations of the java platform19), it will be possible to base the degree of condence on the opinions of experts familiar with the suite. but a custom test suite, however credible, may place an unreasonable burden on those assessing the dependability case.until major advances are made, therefore, testing should be regarded in general as only a limited means of nding ˚aws, and the evidence of a clean testing run should carry weight in a dependability argument only to the extent that its implications for critical properties can be explicitly justied.the role of analysisbecause testing alone is insufcient, for the foreseeable future the dependability claim will also require evidence produced by analysis. moreover, because analysis links the software artifacts directly to the claimed properties, for the highest levels of dependability, the analysis component of the dependability case will usually contribute condence at lower cost.analysis may involve wellreasoned informal argument, formal proofs of code correctness, and mechanical inference (as performed, for example, by ﬁtype checkersﬂ that conrm that every use of each variable in a program is consistent with the properties that the variables were dened to have). indeed, the dependability case for even a relatively simple system will usually require all of these kinds of analysis, and they will need to be tted together into a coherent whole.19 for more information on the java compatibility kit, see <https://jck.dev.java.net/>.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.proposed approach 75type checking, for example, may be used to establish the independence of modules; known properties of the operating system may be used to justify the assumption that address space separation is sound; modular correctness proofs used to establish that, under these assumptions, the software satises its intrinsic specications; and informal argument, perhaps augmented with some formal reasoning, to make the link to the crucial extrinsic properties.an argument taking the form of a chain of reasoning cannot be stronger than its weakest link. (recent research20 on combining diverse arguments opens the possibility that independent, weak arguments for the dependability of a system could some day be combined to provide a quantiably stronger argument.) it will therefore be necessary to ensure that the tools and notations used to construct and check the argument are robust. if they are not, extraordinary efforts will be required to overcome their limitations. for example, if a language is used that does not require that the allowable properties of every program object are tightly dened and enforced (i.e., a ﬁtypeunsafeﬂ or ﬁweakly typedﬂ language), a separate, explicit argument will need to be constructed to ensure that there are no violations of memory discipline that would compromise modular reasoning. if the programming language has constructs that are not precisely dened, or that result in compilerdependent behavior, it will be necessary to restrict programmers to a suitable subset that is immune to the known problems.as noted in chapter 1, there are difculties and limits to contemporary software analysis methods, owing in part to the need for a highly trained and competent software development staff. indeed, the quality of the staff is at least as important as the development methods and tools that are used, and so these factors should also be included in the evidence.rigorous process: preserving the chain of evidencealthough it might be possible to construct a dependability case after the fact, in practice it will probably only be achievable if the software is built with the dependability case in mind. each step in developing the software needs to preserve the chain of evidence on which will be based the argument that the resulting system is dependable. at the start, the domain assumptions and the required properties of the system should be made explicit; they should be expressed unambigu20 for example, robin bloomeld and bev littlewood, 2006, ﬁon the use of diverse arguments to increase condence in dependability claims,ﬂ in structure for dependability, d. besnard, c. gacek, and c.b. jones, eds., springerverlag, new york, pp. 254268.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.76 software for dependable systemsously and in a form that permits systematic analysis to ensure that there are no unresolvable con˚icts between the required properties. because each subsequent stage of development should preserve the evidence chain that these properties have been carried forward without being corrupted, each form in which the design or implementation requirements are expressed should support sufcient checking that the required properties have been preserved.what is sufcient will vary with the required dependability, but preserving the evidence chain necessitates that the checks are carried out in a disciplined way, following a documented procedure and leaving auditable recordsšin other words, a rigorous process. for example, if the dependability argument relies, in part, on reasoning from the properties of components, then the system build process should leave evidence that the system has been built out of the specic versions of each component for which there is evidence that the component has the necessary properties. this can be thought of as ﬁrigorous conguration management.ﬂcomponents and reusecomplex components are seldom furnished with the information needed to support dependability arguments for the systems that use them. for use within a larger argument, the details of the dependability case of a component need not be known (and might involve proprietary details of the component™s design). but the claims made for a component should be known and clearly understood, and it should be possible to assess their credibility by, for example, the reputation of a thirdparty reviewer (in much the same way as the faa credibly assures the airworthiness of aircraft) or the nature of the evidence. not all systems and not all properties are equally critical, and not all the components in a system need assurance to the same level: for example, we may demand that one component can fail to satisfy some property no more than one time in a billion, while for another property we might tolerate one failure in a thousand. until recently, there has been little demand for components to be delivered with the claims, argument, and evidence needed to support the dependability case for a system that uses the component. at lower levels of criticality, and in accidental systems, explicit dependability cases have seldom been constructed, so there has been no perceived need for componentlevel cases. at the other extreme, systems with highly critical assurance goals (such as airplanes) have driven their dependability cases down into the details of their components and have lacked regulatory mechanisms to support use of prequalied critical components, which would allow the case for the larger system software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.proposed approach 77to use the case for its components without inquiring into all the details of the components themselves.with greater reuse of components, and a concomitant awareness of the risks involved (especially of using commodity operating systems in critical settings), componentlevel assurance will become an essential activity throughout the industry. in the case of critical systems such as airplanes, it used to be the case that their software was built on highly idiosyncratic platforms that were seldom reused from one airplane to the next, and the same was true of the architectural frameworks that tie multiple computer systems and buses together to support faulttolerant functions such as autopilot, autoland, ˚ight management, and so on. nowadays, however, the software is generally built on realtime operating systems such as lynxos178 that are highly specialized but nonetheless standardized components, and standardized architectural frameworks such as primus epic and the timetriggered architecture (tta) have emerged to support integrated modular avionics (ima).to support these developments, the faa developed an advisory circular on reusable software components,21 and guidelines for ima have been developed by the appropriate technical bodies (sc200 of rtca and wg60 of eurocae) and are currently being voted on. both of these developments are rather limited, however, in that they allow only for a software component that has been used in the traditional assurance case for a certied airplane to take the assurance data developed in that certication into the assurance case for additional airplanes; they fall short of allowing the assurance case for a system to build on the assurance cases for its components.in the case of lesscritical systems, much attention has been focused recently on the use of commercial offtheshelf (cots) subsystems and software of uncertain pedigree/unknown provenance (soup). while the attention has focused mostly on the use of architectural mechanisms (for example, wrappers) to mitigate the unknown (un)reliability of these components, it has also highlighted the lack of assurance data for these components: it matters less that they are unreliable than that it is unknown how unreliable they are, and in what ways their unreliability is manifested.accidental systems often use cots and soup and do so in contexts that promote criticality creep (see previous discussion). if these cases were recognized appropriately as systems and subjected to an appropriate dependability regime, the cost of providing adequate dependability 21 federal aviation administration (faa), 2004, ﬁreusable software components,ﬂ ac 20148, faa, washington, d.c. available online at <http://www.airweb.faa.gov/regulatoryandguidancelibrary/rgadvisorycircular.nsf/0/ ebfccb29c0e78fff86256f6300617bdd?opendocument>.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.78 software for dependable systemsevidence for the cots/soup component might exceed the cost of developing a new component when high dependability is required. it is apparent that at all levels of criticality it is currently impossible to develop dependability cases for systems based solely on those cases for their components. in the case of critical systems such as airplanes this is mostly because the regulatory framework does not allow it, in part because the science base does not yet provide the ability to reason about systemlevel properties such as safety or security based solely on the properties of the system™s components. in the case of less critical and accidental systems, it is often because such systems rely on cots and soup, for which no suitable assurance data are available.expertisebuilding software is hard; building dependable software is harder. although the approach advocated in this report is designed to be as free as possible from the fetters of particular technologies, it also assumes that developers are using the very best techniques and tools available. a development team that is unfamiliar and inexperienced with best practices is very unlikely to succeed.this section therefore contains an outline of some of today™s best practices. it might be used in many ways: for educational planning, for assessing development organizations, for evaluating potential recruits, and even as the basis for licensing. however, the committee offers the outline only as guidance and would not want it to be seen as binding in all circumstances. few best practices have universal application, and most need to be adjusted to the context of a particular problem.different problems and different development contexts call for different practices. moreover, what is considered to be best practice changes over time, as new languages and tools appear and ideas about how to develop software continue to mature. the committee therefore felt it would be unwise to tie its recommendations to particular practices. in addition, merely applying a set of best practices absent a carefully constructed dependability case does not warrant condence in the system™s dependability.at the same time, in order to make concrete the importance of best practices, the committee decided to offer a list of practices that it regards as representative of a broad consensus at the time of writing. it also seemed desirable to provide some guidance regarding today™s best practices, especially since developers in smaller organizations are often unaware of simple practices that can dramatically improve software quality.this section begins with a discussion of simplicity, because a commitment to simplicity is key to achieving justiable condence and dependsoftware for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.proposed approach 79able software. a commitment to simplicity is often the mark of true expertise. the list of particular best practices that follows this discussion is by no means exhaustive. it represents the consensus of the committee on a core set of practices that can be widely applied and that can bring dramatic benet at relatively low cost. for the most part, these practices represent minimal standards of software engineering. in some cases, for development of a noncritical system in which high dependablity is not required, less stringent practices may make sense, as noted in the list.simplicitythe price of reliability is the pursuit of the utmost simplicity. it is a price which the very rich nd most hard to pay.22in practice, the key to achieving dependability at reasonable cost is a serious and sustained commitment to simplicity. an awareness of the need for simplicity usually comes only with bitter experience and the humility gained from years of practice. moreover, the ability to achieve simplicity likewise comes from experience. as alan perlis said, ﬁsimplicity does not precede complexity, but follows it.ﬂ23the most important form of simplicity is that produced by independence, in which particular systemlevel properties are guaranteed by individual components, much smaller than the system as a whole, whose preservation of these properties is immune to failures in the rest of the system. independence can be established in the overall design of the system with the support of architectural mechanisms. its effect is to dramatically reduce the cost of constructing a dependability case for a property, since only a relatively small part of the system needs to be considered. where independence is not possible, wellformed dependence is critical. independence allows the isolation of safe critical functions to a small number of components. wellformed dependence (wherein a lesscritical service may depend on a critical service but not vice versa) allows critical services to be safely used by the rest of the system. independence and wellformed dependence are important design principles of overall system architecture. simplicity has wider applications, however, which the rest of this section discusses.a major attraction of software as an implementation medium is its capacity for complexity. functions that are hard, expensive, or impossible 22 c.a.r. hoare, 1981, ﬁthe emperor™s old clothesﬂ (turing award lecture), communications of the acm 24(2):7583. available online at <http://portal.acm.org/citation.cfm?id=358561>.23 alan j. perlis, 1982, ﬁepigrams on programming,ﬂ sigplan notices 17(9):713.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.80 software for dependable systemsto implement by other means (whether automatically in physical devices or manually in human organizations) can often be realized at low cost in software. indeed, the marginal cost of complexity in software can seem negligible, as the cost of computational resources drops. in fact, however, complexity can in˚ict large costs. when a software system grows as new and more complex functions are added, its structure tends to deteriorate, and each new modication becomes harder to perform, requiring more parts of the code to be changed. it is not uncommon for a system to collapse under the weight of its own complexity.24 developers usually cannot shield the user from the complexity of software. as the specication becomes more complex, it typically loses any coherence it once possessed. the user has no intelligible conceptual model of the system™s behavior and can obtain predictable results only by sticking to welltried scenarios.complexity has, of course, a legitimate role. after all, software is often used precisely to satisfy the need for complex functions that are more cheaply and reliably implemented by software than by other means, mechanical or human. but complexity exacts a heavy price. the more complex a system is, the less well it is understood by its developers and the harder it is to test, review, and analyze. moreover, complex systems are likely to consist of complex individual components. complex individual components are more likely to fail individually than simpler components and more likely to suffer from unanticipated interactions. these interactions are most serious amongst systems and between systems and their human users; in many accidents (for example, at three mile island25), users unwittingly took a system toward catastrophe because they were unable to understand what the system was doing.whether a system™s complexity is warranted is, of course, a difcult judgment, and systems serving more users and offering more powerful functionality will generally be more complex. moreover, the demand for dependability itself tends to increase complexity in some areas. for example, a system may require a very robust storage facility for its data. this will inevitably make the system more complex than one in which data loss can be tolerated. but the lesson of simplicity still applies, and a designer committed to simplicity would choose, for example, a standard replication scheme over a more complicated and ad hoc design that attempts to exploit the particular properties of the data.24 the failure of netscape, for example, has been attributed in part to the company™s inability to extricate itself from the complexity of its navigator browser. see michael a. cusumano and david b. yofe, 1998, competing on internet time: lessons from netscape and its battle with microsoft, free press, new york.25 see charles perrow, 1999, normal accidents, princeton university press, princeton, n.j.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.proposed approach 81the overriding importance of simplicity in software development has been championed for decades. formal methods researchers, such as hoare (quoted above), were among the rst to stress its value because they discovered early that extra complexity rapidly destroys the ability to generate evidence for dependability. many practitioners have argued that the complexity of software is inherent to the task at hand, but this position has eroded, and views such as those re˚ected in the dicta of agile methodologiesšﬁyou aren™t gonna need itﬂ and ﬁthe simplest thing that worksﬂšare gaining ground.there is no alternative to simplicity. advances in technology or development methods will not make simplicity redundant; on the contrary, they will give it greater leverage. to achieve high levels of dependability in the foreseeable future, striving for simplicity is likely to be by far the most costeffective of all interventions. simplicity is not easy or cheap, but its rewards far outweigh its costs.here are some examples of how a commitment to simplicity can be demonstrated throughout the stages of a development:ł requirements. a development should start with a carefully chosen, minimal set of requirements. complex features often exact a cost that greatly exceeds the benet they bring to users. the key to simplicity in requirements is the construction of abstractions and generalizations that allow simple, uniform functions to be used for multiple purposes. overgeneralization, of course, can itself be a source of gratuitous complexity but can usually be recognized because it makes the requirements more, not less, complicated.ł architecture. small and simple components are easier to reason about and less likely to harbor subtle bugs. simple and clean interfaces reduce the risk of misunderstandings between members of the development team and reduce the incidence of complex interactions, which are the most common cause of bugs in large systems. it is a mistake to believe that richer interfaces with a larger array of more elaborate functions benet the users of the interface; on the contrary, they tend to be less useful and perform more poorly.26 a dependence graph of the code showing which other modules each module depends on can reveal sources of architectural complexity, indicating where layering is violated, where lowlevel modules depend on highlevel ones, where cycles prevent modular reasoning, or where simply a proliferation of dependences suggests a breakdown of the original design.26 butler lampson, 1983, ﬁhints for computer system design,ﬂ acm operating systems review 17(5):3348. (reprinted in ieee software 1(1):1128. available online at <http://research.microsoft.com/lampson/33hints/webpage.html>.)software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.82 software for dependable systemsł trusted base. if the dependability properties of a system can be conned to a small trusted base consisting of only a few components so that the properties can be guaranteed without analyzing the rest of the system (except perhaps to establish certain noninterference properties), powerful techniques such as program verication, which would not be feasible for the system as a whole, can be applied locally to provide a level of condence not attainable by any other means.ł languages. complex development languages can undermine a development by making even a simple design appear complex and by introducing new opportunities for error. developers should be wary of complex and poorly dened modeling languages27 and of programming languages with imprecise semantics, or semantics that are platform or compilerdependent. when other factors dictate the use of an overly complex language, simplicity can often be regained by restricting usage to a welldened and robust subset (such as the spark subset of ada).28ł tools. developers should favor simple tools and should be especially wary of code generation tools whose behavior is poorly understood. tools that perform elaborate functions may need to be complex, but understanding how to use them and assessing their output should not be complex. a code verication tool, for example, might have complex analysis functions, but it should report its results in an intelligible fashion and, ideally, produce a proof that can be independently checked by a simpler tool.ł process. a rigorous process is essential to constructing a dependability case, but an elaborate and complex process that places a heavy burden on developers can be worse than no process at all. excessive documentation is particularly problematic; it diverts attention from more important matters and is usually writeonly. a common tendency is to set elaborate standards in trivial areas: some organizations, for example, have coding standards that specify meticulously how various constructs should be formatted (a task that should be carried out by an editing tool) but fail to address the major weaknesses of the programming language.27 in computer science and allied elds of information management and business process modeling, modeling languages enable software architects, business analysts, and others to specify the requirements of an organizational or software system on a ﬁtopﬂ or architectural level. these languages seek to diagrammatically render system requirements in a manner that management, user groups, and other stakeholders can understand, with the goal of eliciting feedback from these groups. 28 john barnes, 2003, high integrity software: the spark approach to safety and security, addisonwesley, boston, mass.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.proposed approach 83best practicesthe following subsections describe some of today™s specic systemlevel, code and modulelevel, and processlevel best practices. systemlevel practicesthe committee offers a set of systemlevel best practices below.ł prioritization of requirements. prioritize requirements and articulate them simply and directly in terms of key properties rather than as a long list of functions, features, or scenarios.ł requirements vs. specications. the requirements of a software system should describe the intended effect of the system on its environment and not, more narrowly, the behavior of the system at its interface with the environment, which is the subject of the specication.ł realistic demands. do not include requirements that are unrealistic or that cannot be realistically assessed. in particular, vague numerical measures are not a substitute for precise requirements: the requirement of ﬁsix 9s availability,ﬂ for example, makes little sense without a clear articulation of which service is being provided and what constitutes availability or lack thereof.ł environmental assumptions. in the requirements document, clearly separate the demands on the software system being constructed from the demands on the environment or its operators. include an explicit description of the environment, with a glossary that covers the domainspecic terms used throughout the document. articulate precisely and fully any environmental assumptions and have them reviewed by domain experts.ł hazard analysis. for a critical application, perform a hazard analysis that identies the most likely hazards and checks that they have been mitigated appropriately. address security risks by building and evaluating explicit threat models.ł dependability case. for a critical application, construct a dependability case that explains why the system executing in context is likely to satisfy the prioritized requirements.ł usability testing. apply usability testing to user interfaces in the early phases of development and periodically from then on.ł formal modeling and analysis. express requirements and specications precisely and unambiguously. an effective way to do this is to use a formal notation. for a noncritical system, a complete formal model will not generally be costeffective, but it will usually be feasible and desirable to express at least the most important elements formally. ł analysis tools. exploit automatic analysis tools to nd defects in requirements and specications documents and to increase condence in software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.84 software for dependable systemstheir correctness. in noncritical developments this may involve little more than using tools that check for consistent use of names. in critical developments, use tools that offer deeper analysis, such as model checkers and simulators.ł standard solutions. adopt standard solutions for algorithms and protocols unless a strong case has been made for deviating from standard practice. avoid inventing algorithms in areas that are known to be extremely hard to design correctly (for example, distributed consensus, authentication, fault tolerance). if the standard solution seems not to apply, consult an expert.code and modulelevel practicesother best practices would apply at the code and module levels:ł interfaces. design interfaces between modules that are small and welldened. exploit programming language mechanisms to express the module structure and check it at compile time. specify all public interfaces fully and integrate the specications with the code, using, for example, a tool such as javadoc.29ł data abstraction. minimize the scope and accessibility of all program components. hide the representation of data using data abstraction, and use programming language mechanisms to enforce it. make data types immutable whenever possible.ł inheritance. inheritance is a powerful but dangerous programming feature. use it sparingly, and whenever possible favor composition (adding functionality by embedding one object explicitly in another) over inheritance. design for inheritance or prohibit it, and do not extend classes that were not designed with extension in mind. in critical applications, avoid inheritance or ensure that adequate time has been allowed for the extensive additional verication activity that will be required.ł module dependences. evaluate the code structure by constructing a module dependence diagram (preferably with an automated tool), and modify the code to eliminate complex or troublesome dependences (especially those violating layering, and those forming cycles).ł standard libraries. use standard libraries unless (1) sufciently robust libraries are not available for the functionality desired or (2) a much smaller and simpler library can be constructed for the problem at hand using published and peerreviewed algorithms.29 for more information about the javadoc tool, see <http://java.sun.com/j2se/ javadoc/>.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.proposed approach 85ł safe language. use a safe programming language where feasible and exploit the features that amplify static type checking (such as generics). avoid extralinguistic or borderline features (such as native methods, re˚ection, and serialization). know and avoid the traps and pitfalls of the platform. for a critical application, consider using a robust subset of the language (e.g., misrac and spark); using an unsafe language (such as c) is unacceptable unless extraordinary measures (such as proof of type correctness) have been taken to mitigate the risks.30ł coding standards. establish clear and simple coding standards to enforce good practices in the use of programming language features.31 conventions for naming and layout are useful, especially because they amplify the power of simple lexical tools, but they are secondary to standards that have a direct impact on dependability. appropriate coding standards are especially effective at eliminating security vulnerabilities.ł defensive programming. make liberal use of runtime assertions to detect ˚aws in the code and incorrect environmental assumptions, and disable them in the deployed code only after careful consideration. assertions that embody preconditions, postconditions, and representation invariants are especially effective.ł logging failures. generate a log of messages that record in detail the circumstances of any detected failure that occurs at runtime. the message log should be examined frequently even if there are no serious failures and should be archived for later analysis.ł testing. automate testing, especially for regressions. use tools to measure test coverage and aim to achieve full coverage of all statements. 30 almost all programming languages that are currently used in industry permit the programmer to write syntactically correct programs whose meaning is uncertain. in c, for example, the language standard allows the order in which the compiled code evaluates the individual elements of an expression to change each time the program is compiled. if an expression contains a function call, and the function changes the value of a variable that is also used in the expression, the value of the expression will depend on the order of evaluation, and the meaning of the program is undened and may change following a recompilation. some language standards attempt to resolve the problem of undened programs by declaring them illegal but leave the compiler writer helpless to tell the programmer that their program is illegal before it is executed. for example, if an attempt is made to store a value in the 11th cell of an array dened in the c language to have 10 elements, the program is illegal and its behavior is undened, but this will not be detected before the program is executed. this weakness in c is at the heart of the notorious ﬁbuffer over˚owsﬂ that create the security vulnerabilities that have been exploited by many viruses and worms. when a programming language standard allows legal programs to have an undened meaning, or where it declares certain programs to be illegal but the illegality cannot practically be determined before the program is executed, the programming language is said to be ﬁunsafe.ﬂ31 for a good example of a small collection of welljustied and easytoapply rules, see g.j. holzmann, 2006, ﬁthe power of ten: rules for developing safety critical code,ﬂ ieee computer 39(6):9597.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.86 software for dependable systemsevery method of an exported application programming interface (api) should be tested. do not regard testing as a tool to eliminate errors; instead, regard it as a quality control tool, and discard software that contains signicant numbers of errors. change the design if necessary to make thorough testing feasible.ł static analysis. the compiler should be used in its strictest mode, and all code should pass without warnings. use a static analysis tool to detect anomalies in the code; several such tools are now readily available. specialized static analyses can establish the absence of certain kinds of security vulnerabilities.ł code review. conduct systematic reviews of all code as early as possible, before the code is placed in the project repository.ł incremental build. integrate the code of a system early and often. include all checking tools in the automatic build process, including static analyses, unit and regression tests, and dependency analyses.processlevel practicesbest practices at the process level include the following:ł process. a robust and clear quality management system that is appropriate to the development organization and the character of the software being developed should be chosen, documented, and adhered to. individuals should be trained in the aspects of the system that are relevant to their roles, and the process should encompass verication that its requirements are being adhered to and a systematic effort to review the costs and benets of the process and improve it as appropriate.32 ł risk management. identify key risks (of failure in development or failures of the product), record them in a risk register (essentially a table of all known risks), and articulate a plan to mitigate them. an incremental approach is most likely to succeed, focusing on major risks early on, developing core features rst (including those that will have a signicant impact on product architecture), and minimizing complexity.ł project planning. the project should have an explicit plan with milestones against which progress is systematically evaluated. the plan should explicitly address the risks identied in the risk register.ł quality planning. the project should have an explicit quality plan that articulates the quality criteria and describes how the quality criteria will be achieved and how the product will be assessed against them.32 key processes that should be dened include specication, design, programming, version control, risk management, reviewing, testing, management of subcontractors, contract reviews, and documentation. iso standard 90003 provides an example of this sort of process denition and management.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.proposed approach 87ł version control. all of the signicant project documents (requirements, designs, code, plans, reports, and so on) should contain a date, version number, and change history and be kept under strict version control. in particular, a standard source code control system should be used that provides versioning, backup, and con˚ict detection.ł bug tracking. all reported bugs should be documented in a database and indexed against the location where they were discovered in the code, design, or other documentation. all bug xes should be fully documented and indexed against the appropriate bug report, and should result in reverication, including reexecution of regression test suites and the creation of new regression test cases. in any important application, each bug should be traced to its origin in the development, and a review should ensue to determine whether there are other similar bugs and what modications to the development process could reduce the likelihood of such bugs occurring in the future.ł phased delivery. deliver a system in phases, with the most basic and important functions delivered in the rst phase and additional functions delivered in later phases, in order to exploit feedback from users and reduce risk.ł independent review. in a critical application, reviews by the development team should be augmented by reviews by an independent party.feasibility of the overall approachthe approach to justiable condence and dependable software proposed in this chapter and the technical practices it involves should be adoptable without signicant risk, because the practices have already been successfully applied by a few companies, as illustrated by the four papers cited in the next three paragraphs. the importance of taking a systems perspective and regarding the direct human users of the computer interface as part of the overall system is widely recognizedšin the aviation industry, for example, the aircraft is seen as a single system and the likelihood of error by the pilot is a factor treated explicitly in the system design and in safety analysis and certication. the importance of simplicity, even in complex applications, has long been understood in highsecurity systems, where the software that protects data integrity and condentiality is kept as simple as possible and often implemented as a ﬁsecurity kernel.ﬂ an example is described in a paper in ieee software.33the benets of exploiting analysis in addition to testing have been demonstrated on several projects reported in the literature. a good analy33 r. chapman and a. hall, 2002, ﬁcorrectness by construction: developing a commercially secure system,ﬂ ieee software (january/february):1825. software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.88 software for dependable systemssis of a number of commercial projects is contained in a paper in ieee transactions on software engineering.34 the importance and effectiveness of capturing environmental assumptions is explained with reference to an ecommerce system, a safety protection system, and a railway signaling system in a 2001 conference report.35evidencebased dependability cases and explicit claims are widely used in safetycritical software development but have also been shown to be costeffective when building commercial applications.36 the common experience, from these reports and others, is that these technical recommendations are practical to adopt and effective in use by experts. as with all engineering, costeffectiveness is a primary objective; making dependability claims explicit allows developers to ensure that they achieve the necessary dependability without overengineering.34 s. king, j. hammond, r. chapman, and a. pryor, 2000, ﬁis proof more costeffective than testing?ﬂ ieee transactions on software engineering 26(8):675686.35 j. hammond, r. rawlings, and a. hall, 2001, ﬁwill it work?ﬂ proceedings of the 5th ieee international symposium on requirements engineering, august.36 adrian hilton, 2003, ﬁengineering software systems for customer acceptance.ﬂ available online at <http://www.praxishis.co.uk/pdfs/customeracceptance.pdf>.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.3 broader issuesthe preceding chapter outlined a comprehensive approach to the development of certiably dependable software. the proposed approach has implications not only for how software is produced and evaluated but also for government policy, legislation, and regulation; education; and research. each of these areas warrants indepth studies of its own, and the committee recognizes that policy prescriptions in particularšespecially in light of the limited data and evidence available in the arena of certiably dependable softwarešcan have complex and unpredictable ramications. the committee has therefore chosen to refrain from making concrete and prescriptive recommendations aimed at particular agencies or specic domains. nevertheless, it seemed useful to outline some of the relevant issues and note areas for further investigation and consideration.transparencydependable systems need dependable components, tools, and software companies, so it is important that customers and users be able to make an informed judgment when choosing suppliers and products. this only becomes possible when the criteria and evidence underlying claims of dependability are transparent.economists have established that if consumers cannot reliably observe quality before they buy, then sellers may get little economic benet from providing higher quality than their competitors, and overall quality can 89software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.90 software for dependable systemsdecline.1 because their reputation will affect future sales, sellers strive to maintain some minimum level of quality. if consumers rely heavily on such branding, though, it becomes more difcult for new rms to enter the market. in this case, the software industry could lose out on quality or other improvements because new and innovative rms had limited means of proving their quality. information asymmetries of this type can be mitigated if dependability claims are explicit and backed by evidence, as long as the evidence is available for inspection by potential buyers.such transparency, in which those claiming dependability for their software make available the details of their claims, criteria, and evidence, is thus essential for providing the correct market conditions under which informed choices can be made and the more dependable suppliers can prosper.to assess the credibility of such details effectively, an evaluator should be able to calibrate not only the technical claims and evidence but also the organization that produced them, because the integrity of the evidence chain is vital and cannot easily be assessed without supporting data. this suggests that data of a more general nature should be made available, including the qualications of the personnel involved in the development; the track record of the organization in providing dependable software, which might include, for example, defect rates on previous projects; and the process by which the software was developed and the dependability argument constructed, which might include process manuals and metrics, internal standards documents, applicable test suites and results, and tools used.a company is likely to be reluctant to reveal data that might be of benet to a competitor or that might tarnish the company™s reputation. it is also likely that demands to publish defect rates would result in careful redenitions of what constitutes a defect. these concerns, however, should not deter users from demanding such information, but the demands should be reasonable, welldened, and commensurate with the dependability claimed and the consequences of failure. the willingness of a supplier to provide such data, and the clarity and integrity of the data that it provides, will be a strong indication of its attitude toward dependability, since a supplier who truly understands the role of evidence in establishing dependability will be eager to provide such evidence, and a supplier who does not understand the need for evidence is unlikely to understand all the other attributes of dependability. one would not expect the users of a commodity operating system for standard ofce purposes to press for such information, although it would 1 see, for example, george a. akerlof, 1970, ﬁthe market for ‚lemons™: quality uncertainty and the market mechanism,ﬂ quarterly journal of economics 84:488500.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.broader issues 91be reasonable to expect lists of all known defects, details of the rate at which new defects were reported, and the rate of repair. in contrast, however, the public might reasonably demand that very detailed information about the construction and validation of an electronic voting system be made publicly available. similarly, patients who receive treatment from a potentially lethal medical device should have access to information about its evaluation just as they have access to information about the side effects and risks of medications.it should be noted that providing direct access to evidence is not the only way that a supplier can signal quality. more widespread use of warranties, for example, would help consumers select the more dependable products and suppliers, so long as the warranties are based on explicit claims about the properties of the software and are not simply a marketing gimmick. industry practice with regard to warranties on commercial software varies widely, with some software developers continuing to disclaim all responsibility for the quality of their products and some routinely warranting turnkey systems against all defects.at the same time, consumer condence is not necessarily a good measure of quality. research into the effects of report cards in the health industry has found mixed results. in one study, consumers were found to base their choice of hmo more on the subjective ratings reported on the cards (which are obtained from consumers themselves and are in˚uenced by factors such as the comfort of waiting rooms and availability of parking) than on objective data (such as mammography rates and other data indicating conformance with best practices).2 similar phenomena seem to apply to consumer choice of software, which may be guided more by supercial convenience factors than by inherent quality. this is not to deny consumers the right to weigh factors as they please in their selection of products, of course, but it does mean that popularity with consumers should not be taken as prima facie evidence of quality.accountability and liabilitywhere there is a need to deploy certiably dependable software, it should always be explicit who is accountable, professionally and legally, for any failure to achieve the declared dependability. one benet of making dependability claims explicit is that accountability becomes possible; without explicit claims, there cannot even be a clear determination of what constitutes failure. such accountability can be made explicit in the 2 dafny leemore and david dranove, 2005, ﬁdo report cards tell consumers anything they don™t already know? the case of medicare hmos,ﬂ national bureau of economic research working paper no. 11420, june.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.92 software for dependable systemspurchase contract, as part of the certication of the software, as part of a professional licensing scheme, or in other ways. however, these are not true alternatives to one another because they interactšfor example, a certication scheme might require the use of licensed staff as lead developers or as certiers. no single solution will meet all the circumstances in which certiably dependable software will be deployed, and accountability regimes should therefore be tailored to suit particular circumstances.at present, it is common for software developers to disclaim, so far as possible, liability for defects in their products to a greater extent than customers and society expect from manufacturers in other industries. clearly, no software should be considered dependable if it is supplied with a disclaimer that releases the manufacturer from providing a warranty or other remedies for software that fails to meet its dependability claims. determining appropriate remedies, however, was beyond the scope of this study and would have required careful analysis of benets and costs, taking into account not only the legal issues but also the state of software engineering, the various submarkets for software, economic impact, and the effect on innovation.certificationto establish that software is dependable involves inspection and analysis of the dependability claim and the evidence that is offered in its support. where the customers of the software are not able to carry out that work themselves (for lack of time or expertise) they will need to involve a third party whose judgment they can rely on to be independent of pressures from the vendor or other parties. evaluating the dependability case is where certication regimes come into play.such independence must be demonstrated if third parties are to be successfully used in this role. thirdparty assessors have been successful in other eldsšthe licensed engineers who carry out certicateofairworthiness inspections on aircraft, for example, and the ﬁauthorized bodiesﬂ who perform inspections in the european rail industryšand there is no fundamental reason that such assessment should not work in the software industry too.certication can take many forms, from selfcertication to independent thirdparty certication by a licensed certication authority. no single certication regime is suitable for all circumstances, so a suitable scheme should be chosen by each customer and vendor to suit the circumstances of the particular requirement for certiably dependable software. industry groups and professional societies should consider developing model certication schemes for their industries, taking account of the detailed recommendations in this report. any certication regime focussoftware for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.broader issues 93ing on dependability should make use of a dependability case, as has been described throughout this report.certication should always explicitly allocate accountability for the failure of the software to meet the claimed dependability requirements. in general, such accountability should lie with the person making the claim for dependabilityšperhaps the software manufacturer, the system manufacturer (especially where cots software has been incorporated in a system), or the certier.evidence and opennessevidence is the central theme of this report. in the arena of particular software products and systems, the committee has argued that condence in the dependability of a system must rest on concrete evidence. and in the broader arena of technology advances, including nding better approaches and methodologies to developing software as well as developing innovative new tools, it has argued that evidence supporting or contradicting particular approaches is an essential enabler of progress. determining whether to build and eld a software system that could offer great benets but also pose a potentially catastrophic risk calls for a plausible and transparent costbenet analysis that explicitly and carefully considers the evidence.in both arenasšindividual software products and technology advancesšthere is currently a dearth of evidence, which seriously hampered the committee™s work, making it hard to resolve debate or reach an informed consensus on some issues. obtaining and recording better evidence is crucial. a key obstacle is a lack of transparency and the inability to look into the system under consideration and see how it was developed. in some cases, evidence is not available. many software developers, for example, are not withholding data but have simply not seriously considered using the evidence they have for evaluating the dependability of their product. in other cases, however, evidence exists but cannot be used effectively because no one who sees it has sufcient expertise.some software producers might be driven to hide evidence that could damage perceptions of their product. but others choose not to disclose evidence because they are reasonably concerned about revealing proprietary information that would aid competitors or because they have no incentive to pay the costs of organizing and disseminating the data. the committee is loath, therefore, to propose regulations or standards that might compel software producers to reveal proprietary information.however, because such evidence would be valuable for the software industry and its consumers, it is important to rescue it from the shadows and make it more available. the committee encourages consumers to software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.94 software for dependable systemsdemand better information about the dependability of software products and to be suspicious of any dependability claims that are not allowed to be evaluated by an independent third party.likewise, the committee encourages those in government who procure and eld critical systems to be skeptical of manufacturers™ claims and to recognize that public scrutiny can be a good thing. in some domains, secrecy will remain important; it would not be sensible, for example, to insist that the designs and dependability cases for defense systems be made public. secrecy, however, is often overrated, and much of the research community has come to believe that secrecy prevents it from examining the mechanism in question, robbing society of the peer review that would otherwise take place. furthermore, the condence of the public might be seriously undermined if important information is withheld by government ofcials that might bear on the decision to eld a system. electronic voting is a prime example of this. despite accusations of serious failures and vulnerabilities in voting software, its manufacturers, along with the state ofcials who award the contracts and are responsible for assessing the dependability of the software, have in some cases been reluctant to give out information that would allow independent experts to make their own judgments and may have forfeited society™s chance to have better software and may even have damaged the credibility of the electoral process itself.3security concernsbecause the committee has argued that the same broad principles should apply to a variety of systems in different application domains, it has not made recommendations specic to any particular area. security, however, demands special consideration, because although security concerns are greater for certain kinds of systems, almost all systems are vulnerable to malicious attack to some degree. effort invested in building a dependability case for a system is much less useful if there are security vulnerabilities that bring into question the most basic assumptions made about the behavior of components and their independence. in short, security vulnerabilities can undermine the entire dependability case and therefore need to be addressed as an integral part of the case. most software systems are networked and therefore open to attack; 3 see, for example, national research council (nrc), 2006, ﬁletter report on electronic voting,ﬂ the national academies press, washington, d.c.; and nrc, 2005, asking the right questions about electronic voting, the national academies press, washington, d.c. available online at <http://books.nap.edu/catalog.php?recordid=11704> and <http://books.nap.edu/catalog.php?recordid=11449>, respectively.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.broader issues 95these clearly need a security audit. for systems that have been isolated, an audit is also likely to be essential, because the inconvenience of isolation is usually a response to the perceived risk of malicious attack.the security of a product or system (and, consequently, certication thereof) involves two somewhat distinct facets of the product or system: (1) the presence of security features such as access controls that allow the owner of the product or system to dene and enforce security policy and (2) the ability of the product or system to resist hostile attack.4 however, it should also be noted that the mere presence of security features is not sufcient in and of itself. indeed, given the increasing complexity of systems and security features, the usability and complexity of security conguration is a signicant concern as well. it is important that it be likely, not just possible, that a system™s administrators will congure its security features correctly. due effort is needed to evaluate and show that feasible and expected congurations do not result in obvious vulnerabilities and to ensure that it is clear to those conguring the system what the appropriate congurations are.the presence and correctness of security features can be certied by measures similar to those used to certify that other functional requirements are present and correct, and such certication is the domain of today™s common criteria (cc). however, certication of the ability to resist attack needs to begin by considering the kinds of attacks that might be directed at the product or system (sometimes referred to as a threat analysis) and then proceeding to review the measures that the developer took to prevent attacks from being successful. this review examines not only the developer™s process but also the effectiveness of the specic techniques that were applied to identify and remove vulnerabilities, and it rests on evidence that the developer in fact applied those techniques thoroughly and effectively.while the cc assess security features, a new paradigm is needed to provide the owners of products and systems with a meaningful certication of resistance to attack. the approach to dependable software that this report proposes is germane to the development of such a certication paradigm. in particular, attention must be paid to articulating and evaluating assumptions about the environment in which the system operates and in which malicious attackers reside. the analysis is harder for security than for other properties, because the interface between the system and the environment is not easily described. this is true in general, of course. a system that controls a motor, for example, might need to account not 4 for a brief overview of cybersecurity issues generally, see nrc, 2002, cybersecurity today and tomorrow: pay now or pay later, the national academies press, washington, d.c. available online at <http://www.nap.edu/catalog.php?recordid=10274>.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.96 software for dependable systemsonly for the electrical load but also for the heating effect of the motor on nearby sensors. in the security realm, however, the concerns are central, since attackers aim to exploit hidden aspects of the interface that a security audit might have neglected. for example, attacks on smartcards, have been devised that rely on monitoring ˚uctuations in the electrical load that the device presents.5 this means that security analysts should always be attentive to the risks of new kinds of attacks, and that security cases should be revisited as new attacks are discovered.a repository of software dependability datatransparency and openness alone are not enough, however. few people have the time and expertise to carefully examine and understand arcane data. developing a substantial repository of credible evidence will require a concerted effort to record, analyze and organize data. such an effort would probably involve at least two distinct components, both aimed at involving software engineering experts more directly in accident analysis and reporting.first, software experts should be actively involved in accident analysis. in many accidents software is either a contributing or a central factor, yet it is common for review panels not to examine the software at a level of detail commensurate with its role. experts in other elds tend to minimize the role of software and underestimate the threats it poses. it is common, for example, to blame users for taking inappropriate actions despite egregious ˚aws in the design of the user interface.6 second, reports of failures and accidents should, whenever possible, be accompanied by the software artifacts themselves so that experts can evaluate a report on the basis of the same evidence that was made available to the report™s authors. concerns about proprietary material and the risk of exposing security vulnerabilities in existing systems should of course be taken into account, but the ease of publishing large artifacts in the era of the web and the value of making the information widely avail5 see, for example, o. kommerling and m. kuhn, 1999, ﬁdesign principles for tamperresistant smartcard processors,ﬂ proceedings of the usenix workshop on smartcard technology (smartcard ™99), chicago, ill., may 1011, usenix association, pp. 920. available online at <http://www.cl.cam.ac.uk/~mgk25/sc99tamper.pdf> for a discussion of various smartcard vulnerabilities.6 the panama radiotherapy accidents are a good example of this phenomenon. see iaea, 2001, ﬁinvestigation of an accidental exposure of radiotherapy patients in panama: report of a team of experts, 26 may1 june 2001,ﬂ iaea, vienna, austria. available online at <http://wwwpub.iaea.org/mtcd/publications/pdf/pub1114scr.pdf>. also, see m.h. lützhöft and s.w.a. dekker, 2002, ﬁon your watch: automation on the bridge,ﬂ journal of navigation 55(1):8396.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.broader issues 97able should make disclosure the default position and place the burden of proof on those who would argue against it.how exactly these goals should be achieved in terms of policy prescriptions is beyond the purview of this report. a centralized approach, in which government agencies (the fda, ntsb, faa, and so on) maintain public databases and supervise the collection and dissemination of data, might make certain aspects of this process, such as crosscomparisons, easier. on the other hand, there is value in decentralized approaches, in which software specialists form local teams that oversee software in particular domains and locations, such as the software oversight committees proposed by gardner and miller for medical software systems.7educationin many high school and indeed some collegelevel programming courses, students are introduced to programming as a mechanistic activity, in which programs are developed by trial and error. such experimentation and exploration can be healthy; as in other elds of design and engineering, exploring new ideas is essential, especially for novices. however, as argued elsewhere in this report, the development of dependable software should ultimately be seen as an engineering activityšas argued elsewhere in this report. thus a curricular emphasis on nding the essence of a program and solving it convincingly is preferable to mastering the accidental intricacies of particular software systems. moreover, the absence of exemplars and overexposure to software that is overly complicated or otherwise poorly designed can make it harder to teach students to appreciate the important qualities of good design, such as clarity, simplicity, and tness to purpose. introducing the notion of dependability in educational contexts would require (1) a recognition of the realworld factors that lead to complexity and (2) discussion of explicit examples of clarity and simplicity in the design of large systems and the tradeoffs involved in their design. in high school computer science education, giving students a foundation in the ideas of dependability would require greater emphasis on programming as a design activity, on the qualities of a good program, and on the process of constructing programs and reasoning about them. the intricacies of the programming language or platform lowlevel execution details would receive less emphasis. programming with an eye toward dependability and a rudimentary dependability case would be used to 7 randolph miller and reed m. gardner, 1997, ﬁrecommendations for responsible monitoring and regulation of clinical software systems,ﬂ journal of the american medical informatics association (4):442457. software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.98 software for dependable systemshelp develop a student™s ability to crystallize ideas and make them precise and to structure and dissect arguments.decisions on the curriculum are often motivated by the desire to make students ﬁcomputer literate.ﬂ the goal is laudable, but it is important that such literacy not be construed merely as familiarity with the details of today™s software products. the ability to operate a computer and use standard desktop applications with basic competency is essential, but it is also important for students to have an understanding of how the computing infrastructure as a whole works and why it sometimes fails.8 computer literacy should not be confused with computer science and software engineering, and it is important that students understand the difference. in addition, mathematics is important for the education of software engineers, especially combinatorics and discrete mathematics, including the theory of sets, relations, and graphs.at the university level, an emphasis on dependability would mean that the software and computer science curriculum should address more explicitly the topics that are the foundation for dependable software. students need to have a broader understanding of the role of software and computers in larger systems and need to be familiar with the basic principles of systems engineering. topics that support dependability include a basic introduction to formal methods, with an emphasis on system modeling rather than proofs of correctness, along with usability and human factors. security and dependability are usually treated as specialized topics, but they should be integrated into the curriculum more fully and encountered by students repeatedly, especially when learning how to program.the mathematical background of students studying computer science and software engineering would need to be expanded to include not only discrete mathematics (set theory and logic) but also probability and statistics, whose importance in many elds of computing is growing and which are particularly important for understanding dependability issues. because the mathematics courses offered to computer science students are often designed with mathematicians in mind rather than engineers, they tend to focus on meta results and proof. most computer science students, especially those interested in software, would benet more from mathematics courses that focus on using mathematical constructs to model and reason about systems.8 for more on the importance of literacy and ˚uency with information technology, see the nrc report being fluent with information technology (national academy press, 1999, pp. 34), which argued that it ˚uency ﬁis fundamentally integrative, calling upon an individual to coordinate information and skills with respect to multiple dimensions of a problem and to make overall judgments and decisions taking all such information into account.ﬂsoftware for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.broader issues 99in software projects, one way to encourage attention to dependability concerns would be to require students to build programs that respond gracefully to unanticipated input as a way of introducing them to the most fundamental principles of building secure software. more generally, students should be encouraged not merely to achieve a running program that passes a suite of tests but also to develop a deeper understanding of why the program works and to assess their condence in its dependability by developing minidependability cases of their own based on an honest appraisal of their own abilities, on the strength of their argument that it works, and on the signicance and likelihood of adverse events in the environment.researchalthough the committee believes that the approach outlined in this report might substantially improve the dependability of software, it recognizes that these measures alone cannot overcome the evergrowing demands for software with more complex functionality, operating in more invasive and critical contexts. major technological advances are therefore essential for the future of the industry. while such advances might be produced by the computer industry alone, its history to date (and the dramatic success of federal investment, for example, in networking) suggests that advances will come more quickly and at lower cost if signicant investments are made in fundamental research. in the united states, the high condence software and systems coordinating group (hcss cg) of the national coordination ofce for networking and information technology research and development (nitrd) coordinates many research activities in areas relevant to this report, focusing on scientic foundations and technologies for innovative systems design, systems and embedded application software, and assurance and verication to enable the routine production of reliable, robust, safe, scalable, secure, stable, and certiably dependable itcentric physical and engineered systems comprising new classes of advanced services and applications. these systems, often embedded in larger physical and it systems, are essential for the operation of the country™s critical societal infrastructures, acceleration of u.s. capability in industrial competitiveness, and optimization of citizens™ quality of life.9 the importance of software dependability suggests that funding could be focused on areas that might lead to more dependable software. 9 for more information, see the nitrd hcss cg home page online at <http://www.nitrd.gov/subcommittee/hcss.html>.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.100 software for dependable systemssome areas that seem to merit attention and follow from the overarching recommendations and approach of this study are covered brie˚y below. they should not be construed as exclusive but as providing an indication of what sorts of research questions the approach raises:ł testing as evidence. testing is currently the most widely used technique for nding bugs in code, and when it is performed systematically and extensively, it can be an important element of a dependability case. as noted earlier in the report, however, it is hard to determine what level of dependability is assured when a system passes a given test suite. clearly, an exhaustive test that covers every state and history that could possibly occur in the eld would be tantamount to proof (and perhaps better). at the other end of the spectrum, passing a few dozen ad hoc tests provides little information about the ˚aws that might remain. the former approach is almost never feasible and the latter is insufcient. the gray area in the middle merits consideration. can concrete dependability claims be based on limited testing? can the absence of certain classes of error be assured by the successful execution of certain test cases? could stronger claims be based on testing if novel forms of coverage (such as execution of all possible traces for a limited heap size or number of context switches) are used? might testing with respect to a known operational prole be substantiated by online monitoring to ensure that the prole used for testing remains an accurate representation of actual operation? although considerable literature on testing exists, there is an opportunity for further research to be undertaken focused specically on methods that create evidence that a system has some explicit dependability properties to a high degree of condence.ł checking code against domainspecic properties. recent years have seen many advances in techniques for automatic code checking, and there is renewed interest in program verication (witness the recent proposal of a grand challenge in this area10). these techniques will be essential to the construction of dependability cases, especially if they are capable of handling domainspecic properties rather than just local properties of the code that cannot be assembled into a systemwide argument for dependability.ł strong languages and tools for independence arguments. as discussed above, the cost of constructing a case for dependability with respect to a particular critical property would be reduced by restricting the codelevel argument to a small proportion of the modules. using unsafe languages compromises any modularity that would otherwise make such an inde10 see c.a.r. hoare, 2003, ﬁthe verifying compiler: a grand challenge for computing research,ﬂ journal of the acm 50(1):6369.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.broader issues 101pendence argument plausible. for example, as noted previously, in a program written in a language such as c, an outofbounds array access can overwrite data structures that are not accessible by name, so that one cannot rely on the use of names to determine how one module might interact with another. research is needed to understand whether using safe languages or other tools could justify independence and help structure dependability arguments, and how independence arguments might be made when there are good reasons to use an unsafe language.ł composing component dependability cases. complex software components are seldom furnished with the information needed to support dependability arguments for the systems that use them. for use within a larger argument, the details of the dependability case of a component need not be known. until recently, there has been little demand for components to be delivered with the claims, arguments, and evidence needed to support the dependability case for a system that uses the component. at lower levels of criticality, and in accidental systems, explicit dependability cases have seldom been constructed, so there has been no perceived need for componentlevel cases. at the other extreme, the dependability cases for systems with highly critical assurance goals (such as airplanes) have focused on the details of their components. in addition, there have been few regulatory mechanisms applicable to such systems to support the use of prequalied critical components that would allow the dependability case for the larger system to use the applicable cases for its components without inquiring into all the details of the components themselves. with greater reuse of components, and a concomitant awareness of the risks involved (especially of using commodity operating systems in critical settings), componentlevel assurance will become an essential activity throughout the industry, and it will be necessary to nd ways to compose the dependability arguments of components into an argument for the system as a whole. the research challenges involve not only investigating how this might be done, but also how to account for, and mitigate, varying levels of condence in the component arguments.ł modeling and reasoning about environments. as explained earlier in this report, the dependability of a system usually rests on assumptions about the behavior of operators and devices in the environment of the system and, more broadly, on the human organization in which the system is deployed. the dependability case should therefore involve reasoning about interactions between the system and its environment. the necessary formal foundations for such reasoning are perhaps already available, since an operator or physical device can be modeled along with the system, for example, as a state machine. it is not clear, however, how to model the environment and structure environmental assumptions; how to account software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.102 software for dependable systemsfor human behavior or larger organizational effects; how to handle normal and malicious users; or how to express crucial properties.ł reasoning about failstop systems. the critical dependability properties of most critical systems will take the form ﬁx should never happen, but if it does, then y must happen.ﬂ for example, the essential property of a radiotherapy machine is that it not overdose the patient. yet some amount of overdose occurs in many systems, and any overdose that occurs must be reported. similarly, any failstop system is built in the hope that certain failures will never occur but is designed to fail in a safe way should they occur. it therefore seems likely that multiple dependability cases are needed, at different levels of assurance, each making different assumptions about which adverse events in the environment and failures in the system itself might occur. the structuring of these cases and their relationship to one another is an important topic of investigation.ł making stronger arguments from weaker ones. a chain can be stronger than even its strongest link if the links are joined in parallel rather than in series. similarly, weaker arguments can be combined to form a single stronger argument. a dependability case will typically involve evidence of different sorts, each contributing some degree of condence to the overall dependability claim. it would be valuable to investigate such combinations, to determine what additional credibility each argument brings, and under what conditions of independence such credibility can be maximized.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.4 findings and recommendationsin this chapter, the committee distills its proposed approach and ndings and brie˚y discusses its recommendations for achieving justiable condence in dependable software systems.findingsimprovements in software development are needed to keep pace with societal demands for software. avoidable software failures have already been responsible for loss of life and for large economic losses. the quality of software produced by the industry is extremely variable, and there is inadequate oversight in some critical areas. unless improvements are made, more pervasive deployment of software in the civic infrastructure1 may lead to catastrophic failures. software has the potential to bring dramatic benets to society, but it will not be possible to realize these benetsšespecially in critical applicationsšunless software becomes more dependable.more data are needed about software failures and the efcacy of development approaches. assessment of the state of the software industry, the risks posed by software, and progress made is currently hampered 1 as an indication of the growth in the pervasiveness of software, the bureau of labor statistics found in 2003 that the output of prepackaged software increased annually by 26.5 percent between 1990 and 2000, growth attributed to ﬁthe increased use of computers and the rising demand for reliable, userfriendly software.ﬂ see <http://www.bls.gov/opub/ted/2003/feb/wk3/art01.htm>. 103software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.104 software for dependable systemsby the lack of a coherent source of information about software failures. careful documentation and analysis of failures has had dramatic impact in other areas. more attention should be paid to the contributions of software to accidents, and repositories of accident reports are needed that include sufcient details to enable the analysis of trends and an evaluation of technologies and methods. without a concerted effort to collect better data, investment in software technology and research may be misdirected, ineffective practices will remain, and adoption of the most effective methods will be hindered. in the absence of a federal initiative, the situation might improve dramatically if all the parties currently involved in software production, regulation, and accident reporting were to monitor systems more pervasively and systematically for failures; involve software experts to a greater degree in the investigation of failures of systems that include software as a component; and insist on greater transparency in every aspect of software development and deployment than is currently expected.recommendationsto builders and users of softwaremake the most of effective software development technologies and formal methods. a variety of modern technologiesšin particular, safe programming languages, static analysis, and formal methodsšare likely to reduce the cost and difculty of producing dependable software. elementary best practices, such as source code control and systematic defect tracking, should be universally adopted, and development organizations that fail to use them should not be regarded as sources of dependable software. advanced practitioners, especially those working in specialized domains, may be justied in creating their own framework of processes and practices that embodies these recommended elements. but those who are not already familiar with the best practices of the industry (described previously) should rst ensure that their developments adhere to these elements and then consider diverging only under extraordinary circumstances. formal methods have been shown to be effective only for small to mediumsized critical systems and have not been widely adopted. furthermore, they require a new mindset and may demand staff with greater expertise, especially in the early stages of development. nevertheless, key elements of formal techniques would aid in the costeffective construction of dependability cases and could be widely applied, especially in combination with the incrementality and minimality encouraged in some development approaches such as those currently labeled ﬁagile.ﬂsoftware for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.findings and recommendations 105follow proven principles for software development. the committee™s proposed approach also includes adherence to the following principles: ł take a systems perspective. a systems perspective should be adopted in which the dependability of software is viewed not in terms of intrinsic properties (such as the incidence of bugs in the code) but in terms of the system as a whole, including interactions among people, process, and technology and encompassing both the physical and organizational environment of the system. engineering of software should be driven by a consideration of risks and their mitigation, and wellestablished risk analysis and reduction techniques that are applied in other domains (such as hazard analysis) should be routinely applied to software. different levels of assurance will be appropriate for different systems and for dependability properties within a single system.ł exploit simplicity. if dependability is to be achieved at reasonable cost, simplicity should become a key goal, and developers and customers must be willing to accept the compromises it entails. unfettered growth in the complexity of the functionality offered is incompatible with dependability. the architecture of the software should re˚ect the prioritization of requirements, ideally so that the critical properties can be established by examining closely only a small portion of the software, relying on independence arguments to account for lack of interference from the remaining portions.make a dependability case for a given system and context: evidence, explicitness, and expertise. a software system should be regarded as dependable only if sufcient evidence is presented to substantiate the dependability claim. the evidence should take the form of a dependability case that explains why the critical properties hold, and it will involve reasoning about both the code and the environmental assumptions. to the extent that this reasoning can be supported by automated tools, it will be more credible. the dependability properties should be explicitly articulated and carefully prioritized; the assumed properties of the environment should be made explicit also. this approach gives considerable leeway to developers to use whatever practices are best suited to the problem at hand. in particular, it allows the use of less robust components and languages at the expense of having to mitigate the risk with a more elaborate dependability argument. despite this ˚exibility, in practice the challenges of developing dependable software are sufciently great that developers will need considerable expertise and will have to justify any deviations from best practices.demand more transparency, so that customers and users can make more informed judgments about dependability. customers and users software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.106 software for dependable systemscan make informed judgments when choosing suppliers and products only if the claims, criteria, and evidence for dependability are transparent. the willingness of a supplier to provide data beyond the dependability case proper (about the qualications of personnel, its track record in providing dependable software, and the process it used) and the clarity and integrity of the data that it provides will be a strong indicator of its attitude toward dependability.make use of but do not rely solely on process and testing. testing will be an essential component of a dependability case but will not in general sufce, because even the largest test suites typically used will not exercise enough paths to provide evidence that the software is correct, nor will they have sufcient statistical signicance for the levels of condence usually desired. testing is a vital aspect of every development, not only because it exposes ˚aws but also because it provides feedback on the quality of the development process. software that fails many test cases probably cannot be made dependable and should perhaps be abandoned. adherence to a particular process will not sufce as evidence either. there is no established universal correlation between process and dependability, although demonstrated adherence to process contributes to the dependability case. in other words, rigorous process is essential for preserving the chain of dependability evidence but is not per se evidence of dependability. without a rigorous process, however, evidence produced by the developers will not be credible, and it is unlikely that the developing organization will be able to identify and correct ˚aws in the way it produces software. an effective process need not be a burdensome one, and too elaborate a process (especially if it requires the production of excessive documentation) can be damaging.base certication on inspection and analysis of the dependability claim and the evidence offered in its support. because testing and process alone are insufcient, the dependability claim will require, in addition, evidence produced by analysis. analysis may involve wellreasoned informal argument, formal proofs of code correctness, and mechanical inference (as performed, for example, by type checkers). indeed, the dependability case for even a relatively simple system will usually require all of these kinds of analysis, and they will need to be tted together into a coherent whole. a developer that uses cots components will either have to demonstrate in the dependability case that their failure will not undermine the crucial dependability properties or will have to incorporate in the case appropriate claims about the properties of the components themselves. absent careful engineering, a system can become as vulnerable as its weakest components, so the inclusion of standard desktop software in critical applications should be carefully examined. where the customer for the software is not able to carry out that work itself software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.findings and recommendations 107(through lack of time or lack of expertise) it will need to involve a third party whose judgment it can rely on to be independent of commercial pressures from the vendor. certication can take many forms, from selfcertication through independent thirdparty certication by a licensed certication authority. include security considerations in the dependability case. by violating assumptions about how components behave, about their interactions, or about the expected behavior of users, security vulnerabilities can undermine the case made for dependability properties. the dependability case must therefore account explicitly for security risks that might compromise its other aspects. it is also important to ensure that security certications give meaningful assurance of resistance to attack. owners of products and systems whose security has been certied expect that if they deploy the products and systems properly, most attacks against those products or systems will fail. today™s security certication regimes do not provide this condence, and new security certication regimes are needed. such certication regimes can be built by applying the other ndings and recommendations of this report, with an emphasis on the role of the environmentšin particular, the assumptions made about the potential actions of a hostile attacker and the likelihood that new classes of vulnerabilities will be discovered and new attacks developed to exploit them.demand accountability and make it explicit. where there is a need to deploy certiably dependable software, it should always be made explicit who is accountable, professionally and legally, for any failure to achieve the declared dependability. at present, it is common for software developers to disclaim liability for defects in their products to a greater extent than customers and society expect from manufacturers in other industries. clearly, no software should be considered dependable if it is supplied with a disclaimer that withholds the manufacturer™s commitment to provide a warranty or other remedies for software that fails to meet its dependability claims. the appropriate scope of remedies was not determined in this study, however, and would require a careful analysis of benets and costs.to agencies and organizations that support  software education and researchthe committee was not constituted or charged to recommend budget levels or to assess tradeoffs between software dependability and other priorities. however, the committee does conclude that the increasing importance of software to society and the extraordinary challenge currently faced in producing software of adequate dependability provide a strong rationale for investment in education and research initiatives. software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.108 software for dependable systemsplace greater emphasis on dependabilityšand its fundamental underpinningsšin the high school, undergraduate, and graduate education of software developers. many practitioners do not have an adequate appreciation of software dependability issues, are not aware of the most effective development practices, or are not capable of applying them appropriately. a focus on dependability considerations in high school, undergraduate, and graduate educational contexts is therefore needed. the importance of dependability for software is not adequately stressed in most degree programs in the united states. more emphasis should be placed on systems thinking; on requirements, specication, and largescale design; on security; on usability; on the development of robust and resilient code; on basic discrete mathematics and statistics; and on the construction and analysis of dependability arguments. federal agencies that support information technology research and development should give priority to basic research to further softwareenabled system dependability, emphasizing a systems perspective and evidence. until there is a dramatic improvement in the methods, languages, and tools of software development, there will be systems that cannot be constructed to appropriate levels of dependability. moreover, even when this is possible, the cost will be higher than it should be. because of the increasing importance of software to our society and the extraordinary challenge of producing software of adequate dependability, research is needed that emphasizes a systems perspective and ﬁthe three e™s,ﬂ and such research should be a priority for funding agencies. the research should be informed by a systems view that assigns greater value to advances that are likely to have an impact in a world of large systems interacting with other systems and operators in a complex physical environment and organizational context.* * *the committee believes that the approach discussed here will substantially improve the dependability of many critical software systems being produced today. while the economic tradeoffs are different in individual cases, the committee believes that its recommendations are generally applicable to many nonsafetycritical systems as wellša consideration that becomes increasingly important as cots components are reused in critical systems and accidental systems are formed from a mix of critical and noncritical components. applying the committee™s approach to all software systems, safetycritical and nonsafetycritical alike, promises to alleviate the heavy costs and frustrations that lowquality software imposes even in noncritical applications.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.findings and recommendations 109in the long term, innovations in software engineering are likely to bring dramatic improvements in dependability. software systems are complex and, just as in other sorts of complex systems, failures will inevitably occur. but if our society succeeds in this ambitious program, we can hope that, 10 or 20 years from now, the adoption of ambitious and potentially dangerous new systems will be justied by rational arguments; a broad consensus in the software industry will guide standard practice; the production of software will be less expensive and more predictable than it is today; and the incidence of software failures will be low and welldocumented.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.5 bibliographyadams, e. 1984. ﬁoptimizing preventive service of software products.ﬂ ibm journal of research 28(1):214.akerlof, george a. 1970. ﬁthe market for ‚lemons™: quality uncertainty and the market mechanism.ﬂ quarterly journal of economics 84(3):488500.alvesfoss, jim, bob rinker, and carol taylor. 2002. ﬁmerging safety and assurance: the process of dual certication for faa and the common criteria.ﬁ available online at <http://www.csds.uidaho.edu/comparison/slides.pdf>.amey, peter. 2002. ﬁcorrectness by construction: better can also be cheaper,ﬂ crosstalk magazine, the journal of defence software engineering, march. available online at <http://www.praxishis.com/pdfs/cbycbettercheaper.pdf>.avizienis, a., j.c. laprie, b. randell, and c. landwehr. 2004. ﬁbasic concepts and taxonomy of dependable and secure computing.ﬂ ieee transactions on dependable and secure computing 1(1):1133.barnes, john. 2003. high integrity software: the spark approach to safety and security. addisonwesley, boston, mass.bbc news. 2005. ﬁhospital struck by computer virus.ﬂ august 22. available online at <http://news.bbc.co.uk/1/hi/england/merseyside/4174204.stm>.beck, kent. 1999. extreme programming explained: embrace change. addisonwesley, new york. besnard, d., c. gacek, and c.b. jones, eds. 2006. structure for dependability, springerverlag, new york.boyapati, chandrasekhar, sarfraz khurshid, and darko marinov. 2002. ﬁkorat: automated testing based on java predicates.ﬂ acm/sigsoft international symposium on software testing and analysis, rome, italy. july.butler, r., and g. finelli. 1993. ﬁthe infeasibility of quantifying the reliability of lifecritical realtime software.ﬂ ieee transactions on software engineering 19(1):312.chapman, r., and a. hall. 2002. ﬁcorrectness by construction: developing a commercially secure system.ﬂ ieee software (january/february):1825.110software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.bibliography 111chillarege, r. 1999. ﬁsoftware testing best practices,ﬂ ibm technical report rc 21457 log 96856.civil aviation authority. 2003. ﬁcap 670: air trafc services safety requirements.ﬂ available online at <http://www.caa.co.uk/docs/33/cap670.pdf>.computer economics. 2003. ﬁvirus attack costs on the risešagain.ﬂ available online at <http://www.computereconomics.com/article.cfm?id=873>.cook, richard, and michael o™connor. forthcoming. ﬁthinking about accidents and systems,ﬂ in improving medication safety, k. thompson and h. manasse, eds. american society of healthsystem pharmacists, washington, d.c.cook, r., and j. rasmussen. 2005. ﬁgoing solid: a model of system dynamics and consequences for patient safety.ﬂ quality and safety in health care 14(2):130134.cook, r.i., d.d. woods, and m.b. howie. 1992. ﬁunintentional delivery of vasoactive drugs with an electromechanical infusion device.ﬂ journal of cardiothoracic and vascular anesthesia 6:238244.cook, r.i., d.d. woods, and c. miller. 1998. ﬁa tale of two stories: contrasting views on patient safety.ﬂ national patient safety foundation, chicago, ill., april. available online at <http://www.npsf.org/exec/report.html>.cusumano, michael a., and david b. yofe. 1998. competing on internet time: lessons from netscape and its battle with microsoft. free press, new york.dahl, o.j., e.w. dijkstra, and c.a.r. hoare. 1972. structured programming. academic press, new york.department of transportation, ofce of the inspector general. 2005. ﬁstatus of faa™s major acquisitions: cost growth and schedule delays continue to stall air trafc modernization.ﬂ report number av2005061, may 26.dornheim, michael a. 2005. ﬁcodes gone awry.ﬂ aviation week & space technology, february 28, p. 63.faa (federal aviation administration). 2003. charter for the certication process study (cps) response aviation rulemaking committee. january 16.faa. 2004. ﬁreusable software componentsﬂ (ac 20148). faa, washington, d.c. available online at <http://www.airweb.faa.gov/regulatoryandguidancelibrary/rgadvisorycircular.nsf/0/ebfccb29c0e78fff86256f6300617bdd?opendocument>.fda (food and drug administration). 2002. ﬁgeneral principles of software validation; final guidance for industry and fda staff.ﬂ available online at <http://www.fda.gov/cdrh/comp/guidance/938.html>.fda. 2003. ﬁwhy is human factors engineering important for medical devices?ﬂ available online at <http://www.fda.gov/cdrh/humanfactors/important.html>.fenton, n.e., and m. neil. 1998. ﬁa strategy for improving safety related software engineering standards.ﬂ ieee transactions on software engineering 24(11):10021013.fitzgibbon, chris. 1998. ﬁimpact of iso 9001 on software quality.ﬂ capital quality news. available online at <http://www.orioncanada.com/impact.htm>.frankl, phyllis g., and elaine j. weyuker. 1993. ﬁa formal analysis of the faultdetecting ability of testing methods.ﬂ ieee transactions on software engineering 19(3):202213.freeman, sholnn. 2005. ﬁtoyota attributes prius shutdowns to software glitch.ﬂ wall street journal, may 16. available online at <http://online.wsj.com/articleprint/sb111619464176634063.html>.gacek, cristina, and budi arief. 2004. ﬁthe many meanings of open source.ﬂ ieee software 21(1):3440.gage, deborah, and john mccormick. 2004. ﬁwe did nothing wrong.ﬂ baseline, march 4. available online at <http://www.baselinemag.com/article2/0,1540,1543571,00.asp>.gao (general accounting ofce). 1986. ﬁmedical devices: early warning of problems is hampered by severe underreporting,ﬂ u.s. government printing ofce, washington, d.c. gao publication pemd871.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.112 software for dependable systemsgao. 1992. patriot missile software problem. report of the information management and technology division. available online at <http://www.fas.org/spp/starwars/gao/im92026.htm>.gao. 2003. ﬁtactical aircraft, status of the f/a22 program: statement of allen li, director, acquisition and sourcing management.ﬂ gao33603t. april 2.gardner, reed m., and randolph miller. 1997. ﬁrecommendations for responsible monitoring and regulation of clinical software systems.ﬂ annals of internal medicine 127(9): 842845. geppert, l. 2004. ﬁlost radio contact leaves pilots on their own,ﬂ ieee spectrum 41(11):1617, november.german, andy, and gavin mooney. 2001. ﬁair vehicle software static code analysisšlessons learnt.ﬂ proceedings of the ninth safetycritical systems symposium. felix redmill and tom anderson, eds. springerverlag, bristol, united kingdom. glass, robert l. 2005. ﬁit failure ratesš70 percent or 1015 percent?ﬂ ieee software 22(3):112.goetz, brian, tim peierls, joshua bloch, joseph bowbeer, david holmes, and doug lea. 2006. java concurrency in practice. addisonwesley, boston, mass.gps news. 2004. ﬁtanker truck shutdown via satellite.ﬂ available online at <http://www.spacedaily.com/news/gps03zn.html>.greenwell, william s., and john c. knight. 2005. ﬁwhat should aviation safety incidents teach us?ﬂ technical report cs200312. university of virginia, charlottesville, va. available online at <http://www.cs.virginia.edu/~techrep/cs200312.pdf>.grimaldi, james v., and guy gugliotta. 2001. ﬁchemical plants feared as targets.ﬂ washington post, december 16, p. a01. guth, robert. 2003. ﬁmake software more reliable.ﬂ the wall street journal. november 17.guttman, william. 2002. ﬁthe private sector: sustainable computing.ﬂ pittsburgh postgazette, december 10. available online at <http://www.postgazette.com/businessnews/20021210forumguttmanp6.asp>.hall, anthony. 1996. ﬁusing formal methods to develop an atc information system.ﬂ ieee software 13(2):6676.hammond, j., r. rawlings, and a. hall. 2001. ﬁwill it work?ﬂ proceedings of the 5th ieee international symposium on requirements engineering, august.hilton, adrian. 2003. ﬁengineering software systems for customer acceptance.ﬂ available online at <http://www.praxishis.co.uk/pdfs/customeracceptance.pdf>.hinchey, michael g., and jonathan p. bowen, eds. 1999. industrialstrength formal methods in practice. springer, london, united kingdom.hoare, c.a.r. 1981. ﬁthe emperor™s old clothesﬂ (turing award lecture), communications of the acm 24(2):7583. available online at <http://portal.acm.org/citation.cfm?id=358561>.hoare, c.a.r. 1996. ﬁhow did software get so reliable without proof?ﬂ lecture notes in computer science 1051:117. hoare, c.a.r.. 2003. ﬁthe verifying compiler: a grand challenge for computing research.ﬂ journal of the acm 50(1):63œ69.hollnagel, e., d.d. woods, and n. leveson, eds. 2006. resilience engineering: concepts and precepts. ashgate, aldershot, united kingdom.holzmann, g.j. 2006. ﬁthe power of ten: rules for developing safety critical code.ﬂ ieee computer 39(6):9597.iaea (international atomic energy agency). 2001. ﬁinvestigation of an accidental exposure of radiotherapy patients in panama: report of a team of experts, 26 may1 june 2001.ﬂ iaea, vienna, austria. available online at <http://wwwpub.iaea.org/mtcd/publications/pdf/pub1114scr.pdf>. software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.bibliography 113institute of medicine. 2000. to err is human: building a safer health system. national  academy press, washington, d.c. available online at <http://books.nap.edu/catalog.php?recordid=9728>.instrument society of america. 1996. ﬁapplication of safety instrumented systems for the process industries.ﬂ isas84.011996 (s84.01). jackson, d., and j. wing. 1996. ﬁlightweight formal methods.ﬂ ieee computer magazine 29(4)2122.jackson, michael. 1996. software requirements & specications. addisonwesley and acm press.jackson, michael. 2000. ﬁthe real world.ﬂ in millennial perspectives in computer science: proceedings of the 1999 oxfordmicrosoft symposium in honour of c a r hoare, jim davies, bill roscoe, and jim woodcock, eds. palgrave macmillan.jackson, michael. 2001. problem frames: analysing and structuring software development problems. addisonwesley, boston, mass.jackson, michael. 2004. ﬁseeing more of the world.ﬂ ieee software 21(6):8385. available online at <http://mcs.open.ac.uk/mj665/seemore3.pdf>.johnson, c.w. 2003. failure in safetycritical systems: a handbook of accident and incident reporting. university of glasgow press, glasgow, scotland.keizer, gregg. 2004. ﬁunprotected pcs fall to hacker bots in just four minutes.ﬂ tech web, november 30. available online at <http://www.techweb.com/wire/security/54201306>.khurshid, s., and d. marinov. 2004. ﬁtestera: specicationbased testing of java programs using sat.ﬂ automated software engineering journal 11(4):403434.kilbridge, peter. 2003. ﬁcomputer crash: lessons from a system failure.ﬂ new england journal of medicine 348(march 6):881882.king, s., j. hammond, r. chapman, and a. pryor, eds. 2000. ﬁis proof more costeffective than testing?ﬂ ieee transactions on software engineering 26(8):675686.knight, john c. 2002. ﬁsoftware challenges in aviation systems.ﬂ lecture notes in computer science 2434:106112. koppel, ross, joshua p. metlay, abigail cohen, brian abaluck, a. russell localio, stephen e. kimmel, and brian l. strom. 2005. ﬁrole of computerized physician order entry systems in facilitating medication errors.ﬂ journal of the american medical association 293(10):11971203.kommerling, o., and m. kuhn. 1999. ﬁdesign principles for tamperresistant smartcard processors,ﬂ proceedings of the usenix workshop on smartcard technology (smartcard ™99), chicago, ill., may 1011. usenix association. available online at <http://www.cl.cam.ac.uk/~mgk25/sc99tamper.pdf>ladkin, peter, translator. 1994. translation of report on the accident to airbus a320211 aircraft in warsaw on 14 september 1993. main commission, aircraft accident investigation, warsaw. available online at <http://www.rvs.unibielefeld.de/publications/incidents/docs/comandrep/warsaw/warsawreport.html>.lampson, butler. 1983. ﬁhints for computer system design.ﬂ acm operating systems review 17(5):3348. reprinted in ieee software 1(1):1128. available online at <http://research.microsoft.com/lampson/33hints/webpage.html>.layton, c., p.j. smith, and c.e. mccoy. 1994. ﬁdesign of a cooperative problemsolving system for enroute flight planning: an empirical evaluation.ﬂ human factors 36:94119.lee, insup, and george pappas. 2005. final report of high condence medical device software and systems (hcmdss) workshop, philadelphia, pa., june 23. available online at <http://rtg.cis.upenn.edu/hcmdss/hcmdssnalreport060206.pdf>. leemore, dafny, and david dranove. 2005. ﬁdo report cards tell consumers anything they don™t already know? the case of medicare hmos,ﬂ nber working paper no. 11420. national bureau of economic research, cambridge, mass.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.114 software for dependable systemsleveson, nancy. 1995. safeware: system safety and computers. addisonwesley, boston, mass.leveson, nancy, and clark s. turner. 1993. ﬁan investigation of the therac25 accidents.ﬂ ieee computer 26(7):1841.levy, matthys, and mario salvadori. 1992. why buildings fall down. w.w. norton & company, new york.lin, l., r. isla, k. doniz, h. harkness, k. vicente, and d. doyle. 1998. ﬁapplying human factors to the design of medical equipment: patient controlled analgesia.ﬂ journal of clinical monitoring 14:253263.lin, l., k. vicente, and d.j. doyle. 2001. ﬁpatient safety, potential adverse drug events, and medical device design: a human factors engineering approach.ﬂ journal of biomedical informatics 34(4):274284.lions, j.l. 1996. ﬁariane 5: flight 501 failure.ﬂ report by the inquiry board. available online at <http://www.cs.unibo.it/~laneve/papers/ariane5rep.html>.litcheld, david. 2006. ﬁwhich database is more secure? oracle vs. microsoft.ﬂ ngssoftware insight security research. available online at <http://www.databasesecurity.com/dbsec/comparison.pdf>.littlewood, b., and l. strigini. 1993. ﬁvalidation of ultrahigh dependability for softwarebased systems.ﬂ communications of the acm 36(11):6980.loeb, vernon. 2002. ﬁ‚friendly fire™ deaths traced to dead battery: taliban targeted, but u.s. forces killed.ﬂ washington post, march 24, p. a21.lohr, steve. 2003. ﬁ2 companies to announce u.s. clearance for linux security.ﬂ new york times, august 5. available online at <http://www.nytimes.com/2003/08/05/technology/05blue.html>.lynuxworks. 2002. ﬁlynuxworks to offer first do178b certiable posix rtos.ﬂ december 10. available online at <http://www.lynuxworks.com/corporate/news/press/2002/121002a.php3>.mackenzie, donald. 2001. mechanizing proof: computing, risk, and trust. mit press, cambridge, mass.maisel, william h., michael o. sweeney, william g. stevenson, kristin e. ellison, and laurence m. epstein. 2001. ﬁrecalls and safety alerts involving pacemakers and implantable cardioverterdebrillator generators.ﬂ journal of the american medical association 286:793799.michaels, daniel, and andy pasztor. 2006. ﬁincidents prompt new scrutiny of airplane software glitches.ﬂ wall street journal, may 30, p. a1.miller, randolph, and reed m. gardner. 1997. ﬁrecommendations for responsible monitoring and regulation of clinical software systems.ﬂ journal of the american medical informatics association (4):442457.montgomery, kathryn. 2006. how doctors think, clinical judgment and the practice of medicine. oxford university press, oxford, united kingdom.naur, p., and b. randell, eds. 1969. software engineering: report on a conference sponsored by the nato science committee. garmisch, germany, october 711. nato scientic affairs division, brussels, belgium. available online at <http://homepages.cs.ncl.ac.uk/brian.randell/nato/>.nebeker, jonathan r., jennifer m. hoffman, charlene r. weir, charles l. bennett, and john f. hurdle. 2005. ﬁhigh rates of adverse drug events in a highly computerized hospital.ﬂ archives of internal medicine 165:11111116.nrc (national research council). 1999a. being fluent with information technology. national academy press, washington, d.c. available online at <http://books.nap.edu/catalog.php?recordid=6482>.nrc. 1999b. trust in cyberspace. national academy press, washington, d.c. available online at <http://books.nap.edu/catalog.php?recordid=6161>.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.bibliography 115nrc. 2002. cybersecurity today and tomorrow: pay now or pay later. the national academies press, washington, d.c. available online at <http://www.nap.edu/catalog.php?recordid=10274>.nrc. 2003a. critical information infrastructure protection and the law: an overview of key issues. the national academies press, washington, d.c. available online at <http://books.nap.edu/catalog.php?recordid=10274>.nrc. 2003b. the internet under crisis conditions: learning from september 11. the national academies press, washington, d.c. available online at <http://books.nap.edu/catalog.php?recordid=10685>.nrc. 2004. summary of a workshop on software certication and dependability. the national academies press, washington, d.c. available online at <http://books.nap.edu/catalog.php?recordid=10569>.nrc. 2005. asking the right questions about electronic voting. the national academies press, washington, d.c. available online at <http://www.nap.edu/catalog.php?recordid=11449>.nrc. 2006. ﬁletter report on electronic voting.ﬂ the national academies press, washington, d.c. available online at <http://www.nap.edu/catalog.php?recordid=11704>.nunnally, m., c.p. nemeth, v. brunetti, and r.i. cook. 2004. ﬁlost in menuspace: user interactions with complex medical devices.ﬂ ieee transactions on systems, man and cyberneticsšpart a: systems and humans 34(6):736742.olavsrud, thor. 2003. ﬁwhite house email system slows to a crawl.ﬂ dc.internet.com, july 18. available online at <http://dc.internet.com/news/article.php/2237391>.page, d., p. williams, and d. boyd. 1993. report of the inquiry into the london ambulance service, communications directorate, south west thames regional health authority, london, february. available online at <http://www.cs.ucl.ac.uk/staff/a.finkelstein/las/lascase0.9.pdf>.parnas, d.l., and j. madey. 1995. ﬁfunctional documentation for computer systems.ﬂ science of computer programming 25(1):4161.perlis, alan j. 1982. ﬁepigrams on programming.ﬂ sigplan notices 17(9):713.perrow, charles. 1999. normal accidents. princeton university press, princeton, n.j.perrow, charles. 2007. the next catastrophe: reducing our vulnerabilities to natural, industrial, and terrorist disasters. princeton university press, princeton, n.j.perry, shawna j., robert l. wears, and richard i. cook. 2005. ﬁthe role of automation in complex system failures.ﬂ journal of patient safety 1(1):5661.petroski, henry. 2004. to engineer is human. st martin™s press, new york.p˚eeger, shari lawrence. 1998. ﬁunderstanding and improving technology transfer in software engineering.ﬂ report dacssoar981. dod data and analysis center for software. p˚eeger, shari lawrence, and les hatton. 1997. ﬁinvestigating the in˚uence of formal methods.ﬂ ieee computer 30(2):3343.research triangle institute. 2002. the economic impacts of inadequate infrastructure for software testing (final report). prepared for gregory tassey, national institute of standards and technology, acquisition and assistance division. available online at <http://www.rti.org/pubs/softwaretesting.pdf>.rice, lynne l., and andrew lowery. 1995. ﬁpremarket notication 510(k): regulatory requirements for medical devices.ﬂ division of small manufacturers assistance. u.s. department of health and human services, publication fda 954158. center for devices and radiological health. available online at <http://www.fda.gov/cdrh/devadvice/314.html>.rubin, avi, tadayoshi kohno, adam stubbleeld, and dan s. wallach. 2004. ﬁanalysis of an electronic voting system.ﬂ ieee symposium on security and privacy, oakland, calif. available online at <http://avirubin.com/vote.pdf>. software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.116 software for dependable systemssarter, n., and d.d. woods. 1995. ﬁhow in the world did we get into that mode? mode error and awareness in supervisory control.ﬂ human factors 37:519.sarter, n., d.d. woods, and c. billings. 1997. ﬁautomation surprises,ﬂ handbook of human factors/ergonomics, g. salvendy, ed., 2nd ed., wiley, new york, pp. 19261943. (reprinted in n. moray, ed., ergonomics: major writings, taylor & francis, boca raton, fla., 2004.)schneider, fred. 2000. ﬁenforceable security policies.ﬂ acm transactions on information and system security (tissec) 3(1):3050.sha, lui. 2001. ﬁusing simplicity to control complexity.ﬂ ieee software 18(4):2028. shankland, stephen. 2003. ﬁsuse linux gets security credentials.ﬂ cnet news.com, august 5. available online at <http://news.com.com/2100101635059846.html?tag=fdtop>.shooman, m.l. 1996. ﬁavionics software problem occurrence rates.ﬂ the seventh international symposium on software reliability engineering (issre ™96), p. 55. available online at <http://doi.ieeecomputersociety.org/10.1109/issre.1996.558695>.shortliffe, edward h. 2005. ﬁstrategic action in health information technology: why the obvious has taken so long.ﬂ health affairs 24(5):12221233. slabodkin, gregory. 1998. ﬁsoftware glitches leave navy smart ship dead in the water.ﬂ government computer news, july 13. available online at <http://www.gcn.com/print/1717/337271.html>.smith, p.j., e. mccoy, and c. layton. 1997. ﬁbrittleness in the design of cooperative problemsolving systems: the effects on user performance.ﬂ ieee transactions on systems, man and cyberneticsšpart a 27(3):360371.starbuck, w.h., and m. farjoun, eds. 2005. organization at the limit: nasa and the columbia disaster. blackwell, malden, mass.ﬁtanker truck shutdown via satellite.ﬂ 2004. gps news, november 4. available online at <http://www.spacedaily.com/news/gps03zn.html>. taylor, andrew. 2001. ﬁit projects sink or swim,ﬂ based on author™s m.b.a. dissertation, bcs review.thibodeau, patrick. 2003. ﬁnasa leads efforts to build better software.ﬂ computerworld, february 7. available online at <http://www.computerworld.com/softwaretopics/software/story/0,10801,78362,00.html>.tiernan, ray. 2003. ﬁwhen computing was reliable.ﬂ osopinion.com, march 17.trimble, stephen. 2005. ﬁavionics redesign aims to improve f/a22 stability.ﬂ flight international, august 23.verton, dan. 2003. ﬁgao reports focused on nasa it workforce issues.ﬂ computerworld, february 4. available online at <http://www.computerworld.com/careertopics/careers/labor/story/0,10801,78172,00.html>.wall, robert. 2003. ﬁcode red emergency.ﬂ aviation week & space technology, june 9, pp. 3536.wears, robert l., and marc berg. 2005. ﬁcomputer technology and clinical work: still waiting for godot.ﬂ journal of the american medical association 293:12611263.weaver, nicholas, and vern paxson. 2004. ﬁa worstcase worm.ﬂ paper presented at the third annual workshop on economics and information security (weis04), march 1314. available online at <http://www.dtc.umn.edu/weis2004/weaver.pdf>.williams, laurie, robert r. kessler, ward cunningham, and ron jeffries. 2000. ﬁstrengthening the case for pair programming.ﬂ ieee software 17(4):1925.woods, d.d., and e. hollnagel. 2006. joint cognitive systems: patterns in cognitive systems engineering. taylor & francis, boca raton, fla.yurcik, william, and david doss. 2001. ﬁachieving faulttolerant software with rejuvenation and reconguration.ﬂ ieee software 18(4):4852.yurcik, william, and david doss. 2002. ﬁsoftware technology issues for a u.s. national missile defense system.ﬂ ieee technology and society magazine 21(2):3646. software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved. appendixessoftware for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.a biographies of committee  members and staffcommittee membersdaniel jackson (chair) is a professor of computer science at the massachusetts institute of technology (mit). he received an m.a. from oxford university (1984) in physics and an s.m. (1988) and ph.d. (1992) from mit in computer science. he was a software engineer for logica uk ltd. (19841986) and an assistant professor of computer science at carnegie mellon university (19921997). he has broad interests in many areas of software engineering, especially in specication and design, critical systems, formal methods, static analysis, and model checking. dr. jackson is the author of software abstractions: logic, language, and analysis (mit press, 2006). joshua bloch is a principal software engineer at google. previously he was a distinguished engineer at sun microsystems, where he was an architect in the core java platform group. he wrote the bestselling book effective java, winner of the 2002 jolt award. he led the design and implementation of many parts of the java platform, including the collections framework, tiger language enhancements (jsr201), annotations (jsr175), multiprecision arithmetic, preferences (jsr10), and assertions (jsr41). previously he was a senior systems designer at transarc corporation, where he designed and implemented many parts of the encina distributed transaction processing system. he holds a ph.d. in computer science from carnegie mellon university and a b.s. in computer science from columbia university.119software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.120 software for dependable systemsmichael dewalt is chief scientist, aviation systems, for certication services, inc., a seattlearea aviation consultancy. mr. dewalt is authorized by the faa, as a consultant designated engineering representative (der), to approve software for any aircraft system, at any software level. in addition to his der duties, he helps clients who have unusual project requirements to develop acceptable softwareapproval techniques. for 11 years, he was the faa™s national resource specialist for aircraft software. he was responsible for starting the international committee that created do178b and served as its secretary. he was also secretary of the committee that created do248b and do278. mr. dewalt has been involved with both civil and military software avionics and certication for 26 years, working for airframe manufacturers and avionics suppliers. in addition to his der certicate, he has a b.s.e.e., a master™s in software engineering, and a commercial pilot™s license.reed gardner is a professor and chair of the department of medical informatics at the university of utah. he has been a codirector of medical computing at lds, cottonwood, and alta view hospitals in salt lake city. he is one of the principal developers and evaluators of the medical expert system known as help (health evaluation through logical processing). dr. gardner™s primary academic and research interests are evaluating the benets of medical expert systems as they relate to quality and costeffectiveness; development of software oversight committee methods for evaluation of safety and effectiveness of medical software and systems; public health informatics; applying computers in intensivecare medicine; and developing devices and communications methods to acquire patient data at the bedside. he is the author or coauthor of more than 300 articles in the elds of medical informatics and engineering. dr. gardner has been a journal editor and on the editorial boards of critical care medicine and other critical care journals as well as the journal of the american medical informatics association (jamia). he is a fellow of the american college of medical informatics and past president of the american medical informatics association. dr. gardner holds a b.s.e.e. from the university of utah (1960) in electrical engineering and a ph.d. from the university of utah (1968) in biophysics and bioengineering.peter lee is a professor of computer science at carnegie mellon university. he joined the faculty of carnegie mellon™s school of computer science in 1987, after completing his doctoral studies at the university of michigan. he is known internationally for his research contributions in areas related to information assurance, especially the application of programming language technology to operating systems design, networking, and computer security. dr. lee is best known for his coinvention of the software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.appendix a 121proofcarrying code technology for ensuring the security of mobile code. today, proofcarrying code is the subject of several darpa and nsfsponsored research projects and forms the basis for the products and services provided by cedilla systems incorporated, a java technology startup company he cofounded in 1999. dr. lee is also the associate dean for undergraduate education in carnegie mellon™s school of computer science. in this capacity, he has been involved in the administration of carnegie mellon™s undergraduate programs in computer science. his tenure as associate dean has seen the undergraduate program rise to national prominence, both for its intensive problemoriented curriculum and for its success in attracting and retaining women in the eld of computer science. he has published extensively in major international symposia and is the author of two books. he has been invited to give distinguished lectures and keynote addresses at major universities and symposia and has been called on as an expert witness in key judicial court cases such as the sun v. microsoft ﬁjava lawsuit.ﬂ dr. lee has also been a member of the army science board since 1997, for which he has served on four major summer studies, and the cochair of a technology panel for the 2001 defense science board study on defense science and technology. in addition to holding m.s. and ph.d. degrees in computer and communication sciences, dr. lee earned a b.s. in mathematics from the university of michigan in 1982. he has been a principal investigator on several darpa, nsf, and nasa grants and contracts.steven b. lipner is senior director of security engineering strategy at microsoft. he is responsible for dening microsoft™s security development lifecycle and the plans for its evolution and application to new product generations. his team also denes and executes programs to help microsoft customers deploy and operate their systems securely. mr. lipner has been at microsoft since 1999. he joined the company after working at trusted information systems, the mitre corporation, and digital equipment corporation, among others. he has more than 35 years™ experience in computer and network security as a researcher, development manager, and business unit manager. he holds 11 patents in computer and network security and served two terms as a member of the u.s. information security and privacy advisory board. mr. lipner is coauthor with michael howard of the security development lifecycle. he holds an m.s. (1966) in civil engineering from mit and attended the program for management development at the harvard graduate school of business administration.charles perrow is a professor emeritus of sociology at yale university. he was a vice president of the eastern sociological society; a fellow of software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.122 software for dependable systemsthe center for advanced study in the behavioral sciences; fellow of the american academy for the advancement of science; resident scholar at the russell sage foundation; fellow, shelly cullom davis center for historical studies; visitor, institute for advanced study; and a former member of the national research council™s committee on human factors, the sociology panel of the national science foundation, and of the editorial boards of several journals. an organizational theorist, he is the author of six booksšthe radical attack on business; organizational analysis: a sociological view; complex organizations: a critical essay; normal accidents: living with high risk technologies; the aids disaster: the failure of organizations in new york and the nation, with mauro guillen; organizing america: wealth, power, and the origins of american capitalismšand over 50 articles. his current interests are in managing complexly interactive, tightly coupled systems (including hospitals, nuclear plants, power grids, the space program, and intelligent transportation systems); the challenge and limits of networkcentric warfare; selforganizing properties of the internet, the electric power grid, networks of small rms, and terrorist organizations; and the possibilities for restructuring society to reduce our increasing vulnerability to disasters, whether natural, industrial/technological, or deliberate. these interests grow out of his work on ﬁnormal accidents,ﬂ with its emphasis on organizational design and systems theory, and re˚ect current consultations and workshops with nasa, the faa, naval war college, daimlerchrysler, nih, and nsf.jon pincus is the general manager of strategy development in microsoft™s online services group, where he leads a broadbased effort to develop, analyze, work for the adoption of, and execute gamechanging strategies in the online services space. key principles include a global focus, usercentricity, attention to perspectives other than the usual ones, virtuouscycle ecosystems, and leveraging microsoft™s assets. in his previous role in the systems and networking group at microsoft research, he focused on security, privacy, and reliability of software and softwarebased systems. his major interests include applying perspectives and insights from the social sciences and humanities to the construction and application of these systems (which inevitably blends into cultural issues throughout the disciplines of software engineering and computer science, as well as at microsoft and other organizations that produce software and systems); measurement of security and privacy; and the exploitation and mitigation of lowlevel programming defects such as buffer overruns. in his premicrosoft days, he was founder and chief technology ofcer at intrinsa, which was acquired by microsoft in 1999 along with prex and the rest of the company™s assets. he has also worked in design automation (placesoftware for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.appendix a 123ment and routing for ics and cad frameworks) at ge calma and eda systems. john rushby is program director for formal methods and dependable systems at sri international. he worked at the atlas computer laboratory (now part of the computation and information department of the central laboratory of the u.k. research councils), as a lecturer in the computer science department at manchester university, and as a research associate in the department of computing science at the university of newcastle upon tyne, before joining sri in 1983. at sri, he was successively promoted to computer scientist, senior computer scientist, program manager and, from 1986 to 1990, the acting director of csl. in 1991 dr. rushby assumed his current role as program director. he is interested primarily in the design and assurance of critical systems, including properties such as security and safety, mechanisms such as kernelization and fault tolerance, and formal methods for assurance. he considers the main value of formal methods to lie in their use for constructing mathematical models whose properties can be analyzed and veried by computational means. this has led him to focus on the development of effective tools for formal methods. dr. rushby holds a ph.d. in computer science from the university of newcastle (1977).lui sha holds a ph.d. and an m.s. in electrical and computer engineering from carnegie mellon university and a b.s.e.e. from mcgill university. he is donald b. gillies chair professor of computer science at the university of illinois at urbanachampaign. before joining uiuc in 1998, he was a senior member of the technical staff at the software engineering institute at carnegie mellon university, which he joined in 1986. he is a fellow of the acm and a fellow of the ieee for ﬁtechnical leadership and research contributions which enabled the transformation of realtime computing practice from an ad hoc process to an engineering process based on analytic methods.ﬂ he was the chair of the ieee realtime systems technical committee from 1999 to 2000 and received that committee™s outstanding technical contributions and leadership award in december 2001. dr. sha™s accomplishments are many. he led the development of generalized rate monotonic theory, which has transformed hardware and software open standards in realtime computing; has been supported by nearly all the commercially available realtime operating systems, middleware, and modeling tools; and has been taught in realtime computing courses around the world. his work was cited in the selected accomplishment section of the 1992 national academy of science™s report computing the future: a broader agenda for computer science and engineering. his expertise in dependable realtime computing systems has made him an indissoftware for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.124 software for dependable systemspensable resource for many national hightechnology projects, including critical assistance to the international space station, the global positioning system software upgrade, mars pathnder, f22 avionics stability improvement, and f35 mission system architecture. martyn thomas graduated as a biochemist from university college, london, and immediately entered the computer industry. from 1969 to 1983, he worked in universities (in london and the netherlands), in industry (designing switching software for stc), and at the south west universities regional computer centre in bath. in 1983 (with david bean), he founded a software engineering company, praxis, to exploit modern software development methods. in december 1992, praxis was sold to deloitte and touche, an international rm of accountants and management consultants, and mr. thomas became a deloitte consulting international partner while remaining chair and, later, managing director of praxis. he left deloitte consulting in 1997. mr. thomas is now an independent consultant software engineer specializing in the assessment of large, realtime, safetycritical, softwareintensive systems, software engineering, and engineering management. he serves as an expert witness where complex software engineering issues are involved. he is a visiting professor in software engineering at the university of oxford and a visiting professor at the university of bristol and the university of wales, aberystwyth. he has advised the u.k. government and the commission of the european union on policy in the elds of software engineering and vlsi design. he has had close links with the academic research community throughout his career, as a member of two university funding council research assessments in computer science, numerous international conference program committees, and several u.k. government and research council panels and boards. he has been a member of the it foresight panel of the u.k. government ofce of science and technology, a member of the advisory board for the dera systems and software engineering centre, and a member of the research advisory council of the u.k. civil aviation authority. he is a fellow of the british computer society and of the institution of engineering and technology. he currently serves on the engineering and technology strategic panel of the british computer society, the it sector panel of the iet, the advisory group to the foresight cyber trust and crime prevention project, the executive of the u.k. computing research committee, and as a member of the advisory council of the foundation for information policy research. he is chair of the steering committee for the u.k. interdisciplinary research collaboration on dependable systems (dirc) and a former member of the the u.k. engineering and physical sciences research council. in 2007, software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.appendix a 125he was awarded the commander of the order of the british empire (cbe) for services to software engineering.scott wallsten is a senior fellow and director of communications policy studies at the progress and freedom foundation (pff) and also a lecturer in stanford university™s public policy program. before joining pff he was a senior fellow at the american enterprise institute (aei)brookings joint center for regulatory studies and a resident scholar at the aei. he has also served as an economist at the world bank, a scholar at the stanford institute for economic policy research, and a staff economist at the u.s. president™s council of economic advisers. dr. wallsten™s interests include industrial organization and public policy, and his research has focused on regulation, privatization, competition, and science and technology policy. his work has been published in numerous academic journals, including the rand journal of economics, the journal of industrial economics, the journal of regulatory economics, and regulation, and his commentaries have appeared in newspapers throughout the world. he has a ph.d. in economics from stanford university.david woods is a professor in the institute for ergonomics at the ohio state university. he was president (19981999) and is a fellow of the human factors and ergonomic society and is also a fellow of the american psychological society and the american psychological association. he has received the ely award for best paper in the journal human factors, the kraft innovators award from the human factors and ergonomic society for developing the foundations of cognitive engineering, a laurels award from aviation week and space technology for research on the human factors of highly automated cockpits, an ibm faculty award, and ve patents for computerized decision aids. he was on the board of the national patient safety foundation from its founding until 2002 and was associate director of the midwest center for inquiry on patient safety (gaps center) of the veterans health administration from 1999 to 2003. he is coauthor of the monographs behind human error and a tale of two stories: contrasting views of patient safety and the books joint cognitive systems: foundations of cognitive systems engineering and joint cognitive systems: patterns in cognitive systems engineering, and coeditor of resilience engineering. his research includes studies of data overload in control centers, critical care medicine, and inferential analysis; eld studies of team work between people; and automation in anesthesiology, aviation, space mission operations, disaster response, and health care. his work on how to make systems resilient to improve safety is based on accident investigations in nuclear power, medicine, and space operations.  multimedia overviews of his research are available at <http://csel.eng.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.126 software for dependable systemsohiostate.edu/woods/>. based on this body of work, he has been an advisor to various government agencies and other organizations on issues pertaining to human performance and error, including the federal aviation administration, nuclear regulatory commission, national patient safety foundation, veterans health administration, and national science foundation, and was an advisor to the columbia accident investigation board. most recently he served on a national academy of engineering/institute of medicine study panel that applied engineering to improve health care systems and on a national research council panel that dened the future of the national air transportation system. dr. woods earned a ph.d. from purdue university in 1979.stafflynette i. millett is a senior program ofcer and study director at the computer science and telecommunications board of the national academies. she is currently involved in several cstb projects, including a study on softwareintensive systems producibility, an assessment of the social security administration™s egovernment strategy, and a comprehensive exploration of biometrics systems, among other things. she was the study director for the cstb project that produced who goes there? authentication technologies and their privacy implications and idsšnot that easy: questions about nationwide identity systems. her portfolio includes signicant portions of cstb™s recent work on software and on identity systems and privacy. she has an m.sc. in computer science from cornell university, along with a b.a. in mathematics and computer science with honors from colby college. her graduate work was supported by both an nsf graduate fellowship and an intel graduate fellowship.david padgham rejoined cstb as an associate program ofcer in the spring of 2006 following nearly 2 years as a policy analyst in the association for computing machinery™s (acm™s) washington, d.c., ofce of public policy, where he worked closely with that organization™s public policy committee, usacm. previously, mr. padgham spent nearly 6 years with cstb, working onšamong other thingsšthe studies that produced trust in cyberspace; funding a revolution; broadband: bringing home the bits; lc21: a digital strategy for the library of congress; and the internet™s coming of age. currently, he is focused on the cstb projects related to health care informatics, computing performance, and software dependability. he holds a master™s degree in library and information science from the catholic university of america in washington, d.c., and a bachelor of arts degree in english from warren wilson college in asheville, n.c.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.appendix a 127gloria westbrook recently left the computer science and telecommunications board where she was a senior program assistant. she previously served as the executive assistant to the directors of the ofce of youth programs and the youth opportunity grant program at the d.c. department of employment services (does). in 2003, ms. westbrook was selected to be the lead administrator of a team that successfully administered a $4 million summer youth employment program that registered over 5,000 district youth. in addition, ms. westbrook has also served as the executive assistant to the director of does, where she was appointed by the director to serve as his elite liaison to the d.c. mayor and his cabinet, members of the d.c. council, and members of congress. while serving in the director™s ofce, ms. westbrook received the meritorious service award and the workforce development administrator™s award of appreciation for dedication of service. she also became a member of the national association of executive secretaries and administrative assistants. she attended duke ellington school of the performing arts for ballet and went on to further her dance education at the university of the arts in philadelphia. phil hilliard was a research associate with the computer science and telecommunications board until may 2004. he provided research support as part of the professional staff and worked on projects focusing on telecommunications research, supercomputing, and dependable systems. before joining the national academies, he worked at bellsouth in atlanta, georgia, as a competitive intelligence analyst and at ncr as a technical writer and trainer. he has a master™s in library and information science from florida state university, an m.b.a. from georgia state university, and a b.s. in computer and information technology from the georgia institute of technology.penelope smith worked temporarily with the computer science and telecommunications board between february and july 2004 as a senior program assistant. prior to joining the national academies, she worked in rural angola as a health project manager and community health advisor for concern worldwide. she also worked for emory university as a project coordinator and researcher on reproductive health and hiv and for the centers for disease control as a technology transfer evaluator for hiv/aids programs. she earned an m.p.h. from emory university and a b.a. in medical anthropology from the university of california at santa cruz. she is also a certied health education specialist.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.b open session briefersalthough the individuals listed below provided much useful information of various kinds to the committee, they were not asked to endorse this study™s conclusions or recommendations, nor did they see the nal draft of this report before its release. december 1819, 2003 washington, d.c.helen gill, national science foundationsol greenspan, national science foundationpaul l. jones, food and drug administrationcarl landwehr, national science foundationernie lucier, federal aviation administrationbrad martin, national security agencypaul miner, nasa ralph wachter, ofce of naval researchapril 1921, 2004 workshop on software certification and dependability washington, d.c.kent beck, three rivers instituterichard cook, university of chicago128software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.appendix b 129david dill, stanford universitymatthias felleisen, northeastern universitybrent goldfarb, university of marylandanthony hall, praxis critical systemsbob harper, carnegie mellon universitymats heimdahl, university of minnesota chuck howell, mitre corporationdoug jones, university of iowashriram krishnamurthi, brown universityjim larus, microsoft researchisaac levendel, independent consultantgary mcgraw, cigitalpeter neumann, sri internationalbob noel, mitre corporationgene rochlin, university of california, berkeleyavi rubin, johns hopkins universitybill scherlis, carnegie mellon universityted selker, massachusetts institute of technologyandré van tilborg, ofce of the secretary of defensemay 1819, 2004 cambridge, massachusettsjames baker, u.s. air forcemichael cusumano, massachusetts institute of technologymichael hammer, hammer and companymike lai, microsoft butler lampson, microsoft researchalfred spector, ibm researchrichard stanley, mitrefebruary 1618, 2005 mountain view, californiabill bush, sun microsystemswindow snyder, microsoftsoftware for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.c statement of taskthis project will convene a mixed group of experts to assess current practices for developing and evaluating missioncritical software, with an emphasis on dependability objectives. the goal of this study is to identify the kinds of system properties for which certication is desired, how that certication is obtained today, and, most important, what design and development methods, including methods for establishing evidence of trustworthiness, could lead to future systems structures that are more easily certied. where these methods cannot be identied, the study will identify a research agenda that would lead to their discovery. the committee will address system certication, examining a few different application domains (e.g., medical devices and aviation systems) and their approaches to software evaluation and assurance. this should provide some understanding of what common ground and disparities exist. the discussion will engage members of the fundamental research community, who have been scarce in this arena. it will consider approaches to systematically assessing systems™ user interfaces. it will examine potential benets and costs of improvements in evaluation of dependability as performance dimensions. it will evaluate the extent to which current tools and techniques aid in ensuring and evaluating dependability in software and investigate technology that might support changes in the development and certication process. it will also use the information amassed to develop a research agenda for dependable software system development and certication, factoring in earlier high condence software and 130software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.appendix c 131systems research planning. it will also investigate ideas for improving the certication processes for dependabilitycritical software systems. the work of the expert committee will culminate in a written report with recommendations, which will be subject to national research council review processes.software for dependable systems: sufficient evidence?copyright national academy of sciences. all rights reserved.
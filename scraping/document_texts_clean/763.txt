detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/763information technology and the conduct of research: theuser's view88 pages | 8.5 x 10 | paperbackisbn 9780309038881 | doi 10.17226/763panel on information technology and the conduct of research, national academy ofsciences, national academy of engineering, institute of medicineinformation technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.information technology and theconduct of researchthe user's viewreport of the panel on information technology and the conductof researchnational academy of sciencesnational academy of engineeringinstitute of medicinecommittee on science, engineering, and public policynational academy presswashington, d.c. 1989iinformation technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.national academy press 2101 constitution avenue, nw washington, dc 20418the national academy of sciences (nas) is a private, selfperpetuating society of distinguished scholars in scientific and engineeringresearch, dedicated to the furtherance of science and technology and their use for the general welfare. under the authority of its congressionalcharter of 1863, the academy has a working mandate that calls upon it to advise the federal government on scientific and technical matters.the academy carries out this mandate primarily through the national research council, which it jointly administers with the nationalacademy of engineering and the institute of medicine. dr. frank press is president of the nas.the national academy of engineering (nae) was established in 1964, under the charter of the nas, as a parallel organization of distinguished engineers, autonomous in its administration and in the selection of members, sharing with the nas its responsibilities for advisingthe federal government. dr. robert m. white is president of the nae.the institute of medicine (iom) was chartered in 1970 by the nas to enlist distinguished members of appropriate professions in theexamination of policy matters pertaining to the health of the public. in this, the institute acts under both the academy's 1863 congressionalcharter responsibility to be an adviser to the federal government and its own initiative in identifying issues of medical care, research, and education. dr. samuel o. thier is president of the iom.the committee on science, engineering, and public policy is a joint committee of the national academy of sciences, the nationalacademy of engineering, and the institute of medicine. it includes members of the councils of all three bodies.this study received support from the department of energy, the national aeronautics and space administration, the national bureau ofstandards and national oceanic and atmospheric administration of the department of commerce, the national library of medicine, and thenational science foundation. opinions, findings, conclusions, or recommendations expressed in this publication are those of the panel oninformation technology and the conduct of research of the committee on science, engineering, and public policy, and do not necessarilyreflect the views of the sponsors.library of congress cataloginginpublication datapanel on information technology and the conduct of research (u.s.)information technology and the conduct of research.bibliography: p.includes index.1. information technologyšscientific applications. 2. researchštechnological innovations. i. title.q180.55.i45p36 1989 001.4'2 8828903isbn 030903888xcopyright © 1989 by the national academy of sciencesno part of this book may be reproduced by any mechanical, photographic, or electronic process, or in the form of a phonographic recording, nor may it be stored in a retrieval system, transmitted, or otherwise copied for public or private use, without written permission from thepublisher, except for the purposes of official use by the united states government.printed in the united states of americacover photograph:temperature field in january at a depth of 225 meters, from a global model of the oceanic general circulation. deep reds represent temperatures of 24°c and deep blues are 2°c. picture courtesy of michael cox. reproduced from ﬁcomputer modeling in physical oceanographyfrom the global circulation to turbulence,ﬂ william r. holland and james c. mcwilliams. physics today, vol. 40, no. 10, p. 52.iiinformation technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.panel on information technology and the conduct of researchdonald n. langenberg (chair), chancellor, university of illinois at chicagow. richards adrion, chair, computer and information science department, university of massachusetts,amherstjoseph ballam, professor, department of physics, stanford linear accelerator center, stanford, californiabruce g. buchanan, professor and codirector, center for parallel, distributed, and intelligent systems,university of pittsburgh, pittsburgh, pennsylvaniawilliam j. emery, professor, department of aerospace engineering science, university of colorado,boulderdavid a. hodges, professor, department of electrical engineering and computer sciences, university ofcalifornia, berkeleydavid a. hoffman, professor, department of mathematics and statistics, university of massachusetts,amherstf. thomas juster, professor of economics, university of michigan, ann arborsara b. kiesler, professor, department of social and decision sciences, carnegie mellon university,pittsburgh, pennsylvaniakenneth m. king, president, educom, princeton, new jerseyrobert langridge, professor, schools of pharmacy and medicine, university of california, san francisconina w. matheson, director, william h. welch medical library, the johns hopkins university,baltimore, marylanddavid a. pensak, corporate advisor, computing technology, e. i. du pont de nemours and co.,wilmington, delawareallan h. weis, vice president, data systems division, ibm enterprise systems, white plains, new yorkiiiinformation technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.staffjohn r. b. clement, study directoraudrey pendergast, staff officerann k. finkbeiner, writernisha govindani, secretaryjoan rood, secretaryivinformation technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.committee on science, engineering, and public policycornelius j. pings (chairman), provost, university of southern california, los angeles, californiagilbert s. omenn, dean, school of public health and community medicine, university of washington,seattle, washington(former chairman*)h. norman abramson, executive vicepresident, southwest research institute, san antonio, texasalbert m. clogston, member, center for materials science, los alamos national laboratory, losalamos, new mexicophilip m. condit, executive vicepresident, boeing commercial airplane company, seattle, washingtonemilio q. daddario, washington, d.c.gerald p. dinneen,* vice president, science and technology, honeywell incorporated, minneapolis,minnesotaalfred p. fishman, william maul measey professor of medicine, and director, cardiovascularpulmonarydivision, university of pennsylvania school of medicine, philadelphia, pennsylvaniaralph e. gomory, senior vicepresident for science and technology, ibm corporation, armonk, new yorkzvi griliches,* nathaniel ropes professor of political economy, department of economics, harvarduniversity, cambridge, massachusettsarthur kelman, wisconsin alumni research foundation senior research professor of plant pathologyand bacteriology, department of plant pathology, university of wisconsin, madison, wisconsinfrancis e. low, institute professor, department of physics, massachusetts institute of technology,cambridge, massachusetts* term expired june 30, 1988.vinformation technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.john d. roberts,* institute professor of chemistry, division of chemistry and chemical engineering,california institute of technology, pasadena, californiakenneth j. ryan, kate macy ladd professor of obstetrics and gynecology, harvard medical school; andchairman, department of obstetrics and gynecology, brigham and women's hospital, boston,massachusettsherbert a. simon, richard king mellon university professor, department of computer science andpsychology, carnegie mellon university, pittsburgh, pennsylvaniaex officiofrank press, president, national academy of sciencesrobert m. white, president, national academy of engineeringsamuel o. thier, president, institute of medicinestaffallan r. hoffman, executive directormyron f. uman, associate executive directorbarbara a. candland, administrative coordinatorcathy d. williams, administrative secretaryviinformation technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.prefacethe everpresent urge to categorize our fellow humans leads in this computer age to the categoriesﬁcomputer literateﬂ and ﬁcomputer illiterate.ﬂ it might seem obvious that scientists and engineers, particularlythose engaged in research, must all be computer literate. after all, such people work with numbers and data, andisn't that what computers are all about? yet the most superficial survey of researchers will reveal a wide range ofcapabilities in the use of information technology (computers plus telecommunications) in research. it will alsoreveal endemic frustration and dissatisfaction.why? is not the work of those researchers whose subject is information technology itself yielding a steadystream of new capabilities for their colleagues in other fields? yes it is, and some of the new capabilities cantruly be called revolutionary. are not researchers in many fields continually finding new ways to applyinformation technology to do old things faster, better, and cheaper, and to do new things which just yesterdaywere beyond the realm of possibility? yes, that is so. so much so, in fact, that there are many who believe thatthe pervasive use of information technology in the conduct of research is changing profoundly the very meaningof the word ﬁresearch.ﬂ are not our institutions, agencies, and companies, our policymakers, managers, andvendors finding ways to place the new instruments of information technology in the hands of more and moreresearchers? yes, despite the usual fiscal constraints, they are.then what's the problem? indeed, is there one? it was the suspicion that there is a problem (many, actually),that there are serious impediments to the wider and more effective use of information technology in research,that led the committee on science, engineering, and public policy (cosepup) to form the panel oninformation technology and the conduct of research and to charge it to explore the situation and to report itsfindings, conclusions, and recommendations. i agreed to chair the panel because, as a scientist turned universityadministrator and federal official, and a computer illiterate, i was excited by theprefaceviiinformation technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.prospect of learning something of an issue i sense is of paramount importance to the future of the global researchenterprise. this report is the result of the panel's deliberations. our subject is broad, its literature is scattered, andsome of its facets are still more art than science. we cannot claim to have produced the definitive picture of theissue. we hope we have made a case for the importance of understanding and addressing it, and perhaps shedsome light on a creature that reminds some of us of the elephant once investigated by an earlier panel.if this report has value, it is due to the salient characteristic of the panel reflected in the report's subtitle,ﬁthe user's view.ﬂ most of the panel members are researchers active in disciplines not encompassed by theterm ﬁinformation technology.ﬂ they are expert but skeptical users of information technology in their ownresearch, in possession of exciting visions of what this technology might bring to their fields, and of experiencedviews of what it has brought, and at what cost. from my youth i remember an ad for an automobile, which urgedthe reader to ﬁask the man who owns one!ﬂ there's wisdom in that slogan; in the absence of a considerable bodyof established knowledge, our panel focused on asking the men and women ﬁwho own one.ﬂ the result is areport that should speak to researchers experienced in the application of information technology, as well as thosewho would like to gain more experience, and of course to those engaged in supporting research. we believe it isworth the reader's attention.the conception and early stages of the study owe much to the committee on science, engineering, andpublic policy. in particular, two former members of the committee deserve mention: floyd e. bloom, whoseterm with the committee ended soon after this study's initiation; and gilbert s. omenn. dr. bloom conceived thetopic and played a central role in its birth. dr. omenn chaired cosepup during the inception and most of theexecution of the study. we also must thank norman metzger of the national research council, who developedthe study's charge and initially served as study director.support for the study was provided by several federal agencies: the department of energy; the nationalaeronautics and space administration; the national bureau of standards and national oceanic and atmosphericadministration of the department of commerce; the national library of medicine; and the national sciencefoundation. sun microsystems, inc., donated an advanced workstation for the purpose of report production.the study benefited from the opinions and reviews of many people: research users, experts in computing,computing applications, and communications, and policymakers. we received helpful advice and suggestionsfrom too many persons to mention by name; but we acknowledge the vital part their input played. at the finalstages of the report's preparation, four members of cosepup served as a review group: john d. roberts, aschair, and alfred p. fishman, francis e. low, and herbert a. simon. we did not always take the advice offered;but we always profited from it. of course, the report's statements, findings, and recommendations remain thesole responsibility of the panel members.prefaceviiiinformation technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.the panel is particularly grateful to its professional staff: john clement, audrey pendergast, annfinkbeiner, and nisha govindani, who supported the study both intellectually and logistically, while exhibitingexemplary patience. the panel also owes special thanks to allan hoffman, executive director of cosepup, forinput and support from conception to final dissemination; and also to barbara candland and cathy d. williamsof the cosepup staff. without the efforts of all these people, the study truly would not have taken place.one final, and important, point: i share with many researchers a strong belief that much of the power ofscience (whether practiced by scientists, engineers, or clinical researchers) derives from the steadfastcommitment to free and unfettered communication of information and knowledge. this principle has been partof the ethos of the global research community for centuries, and has served it and the rest of humanity well. ifasked to distill one key insight from my service on this panel, i would respond with the assertion thatinformation technology is of truly enormous importance to the research community, and hence to all humanity,precisely because it has the potential to enhance communication of information and knowledge within thatcommunity by orders of magnitude. we can now only dimly perceive what the consequences of that fact may be.that there is a revolution occurring in the creation and dissemination of information, knowledge, and, ultimately,understanding is clear to me. it is also clear to me that it is critically important to maintain our commitment tofree and unfettered communication as we explore the uses of information technology in the conduct of research.if my colleagues and i succeed through this report in conveying some sense of this to our readers, and of thenecessity that many individuals and institutions be attentive to it, we will have discharged our duty.donald n. langenbergchairprefaceixinformation technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.prefacexinformation technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.contents executive summary 1 introduction 7 the use of information technology in research 11 the conduct of research 13 data collection and analysis 14 communication and collaboration among researchers 18 information storage and retrieval 23 new opportunities: approaching the revolution asymptotically 30 institutional and behavioral impediments to the use of information technology inresearch 34 panel findings and recommendations 47 findings 47 recommendations 50appendix a: list of position papers 57appendix b: biographies of panel members 58 bibliography and selected readings 63 index 69contentsxiinformation technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.xiicontentsinformation technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.information technology and the conduct of researchxiiithe user's viewinformation technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.xivinformation technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.executive summaryinformation technologyšthe set of computer and telecommunications technologies that makes possiblecomputation, communication, and the storage and retrieval of informationšhas changed the conduct ofscientific, engineering, and clinical research. this report examines present trends, future potential, andimpediments to the use of information technology in support of research. written from the viewpoint of theresearcher using information technology and including many examples, the report offers a number ofrecommendations directed to two principal audiences: policymakers and leaders of institutions responsible forthe support and management of research, and researchers themselves.the first programmable, electronic, digital computer was created nearly five decades ago. at first,computers simply substituted for other means of carrying out arithmetic calculations; they were large, expensive,often unreliable, and accessible only to a minority of scientists and engineers. with the advent of the integratedcircuit (the semiconductor ﬁchipﬂ), computational speed and power increased dramatically, and computer usebecame widespread. recently, computer technology has been joined with telecommunications technology tocreate a new entity: information technology, which has done much to remove the constraints of speed, cost, anddistance from the researcher.on the whole, information technology has led to improvements in research. new avenues for scientificexploration have opened. researchers can collaborate more widely and efficiently. much more data are availablefor analysis. analytic capabilities have improved significantly, along with the capability to present results asvisual images. new information technologies offer further opportunities to improve research. but widespread useof computers in research has not come about without problems. some of these difficulties are technological,some financial. underlying many of them are complex institutional and behavioral constraints.executive summary1information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.the report examines three aspects of the research process: data collection and analysis, communication andcollaboration among researchers, and information storage and retrieval.in data collection and analysis, a number of trends are discussed, including growth in the amount of information researchers can store and analyze; creation of new families of computercontrolled instruments; proliferation of computer networks dedicated to research; and increasing availability of software ﬁpackagesﬂ supporting research activities.among the difficulties associated with data collection and analysis are uneven access to computingresources, problems in obtaining support for software development and maintenance, and unnecessarycomplexities of transmitting data over computer networks.communication and collaboration among researchers are changing. not only can information be sharedmore and more quickly, but researchers are also developing new collaborative arrangements. three technologiesare involved: word processing, electronic mail, and computer communications networks. word processing andelectronic mail are arguably the most pervasive of all the routine uses of computers in research communication.electronic mailšsending text from one computer user to another over the networksšis partially replacingwritten and telephone communication among many communities of scientists. scientists increasingly usenetworks for conversation and for repeated exchanges of text and data files. among the most important of thepotential applications of information technology is the emergence of a truly national research network.the principal difficulties with communicating via electronic mail and file transfer technologies involveincompatibility between different text and data processing systems and between network protocols. alsosignificant are network limitations: addressing conventions are cumbersome and unhelpful, locator services arenearly nonexistent, and overall network availability and reliability need improvement.electronic storage and retrieval of information hold enormous advantages: information can be storedeconomically, found quickly without going to another location, and moved easily. for all disciplines, bothscientific data and reference database promise to be significant sources of knowledge for basic research.however, a number of problems need to be resolved. researchers have difficulty getting access to data stored byother researchers. even when researchers get access to colleagues' data, they have difficulty reading them.finally, when researchers get access to and read each others' databases, they often lack information on thequality of the data.the primary difficulty encountered with reference databases is in conducting searches. most informationsearches at present are incomplete, cumbersome, inefficient, expensive, and executable only by specialists.there is a pressing need for new, more compact, and more permanent forms of data storage. stored datagradually become useless, either because the storage media decay or the storage technology itself becomesobsolete. underlyingexecutive summary2information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.difficulties in information storage and retrieval are significant problems in the institutional management ofresources.new computerbased technologies offer the prospect of new ways of finding, understanding, storing, andcommunicating information, and should increase both the capabilities and the productivity of researchers.among these new technologies are simulations, new methods of presenting observational and computationalresults as visual images, the use of knowledgebased systems as ﬁintelligent assistants,ﬂ and more flexible andintuitive ways for people to interact with, and control, computers.the panel has identified a number of problem areas in which institutional and behavioral impedimentsunderline many difficulties in the use of information technology in research. these areas include issues of costs and cost sharing: financial impediments are chronic. although institutions will continueto do their best, information technology for research will continue to need more funds. the panelbelieves that increased support of information technology in research deserves high priority. the problem of standards: simplified, consistent standards for operation of, and interconnection among,computer systems could have major impacts on research communications and productivity; however,such standards are largely absent, and their development is a slow and controversial process. legal and ethical constraints: the need to safeguard and maintain confidentiality of data on humansubjects is a major issue; also likely to become increasingly significant is the question of responsibilityin computersupported decision making in engineering, clinical practice, or research. gaps in training and education: learning to use information technology presently requires significantinitial investments of time and effort, and researchers who make these investments often receiveinsufficient help. although the problem is likely to diminish with time, it affects current attitudes ofmany researchers toward the use of information technology. the perceived risks of organizational change: organizations and administrators can understandably bereluctant to make the substantial changes required to make use of information technology. of fundamental importance, the lack of an infrastructure for the use of information technology inresearch: access to expertise, and support mechanisms to encourage such experts; tools for developingand managing software; systems for storing and retrieving information; and support services forcommunication and collaboration among researchers.the report concludes with three major recommendations.recommendation ithe institutions supporting the nation's researchers must recognize and meet their responsibilities to developand support policies, services, and standardsexecutive summary3information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.that help researchers use information technology more widely and productively. specifically universities should provide accessible, expert help in learning and using information technology. universities departments, and scientific and professional groups, should establish career ladders forscientific programming positions. funding agencies should provide support for scientific programming and for help services in learningand using information technology systems for research. scientific associations should establish disciplinary standards for the storage and indexing of scientificdata. university departments, and scientific and professional groups, should implement mechanisms for theevaluation, merit (peer) review, and dissemination of software useful in the conduct of research. vendors, in collaboration with scientific groups, should establish standards for simplified and consistentusermachine interfaces. network administrators should provide simple user interfaces and addressing schemes, add gateways toother networks, improve system reliability and capacity, and provide online help, such as guides toservices and mail addresses of individuals who can answer questions. information service providers should create simplified common standards for accessing and queryinginformation sources and eventually provide unified access to information. software vendors, and scientific and professional groups, should create program libraries and makethem accessible through the networks.recommendation iithe institutions supporting the nation's researchers, led by the federal government, should develop aninterconnected national information technology network for use by all qualified researchers. specifically the office of science and technology policy (ostp) in the executive office of the president, and thefederal agencies responsible for supporting and performing research and development, should plan andfund a nationwide infrastructure for computerbased research communication. planning and development of this nationwide infrastructure should be guided by users of informationtechnology in research, rather than by technical experts in information technology or hardware orsoftware vendors. the panel believes strongly that such a national network is too important to the futureof research to be left only to the technical experts. the national research network should be founded on the fundamental premise of open access to allqualified researchers/scholars that has nurtured the world's scientific community for centuries.executive summary4information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved. the national research network should be developed in an evolutionary manner, making full use of theexisting successful networks for research.recommendation iiito facilitate implementation of recommendations i and ii, and to focus attention on the opportunities andimpediments associated with research uses of information technology, the panel recommends the establishmentat a national level of a user's group to oversee and advise on the evolution and use of information technology insupport of scientific, engineering, and clinical research.specifically, the national research council (nrc) should charge a standing committee or board (whetherexisting or newly created) with the mandate to oversee and advise on research use of information technology.the membership of this board should include a majority of users from a variety of research disciplines.executive summary5information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.executive summary6information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.introductionthis report is about how information technology has changed the conduct of scientific, engineering, andclinical research.information technology is that set of computer and telecommunications technologies that makes possiblecomputation, communication, and the storage and retrieval of information. the term, therefore, includes computer hardware of all kinds, from microprocessors dedicated to specific research tasks to the largestsupercomputers; communications networks that link researchers to each other and to resources of various kinds; and computer software that researchers use to design and run scientific projects, and to manage theinformation that the projects yield.the effect of information technology on the conduct of research has long been a concern of the committeeon science, engineering, and public policy (cosepup), a joint unit of the national academy of sciences, thenational academy of engineering, and the institute of medicine. a previous cosepup report, frontiers inscience and technology: a selected outlook (w. h. freeman, 1983), discussed ways of improving scientific andtechnical communication and asked, ﬁhow can scientists and engineers be encouraged to use the new electronicmodalities innovatively and effectively?ﬂthe science policy community has occasionally discussed how computer and communication technologiesaffect research, although mostly as a corollary to other policy issues such as the need for support for advancedcomputing, the computing requirements of individual research disciplines, national security concerns aboutinformation dissemination, or the potential of developments in information technology research. for example, inseptember 1985, the house of representatives' committee on science and technology held hearings on ﬁtheintroduction7information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.impact of the information age on scienceﬂ as part of its study of u.s. science policy (u.s. congress, 1986a,b).cosepup members felt that the subject deserved a thorough examination. in july 1985, a cosepupplanning group recommended a study, and in december, 1986, after approval and selection of a panel, the studyformally began. the study has received financial support from the department of energy, the nationalaeronautics and space administration, the national bureau of standards and national oceanic and atmosphericadministration of the department of commerce, the national library of medicine, and the national sciencefoundation.cosepup's charge to the panel on information technology and the conduct of research was to examine current and prospective applications of information technologyšcomputers andcommunicationsšto improve productivity in selected fields of scientific and engineering research,including the biological, physical, social, and engineering sciences; identify impediments to the effective use of these technologies in researchšsuch impediments may beinstitutional, financial, behavioral, and technical; examine the behavioral and cultural changes required to exploit the new opportunities offered by theseinformation technologies; and suggest appropriate actions by federal agencies, as well as by universities and other researchinstitutions, manufacturers, vendors, scientific associations, and individual researchers.in considering its charge, the panel decided that the goal of the study would be to recommend how tostimulate research through the use of information technology, and the focus of the study would be on bothcurrent and prospective uses. because little research on general scientific uses of information technology nowexists, the panel gathered additional information from several disciplines representing the range of scientific andengineering research. experts were provided with a list of questions, and commissioned to prepare papers on theuse of information technology in their fields. they were asked to represent their colleagues' views as well astheir own. the panel believes that these papers reflect the diversity in information technology uses within andacross the scientific disciplines. the papers provided essential inputs to the panel's discussions and eventualrecommendations. they are listed in appendix a, and are available from cosepup on request.the report is written from the point of view, not of those who specialize in information technology, but ofthose who use it. for all researchers, information technology is beneficial; for some, it has become central. someresearchers want only better access to current technologies; others urgently want much more. the panel does notpresume to prescribe a single model for all researchers, but it does believe that the products of the informationage are invaluable and should be available to researchers who need them. therefore, in what follows, the panelintroduction8information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.does not appraise developments in computing hardware or software but emphasizes instead how thesedevelopments affect the productivity of researchers.the report is directed to two principal audiences. one audience includes the policymakers and leaders ofthose institutions responsible for the support and management of research. for this audience, the panel describesissues and impediments, and recommends ways of helping the research community enhance its use ofinformation technology. the second audience is the users themselves, that is, research scientists and engineers.the panel hopes that researchers will find their concerns about information technology addressed clearly and intheir own terms. in addition, the panel hopes that practices in other disciplines may spark readers' ideas for theirown research.the panel discovered early that there is almost no systematic information on the users and uses ofinformation technology. for example, the panel cannot estimate how many or what proportion of scientists usecomputers in different fields, how access to networks and computer facilities is distributed across disciplines, orto what extent useful applications are disseminated throughout the research community. systematic collection ofsuch information is essential to the development of intelligent policy. researchers' experiences in usinginformation technology can help guide decisions about policy and resource allocation. in turn, these decisionswill shape the technological and institutional advances that break down impediments to the further use ofinformation technology. this process will continue to change the nature of scientific, engineering, and clinicalresearch itself.finally, the panel has come to a view of new ways of managing scientific knowledge and conductingscientific research. in this view, scientists are more productive because they are using the power of computersboth to augment their intellectual efforts and to improve communication. with artificial barriers tocommunication lowered, science itself is closer to the open, collaborative search that is its goal.the next section of the report describes present trends, future potential, and impediments to the use ofinformation technology in support of research, drawing on examples from a number of fields. the report's finalsection summarizes the panel's findings and recommendations.introduction9information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.introduction10information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.the use of information technology in researchin this chapter we examine the effect of information technology on the conduct of research. newtechnologies offer new opportunities, although pervasive use of computers in research has not come aboutwithout problems. some of these problems are technological, some financial. underlying many of them arecomplex institutional and behavioral constraints.nearly five decades ago, the first programmable, electronic, digital computer was switched on. that dayscience acquired a tool that at first simply facilitated research, then began to change the way research was done.today these changes continue, and now amount to a revolution.electronic digital computers at first simply replaced earlier technologies. researchers used computers to doarithmetic calculations previously done with paper and pencil, slide rules, abacuses, or roomfuls of peoplerunning mechanical calculators. benefits offered by the earliest computers were more quantitative thanqualitative; bigger computations could be done faster, with greater reliability, and perhaps more cheaply. butcomputers were large, expensive, required technically expert operators and programmers, and consequently wereaccessible only to a relatively small fraction of scientists and engineers.one human generation and several computer generations later, with the advent of the integrated circuit (thesemiconductor ﬁchipﬂ), computational speed increased by a factor of 1 trillion, computational cost decreased bya factor of 10 million, and the smallest useful calculator went from the size of a typewriter to the size of awristwatch. at present, personal computers selling for a few thousand dollars can put significant computingpower on the desk of every scientist. meanwhile, advances in the software through which people interact withand instruct computers have made computers potentially accessible to people with no specific training incomputation. more recently, computer technology has joined telecommunications technology to create a newentity,the use of information technology in research11information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.ﬁinformation technology.ﬂ information technology has done much to remove from the researcher the constraintsof speed, cost, and distance.on the whole, information technology has led to improvements in research. new avenues for scientificexploration have opened. the amount of data that can be analyzed has expanded, as has the complexity ofanalyses. and researchers can collaborate more widely and efficiently.different scientific disciplines use information technology differently. uses vary according to thephenomena the discipline studies and the rate at which the discipline obtains information. in such disciplines ashigh energy physics, neurobiology, chemistry, or materials science, experiments generate millions ofobservations per second, and these must be screened and recorded as they happen. for these disciplines,computers that can handle large amounts of information quickly are essential and have made possible researchthat was previously impractical. other disciplines, such as economics, psychology, or public health, gather dataon events that accumulate slowly over relatively long periods of time. these disciplines also need computerswith large capacities, but do not need the capability to react in ﬁreal time.ﬂ most disciplines use informationtechnology in ways that fall somewhere in the range between these two extremes.boxes supplement or expand points in the text: the first two below deal with specific disciplines.high energy physics: science drives the leading edge of informationtechnology.an example helps to illustrate the direction in which many disciplines are moving: high energy physicscould not be done without information technology, and offers an extreme example of the trends forcomputing and communication needs in many scientific disciplines.most high energy physicists work on the same set of questions: what is the behavior of the mostelementary particles, and what is the nature of the fundamental forces between them? their experimentsare conducted in machines called accelerators, devices that produce beams of protons, electrons, or otherparticles that are accelerated to highspeeds and huge energies. there are two types of accelerators: thosein which two beams of particles are made to collide with each other (colliders), and those in which a beamhits stationary targets. physicists then reconstruct the collision to find new phenomena.remarkable results have emerged from high energy physics experiments conducted over the past twodecades. for instance, a nobel prizewinning experiment carried out at the protonantiproton collider at theeuropean center for nuclear research (cern) in switzerland, discovered two new particles known as thew and the z. their existence had been predicted by a theory claiming that the weak and electromagneticforces, seemingly unrelated at low energy levels, were in fact manifestations of a single force, called theelectroweak interaction, which would appear at sufficiently high energies. this discovery is a significantstep toward the description of all known interactionsšgravity, electromagnetism, and the strong (nuclear)and weak (radioactive decay) forcesšas manifestations of a single unifying force.the process by which some tens of thesethe use of information technology in research12information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.the panel recognizes the diversity in research methods, and differences in needs for informationtechnology. but the needs of researchers show sufficient commonalities across research fields to make a searchfor common solutions worthwhile.the conduct of researchthe everyday work of a researcher involves such activities as writing proposals, developing theoreticalmodels, designing experiments and collecting data, analyzing data, communicating with colleagues, studyingresearch literature, reviewing colleagues' work, and writing articles. information technology has had importanteffects on all these activities, and more change is in the offing. to illustrate these effects, we examine threeparticular aspects of research: data collection and analysis, communications and collaboration, and informationstorage and retrieval. in each area, we discuss how researchers currently use information technology and whatdifficulties they encounter. in a final part of this section, we discuss new technological opportunities and theirimplications for the conduct of research.new w and z particles were isolated from millions of collision events in the cern accelerator offers astriking illustration of the dependence of high energy physics on the most advanced aspects of informationtechnology. three steps are involved. first, data are acquired in real time as the experiment progresses;second, the data obtained are transformed into flight paths, from which the particles making the paths areidentified; and third, the event itself is reconstructed, and those few events exhibiting the very specialcharacteristics of the new phenomenon are identified. in each of these steps computers are vital: to triggerthe identification of interesting events; to establish particle tracks from the data; and to carry out analysisand interpretation.in the future, high energy physicists will demand more from information technology than it can nowdeliver. proposed new particle accelerators, such as the superconducting super collider (ssc), areexpected to produce several million collisions every second, of which only one or two collisions a secondcan be recorded. selecting this tiny fraction of the produced events in a manner that does not throw awayother interesting data is a tremendous challenge. it is hoped that ﬁfarmsﬂ of dedicated microprocessorsmight be able to examine tens of thousands of collision events per second, so that sophisticated selectionmechanisms can screen all collisions and select the very few that are to be recorded. the computerprograms that need to be developed for these tasks are of unprecedented size and complexity, and willchallenge the capabilities of both the physicists programming them and the information technology softwaresupport available to the programmers.even the small fraction of recorded events will result in some ten million collisions to be analyzed in ayear. processing one year's worth of saved data from the ssc would take a modern midsized computer500 years;the use of information technology in research13information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.data collection and analysiscurrent use collecting and analyzing data with computers are among the most widespread uses ofinformation technology in research. computer hardware for these purposes comes in all sizes, ranging frompersonal computers to microprocessors dedicated to specific instrumentational tasks, large mainframe computersserving a university campus or research facility, and supercomputers. computer software ranges from generalpurpose programs that computer numeric functions or conduct statistical analyses to specialized applications ofall sorts.the panel has identified five trends in the use of information technology in data collection and analysis:obviously, a faster processing rate is required. although no computer currently on the market wouldhandle this load in reasonable time, existing plans suggest that, by the time it is needed, some combinationof dedicated microprocessors and large mainframe systems will be available.high energy physicists are also highly dependent on networks. accelerators are located in only sevenmain laboratories in the united states, switzerland, west germany, the soviet union, and japan; thephysicists who use them are located in many hundreds of universities and institutions scattered around theworld. almost every high energy experiment, large or small, is a result of international collaboration: forinstance, one detector installed around one of the collision points of the accelerator at the fermi nationallaboratory is run by a collaboration of four foreign and thirteen u.s. institutions, involving some 200physicists. physicists at several institutions designed different parts of the detector; since the detector hasto work as an integrated apparatus, the physicists had to coordinate their work closely. different physicistsare also interested in different aspects of the experiment, and subsequent analysis of the data dependscrucially on adequate networking.future networking needs for high energy physics involve very high transmission speeds (as high as 10megabits per second) between laboratories, with provision for exchange of collision event files, graphics,and video conferencing. present long distance communication links are limited to lower transmissionspeeds (typically, 56 kilobits per second); each university physics group could use a 1.5 megabit persecond line for its own research needs. the provision of these facilities would be of enormous benefit touniversitybased physicists and students who cannot travel frequently to accelerator sites. increased use of computers for research. this trend coincides with large and continued increases in thespeed and power of computers and corresponding declines in their costs. dramatic increases in the amount of information researchers can store and analyze. for example,researchers can now process and manipulate observations in a database consisting of 18 years × 3,400individuals × 1,000 variables per individual for each year, create sets of relationships among theseobservations,the use of information technology in research14information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.and then subject the data to complex statistical analyses, all at a cost of less than $100. two decadesago, that kind of analysis could not have been conducted, and a much simpler analysis would have costat least ten times as much. the creation of new families of instruments in which computer control and data processing are at thecore of observation. for example, in new telescopes, imagematching programs on specializedcomputers align small mirrors to produce the equivalent lightgathering power of much largertelescopes with a single mirror. for instruments such as radiotelescope interferometers, the computerintegrates data from instruments that are miles apart. for computerassisted tomographic scanners, thecomputer integrates and converts masses of data into threedimensional images of the body. increased communication among researchers, resulting from the proliferation of computer networksdedicated to research, from a handful in the early 1970s to over 100 nationwide at present. differentnetworks connect different communities. biologists, high energy physicists, magnetic fusion physicists,and computer scientists each have their own network; oceanographers, space scientists, andmeteorologists are also linked together. networks also connect researchers with one another regionally;an example is nysernet, the new york state education and research network. researchers withdefense agency contracts are linked with one network, as are scientists working under contract to thenational aeronautics and space administration (nasa). such networks allow data collection andanalysis to be done remotely, and data to be shared among colleagues. increasing availability of software ﬁpackagesﬂ for standard research activities. robust, standardizedsoftware packages allow researchers to do statistical analyses of their data, compute complexmathematical functions, simplify mathematical expressions, maintain large databases, and designeverything from circuits to factories. many of these packages are commercial products, with highquality documentation, service, and periodic updates. others are freely shared software of use to aspecialized community without the costs or benefits of commercial software.one example illustrating several of the above trends is a system that geophysicists have set up to predictearthquakes more accurately. networks of seismographs cover the western united states. one such network innorthern california is called calnet. information from the 264 seismographs in calnet goes to a specialpurpose computer called the realtime picker. the software on the realtime picker looks at data as they come inand identifies exceptional events: patterns that indicate a coming earthquake. then it notifies scientists of theevents by telephone and sends graphics displays of locations and magnitudes, all within minutes.difficulties encountered the difficulties that researchers encounter using information technology to collectand analyze data vary in importance depending on the particular discipline.the use of information technology in research15information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.one difficulty is uneven access to computing resources. information technology is not equally accessible toall researchers who could benefit from its use, even though broadening access is a continuing focus ofinstitutions and funding agencies. to take an example from the field of statistics: according to a 1986 report onthe workshop on the use of computers in statistical research, sponsored by the institute for mathematicalstatistics, ﬁ–the quality and quantity of computational resources available to researchers today variesdramatically from department to department – perceived needs appear to vary just as dramatically – [while]departments that already have significant computer hardware feel a strong need for operating support, –departments that do not have their own computational resources feel an equally strong need for hardware.ﬂ(eddy, 1986, p. iii.)exclusion from resources happens for a variety of reasons, all reducible in the end to financial constraints.not all academic or research institutions have links to networks; in addition, access to networks can beexpensive, so not everyone who wants it can afford it. in some cases, since access to networks often mediatesaccess to resources such as supercomputers, exclusion from networks can mean exclusion from advancedcomputing.one of the most frustrating difficulties for researchers is finding the right software. software that iscommercially available is often unsuited to the specialized needs of the researcher. in those fields in whichindustry has an interest, however, commercial software is being developed in response to a perceived market.software could be custom designed for the researcher, but relatively few researchers pay directly for softwaredevelopment, partly because research grants often cannot be used to support it. consequently, most researchsee box on software, page 18.research mathematics and computationcomputation and theory in mathematics are symbiotic processes. machine computing power hasmatured to the point where mathematical problems too complicated to be understood analytically can becomputed and observed. phenomena have been observed for the first time that have initiated entirely newtheoretical investigations. the theory of the chaotic behavior of dynamic systems depends fundamentallyon numerical simulations; the concept of a ﬁstrange attractorﬂ was formulated to understand the results of aseries of numerical computations. recent advances in the theory of knots have relied on algebraiccomputations carried out on computers. these advances can be directly applied to such important topicsas understanding the folding of dna molecules. in the field of geometry, numerical simulation has beenused recently to discover new surfaces whose analytic form was too difficult to analyze directly. thesimulations were understood by the use of computer graphics, and led to the explicit construction of infinitefamilies of new examples.the modern computer is the first laboratory instrument in the history of mathematics. not only is itbeing used increasingly for research in pure mathematics, but, equally important, the prevalence ofscientific computing in other fields has provided the methe use of information technology in research16information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.ers, although they are not often skilled software creators, develop their own software with the help of graduatestudents. the result meets researchers' minimum needs but typically lacks documentation and is designed for onepurpose only. such software is not fully understood by any one person, making it difficult to maintain ortransport to other computing environments. this means that the software often cannot be used for relatedprojects, and the scientific community wastes time, effort, and money duplicating one another's efforts. insections to follow we examine how this problem is being addressed by professional associations, nonprofitgroups, and corporations.dium for communication between the mathematician and the physical scientist. here modern graphicsplays a critical role. this interaction is particularly strong in materials science, where the behavior of liquidcrystals and the shapes of complex polymers are being understood through a combination of theoreticaland computational advances.in spite of all this, mathematics has been one of the last scientific disciplines to be computerized. morethan other fields, it lacks instrumentation and training. this prevents the mathematician from using moderncomputing hardware and techniques in attacking research problems, and at the same time isolates him/herfrom productive communication with scientific colleagues.of course, mathematics is an important part of the foundation and intellectual basis of most of themethods that underlie all scientific use of computational machinery. to use today's highspeed computingmachines, new techniques have been devised. the need for new techniques is providing a seriouschallenge to the applied mathematician, and has placed new and difficult problems on the desk of thetheorist; algorithms themselves have become an object of serious investigation. their refinement andimprovement have become at least as important to the speed and utility of highspeed computing as theimprovement of hardware.some disciplines are limited by available computer power because computers needed are not on the market.some contemplated calculations in theoretical physics, quantum chemistry, or molecular dynamics, for example,could use computers with much greater capacity than any even on the drawing boards. in other cases, datagathering is limited by the hardware presently available. most commercial computers are not designed toaccommodate hardware and programs that select out interesting information from observational data, andscientists who want such computers must build them.another difficulty researchers encounter is in transmitting data over networks at highspeed. for researcherssuch as global geophysicists who use data collected by satellite, a large enough volume of information can besent in a short enough time, but transmission is unreliable. researchers often encounter delays and incur extracosts to compensate for ﬁnoiseﬂ on highspeed networks. technological solutions such as optical fiber and errorcorrecting coding are currently expensive to install and implement and are often unavailable in certaingeographic regions or for certain applications.the use of information technology in research17information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.communication and collaboration among researcherscurrent use researchers cannot work without access to collaborators, to instruments, to informationsources and, sometimes, to distant computers. computers and communication networks are increasinglynecessary for that access. three technologies are concerned with communications and collaboration: wordprocessing, electronic mail, and networks. word processing and electronic mail are arguably the most pervasiveof all the routine uses of computers in research communication. electronic mailšsending text from onecomputer user to another over the networksšis replacing written and telephone communication among manycommunities of scientists, and is changing the ways in which these communities are defined. large,collaborative projects, such as oceanographic voyages, use electronic mail to organize and schedule experiments,coordinate equipment arrivals, and handle other logisticalif kitchen appliances were like softwareif kitchen appliances were like programs, they would all look alike sitting on the counter. they would allbe gray, featureless boxes, into which one places the food to be processed. the door to the box, like thebox itself, is completely opaque.on the outside of each box is a general description of what the box does. for instance, one box mightsay: ﬁmakes anything a mealﬂ; another: ﬁcooks perfectly every timeﬂ; another: ﬁnever more than 100calories a serving.ﬂ you can never be exactly sure what happens to food when it is placed in these boxes.they don't work with the door open, and the 200page user's manual doesn't give any details.working in a kitchen would be a matter of becoming familiar with the idiosyncracies of a small numberof these boxes and then trying to get done what you really want done using them. for instance, if you wanta friedegg sandwich, you might try the ﬁmakes anything a mealﬂ box, since a sandwich is a sort of meal.but because you know from past experience that this box leaves everything coated with grease, you usethe ﬁnever more than 100 caloriesﬂ box to postprocess the output. and so on. the result is never what youreally want, but it is all you can do.you aren't allowed to look inside the boxes to help you do what you really want to do. each box issealed in epoxy. no one can break the seal. if the box seems not to be working right, there is nothing youcan do. even calling the manufacturer is no help, because the box is not under warranty to be fit for anyparticular purpose. the manufacturer do have help lines, but not for help with broken boxesšrather to helpyou figure out how to use functioning boxes. but don't try to ask how your box works. the helpline peopledon't know, or if they do, they won't tell you. several times a year you get a letter from the manufacturertelling you to ship them your old box and they will send you a new one. if you do so, you find yourself with ashinier box, which does whatever it did before a little faster, or perhaps it does a little morešbut since youwere never sure what it did before, you cannot be sure it's better now.source: mark weiser, 1987. ﬁsource code,ﬂ ieee computer, 20(11): 66œ73.see box on document processing, page 19. the use of information technology in research18information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.details. with the advent of electronic publishing tools that help lay out and integrate text, graphics, and pictures,mail systems that allow interchange of complex documents will become essential.networks range in size from small networks that connect users in a certain geographic area, to national andinternational networks. scientists at different sites increasingly use networks for conversations by electronic mailand for repeated exchanges of text and data files.the panel has identified two major trends in the way information technology is changing collaboration andcommunication in scientific research: information can be shared more and more quickly. for example, one of the first actions of the federalgovernment after the discovery of the new hightemperature superconductors was to fund, through thedepartment of energy's ames laboratory, the creation of a superconductivity information exchange.the laboratory publishes a biweekly newsletter on advances in hightemperature superconductivityresearch, available in both paper and electronic forms; the electronic version is sent out to some 250researchers. researchers are making new collaborative arrangements. the technology of networks providesincreased convenience and faster turnaround timesšoften several completed message exchanges in oneday. for shorter messages, special software allows realtime exchanges.see box on collaboration, page 20.document processing[an] area of significant change is document processing. this began in the 1960s with a few simpleprograms that would format typed text. in the context of unix* in the 1970s, these ideas led to a newgeneration of document processing programs and languages, such as scribe and the unixbased toolstroff, eqn, tbl, and pic. the quintessence of these ideas are knuth's tex and metafont systems, whichhave begun to revolutionize the world's printing industry. in workstations, these ideas have producedwysiwyg (wizzywig, or ﬁwhat you see is what you getﬂ) systems that display formatted text exactly as itwill appear in print. international standards organizations are considering languages for describingdocuments, and some software manufacturers are constructing systems, such as the postscriptprotocols, embodying these ideas. the nsfsponsored expres project, at the university of michigan andcarnegie mellon university, illustrates a serious effort to develop a standard method of exchanging fullscientific documents by network. lowcost laser printers now make advanced document preparation andprinting facilities available to many people with workstations and personal computers. it is now possible foreveryone to submit highquality, cameraready copy directly to publishers, thus speeding the publication ofnew results; however, it is no longer true that a wellformatted document can be trusted to have undergonea careful review and editing before being printed.source: peter j. denning, 1987, position paper: information technology in computing.the use of information technology in research19information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.as lederberg noted a decade ago (lederberg, 1978), digital communication allows scientists to definecollegial relationships along the lines of specialized interests rather than spatial location. this is immenselybeneficial to science as a whole, but causes some consternation among administrators who find more loyalty todisciplines than to institutions.technologies in the process of development show the networks' remarkable potential. multimedia mailallows researchers to send a combination of still images, video, sound, and text. teleconferencing providessimultaneous electronic links among several groups. electronic chalkboards allow researchers to draw on theirchalkboard and have the drawing appear on their computer and on the computers of collaborators across thecountry. directory services, or ﬁnameservers,ﬂ supply directories of the names and network addresses of users,processes, and resources on a given network or on a series of connected networks. program distribution servicesinclude the supply of mathematical software to subscribers. a spectacular new technology is represented in themetal oxide semiconductor implementation system (mosis), a service that contracts for the manufacture ofvery largescale integrated (vlsi) chips from circuit diagrams pictured on a subscriber's screen. fabrication timeis often less than 30 days. in one notable example, the researchers designing a radiotelescope in australiadesigned custom chips for controlling the telescope. mosis returned the chips in a matter of days; the normalmanufacturing process would have taken months and would have delayed the development of the instrumentconsiderably.new forms of collaboration through the networksthe development of common lisp (a programming language) would most probably not have beenpossible without the electronic message system provided by arpanet, the department of defense'sadvanced research projects agency network. design decisions were made on several hundred distinctpoints, for the most part by consensus, and by simple majority vote when necessary. except for two oneday facetoface meetings, all of the language design and discussion was done through the arpanetmessage system, which permitted effortless dissemination of messages to dozens of people, and severalinterchanges per day.the message system also provided automatic archiving of the entire discussion, which has provedinvaluable in preparation of this reference manual. over the course of thirty months, approximately 3000messages were sent (an average of three per day), ranging in length from one line to twenty pages– itwould have been substantially more difficult to have conducted this discussion by any other means, andwould have required much more time.source: guy steele, 1984. common lisp: the language. bedford, ma: digital press, pp. xixii.reprinted with permission. copyright digital press/digital equipment corporation.the use of information technology in research20information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.to share complex information (such as satellite images) over the networks, researchers will need to be ableto send entire pictures in a few seconds. one technique that is likely to receive more attention in the future is datacompression, which removes redundant information and converts data and images to more compact forms thatrequire less time to transmit.among the most important of potential applications of information technology is the emergence of a trulynational research networkšthat is, a set of connections, or gateways, between networksšto which everyresearcher has access. the national science foundation has announced its intention to serve as a lead agency inthe development of such a network, beginning with a backbone, called nsfnet, that links the nsfsupportedsupercomputing centers, and widening to include other existing networks.widespread access to networks will also offer much more than just communications links. they canbecome what the network serving the molecular biology community aims to be: a fullfledged information system.difficulties encountered the principal difficulty with communicating across research communities viaelectronic mail and file transfer technologies is incompatibility. the networks were formed independently,evolved over many years, and are now numerous. consequently, networks use different protocols, that is,different conventions for packaging data or text for transmission, for locating an appropriate route from sender toreceiver over the physical network, and for signaling the start and stop of a message. for example, a physicist onthe high energy physics network (hepnet) trying to send data to a physicist on one of the regional networkswould first have to ask ﬁwhat network are you on?ﬂ; ﬁhow do i address you?ﬂ; and ﬁwhat form do you want theinformation in?ﬂ in the gateway between two networks, the protocols of the first network must be removed fromthe message and the protocols for the second added. under heavy traffic loads, the gateways can becomebottlenecks. as a result, navigating from one network to a researcher on another is timeconsuming, tiresome,and often unreliable; navigating over two networks to a researcher on a third is prohibitively complex.text can frequently be moved from one word processing system to another only with significant loss offormatting informationšincluding the control of spacing, underlining, margins, or indentations. graphics canonly rarely be included with text. such issues of compatibility may delay the expansion of electronic publishingas well as electronic proposal submission and reviewšthe goals of the national science foundation's expresproject.the issues are summarized succinctly by denning: ﬁmost word processors are inadequate for scientificneeds: they cannot handle graphs, illustrations, mathematics and layout, and myriad file formats make exchangeextremely difficult. with so many experts and so much competition in the market, it is hard to win agreement onstandards. there is virtually no electronic support for the remainder of the process of scientific publicationšsubmission, review, publication, andthe use of information technology in research21information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.distribution. these issues can be expected to be resolved over the next few years, as document interchangeformats are adopted by standards organizations and incorporated into software revisions and equipmentupgrades. however, the transition process will not be painlessﬂ (denning, 1987, pp. 26œ27).in addition, some networks limit use under certain circumstances; for instance, one network barscommunication among researchers at industrial laboratories. the fear is that corporations would use a researchnetwork for commercial profit or even for sales or marketing. the panel believes such fear is misplaced and thatnetworks should be open for all research communication.on the whole, the management of the networks is anarchic. networks operate not as though they were aservice vital to the health of the nation's research community but as small fiefdoms, each with strong disciplinarydirection, with little incentive to collaborate. the national science foundation has taken an early leadership role,with such initiatives as nsfnet, which addresses many of the current networking problems, and the expresproject, which establishes standards for the electronic exchange of complex documents. such efforts to provideintegration and leadership are vital to increased research productivity.boxes on pages 22œ27 examine network use alternatives.from a network to an information resource prototype: bionetbionet is a nonprofit resource for molecular biology computing that provides access to software,recent versions of databases relevant to molecular biology, and electronic communications facilities. workis in progress to expand bionet as a logical network reaching molecular biologists throughout theresearch community worldwide. many existing physical networks are in use by molecular biologists, and itis bionet's aim to utilize them all. bionet is working on plans to provide molecular biologists with accessto one or more supercomputers or parallel processing resources. special programs will be developed toprovide molecular biologists with an easy interface to submit supercomputer jobs.especially active are the methodsandreagents bulletin board (for requesting information onlab protocols and/or experimental reagents) and the researchnews bulletin board, which has becomea forum for posting interesting scientific developments and also a place where scientists can introduce theirlabs and research interests to the rest of the electronic community. bulletin boards have been instituted forthe genbank and embl nucleic acid sequence databases. copies of messages on these bulletin boardsare forwarded to the database staff members for their attention. these bulletin boards serve as a mediumfor discussing issues relating to the databases and as a place where users of the databases can obtainassistance. along these same lines bionet has developed the genpub program that facilitatessubmission of sequence data and authorentered annotations in computerreadable form directly togenbank and embl via the electronic mail network.the journals cell and cabios have established accounts on bionet and the journal of biologicalchemistry and several othersthe use of information technology in research22information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.information storage and retrievalcurrent uses how information is stored determines how accessible it is. scientific texts are generallystored in print (in the jargon, in hard copy) and are accessible through the indices and catalogs of a library. sometexts, along with programs and data, however, are stored electronicallyšon disks or magnetic tapes to be run incomputersšand are generally more easily accessible. in addition, collections of data, known as databases, aresometimes stored in a central location. in general, electronic storage of information holds enormous advantages:it can be stored economically, found quickly without going to another location, and moved easily.one kind of database holds factual scientific data. the chemical abstracts service, for example, has alibrary of the molecular structures of all chemical substances reported in the literature since 1961. genbank is alibrary of known genetic sequences. both the national aeronautics and space administration and the nationaloceanic and atmospheric administration have thousands of tapes holding data on space and the earth andatmosphere.will also soon be on board. several journals have indicated an interest in publishing research abstractson bionet in advance of hardcopy articles.annotated examples of program usage have been included into the help me system. the examples,formatted to be suitable for printing out as a manual, cover the major uses of the bionet software for dataentry, gel management, sequence, structure and restriction site analysis, cloning simulations, databasesearches, and sequence similarities and alignments. a manual of standard molecular biology lab protocolshas also been added to help me for users to reference.one of bionet's major goals is to serve as a focus for the development and sharing of new softwaretools. towards achieving this goal, bionet has made available to the community a wide variety ofimportant computer programs donated by a number of software developers. a collaborative effort hasoccurred between the bionet staff and the software authors to expand the usefulness of importantsoftware by making it compatible with a number of hardware and user community constraints.bionet provides an increasing number of databases online: lists of restriction enzymes; a bank ofcommon cloning vector restriction maps and complete vector sequences; a database of regularexpressions derived from published consensus sequences; the searchable full text of a recent revision ofﬁgenetic variations of drosophila melanogasterﬂ by dan l. lindsley and e.h. grell (the drosophila ﬁredbookﬂ). some of these can be used as input to search programs. bionet invites curators of genetic andphysical genome maps to use this resource for the collection, maintenance, and distribution of theirdatabases.source: roode et al., 1988. ﬁnew developments at bionet,ﬂ nucleic acids research, 16(5):1857œ1859.the use of information technology in research23information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.a second kind of database, a reference database, stores information on the literature of the sciences. forexample, chemical abstracts service has abstracted all articles published in journals of chemistry since 1970and makes the abstracts available electronically. the national library of medicine operates services that index,abstract, and search the literature database (known as medlars). in addition, it distributes copies of thedatabase for use on local computers and has developed a communications package, called grateful med,that simplifies searching the major medlars filesšover six million records through 1987. in addition tobiomedicine and clinical medicine, the national library of medicine partially covers the literature of thedisciplines of population control, bioethics, nursing, health administration, and chemistry. one of its mostimportant databases, for instance, is toxline, which references the chemical analysis of toxins. informationsearch services have grown up around these and other databases, including a number of commercial ones, andnow constitute a substantial industry.birth of a network: a history of bitnet (excerpted).bitnet (because it's time network) began as a single leased telephone line between the computercenters of the city university of new york (cuny) and yale university. it has developed into aninternational network of computer systems at over 800 institutions worldwide. because membership is notrestricted by disciplinary specialty or funding ability, bitnet plays a unique role in fostering the use ofcomputer networking for scholarly and administrative communication both nationally and internationally.in 1981, cuny and yale had been using internal telecommunications networks to link computers oftheir own. the new york/new haven link allowed the same exchanges to take place between twouniversities. the founders of bitnetšira fuchs, then a cuny vice chancellor, and greydon freeman, thedirector of the yale computing centeršrealized that the fledgling network could be used to share a widerange of data. furthermore, the ease and power of electronic mail showed new potential for cooperativework among scholars; collective projects could now be undertaken that would have been difficult orimpossible if conducted by postal mail or by phone.fuchs and freeman approached the directors of other academic computer centers with major ibminstallations to invite them to become members of the new network. the plan of shared resources thatbitnet offered included two proposals: a) that each institution pay for its own communications link to thenetwork; and b) that each provide facilities for at least one new member to connect. software was used tocreate a storeandforward chain of computers in which files, messages, and commands are passed onwithout charge from site to site to their final destination. bitnet became a transcontinental network in1982 when the university of california at berkeley leased its own line to cuny. berkeley agreed to allowother califora database, taken together with the procedures for indexing, cataloging, and searching it, makes up aninformation management system. some potentials of information management systems have been predicted foryears, beginning withthe use of information technology in research24information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.vannevar bush's memex (bush, 1945). the box on pages 28œ29 illustrates a current working informationmanagement system that links texts and databases in genetics and medicine.nia institutions to link to the network through its line, in return for some expense sharing.in 1984, ibm agreed to support cuny and educom (a nonprofit consortium of colleges, universities,and other institutions founded in 1964 to facilitate the use and management of information technology) inorganizing a centralized source of information and services to accommodate the growing number ofbitnet users. educom set up a network information center (bitnic), whose ongoing functions includethe handling of registration of new members; at the same time, cuny established a development andoperations center (bitdoc), which develops tools for the network.bitnet's success (it is now in all fifty states) led to the formation of a worldwide network of computersusing the same networking software: in europe and the middle east (earn, the european academicresearch network), canada (netnorth), japan, mexico, chile, and singapore (all of which are members ofbitnet). there is also active interest from other countries in the far east, australia and new zealand, andsouth america. although political and funding considerations have forced their administrative segregation,bitnet, earn, and netnorth form one topologically interconnected network.success has also meant some further structuring of what had once been essentially a buddy system.bitnet is now governed by a board of trustees elected by and from its membership. the members of theboard each participate in various policymaking committees focusing on network usage, finance andadministration, bitnic services and activities, and technical issues. what began as a simple device forintercampus sharing is simple no longer.source: holland cotter, 1988. birth of a network: a history of bitnet. cuny/university computercenter communications, 14:1œ10.difficulties encountered for all disciplines, both factual and reference databases promise to be significantsources of knowledge for basic research. but to keep this promise, a pandora's box of problems will have to besolved.difficulties encountered with factual databases, stated succinctly, are: the researcher cannot get access todata; if he can, he cannot read them; if he can read them, he does not know how good they are; and if he findsthem good, he cannot merge them with other data. researchers have difficulty getting access to data stored byother researchers. such access permits reanalysis and replication, both essential elements of the scientificprocess. at present, with a few exceptions, data storage is largely an individual researcher's concern, in line withthe tradition that researchers have first rights to their data. the result has been a proliferation of idiosyncraticmethods for storing, organizing, and indexing data, with one researcher's data essentially inaccessible to all otherresearchers.the use of information technology in research25information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.even if a researcher gets access to a colleague's data, he may not be able to read them. the formats withwhich data are written on magnetic tapešlike the formats used in word processing systemsšvary fromresearcher to researcher, even within disciplines. the same formatting problems prohibit the researcher frommerging someone else's data into his own database. in order either to read or to merge another's data,considerable effort must be dedicated to converting tape formats.finally, when a researcher gets access to and reads another's database, he often has no notion of the qualityof the data it contains. a number of proposals (see branscomb, 1983, national research council, 1978) havebeen made for the creation of what are called evaluated databases, in which data have been verified byindependent assessment.in fields such as organizational science or public health, the costs of collecting and storing data are so largethat researchers often have to depend on case studies of organizations or communities to test hypotheses.researchers in these fields have proposed combining data from many surveys into databases of national scope. ifdifferences in research protocols and database formats can be resolved, such national databases can increase thequality and effectiveness of research.the study panel's experience with its own electronic mail is instructive.most of the members of the panel use electronic mail in their professional work; some use itextensively, exchanging as many as seventy messages in one day. at their first meeting, panel membersand staff decided it would be useful to establish electronic communication links for the panel. using anetwork to which he had access, one of the panel members devised a distributionlist scheme for thepanel. he designed a system that would allow panel members to exchange messages or documents easilyby naming a common group ﬁaddress.ﬂ this group address would connect everyone by name from theirown network. panel members would not have to remember special codes or routes to other networks, butcould use their own familiar network. also, messages could be sent to one, several, or all of the panelmembers at once.between december 1986 and march 1988, nearly 2,000 messages went out using the panel's specialelectronic group address. in line with what has been found in systematic research on electronic mail by adhoc task groups (finholt, sproull and kiesler, 1987), most of the messages went from study staff managingthe project to panel members. typically, staff used electronic mail to perform coordinating and attentionalfunctions, e.g., to structure meetings, to ask panel members for information or to perform writing tasks, andto provide members with progress reports. in addition, some panel members sent mail through othernetwork channels to each other; for instance, two panel members exchanged electronic mail aboutcomputers in the oceanographic community through bitnet, arpanet, and omnet.although previous research and our ownthe use of information technology in research26information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.the primary difficulty encountered with reference databases is in conducting searches. most informationsearches at present are incomplete, cumbersome, inefficient, expensive, and executable only by specialists.searches are incomplete because databases themselves are incompletešupdating a database is difficult andexpensivešand because information is stored in more than one database. searches are cumbersome andinefficient because different databases are organized according to different principles and cannot readily besearched except by commands specific to each database. searches are expensive because access is expensive (asmuch as $300 per hour), because network linkages to the databases impose substantial surcharges, and becausethe inefficiency of the systems means that searches may have to be repeated.a difficulty common to both scientific and reference databases is a pressing need for new and morecompact forms of data storage. disciplines such as oceanography, meteorology, space sciences, and high energyphysics have already gathered so much data that more efficient means of storage are essential; and others arefollowing close behind. one solution seems to lie in optical disk storage, for which various alternativetechnologies are under development. currently, these new techniques lack commonly accepted standards.informal observations agree in suggesting that the electronic group mail scheme helped the panel towork more efficiently, the system was used much less extensively than had been originally envisioned. forexample, when delivery of report drafts was crucial, the staff relied on overnight postal mail. networkservice inadequacies and technical problems are partly to blame; for example, it took months beforemessages could be sent predictably and reliably to every panel member. because the networks do notfacilitate access to service support (comparatable to telephone system operators, for example), panelmembers had to rely on their own resources to remedy any system inefficiencies. for example, changes toelectronic mail addresses in the system could not be made after a few months, so that new addresses hadto be added to individual messages.such technical problems, though by no means insurmountable, were annoying. analysis of a sample ofmessages received by panel staff indicates that approximately 10 percent contained some complaint aboutdelays, losses of material in transmission, or unavailability of the group mail system. often, documentswere difficult to read because document formatting codes embedded in the document files were removedprior to transmission. a message legible on one system might be filled with unintelligible characters whenreceived on another. at considerable difficulty, some panel members converted messages receivedelectronically to formats they could read using their text editors. then they would type in their ownrevisions, which once again would have to be converted to plain formats to be sent back through thenetworks. this experience suggests that much needs to be done to make internetwork communication bygroups more efficient and easier to use.the use of information technology in research27information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.another difficulty is that stored data gradually become useless, either because the storage media decay orthe storage technology itself becomes obsolete. data stored on variant forms of punched cards, on paper tape, oron certain magnetic tape formats may be lost due to the lack of reading devices for such media. even if thedevices still exist, some data stored on magnetic tapes will be lost as the tapes age, unless tapes are copiedperiodically. needless to say, such preservation activities often receive low priority.an important archival activity that also receives a low priority is the conversion of primary and referencedata from precomputer days into machine readable form. in this regard, the efforts of the chemical abstractservice to extend their chemical substance and reference databases are praiseworthy.how a library uses computers to advance productivity in sciencein 1985 the william h. welch medical library of the johns hopkins university began a uniquecollaboration with dr. victor a. mckusick, the johns hopkins university press, and the national library ofmedicine to develop and maintain an online version of mckusick's book mendelian inheritance in man(known as omim, for online mendelian inheritance in man). while the book contains 3,900 phenotypes (aspecific disorder or substance linked to a genetic disease) and updates are issued approximately every fiveyears, omim currently describes more than 4,300 phenotypes and is updated every week. a gene map isavailable, keyed to the phenotype descriptions.any registered user worldwide can dial up omim and search its contents through a simple threestepprocess: 1) state the search in simple english (e.g., relationship between duchenne muscular dystrophyand growth deficiency hormone); 2) examine the list of documents, which are presented in ranked order ofrelevance; and 3) select one or more documents to read in detail. having selected a document, thesearcher can determine through a single keystroke whether the phenotype has been mapped to a specificchromosome. omim entries are also searchable in a related file, the human gene mapping library(hgml) at yale university. by mid1988, researchers will be able to use the same access code to enterand search three related databases: hgml in new haven, the jackson laboratory mouse map in barharbor, and omim in baltimore.omim is more than an electronic text. it is a dynamic databases with many applications. searching theknowledge base is only one of its uses. it can be used as a working tool. for example, at the last biennialinternational human gene mapping conference in paris (september 1987) the results of the committees'deliberations were used to update and regenerate the database each evening. everyanother difficulty in storing information is private ownership. by tradition, researchers hold their dataprivately. in general, they neither submit their data to central archives nor make their data available viacomputer. increasingly, however, in disciplines like meteorology and the biomedical sciences, submission ofprimary data to data banks has become accepted as a duty. in the field of economics, the national sciencefoundation now requires that data collected with the support of the economics program be archived in machinereadablethe use of information technology in research28see box on satellitederived data, page 30.information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.form, and that any professional article citing program support be accompanied by a fully documented diskdescribing the underlying data. in the social sciences, a 1985 report of the national research council'scommittee on national statistics recommended both that ﬁsharing data should be a regular practiceﬂ and that aﬁcomprehensive reference service for computerreadable social science data should be developed.ﬂ (fienberg,martin, and straf, 1985.)morning, the conferees had fresh files to consult. this information was available worldwide at the sametime. in the future, these conferences can take place electronically as frequently as desired by the scientificcommunity.omim is a node in an emerging network of biotechnology databases, data banks, tissue repositories,and electronic journals. in a few years, it may be possible to enter any of these files from any one of therelated files. through this kind of linkage, omim may serve as a bridge between the molecular geneticistsand the clinical geneticists. currently, these databases are primarily text or numerical files. as technologyimproves and becomes ubiquitous, and as network bandwidth expands, databases will routinely includevisual images and complex graphics. it may also be possible to jump from one point within a file to relevantand related points deep within other files.omim and its future manifestations result from collaborative efforts and support from diverse groups.dr. victor a. mckusick is the scientific expert responsible for the knowledge base; his editorial staff addsnew material and updates the database. the national library of medicine developed omim as part of itsonline reference works program. the welch medical library provides the computers, network gateways,database maintenance and management, and user support. finally, the howard hughes medical instituteprovides partial support for access, maintenance, and future development of the system.the welch library must work closely with both the author and the users to represent researchknowledge in ways that best suit the users' purposes. it must be able to respond quickly to the changingneeds of the author and the users. it is in a unique position to study and engineer a new kind of knowledgeutility. the omim effort is part of a project to develop a range of online texts and databases in genetics andinternal medicine, carried out in the library's laboratory for applied research in academic information.in addition, peer review of articles and proposals has been constrained by the difficulty of gaining access tothe data used for analysis. if writers were required to make their primary data available, reviewers could repeat atleast part of the analyses reported. such review would be more stringent, would demand more effort fromreviewers, and raises a number of operational questions that need careful consideration; but it would arguablylead to more careful checking of published results.underlying the difficulties in information storage and retrieval are problems in the institutional managementof resources. who is to manage, maintain, and update information services? who is to create and enforcestandards? at present the research community has three alternative answers: the federal government, whichmanages such resources as medline and the genbank; professionalthe use of information technology in research29information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.societies, such as the american chemical society, which manages the chemical abstracts service, and theamerican psychological association, which manages psychological abstracts; and private forprofit enterprisessuch as the institute for scientific information.handling satellitederived observational dataat present both the national aeronautics and space administration (nasa) and the national oceanicand atmospheric administration (noaa) operate earthorbiting satellites and collect data from them. bothnoaa and nasa store large volumes of primary data from the satellites on digital tape. both have facedproblems, although each organization's problems are different. noaa, until 1985, had a system that, forpurposes of satellite operations, stored environmental satellite data on a terabit memory system (tbm).the tbm technology was used from 1978 to 1985, at which time it became obsolete; the more than 1,000tapes of data collected have been reduced by about 40 percent in transforming most of the useful materialsto standard digital tape for storage. nasa has used standard digital tape and disk storage technologiesand, since ceding the landsat satellites, has recorded and saved data from its research earthobservingsatellites as needed.both nasa and noaa face real problems in making data accessible for scientific analysis. nasa hasexpended time, effort, and money building a number of satellite data distribution systems that providedigital data archives and a catalog of satellite data holdings, as well as images and graphical analysesproduced from satellite data. for example, nasa's national space science data center received and filledsome 2,500 requests for tapes, films, and prints in the first half of fiscal 1988, and also provided networkaccess to specific databases. noaa has been largely unable to get financial support for its proposedsatellite data management systems. selection of needed information from among the data availableremains a problem. some pilot systems under development at both agencies succeed in leading in the userthrough a catalog, but fail to contain much valuable new information and data. both agencies continue tohold great amounts of environmental satellite data in their permanent achieves that are difficult to access,expensive to acquire, and as a result are ignored by many researchers who could benefit from their use.much remains to be done to improve access to important satellitederived data.new opportunities: approaching the revolution asymptoticallythe information technologies and institutions of the past that revolutionized scholarly communicationšwriting, the mails, the library, the printed book, the encyclopedia, the scientific societies, the telephonešmadeinformation more accessible, durable, or portable. the advent of digital information technology and managementcontinues the revolution, suggesting a vision, still somewhatthe use of information technology in research30information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.incoherent, of new ways of finding, understanding, storing, and communicating information.some technologies involved in the revolution are simulations of natural (or hypothesized) phenomena; visualization of phenomena through graphical displays of data; and emerging use of knowledgebased systems as ﬁintelligent assistantsﬂ in managing and interpreting data.simulations allow examination of hypotheses that may be untestable under normal conditions. plasmaphysicists simulate ways of holding and heating a hot, turbulent plasma until it reaches the temperaturesnecessary for fusion. cosmologists simulate the growth of galaxies and clusters of galaxies in an infant universe.engineers simulate the growth of fractures in a metal airplane wing or nuclear reactor. chemists' simulationsmay someday be sophisticated enough to screen out unproductive experiments in advance. drug companies areconsidering the use of simulations to design drugs for a particular function, for example, a nonaddictive drugthat also kills pain. in general, simulations extend researchers' ability to model a system and test the modeldeveloped.see box on simulation, below.see box on visualization, pages 3233.uses of simulation in econometricssimulation techniques take estimated relationships or numerical models that appear to be consistentwith observations of actual behavior and apply them to problems of predicting the changes induced by time,or of measuring the relationships among sets of economic variables. for example, simulation models havebeen utilized to study the effects of oil price changes on the rate of inflation, proposed policies regardinglabor law, and future interest rates. in addition, exchanges among groups of agents in an economy havebeen used in dynamic inputoutput analysis to make inferences about the feasible or likely future course ofeconomic growth in the entire economy or within specific industries or regions.there is a growing interest in investigating the properties of models that represent the workings offirms, markets, and whole economies as nonlinear adaptive systems. recently this has begun to expandthe reliance placed by essentially theoretical researchers upon extensive applications of numericalsimulation methods. finally, in both extensions of the line of inquiry just noted and in other contexts, directsimulation of stochastic processes via monte carlo techniques can be used by economists to gain insightsinto the properties of stochastic systems that resist deductive techniques due to their (current) analyticintractability.source: paul a. david and w. edward steinmuller, 1987. position paper: ﬁthe impact of informationtechnology upon economic science,ﬂ p.21.visualization techniques turn the results of numerical computations into images. the remarkable ability ofthe human brain to recognize patterns in pictures allows faster understanding of results in solutions to complexproblems, as well as faster ways of interacting with computer systems and models. forthe use of information technology in research31information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.instance, while small molecules have a few dozen atoms and are easy to visualize, large molecules, like proteins,have tens of thousands of atoms. a useful physical model of the structure of a protein might stand six feet highand cost several thousand dollars. moreover a researcher could not slice a physical model to see how it looksinside; with visualization techniques, he could. visualization is the single advanced technology most widelymentioned by panel members and position paper writers. (for a critical analysis of opportunities in visualimaging, see mccormick, defanti, and brown, 1987.)visualization in scientific computingscientists need an alternative to numbers. a technical reality today and a cognitive imperativetomorrow are the use of images. the ability of scientists to visualize complex computations and simulationsis absolutely essential to ensure the integrity of analyses, to provoke insights, and to communicate thoseinsights with others.several visually oriented computerbased technologies already exist today. some have been exploitedby the private sector, and offtheshelf hardware and software can be purchased; others require newdevelopments; and still others open up new research areas. visualization technology, well integrated intotoday's workstation, has found practical application in such areas as product design, electronic publishing,media production and manufacturing automation. management has found that visualization tools make theircompanies more productive, more competitive, and more professional.so far, however, scientists and academics have been largely untouched by this revolution incomputing. secretaries who prepare manuscripts for scientists have better interactive control and visualfeedback with their word processors than scientists have over large computing resources that cost severalthousand times as much.traditionally, scientific problems that required largescale computing resources needed all theavailable computational powerintelligent assistants can serve as interfaces between the researcher and the computer. just as computersincrease our power to collect, store, filter, and retrieve data, they can also help us reason about the data. over thelast three decades, computer scientists have been developing methods for symbolic information processing orartificial intelligence. while these programs are not fully intelligent in the sense that humans are, they allowcomputers to solve problems that are not reducible to equations.artificial intelligence programs have been written for many scientific tasks. these tasks are not expressiblein terms of numerical operations alone, and, thus, require symbolic computation. the programs fall into a generalclass, called expert systems, because they are programmed to reach decisions in much the same way as expertsdo. expert systems have been successfully applied to industrial areas such as manufacturing and banking. todate, only a few prototype systems have been written for scientific research. prototypes include programs thatassist in chemical synthesis planning, in planning experiments in molecular genetics, in interpreting mass aspectsof organic molecules, in trouthe use of information technology in research32information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.bleshooting particle beam lines for high energy physicists, and in automated theory formulation in chemistry,physics, and astronomy.to perform the analyses or simulations. the ability to visualize results or guide the calculationsthemselves requires substantially more computing power.electronic media, such as videotapes, laser disks, optical disks, and floppy disks, are now necessaryfor the publication and dissemination of mathematical models, processing algorithms, computer programs,experimental data, and scientific simulations. the reviewer and the reader will need to test models,evaluate algorithms, and execute programs themselves, interactively, without an author's assistance.scientific publication needs to be extended to make use of visualizationcompartible media.reading and writing were only democratized in the past 100 years and are the acceptedcommunication tools for scientists and engineers today. a new communication tool, visualization, in timewill also be democratized and embraced by the great researchers of the future.the introduction of visualization technology will profoundly transform the way science is communicatedand will facilitate the commission of largescale engineering projects. visualization and science go hand inhand as partners. no one ever expected gutenberg to be shakespeare as well. perhaps we will not haveto wait 150 years this time for the geniuses to catch up to the technology.source: b. h. mccormick, t. a. defanti, and m. d. brown, 1987. visualization in scientificcomputing (nsf report). computer graphics 21(6), acm siggraph: new york, association forcomputing machinery.the methods to needed to assist with complex reasoning tasks are themselves the subject of considerableresearch in such fields as computer science, cognitive science, and linguistics. research in these fields, in turn, isproducing tools that facilitate research in other disciplines.as these methods are used more widely in the future, some experts predict the conduct of research willchange dramatically. intelligent assistants, in the form of software, can carry out complex planning andinterpretation tasks as instructed, leaving humans free to spend time on other tasks. when these reasoningprograms are coupled to systems with datagathering capabilities, much of the drudgery associated with researchplanning, data collection, and analysis can be reduced. research laboratories and the conduct of research willbecome even more productive. when every researcher has intelligent assistants at his/her disposal and when thefunctions of these assistants are interlinked, science will expand the fronties of knowledge even more rapidlythan it now does.future technologies will provide other forms of research support. programs that recognize and follownaturallanguage commands, like ﬁgive me the data from this file,ﬂ can simplify interaction between theresearcher and computer systems. spokenlanguage recognition offers the advantage of handsfree interaction.speech production, in which computers generate connected sentences in responses to instructions, will,according to one author, lead to a revolutionary expansion in the use of computers in business and officeenvironments (koening, 1987). a variety of manipulative interfaces of different kinds are under activethe use of information technology in research33information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.exploration (foley, 1987). for example, the ﬁdata gloveﬂ is a glove on a computer screen that is an image of aspeciallyengineered glove on a researcher's hand. the data glove follows the motions of the researcher's hand,permitting a researcher, for instance, to manipulate a molecule directly on screen. when the data glove iscoupled with feedback devices in the researcher's glove, a researcher can ﬁfeelﬂ the fit between two molecularstructure surfaces.the panel believes that the mature and emerging information technologies, taken together, suggest a visionof new approaches to scientific and engineering research. the vision focuses on an open infrastructure forresearch support and communication among researchers, along with the services for maintaining thisinfrastructure. below are several examples of parts of the vision and of forms the vision could take. we discussfurther steps in the report's final section on recommendations.see boxes on pages 35œ41.institutional and behavioral impediments to the use of informationtechnology in researchunderlying many of the difficulties we have discussed in the use of information technology in research areinstitutional and behavioral impediments. we have identified six such impediments that seem to affect researchin most or all disciplines:molecular graphicsthe use of interactive computer graphics to gain insight into chemical complexity began in 1964.interactive graphics is now an integral part of academic and industrial research on molecular structures andinteractions, and the methodology is being successfully combined with supercomputers to model complexsystems such as proteins and dna. techniques range from simple blackandwhite bitmappedrepresentations of small molecules for substructure searches and synthetic analyses to the mostsophisticated 3d color stereographic displays required for advanced work in genetic engineering and drugdesign.the attitude of the research and development community toward molecular modeling has changed.what used to be viewed as a sophisticated and expensive way to make pretty pictures for publication isnow seen as a valuable tool for the analysis and design of experiments. molecular graphics complementscrystallography, sequencing, chromatography, mass spectrometry, magnetic resonance, and the othertools of the experimentalist, and is an experimental tool in its own right. the pharmaceutical industry,especially in the new and flourishing fields of genetic and protein engineering, is increasingly usingmolecular modeling to design modifications to known drugs and to propose new therapeutic agents.source: b. h. mccormick, t. a. defanti, and m. d. brown, 1987. visualization in scientificcomputing (nsf report). computer graphics 21(6). acm siggraph: new york, association forcomputing machinery.the use of information technology in research34information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.(1) issues of costs and cost sharing;(2) the problem of standards;(3) legal and ethical constraints;(4) gaps in training and education;(5) risks of organizational change; and(6) most fundamental, the absence of an infrastructure for the use of information technology.issues of costs and cost sharing many forces drive developments in information technology and itsapplication to research. the result of these developments is constantly increasing requirements for higherperformance computer and communications equipment, making current equipment obsolete. universities andother research organizations are spending increasing fractions of their budgets on information technology tomaintain competitive research facilities and to support computerrelated instruction. at a number of privateresearch universities, for example, tution has increased faster than inflation for a number of years, in part tocover some of these costs. it is unrealistic to rely on such funding sources to cover further cost increases that willbe required to build local network infrastructures.research on integrated information systemsnearly a decade ago the association of american medical colleges (aamc) recognized the strategicimportance of information technology to the conduct of biomedical research. in response to a studyreleased by the aamc in 1982, the national library of medicine has supported eleven institutions in effortsto develop strategic plans and prototypes of an integrated academic information management system(iaims). the objective of iaims is to develop the institutional information infrastructure that permits theirindividuals to access information they need for their clinical or research work from any computer terminalwhenever and whenever it is needed, pull that information into a local environment, and read, modify,transform it, or otherwise use it for many different purposes.several pilot prototype models have emerged. the baylor medical college is developing a ﬁvirtualnotebook,ﬂ a set of tools for researchers to collect, manipulate, and store data. georgetown medical centerhas a model called biosynthesis that automatically routes a user's query from one database to another.the knowledge sector development of a comprehensive patient management clinical decision supportsystem called help is the iaims project focus at the university of utab; and johns hopkins university isdeveloping a knowledge workstation.a related issue is who will pay for the costs of research computing support historically, such costs havebeen partially recovered by bundling them into charges for use of timeshared mainframe computers. as usagehas moved from campus mainframes to other options (ranging from supercomputer centers to workstations andpersonal computers), this source of revenue has been lost, while the needs for administrative staff and supportpersonnel for consulting.the use of information technology in research35information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.training, and documentation have continued. efforts to move research support into indirect cost categories havenot succeeded as many research institutions and universities fact caps on indirect cost rates and have no room toaccommodate new costs.advances in communications and computing generate new service that require subsidy during the first yearsof their existence if they are to be successfully tested. this is particularly true of networkrelated services.building services into a national network for research will require significant federal, state, and institutionalsubsidy, which cannot be recovered from user service charges until largescale connectivity has been achievedand services are mature. sources for these subsides must be determined.a resonable model.although the panel is unaware of anything precisely like the vision it holds for sharing information,proposals for the newly established national center for biotechnology information (ncbi) at the nationallibrary of medicine may come close. the ncbi proposes to facilitate easy and effective access to acomprehensive array of information sources that support the molecular biology research community.many, but not all, of these sources are electronic. they encompass raw data, text, bibliographicinformation, and graphic representations. ownership and responsibility for development and maintenanceof these sources range from individual researchers to departmental groups, institutes, professionalorganizations, and federal agencies. each was designed to serve specific needs and audiences, created inmany different hardware configurations and software applications. consequently, ncbi's mission requiresexperts in both information technologies and biotechnologies, ncbi staff must provide directories to knowledge sources; create useful network gateways between systems; assist users in using databases effectively; reduce incompatibilities in retrieval approaches, vocabulary, nomenclature and data structures; promote standards for representing information that will reduce redundancy and detect inconsistenciesor errors; provide useful tools for manipulating and displaying data; and identify new analytic and descriptive services and systems.some computingintensive universities (e.g., carnegie mellon university and brown university) andmedical centers (e.g., johns hopkins university, the university of utah, baylor university, and dukeuniversity) are also attempting to develop instances of the vision.methods used for cost recovery can have significant impacts on usage. two alternatives are to charge usersfor access to services or to charge users for the amount of service used. networks such as bitnet have grownsubstantially in connectivity and use because they have fixed annual institutional charges for membership andconnection, but charge no fees for use. useinsensitive charge methods (often referred to as the library model)are attractive to institutions because costs can be treated as infrastructure costs and are predictable. chargesthe use of information technology in research36information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.for amount of use, in contrast, can inhibit usage; a major inhibitor to use of commercial databases forinformation searches, for instance, is the unpredictability of user charges for time spent searching the databases.during the development of network services, it seems desirable to recover costs through fixed access chargeswherever possible.the problem of standards the development of standards for interconnection makes it possible for everytelephone in the world to communicate with every other telephone. the absence of commonly held andimplemented standards that would allow computers to communicate with every other computer and to accessinformation in an intuitive and consistent way is a major impediment to scholarly communication, to the sharingof information resources, and to research productivity.standards for computer communication are being developed by many groups. the pace of these efforts ispainfully slow, however, and the process is intensely political. the technologies are developing faster than ourability to define standards that can make effective use of them. further, standards that are developed prematurelycan inhibit technological progress; standards developed by one group (for example, and equipment vendor) inisolation create islands of users with whom effective communications is difficult or impossible.development of standards not only improves efficiency but also reduces costs. open interconnectionstandards permit competition among vendors, which leads to lowered costs and improved capabilities.proprietary standards restrict competition and lead to increased costs. federal government procurement ruleshave been major source of pressure on vendors to support open standards.current mechanisms for reaching agreement on standards need examination and significant improvement.such examination needs input from user groups, which will have to exert pressure on standards bodies and onthe vendors who are major players in the standardsetting process.legal and ethical constraints the primary legal and ethical constraints to wider use of informationtechnology are issues of the confidentiality of, and access to, data. the following discussion will only illustratethese issues; we believe they are too important and too specialized to be adequately addressed in a document asgeneral as this one. in the report's final section, we recommend the establishment of a body that will study andadvise on these issues.information technology has made possible largescale research using data on human subjects. for the firsttime, researchers can merge data collected by national surveys with data collected in medical, insurance, or taxrecords. for instance, in public health research, longterm studies of workers exposed to specific hazards can becarried out by linking health insurance data on costs with internal revenue data on subsequent earnings, socialsecurity data on disability payments, and mortality data, including data and cause of death (steinwachs, 1987,position paper: information technology and the conduct of public healththe use of information technology in research37information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.research). the scientific potential of such data mergers is enormous; the actual use of mergers is small,primarily because of concerns about privacy and confidentiality.the right to confidentiality of personal information is held strongly in our society. concerns about theconflict between researchers' needs and citizens' rights have been extensively explored by a number of scientificworking groups, under the auspices of both governmental agencies such as the census bureau and private groups(for example, the national academy of sciences). as more information about individuals is collected and crosslinked, fears are raised that determined and technically sophisticated computer experts will be able to identifyspecific individuals, thus breaching promises of confidentiality and privacy of information. the census bureau,in particular, fears that publicity surrounding such breaches of confidentiality will undermine public confidenceand inhibit cooperation with the decennial censuses.although there have been discussions and legislative proposals for outright restrictions on mergers ofgovernment survey or census data, a reasonable alternative seems to be to impose severe penalties on researcherswho breach confidentiality by making use of information on specific individuals. the issue here, as elsewhere inpublic policy problems, is the balance of benefits against costs. does better research balance the risk ofcompromising perceived fundamental rights to privacy? this is a topic that will need to be debated among bothresearchers and concerned constituencies in the general public.the far side of the dream: the library of the futureﬁcan you imagine that they used to have libraries where the books didn't talk to each other?ﬂ [marvinminsky, mit]the libraries of today are warehouses for passive objects. the books and journals sit on shelves,waiting for us to use our intelligence to find them, read them, interpret them, and cause them finally todivulge their stored knowledge. ﬁelectronicﬂ libraries of today are no better. their pages are pages of datafiles, but the electronic page images are equally passive.now imagine the library as an active, intelligent ﬁknowledge server.ﬂ it stores the knowledge of thedisciplines in complex knowledge structures (perhaps in a formalism yet to be invented). it can reason withthis knowledge to satisfy the needs of its users. the needs are expressed naturally, with fluid discourse.the system can, of course, retrieve and exhibit (the electronic textbook). it can collect relevant information;it can summarize; it can pursue relationships.it acts as a consultant on specific problems, offering advice on particular solutions, justifying thosesolutions with citations or with a fabric of general reasoning. if the usera related issue is that of acceptable levels of informed consent for human subjects. at present, consent isusually obtained from each respondent to a survey; it is described as informed because the respondentunderstands what will be done with responsesšusually, that they will be used only for some specific researchproject. datacollecting organizations protect the confidentithe use of information technology in research38information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.ality of the information obtained from respondents, but guarantee only that information about specificindividuals will not be released in such a way that they can be identified. the extent to which informed consentcan be given to unknown future uses of survey data, in particular to their merger with other data sources, is ofgreat concern to survey researchers. controlling the eventual uses of merged, widely distributed data sets wouldbe difficult.can suggest a solution or a hypothesis it can check this, even suggest extensions. or it can critique theuser viewpoint, with a detailed rationale of its agreement or disagreement.– the user of the library of the future need not be a person. it may be another knowledge systemšthat is, any intelligent agent with a need for knowledge. such a library will be a network of knowledgesystems, in which people and machines collaborate.publishing is an activity transformed. authors may bypass text, adding their increment to humanknowledge directly to the knowledge structures. since the thread of responsibility must be maintained, andsince there may be disagreement as knowledge grows, the contributions are authored (incidentally allowingfor the computation of royalties for access and use). knowledge base maintenance (ﬁupdatingﬂ) itselfbecomes a vigorous part of the new publishing industry.source: edward a. feigenbaum, 1986. autoknowledge: from file servers to knowledge servers. in:medinfo 86. r. salamon, b. blum, and m. jorgensen, eds. new york: elsevier science publishers b.v.(northholland).another concern that needs to be addressed is one of responsibility in computersupported decision making.scientists, engineers, and clinicians more and more frequently will use complex software to help analyzeand interpret their data. who then is morally and legally responsible for the correctness of their interpretations,and of actions based on them? experiments involving dangerous materials or human lives may soon becontrolled by computers, just as many commercial aircraft landings are at present. computers may be capable offaster or more precise determinations in some situations than humans. but software designers lack strongguidelines on assignment of responsibility in case of malfunction or unforesen disaster, and lack the expertise toguarantee against malfunctions or disasters. with complex software overlaid on complex hardware, it isimpossible to prove beyond a doubt in all circumstances that both hardware, and software are performingprecisely as they were specified to perform.gaps in training and education the training and education necessary for using information technologyare lacking. two decades ago many researchers dealt with computers only indirectly through computerprogrammers who worked in data processing centers. the development of information technology has broughtcomputing into the researcher's laboratory and office. as a result, the level of computing competence expected ofresearchers, their support staff, and their students has increased manyfold.the use of information technology in research39information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.computers are changing what students need to learn. undergraduate students of chemistry, for example,need more than the standard courses in organic, inorganic, analytic, and physical chemistry; in the view of manypracticing chemists, they should also have courses in calculus, differential equations, linear algebra, andcomputer simulation techniques, and through formal courses or practical research experience, should becompetent in mathematical reasoning, electronics, computer programming, numerical methods, statisticalanalysis, and the workings of information management systems (counts, 1987, position paper: the impact ofinformation technologies on the productivity of chemistry).neither students nor researchers can obtain adequate training and education through onetime trainingcourses. because the numbers of new tools are multiplying, researchers need ways to continuously learn about,evaluate, and, if necessary, adopt these new tools. using commercial programs and tutorial systems only partlyalleviates the problem because the technologies often change faster than such supports can accommodate to thechanges. instructors in the uses of information technologies within the disciplines are rare. senior researchers areespecially hard hit. the panel took no formal survey, but informal discussions suggest that most seniorresearchers have had exposure to no more than a onesemester programming course and have few of the skillsneeded to evaluate and use the available technology.documents as linked pieces: hypertextthe vision of computing technology revolutionizing how we store and access knowledge is as old asthe computing age. in 1945 vannevar bush proposed memex, an electroopticalmechanical informationretrieval system that could create links between arbitrary chunks of information and allow the user to followthe links in any desired manner. in the early 1960s, ted nelson introduced ﬁhypertext,ﬂ a form ofnonsequential writing: a text branches and allows choices to the reader, best read at an interactive screen.in 1968, doug englebart demonstrated a simple hypertext system for hierarchicallystructured documentsšthat is, a list of sections, each of which decomposes into a list of subsections, each of which decomposesinto a list of paragraphs, and so onšto which annotations could be added during a multipleworkstationconference. today hypertext refers to information storage in which documents are preserved as networksof linked pieces rather than as a single linear string of characters; readers can add links and follow links atwill.nelson's xanadu system is perhaps the most ambitious hypertext system proposed. xanadu wouldmake all the world's knowledge accessible in a global distributed database to which anyone can addinformation,for all researchers, learning advanced computing means taking a risk. they must interrupt their work andpay attention to something new and temporarily unproductive. they must become novices, often where sourcesof appropriate instruction and help are unclear or inaccessible. the investment of time and level of frustration arelikely to be high. understandably, many researchers cannot find the time and the confidence to learn technicalcomputing; some justify theirthe use of information technology in research40information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.choices with negative attitudes, for example: ﬁi get enough communications as it is; i don't need a computernetwork,ﬂ or ﬁif i put my data on the computer, others will steal it,ﬂ or ﬁwe are doing fine as things are; whychange at this point?ﬂand in which anyone can browse or search for information. a document is a set of one or more linkednodes of text, plus links to nodes already in the global database; a document may be mostly links,constructed out of pieces already in the database. users pay a fee proportional to the number of charactersthey have stored. anyone accessing an item in the global database pays an access charge, a portion ofwhich is returned to the owner as a royalty. individuals can store private documents that cannot have publiclinks pointing to them and can attach annotations to public documents that become available to everyonereading those documents. documents can be composed of different parts including text, graphics, voice,and video. intermedia, a hypertext system with some of these properties, has been implemented atbrown university and has been used to organize information in a humanities course for presentation tostudents. smallscale hypertext systems, such as apple's hypercards for the macintosh, are available onpersonal computers; their promoters claim these systems will change information retrieval as radically asspreadsheets changed accounting a few years ago.source: peter and dorothy denning, personal communication, 1987.given these natural but negative attitudes, organizations are sometimes slow in responding to demands fornew information technologies. some research organizations view these attitudes as unchangeable and wait tointroduce advanced computing until existing researchers move or retire. others are actively replacing personnelor creating new departments for computational researchers. still others are attempting to change attitudes bygiving researchers the necessary time and support systems. while we have no data on changes in productivity,there is some evidence that in organizations following the latter course, existing researchers at all ranks canachieve as high computing competence as new personnel (kiesler and sproull, 1987).because people are now being introduced to computing skills at earlier stages of schooling, the lag incomputer expertise is disappearing. over time, alternatives to personal expertise in the form of userfriendlysoftware or individual assistance from specialists will also develop.risks of organizational change changing an organization to make way for advanced informationtechnology and its attendant benefits entails real risks. administrators and research managers are often reluctantto incur the costsšfinancial, organizational, behavioralšof new technology. in some cases, administrators andresearch managers relegate computer resourcesšhardware, software, and peoplebased support servicesšto alower priority than the procurement and maintenance of experimental equipment. the result can be a longtermsuppression of the development and use of the tools of information technology.see box on electronic laboratory notebook, page 42.the use of information technology in research41information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.in other cases, administrators are misled into underestimating the time and resources required to deploy newinformation technology. efforts to develop effective networks have been insufficiently supported by governmentplanners and research institution administrators, who have been led to assume that technology and services toprovide network access are easily put in place. some administrators have promoted change, but without adequateplanning for the resources or infrastructure needed to support users. problems such as these are exacerbated byoverly optimistic advice given the administrators by technological enthusiasts. this particular impedimentprobably cannot be overcome. it can, however, be alleviated by establishing collaborative arrangements todevelop plans for and share the costs of change. educom, for example, is a consortium of research universitieswith large computing resources that promotes longrange planning and sharing of resources and experiences.legal constraints to an electronic version of a laboratory notebooktoday, the paper laboratory notebook is the only legally supportable document for patent applicationsand other regulatory procedures connected with research. some organizations, however, routinelydistribute electronic versions of laboratory notebook information to managers and other professionals whowould otherwise have to visit the research site physically or request photocopies. the benefits of legalelectronic notebooks are speculative but attested to by those using them informally (liscouski, 1987). first,they would help give researchers access to information or expertise that is otherwise lost because peoplehave moved or reside in different departments. second, they would allow research managers andresearchers to observe and compare changes in results over time. third, they would eliminate or makeeasier the assembly of paper versions of documents needed for government agencies. the barrier to anelectronic notebook is socialšits lack of acceptance as a legal document. such acceptance could takeplace if legal conditions for an electronic systemšstorage, format, securityšwere delineated. however,researchers, scientific associations, and government agencies have failed to develop such guidelines. thisfailure is probably connected to the traditions of privacy in laboratory notebooks, to the inability to forecasthow an electronic system would stand up in court, (and related to that, the risk and unacceptable cost toany single institution of developing a system), and to the uncertainty of the ultimate benefits on some widelyaccepted index of research effectiveness. whatever the reasons, the end result is that a complete andaccepted electronic notebook remains undeveloped.absence of infrastructure most fundamental of all the institutional and behavioral impediments to the useof information technology is the absence of an infrastructure that supports that use. just as use of a largecollection of books is made possible by a building and shelves in which to put them, a cataloguing system,borrowing policies, and reference librarians to assist users, so the use of a collection of computers and computernetworks is supported by the existencethe use of information technology in research42information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.of institutions, services, policies, and expertsšin short, by an infrastructure. on the whole, informationtechnology is inadequately supported by current infrastructures.an infrastructure that supports information technology applications to research should provide access to experts who can help; ways of supporting and rewarding these experts; tools for developing software, and a market in which the tools are evaluated against one another anddisseminated; communication links among researchers, experts, and the market; and analogs to the library, places where researchers can store and retrieve information.several different kinds of experts in information technology help researchers. some are specialists inresearch computing. some are programmers who develop and maintain software specific to research. others arespecialists who carry out searches. still others are ﬁgatekeepers,ﬂ who help with choices of software andhardware. gatekeepers are members of an informal network of helpers centered around advocates andspecialists, experts in both a discipline and in information technology who become known by reputation.overdependence on gatekeepers creates other problems: as with any informal service, some advice received maybe narrowly focused or simply wrong and the number of persons wanting free information often becomes largerthan the number of persons able to provide it. as a result, the gatekeepers may become overloaded andeventually retreat from their gatekeeping roles.to hold on to expert help of all types, research and funding institutions must find ways of supporting andrewarding it. while institutions and disciplines have evolved ways of rewarding researchersšpublication inrefereed journals, promotion, tenurešno such systems yet reward expert help.another aspect of the needed infrastructure is some formal provision for developing and disseminatingsoftware for specific research applications. tools for constructing reliable, efficient, customized, and welldocumented software are not used in support of scientific research. computer science, as a supporting discipline,needs to facilitate rapid delivery of finished software, and easy extension and revision of existing software. thedepartment of defense has recently pioneered the creation of a software engineering institute at carnegiemellon university. efforts to create tool building and research resources for nondefense software are worthencouraging.development and dissemination of scientific software could be speeded in many cases by adoption ofemerging commercial standards. these standards are supported by many vendors for a variety of computingenvironments. the temptation to narrowly match software to specific applications should be resisted in favor ofstandard approaches.the use of information technology in research43information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.software, once developed, needs to be evaluated and disseminated. the research establishment nowevaluates research information principally through peer review of funding proposals and manuscripts submittedfor publication. software needs to be dealt with in a similar manner. educom has recently announced itssupport of a peerreview process for certain kinds of academic software. other prototypes of systems forevaluating and disseminating software already exist (see boxes on bionet and on ibm's software market).these prototypes couple an electronic ﬁmarket,ﬂ through which software can be disseminated, with aconferencing capability that allows anyone with access to contribute to the evaluation of the market wares. thesystem provides an extremely important feature: those contributors who are most successful in the open marketcan automatically be identified and given credit in much the same way as authors of books and research papersnow are.an example of a software market infrastructure: ibm researchibm's internal computer network connects over 2,000 individual computers worldwide, providing ibm'sresearchers, developers, and other employees with communications facilities such as electronic mail, filetransfers, and access to remote computers. in recent years, software repositories and online conferencingfacilities have grown and flourished, and become one of the primary uses of the network. with a singlecommand, any ibmer has access to some 3,000 software packages, developed by other ibmers aroundthe world and made available through the network. many of these packages are computer utilities andprogramming tools, but others are tools for research. they include statistical and graphics applications,simulation systems, and ai and expert system shells, as well as many everyday utilities to make generaluse of the computer simpler. the high level of interconnection offered by the network and the centralizationof information offered by the repositories allows scientists with a particular need to see if software to satisfythat need is available, to obtain it if it is, and to develop it if it is not, with confidence that they are notduplicating the efforts of some colleague.the online conferences (public specialpurpose electronic bulletin boards), which are as widespreadand accessible as the software repositories, allow users of the software (and of commercial and othersoftware) to exchange experiences, questions, and problems. these conferences provide a form of peerreview for the software developer. for internally developed software, they provide a fast and convenientchannel between the software author and the users; authors with an interest in improving their programshave instant access to user suggestions and tothe infrastructure for information technology also depends on communication links. the panel believes thatone of the most important services that computer networks can provide is the link between users and expert help.existing links often take the form of electronic bulletin boards on various networks; other mechanisms also exist.until more formal mechanisms come about, open communication with pioneers, advocates, and enthusiasts isone ofsee software market, box below. the use of information technology in research44information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.the best ways to allow new technologies to be disseminated and evaluated by research communities.eager testers. users with a special need or a hard question have equally fast access to the author forenhancements or answers.the conferences also allow users with common interests to exchange other sorts of information in thetraditional bulletin board style. ai researchers debate the usefulness of the concept of intentionality ordiscuss how software engineering methodologies apply to expert systems development; computer graphicsand vision workers talk about the number of bits required to present a satisfactory image to the human eye.over 100 individual conferences support thousands of separate discussions about computer hardwareand software and virtually all other aspects of ibm's undertakings. the software repositories provide aﬁreviewedﬂ set of tools and applications for a broad population on a wide spectrum of problems.the organization that originally sets up a repository or a conference generally provides user support forit (answering ﬁhow to do itﬂ questions), and installation and maintenance of local services is usually handledeither by an onsite group that has an interest in the specialty served by the facility, or on a more formalbasis by the local information systems department.the benefits of these repositories and conferences are at least as widely distributed and probably evenharder to quantify, but the success of these software libraries and online conferences within ibm shouldserve as an encouraging sign for others with the same sorts of needs. a market can be made to succeed,provided that high levels of standardization and compatibility in both hardware and software can beachieved. such levels of interoperability have, so far, been easier to achieve at commercial institutionssuch as ibm research than at research universities. such as ibm research than at research universities.a final piece of infrastructure largely missing is housing and support for the storing and sharing ofinformation. such a function could be performed by disciplinary groups or, more generally, at the universitylevel. many university libraries have a professional core staff whose members hold faculty rank and function notonly as librarians but also as researchers and teachers. some university computer centers operate similarly.national laboratories, like astronomical observatories and accelerator facilities, have a core staff of astronomersor physicists whose main task is to serve outside users while also maintaining their own research programs.the existence of such a professional staff involved in the storage and retrieval of information for adiscipline would provide a means of recognizing, rewarding, and providing status to these people. in some cases,a university might wish to consider integrating its information science department with its computer center andits library.the use of information technology in research45information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.the use of information technology in research46information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.panel findings and recommendationsthe vision of a more productive research enterprise, and the recognition of difficulties in reaching thatvision, prompted this report. in preceding sections, we reviewed many aspects of the use of informationtechnology in research. in this section we present our findings and recommendations.findingsthe panel finds that1. information technology has already had a significant and widespread impact on the conduct ofresearch. for the future, that impact amounts to a revolution.computer and communication technologies are valuable to every scientific discipline and essential to agrowing number of them. the panel examined the uses of information technology in ten science and engineeringdisciplines. although these fields use information technology in different ways, the panel also found manysimilarities. these can be summarized as follows: as the power and speed of computers have increased, numerical computations of increasing complexityhave become practical. the result has been more realistic simulation of systems either physicallyimpossible or too costly to study directly. another consequence of increased computational power has been the capability to collect, store,retrieve, manipulate, and analyze enormous quantities of information. electronic storage of informationmeans that huge information archivesšfor example, the human genome projectšare not only feasiblebut also accessible to more researchers from different disciplines. an additional consequence of increased computational power has been the capability to present theresults of numerical computations as visual images. thepanel findings and recommendations47information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.remarkable ability of the human brain to recognize patterns in pictures allows faster understanding ofcomputational results and speedier, more efficient interaction with models when numbers are turnedinto visual images. information technology allows computers to take over monitoring and control of scientific instruments.this in turn makes scientific observation more convenient, more reliable, and often lower in cost; insome cases, it has led to new computerbased instruments that extend the bounds of observation. information technology has greatly expanded the capabilities for communication among researchers.communications systems mediated by computers have led to the rapid and relatively inexpensiveexchange of everything from memoranda to massive data files.the panel further finds that:2. significant impediments to the widespread use of information technology in research require carefulattention. some impediments are technical and financial. other impediments, which up to now havereceived the least attention, are behavioral and institutional. technical impediments are serious in a few fields. fields doing largescale experiments, using satellitesto gather data, and using graphics to analyze large amounts of data will for the foreseeable future needcomputers, software, and networks that are bigger, faster, more capable, and more efficient. the needsof these fields deserve, and will continue to receive, attention from computer scientists and engineers,from information technology manufacturers, and from institutions such as the federal government anduniversities. financial impediments are chronic. despite the decreasing cost of hardware, no sources will ever supplyenough money to provide every researcher the best information technology environment. theinstitutions that fund researchers will continue to do their best, and information technology willcontinue to need more funding.while the panel cannot suggest detailed means for increasing the total resources devoted to informationtechnology, it does feel strongly that the provision of such resources is critical to progress in american clinical,engineering, and scientific research. in the resourceconstrained environment likely to confront research in thefuture, difficult decisions will be made on reallocating necessary resources. with a declining population poolfrom which to draw new scientists, clinicians, and engineers, and with the increasing complexity of research, thepanel believes that increased support of information technology in research deserves high priority. if the nation'sbest researchers are not given the appropriate tools, our role in the international scientific community willcontinue to diminish in importance. the limited resources of federal agencies must continue to be allocated onthe basis of scientific merit and significance of the proposed research activities.panel findings and recommendations48information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved. the panel believes that serious impediments to increasing the use of information technology in researchare behavioral and institutional. these can be sorted into four major categories: problems of access,problems with learning and use, attitudes of individual researchers, and problems of management. problems of access. computer networks, hardware, and software are not necessarily accessible to theresearchers who want them, nor can everyone who wants them afford them. network access is stilllimited and inconvenient. the limitation is especially important since network access permits bothcollaboration with distant colleagues and access to computing resources. for many researchers in alarge number of fields, the hardware to which they have access is adequate. some researchers, however,require large amounts of time on specialized, expensive hardware; supercomputers leads researchers toreconsider scientific problems, one can anticipate increased demands for access and network use;researchers in the future may find time on supercomputers limited and networks congested.finding the right software is a more serious problem. software that is commercially available is oftenunsuited to the specialized needs of the researcher, and professionals who create software for research groups arerare. consequently, many researchers, who are usually not skilled software creators, develop their own. toooften, members of the research community waste time, effort, and money duplicating one another's efforts.what appears to be lacking are institutional means for providing the services of skilled professionals tocreate and maintain appropriate software. funding sources do not consider support for such professionalsessential; institutions give them neither recognition nor career status.also needed is an institutional system to collect, review, document, and disseminate scientific andengineering application software. prototypes exist but need broader evaluation. such a system could evaluatesoftware's effectiveness and eventually lead to standardization of software for wider use. problems with learning and use. learning to use information technology requires a large investment intime and effort before the investment pays off, and help is hard to find. researchers who needinformation technology face difficult choices: they must either learn to use whatever hardware andsoftware the market offers, create their own, or do without. when researchers learn to use existinginformation technology, they receive haphazard help in learning; instruction or specialists ininformation technology are often unavailable. neither the researchers' disciplines nor their institutionsprovide incentives for learning. furthermore, researchers must invest time at the expense of theirresearch productivity during the learning period.once researchers learn the necessary information technology, they face problems in using it. networks posea significant problem. most of the approximately 100 researchoriented computer networks in the united stateswere established to serve the needs of small and widely scattered communities ofpanel findings and recommendations49information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.researchers. as a result, a researcher on one network wanting to communicate with a researcher on another facesproblems of compatibility. the technical aspects of these problems are tractable; the institutional and behavioralaspects are less so. no one agrees on how procedures for using the networks might be standardized. thenetworks are not well coordinated with one another, and users have limited opportunity to suggest improvements. problems of the attitudes of individual researchers are twofold. for reasons enumerated above,researchers often approach new information technology applications cautiously. when seniorresearchers, who are involved in peer review and decisions about publications and research proposals,are resistant, their attitudes can lead to the rejection of innovative applications of informationtechnology to research. another problem is that of proprietary attitudes toward research data in manydisciplines. so long as primary data are viewed as the exclusive property of the researcher who collectsthem, they will not be available to other researchers. even if data are made available, they will be left inidiosyncratic formats with insufficient explanatory documentation; and the effort required to make thesedata usable to the research community will not receive high priority. increased access to data does,however, raise issues of how large volumes of primary data should be stored, and of the need forvalidating stored information. problems of managing information technology. some basic questions need to be addressed: who is tomanage, maintain, and update information services? who will create standards? how will costs becharged, and who will pay for them? no current institutional framework provides the ideal answer tothese questions. federal agencies, professional societies and scientific associations, and private profitmaking groups need to consider how to address the needs of research users of information technology.recommendationsrecommendation ithe institutions supporting the nation's researchers must recognize and meet their responsibilities to developand support policies, services, and standards that help researchers use information technology more widely andproductively. specifically, we recommend that universities provide accessible, expert help in learning and using information technology. university departments, and scientific and professional groups, establish career ladders for scientificprogramming positions. funding agencies provide support for scientific programming and for help services in learning andusing information technology systems for research.panel findings and recommendations50information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved. scientific associations establish disciplinary standards for the storage and indexing of scientific data. university departments, and scientific and professional groups, implement mechanisms for theevaluation, merit (peer) review, and dissemination of software useful in the conduct of research. vendors, in collaboration with scientific groups, establish standards for simplified and consistent usermachine interfaces. network administrators provide simple user interfaces and addressing schemes, add gateways to othernetworks, improve system reliability and capacity, and provide online help, such as guides to servicesand mail addresses of individuals who can answer questions. information service providers create simplified common standards for accessing and queryinginformation sources, and eventually provide unified access to information. software vendors, and scientific and professional groups, create program libraries and make themaccessible through the networks.rationale. information technology is now becoming an essential component of the research environment.the services needed by research users include access to computers; access to networks, both local and widearea; longterm storage of and access to data; hardware maintenance and augmentation; help in learning to use existing software and services; production of new software and customization of existing software; and collection, review, documentation, and dissemination of software.in some instances these services may be efficiently provided by a central organization; in others, bydecentralized groups. the services may be provided either by augmenting the responsibilities of existing groupsor by creating new groups. currently such services are being provided in a variety of ways, with highly variabledegrees of success and efficiency, across many laboratories, professional societies, universities, and federalagencies. in the most successful models, the researchers feel their needs are paramount. these models need to bepublicized, evaluated, and disseminated so that policy and allocation decisions are well informed by the views ofresearch users of information technology.with regard to policies, the panel recognized a number of issues, cited earlier, that need further discussion;the appropriate groups may wish to consider the specific recommendations that follow. the federal government, through policies of its researchsupporting agencies, should ensure propersupport for software development for scientific research. software developed should meet minimalstandards of compatibility, reliability, and documentation, and should be made available to otherresearchers.panel findings and recommendations51information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved. as a general principle, data collected with government support rightfully belongs in the public domain,although the right of researchers to reasonable time for first publication must be respected. federalagencies, scientific societies and professional associations, university consortia, and other privategroups may wish to make specific recommendations based on their reexamination of the implications ofthe adoption of such a policy. such recommendations might include the creation and maintenance ofdata banks and indices to data by research communities, the potential of evaluated databases, and thepossibility of including reanalyses of data as part of peer review of publications. there is a pressing need for new and more compact forms of data storage. one particular area in whichthese techniques would be useful is that of image compression. one solution seems to lie in optical diskstorage. unfortunately, these new techniques are still immature and lack commonly accepted standards.federal agencies should encourage engineering research on optical storage techniques for scientificpurposes. efforts to create tool building and research resources for nondefense software should be encouraged. the federal government should fund pilot efforts of two kinds. one would implement informationstorage and software dissemination concepts for selected disciplines. the other would implement theconcept of software markets, with emphasis on the development of generic tools useful in several ormany disciplines. in addition, these pilot projects should be coupled with exploration of such policy issues as protection ofconfidentiality of information about human subjects, protection of intellectual property, and informationsecurity concerns in a global electronic information environment.recommendation iithe institutions supporting the nation's researchers, led by the federal government, should develop aninterconnected national information technology network for use by all qualified researchers.specifically, we recommend that the office of science and technology policy (ostp) in the executive office of the president and thefederal agencies responsible for supporting and performing research and development plan and fund anationwide infrastructure for computerbased research communication. planning and development of this nationwide infrastructure be guided by users of informationtechnology in research, rather than by technical experts in information technology or hardware orsoftware vendors. the panel believes strongly that such a national network is too important to the futureof research to be left only to the technical experts.panel findings and recommendations52information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved. the national research network be founded on the fundamental premise of open access to all qualifiedresearchers/scholars that has nurtured the world's scientific community for centuries. the national research network be developed in an evolutionary manner, making full use of the existingsuccessful networks for research.rationale. the panel views the federal government's role in developing a national network for research asanalogous to its role in developing the nation's network of roads, streets, and highways. here, the federalgovernment has planned and funded the interstate highway system and the national highways, and has imposedor encouraged certain national standards. state and local governments have planned and funded a network ofhighways, roads, and streets that is fully interconnected and compatible with the federal framework. in thenational research network, analogs would be research institutions, scientific and professional associations, andcorporate groups operating individually or banded together in consortia.funding for this undertaking, as outlined in the report of the federal coordinating council on science,engineering, and technology (fccset) committee issued recently (office of science and technology policy,1987) should be made available to ensure an advanced national network infrastructure and services for thenation's research communities. appropriate division of responsibilities among federal and state agencies andresearch institutions warrants careful attention.the panel notes that the director of the national science foundation has announced the foundation'sintention to serve as lead agency in developing a national network. the panel strongly supports the concept of alead agency, believing that leadership in coordination of support for a national research network is an essentialelement in the nation's science policy. the panel believes the national science foundation would be anappropriate lead agency, given its legislatively mandated responsibility for supporting research and educationacross the full range of science and engineering disciplines.the creation of a national network will take a considerable amount of time. in the meantime some valuableand muchused networksšsuch as bionet, omnet, bitnet, or hepnetšexist within particular scientificor academic communities. many of these networks have been improved with advice from their users, and can bean invaluable source of advice to the designers of any national science network. it is particularly important,therefore, that the existing academic and disciplinary networks continue to receive support until such time asthey can either be integrated into or supplanted by the national network.the panel's recommendation for a national research network is similar in substance and spirit to the moredetailed recommendations contained in a recent report of the national research network review committee ofthe nrc's computer science and technology board (national research council, 1988).crucial to setting up and running a national network infrastructure is participation by users. the panel urgesthat agencies work with an advisory boardpanel findings and recommendations53information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.largely composed of users. a research network of national scope must be oriented toward the research user ofinformation technology. its philosophy and structure should be such that learning, entering, using, and leavingthe network are simple. this should involve, among other things, an easily interpreted program for helping theuser, instructions for different levels of use, and simple connections between the network and many varieties ofterminals, personal computers, and workstations. the network should be capable of transmitting graphics,symbols, and large amounts of data quickly. it should have gateways to networks in other countries. it should besupported by a professional staff whose main task is to help users.the panel recognizes that constraints on access to information are sometimes warranted in cases where theprivacy of personal information, the protection of human subjects or of intellectual property, or national securityconcerns are of overriding importance. nevertheless, the panel believes that the interests of the global researchcommunity are best served by establishing open and unfettered access as a fundamental presumption in theoperation of a national research network.recommendation iiito facilitate implementation of recommendations i and ii, and to focus continuing attention on theopportunities and impediments associated with research uses of information technology, the panel recommendsthe establishment at a national level of a user's group to oversee and advise on the evolution and use ofinformation technology in support of scientific, engineering, and clinical research.specifically, the national research council (nrc) should charge a standing committee or board (whetherexisting or newly created) with the mandate to oversee and advise on research use of information technology.the membership of this board should include a majority of users from a variety of research disciplines.rationale. the problems and needed changes addressed by recommendations i and ii are diverse and donot have shortterm solutions, and, therefore, require some institutional setting for ongoing examination anddiscussion. leadership and coordination in the application of information technology in research would beprovided best by a single organization.many organizations currently promote developments in information technology at a national level. amongthese are: the national science foundation and its computer and information science and engineering (cise)directorate and division of advanced scientific computing, especially through the nsfnet initiative,expres program, and the panel on graphics, image processing, and workstations; the national researchcouncil, especially its computer science and technology board, numerical data advisory board, committeeon nationalpanel findings and recommendations54information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.statistics, and board on telecommunications and computer applications; a variety of initiatives within thenational library of medicine; and the federal coordinating council on science, engineering, and technology(fccset) of the office of science and technology policy (especially its committees on networking and onhigh performance computing). these activities, however, are fragmented, specialized, and tend to give littleattention to the behavioral aspects of research uses of information technology.the panel recommends location of the user's group within the national research council (nrc) rather thana federal agency because it believes that the group ought to be free to focus on the interests and concerns ofusers, unconstrained by immediate concerns for the distribution of funds among disciplines or agencies. severalboards or committees now existing at the nrc have titles related to the charge to be given this new board.however, it is the panel's view that none of the existing groups represents adequately the needs of users.whether one of the existing groups might be reconstituted to create a body of the required nature or whether anew body might be founded is, in the view of the panel, far less important than the nature of the resulting body.what is needed is a group of researchers who use information technology, supplemented by a few expertproviders of information technology.the national research council unit (whether existing or new) would have several specific functions. onefunction would be to advise policymakers on a broad range of issues related to research uses of informationtechnology. some of these issues are: international implications of networking; national security concernsassociated with scientific databases; and issues of cost for network communications. ad hoc study panels wouldbe established as appropriate.another important function of the unit would be to collect, analyze, and disseminate data on howresearchers use information technology. in the course of its deliberations the panel became painfully aware thatthese data presently do not exist. such data would inform not only the actions of those responsible for thesupport of research but would also apprise researchers themselves of new opportunities offered by informationtechnology. a model for such activities among present national research council operating groups might be theoffice of scientific and engineering personnel.this mechanism would also provide a forum to facilitate the transfer of technology: by sponsoringworkshops for scientists on newly developed information technology and on coordination of approaches tosimplified standards; by exchanging information with technology developers; and by coordinating interactionamong scientific organizations and professional associations. the board would disseminate information oncurrent uses of information technology and new developments.the unit would also convene meetings between researchers and those responsible for supporting research.by providing a forum for discussion, it would ensure that the needs of the research community are brought to theattention of appropriate officials and administrators.panel findings and recommendations55information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.panel findings and recommendations56information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.appendix alist of position papersdrafts of these papers can be obtained by contacting the study director, dr. john r. b. clement, or thecommittee on science, engineering, and public policy, at the following address:national academy of sciences, room nas 2462101 constitution avenue, n.w.washington, d.c. 20418richard j. blakely, u.s. geological survey, menlo park, california: ﬁcomputers and solidearthgeophysicsﬂrichard w. counts, quantum chemistry exchange program, indiana university:ﬁthe impact of information technologies on the productivity of chemistryﬂpaul a. david and w. edward steinmueller, center for economic policy research, stanforduniversity: ﬁthe impact of information technology upon economic scienceﬂpeter j. denning, research institute for advanced computer science, nasa ames research center, moffetfield, california: ﬁinformation technology in computingﬂjohn hubbard, department of mathematics, cornell university: ﬁcomputers in mathematicsﬂharvey newman, physics department, california institute of technology: ﬁcomputing and datacommunications for high energy physicsﬂcynthia h. null, phychology department, the college of william and mary, and bert f. green,psychology department, the johns hopkins university: ﬁcomputers in behavioral scienceﬂdonald m. steinwachs, school of hygiene and public health, the johns hopkins university: ﬁinformationtechnologies and the conduct of public health researchﬂappendix a57information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.appendix bbiographies of panel membersdonald n. langenberg, chancellor, university of illinois at chicago, chicago, illinois (chairman).for much of his career, dr. langenberg was professor of physics and, for a time, also professor ofelectrical engineering and science at the university of pennsylvania. there he directed a materials researchfacility, the laboratory for research on the structure of matter, and served as vice provost for graduate studiesand research. from 1980 to 1982, he served as deputy director of the national science foundation and, forseveral months, as acting director. he became the first chancellor of the university of illinois' chicago campus(uic) in 1983. his research interests were in condensed matter physics and included the fermi surfaces ofmetals and semiconductors, tunneling, josephson effects and nonequilibrium phenomena in superconductors, andprecision measurement and the fundamental physical constants. he has served on many boards and advisorycommittees, and is currently a member of the board of directors of the american association for theadvancement of science (aaas). dr. langenberg holds a b.s. degree from iowa state university, an m.s.degree from the university of california at los angeles, and a ph.d. degree from the university of california atberkeley, all in physics. he also holds an honorary m.a. and an honorary d.sc. from the university ofpennsylvania. unfortunately, he is, as noted in the preface, computer illiterate.w. richards adrion, chair, computer and information science department, university ofmassachusetts, amherst, massachusetts.dr. adrion came to the university of massachusetts in 1986 from the national science foundation, wherehe was most recently chief scientist for the directorate for computer and information science and engineering,and before that deputy director for computer research. previously, he served as manager of the softwareengineering group at the national bureau of standards, and has been on the faculty at both oregon stateuniversity and the university of texas atappendix b58information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.austin, and as adjunct professor at the american university, george washington university, and georgetownuniversity. his research interests are in the areas of programming systems and software engineering, especiallyprogramming environments, program verification, and object bases for software development. dr. adrion earnedbachelor's and master's degrees in electrical engineering at cornell university, and a ph.d. degree in the samesubject at the university of texas at austin.joseph ballam, professor of physics, stanford linear accelerator, stanford university, stanford,california.dr. ballam is emeritus associate director of the stanford linear accelerator center and head of its researchdivision. he was on the physics faculty of princeton university and a professor of physics at michigan stateuniversity. his research interests include elementary particles, cosmic rays, and experimental high energyphysics. dr. ballam received a b.s. degree from the university of michigan and a ph.d. degree from theuniversity of california at berkeley.bruce g. buchanan, professor and codirector, center for parallel, distributed, and intelligentsystems, university of pittsburgh, pittsburgh, pennsylvania.for many years dr. buchanan was professor of computer science research and codirector of theknowledge systems laboratory at stanford university. in 1988 he moved to the university of pittsburgh.professor buchanan's main research interest is in artificial intelligence, in particular the design of intelligentcomputer programs that assist scientists and physicians. these include programs and methods for knowledgeacquisition and machine learning, scientific hypothesis formation, and construction of expert systems. he wasone of the principals in the design and development of dendral, metadendral, myci, emycin, andprotean systems. dr. buchanan holds a b.a. degree in mathematics from ohio wesleyan university, andm.s. and ph.d. degrees in philosophy from michigan state university.william j. emery, professor, aerospace engineering science, colorado center for astrodynamicsresearch, university of colorado, boulder, colorado.trained as a physical oceanographer, dr. emery leads a group concentrating on satellite remote sensing ofatmosphere and ocean. in cooperation with noaa's program for regional observing and forecasting services(profs), his group operates a satellite receiving system to collect data from operational weather satellites. hisresearch interests include largescale ocean and atmosphere problems with an emphasis on the analysis of largevolumes of data. as a consequence of the need to analyze satellite images, he has developed new imageprocessing tools for sun, dec/spx, and macintosh ii workstations in order to provide students with easyaccess to a variety of display systems. dr. emery has a b.s. from brigham young university in mechanicalengineering and a ph.d. from the university of hawaii in physical oceanography.appendix b59information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.david a. hodges, professor of electrical engineering and computer science, university of california,berkeley, california.dr. hodges teaches and researches microelectronics technology and design, communications and computersystems, and computerintegrated manufacturing systems at the university of california, where he has been amember of the faculty since 1970. before then, he held positions at bell telephone laboratories, in thecomponents area at murray hill and as head of the systems elements department at holmdel. he is a member ofthe national academy of engineering and has served on several national academy of engineering and nationalresearch council committees. dr. hodges' degrees are in electrical engineering; he earned his b.s. degree atcornell university and his m.s. and ph.d. degrees at the university of california, berkeley.david a. hoffman, professor, department of mathematics and statistics, university ofmassachusetts, amherst, massachusetts.david hoffman is professor of mathematics at the university of massachusetts at amherst, and a memberof the geometry analysis numerics and graphics center. his recent research interests include the use ofcomputation and computer graphics in the study of extremal surfaces in mathematics and polymer physics. hehas held positions at the university of michigan, stanford university, and impa, rio de janeiro. dr. hoffmanhas a b.a. degree from the university of rochester and m.s. and ph.d. degrees from stanford university inmathematics.f. thomas juster, professor of economics, university of michigan, ann arbor, michigan.dr. juster is also research scientist in the survey research center at the university of michigan's institutefor social research. he has served on a number of national research council committees, including thecommittee on national statistics, and chairs both the nrc committee on the supply and demand formathematics and science teachers as well as the american economic association committee on the quality ofeconomic data. dr. juster is a fellow of the american statistical association. his research includes the design ofeconomic and social accounting systems, the development of measures of economic welfare, consumer behaviorand forecasting, and analysis of household saving and asset accumulation behavior. dr. juster received a b.s.degree from rutgers university in education and a ph.d. degree from columbia university in economics.sara b. kiesler, professor, social sciences and social psychology, department of social anddecision sciences, carnegie mellon university, pittsburgh, pennsylvania.dr. kiesler has been on the faculty of the college of humanities and social sciences and of the roboticsinstitute at carnegie mellon since 1979; previously, she was a staff member of the national research counciland a professor at the university of kansas. she was the senior staff director of three national researchappendix b60information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.council committees that produced reports on aging, basic research in education, and behavioral and socialsciences. she has served on several committees of the national academies of sciences (nas) and engineering(nae). her research interests include the study of the introduction and impact of computer and computercommunication technologies in groups and organizations. dr. kiesler received a b.s. degree from simmonscollege in social science, an m.a. degree from stanford university, and a ph.d. degree from ohio stateuniversity, both in psychology.kenneth m. king, president, educom, princeton, new jersey.kenneth king currently serves as president of educom, a consortium of 550 universities founded todevelop and work toward common goals in information technology and communications. previously, he wasvice provost for computing at cornell university, where he was responsible for all academic and administrativecomputing. this included active programs in the development of instructional software for microcomputers, innetworking, in library system development, and a national supercomputer center. before that, he was professor,vice chancellor for university systems, and university dean for computer systems at the city university of newyork. he also was director of columbia university's computer center, and manager of columbia's ibm watsonscientific computing laboratory. dr. king received a b.a. degree in physics from reed college and a ph.d.degree in theoretical physics from columbia university.robert langridge, professor of pharmaceutical chemistry in the school of pharmacy andprofessor of biochemistry and biophysics in the school of medicine, university of california, san francisco,california.dr. langridge also directs the computer graphics laboratory at his university's school of pharmacy. hehas been a professor in the biochemistry department at princeton university and a professor in the department ofbiophysics at the university of chicago. his research includes computer graphics, biomolecular structure andfunction, protein engineering, and drug design. in particular, he uses computer graphics to visualize the motionsof molecules in three dimensions and in time. dr. langridge holds a b.sc. degree from the university of londonin physics and a ph.d. degree from king's college, the university of london, in crystallography.nina w. matheson, associate professor of medical information and director, william h. welchmedical library, the johns hopkins university, baltimore, maryland.before going to johns hopkins, ms. matheson was special consultant to the national library of medicinein bethesda, maryland, and assistant director of health information management studies at the association ofamerican medical colleges in washington, d.c. she had previously been assistant research professor in thedepartment of health care sciences and director of the himmelfarbappendix b61information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.health science library at the george washington university, where she introduced automated library operatingsystems. she has served as president and member of the board of both the medical library association and theassociation of academic health sciences library directors, and is a member of the national library ofmedicine's board of regents. ms. matheson's b.a. and m.l. degrees are in english and library science, fromthe university of washington.david a. pensak, corporate advisor, computing technology, e.i. du pont de nemours & co., inc.,wilmington, delaware.dr. pensak has been at du pont since 1974, where he manages the corporation's computer science researchand development. his organization is chartered with identifying those areas of science and technology in which atwo order of magnitude (or greater) increase in complexity or capacity of modelling would permit revolutionaryadvances in understanding (and the construction of the appropriate hardware and software to achieve thesegoals). he was a member of the american chemical society's task force on large scale computing. hisresearch includes structureactivity correlations, theory of catalysis, interactive graphics, programming languagedesign and human engineering, systems, artificial intelligence, and parallel computer architectures. dr. pensakhas a b.a. degree from princeton university in chemistry and m.a. and ph.d. degrees from harvard university,also in chemistry.allan h. weis, vice president, data systems division, ibm enterprise systems, white plains, newyork.mr. weis has worldwide responsibility for the strategy, development, and technical support of ibm's largesystem for numerically intensive computing.his career at ibm, which began in 1961, has included assignments in research, development, new businessareas, and information systems. most recently, mr. weis was responsible for the computing and communicationsystem at ibm's research laboratories, and for the direction of a number of advanced technology programs.mr. weis majored in electrical engineering at the university of kansas, and received his m.s. from themassachusetts institute of technology on an alfred p. sloan fellowship. he is a member of the board ofdirectors of nysernet, and is on the executive committee of the nsfnet.appendix b62information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.bibliography and selected readingsbarbacci, mario r., a. nico habermann, and mary shaw. the software engineering institute: bridging practice and potential. ieeesoftware, volume 2, number 6, pp. 4œ21, november 1985.bardon, marcel. a national computing environment for academic research (kent k. curtis, chair). nsf working group on computers forresearch. washington, d.c.: national science foundation, july 1983.bikson, tora k., barbara a. gutek, and don a. mankin. implementing computerized procedures in office settings. santa monica, ca: therand corporation. r3077nsf/iris. october, 1987.bird, peter. formation of the rocky mountains, western united states: a continuum computer model. science, volume 239, pp. 1501œ1507,25 march 1988.bowler, ken c., alastair d. bruce, richard d. kenway, g. stuart pawley, and david j. wallace. exploiting highly concurrent computers forphysics. physics today, special issue: computational physics, volume 40, number 10, pp. 40œ48, october 1987.bowman, carlos m., john a. nosal, and anne e. rogers. effect of new technology on information transfer in the 1990s. journal ofchemical information and computing sciences, volume 27, pp. 147œ151, 1987.branscomb, lewis m. improving r&d productivity: the federal role. science, volume 222, pp. 133œ135, 14 october 1983.brooks, frederick p., jr. no silver bullet: essence and accidents of software engineering. ieee computer, volume 20, number 4, pp. 10œ19,april 1987.brown, frank r., and norman h. christ. parallel supercomputers for lattice gauge theory. science, volume 239, pp. 1393œ1400, 18 march1988.bush, vannevar. as we may think. the atlantic monthly, volume 176, number 1, pp. 101œ108, july 1945.caming, h.w. william. protection of personal data in the united states. the information society, volume 3, number 2, pp. 113œ130, 1984.cotter, holland. birth of a network: a history of bitnet. cuny/university computer center communications, volume 14, number 1œ2,pp. 1œ10, januaryfebruary 1988.davis, ruth m. where will technology put the library of the 21st century? bulletin of the medical library association, volume 75, number 1,pp. 1œ6, january 1987.delisi, charles. computers in molecular biology: current applications and emerging trends. science, volume 240, pp. 47œ52, april 1, 1988.denning, peter j. paradigms crossed. editorial. communications of the acm, volume 30, number 10, p. 808, october 1987.. the science of computing: electronic publishing. american scientist, volume 74, pp. 582œ585, 1986.. the science of computing: a new paradigm for science. american scientist, volume 75, pp. 572œ573, novemberdecember 1987.drake, miriam. from print to nonprint materials: library information delivery systems. educom bulletin, pp. 28œ30, spring 1988.eddy, william f. (chairman). computers in statistical research: report of a workshop on the use of computers in statistical research. washington, d.c.: the institute for mathematical statistics, 1986.educom networking and telecommunications task force. a national higher education network: issuesbibliography and selected readings63information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.and opportunities. nttf paper number one. princeton, nj: educom, may 1987.ezzell, carol. aids workers going for computer link. nature, volume 332, p. 194, 17 march 1988.feigenbaum, edward a. autoknowledge: from file servers to knowledge servers. in: salamon, r., b. blum, and m. jorgensen (eds.)medinfo 86, pp. xliiiœxlvi. elsevier science publishers b.v. (northholland), 1986.fienberg, stephen e., margaret e. martin, and miron l. straf (eds.) sharing research data. washington, d.c.: national academy press, 1985.foley, 1987. manipulative interfaces.freeman, john. data quality and the development of organizational social science: an editorial essay. administrative science quarterly,volume 31, pp. 298œ303, 1986.gillespie, robert g. (with deborah a. dicaro). computing and higher education: an accidental revolution. washington, d.c.: nationalscience foundation, 1981.green, bert f. adaptive testing by computer. in: r.b. ekstrom (ed.), measurement, technology, and individuality in education. newdirections for testing and measurement, number 17. san francisco, josseybass, march 1983.haggin, joseph. process control no longer separate from simulation, design. chemical and engineering news, volume 62, number 14, pp. 7œ16, april 2, 1984.. workshop underscores wide use of computers in chemical engineering. chemical and engineering news, volume 65, number 45, pp. 27œ29, november 9, 1987.hey, anthony j. g., john h. merlin, martin w. ricketts, michael t. vaughn, david c. williams. topological solutions in gauge theory andtheir computer graphic representation. science, volume 240, pp. 1163œ1168, 27 may 1988.hoffman, david. the computeraided discovery of new embedded minimal surfaces. the mathematical intelligencer, volume 9, number 3,pp. 8œ21, 1987.holland, william r., and james c. mcwilliams. computer modeling in physical oceanography from the global circulation to turbulence.physics today, special issue: computational physics, volume 40, number 10, pp. 51œ57, october 1987.holmstrom, engin inel. access to supercomputers. higher education panel report number 69. washington, d.c.: american council oneducation, january 1986.hut, piet, and gerald jay sussman. advanced computing for science. scientific american, special issue: the next computer revolution,volume 257, pp. 144œ153, october 1987.ieee computer society. proceedings: first international conference on supercomputing systems. st. petersburg, florida, december 16œ20,1985. ieeecs order number 654. washington, d.c.: ieee computer society press, 1985.kahn, robert e. networks for advanced computing. scientific american, special issue: the next computer revolution, volume 257, pp. 136œ143, october 1987.karplus, martin. molecular dynamics simulation of proteins. physics today, special issue: computational physics, volume 40, number 10,pp. 68œ72, october 1987.kerr, richard a. pluto's orbital motion looks chaotic. science, volume 240, pp. 986œ1987, 20 may 1988.kiesler, sara b., and lee s. sproull (eds.) computing and change on campus. cambridge, england: cambridge university press, 1987.koening, 1987. speech production and its impact on use of computers.lax, peter d. report of the panel on largescale computing in science and engineering (p.d. lax, chair). washington, d.c.: nationalscience foundation, december 26, 1982.leaf, jesse j. databases turn computers into science libraries. computers in physics, january/february 1988, volume 2, number 1, pp. 24œ31.lederberg, joshua. digital communications and the conduct of science: the new literacy. proceedings of the ieee, volume 66, number 11,pp. 1314œ1319, november 1978.lievrouw, leah a. the communication network as interpretive environment: ﬁsensemakingﬂ among biomedical research scientists.unpublished doctoral dissertation, university of southern california, april 1986.liscouski, joseph g. note on barriers to information technology in research. personal communication, january 23, 1987.londer, randi. access to supercomputers. mosaic, volume 16, number 3, pp. 26œ32, 1985.lunin, lois f., and peter b. schipma (eds.) perspectives on cdrom for information storage and retrieval. journal of the american society for information science, special issue, volume 39, number 1, january 1988.mace, scott. university net thrives as 8,000node system. user profile. infoworld, november 2, 1987, p. 24.malone, thomas w., joanne yates, and robert i. benjamin. electronic markets and electronic hierarchies. communications of the acm, volume 30, number 6, pp. 484œ497, june 1987.manuel, tom. what's holding back expert systems? inside technology, special report. electronics, pp. 59œ63, august 7, 1986.marshall, eliot. the scourge of computer viruses. science, volume 240, pp. 133œ134, april 8, 1988.mathis, robert f. the last ten percent. ieee transactions of software engineering, volume se12, number 6, pp. 705œ712, june 1986.bibliography and selected readings64information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.mccormick, bruce h., thomas a. defanti, and maxine d. brown. visualization in scientific computing. (nsf report) computer graphics, volume 21, number 6, pp. 1œ14. new york, ny: association for computing machinery, acm siggraph, november 1987.metzger, norman. a third kind of science. mosaic, volume 16, number 3, pp. 2œ7, 1985.morris, james h., mahadev satyanarayanan, michael h. connor, john h. howard, david s. h. rosenthal, and f. donelson smith. andrew:a distributed personal computing environment. communications of the acm, volume 29, number 3, pp. 184œ201, march 1986.national academy of sciences, national academy of engineering, institute of medicine. committee on science, engineering, and publicpolicy. frontiers in science and technology: a selected outlook. new york, ny: w.h. freeman and company, 1983.. research briefing on computer architecture. 1984. reprinted in: new pathways in science and technology. new york, ny: vintagebooks, 1985.national academy of sciences/national research council, ad hoc committee on resources for the mathematical sciences . renewing u.s.mathematics: critical resource for the future (e. e. david, chair). washington, d.c., national academy press, 1984.national commission for employment policy. computers in the workplace: selected issues. report no. 19. washington, d.c.: nationalcommission for employment policy, march 1986.national library of medicine. long range plan executive summary. report of the board of regents. washington, d.c.: national institutes ofhealth, january 1987.. long range plan. report of panel 2: locating and gaining access to medical and scientific literature. washington, d.c.: nationalinstitutes of health, december 1986.. long range plan. report of panel 4: medical informatics. washington, d.c.: national institutes of health, december 1986.national research council, committee on the applications of mathematics, office of mathematical sciences, commission on physicalsciences, mathematics, and resources. computational modeling and mathematics applied to the physical sciences. washington,d.c.: national academy press, 1984.national research council, committee on computerassisted modeling, board on basic biology, commission on life sciences. computerassisted modeling: contributions of computational approaches to elucidating macromolecular structure and function. washington,d.c.: national academy press, 1987.national research council, committee on data needs, numerical data advisory board, assembly of mathematical and physical sciences.national needs for critically evaluated physical and chemical data. washington, d.c.: national academy of sciences, 1978.national research council, computer science and technology board, commission on physical sciences, mathematics, and resources. the national challenge in computer science and technology. washington, d.c.: national academy press, 1988.national research council, national research network review committee, computer science and technology board, commission onphysical sciences, mathematics, and resources. toward a national research network. washington, d.c.: national academypress, 1988.national research council, panel on education, numerical data advisory board, commission on physical sciences, mathematics, andresources. improving the treatment of scientific and engineering data through education. washington, d.c.: national academypress, 1986.national research council, panel on mathematical sciences, board on mathematical sciences, commission on physical sciences,mathematics, and resources. mathematical sciences: a unifying and dynamic resource (phillip a. griffiths, chair). washington,d.c.: national academy press, 1986.national research council, physics survey committee, board on physics and astronomy, commission on physical sciences, mathematicsand resources. physics through the 1990s: an overview. washington, d.c.: national academy press, 1986.norris, william c. computer generations and technological cooperation. the bridge, volume 16, number 4, pp. 19œ24, winter 1986.nysernet. market and economic impact study: lessons from four networks. second report. new york, ny: new york state educationand research network, inc., april 5, 1988.office of science and technology policy, executive office of the president . a research and development strategy for high performance computing. committee on computer research and applications, federal coordinating council on science, engineering, andtechnology (fccset). washington, d.c.: executive office of the president, november 20, 1987.. report of the federal coordinating council on science, engineering, and technology panel on advanced computer research in thefederal government. washington, d.c.: executive office of the president, june 1985.. research in very high performance computing: policy recommendation and research requirebibliography and selected readings65information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.ments statement. washington, d.c.: executive office of the president, november 1985.peterson, ivars. packing it in: fractals play an important role in image compression. science news, volume 131, number 18, pp. 283œ285,may 2, 1987.. picture this: the sounds of speech lead to novel ways of representing complex data. science news, volume 131, pp. 392œ395, june 20,1987.. twists of space: an artist, a computer programmer and a mathematician work together to visualize exotic geometric forms. sciencenews, volume 132, pp. 264œ266, october 24, 1987.. reaching for the supercomputing moon. science news, volume 133, number 11, pp. 172œ173, march 12, 1988.. a digital matter of life and death. science news, volume 133, number 11, pp. 170œ171, march 12, 1988.. a computer at your fingertips. science news, volume 133, number 22, p. 351, may 28, 1988.. highways for information. science news, volume 133, number 25, pp. 394œ395, june 18, 1988.porter, alan l., and frederick a. rossini. current and future uses of the computer: industrial r&d in the united states. r&d management,volume 16, number 4, pp. 279œ289, 1986.quarterman, john s., and j. c. hoskins. notable computer networks. communications of the acm, volume 29, number 10, pp. 932œ971,october 1986.raveche, harold j., duncan h. lawrie, and alvin m. despain. a national computing initiative: the agenda for leadership. report of thepanel on research issues in largescale computational science and engineering. philadelphia, pa: society for industrial andapplied mathematics, 1987.rheinboldt, werner c. (chairman) future directions in computational mathematics, algorithms, and scientific software. report of the panelon future directions in computational mathematics, algorithms, and scientific software . philadelphia, pa: society for industrialand applied mathematics, 1985.roberts, leslie. academy backs genome project. science, volume 239, pp. 725œ726, 12 february 1988.roode, david, rob liebschutz, sunil maulik, terry friedemann, david benton, and david kristofferson. new developments at bionet.nucleic acids research, volume 16, number 5, part a, pp. 1857œ1859, 1988.rumble, john r. why can't we access more numeric data via computers? in: williams, m. e., and t. h. hogan (eds.). proceedings of thenational online meeting, pp. 325œ330. medford, n.j.: learned information, inc., 1984.simon, herbert a. whether software engineering needs to be artificially intelligent. ieee transactions on software engineering, volumese12, number 7, pp. 726œ732, july 1986.steele, guy. common lisp: the language. bedford, ma: digital press, 1984.straub, galen k. role for pcs in theoretical physics? in: kahaner, david k. (ed.). science on small computers. siam news, p. 12, march1988.thomsen, dietrick e. seeking supernovas systematically. science news, volume 132, number 10, pp. 156œ157, september 5, 1987.. big telescopes on a roll. science news, volume 132, pp. 170œ171, september 12, 1987.turner, judith a. science agency picks michigan group to run computer network: $50 million ﬁnsfnetﬂ will link supercomputers,researchers. the chronicle of higher education, pp. a1, a14, november 25, 1987.. project linking different computers may alter the way researchers collaborate. the chronicle of higher education, volume 34, number27, pp. a13, a20, march 16, 1988.. uncertainties strain computer networks for u.s. professors. the chronicle of higher education, pp. a12œa13, april 6, 1988.. email technology has boomed, but manners of its users fall short of perfection. the chronicle of higher education, pp. a12œa16, april13, 1988.. plan for $5million prototype of electronic research library announced. the chronicle of higher education, p. a27, june 1, 1988.u.s. congress. general accounting office. satellite data archiving: u.s. and foreign activities and plans for environmental information.report rced88œ201. washington, d.c.: united states general accounting office, september 1988.u.s. congress. general accounting office. space operations: nasa's use of information technology. report imtec87œ20. washington,d.c.: united states general accounting office, april 1987.u.s. congress. house committee on science and technology. task force on science policy. the impact of the information age on science. science policy studyšhearings volume 10 (september 10œ12, 1985). no. 103. washington, d.c.: u.s. government printingoffice, 1986a.u.s. congress. house committee on science and technology. task force on science policy. the impact of the information age on science. science policy studyšbackground report no. 5. washington, d.c.: u.s. government printing office, september 1986b.u.s. congress. office of technology assessment. information technology r&d: critical trends and issues. new york, n.y.: pergamonpress, 1985.u.s. congress. office of technology assessment. science technology and the constitutionšbackground paper, otabpcit43.washington, d.c.: u.s. government printing office, september 1987.bibliography and selected readings66information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.university of michigan. expres newsletter. volume 1, number 1. available from: violet elder, csmil, c2420 business administrationbuilding, 904 monroe st., ann arbor, mi 48109 . june 1987.warlick, charles h. academic computing facilities and services in higher educationša survey. educom bulletin, volume 21, number 3,pp. 2œ7, fall/winter 1986.weiser, mark. source code. ieee computer, volume 20, number 11, pp. 66œ73, november 1987.weiss, rick. organic origami: scientists study the art of protein folding. science news, volume 132, pp. 344œ346, november 28, 1987.westbrook, j. h., h. behrens, g. dathe, and s. iwata (eds.) materials data systems for engineering. proceedings of a codata workshop,september 22œ27, 1985. karlsruhe, frg: fachinformationszentrum, 1986.westbrook, j. h., and l. r. mccreight (eds.) computerized aerospace materials data (computerized materials property and design data foraerospace technology). new york, n.y.: american institute of aeronautics and astronautics, 1987.westbrook, j.h., and j. r. rumble, jr. (eds.) computerized materials data systems. office of standard reference data. gaithersburg, md:national bureau of standards, 1983.winkler, karlheinz a., jay w. chalmers, stephen w. hodson, paul r. woodward, and norman j. zabusky. a numerical laboratory.physics today (special issue: computational physics), volume 40, number 10, pp. 28œ37, october 1987.wooster, harold. historical note: shining palaces, shifting sands: national information systems. journal of the american society for information science, volume 38, number 5, pp. 321œ335, september 1987.wright, robert. virtual reality. the information age. the sciences, volume 27, number 6, pp. 8œ10, november/december 1987.zurer, pamela s. computers gaining firm hold in chemical labs. special report. chemical and engineering news, pp. 21œ46, august 19,1985.bibliography and selected readings67information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.bibliography and selected readings68information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.indexaacademic sector, see universitiesaccess to computing resources, 37, 49, 50, 51compatibility/incompatibility (systems), 2, 2122, 23, 25,27, 36, 4950databases, 2529hardware, 2, 17networks, 16, 49, 51software, 11, 1617administration and administrators, see management andmanagersalgorithms, 17american chemical society (chemabstracts), 23, 24, 28, 30american psychological association, 30ames laboratory, 19arpanet, 20, 26artificial intelligence and expert systems, 3, 31, 3233, 35,3839association of american medical colleges (aamc), 35associations, see professional associationsbbaylor medical college, 35behavioral factors, see human factorsbionet, 2223bitdoc, 25bitnet, 24, 26, 36bitnic, 25brown university, 41bush, vannevar, 40ccabios, 22california, 15calnet, 15canada, 25carnegie mellon university, 19, 43cell, 22chemical abstracts service, 23, 24, 28, 30chips (memory), 20city university of new york, 2425committee on national statistics, 29committee on science and technology, 78committee on science, engineering, and public policy, 78common lisp, 20communications technology, 1, 1822, 48electronic mail, 2, 18, 19, 2022, 2627networks (computers), 2, 3, 1427, 3637, 4041, 4951, 53telecommunications, 1, 1112compatibility/incompatibility (systems), 2, 23, 25, 4950databases, 27, 36networks, 2122, 36, 4950see also standardscomputational sciences,high energy physics, 1214mathematical theory, 1617medical sciences, 2223, 2829, 35, 36computer graphicsearthquakes, 15interactive, 20, 34, 40index69information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.mathematicsphysical science interface, 17satellites, 30computer hardware, 7access to, 2, 17data collection and analysis, 14historical developments in, 2, 11microprocessors, dedicated, 1314optical disks, 27, 52semiconductors, 20supercomputers, 21, 22, 49computer simulation, 3, 31, 47in econometrics, 31numerical computations, 1617computer software, 2, 7access to, 11, 1617customized, 1617, 43, 49data collection and analysis, 14evaluation, 4344expert systems, 3, 31, 3233, 35federal government, 52high energy physics and, 13ibm infrastructure, 4445institutional infrastructure, 3instrument control, 15, 48libraries of, 4, 51molecular biology, 2223, 2829, 36networks (computers), 2, 3, 1427, 3637, 4041, 4951, 53packages, 15, 1617, 18, 4445standards, 3word processing, 2, 18, 2122computer visualization, 3, 21, 30, 3132, 4748confidentiality, 3, 3738, 52congress, 78ddatabases, 23, 15, 2428economics, 3637hypertext, 4041molecular biology, 2223, 2829, 36searching, 2, 24, 27, 28see also specific databasesdata collection and analysis, 2, 1417capacities, 14decision making, 3, 39policymakers, 1, 9, 29, 55defense, 15, 20, 43denning, peter, j., 19, 2223department of commerce, 8department of defense, 20, 43department of energy, 8, 19document processing, 19eearthquakes, 15econometrics, 31economic factors, 3, 4, 48, 49, 50access to computers, 16costs and cost sharing, 1415, 3537satellite data system, 30study funding, 8time sharing, 35education and training, 3, 3941, 49universities, 4, 14, 19, 2425, 35, 36educom, 25, 44electronic mail, 2, 18, 19, 20case study, 2627incompatibility problems, 21molecular biology, 22electronic publishing, 19, 2122embl, 22englebart, doug, 40ethics, see law and ethicseuropean access research network (earn), 25expertise (human), 4, 43, 44, 52early computers, 11policymakers, 1, 9, 29, 55study methodology, 8, 57expert systems and artificial intelligence, 3, 31, 3233, 35,3839expres, 19, 21, 22, 54ffederal coordinating council on science, engineering, andtechnology (fccset), 53, 55federal government, 48, 5153databases, 23, 2930national research network, 45, 21, 5254panel study funding, 8fermi national laboratory, 14file transfer, 21frontiers in science and technology: a selected outlook, 7ggatekeepers, 43genbank, 22, 23, 29genpub, 22genetics, 22, 23, 2829geometry, 16georgetown medical center, 35graphics, see computer graphicshhelp, 35high energy physics network, 21index70information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.human factors, 3, 8, 49attitudinal, 4041, 50confidentiality, 3, 3738, 52education and training, 3, 3941, 49infrastructure, 3, 45, 21, 34, 35, 4245, 5254institutional factors, 34, 4142, 49, 51law and ethics, 3, 22, 3738, 42, 50, 52management and managers, 1, 4, 9, 20, 22, 29, 41, 50, 51usermachine interfaces, 4human gene mapping library (hgml), 28hypertext, 4041iibm, 2425, 4445images and image processing, see computer visualizationincompatibility (systems), see compatibility/incompatibility;standardsindexes, 24, 25information dissemination, 2, 4344national research network, 45, 21, 5254information storage and retrieval, 23, 14, 2330databases 23, 15, 2223, 2428, 29, 3637, 4041networks (computers), 2, 3, 1427, 3637, 4041, 4951, 53information technology, definition and development of, 1, 7,1112infrastructure, 3, 34, 35, 4245national, 45, 21, 5254institute for mathematical statistics, 16institute for scientific information, 30institutional factors, 34, 4142, 49, 51instrument control, 15, 48integrated academic information management system(iaims), 35intellectual property, 22, 42, 50, 52intermedia, 41international activitiesgene mapping conference, 2829networks (computers), 14, 22, 2425standards, 19jjackson laboratory mouse map, 28johns hopkins university, 35llandsat, 30languages (programming), 20law and ethicsconfidentiality, 3, 3738, 52decisionmaking liability, 38intellectual property, 22, 42, 50, 52laboratory notebooks, electronic, 42security of information, 22, 3738, 42, 52library science, 19, 24, 28, 3839, 51lisp, 20mmanagement and managers, 1, 9, 20, 29, 41, 50of networks, 4, 22, 51mathematicssimulations, 1617, 47software packages, 15mckusick, victor a, 2829medical sciences, 24, 35genetics, 22, 23, 2829molecular biology, 2223, 2829, 36medlars, 24medline, 29memex, 25, 40mendelian inheritance in man, 28metafont, 19metal oxide semiconductor implementation system, 20microprocessors, 1314military sciences, see defensemodels, see computer simulationmolecular biology, 2223, 2829, 36nnational academy of engineering, 7national academy of sciences, 7national aeronautics and space administration, 8, 15, 23, 30national bureau of standards, 8national center for biotechnology information, 36national library of medicine, 8, 24, 2829, 35, 36, 55national oceanic and atmospheric administration, 8, 23, 30national research council, 29, 5455national science foundation, 8, 19, 21, 22, 2829, 53, 54national space science data center, 30nelson, ted, 40netnorth, 25networks (computers), 2, 15, 18, 2021, 53access to, 16, 49, 51earthquake prediction, 15economics, 3637electronic mail, 2, 18, 19, 2022, 2627high energy physics research and, 14hypertext, 4041incompatibility problems, 2122, 36, 4950index71information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.international, 14, 22, 2425molecular biology, 2223national, 21reliability of transmission, 17standards, 3university, 14, 2425see also specific networksnew york state education and research network(nysernet), 15nsfnet, 21, 22, 54ooffice of science and technology policy, 4, 52, 53, 55omim, 2829omnet, 26optical disks, 27, 52organizational factors, see institutional factors;networks (computers)ppanel on information technology and the conduct ofresearch, 89, 2627particle physics, see physicspatents, 42personal computers, 11, 19physicshigh energy, 1215, 21mathematics and, 1617policymakers, 1, 9, 29, 55policy recommendations, 35, 5055postal services, see electronic mailpostscript, 19professional associations, 4, 17, 30, 35, 50, 55programs and programming, see computer software;languages (programming)psychological abstracts, 30publishing, 19, 2122qquality control, 2see also standardsrreal time, 13, 15ssatellites, 30scribe, 19searching (databases), 2, 24, 27, 28security issues, 22, 3738, 42, 52semiconductors, 20simulation, see computer simulationsoftware, see computer softwarespeech production (computers), 33standards, 3, 4, 19, 27, 37, 43, 51, 53, 55document processing, 19, 2122statistics, 15, 16supercomputers, 21, 22, 49superconducting super collider, 1314superconductivity, 19ttelecommunications, 1, 1112networks (computers), 2, 3, 1427, 3637, 4041, 4951, 53terabit memory system, 30tex, 19time sharing, 35toxline, 24training, see education and traininguuniversities, 4, 19, 28, 35, 36, 41, 42, 43, 50networks (computers), 14, 2425see also specific universitiesuniversity of california at berkeley, 2425university of michigan, 19university of utah, 35unix, 19vvendors, 4, 51, 52very largescale integration, 20visualization, see computer visualizationwwilliam h. welch medical library, 2829word processing, 2, 18, 2122workshop on the use of computers in statistical research, 16workstations, 19wysiwyg, 19xxanadu, 4041yyale university, 24, 28index72information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.a ﬁfractal dragonﬂ generated by the ibm fellow benoit b. mandelbrot, the originator of fractal geometry. this isan example of the ﬁjulia set,ﬂ which in turn is an example of a ﬁspeller setﬂ of a dynamic system. this may seem tobe an extremely complicated shape, yet it has a very simple equation based on the formula from the front cover ofthe book the fractal geometry of nature by benoit b. mandelbrot, 1982, w. h. freeman and company.73information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.the ﬁfractal planetriseﬂ by ibm scientist richard f. voss. in spite of its startling realism, every element in thispicture is artificially generated. the striking resemblance between some fractal images and familiar landscapesillustrates the fact that fractals describe aspects of nature that have formerly eluded mathematical description. fromthe back cover of the book the fractal geometry of nature by benoit b. mandelbrot, 1982, w. h. freeman andcompany. courtesy of the ibm corporation.van der waals surface of dihydrofolate reductase and methotextrate. red is oxygen, blue is nitrogen, green iscarbon, and yellow is phosphorus. produced by the computer graphics laboratory, university of california, sanfrancisco. © regents, university of california.74information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.this realisticlooking landscape is actually only a few hundred atoms square and approximately 10 atoms high.generated by ibm scientist richard f. voss, who used a computer to add color, lighting, and shading to a scanningtunneling microscope image of thermally roughened silicon. in addition to the esthetic benefits of such pictures,scientists and engineers can use such images to understand the properties of critical materials such as silicon. colorcoding, for example, can emphasize and delineate specific atomic areas of interest, such as atomic trace impuritiesor surface defects. courtesy of the ibm corporation.frame from a computeranimated film depicting clustering of matter in the early evolution of the universe. the filmitself was produced by a collaborative effort between an astrophysicist and a hollywood specialeffects graphicsfirm. © joan m. centrella, drexel university.75information technology and the conduct of research: the user's viewcopyright national academy of sciences. all rights reserved.scherk's doubly periodic minimal surface, which has recently been proposed as a model microstructure for grainboundaries in copolymers. nature, august 8, 1988.© j. t. hoffman.76
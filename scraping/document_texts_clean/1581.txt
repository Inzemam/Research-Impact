detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/1581computers at risk: safe computing in the information age320 pages | 6 x 9 | paperbackisbn 9780309043885 | doi 10.17226/1581system security study committee, commission on physical sciences, mathematics,and applications, national research councilcomputers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.computers at risksafe computing in the information agesystem security study committeecomputer science and telecommunications boardcommission on physical sciences, mathematics, and applicationsnational research councilnational academy press1991icomputers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.national academy press 2101 constitution avenue, n.w. washington, d.c. 20418notice: the project that is the subject of this report was approved by the governing board of thenational research council, whose members are drawn from the councils of the national academyof sciences, the national academy of engineering, and the institute of medicine. the members ofthe committee responsible for the report were chosen for their special competences and with regardfor appropriate balance.this report has been reviewed by a group other than the authors according to proceduresapproved by a report review committee consisting of members of the national academy of sciences, the national academy of engineering, and the institute of medicine.the national academy of sciences is a private, nonprofit, selfperpetuating society of distinguished scholars engaged in scientific and engineering research, dedicated to the furtherance ofscience and technology and to their use for the general welfare. upon the authority of the chartergranted to it by the congress in 1863, the academy has a mandate that requires it to advise the federal government on scientific and technical matters. dr. frank press is president of the nationalacademy of sciences.the national academy of engineering was established in 1964, under the charter of thenational academy of sciences, as a parallel organization of outstanding engineers. it is autonomousin its administration and in the selection of its members, sharing with the national academy of sciences the responsibility for advising the federal government. the national academy of engineeringalso sponsors engineering programs aimed at meeting national needs, encourages education andresearch, and recognizes the superior achievements of engineers. dr. robert m. white is presidentof the national academy of engineering.the institute of medicine was established in 1970 by the national academy of sciences tosecure the services of eminent members of appropriate professions in the examination of policy matters pertaining to the health of the public. the institute acts under the responsibility given to thenational academy of sciences by its congressional charter to be an adviser to the federal government and, upon its own initiative, to identify issues of medical care, research, and education. dr.samuel o. thier is president of the institute of medicine.the national research council was organized by the national academy of sciences in 1916 toassociate the broad community of science and technology with the academy's purposes of furthering knowledge and advising the federal government. functioning in accordance with general policies determined by the academy, the council has become the principal operating agency of both thenational academy of sciences and the national academy of engineering in providing services tothe government, the public, and the scientific and engineering communities. the council is administered jointly by both academies and the institute of medicine. dr. frank press and dr. robert m.white are chairman and vice chairman, respectively, of the national research council.support for this project was provided by the defense advanced research projects agencyunder contract no. n0001489j1731. however, the content does not necessarily reflect the position or the policy of the defense advanced research projects agency or the government, and noofficial endorsement should be inferred.library of congress cataloginginpublication datacomputers at risk: safe computing in the information age / system security study committee,computer science and telecommunications board, commission on physical sciences, mathematics, and applications, national research council.p. cm.includes bibliographical references.isbn 03090438831. computer security. i. national research council (u.s.).computer science and telecommunications board. system security study committee.qa76.9.a25c6663 1990005.8šdc20 9022329cipcopyright © 1991 by the national academy of sciencesno part of this book may be reproduced by any mechanical, photographic, or electronic process, or in the form of a phonographic recording, nor may it be stored in a retrieval system, transmitted, or otherwise copied for public or private use, without written permission from the publisher,except for the purposes of official use by the u.s. government.printed in the united states of america first printing, december 1990 second printing, march 1991third printing, april 1992 fourth printing january 1992 fifth printing march 1994iicomputers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.system security study committeedavid d. clark, massachusetts institute of technology, chairmanw. earl boebert, secure computing technology corporationsusan gerhart, microelectronics and computer technology corporationjohn v. guttag, massachusetts institute of technologyrichard a. kemmerer, university of california at santa barbarastephen t. kent, bbn communicationssandra m. mann lambert, security pacific corporationbutler w. lampson, digital equipment corporationjohn j. lane, shearson, lehman, hutton, inc.m. douglas mcilroy, at&t bell laboratoriespeter g. neumann, sri internationalmichael o. rabin, harvard universitywarren schmitt, sears technology servicesharold f. tipton, rockwell internationalstephen t. walker, trusted information systems, inc.willis h. ware, the rand corporationmarjory s. blumenthal, staff directorfrank pittelli, cstb consultantdamian m. saccocio, staff officermargaret a. knemeyer, staff associatedonna f. allen, administrative secretarycatherine a. sparks, senior secretaryiiicomputers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.computer science and telecommunicationsboardjoseph f. traub, columbia university, chairmanalfred v. aho, at&t bell laboratoriesjohn seely brown, xerox corporation palo alto research centerfrank p. carrubba, hewlettpackard companydavid j. farber, university of pennsylvaniasamuel h. fuller, digital equipment corporationjames freeman gilbert, university of california at san diegowilliam a. goddard iii, california institute of technologyjohn l. hennessy, stanford universityjohn e. hopcroft, cornell universitymitchell d. kapor, on technology, inc.sidney karin, san diego supercomputer centerleonard kleinrock, university of california at los angelesrobert langridge, university of california at san franciscorobert l. martin, bell communications researchwilliam f. miller,sri internationalabraham peled, ibm t.j. watson research centerraj reddy, carnegie mellon universityjerome h. saltzer, massachusetts institute of technologymary shaw, carnegie mellon universityeric e. sumner, institute of electrical and electronics engineersivan e. sutherland, sutherland, sproull & associatesgeorge l. turin, teknekron corporationvictor vyssotsky, digital equipment corporationwillis h. ware, the rand corporationwilliam wulf, university of virginiamarjory s. blumenthal, staff directoranthony m. forte, senior staff officerherbert lin, staff officerdamian m. saccocio, staff officerrenee a. hawkins, staff associatedonna f. allen, administrative secretarylinda l. joyner, project assistantcatherine a. sparks, senior secretaryivcomputers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.commission on physicalsciences,mathematics, and applications*norman hackerman, robert a. welch foundation, chairmanpeter j. bickel, university of california at berkeleygeorge f. carrier, harvard universityherbert d. doan, the dow chemical company (retired)dean e. eastman, ibm t.j. watson research centermarye anne fox, university of texasphillip a. griffiths, duke universityneal f. lane, rice universityrobert w. lucky, at&t bell laboratorieschristopher f. mckee, university of california at berkeleyrichard s. nicholson, american association for the advancement ofsciencejeremiah p. ostriker, princeton university observatoryalan schriesheim, argonne national laboratoryroy f. schwitters, superconducting super collider laboratorykenneth g. wilson, ohio state universitynorman metzger, executive director* the project that is the subject of this report was initiated under the predecessorgroup of the commission on physical sciences, mathematics, and applications, whichwas the commission on physical sciences, mathematics, and resources, whosemembers are listed in appendix g.vcomputers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.vicomputers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.prefacethe computer science and technology board, which became thecomputer science and telecommunications board in september 1990, formedthe system security study committee in response to a fall 1988 request fromthe defense advanced research projects agency (darpa) to address thesecurity and trustworthiness of u.s. computing and communications systems.the committee was charged with developing a national research, engineering,and policy agenda to help the united states achieve a more trustworthycomputing technology base by the end of the century. darpa asked thecommittee to take a broad outlookšto consider the interrelationship of securityand other qualities (e.g., safety and reliability), commercialization as well asresearch, and the diverse elements of the research and policy communities. inkeeping with darpa's initial request, the committee focused on securityaspects but related them to other elements of trustworthiness.the system security study committee was composed of sixteenindividuals from industry and academia, including computer andcommunications security researchers and practitioners and software engineers.it met in may, august, and november of 1989 and in february, april, and julyof 1990. its deliberations were complemented by briefings from and interviewswith a variety of federal government researchers and officials and securityexperts and others from industry. a central feature of the committee's work wasthe forging of a consensus in the face of different technical and professionalperspectives. while the committee drew on both the research literature andpublications aimed at security practitioners, it sought to combine the researchand practitioner perspectives to provide a more unified asprefaceviicomputers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.sessment than might perhaps be typical. given the goal of producing anunclassified report, the committee focused on the protection of sensitive butunclassified information in computer and communications systems. theorientation toward an unclassified report also limited the extent to which thecommittee could probe tensions in federal policy between intelligencegatheringand securityproviding objectives.this report of the system security study committee presents itsassessment of key computer and communications security issues and itsrecommendations for enhancing the security and trustworthiness of the u.s.computing and communications infrastructure.david d. clark, chairmansystem security study committeeprefaceviiicomputers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.acknowledgmentsthe system security study committee appreciates the generous assistanceprovided by carl landwehr of the naval research laboratory and a group offederal liaisons that he coordinated, including anthony adamski of the federalbureau of investigation, dennis branstad of the national institute of standardsand technology, leon breault of the department of energy, richard carr ofthe national aeronautics and space administration, richard demillo of thenational science foundation (preceded by john gannon), c. terrance irelandof the national security agency, stuart katzke of the national institute ofstandards and technology, robert morris of the national security agency,karen morrissette of the department of justice, mark scher of the defensecommunications agency, and kermith speierman of the national securityagency. these individuals made themselves and their associates available tothe committee to answer questions, provide briefings, and supply valuablereference materials.the committee is grateful for special briefings provided by william vanceof ibm, john michael williams of unisys, and peter wild of coopers andlybrand. additional insight into specific issues was provided by severalindividuals, including in particular mark anderson of the australian electronicsresearch laboratory, carolyn conn of ge information services, jay crawfordof the naval weapons center at china lake, california, george dinolt of fordaerospace corporation, morrie gasser and ray modeen of digital equipmentcorporation, james giffin of the federal trade commission, j. thomas haighof secure computing technology corporation, james hearn of the nationalsecurity agency, frank houston of the food and drug administration,christian jahl of the german industrie anlagen betriebsacknowledgmentsixcomputers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.gesellschaft, ian king of the u.k. communicationselectronics securitygroup, stewart kowalski of the university of stockholm, milan kuchta of thecanadian communications security establishment, timothy levin of geminicomputers, inc., michael nash representing the u.k. department of trade andindustry, stephen purdy and james bauer of the u.s. secret service, johnshore of entropic research laboratory, inc., linda vetter of oraclecorporation, larry wills of ibm, and the group of 30 corporate securityofficers who participated in a small, informal survey of product preferences.the committee appreciates the encouragement and support of stephensquires and william scherlis of darpa, who provided guidance, insights, andmotivation. it is particularly grateful for the literally hundreds of suggestionsand criticisms provided by the ten anonymous reviewers of an early draft.those inputs helped the committee to tighten and strengthen its presentation, forwhich it, of course, remains responsible.finally, the committee would like to acknowledge the major contributionthat the staff of the computer science and telecommunications board hasmade to this report, in particular thanking marjory blumenthal, damiansaccocio, frank pittelli, and catherine sparks. they supplied not only verycapable administrative support, but also substantial intellectual contributions tothe development of the report. the committee also received invaluableassistance from its editor, susan maurizi, who labored under tight timeconstraints to help it express its ideas on a complex and jargonfilled subject. itcould not have proceeded effectively without this level of support from thenational research council.david d. clark, chairmansystem security study committeeacknowledgmentsxcomputers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.contents executive summary 11 overview and recommendations 7 computer system security concerns 8 trendsšthe growing potential for systemabuse 10 the need to respond 11 toward a planned approach 13 achieving understanding 13 the nature of security: vulnerability,threat, and countermeasure 13 special security concerns associatedwith computers 15 security must be holisticštechnology,management, and social elements 17 commercial and military needs are different 18 putting the need for secrecy into perspective 20 building on existing foundations 21 scope, purpose, contents, and audience 24 recommendations 26recommendation 1: promulgate comprehensive generallyaccepted system security principles(gssp) 27recommendation 2: take specific shortterm actions thatbuild on readily available capabilities 32recommendation 3: gather information and provide education 36recommendation 4: clarify export control criteria, and set upa forum for arbitration 37contentsxicomputers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.recommendation 5: fund and pursue needed research 39recommendation 6: establish an information security foundation 43 conclusion 45 notes 452 concepts of information security 49 security policiesšresponding to requirements for confidentiality, integrity, andavailability 52 confidentiality 52 integrity 54 availability 54 examples of security requirements for different applications 55 management controlsšchoosing the meansto secure information and operations 56 preventing breaches of securityšbasicprinciples 56 responding to breaches of security 59 developing policies and appropriate controls 59 risks and vulnerabilities 61 securing the whole system 65appendix 2.1š privacy 66appendix 2.2š informal survey to assess security requirements 69 notes 723 technology to achieve secure computer systems 74 specification vs. implementation 75 specification: policies, models, and services 76 policies 77 models 80 flow model 80 access control model 81 services 83 authentication 84 authorization 87 auditing 88 implementation: the trusted computing base 88 computing 91 hardware 91 operating system 92 applications and the problem of malicious code 93contentsxiicomputers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved. communications 93 secure channels 94 authenticating channels 96 security perimeters 98 methodology 99 conclusion 99 notes 1004 programming methodology 102 software is more than code 104 simpler is better 106 the role of programming languages 107 the role of specifications 108 relating specifications to programs 109 formal specification and verification 111 hazard analysis 113 structuring the development process 114 managing software procurement 115 scheduling software development 116 education and training 117 management concerns in producing securesoftware 118 what makes secure software different 119 recommended approaches to sound development methodology 120 notes 1225 criteria to evaluate computer andnetwork security 124 security evaluation criteria in general 125 security characteristics 125 assurance evaluation 127 tradeoffs in grouping of criteria 130 comparing national criteria sets 133 reciprocity among criteria sets 135 system certification vs. product evaluation 137 recommendations for product evaluation andsystem certification criteria 139 notes 1416 why the security market has notworked well 143 the market for trustworthy systems 143 a soft market: concerns of vendors 146contentsxiiicomputers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved. federal government influence on the market 149 procurement 149 strategic federal investments in researchand development 150 export controls as a market inhibitor 152 technology transfer: rationale for controlling security exports 153 export control of cryptographic systems and components 154 export control of trusted systems 156 the commercial imperative 157 consumer awareness 159 insurance as a market lever 161 education and incident tracking for security awareness 162 education 162 incident reporting and tracking 163 technical tools to compensate for limitedconsumer awareness 164 regulation as a market influence: productquality and liability 165 product quality regulations 166 product liability as a market influence 167 software and systems present specialproblems 170 toward equitable allocation of liability 171appendix 6.1š export control process 173appendix 6.2š insurance 174 notes 1767 the need to establish an information security foundation 179 actions needed to improve computer security 179 attributes and functions of the proposed newinstitution 180 other organizations cannot fulfill isf's mission 183 government organizations 183 private organizations 184 why isf's mission should be pursued outsideof the government 185 a new notforprofit organization 186 critical aspects of an isf charter 187 startup considerations 188 funding the isf 188 alternatives to the isf 190contentsxivcomputers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.appendix 7.1š a history of government involvement 192appendix 7.2 š security practitioners 201 notes 2048 research topics and funding 206 a proposed agenda for research to enhancecomputer security 208 directions for funding security research 211 funding by the defense advancedresearch projects agency 212 funding by the national science foundation 212 promoting needed collaboration 213 notes 214 bibliography 216 appendixes a the orange book 243b selected topics in computer security technology 246c emergency response teams 276d models for gssp 278e highgrade threats 283f glossary 286g list of members of the former commission onphysical sciences, mathematics, andresources 303contentsxvcomputers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.contentsxvicomputers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.executive summarycomputer systems are coming of age. as computer systems become moreprevalent, sophisticated, embedded in physical processes, and interconnected,society becomes more vulnerable to poor system design, accidents that disablesystems, and attacks on computer systems. without more responsible designand use, system disruptions will increase, with harmful consequences forsociety. they will also result in lost opportunities from the failure to putcomputer and communications systems to their best use.many factors support this assessment, including the proliferation ofcomputer systems into ever more applications, especially applications involvingnetworking; the changing nature of the technology base; the increase incomputer system expertise within the population, which increases the potentialfor system abuse; the increasingly global environment for business andresearch; and the global reach and interconnection of computer networks, whichmultiply system vulnerabilities. also relevant are new efforts in europe topromote and even mandate more trustworthy computer systems; europeancountries are strengthening their involvement in this arena, while the unitedstates seems caught in a policy quagmire. although recent and highlypublicized abuses of computer systems may seem exceptional today, eachillustrates potential problems that may be undetected and that are expected tobecome more common and even more disruptive. the nature and the magnitudeof computer system problems are changing dramatically.the nation is on the threshold of achieving a powerful informationinfrastructure that promises many benefits. but without adequate safeguards, werisk intrusions into personal privacy (given the growingexecutive summary1computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.electronic storage of personal information) and potential disasters that can causeeconomic and even human losses. for example, new vulnerabilities areemerging as computers become more common as components of medical andtransportation equipment or more interconnected as components of domesticand international financial systems. many disasters may result from intentionalattacks on systems, which can be prevented, detected, or recovered fromthrough better security. the nation needs computer technology that supportssubstantially increased safety, reliability, and, in particular, security.security refers to protection against unwanted disclosure, modification, ordestruction of data in a system and also to the safeguarding of systemsthemselves. security, safety, and reliability together are elements of systemtrustworthinessšwhich inspires the confidence that a system will do what it isexpected to do.in many ways the problem of making computer and communicationssystems more secure is a technical problem. unlike a file cabinet, a computersystem can help to protect itself; there exists technology to build a variety ofsafeguards into computer systems. as a result, software, hardware, and systemdevelopment presents opportunities for increasing security. yet knowntechniques are not being used, and development of better techniques is laggingin the united states. from a technical perspective, making computer systemtechnology more secure and trustworthy involves assessing what is at risk,articulating objectives and requirements for systems, researching anddeveloping technology to satisfy system requirements, and providing forindependent evaluation of the key features (to assess functionality) and theirstrength (to provide assurance). all of these activities interact.attaining increased security, in addition to being a technical matter is alsoa management and social problem: what is built and sold depends on howsystems are designed, purchased, and used. in today's market, demand fortrustworthy systems is limited and is concentrated in the defense communityand industries, such as banking, that have very high levels of need for security.that today's commercial systems provide only limited safeguards reflectslimited awareness among developers, managers, and the general population ofthe threats, vulnerabilities, and possible safeguards. most consumers have norealworld understanding of these concepts and cannot choose products wiselyor make sound decisions about how to use them. practical security specialistsand professional societies have emerged and have begun to affect securitypractice from inside organizations, but their impact is constrained by lack ofboth managementexecutive summary2computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.awareness and public awareness of security risks and options. even whenconsumers do try to protect their own systems, they may be connected vianetworks to others with weaker safeguardsšlike a polluting factory in adensely populated area, one person's laxness in managing a computer systemcan affect many. as long as demand remains at best inconsistent, vendors havefew incentives to make system products more secure, and there is little evidenceof the kind of fundamental new system development necessary to make systemshighly trustworthy. the market does not work well enough to raise the securityof computer systems at a rate fast enough to match the apparent growth inthreats to systems.the u.s. government has been involved in developing technology forcomputer and communications security for some time. its efforts have relatedlargely to preserving national security and, in particular, to meeting one majorsecurity requirement, confidentiality (preserving data secrecy). but theseprograms have paid little attention to the other two major computer securityrequirements, integrity (guarding against improper data modification ordestruction) and availability (enabling timely use of systems and the data theyhold). these requirements are important to government system users, and theyare particularly and increasingly important to users of commercial systems.needed is guidance that is more wideranging and flexible than that offered bythe socalled orange book published by the national security agency, and itshould be guidance that stimulates the production of more robust, trustworthysystems at all levels of protection.overall, the government's efforts have been hamstrung by internecineconflict and underfunding of efforts aimed at civilian environments. theseproblems currently appear to be exacerbated, at precisely the time that decisiveand concerted action is needed. a coherent strategy must be established now,given the time, resources, planning, and coordination required to achieveadequate system security and trustworthiness. the reorganization of andperceived withdrawal from relevant computer securityrelated activities at thenational security agency and the repeated appropriations of minimal fundingfor relevant activities at the national institute of standards and technology arestrong indications of a weak u.s. posture in this area. a weak posture isespecially troubling today, because of the momentum that is building overseasfor a new set of criteria and associated system evaluation schemes andstandards. influencing what can be sold or may be required in overseas markets,these developments and the u.s. response will affect the competitiveness ofu.s. vendors and theexecutive summary3computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.options available to users of commercial computer systems worldwide. theywill also affect the levels of general safety and security experienced by thepublic.this report characterizes the computer security problem and advancesrecommendations for containing it (chapter 1). it examines concepts of andrequirements for computer security (chapter 2), the technology necessary toachieve system security and trustworthiness, and associated development issues(chapter 3), programming methodology (chapter 4), the design and use ofcriteria for secure computer system development and evaluation of computersystem security relative to a set of criteria (chapter 5), and problemsconstraining the market for trustworthy systems (chapter 6). the systemsecurity study committee concluded that several steps must be taken to achievegreater computer system security and trustworthiness, and that the bestapproach to implementing necessary actions is to establish a new organization,referred to in the report as the information security foundation (isf). theconcept of the isf and the roles and limitations of organizations that currentlyhave significant responsibilities in the computer security arena are discussedtogether (chapter 7). topics and tactics for research to enable neededtechnology development are outlined (chapter 8). supporting the individualchapters are appendixes that provide further details on selected technical andconceptual points.the committee urges that its recommendations be considered together asintegral to a coherent national effort to encourage the widespread developmentand deployment of security features in computer systems, increase publicawareness of the risks that accompany the benefits of computer systems, andpromote responsible use and management of computer systems. toward the endof increasing the levels of security in new and existing computer andcommunications systems, the committee developed recommendations in sixareas. these are outlined below and developed further in the full report.1. promulgation of a comprehensive set of generally acceptedsystem security principles, referred to as gssp, which wouldprovide a clear articulation of essential security features,assurances, and practices. the committee believes that there is abasic set of securityrelated principles for the design, use, andmanagement of systems that are of such broad applicability andeffectiveness that they ought to be a part of any system with significantoperational requirements. this set will grow with research andexperience in new areas of concern, such as integrity and availability,and can also grow beyond the specifics of security to deal with otherrelated aspects of system trust, such as safety. gssp should enunciateand codifyexecutive summary4computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.these principles. successful gssp would establish a set of expectationsabout and requirements for good practice that would be wellunderstood by system development and security professionals,accepted by government, and recognized by managers and the publicas protecting organizational and individual interests against securitybreaches and associated lapses in the protection of privacy. gssp,which can be built on existing material (e.g., the orange book), wouldprovide a basis for resolving differences between u.s. and othernational and transnational criteria for trustworthy systems and forshaping inputs to international security and safety standards discussions.2. a set of shortterm actions for system vendors and users that buildon readily available capabilities and would yield immediate benefits, including (for users) formation of security policy frameworksand emergency response teams, and (for vendors) universalimplementation of specific minimal acceptable protections fordiscretionary and mandatory control of access to computing resources,broader use of modern software development methodology,implementation of security standards and participation in their furtherdevelopment, and procedures to prevent or anticipate the consequencesof inadvisable actions by users (e.g., systems should be shipped withsecurity features turned on, so that explicit action is needed to disablethem).3. establishment of a systemincident data repository andappropriate education and training programs to promote publicawareness.4. clarification of export control criteria and procedures for secure or trusted systems and review for possible relaxation of controls onthe export of implementations of the data encryption standard (des).5. funding and directions for a comprehensive program of research.6. establishment of a new organization to nurture the development, commercialization, and proper use of trust technology, referred to as the information security foundation, or isf. the committeeconcludes that existing organizations active in the security arena havemade important contributions but are not able to make the multifacetedand largescale efforts that are needed to truly advance the market andthe field. the proposed isf would be a private, notforprofitorganization. it would be responsible for implementing much of whatthe committee has recommended, benefiting from the inherentexecutive summary5computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.synergies: isf should develop gssp, develop flexible evaluationtechniques to assess compliance with gssp, conduct research relatedto gssp and evaluation, develop and maintain an incidenttrackingsystem, provide education and training services, broker and enhancecommunications between commercial and national security interests,and participate in international standardization and harmonizationefforts for commercial security practice. in doing these things it wouldhave to coordinate its activities with agencies and other organizationssignificantly involved in computer security. the isf would need thehighest level of governmental support; the strongest expression of suchsupport would be a congressional charter.although the system security study committee focused on computer andcommunications security, its recommendations would also support efforts toenhance other aspects of systems such as reliability and safety. it does not makesense to address these problems separately. many of the methods andtechniques that make systems more secure make them more trustworthy ingeneral. the committee has framed several of its recommendations so as torecognize the more general objective of making systems more strustworthy,and specifically to accommodate safety as well as security. the committeebelieves it is time to consider all of these issues together, to benefit fromeconomies in developing multipurpose safeguards, and to minimize any tradeoffs.with this report, the committee underscores the need to launch now aprocess that will unfold over a period of years, and that, by limiting theincidence and impact of disruptions, will help society to make the most ofcomputer and communications systems.executive summary6computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.1overview and recommendationswe are at risk. increasingly, america depends on computers. they controlpower delivery, communications, aviation, and financial services. they are usedto store vital information, from medical records to business plans to criminalrecords. although we trust them, they are vulnerablešto the effects of poordesign and insufficient quality control, to accident, and perhaps mostalarmingly, to deliberate attack. the modern thief can steal more with acomputer than with a gun. tomorrow's terrorist may be able to do more damagewith a keyboard than with a bomb.to date, we have been remarkably lucky. yes, there has been theft ofmoney and information, although how much has been stolen is impossible toknow.1 yes, lives have been lost because of computer errors. yes, computerfailures have disrupted communication and financial systems. but, as far as wecan tell, there has been no successful systematic attempt to subvert any of ourcritical computing systems. unfortunately, there is reason to believe that ourluck will soon run out. thus far we have relied on the absence of maliciouspeople who are both capable and motivated. we can no longer do so. we mustinstead attempt to build computer systems that are secure and trustworthy.in this report, the committee considers the degree to which a computersystem and the information it holds can be protected and preserved. thisrequirement, which is referred to here as computer security, is a broad concept;security can be compromised by bad system design, imperfect implementation,weak administration of procedures, or through accidents, which can facilitateattacks. of course, if we are to trust our systems, they must survive accidents asoverview and recommendations7computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.well as attack. security supports overall trustworthiness, and vice versa.computer system security concernssecurity is a concern of organizations with assets that are controlled bycomputer systems. by accessing or altering data, an attacker can steal tangibleassets or lead an organization to take actions it would not otherwise take. bymerely examining data, an attacker can gain a competitive advantage, withoutthe owner of the data being any the wiser.computer security is also a concern of individuals, including many whoneither use nor possess computer systems (box 1.1). if data can be accessedimproperly, or if systems lack adequate safeguards, harm may come not only tothe owner of the data, but also to those to whom the data refers. the volume andnature of computerized databases mean that most of us run the risk of havingour privacy violated in serious ways. this is particularly worrisome, since thosein a position to protect our privacy may have little incentive to do so (turn,1990).the threats to u.s. computer systems are international, and sometimes alsopolitical. the international nature of military and intelligence threats has alwaysbeen recognized and addressed by the u.s. government. but a broaderinternational threat to u.s. information resources is emerging with theproliferation of international computer networkingšinvolving systems forresearchers, companies, and other organizations and individualsšand a shiftfrom conventional military conflict to economic competition.2 theconcentration of information and economic activity in computer systems makesthose systems an attractive target to hostile entities. this prospect raisesquestions about the intersection of economic and national security interests andthe design of appropriate security strategies for the public and private sectors.finally, politically motivated attacks may also target a new class of system thatis neither commercial nor military: computerized voting systems.3outside of the government, attention to computer and communicationssecurity has been episodic and fragmented. it has grown by spurts in response tohighly publicized events, such as the politically motivated attacks on computercenters in the 1960s and 1970s and the more recent rash of computer virusesand penetrations of networked computer systems.4 commercial organizationshave typically concentrated on abuses by individuals authorized to use theirsystems, which typically have a security level that prevents only the moststraightforward of attacks.overview and recommendations8computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.box 1.1 sampler of computer system problems:evidence of inadequate trustworthinessfailures of system reliability, safety, or security are increasingly seriousšand apparently increasing in number. notable are the following: a $259 million volkswagen currency exchange scam involving phonytransactions; the nearly successful attempt to use thousands of phony bank ofamerica automatic teller machine cards fabricated with personalidentification numbers pirated from an online database; an almostsuccessful $15.2 million pennsylvania lottery fraud attempt inwhich the database of unclaimed ticket numbers was used in thefabrication of a ticket about to expire; and thousands of reported virus attacks and hundreds of different virusesidentified (e.g., stoned, devil's dance, 1260, jerusalem, yankeedoodle, pakistani brain, icelandic2, ping pong, december 24, to citejust a few).penetrations and disruptions of communication systems appear to beincreasing: design error freezing much of at&t's longdistance network; the german chaos computer club breakins to the nationalaeronautics and space administration's space physics analysis network; the west german wily hacker attacks (involving internationalespionage) on lawrence berkeley laboratory; the internet worm incident in which several thousand computers werepenetrated; and takeovers of tv satellite uplinks.individual privacy has been compromised. for example, deficientsecurity measures at major credit agencies have allowed browsing andsurreptitious assignment of thousands of individuals' credit histories to others.health care has been jeopardized by inadequate system quality as wellas by breaches of security: an error in the computer software controlling a radiation therapymachine, a therac 25 linear accelerator, resulted in at least threeseparate patient deaths when doses were administered that were morethan 100 times the typical treatment dose. a michigan hospital reported that its patient information had beenscrambled or altered by a virus that came with a vendor's image displaysystem. a cleveland man allegedly mailed over 26,000 virusinfected disketteswith aids prevention information to hospitals, businesses, andgovernment agencies worldwide.note: none of the cases cited above involved any classified data.references to all of them can be found in neumann (1989).overview and recommendations9computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.while weak computer security obviously affects direct and indirect usersof computer systems, it may have less obvious but still important impacts onvendors of computer systems. the role of security and trust in productdevelopment and marketing should grow, and not only because it is in thepublic interest. in particular, failure to supply appropriate security may putvendors at a serious competitive disadvantage. even though u.s. firms leadoverall in the computer and communications market, several europeangovernments are now promoting product evaluation schemes and standards thatintegrate other elements of trust, notably safety, with security. thesedevelopments may make it difficult for american industry to sell products inthe european market.5although the committee focuses on technical, commercial, and relatedsocial concerns, it recognizes that there are a number of related legal issues,notably those associated with the investigation and prosecution of computercrimes, that are outside of its scope. it is important to balance technical andnontechnical approaches to enhancing system security and trust. accordingly,the committee is concerned that the development of legislation and case law isbeing outpaced by the growth of technology and changes in our society. inparticular, although law can be used to encourage good practice, it is difficult tomatch law to the circumstances of computer system use. nevertheless, attackson computer and communication systems are coming to be seen as punishableand often criminal acts (hollinger and lanzakaduce, 1988) within countries,and there is a movement toward international coordination of investigation andprosecution. however, there is by no means a consensus about what uses ofcomputers are legitimate and socially acceptable. free speech questions havebeen raised in connection with recent criminal investigations into disseminationof certain computerrelated information.6 there are also controversiessurrounding the privacy impacts of new and proposed computer systems,including some proposed security safeguards. disagreement on thesefundamental questions exists not only within society at large but also within thecommunity of computer specialists.7trendsthe growing potential for system abuseoverall, emerging trends, combined with the spread of relevant expertiseand access within the country and throughout the world, point to growth in boththe level and the sophistication of threats to major u.s. computer andcommunications systems. there is reason to believe that we are at adiscontinuity: with respect to computeroverview and recommendations10computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.security, the past is not a good predictor of the future. several trends underliethis assessment: networking and embedded systems are proliferating, radically changingthe installed base of computer systems and system applications.8 computers have become such an integral part of american business thatcomputerrelated risks cannot be separated from general business risks. the widespread use of databases containing information of a highlypersonal nature, for example, medical and credit records, leaves theprivacy of individuals at risk. the increased trust placed in computers used in safetycritical applications(e.g., medical instruments) increases the likelihood that accidents orattacks on computer systems can cost people their lives. the ability to use and abuse computer systems is becoming widespread.in many instances (e.g., design of computer viruses, penetration ofcommunications systems, credit card system fraud) attacks are becomingmore sophisticated. the international political environment is unstable, raising questionsabout the potential for transnational attacks at a time when internationalcorporate, research, and other computer networks are growing.the need to responduse of computer systems in circumstances in which we must trust them iswidespread and growing. but the trends identified above suggest that whatevertrust was justified in the past will not be justified in the future unless action istaken now. (box 1.2 illustrates how changing circumstances can profoundlyalter the effective trustworthiness of a system designed with a given set ofexpectations about the world.) computer system security and trustworthinessmust become higher priorities for system developers and vendors, systemadministrators, general management, system users, educators, government, andthe public at large.this observation that we are at a discontinuity is key to understanding thefocus and tone of this report. in a time of slow change, prudent practice maysuggest that it is reasonable to wait for explicit evidence of a threat beforedeveloping a response. such thinking is widespread in the commercialcommunity, where it is hard to justify expenditures based on speculation.however, in this period of rapid change, significant damage can occur if onewaits to develop a countermeasure until after an attack is manifest. on the onehand, it mayoverview and recommendations11computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.box 1.2 personal computers: securitydeteriorates with circumstancespersonal computers (pcs), such as the popular ibm pc running the ms/dos operating system, or those compatible with it, illustrate that what wasonce secure may no longer be. security was not a major consideration fordevelopers and users of early pcs. data was stored on floppy disks thatcould be locked up if necessary, and information stored in volatile memorydisappeared once the machine was turned off. thus the operating systemcontained no features to ensure the protection of data stored in thecomputer. however, the introduction of hard disks, which can store largeamounts of potentially sensitive information in the computer, introduced newvulnerabilities. since the hard disk, unlike the floppy disk, cannot be removedfrom the computer to protect it, whoever turns on the pc can have access tothe data and programs stored on the hard disk. this increased risk can stillbe countered by locking up the entire machine. however, while the machineis running, all the programs and data are subject to corruption from amalfunctioning program, while a dismounted floppy is physically isolated.the most damaging change in the operating assumptions underlying thepc was the advent of network attachment. external connection via networkshas created the potential for broader access to a machine and the data itstores. so long as the machine is turned on, the network connection can beexercised by a remote attacker to penetrate the machine. unfortunately, ms/dos does not contain security features that, for example, can protect againstunwanted access to or modification of data stored on pcs.a particularly dangerous example of compromised pc security arisesfrom the use of telecommunication packages that support connecting fromthe pc to other systems. as a convenience to users, some of thesepackages offer to record and remember the user's password for othersystems. this means that any user penetrating the pc gains access not onlyto the pc itself but also to all the systems for which the user has stored hispassword. the problem is compounded by the common practice of attachinga modem to the pc and leaving it turned on at night to permit the user to dialup to the pc from home: since the pc has no access control (unless thesoftware supporting the modem provides the service), any attacker guessingthe telephone number can attach to the system and steal all the passwords.storing passwords to secure machines on a machine with no securitymight seem the height of folly. however, major software packages for pcsinvite the user to do just that, a clear example of how vendors and usersignore security in their search for ease of use.overview and recommendations12computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.take years to deploy a countermeasure that requires a major change to abasic system. thus, for example, the current concern about virus attacks derivesnot from the intrinsic difficulty of resisting the attacks, but from the total lack ofa countermeasure in such popular systems as ms/dos and the applemacintosh operating system. it will take years to upgrade these environments toprovide a technical means to resist virus attacks. had such attacks beenanticipated, the means to resist them could have been intrinsic to the systems.on the other hand, the threats are changing qualitatively; they are more likely tobe catastrophic in impact than the more ordinary threat familiar to securityofficers and managers. this report focuses on the newer breed of threat tosystem trustworthiness.the committee concludes, for the various reasons outlined above anddeveloped in this report, that we cannot wait to see what attackers may devise,or what accident may happen, before we start our defense. we must develop alongterm plan, based on our predictions of the future, and start now to developsystems that will provide adequate security and trustworthiness over the nextdecade.toward a planned approachtaking a coherent approach to the problem of achieving improved systemsecurity requires understanding the complexity of the problem and a number ofinterrelated considerations, balancing the sometimes conflicting needs forsecurity and secrecy, building on groundwork already laid, and formulatingand implementing a new plan for action.achieving understandingthe nature of security: vulnerability, threat, and countermeasurethe field of security has its own language and mode of thought, whichfocus on the processes of attack and on preventing, detecting, and recoveringfrom attacks. in practice, similar thinking is accorded to the possibility ofaccidents that, like attacks, could result in disclosure, modification, ordestruction of information or systems or a delay in system use. security istraditionally discussed in terms of vulnerabilities, threats, and countermeasures.a vulnerability is an aspect of some system that leaves it open to attack. athreat is a hostile party with the potential to exploit that vulnerability and causedamage. a countermeasure or safeguard is an added step or improved designthat eliminates the vulnerability and renders the threat impotent.overview and recommendations13computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.a safe containing valuables, for example, may have a noisy combinationlockša vulnerabilityšwhose clicking can be recorded and analyzed to recoverthe combination. it is surmised that safecrackers can make contact with expertsin illegal eavesdroppingša threat. a policy is therefore instituted thatrecordings of random clicking must be played at loud volume when the safe isopenedša countermeasure.threats and countermeasures interact in intricate and often counterintuitiveways: a threat leads to a countermeasure, and the countermeasure spawns a newthreat. few countermeasures are so effective that they actually eliminate athreat. new means of attack are devised (e.g., computerized signal processing toseparate ''live" clicks from recorded ones), and the result is a more sophisticatedthreat.the interaction of threat and countermeasure poses distinctive problemsfor security specialists: the attacker must find but one of possibly multiplevulnerabilities in order to succeed; the security specialist must developcountermeasures for all. the advantage is therefore heavily to the attacker untilvery late in the mutual evolution of threat and countermeasure.9if one waits until a threat is manifest through a successful attack, thensignificant damage can be done before an effective countermeasure can bedeveloped and deployed. therefore countermeasure engineering must be basedon speculation. effort may be expended in countering attacks that are neverattempted.10 the need to speculate and to budget resources for countermeasuresalso implies a need to understand what it is that should be protected, and why;such understanding should drive the choice of a protection strategy andcountermeasures. this thinking should be captured in security policiesgenerated by management; poor security often reflects both weak policy andinadequate forethought.11security specialists almost uniformly try to keep the details ofcountermeasures secret, thus increasing the effort an attacker must expend andthe chances that an attack will be detected before it can succeed. discussion ofcountermeasures is further inhibited because a detailed explanation ofsophisticated features can be used to infer attacks against lesser systems.12 aslong as secrecy is considered important, the dissemination, without motivation,of guidelines developed by security experts will be a key instrument forenhancing secure system design, implementation, and operation. the need forsecrecy regarding countermeasures and threats also implies that society musttrust a group of people, security experts, for advice on how to maintain security.overview and recommendations14computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.confidence in countermeasures is generally achieved by submitting themfor evaluation by an independent team; this process increases the lead times andcosts of producing secure systems. the existence of a successful attack can bedemonstrated by an experiment, but the adequacy of a set of countermeasurescannot. security specialists must resort to analysis, yet mathematical proofs inthe face of constantly changing systems are impossible.in practice, the effectiveness of a countermeasure often depends on how itis used; the best safe in the world is worthless if no one remembers to close thedoor. the possibility of legitimate users being hoodwinked into doing what anattacker cannot do for himself cautions against placing too much faith in purelytechnological countermeasures.the evolution of countermeasures is a dynamic process. security requiresongoing attention and planning, because yesterday's safeguards may not beeffective tomorrow, or even today.special security concerns associated with computerscomputerization presents several special security challenges that stemfrom the nature of the technology, including the programmability of computers,interconnection of systems, and the use of computers as parts of complexsystems. a computing system may be under attack (e.g., for theft of data) for anindefinite length of time without any noticeable effects, attacks may bedisguised or may be executed without clear traces being left, or attacks may berelated to seemingly benign events. thus "no danger signals" does not meanthat everything is in order.13 a further complication is the need to balancesecurity against other interests, such as impacts on individual privacy. forexample, automated detection of intrusion into a system, and other safeguards,can make available to system administrators significant information about thebehavior of individual system users.to some extent, those attributes of computing that introduce vulnerabilitiescan also be used to implement countermeasures. a computer system (unlike afile cabinet) can take active measures in its defense, by monitoring its activityand determining which user and program actions should be permitted(anderson, 1980). unfortunately, as discussed later in this report, this potentialis far from realized.programmability the power of a generalpurpose computer lies in itsability to become an infinity of different machines through programming.14 thisis also a source of great vulnerability, because if a system can be programmed,it can be programmed to do bad things.overview and recommendations15computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.thus by altering program text a computer virus can transform a familiarand friendly machine into something else entirely (cohen, 1984).the vulnerability introduced by programmability is compounded by thedegree to which the operation of a computer is hidden from its user. whereas anindividual concerned about security can inspect a mechanical typewriter andsafely conclude that the effects of pressing a key are the appearance of a letteron the paper and the imprint of a letter on the ribbon, he can gain no suchconfidence about the operation of a word processor. it is clear that the pressingof a word processor's key causes the appearance of a letter on the screen. it is inno sense clear what else is happeningšwhether, for instance, the letters arebeing saved for subsequent transmission or the internal clock is beingmonitored for a "trigger date" for the alteration or destruction of files.embeddedness and interconnection the potential for taking improperirreversible actions increases with the degree to which computers are embeddedin processes.15 the absence of human participation removes checks for thereasonableness of an action. and the time scale of automatic decisions may betoo short to allow intervention before damage is done.interconnection enables attacks to be mounted remotely, anonymously, andagainst multiple vulnerabilities concurrently, creating the possibility ofoverwhelming impacts if the attacks are successful. this risk may not beunderstood by managers and system users. if a particular node on a massive,heterogeneous network does not contain any sensitive information, its ownersmay not be motivated to install any countermeasures. yet such "wideopen"nodes can be used to launch attacks on the network as a whole, and little can bedone in response, aside from disconnecting. the "wily hacker," for example,laundered his calls to defenserelated installations through various universitycomputers, none of which suffered any perceptible loss from his activities. theinternet worm of november 1988 also showed how networking externalizesrisk. many of the more than 2,000 affected nodes were entered easily once a"neighbor" node had been entered, usually through the electronic equivalent ofan unlocked door.in many cases, communication and interconnection have passed wellbeyond the simple exchange of messages to the creation of controlledopportunities for outsiders to access an organization's systems to facilitate eitherorganization's business. online access by major telephone customers totelephone system management data and by large businesses to bank systems fortreasury managementoverview and recommendations16computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.functions are two examples of this phenomenon. a related development iselectronic data interchange (edi), in which companies have computercommunications links with suppliers and customers to automate ordering,queries about the status of orders, inventory management, market research, andeven electronic funds transfer (eft). edi and eft may add an additionalsystem layer or interconnection where systems are mediated by thirdpartysuppliers that collect, store, and forward messages between various parties invarious organizations. this situation illustrates the need for trustworthiness incommon carriage. in short, a wide range of organizations are connected to eachother through computer systems, sometimes without knowing they areinterconnected.interconnection gives an almost ecological flavor to security; it createsdependencies that can harm as well as benefit the community of those who areinterconnected. an analogy can be made to pollution: the pollution generated asa byproduct of legitimate activity causes damage external to the polluter. arecognized public interest in eliminating the damage may compel theinstallation of pollution control equipment for the benefit of the community,although the installation may not be justified by the narrow selfinterest of thepolluter. just as average citizens have only a limited technical understanding oftheir vulnerability to pollution, so also individuals and organizations today havelittle understanding of the extent to which their computer systems are put at riskby those systems to which they are connected, or vice versa. the public interestin the safety of networks may require some assurances about the quality ofsecurity as a prerequisite for some kinds of network connection.security must be holisticštechnology, management, and social elementscomputer security does not stop or start at the computer. it is not a singlefeature, like memory size, nor can it be guaranteed by a single feature or even aset of features. it comprises at a minimum computer hardware, software,networks, and other equipment to which the computers are connected, facilitiesin which the computer is housed, and persons who use or otherwise come intocontact with the computer. serious security exposures may result from anyweak technical or human link in the entire complex. for this reason, security isonly partly a technical problem: it has significant procedural, administrative,physical facility, and personnel components as well. the general accountingoffice's recent criticisms of financial computer systems, for example,highlighted the risks associated with poor physicaloverview and recommendations17computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.and administrative security (gao, 1990a), which sets the stage for evenamateur attacks on critical systems.box 1.3 security vs. reliability: a telephonebilling system as an exampleconsider, for example, a telephone billing system that computes theduration of a call by recording the time but not the date at the start and end ofa call. the system cannot bill calls over 24 hours. thus a call of 24 hours and3 minutes would be billed for 3 minutes. in the normal course of events, suchcalls are very rare, and in the absence of an active threat it is possible tovisualize an analysis whose conclusion is that the error is not worth fixing.that is, the revenue lost from that tiny number of calls that "naturally" lastmore than 24 hours would not cover the cost of making the fix. but thediscovery of this error by an active threat (e.g., bookies) turns it immediatelyinto a vulnerability that will be exploited actively and persistently until it isfixed. the tolerance for error is therefore very much less when one considers"security" than it is when one is simply concerned with "reliability."paralleling concerns about security are concerns about system safety andthe need for assurance that a system will not jeopardize life or limb. steps thatenhance computer security will enhance safety, and vice versa.16 mechanismsused to achieve security are often similar to those used to achieve safety,reliability, and predictability. for example, contingency planning (which mayinvolve system backup activities and alternative equipment and facilities) canprotect an organization from the disruption associated with fires and othernatural disasters, and it can help an organization to recover from a securitybreach.nevertheless, the environment in which those mechanisms operate differswhen the principal concern is security. in particular, traditional risk analysisrelies on statistical models that assume that unlikely events remain unlikelyafter they have occurred once. security analyses cannot include suchassumptions (see box 1.3). security is also distinguished from safety in that itinvolves protection against a conscious action rather than random unfortunatecircumstances.17commercial and military needs are differentthere has been much debate about the difference between military andcommercial needs in the security area. some analyses (ota, 1987b) havecharacterized socalled military security policies (i.e., thoseoverview and recommendations18computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.concerned with national security or classified data) as being largely orexclusively concerned with secrecy, and commercial security policies (i.e.,those of interest to the private sector) as being concerned with the integrity orreliability of data. this distinction is both superficial and misleading. nationalsecurity activities, such as military operations, rely heavily on the integrity ofdata in such contexts as intelligence reports, targeting information, andcommand and control systems, as well as in more mundane applications such aspayroll systems. private sector organizations are concerned about protecting theconfidentiality of merger and divestiture plans, personnel data, trade secrets,sales and marketing data and plans, and so on. thus there are many commonneeds in the defense and civilian worlds.commonalities are especially strong when one compares the military towhat could be called infrastructural industriesšbanking, the telephone system,power generation and distribution, airline scheduling and maintenance, andsecurities and commodities exchanges. such industries both rely on computersand have strong security programs because of the linkage between security andreliability. nonsecure systems are also potentially unreliable systems, andunreliability is anathema to infrastructure.nevertheless, specific military concerns affect the tack taken to achievesecurity in military contexts. thus far, system attacks mounted by nationalintelligence organizations have been qualitatively different from attacksmounted by others (see appendix e). this qualitative difference has led to basicdifferences in system design methodology, system vulnerability assessment,requirements for secrecy vs. openness in system design, and so on.other differences stem from the consequences of a successful attack.national security countermeasures stress prevention of attack, and onlysecondarily investigation and pursuit of the attackers, since the concept ofcompensatory or punitive damages is rarely meaningful in a national securitycontext. private sector countermeasures, however, are frequently orientedtoward detectionšdeveloping audit trails and other chains of evidence that canbe used to pursue attackers in the courts.a final set of differences stem from variations in the ability to control whohas access to computer systems. threats can come from outsiders, individualswho have little or no legitimate access to the systems they are attacking, or frominsiders, individuals who abuse their right to legitimate access. embezzlementand theft of trade secrets by employees are familiar insider threats. effectiveattacks often combine the two forms: a determined and competent group ofoutsiders aided by a subverted insider (early, 1988).overview and recommendations19computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.the national security community conducts extensive background checkson individuals before it grants access to systems or information. itscountermeasures, therefore, tend to emphasize attacks by outsiders.nonetheless, recognition of its own insider threats has led to an increasedemphasis on accountability, auditing, and other measures to follow up onimproper as well as accidental incidents. the private sector, by contrast, islimited by privacy and civil rights legislation in its ability to deny employmentto individuals based on indepth background investigations. this situation,together with the fact that most commercial applications are wide open tosimple physical attacks and also have lacked external system connections,contributes to the private sector's historic emphasis on the threats posed byinsiders (employees). of course, the increasing interconnection andglobalization of business, research, and other activities should raise the level ofconcern felt by all segments of the economy about outside threats.the security needs of both commercial and defense sectors are matters ofpublic interest. partly because understanding of security is uneven, thecomputer and communications market has moved slowly and unevenly. likeother complex and sophisticated products, computer software and systems aredifficult for the average consumer to understand and evaluate. this situation hasdepressed potential demand for security, and it has resulted in public andprivate efforts to stimulate and guide the market that, while well intended, fallshort of what is needed. this is one area where it is generally agreed that someform of institutional support is not only desirable but also most valuable.putting the need for secrecy into perspectivethere is a tension between the need for prudent limits on the disseminationof information on vulnerabilities and the need to inform those at risk of specificsecurity problems. the secrecy imperative has historically dominated thecommunications security field. cryptology (the science of making and breakingcodes), for instance, is one of two sciences (the other being atomic energy) thatis given special status under federal statute (kahn, 1967). secrecy has also beenselfimposed; government investigators, prosecutors, and insurancerepresentatives have noted the reluctance of companies that have experiencedcomputer system attacks to report their experiences.concern for secrecy affects the way computer systems are built and used.open discussion of the design of a system offers the benefit of collegial review(see chapter 4) but also involves the risk that attackers may be immediatelyinformed of vulnerabilities. evaluationoverview and recommendations20computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.and analysis may also yield a list of residual vulnerabilities that cannot becountered for technical or economic reasons, and these become the mostimportant secrets associated with the system. the more complex the system, themore difficult the tradeoff becomes because of the increased likelihood thatthose close to the system will overlook something. general education in theproper use of countermeasures leads to a betterinformed user community, but italso leads to a betterinformed community of potential attackers. publicizingspecific vulnerabilities will lead some users to correct them, but will alsoprovide a cookbook for attacking sites that do not hear about or are notmotivated to install the countermeasure.concern for secrecy also impedes technological progress in the securityarea. it has deterred research in the academic community, which places apremium on open discussion and publication. it increases the difficulties facedby people new to the field, who cannot readily find out what has been done andwhat the real problems are; there is much reinventing of wheels. finally,concern for secrecy makes it hard for the few who are well informed to seek thecounsel and collaboration of others.perhaps the most damaging aspect of the secrecy associated with computerand communications security is that it has led many to assume that no problemsexist. "tomorrow will be pretty much like today," is the rationale that guidesmost government, corporate, and individual activities. however, with respect tocomputer security, secrecy makes it extremely hard to know what today isreally like.building on existing foundationsa number of government agencies have addressed portions of thecomputer system security problem, either by developing relevant technology orapplying relevant tools and practices (see box 1.4). two government agencies,the national security agency (nsa)šmost recently through one of its arms,the national computer security center (ncsc)šand the national institute ofstandards and technology (nist; formerly the national bureau of standards)have been particularly active for some 20 years, but neither is positioned toadequately address the nation's needs.the national security agency has been the more active of the twoorganizations. the establishment of the ncsc represented an effort to stimulatethe commercial marketplace. through the ncsc and the publication of thetrusted computer system evaluation criteria, or orange book (u.s. dod,1985d), which outlines different levels of computer security and a process forevaluating the security of computeroverview and recommendations21computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.systems (see appendix a), the nsa has had a noticeable effect (box 1.5).because of its defenseoriented charter, the nsa cannot, however, moreactively foster development or widespread dissemination of technology for usein the nonclassified or commercial world. indeed, its defenserelated focusšspecifically, a focus on systems that process classified informationšhas beennarrowed in recent years.box 1.4 recent major computer securityinitiatives undertaken by the u.s. government establishment of the national computer security center the orange book, trusted network interpretation, related publications,and the trusted products evaluation program national security decision directive 145; revised and recast as nsd 42 the computer fraud and abuse act of 1986 the computer security act of 1987 national telecommunications and information system security policy200šc2 by '92 the secure data network system project nist's integrity workshop program darpa's computer emergency response team programthe national institute of standards and technology's impact on computersecurity has been concentrated within the federal government. nist has limitedtechnical expertise and funds; in fy 1990 its appropriations for the computersecurity program totaled only $2.5 million. although it can organizeworkshops, develop procedural guidelines, and sanction standards efforts, it isnot in a position to develop technology internally or to provide direct support toexternal technology development efforts. the newest (fy 1991) nist budgetrequest called for a doubling of funds to support activities related to computersecurity, and nist has made plans to undertake some initiatives (e.g., anindustryoriented program to combat computer viruses). however, the denial ofnist's fy 1990 request for modest additional funds in this area is symptomaticof the lack of stability and predictability of the political process for governmentfunding in general and funding for nist in particular.18tension between commercial and military interests dominated publicpolicymaking relating to computer security during the 1980s. national securitydecision directive (nsdd) 145, the computer security act of 1987, and themid1990 revision of nsdd 145 (resulting in nsd 42) have progressivelyrestricted nsa to an emphasis on defense systems, leaving civilian (notablycivil government) system securityoverview and recommendations22computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.box 1.5 the rainbow seriessince its formation in 1981, the national computer security center hasdisseminated a collection of criteria and guidelines to assist developers,evaluators, and users in the development of trusted systems. this set ofdocuments has become known as the rainbow series because of thedifferent colors used for each volume's cover. of these documents, perhapsthe most widely known is the socalled orange book, which is formallyknown as the department of defense trusted computer system evaluationcriteria. the following are brief descriptions of some of the documents thatform the rainbow series:trusted computer system evaluation criteria (tcsec) (orange)the tcsec defines criteria for evaluating the security functionality andassurance provided by a computer system. the tcsec formalizes theconcept of a trusted computing base (tcb) and specifies how it should beconstructed and used in order to ensure a desired level of trust.trusted network interpretation (tni) (red)the tni interprets the tcsec with regard to networked computersystems. the tni has been particularly controversial due to the complexsecurity issues that arise when computer networks are used. it has beenundergoing revision.trusted database management system interpretation (tdi) (forthcoming)the tdi interprets the tcsec with regard to database managementsystems. the tdi is expected to be released in late 1990 or early 1991.password management guideline (light green)this document describes a set of good practices for using passwordbased authorization schemes. a similar set of guidelines has also beenissued by the national institute of standards and technology as a federalinformation processing standards publication.glossary of computer security terms (dark green)this document defines the acronyms and terms used by computersecurity specialists, focusing on dod contexts.magnetic remanence security guidelines (dark blue)this document provides procedures and guidance for sanitizingmagnetic storage media (e.g., disks and tapes) prior to their release tononsecure environments.guidance for applying the department of defense trusted computersystem evaluation criteria in specific environments (yellow)this volume provides guidance for applying the tcsec to specificenvironments.overview and recommendations23computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.concerns to nist. partly as a result of the changing policy context, nsahas moved to diminish its interaction with commercial organizations, mostnotably by scaling back the ncsc. the full implications of these moves are yetto be appreciated at the time this report is being completed.meanwhile, no industrybased organization or professional association hasstepped forward to play a leadership role in increasing computer systemsecurity, although the 1980s saw the birth or strengthening of a number ofvolunteer professional associations, and over the past couple of years majorcomputerrelated trade associations (e.g., the computer and businessequipment manufacturers association (cbema) and the computer softwareand services industry association adapso) have begun to explore steps theycan take to better track security problems, notably virus incidents, and toencourage better systems development. however valuable, these efforts arepiecemeal.common technical interests, complementary objectives, and significantdifferences in resources combine to make the existing separate activities aimedat increasing computer security in commercial and military environments anincomplete solution to the problem of increasing the overall level of systemsecurity and trust. a more complete solution calls for the formulation andimplementation of a new, more comprehensive plan that would inject greaterresources into meeting commercial computer security needs./div>scope, purpose, contents, and audiencethis report provides an agenda for public policy, computer andcommunications security research, technology development, evaluation, andimplementation. it focuses on the broad base of deployed computers in theunited states; it does not emphasize the special problems of governmentclassified information systems. this committee is particularly concerned aboutraising the security floor, making sure that the commercial environment onwhich the economy and public safety depend has a better minimum level ofprotection.a number of actions are needed to increase the availability of computerand communications systems with improved security, including: a clear articulation of essential security features, assurances, and practices; enhanced institutional support and coordination for security; and research and development of trustworthy computerbased technology.overview and recommendations24computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.this the appropriate time to develop a new strategy that blends research,establishment of requirements and criteria, and commercial incentives. thecommittee's recommendations in each of the above areas are presented below inthe ''recommendations" section of this chapter. these include recommendationsfor both short and longterm actions.this report is intended to address a variety of audiences, includinggovernment policymakers, vendors, managers responsible for the purchase anduse of computer and communications systems, people involved in computerrelated research and development, educators, and interested members of thegeneral public. the chapters and appendixes that follow provide technical andanalytical detail to further support the assertions, conclusions, andrecommendations presented in this first chapter. chapter 2 describes basic concepts of information security, includingsecurity policies and management controls. chapter 3 describes technology associated with computer andcommunications security, relating technical approaches to securitypolicies and management controls. chapter 4 discusses methodological issues related to building securesoftware systems. chapter 5 discusses system evaluation criteria, which provide yardsticksfor evaluating the quality of systems. this topic is a current focus of muchinternational concern and activity. chapter 6 discusses why the marketplace has failed to substantiallyincrease the supply of security technology and discusses options forstimulating the market. chapter 7 discusses the need for a new institution, referred to as theinformation security foundation. chapter 8 outlines problems and opportunities in the research communityand suggests topics for research and mechanisms for strengthening theresearch infrastructure. appendixes provide further detail on the orange book (a), technology(b), emergency response teams (c), models for proposed guidelines (d),highgrade threats (e), and terminology (f).the nature of the subject of security dictates some limits on the content ofthis report. of necessity, this report anticipates threats in order to guide thedevelopment of effective security policy; it therefore inherently contains adegree of surmise. it leaves things unsaid so as not to act as a textbook forattackers, and therefore it may fail to inform or inspire some whose informationis at risk. and finally, it may carry within it the seeds of its own failure, as thecountermeasuresoverview and recommendations25computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.it may inspire may also lead to new and more effective threats. such is thenature of security.recommendationsthe central concern of this report is how to get more and better computerand communications security into use. five of the committee's sixrecommendations endorse actions with medium to longrange impacts.another, recommendation 2, outlines shortterm actions aimed at immediatelyimproving the security of computing systems. it is clear that system operators,users, and managers need to take effective steps now to upgrade and stabilizetheir operating environments; developers and vendors are likewise urged to useexisting capabilities for immediate enhancement of computer security. also ofconcern are a number of currently unfolding political developments (e.g.,development of harmonized international criteria for trusted system design andevaluation) that call for immediate attention from both public policymakers andvendors in particular. the committee has addressed such developments withinthe body of the report as appropriate.although the committee focused on system security, its recommendationsalso serve other aspects of system trustworthiness, in particular safety andreliability. it does not make sense to address these issues separately. many ofthe methods and techniques that make systems more secure make them moretrustworthy in general. system safety is tied to security, both in method and inobjective. the penetration of computing into the social and economic fabricmeans that, increasingly, what we may want to protect or secure is public safety.increasing the trustworthiness of computer systems requires actions onmany frontsšdeveloping technology and products, strengthening managerialcontrols and response programs, and enhancing public awareness. toward thatend, the committee recommends six sets of actions, summarized as follows:1. promulgating a comprehensive set of generally accepted systemsecurity principles, referred to as gssp (see also chapter 2);2. taking specific shortterm actions that build on readily availablecapabilities (see also chapter 6);3. establishing a comprehensive incident data repository and appropriateeducation programs to promote public awareness (see also chapters 4and 6);4. clarifying export control criteria and procedures (see also chapter 6);overview and recommendations26computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.5. securing funding for a comprehensive, directed program of research(see also chapters 3, 4, and 8); and6. establishing a new organization to nurture the development,commercialization, and proper use of trust technology, referred to asthe information security foundation, or isf (see also chapters 5, 6,and 7).recommendation 1 promulgate comprehensive generallyaccepted system security principles (gssp)1a. establish a set of generally accepted system security principles, or gssp, for computer systems. because of widely varying understandingabout vulnerabilities, threats, and safeguards, system vendors and users needguidance to develop and use trusted systems. it is neither desirable nor feasibleto make all who come into contact with computers experts in computer andcommunications security. it is, however, both desirable and feasible to achievea general expectation for a minimum level of protection. otherwise, responsesto security problems will continue to be fragmented and often ineffective.the committee believes it is possible to enunciate a basic set of securityrelated principles that are so broadly applicable and effective for the design anduse of systems that they ought to be a part of any system with significantoperational requirements. this set will grow with research and experience innew areas of concern, such as integrity and availability, and can also growbeyond the specifics of security to deal with other related aspects of systemtrust, such as safety. gssp should articulate and codify these principles.successful gssp would establish a set of expectations about andrequirements for good practice that would be well understood by systemdevelopers and security professionals, accepted by government, and recognizedby managers and the public as protecting organizational and individual interestsagainst security breaches and lapses in the protection of privacy. analogousbroad acceptance has been accorded to financial accounting standards (whathave been called the generally accepted accounting principles, or gaap) andbuilding codes,19 both of which contain principles defined with industry inputand used or recognized by government as well. to achieve a similar level ofconsensus, one that builds on but reaches beyond that accorded to the orangebook (see appendix a), the gssp development process should be endorsed byand accept input from all relevant communities, including commercial users,vendors, and interested agencies of the u.s. government. the development ofgssp wouldoverview and recommendations27computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.require a level of effort and community participation that is well beyond thescope either of this report or of organizations currently active in the securityarena. the committee therefore recommends that the process of establishinggssp be spearheaded by a new organization discussed below inrecommendation 6.box 1.6 potential elements of generallyaccepted system security principlesthe following set of examples is intended to illustrate the kinds ofprinciples and considerations that might be embodied in gssp. thecommittee emphasizes securityrelated issues but believes that gsspshould also stress safetyrelated practices. quality controlša system is safe and secure only to the extent that itcan be trusted to provide the functionality it is intended to supply. at aminimum, the best known industrial practice must be used for systemdevelopment, and some recognized means for potential purchasers orusers to obtain independent evaluation must be provided. a strongerrequirement would specify that every procedure in the software beaccompanied by text specifying its potential impact on safety andsecurity and arguing that those specifications imply the desiredproperties.* chapter 5 discusses specific proposals for evaluation ofsystems relative to gssp. access control on code as well as dataševery system must have themeans to control which users can perform operations on which pieces ofdata, and which particular operations are possible. a minimummechanism has a fixed set of operations (for example read, write, andexecute) and may only associate permission with static groups of users,but stronger means, such as the ability to list particular users, arerecommended. user identification and authenticationševery system must assign anunambiguous identifier to each separate user and must have the meansto assure that any user is properly associated with the correct identifier.a minimum mechanism for this function is passwords, but strongermeans, such as challengeresponse identity checks, are recommended. protection of executable codeševery system must have the means toensure that programs cannot be modified or replaced improperly.mechanisms stronger than customary access control are recommended,such as a basic system function to recognize certain programs as"installed" or "production" or "trusted,'' and to restrict the access tospecified data to only this class of program. security loggingševery system must have the means to log for lateraudit all securityrelevant operations on the system. at a minimum, thismust include all improper attempts to authenticate a user or to accessdata, all changes to the list of authorized users, and (if appropriate) allsuccessfulpresented in box 1.6 are some potential gssp elements that inoverview and recommendations28computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.fully developed gssp would be elaborated in greater detail. the committeeexpects that gssp would also cover matters of safety that fall outside the scopeof this report. securityrelated operations (user authentications, file opens, and so on).the log must be implemented in such a way that it cannot be altered ordeleted after being written. a stronger version would also prevent thesecurity administrator from deleting the log.security administratoršall systems must support the concept of aspecial class of users who are permitted to perform actions that changethe security state of the system, such as adding users or installingtrusted programs. they must control system code and data sources inappropriate offline facilities. they must employ standard procedures forsystem initialization, backup, and recovery from "crashes."data encryptionšwhile data encryption is not, in itself, an applicationlevel security requirement, it is currently recognized as the method ofchoice for protecting communication in distributed systems. any systemthat can be attached to a network must support some standard meansfor data encryption. a stronger version would forbid software encryption.operational support toolsševery system must provide tools to assistthe user and the security administrator in verifying the security state ofthe system. these include tools to inspect security logs effectively, toolsto provide a warning of unexpected system behavior, tools to inspect thesecurity state of the system, and tools to control, configure, and managethe offline data and code storage and hardware inventory.independent auditšat some reasonable and regular interval, anindependent unannounced audit of the online system, operation,administration, configuration control, and audit records should beinvoked by an agency unrelated to that responsible for the systemdesign and/or operations. such an audit should be analogous to anannual business audit by accounting firms.hazard analysisša hazard analysis must be done for every safetycritical system. this analysis must describe those states of the systemthat can lead to situations in which life is endangered and must estimatethe probability and severity of each under various conditions of usage. itshould also categorize the extent to which hazards are independent ofeach other.* note that the internet engineering advisory board has begun to contemplate "securityimpact statements" for proposed modifications to the large and complex internet.comprehensive gssp must reflect the needs of the widest possiblespectrum of computer users. although some groups with particularresponsibilities (e.g., in banking) might be tempted to reject gssp inoverview and recommendations29computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.favor of defining practices specific to their sectors, the committee believes thatthis would be unfortunate. baselevel security requirements of the sort outlinedabove are broadly applicable and ought to be defined in common (seechapter 2), so that the features required to support gssp can become a part ofgeneralpurpose computing. only as a part of mainstream computing productswill they become available at reasonable cost.in order to serve a wide range of users, gssp must allow variation withcircumstances. the committee concludes (see chapter 5) that gssp should beorganized in a somewhat more unbundled manner than is the orange book.the process of motivating the adoption of gssp could and probablyshould differ across sectors. for example, where computers are used to helpmanage assets, cooperation with the american institute of certifiedprofessional accountants or the financial accounting standards board mightlead to incorporation of gssp into the larger body of standard practice foraccounting. in systems used for health care, gssp might become a part of thefood and drug administration's regulations governing medical equipment.gssp could also be directly incorporated into government requests forproposals (rfps) and other procurement actions. during the development ofgssp it would be necessary to consider mechanisms and options for motivatingadoption of gssp.the committee expects natural forces, such as customers' expectations,requirements for purchasing insurance, vendors' concerns about liability,industry associations, and advertising advantage, to instill gssp in themarketplace. nevertheless it is possible to imagine that in some circumstances,such as for lifecritical systems, certain aspects of gssp might becomemandatory. serious consideration of regulation or other mechanisms forenforcement is both premature and beyond the scope of this report. however,the process implied by the committee's set of recommendations could forcesuch consideration in a few years. that process entails establishing a neworganization, developing gssp, and beginning the dissemination of gsspthrough voluntary means.1b. consider the system requirements specified by the orange book for the c2 and b1 levels as a shortterm definition of generally acceptedsystem security principles and a starting point for more extensivedefinitions. to date and by default, the principal vehicle in the united statesfor raising the level of practice in computer and communications security hasbeen the national computer security center's orange book and its variousinterpretations. althoughoverview and recommendations30computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.the orange book is not a full set of gssp (see appendix a), it is a major stepthat is currently molding the market and is clearly consonant with gssp.the c2 and b1 ratings describe systems that provide baseline levels ofacceptable discretionary security (c2) and systems that provide minimal levelsof acceptable mandatory multilevel security (b1).20 however, the orange bookis not adequate to meet the public's longterm needs, largely because it isincomplete. gssp would provide fuller treatment of integrity, availability, andadvanced techniques for assurance and software development.21 it must addressdistributed systems and evolving architectures (as well as change in theunderlying technologies generally), which means that it should go beyondtrusted computing bases as currently defined.1c. establish methods, guidelines and facilities for evaluating products for conformance to gssp. a mechanism for checking conformance to gsspis required for gssp to have its fullest impact and to protect both vendors andconsumers. as with technical standards, it is possible to claim conformance, butconformance must be genuine for benefits, such as interoperability, to berealized. conformance evaluation is already becoming a prominent issue acrossthe industry because of the proliferation of standards.22 evaluation of securityand safety properties is generally recognized as more difficult than evaluationof conformance to interoperability standards. therefore, methods for evaluatingconformance should be considered for each element of gssp.it will also be necessary both to train evaluators and to establish the extentand timing of independent evaluation. the details of the evaluation processaffect costs to vendors and users as well as the confidence of both in theperformance or quality of a system. in chapter 5 the committee recommendsthat the minimal gssp evaluation include two parts, an explicit designevaluation performed by an outside team, and a coordinated process of trackingfield experience with the product and tracking and reporting security faults.this process ought to be less costly and timeconsuming than the current ncscprocess, thus improving the chances of its widespread acceptance.experience with the current ncsc evaluation process suggests thatindividual products can be evaluated somewhat formally and objectively.however, a system composed of evaluated components may not provide thesecurity implied by component ratings. achieving overall system securityrequires more objective, uniform, and rigorous standards for systemcertification. the committee recommendsoverview and recommendations31computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.that gssp include guidelines for system certification, again building on existingmethodology.1d. use gssp as a basis for resolving differences between u.s. and foreign criteria for trustworthy systems and as a vehicle for shaping inputsto international discussions of security and safety standards. with thecurrent emergence of national evaluation criteria and the proposed harmonizedinformation technology security evaluation criteria (itsec; federal republicof germany, 1990) developed by the united kingdom, france, germany, andthe netherlands, the orange book is no longer the only game in town. just asgssp would serve to extend the orange book criteria to cover integrity andavailability and advanced system development and assurance techniques, itshould also serve as the basis for resolving the differences between the orangebook and international criteria such as the itsec. in the ongoing process ofreconciling international criteria and evaluations, u.s. interests may beinadequately served if the comparatively narrowly focused orange book is thesole basis for u.s. positions.the committee supports a move already under discussion to conductsimultaneous evaluations of products against the orange book and internationalcriteria to improve the understanding of the relationships among differentcriteria and to enhance reciprocity. a concerted effort to simultaneouslyevaluate a series of trusted products can, over a reasonable period of time, bringthe criteria (eventually including gssp) to a common level of understandingand promote the development of reciprocity in ratings.similar concerns pertain to u.s. participation in international standardssetting committees. u.s. participation is often constrained by concerns aboutinternational technology transfer or by limited technical support from industry.the cost of weak participation may be the imposition on the marketplace ofstandards that do not fully reflect u.s. national or industrial interests.recommendation 2 take specific shortterm actions thatbuild on readily available capabilitiessystem users and vendors can take a number of actions that willimmediately improve the security of computing systems.2a. develop security policies. computer system users should thinkthrough their security needs, establish appropriate policies and associatedprocedures, and ensure that everyone in a given organizationoverview and recommendations32computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.knows those policies and procedures and has some understanding of securityrisks and safe computing practices. many organizations have taken thesecommonsense steps; many others have not or could do so more effectively.23at the highest level, these policies provide directions for programs that affectphysical security, contingency planning, electronic access, networking, securityawareness, and so on. within each of these general security areas, policiesshould be developed to identify the specific controls or mechanisms needed tosatisfy organizational objectives.it should be understood that planning and setting policies and proceduresneed not result in wholesale changes to installed systems. many of the mosteffective management controls relate to system operation rather than tofunctional changes to system design, both because operational changes can beaccomplished quickly and because operational weaknesses in computer systemsare among the most severe practical problems today. such changes may notdecrease vulnerabilities, but they can reduce a potential threat by imposingcontrols on potential abusers. two obvious techniques are upgrading the qualityof security administration (e.g., password management, audit analysis, andconfiguration management) and educating individual users about the risks ofimporting software (e.g., contamination by viruses).2b. form computer emergency response teams. the committeerecommends that all organizations dependent on proper operation of computersystems form or obtain access to computer emergency response teams (certs)trained to deal with security violations (see appendix c). these teams shouldbe prepared to limit the impact of successful attacks, provide guidance inrecovering from attacks, and take measures to prevent repetition of successfulattacks.for security problems arising from basic design faults, such as the lack ofsecurity in ms/dos, little remedy can be expected in the short term. however,for problems resulting from implementation flaws, a cert can help byinforming the vendor of the fault, ensuring that the fault receives sufficientattention, and helping to ensure that upgraded software is distributed andinstalled. darpa's cert and other, smaller efforts have demonstrated thepotential of emergency response teams.2c. use as a first step the orange book's c2 and b1 criteria. untilgssp can be articulated and put in place, industry needs some guidance forraising the security floor in the marketplace. the orange book's c2 and b1criteria provide such guidance, which should beoverview and recommendations33computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.valuable not only to conventional computer system vendors (hardware andsoftware) but also to vendors of computerbased medical systems, specializeddatabase management systems, and other computerbased products. vendorswho have not already done so should move to meet c2 and b1 criteria as aconservative step toward instituting gssp.2d. use sound methodology and modern technology to develop highquality software. the committee recommends that developers of securityrelevant software use currentgeneration tools for software engineering. thedevelopment of highquality software, clearly a paramount goal for any project,often is not achieved because of various realworld pressures and constraints(e.g., competitive need for fast release, or customer demand for enhancedperformance). although the development of more trustworthy systems ingeneral is a concern, security in particular can suffer if systems are notconstructed in a methodical and controlled way.poor development practices can have several consequences. first, theymay lead to a system with vulnerabilities that result directly from undetectederrors in the software. (although objective evidence is hard to gather, it seemsthat technical attacks on systems are targeted more to implementation faultsthan to design faults.) second, such a system may be much harder to evaluate,since it is very difficult for an independent evaluator to understand or reviewthe implementation. third, the system may be harder to maintain or evolve,which means that with time, the security of the system may get worse, not better.conventional wisdom about sound development practices applies withspecial force where security is involved (see box 1.7).2e. implement emerging security standards and participate actively intheir design. the committee urges vendors to incorporate emerging securitystandards into their product planning and to participate more actively in thedesign of such standards. in particular, vendors should develop distributedsystem architectures compatible with evolving security standards.24 further,vendors and largesystem users should make the setting of security standards ahigher priority.current attempts to set standards raise two concerns. first, standardssetting committees should strive to make security standards simple, sincecomplexity is associated with a greater potential for security problems.achieving consensus typically results in a standard that combines the interestsof diverse parties, a process that promotes complexity. second, because thereare hundreds of computingrelated standards groups, setting security standardsgets relativelyoverview and recommendations34computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.limited attention and participation. although nist has supported the setting ofsuch standards, emphasis in this country on standards development by theprivate sector makes active industry participation essential. therefore, vendorsshould be encouraged to assign representatives to u.s. standards efforts toensure that (1) the impact of standards that affect security is fully understoodand (2) security standards can be implemented effectively.box 1.7 sound development methodology forsecure software and systems strive for simplicity and smallness where feasible. use software configuration management and control systems for allsource and object code, specifications, documents, test plans andresults, version control, and release tracking. reduce exposure to failure of security. for example, validated copies ofvital data should be kept offline, and contingency plans for extendedcomputer outages should be in place. restrict general access to software development tools and products, andto the physical environment. develop generally available components with welldocumented programlevel interfaces that can be incorporated into secure software. amongthese should be standardized interfaces to security services (e.g.,cryptography) that may have hardware implementations. provide excess memory and computing capacity relative to the intendedfunctionality. this reduces the need to solve performance problems byintroducing complexity into the software. use higherlevel languages. (this suggestion may not apply tointelligence threats.) aim for building secure software by extending existing secure software.furthermore, use mature product or development technology. couple development of secure software with regular evaluation. ifsystem evaluation is to be done by an outside organization, thatorganization should be involved in the project from it inception. schedule more time and resources for assurance than are typical today. design software to limit the need for secrecy. when a project attempts tomaintain secrecy, it must take extraordinary measures, (e.g., cleared"inspectors general") to ensure that secrecy is not abused (e.g., toconceal poorquality work).2f. use technical aids to foster secure operations. the committeerecommends that vendors take technical steps that will help diminish the impactof user ignorance and carelessness and make it easier tooverview and recommendations35computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.administer systems in a secure manner. for example, systems should be shippedwith security features turned on, so that explicit action is needed to disablethem, and with default identifications and passwords turned off, so that aconscious effort is required to enable them. more efforts are needed to developand market tools that could examine the state of a system and report on itssecurity.25 such audit tools (e.g., mit's kuang tool (baldwin, 1988), digitalequipment corporation's inspect, clyde digital's cubic, demax's securepack,and at&t's quest) have proved useful in assuring the continued operationalsecurity of running systems.recommendation 3 gather information and provideeducation3a. build a repository of incident data. the committee recommends thata repository of incident information be established for use in research, toincrease public awareness of successful penetrations and existingvulnerabilities, and to assist security practitioners, who often have difficultypersuading managers to invest in security. this database should categorize,report, and track pertinent instances of system securityrelated threats, risks, andfailures. because of the need for secrecy and confidentiality about specificsystem flaws and actual penetrations, this information must be collected anddisseminated in a controlled manner. one possible model for data collection isthe incident reporting system administered by the national transportationsafety board; two directly relevant efforts are the incident tracking begun bydarpa's computer emergency response team and nist's announced plans tobegin to track incidents.3b. foster education in engineering secure systems. there is a dramaticshortage of people qualified to build secure software. universities shouldestablish software engineering programs that emphasize development of criticaland secure software; major system users should likewise provide for continuingeducation that promotes expertise in setting requirements for, specifying, andbuilding critical software. effective work on critical software requiresspecialized knowledge of what can go wrong in the application domain.competence in software that controls a nuclear reactor, for example, does notqualify one to work on flightcontrol software. working on secure softwarerequires yet more skills, including understanding the potential for attack, forsoftware in general and for the application domain in particular.especially needed is a universitybased program aimed at returning,overview and recommendations36computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.graduatelevel students who are already somewhat familiar with at least oneapplication area. in addition to covering conventional software engineering,such a program would give special emphasis to topics related to criticalsoftware and security26 and could best be developed at universities with stronggraduate engineering and business programs. the committee envisions as aninitial step approximately three such programs, each turning out perhaps 20people a year.given the current shortage of qualified people and the time needed foruniversities to establish appropriate programs, those undertaking large securityrelated development efforts should deal explicitly with the need to educateproject members. both time and money for this should appear in project budgets.3c. provide early training in security practices and ethics. thecommittee recommends that security practices and ethics be integrated into thegeneral process of learning about and using computers. awareness of theimportance of security measures should be integrated into early education aboutcomputing. lessons about socially acceptable and unacceptable behavior (e.g.,stealing passwords is not acceptable) should also be taught when students firstbegin to use computers, just as library etiquette (e.g., writing in library books isnot acceptable) is taught to young readersšwith the recognition, of course, thatsecurity is a more complex subject. this recommendation is aimed at teachers,especially those at the primary and secondary levels. implementing it wouldrequire that organizations and professionals concerned with security get theword out, to organizations that customarily serve and inform teachers anddirectly to teachers in communities.recommendation 4 clarify export control criteria, and setup a forum for arbitrationthe market for computer and communications security, like the computermarket overall, is international. if the united states does not allow vendors ofcommercial systems to export security products and products with relativelyeffective security features, large multinational firms as well as foreignconsumers will simply purchase equivalent systems from foreignmanufacturers. at issue is the ability to export two types of products: (1) trustedsystems and (2) encryption.4a. clarify export controls on trusted systems and differentiateoverview and recommendations37computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.them from orange book ratings. industry has complained for some timeabout current export controls on trusted systems. the requirement for casebycase review of export licenses for trusted systems with orange book ratings ofb3 and above adds to the cost of such systems, because sales may be restrictedand extra time is needed to apply for and receive export approval. theseprospects discourage industry from developing more secure systems; vendorsdo not want to jeopardize the exportability of their mainline commercialofferings.27the committee recommends that orange book ratings not be used asexport control criteria. it also recommends that the department of commerce,in conjunction with the departments of defense and state, clarify for industrythe content of the regulations and the process by which they are implemented.removal of orange book ratings as control parameters would also help toalleviate potential problems associated with multiple, national rating schemes(see chapter 5).the crux of the problem appears to be confusion among orange bookratings, dualuse (military and civilian) technology, and militarycriticaltechnology. security technology intended to counter an intelligencegrade threatis considered military critical and not dual usešit is not aimed at commercialas well as military uses. security technology intended to counter a lower,criminalgrade threat is of use to both defense and commercial entities, but it isnot military critical. since an orange book rating per se is not proof against anintelligencegrade threat, it does not alone signal militarycritical technologythat should be tightly controlled. industry needs to know which features of aproduct might trigger export restrictions.4b. review export controls on implementations of the data encryption standard. the growth of networked and distributed systems has created needsfor encryption in the private sector. some of that pressure has been seen in thepush for greater exportability of products using the data encryption standard(des) and its deployment in foreign offices of u.s. companies.28in principle, any widely available internationally usable encryptionalgorithm should be adequate. nist, working with nsa, is currently trying todevelop such algorithms. however, the committee notes that this effort may notsolve industry's problems, for several reasons. the growing installed base ofdes products cannot be easily retrofitted with the new products. the foreignsupply of des products may increase the appeal of foreign products. finally,nsainfluenced alternatives may be unacceptable to foreign or even u.s.buyers, as evidenced by the american banking association's oppositionoverview and recommendations38computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.to the nsa's proposals to effectively restrict banks to encryption algorithmsdesigned and developed by nsa when the des was last recertified, in 1988.the committee has been apprised that nsa, because of classified nationalsecurity concerns, does not support the removal of remaining restrictions onexport of des. however, there is a growing lack of sympathy in thecommercial community with the nsa position on this matter. the committeerecommends that the administration appoint an arbitration group consisting ofappropriately cleared individuals from industry and the department ofcommerce as well as the department of defense to impartially evaluate if thereare indeed valid reasons at this time for limiting the export of des.29recommendation 5 fund and pursue needed researchthe dramatic changes in the technology of computing make it necessaryfor the computer science and engineering communities to rethink some of thecurrent technical approaches to achieving security. the most dramatic exampleof the problem is the confusion about how best to achieve security in networkedenvironments and embedded systems.at present, there is no vigorous program to meet this need. particularlyworrisome is the lack of academic research in computer security, notablyresearch relevant to distributed systems and networks.30 only in theoreticalareas, such as number theory, zeroknowledge proofs, and cryptology, whichare conducive to individual research efforts, has there been significant academiceffort. although it must be understood that many research topics could bepursued in industrial as well as academic research laboratories, the committeehas focused on strengthening the comparatively weaker research effort inuniversities, since universities both generate technical talent and aretraditionally the base for addressing relatively fundamental questions.the committee recommends that government sponsors of computerscience and technology research (in particular, darpa and nsf) undertakewelldefined and adequately funded programs of research and technologydevelopment in computer security. a key role for nsf (and perhaps darpa),beyond specific funding of relevant projects, is to facilitate increased crosscoupling between security experts and researchers in related fields. thecommittee also recommends that nist, in keeping with its interest in computersecurity and its charter to enhance security for sensitive unclassified data andsystems, provideoverview and recommendations39computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.funding for research in areas of key concern to it, either internally or incollaboration with other agencies that support research.box 1.8 security research agenda security modularityšhow can a set of system components with knownsecurity properties be combined or composed to form a larger systemwith known security properties? how can a system be decomposed intobuilding blocks, units that can be used independently in other systems? security policy modelsšsecurity requirements other than disclosurecontrol, such as integrity, availability, and distributed authentication andauthorization, are not easily modeled. there is also a need for bettermodels that address protocols and other aspects of distributed systems. cost/benefit models for securityšhow much does security (includingalso privacy protection) really cost, and what are its real benefits? new security mechanismsšas new requirements are proposed, asnew threats are considered, and as new technologies become prevalent,new mechanisms are required to maintain effective security. somecurrent topics for research include mechanisms to support criticalaspects of integrity (separation of duty, for example), distributed keymanagement on lowsecurity systems, multiway and transitiveauthentication, availability (especially in distributed systems andnetworks), privacy assurance, and access controllers in networks topermit interconnection of mutually suspicious organizations. increasing effectiveness of assurance techniquesšmore needs tobe known about the spectrum of analysis techniques, both formal andinformal, and to what aspects of security they best apply. also, tools areneeded to support the generation of assurance evidence. alternative representations and presentationsšnew representationsof security properties may yield new analysis techniques. for example,the committee has identified several specific technical issues that justifyresearch (see box 1.8). chapter 8 provides a fuller discussion; chapters 3 and 4address some underlying issues. the list, although by no means complete,shows the scope and importance of a possible research agenda.the committee believes that greater university involvement in largescaleresearchoriented system development projects (comparable to the old arpanetand multics programs) would be highly beneficial for security research. it isimportant that contemporary projects, both inside and outside universities, beencouraged to use stateofthe art software development tools and securitytechniques, in order to evaluate these tools and to assess the expected gain insystem security. also, while academic computer security research traditionallyhas beenoverview and recommendations40computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.performed in computer science departments, several study areas are clearlyappropriate for researchers based in business schools, including assessing theactual value to an organization of information technology and of protectingprivacy. graphics tools that allow system operators to set, explore, and analyzeproposed policies (who should get access to what) and systemconfigurations (who has access to what) may help identify weaknessesor unwanted restrictions as policies are instituted and deployed systemsused. automated security proceduresšresearch is needed in automatingcritical aspects of system operation, to assist the system manager inavoiding security faults in this area. examples include tools to check thesecurity state of a system, models of operational requirements anddesired controls, and threat assessment aids. nonrepudiationšto protect proprietary rights it may be necessary torecord user actions so as to bar the user from later repudiating theseactions. doing this in a way that respects the privacy of users is difficult. resource controlšresource control is associated with the preventionof unauthorized use of proprietary software or databases legitimatelyinstalled in a computing system. it has attracted little research andimplementation effort, but it poses some difficult technical problems andpossibly problems related to privacy as well. systems with security perimetersšnetwork protocol design effortshave tended to assume that networks will provide generalinterconnection. however, as observed in chapter 3, a commonpractical approach to achieving security in distributed systems is topartition the system into regions that are separated by a securityperimeter. this may cause a loss of network functionality. if, forexample, a network permits mail but not directory services (because ofsecurity concerns about directory searches), less mail may be sentbecause no capability exists to look up the address of a recipient.darpa has a tradition of funding significant system development projectsof the kind that can be highly beneficial for security research. examples ofvaluable projects include: use of stateoftheart software development techniques and tools toproduce a secure system. the explicit goal of such an effort should be toevaluate the development process and to assess the expected gain insystem quality. the difficulty of uncovering vulnerabilities throughtesting suggests that a marriage of traditional software engineeringtechniques with formal methods is needed. development of distributed systems with a variety of security properties.a project now under way, with darpa funding, is theoverview and recommendations41computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.development of encryptionbased private electronic mail. another suchproject could focus on decentralized, peerconnected name servers. development of a system supporting some approach to data integrity.there are now some proposed models for integrity, but without workedexamples it will be impossible to validate them. this represents anopportunity for darpanist cooperation.in addition to funding specific relevant projects, both darpa and nsfshould encourage collaboration across research fields. crossdisciplinaryresearch in the following areas would strengthen system trustworthiness: safety: there is growing concern about and interest in the safetyrelatedaspects of computer processing both in the united states andinternationally. faulttolerant computing: much research has been directed at the problemof faulttolerant computing, and an attempt should be made to extend thiswork to other aspects of security. code analysis: people working on optimizing and parallelizing compilershave extensive experience in analyzing both source and object code for avariety of properties. an attempt should be made to see if similartechniques can be used to analyze code for properties related to security. security interfaces: people working in the area of formal specificationshould be encouraged to specify standardized interfaces to securityservices and to apply their techniques to the specification and analysis ofhighlevel security properties. theoretical research: theoretical work needs to be properly integrated inactual systems. often both theoreticians and system practitionersmisunderstand the system aspects of security or the theoretical limitationsof secure algorithms. programming language research: new paradigms require new securitymodels, new design and analysis techniques, perhaps additionalconstructs, and persuasion of both researchers and users that security isimportant before too many tools proliferate. software development environments: myriad tools (e.g., theorem provers,test coverage monitors, object managers, and interface packages) continueto be developed by researchers, sometimes in collaborative efforts such asarcadia. some strategy for integrating such tools is needed to drive theresearch toward more systemoriented solutions.31again, much of this research is appropriate for both commercial andacademic entities, and it might require or benefit from industryoverview and recommendations42computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.university collaboration. certainly, joint industryuniversity efforts mayfacilitate the process of technology transfer. nsf and darpa have a traditionof working with the broad science community and could obviously take onprograms to facilitate needed collaboration. some possible specific actions aresuggested in chapter 8.recommendation 6 establish an information securityfoundationthe public needs an institution that will accelerate the commercializationand adoption of safer and more secure computer and communications systems.to meet that need, the committee recommends the establishment of a newprivate organizationša consortium of computer users, vendors, and otherinterested parties (e.g., property and casualty insurers). this organization mustnot be, or even be perceived to be, a captive of government, system vendors, orindividual segments of the user community.the committee recommends a new institution because it concludes thatpressing needs in the following areas are not likely to be met adequately byexisting entities: establishment of generally accepted system security principles, or gssp; research on computer system security, including evaluation techniques; system evaluation; development and maintenance of an incident, threat, and vulnerabilitytracking system; education and training; brokering and enhancing communications between commercial andnational security interests; and focused participation in international standardization and harmonizationefforts for commercial security practice.why should these functions be combined in a single organization?although the proposed organization would not have a monopoly on all of thesefunctions, the committee believes that the functions are synergistic. forexample, involvement in research would help the organization recruittechnically talented staff; involvement in research and the development ofgssp would inform the evaluation effort; and involvement in gsspdevelopment and evaluation would inform education, training, andcontributions to international criteriasetting and evaluation schemes. further, anew organization would haveoverview and recommendations43computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.more flexibility than those currently focused on security to build strong bridgesto other aspects of trust, notably safety.in the short run, this organization, called the information securityfoundation (isf) in this report, would act to increase awareness andexpectations regarding system security and safety. the pressure provided byorganized tracking and reporting of faults would encourage vendors and usersto pay greater attention to system quality; the development and promulgation ofgssp should cause users and vendors to focus on an accepted base of prudentpractice.in the longer term, a major activity of the isf would be product evaluation.the complex and critical nature of security products makes independentevaluation essential. the only current official source of evaluations, the ncsc,has been criticized as poorly suited to meeting industry's needs, and changes inits charter and direction are reducing its role in this area. the process ofevaluation described in chapters 5 and 7 is intended to address directlyindustry's concerns with the current process and to define a program that can bea success in the commercial marketplace. the committee concludes that someform of system evaluation is a critical aspect of achieving any real improvementin computer security.also in the longer term, the isf would work to bridge the security andsafety arenas, using as vehicles gssp and evaluation as well as the otheractivities. the isf could play a critical role in improving the overall quality andtrustworthiness of computer systems, using the need for better security as aninitial target to motivate its activities.the organization envisioned must be designed to interact closely withgovernment, specifically the ncsc and nist, so that its results can contributeto satisfying government needs. similarly, it would coordinate with operationalorganizations such as darpa's cert, especially if the cert proceeds withits plans to develop an emergencyincident tracking capability. the governmentmay be the best vehicle to launch the isf, but it should be an independent,private organization once functional.as discussed in detail in chapter 7, the committee concludes that the isfwould need the highest level of governmental support; the strongest expressionof such support would be a special congressional charter. such a charter woulddefine isf's role and its relation to the government. at the same time, theorganization should be outside of the government to keep it separate from thefocus on intragovernmental security needs, internecine political squabbles, andthe hiring and resource limitations that constrain ncsc and nist. its majorsource of funds should be member subscriptions and feesoverview and recommendations44computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.for services such as evaluation. it must not depend on government funding forits viability.note that the mission outlined above is much more challenging thandefining standards or providing evaluation of consumer durables (e.g., as doneby underwriters laboratories, inc.). the committee does not know of anyexisting private organization that could take on these tasks.although it recognizes that any proposal for establishing a new institutionfaces an uphill battle, the committee sees this proposal as a test of commitmentfor industry, which has complained loudly about the existing institutionalinfrastructure. commitment to an organization like that proposed can facilitateselfregulation and greatly diminish the likelihood of explicit governmentregulation.if a new organization is not establishedšor if the functions proposed for itare not pursued in an aggressive and wellfunded manner, the most immediateconsequence will be the further discouraging of efforts by vendors to developevaluated products, even though evaluation is vital to assuring that products areindeed trustworthy; the continuation of a slow rate of progress in the market,leaving many system users unprotected and unaware of the risks they face; andthe prospect that u.s. vendors will become less competitive in the internationalsystems market. without aggressive action to increase system trustworthiness,the national exposure to safety and security catastrophes will increase rapidly.conclusiongetting widely deployed and more effective computer andcommunications security is essential if the united states is to fully achieve thepromise of the information age. the technology base is changing, and theproliferation of networks and distributed systems has increased the risks ofthreats to security and safety. the computer and communications securityproblem is growing. progress is needed on many frontsšincludingmanagement, development, research, legal enforcement, and institutionalsupportšto integrate security into the development and use of computer andcommunications technology and to make it a constructive and routinecomponent of information systems.notes1. losses from credit card and communications fraud alone investigated by the secret service rangeinto the millions. see box 1.1 for other examples.overview and recommendations45computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.2. this growth may be aided by recent political changes in eastern europe and the soviet union,which are believed to be freeing up intelligence resources that analysts suggest may be redirectedtoward economic and technological targets (safire, 1990).3. voting systems present special challenges: first, the data is public property. second, votingsystems are information systems deployed to strange locations, handled by volunteers, abused bythe media (''got to know the results by 8 p.m."), and offered by specialty vendors. third, theopenness issue can be evaded by vendors promoting proprietary approaches, in the absence of anyorganized screening or regulatory activity. fourth, the security overhead in the system cannot get inthe way of the operations of the system under what are always difficult conditions. voting systemtechnology makes an interesting case study because it is inherently systemoriented: ballotpreparation, input sensing, data recording and transmission, preelection testing, intrusionprevention, result preservation, and reporting. the variety of product responses are thereforeimmense, and each product must fit as wide a range of voting situations as possible, and beattractive and costeffective. anecdotal evidence suggests a range of security problems for thiscomparatively new application. (hoffman, 1988; ecri, 1988b; saltman, 1988; miscellaneous issuesof risks.)4. viruses can spread by means of or independently of networks (e.g., via contaminated diskettes).5. the committee did not find evidence of significant japanese activity in computer security,although viruses have begun to raise concern in japan as evidenced by japanese newspaper articles,and japanese system development interests provide a foundation for possible eventual action. forcompetitive reasons, both japanese and european developments should be closely monitored.6. a new organization, the electronic frontiers foundation, has recently been launched to defendthese free speech aspects.7. for example, professional journals and meetings have held numerous debates over theinterpretation of the internet worm and the behavior of its perpetrator; the internet worm alsoprompted the issuance or reissuance of codes of ethics by a variety of computer specialistorganizations.8. two recent studies have pointed to the increased concern with security in networks: thecongressional office of technology assessment's critical connections: communication for thefuture (ota, 1990) and the national research council's growing vulnerability of the public switched networks (nrc, 1989b).9. this evolution took roughly two centuries in the case of safecracking, a technology whosesystems consist of a box, a door, and a lock.10. this does not mean that the effort was wasted. in fact, some would argue that this is the heightof success (tzu, 1988).11. for example, a california prosecutor recently observed that "we probably turn down more cases[involving computer breakins] than we charge, because computersystem proprietors haven't madeclear what is allowed and what isn't" (stipp, 1990).12. for example, a description of a magnetic door sensor that is highly selective about the magneticfield it will recognize as indicating "door closed" can indicate to attackers that less sophisticatedsensors can be misled by placing a strong magnet near them before opening the door.13. for example, the gao recently noted in connection with the numerous penetrations of thespace physics analysis network in the 1980s that, "skillful, unauthorized users could enter and exita computer without being detected. in such cases and even in those instances where nasa hasdetected illegal entry, data could have been copied, altered, or destroyed without nasa or anyoneelse knowing" (gao, 1989e, p. 1).overview and recommendations46computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.14. "programming" is to be understood in a general sensešanything that modifies or extends thecapabilities of a system is programming. modification of controls on access to a system, forexample, is a type of programming with significant security implications. even specialpurposesystems with no access to programming languages, not even to a "shell" or command language, areusually programmable in this sense.15. "embeddedness" refers to the extent to which a computer system is embedded in a process, andit correlates with the degree to which the process is controlled by the computer. computercontrolled xray machines and manufacturing systems, avionics systems, and missiles are examplesof embedded systems. higher degrees of embeddedness, generated by competitive pressures thatdrive the push for automation, shorten the link between information and action and increase thepotential for irreversible actions taken without human intervention. by automating much of aprocess, embeddedness increases the leverage of an attacker.16. however, sometimes there will be tradeoffs between security or safety and other characteristics,like performance. such tradeoffs are not unique to computing, although they may be comparativelymore recent.17. it is worth noting, however, that "safety factors" play a role in security. measures such as audittrails are included in security systems as a safety factor; they provide a backup mechanism fordetection when something else breaks.18. even nsa is confronting budget cuts in the context of overall cuts in defense spending.19. for example, the american institute of certified public accountants promulgates statements onauditing standards (sas), and the financial accounting standards board (fasb) promulgateswhat have been called generally accepted accounting principles (gaap). managers accept theimportance of both the standards and their enforcement as a risk management tool. adherence tothese standards is also encouraged by laws and regulations that seek to protect investors and thepublic. (see appendix d.)20. b1 is also the highest level to which systems can effectively be retrofitted with security features.21. an effort by several large commercial users to list desired computer and communicationssystem security features demonstrates the importance of greater integrity protection and theemphasis on discretionary access control in that community. this effort appears to place relativelylimited emphasis on assurance and evaluation, both of which the committee deem important togssp and to an ideal set of criteria. the seed for that effort was a project within american expresstravel related services to define a corporate security standard called c2plus and based, as thename suggests, on the orange book's c2 criteria (cutler and jones, 1990).22. in the past decade, a number of organizations (e.g., corporation for open systems and theformerly independent manufacturing automation protocol/technical office protocol users group)have emerged with the goal of influencing the development of industry standards for computing andcommunications technology and promoting the use of official standards, in part by facilitatingconformance testing (frenkel, 1990).23. the computer security act of 1987, for example, set in motion a process aimed at improvingsecurity planning in federal agencies. the experience showed that it was easier to achievecompliance on paper than to truly strengthen planning and management controls (gao, 1990c).24. examples include iso 7498œ2 (iso, 1989), ccitt x.509 (ccitt, 1989b), and the nsalaunched secure data network system (sdns) standardization program.25. the very availability of such tools puts an extra responsibility on management to eliminate thekinds of vulnerabilities the tools reveal.26. for example, discussions of different project management structures wouldoverview and recommendations47computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.deal with their impact not only on productivity but also on security. discussions of quality assurancewould emphasize safety engineering more than might be expected in a traditional softwareengineering program.27. it is expensive for vendors to maintain two versions of productsšsecure and regular. thus, allelse being equal, regular versions can be expected to be displaced by secure versions. but if salesare restricted, then only the regular version will be marketed, to the detriment of security.28. as this report goes to press, a case is under consideration at the department of state that couldresult in liberalized export of des chips, although such an outcome is considered unlikely.29. as of this writing, similar actions may also be necessary in connection with the rsa publickeyencryption system, which is already available overseas (without patent protection) because itsprinciples were first published in an academic journal (rivest et al., 1978).30. the paucity of academic effort is reflected by the fact that only 5 to 10 percent of the attendeesat recent ieee symposiums on security and privacy have been from universities.31. for vendors, related topics would be trusted distribution and trusted configuration control overthe product life cycle.overview and recommendations48computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.2concepts of information securitythis chapter discusses security policies in the context of requirements forinformation security and the circumstances in which those requirements must bemet, examines common principles of management control, and reviews typicalsystem vulnerabilities, in order to motivate consideration of the specific sorts ofsecurity mechanisms that can be built into computer systemsšto complementnontechnical management controls and thus implement policyšand to stressthe significance of establishing gssp. additional information on privacy issuesand detailing the results of an informal survey of commercial security officers isprovided in the two chapter appendixes.organizations and people that use computers can describe their needs forinformation security and trust in systems in terms of three major requirements: confidentiality: controlling who gets to read information; integrity: assuring that information and programs are changed only in aspecified and authorized manner; and availability: assuring that authorized users have continued access toinformation and resources.these three requirements may be emphasized differently in variousapplications. for a national defense system, the chief concern may be ensuringthe confidentiality of classified information, whereas a funds transfer systemmay require strong integrity controls. the requirements for applications that areconnected to external systems will differ from those for applications withoutsuch interconnection. thus the specific requirements and controls forinformation security can vary.concepts of information security49computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.the framework within which an organization strives to meet its needs forinformation security is codified as security policy. a security policy is a concisestatement, by those responsible for a system (e.g., senior management), ofinformation values, protection responsibilities, and organizational commitment.one can implement that policy by taking specific actions guided bymanagement control principles and utilizing specific security standards,procedures, and mechanisms. conversely, the selection of standards,procedures, and mechanisms should be guided by policy to be most effective.to be useful, a security policy must not only state the security need (e.g.,for confidentialityšthat data shall be disclosed only to authorized individuals),but also address the range of circumstances under which that need must be metand the associated operating standards. without this second part, a securitypolicy is so general as to be useless (although the second part may be realizedthrough procedures and standards set to implement the policy). in any particularcircumstance, some threats are more probable than others, and a prudent policysetter must assess the threats, assign a level of concern to each, and state apolicy in terms of which threats are to be resisted. for example, until recentlymost policies for security did not require that security needs be met in the faceof a virus attack, because that form of attack was uncommon and not widelyunderstood. as viruses have escalated from a hypothetical to a commonplacethreat, it has become necessary to rethink such policies in regard to methods ofdistribution and acquisition of software. implicit in this process ismanagement's choice of a level of residual risk that it will live with, a level thatvaries among organizations.management controls are the mechanisms and techniquesšadministrative,procedural, and technicalšthat are instituted to implement a security policy.some management controls are explicitly concerned with protectinginformation and information systems, but the concept of management controlsincludes much more than a computer's specific role in enforcing security. notethat management controls not only are used by managers, but also may beexercised by users. an effective program of management controls is needed tocover all aspects of information security, including physical security,classification of information, the means of recovering from breaches of security,and above all training to instill awareness and acceptance by people. there aretradeoffs among controls. for example, if technical controls are not available,then procedural controls might be used until a technical solution is found.technical measures alone cannot prevent violations of the trust peopleplace in individuals, violations that have been the source ofconcepts of information security50computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.much of the computer security problem in industry to date (see chapter 6).technical measures may prevent people from doing unauthorized things butcannot prevent them from doing things that their job functions entitle them todo. thus, to prevent violations of trust rather than just repair the damage thatresults, one must depend primarily on human awareness of what other humanbeings in an organization are doing. but even a technically sound system withinformed and watchful management and users cannot be free of all possiblevulnerabilities. the residual risk must be managed by auditing, backup, andrecovery procedures supported by general alertness and creative responses.moreover, an organization must have administrative procedures in place tobring peculiar actions to the attention of someone who can legitimately inquireinto the appropriateness of such actions, and that person must actually make theinquiry. in many organizations, these administrative provisions are far lesssatisfactory than are the technical provisions for security.a major conclusion of this report is that the lack of a clear articulation ofsecurity policy for general computing is a major impediment to improvedsecurity in computer systems. although the department of defense (dod) hasarticulated its requirements for controls to ensure confidentiality, there is noarticulation for systems based on other requirements and management controls(discussed below)šindividual accountability, separation of duty, auditability,and recovery. this committee's goal of developing a set of generally acceptedsystem security principles, gssp, is intended to address this deficiency and isa central recommendation of this report.in computing there is no generally accepted body of prudent practiceanalogous to the generally accepted accounting principles promulgated by thefinancial auditing standards board (see appendix d). managers who havenever seen adequate controls for computer systems may not appreciate thecapabilities currently available to them, or the risks they are taking by operatingwithout these controls. faced with demands for more output, they have had noincentive to spend money on controls. reasoning like the following is common:"can't do it and still stay competitive"; "we've never had any trouble, so whyworry"; "the vendor didn't put it in the product; there's nothing we can do."on the basis of reported losses, such attitudes are not unjustified(neumann, 1989). however, computers are active entities, and programs can bechanged in a twinkling, so that past happiness is no predictor of future bliss.there has to be only one internet worm incident to signal a larger problem.experience since the internet worm involving copycat and derivative attacksshows how a possibility once demonstrated can become an actuality frequentlyused.1concepts of information security51computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.some consensus does exist on fundamental or minimumrequired securitymechanisms. a recent informal survey conducted on behalf of the committeeshows a widespread desire among corporate system managers and securityofficers for the ability to identify users and limit times and places of access,particularly over networks, and to watch for intrusion by recording attempts atinvalid actions (see chapter appendix 2.2). ad hoc virus checkers, well knownin the personal computer market, are also in demand. however, there is littledemand for system managers to be able to obtain positive confirmation that thesoftware running on their systems today is the same as what was runningyesterday. such a simple analog of hardware diagnostics should be afundamental requirement; it may not be seen as such because vendors do notoffer it or because users have difficulty expressing their needs.although threats and policies for addressing them are different fordifferent applications, they nevertheless have much in common, and the generalsystems on which applications are built are often the same. furthermore, basicsecurity services can work against many threats and support many policies.thus there is a large core of policies and services on which most of the users ofcomputers should be able to agree. on this basis the committee proposes theeffort to define and articulate gssp.security policiesresponding to requirementsfor confidentiality,integrity, and availabilitythe weight given to each of the three major requirements describing needsfor information securityšconfidentiality, integrity, and availabilityšdependsstrongly on circumstances. for example, the adverse effects of a system notbeing available must be related in part to requirements for recovery time. asystem that must be restored within an hour after disruption represents, andrequires, a more demanding set of policies and controls than does a similarsystem that need not be restored for two to three days. likewise, the risk of lossof confidentiality with respect to a major product announcement will changewith time. early disclosure may jeopardize competitive advantage, butdisclosure just before the intended announcement may be insignificant. in thiscase the information remains the same, while the timing of its releasesignificantly affects the risk of loss.confidentialityconfidentiality is a requirement whose purpose is to keep sensitiveinformation from being disclosed to unauthorized recipients. theconcepts of information security52computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.secrets might be important for reasons of national security (nuclear weaponsdata), law enforcement (the identities of undercover drug agents), competitiveadvantage (manufacturing costs or bidding plans), or personal privacy (credithistories) (see chapter appendix 2.1).the most fully developed policies for confidentiality reflect the concernsof the u.s. national security community, because this community has beenwilling to pay to get policies defined and implemented (and because the valueof the information it seeks to protect is deemed very high). since the scope ofthreat is very broad in this context, the policy requires systems to be robust inthe face of a wide variety of attacks. the specific dod policies for ensuringconfidentiality do not explicitly itemize the range of expected threats for whicha policy must hold. instead, they reflect an operational approach, expressing thepolicy by stating the particular management controls that must be used toachieve the requirement for confidentiality. thus they avoid listing threats,which would represent a severe risk in itself, and avoid the risk of poor securitydesign implicit in taking a fresh approach to each new problem.the operational controls that the military has developed in support of thisrequirement involve automated mechanisms for handling information that iscritical to national security. such mechanisms call for information to beclassified at different levels of sensitivity and in isolated compartments, to belabeled with this classification, and to be handled by people cleared for accessto particular levels and/or compartments. within each level and compartment, aperson with an appropriate clearance must also have a "need to know" in orderto gain access. these procedures are mandatory: elaborate procedures must alsobe followed to declassify information.2classification policies exist in other settings, reflecting a generalrecognition that to protect assets it is helpful to identify and categorize them.some commercial firms, for instance, classify information as restricted,company confidential, and unclassified (schmitt, 1990). even if an organizationhas no secrets of its own, it may be obliged by law or common courtesy topreserve the privacy of information about individuals. medical records, forexample, may require more careful protection than does most proprietaryinformation. a hospital must thus select a suitable confidentiality policy touphold its fiduciary responsibility with respect to patient records.in the commercial world confidentiality is customarily guarded by securitymechanisms that are less stringent than those of the national securitycommunity. for example, information is assigned to an "owner" (or guardian),who controls access to it.3 such security mechanisms are capable of dealingwith many situations but are not as resistant to certain attacks as aremechanisms based on classification and mandatoryconcepts of information security53computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.labeling, in part because there is no way to tell where copies of information mayflow. with trojan horse attacks, for example, even legitimate and honest usersof an owner mechanism can be tricked into disclosing secret data. thecommercial world has borne these vulnerabilities in exchange for the greateroperational flexibility and system performance currently associated withrelatively weak security.integrityintegrity is a requirement meant to ensure that information and programsare changed only in a specified and authorized manner. it may be important tokeep data consistent (as in doubleentry bookkeeping) or to allow data to bechanged only in an approved manner (as in withdrawals from a bank account).it may also be necessary to specify the degree of the accuracy of data.some policies for ensuring integrity reflect a concern for preventing fraudand are stated in terms of management controls. for example, any taskinvolving the potential for fraud must be divided into parts that are performedby separate people, an approach called separation of duty. a classic example isa purchasing system, which has three parts: ordering, receiving, and payment.someone must sign off on each step, the same person cannot sign off on twosteps, and the records can be changed only by fixed proceduresšfor example,an account is debited and a check written only for the amount of an approvedand received order. in this case, although the policy is stated operationallyšthat is, in terms of specific management controlsšthe threat model is explicitlydisclosed as well.other integrity policies reflect concerns for preventing errors andomissions, and controlling the effects of program change. integrity policieshave not been studied as carefully as confidentiality policies. computermeasures that have been installed to guard integrity tend to be ad hoc and do notflow from the integrity models that have been proposed (see chapter 3).availabilityavailability is a requirement intended to ensure that systems workpromptly and service is not denied to authorized users. from an operationalstandpoint, this requirement refers to adequate response time and/or guaranteedbandwidth. from a security standpoint, it represents the ability to protectagainst and recover from a damaging event. the availability of properlyfunctioning computer systems (e.g., for routing longdistance calls or handlingairline reservations) is essential to the operation of many large enterprises andsometimesconcepts of information security54computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.for preserving lives (e.g., air traffic control or automated medical systems).contingency planning is concerned with assessing risks and developing plansfor averting or recovering from adverse events that might render a systemunavailable.traditional contingency planning to ensure availability usually includesresponses only to acts of god (e.g., earthquakes) or accidental anthropogenicevents (e.g., a toxic gas leak preventing entry to a facility). however,contingency planning must also involve providing for responses to maliciousacts, not simply acts of god or accidents, and as such must include an explicitassessment of threat based on a model of a real adversary, not on a probabilisticmodel of nature.for example, a simple availability policy is usually stated like this: "on theaverage, a terminal shall be down for less than 10 minutes per month." aparticular terminal (e.g., an automatic teller machine or a reservation agent'skeyboard and screen) is up if it responds correctly within one second to astandard request for service; otherwise it is down. this policy means that the uptime at each terminal, averaged over all the terminals, must be at least 99.98percent.a security policy to ensure availability usually takes a different form, as inthe following example: "no inputs to the system by any user who is not anauthorized administrator shall cause the system to cease serving some otheruser." note that this policy does not say anything about system failures, exceptto the extent that they can be caused by user actions. instead, it identifies aparticular threat, a malicious or incompetent act by a regular user of the system,and requires the system to survive this act. it says nothing about other ways inwhich a hostile party could deny service, for example, by cutting a telephoneline; a separate assertion is required for each such threat, indicating the extent towhich resistance to that threat is deemed important.examples of security requirements for different applicationsthe exact security needs of systems will vary from application toapplication even within a single application. as a result, organizations mustboth understand their applications and think through the relevant choices toachieve the appropriate level of security.an automated teller system, for example, must keep personal identificationnumbers (pins) confidential, both in the host system and during transmissionfor a transaction. it must protect the integrity of account records and ofindividual transactions. protection of privacy is important, but not critically so.availability of the host system is important to the economic survival of thebank, although not to its fiduciary responsibility. as compared to theavailability ofconcepts of information security55computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.the host system, the availability of individual teller machines is of less concern.a telephone switching system, on the other hand, does not have highrequirements for integrity on individual transactions, as lasting damage will notbe incurred by occasionally losing a call or billing record. the integrity ofcontrol programs and configuration records, however, is critical. without these,the switching function would be defeated and the most important attribute of allšavailabilityšwould be compromised. a telephone switching system mustalso preserve the confidentiality of individual calls, preventing one caller fromoverhearing another.security needs are determined more by what a system is used for than bywhat it is. a typesetting system, for example, will have to assure confidentialityif it is being used to publish corporate proprietary material, integrity if it isbeing used to publish laws, and availability if it is being used to publish a dailypaper. a generalpurpose timesharing system might be expected to provideconfidentiality if it serves diverse clientele, integrity if it is used as adevelopment environment for software or engineering designs, and availabilityto the extent that no one user can monopolize the service and that lost files willbe retrievable.management controlschoosing the means tosecure information and operationsthe setting of security policy is a basic responsibility of managementwithin an organization. management has a duty to preserve and protect assetsand to maintain the quality of service. to this end it must assure that operationsare carried out prudently in the face of realistic risks arising from crediblethreats. this duty may be fulfilled by defining highlevel security policies andthen translating these policies into specific standards and procedures forselecting and nurturing personnel, for checking and auditing operations, forestablishing contingency plans, and so on. through these actions, managementmay prevent, detect, and recover from loss. recovery depends on various formsof insurance: backup records, redundant systems and service sites, selfinsurance by cash reserves, and purchased insurance to offset the cost ofrecovery.preventing breaches of securityšbasic principlesmanagement controls are intended to guide operations in proper directions,prevent or detect mischief and harmful mistakes, and giveconcepts of information security56computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.early warning of vulnerabilities. organizations in almost every line of endeavorhave established controls based on the following key principles: individual accountability, auditing, and separation of duty.these principles, recognized in some form for centuries, are the basis ofprecomputer operating procedures that are very well understood.individual accountability answers the question: who is responsible for thisstatement or action? its purpose is to keep track of what has happened, of whohas had access to information and resources and what actions have been taken.in any real system there are many reasons why actual operation may not alwaysreflect the original intentions of the owners: people make mistakes, the systemhas errors, the system is vulnerable to certain attacks, the broad policy was nottranslated correctly into detailed specifications, the owners changed their minds,and so on. when things go wrong, it is necessary to know what has happened,and who is the cause. this information is the basis for assessing damage,recovering lost information, evaluating vulnerabilities, and initiatingcompensating actions, such as legal prosecution, outside the computer system.to support the principle of individual accountability, the service calleduser authentication is required. without reliable identification, there can be noaccountability. thus authentication is a crucial underpinning of informationsecurity. many systems have been penetrated when weak or poorlyadministered authentication services have been compromised, for example, byguessing poorly chosen passwords.the basic service provided by authentication is information that astatement or action was made by a particular user. sometimes, however, there isa need to ensure that the user will not later be able to claim that a statementattributed to him was forged and that he never made it. in the world of paperdocuments, this is the purpose of notarizing a signature; the notary providesindependent and highly credible evidence, which will be convincing even aftermany years, that a signature is genuine and not forged. this more stringent formof authentication, called nonrepudiation, is offered by few computer systemstoday, although a legal need for it can be foreseen as computermediatedtransactions become more common in business.auditing services support accountability and therefore are valuable tomanagement and to internal or external auditors. given the reality that everycomputer system can be compromised from within,concepts of information security57computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.and that many systems can also be compromised if surreptitious access can begained, accountability is a vital last resort. auditing services make and keep therecords necessary to support accountability. usually they are closely tied toauthentication and authorization (a service for determining whether a user orsystem is trusted for a given purposešsee discussion below), so that everyauthentication is recorded, as is every attempted access, whether authorized ornot. given the critical role of auditing, auditing devices are sometimes the firsttarget of an attacker and should be protected accordingly.a system's audit records, often called an audit trail, have other potentialuses besides establishing accountability. it may be possible, for example, toanalyze an audit trail for suspicious patterns of access and so detect improperbehavior by both legitimate users and masqueraders. the main drawbacks areprocessing and interpreting the audit data.systems may change constantly as personnel and equipment come and goand applications evolve. from a security standpoint, a changing system is notlikely to be an improving system. to take an active stand against gradualerosion of security measures, one may supplement a dynamically collectedaudit trail (which is useful in ferreting out what has happened) with static auditsthat check the configuration to see that it is not open for attack. static auditservices may check that software has not changed, that file access controls areproperly set, that obsolete user accounts have been turned off, that incomingand outgoing communications lines are correctly enabled, that passwords arehard to guess, and so on. aside from virus checkers, few static audit tools existin the market.the wellestablished practice of separation of duty specifies that importantoperations cannot be performed by a single person but instead require theagreement of (at least) two different people. separation of duty thus strengthenssecurity by preventing any singlehanded subversion of the controls. it can alsohelp reduce errors by providing for an independent check of one person'sactions by another.separation of duty is an example of a broader class of controls that attemptto specify who is trusted for a given purpose. this sort of control is generallyknown as user authorization. authorization determines whether a particularuser, who has been authenticated as the source of a request to do something, istrusted for that operation. authorization may also include controls on the timeat which something can be done (only during working hours) or the computerterminal from which it can be requested (only the one on the manager's desk).just as the goal of individual accountability requires a lowerlevelmechanism for user authentication, so also do authorization controls such asseparation of duty require a lowerlevel mechanism to ensureconcepts of information security58computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.that users have access only to the correct objects. inside the computer, theseenforcement mechanisms are usually called access control mechanisms.responding to breaches of securityrecovery controls provide the means to respond to, rather than prevent, asecurity breach. the use of a recovery mechanism does not necessarily indicatea system shortcoming; for some threats, detection and recovery may well bemore costeffective than attempts at total prevention. recovery from a securitybreach may involve taking disciplinary or legal action, notifying incidentallycompromised parties, or changing policies, for example. from a technicalstandpoint, a security breach has much in common with a failure that resultsfrom faulty equipment, software, or operations. usually some work will have tobe discarded, and some or all of the system will have to be rolled back to aclean state.security breaches usually entail more recovery effort than do acts of god.unlike proverbial lightning, breaches of security can be counted on to striketwice unless the route of compromise has been shut off. causes must be located.were passwords compromised? are backups clean? did some user activitycompromise the system by mistake? and major extra workšchanging allpasswords, rebuilding the system from original copies, shutting down certaincommunication links or introducing authentication procedures on them, orundertaking more user educationšmay have to be done to prevent a recurrence.developing policies and appropriate controlsideally a comprehensive spectrum of security measures would ensure thatthe confidentiality, integrity, and availability of computerbased systems wereappropriately maintained. in practice it is not possible to make ironcladguarantees. the only recipe for perfect security is perfect isolation: nothing in,nothing out. this is impractical, and so security policies will always reflecttradeoffs between cost and risk. the assets to be protected should becategorized by value, the vulnerabilities by importance, and the risks byseverity, and defensive measures should be installed accordingly. residualvulnerabilities should be recognized.planning a security program is somewhat like buying insurance. anorganization considers the following: the value of the assets being protected. the vulnerabilities of the system: possible types of compromise,concepts of information security59computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.of users as well as systems. what damage can the person in front of theautomated teller machine do? what about the person behind it?4 threats: do adversaries exist to exploit these vulnerabilities? do they havea motive, that is, something to gain? how likely is attack in each case? risks: the costs of failures and recovery. what is the worst credible kindof failure? possibilities are death, injury, compromise to national security,industrial espionage, loss of personal privacy, financial fraud, electionfraud. the organization's degree of risk aversion.thence follows a rough idea of expected losses. on the other side of theledger are these: available countermeasures (controls and security services), their effectiveness, and their direct costs and the opportunity costs of installing them.the security plans then become a business decision, possibly tempered bylegal requirements and consideration of externalities (see ''risks andvulnerabilities," below).ideally, controls are chosen as the result of careful analysis.5 in practice,the most important consideration is what controls are available. mostpurchasers of computer systems cannot afford to have a system designed fromscratch to meet their needs, a circumstance that seems particularly true in thecase of security needs. the customer is thus reduced to selecting from amongthe various preexisting solutions, with the hope that one will match theidentified needs.some organizations formalize the procedure for managing computerassociated risk by using a control matrix that identifies appropriate controlmeasures for given vulnerabilities over a range of risks. using such a matrix asa guide, administrators may better select appropriate controls for variousresources. a rough cut at addressing the problem is often taken: how muchbusiness depends on the system? what is the worst credible kind of failure, andhow much would it cost to recover? do available mechanisms address possiblecauses? are they costeffective?the computer industry can be expected to respond to clearly articulatedsecurity needs provided that such needs apply to a broad enough base ofcustomers. this has happened with the orange book visà vis the defensecommunityšbut slowly, because vendors were not convinced the customerbase was large enough to warrant accelerated investments in trust technology.however, for many of the management controls discussed above,concepts of information security60computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.there is not a clear, widely accepted articulation of how computer systemsshould be designed to support these controls, what sort of robustness is requiredin the mechanisms, and so on. as a result, customers for computer security arefaced with a "takeitorleaveit" marketplace. for instance, customers appear todemand passwordbased authentication because it is available, not becauseanalysis has shown that this relatively weak mechanism provides enoughprotection. this effect works in both directions: a service is not demanded if itis not available, but once it becomes available somewhere, it soon becomeswanted everywhere. see chapter 6 for a discussion of the marketplace.risks and vulnerabilitiesrisks arise because an attack could exploit some system vulnerability (see,for example, boxes 2.1 and 2.2). that is, each vulnerability of a system reflectsa potential threat, with corresponding risks. in a sampling of a collection of over3,000 cases of computer system abuse, drawn from the media and personalreporting, the following types of attack, listed roughly in order of decreasingfrequency, predominated (neumann and parker, 1989): misusing authority, through activities such as improper acquisition ofresources (reading of data, theft of programs), surreptitious modification,and denials of service, apparently by authorized users. masquerading, as in one user impersonating another. bypassing intended controls, by means such as password attacks andexploitation of trapdoors. these attacks typically exploit system flaws orhidden circumventive "features." setting up subsequent abuses such as trojan horses, logic bombs, orviruses. carrying out hardware and media abuses, such as physical attacks onequipment and scavenging of information from discarded media.(electronic interference and eavesdropping also belong in this class buthave not been widely detected.) using a computer system as an indirect aid in committing a criminal act,as in autodialing telephone numbers in search of answering modems,cracking another system's encrypted password files, or running an illicitbusiness. (for example, drug operations are becoming increasinglycomputerized.)the cases considered in the sampling cited above often involved multipleclasses of abuse. in attacking the national aeronautics and spaceadministration systems, the west german chaos computerconcepts of information security61computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.club masqueraded, bypassed access controls (partly by exploiting a subtleoperating system flaw), and used trojan horses to capture passwords. theinternet worm of november 1988 exploited weak password mechanisms anddesign and implementation flaws in mailhandling and informationserviceprograms to propagate itself from machine to machine (rochlis and eichin,1989; spafford, 1989a,b). personal computer pest programs typically use trojanhorse attacks, some with viruslike propagation.box 2.1 the wily hackerin august 1986, clifford stoll, an astronomer working at the lawrenceberkeley laboratory, detected an intruder, nicknamed him the wily hacker,and began to monitor his intrusions. over a period of 10 months, the wilyhacker attacked roughly 450 computers operated by the u.s. military and itscontractors, successfully gaining access to 30 of them. prior to detection, heis believed to have mounted attacks for as long as a year.although originally thought to be a local prankster, the wily hackerturned out to be a competent and persistent computer professional in westgermany, with alleged ties to the soviet kgb, and possibly withconfederates in germany.* it is assumed that the wily hacker was lookingfor classified or sensitive data on each of the systems he penetrated,although regulations prohibit the storage of classified data on the systems inquestion.looking for technological keywords and for passwords to other systems,the wily hacker exhaustively searched the electronic files and messageslocated on each system. he carefully concealed his presence on thecomputer systems and networks that he penetrated, using multiple entrypoints as necessary. he made longterm plans, in one instance establishinga trapdoor that he used almost a year later.the most significant aspect of the wily hacker incident is that theperpetrator was highly skilled and highly motivated. also notable is theinvolvement of a u.s. accomplice. tracking the wily hacker required thecooperation of more than 15 organizations, including u.s. authorities,german authorities, and private corporations. the treatment of the wilyhacker by german authorities left some in the united states unsatisfied,because under german law the absence of damage to german systems andthe nature of the evidence available diminished sentencing options.* he has been identified variously as mathias speer or marcus hess, a computer sciencestudent in hanover.sources: stoll (1988); markoff (1988a).the preceding summary of penetrations gives a good view of theconcepts of information security62computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.present situation. however, it is unwise to extrapolate from the present topredict the classes of vulnerability that will be significant in the future. asexpertise and interconnection increase and as control procedures improve, therisks and likely threats will change.6 for example, given recent events, thefrequency of trojan horse and virus attacks is expected to increase.interconnection results in the vulnerability of weak links endangering otherparts of an interconnected system. this phenomenon is particularly insidiouswhen different parts of a system fall under different managements with differentassessments of risk. for example, suppose computer center a used by studentsdetermines that the expected costs of recovery from plausible attacks do notjustify the costs of protective measures. the center has data connections to amore sensitive governmentsponsored research center b, to which somestudents have access. by computer eavesdropping at the studentcenter end, aninvisible intruder learns passwords to the research installation. somewhatparadoxically, the low guard kept at center a forces b to introduce morerigorous and costly measures to protect the supposedly innocuouscommunications with a than are necessary for genuinely sensitivecommunications with installations that are as cautious as b.such scenarios have been played out many times in real life. in savingmoney for itself, installation a has shifted costs to b, creating what economistscall an externality. at the very least, it seems, installation b should be aware ofthe security state of a before agreeing to communicate.system interconnection may even affect applications that do not involvecommunication at all: the risks of interconnection are borne not only by theapplications they benefit, but also by other applications that share the sameequipment. in the example given above, some applications at installation b mayneed to be apprised of the security state of installation a even though they neverovertly communicate with a.in some sectors, the recognition of interdependence has already affectedthe choice of safeguard. for example, a national funds transfer system maydepend on communications lines provided by a common carrier. it is commoncommercial practice to trust that common carriers transmit faithfully, but forfunds transfer such trust is judged to be imprudent, and cryptographic methodsare used to ensure that the carrier need not be trusted for the integrity of fundstransfer (although it is still trusted to ensure availability). the alternative wouldhave been to include the carriers within the trusted funds transfer system, andwork to ensure that they transmit faithfully.concepts of information security63computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.box 2.2 the internet wormthe internet, an international network of computer systems that hasevolved over the last decade, provides electronic mail, file transfer, andremote login capabilities. currently, the internet interconnects severalthousand individual networks (including government, commercial, andacademic networks) that connect some 60,000 computers. the internet hasbecome the electronic backbone for computer research, development, anduser communities.on november 2, 1988, the internet was attacked by a selfreplicatingprogram called a worm that spread within hours to somewhere between2,000 and 6,000 computer systemsšthe precise number remains uncertain.only systems (vax and sun 3) running certain types of unix (variants ofbsd 4) were affected.the internet worm was developed and launched by robert t. morris, jr.,who at the time was a graduate student at cornell university. morrisexploited security weaknesses (in the fingerd, rhosts, and sendmailprograms) in the affected versions of unix. the worm program itself did notcause any damage to the systems that it attacked in the sense that it did notsteal, corrupt, or destroy data and did not alter the systems themselves;however, its rapid proliferation and the ensuing confusion caused severedegradation in service and shut down some systems and networkconnections throughout the internet for two or three days, affecting sites thatwere not directly attacked. ironically, electronic mail messages with guidancefor containing the worm were themselves delayed because of networkcongestion caused by the worm's rapid replication.although morris argued that the worm was an experiment unleashedwithout malice, he was convicted of a felony (the conviction may beappealed) under the computer fraud and abuse act (cfaa) of 1986, thefirst such conviction. reflecting uncertainty about both the applicability of thecfaa and the nature of the incident, federal prosecutors were slow toinvestigate and bring charges in this case.the internet worm has received considerable attention by computingprofessionals, security experts, and the general public, thanks to theabundant publicity about the incident, the divided opinions within thecomputer community about the impact of the incident, and a generalrecognition that the internet worm incident has illuminated the potential fordamage from more dangerous attacks as society becomes more dependenton computer networks. the incident triggered the establishment of numerouscomputer emergency response teams (certs), starting with darpa'scert for the internet; a reevaluation of ethics for computer professionalsand users; and, at least temporarily, a general tightening of security incorporate and government networks.sources: comer (1988); spafford (1989a); rochlis and eichin (1989);and neumann (1990).concepts of information security64computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.in other sectors, including the research community, the design and themanagement of computermediated networks generate communicationvulnerabilities. in these systems (e.g., bitnet) messages travel lengthy pathsthrough computers in the control of numerous organizations of which thecommunicants are largely unaware, and for which message handling is not acentral business concern. responsibility for the privacy and integrity ofcommunications in these networks is so diffuse as to be nonexistent. unlikecommon carriers, these networks warrant no degree of trust. this situation isunderstood by only some of these networks' users, and even they may gambleon the security of their transmissions in the interests of convenience andreduced expenses.securing the whole systembecause security is a weaklink phenomenon, a security program must bemultidimensional. regardless of security policy goals, one cannot completelyignore any of the three major requirementsšconfidentiality, integrity, andavailabilityšwhich support one another. for example, confidentiality is neededto protect passwords. passwords in turn promote system integrity by controllingaccess and providing a basis for individual accountability. confidentialitycontrols themselves must be immune to tamperingšan integrity consideration.and in the event that things do go wrong, it must be possible for administrativeand maintenance personnel to step in to fix thingsšan availability concern.a system is an interdependent collection of components that can beconsidered as a unified whole. a computer operating system, an applicationsuch as a computerized payroll, a local network of engineering workstations, orthe nationwide network for electronic funds transfer each can be considered as asystemšand any one system may depend on others. all of these involvephysical elements and people as well as computers and software. physicalprotection includes environmental controls such as guards, locks, doors, andfences as well as protection against and recovery from fire, flood, and othernatural hazards.although a security program must be designed from a holistic perspective,the program itself need notšindeed should notšbe monolithic. it is best tooperate on a divideandconquer principle, reflecting the classical managementcontrol principle of separation of duty. a system made of mutually distrustfulparts should be stronger than a simple trusted system. on a large scale,communications links define natural boundaries of distrust. within a singlesystem extra strength may be gained by isolating authentication functions andauditingconcepts of information security65computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.records in physically separate, more rigorously controlled hardware. suchisolation of function is universal in serious cryptography.technology alone cannot provide security. in particular, an informationsecurity program is of little avail if its users do not buy into it. the programmust be realistic and maintain the awareness and commitment of allparticipants. further, management actions must signal that security matters.when rewards go only to visible results (e.g., meeting deadlines or savingcosts), attention will surely shift away from securityšuntil disaster strikes.appendix 2.1šprivacyconcern for privacy arises in connection with the security of computersystems in two disparate ways: the need to protect personal information about people that is kept incomputer systems; and the need to ensure that employees of an organization are complying withthe organization's policies and procedures.the first need supports privacy; the institution of policies and mechanismsfor confidentiality should strengthen it. the second, however, is a case in whichneed is not aligned with privacy; strong auditing or surveillance measures maywell infringe on the privacy of those whose actions are observed. it is importantto understand both aspects of privacy.protection of information about individualsthe need to protect personal information is addressed in several laws,notably including the privacy act of 1974 (p.l. 93œ579), which was enactedduring a period of international concern about privacy triggered by advancingcomputerization of personal data.7 a number of authors who have written onthe subject believe that privacy protections are stronger in other countries (turn,1990; flaherty, 1990).the privacy act is based on five major principles that have been generallyaccepted as basic privacy criteria in the united states and europe:1. there must be no personal data record keeping system whose veryexistence is secret.2. there must be a way for individuals to find out what information aboutthem is on a record and how it is used.3. there must be a way for individuals to prevent informationconcepts of information security66computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.obtained about them for one purpose from being used or madeavailable for other purposes without their consent.4. there must be a way for individuals to correct or amend a record ofidentifiable information about them.5. any organization creating, maintaining, using, or disseminatingrecords of identifiable personal data must assure that data are used asintended and must take precautions to prevent misuse of the data.even where most organizations make a reasonable, conscientious effort toprotect the privacy of personal information residing in their computing systems,compromisable system and data access controls often allow intruders to violatepersonal privacy. for example, a survey of 178 federal agencies by the generalaccounting office revealed 34 known breaches in computerized systemscontaining personal information in fiscal years 1988 and 1989; 30 of thoseincidents involved unauthorized access to the information by individualsotherwise authorized to use the systems (gao, 1990e). frequent reports of"hacker" invasions into creditreporting databases and patients' medical recordsprovide ample evidence of the general lack of appropriate protection of personalinformation in computer systems. also, some applications in and of themselvesappear to undermine the privacy act's principle that individuals should be ableto control information about themselves.8 as noted in a recent newspapercolumn,most of us have no way of knowing all the databases that contain informationabout us. in short, we are losing control over the information about ourselves.many people are not confident about existing safeguards, and few areconvinced that they should have to pay for the benefits of the computer agewith their personal freedoms. (lewis, 1990)because of concerns about privacy, companies will increasingly needsecure systems to store information. indeed, in canada, governmentalregulation concerning the requirements for privacy of information aboutindividuals contributed to an ongoing effort to extend the u.s. orange book toinclude specific support for privacy policy.employee privacy in the workplacean employer's need to ensure that employees comply with policies andprocedures requires some checking by management on employees' activitiesinvolving the use of company computing resources; how much and what kind ofchecking are subject to debate.9 a common management premise is that if apolicy or procedure is not enforced, it will eventually not be obeyed, leading toan erosion of respect for and compliance with other policies and procedures.for instance,concepts of information security67computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.consider a policy stating that company computing resources will be used onlyfor proper business purposes. users certify upon starting their jobs (or uponintroduction of the policy) that they understand and will comply with this policyand others. random spot checks of user files by information security analystsmay be conducted to ensure that personal business items, games, and so on, arenot put on company computing resources. disciplinary action may result whenviolations of policy are discovered.the above situation does not, in itself, relate to security. however, onemethod proposed to increase the level of system security involves monitoringworkers' actions to detect, for example, patterns of activity that suggest that aworker's password has been stolen. this level of monitoring provides increasedopportunity to observe all aspects of worker activity, not just securityrelatedactivity, and to significantly reduce a worker's expectation for privacy at work.some managers argue that a worker, while performing workrelatedactivity, should expect arbitrary supervisory observation and review and thatthere is no expectation of privacy in that context. this argument combinesconsideration of privacy with considerations of management style andphilosophy, which are beyond the scope of this report. however, what isrelevant to this report is the fact that computer and communicationstechnologies facilitate greater monitoring and surveillance of employees andthat needs for computer and communications security motivate monitoring andsurveillance, some of which may use computer technology. as thecongressional office of technology assessment has noted, the effects ofcomputerbased monitoring depend on the way it is used (ota, 1987a).there are complex tradeoffs among privacy, management control, andmore general security controls. how, for example, can management ensure thatits computer facilities are being used only for legitimate business purposes if thecomputer system contains security features that limit access to the files ofindividuals? typically, a system administrator has access to everything on asystem. to prevent abuse of this privilege, a secure audit trail may be used. thegoal is to prevent the interaction of the needs for control, security, and privacyfrom inhibiting the adequate achievement of any of the three.note that by tracing or monitoring the computer actions of individuals, onecan violate the privacy of persons who are not in an employee relationship butare more generally clients of an organization or citizens of a country. forexample, the wall street journal reported recently that customer data enteredby a travel agency into a major airline reservation system was accessible to andused by other travel service firms without the knowledge of the customer orconcepts of information security68computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.the travel agency (winans, 1990). computer systems as a mechanism provideno protection for people in these situations; as was observed above, computers,even very secure computers, are only a mechanism, not a policy. indeed, verysecure systems may actually make the problem worse, if the presence of thesemechanisms falsely encourages people to entrust critical information to suchsystems.there is an important distinction between policy and mechanism. acomputer system is a mechanism, but if there is no enforceable policy, amechanism provides no protection. only in the presence of an enforceablepolicy can any protection or assurance occur. while five basic principles thatmake up a recognized privacy policy are summarized above, security, as it isdiscussed in this report, does not provide or enforce such a policy, except in thenarrow sense of protecting a system from hostile intruders. protecting a system(or the information it contains) from the owner of the system is a totallydifferent problem, which will become increasingly important as we proceed to astill greater use of computers in our society.appendix 2.2šinformal survey to assess securityrequirementsin april 1989 informal telephone interviews were conducted by acommittee member with the information security officers of 30 privatecompanies in the aerospace, finance, food and beverage, manufacturing,petrochemical, retail, and utilities industries. within these categories an evendistribution of companies was achieved, and interviewees were distributedgeographically. individuals were asked what basic security features should bebuilt into vendor systems (essential features)šwhat their requirements wereand whether those requirements were being met. their unanimous opinion wasthat current vendor software does not meet their basic security needs.the survey addressed two categories of security measures: prevention anddetection. within the prevention category the focus was on three areas:computers, terminals, and telecommunications and networking.individuals were asked to consider 40 specific security measures. for each,they were asked whether the measure should be built into vendor systems as amandatory (essential) item, be built in as an optional item, or not be built in.user identificationall of the interviewees believed that a unique identification (id) for eachuser and automatic suspension of an id for a certain numberconcepts of information security69computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.of unauthorized access attempts were essential. the capability to prevent thesimultaneous use of an id was considered essential by 90 percent of theindividuals interviewed. a comment was that this capability should becontrollable based either on the id or the source of the access.eightythree percent of the interviewees agreed it is essential that the date,time, and place of last use be displayed to the user upon signon to the system.a comment was that this feature should also be available at other times. thesame number required the capability to assign to the user an expiration date forauthorization to access a system. comments on this item were that the ability tospecify a future active date for ids was needed and that the capability to let thesystem administrator know when an id was about to expire was required.seventythree percent thought that the capability to limit system access tocertain times, days, dates, and/or from certain places was essential.user verification or authenticationall interviewees believed that preventing the reuse of expired passwords,having the system force password changes, having the password alwaysprompted for, and having the id and password verified at signon time were allessential security measures.ninetyseven percent judged as essential the capabilities to implement apassword of six or more alphanumeric characters and to have passwords storedencrypted on the system. eightyseven percent believed that an automatic checkto eliminate easy passwords should be an essential feature, although oneindividual thought that, in this case, it would be difficult to know what to checkfor.sixty percent saw the capability to interface with a dynamic passwordtoken as an essential feature. one recommendation was to investigate the use oficons that would be assigned to users as guides to selecting meaningful (easilyremembered) passwords. thirtythree percent considered a random passwordgenerator essential; 7 percent did not want one.file access controlall interviewees considered it essential to be able to limit access to files,programs, and databases. only 60 percent thought that the capability to limitaccess to a specified time or day should be essential. although all informationsecurity officers of financial organizationsconcepts of information security70computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.thought such a capability should be essential, at least some representatives fromall other categories of businesses preferred that such a feature be optional.eightythree percent agreed that a virus detection and protection capabilityand the ability to purge a file during deletion were essential features. an addedcomment was that vendors should be required to certify a product as being freeof viruses or trapdoors. seventythree percent considered the capability toencrypt sensitive data to be mandatory, but one respondent was opposed to thatfeature because it could complicate disaster recovery (i.e., one might not be ableto access such data in an emergency during processing at an alternate site).ninetyfive percent thought it should be essential to require the execution ofproduction programs from a secure production library and also, if usingencryption, to destroy the plaintext during the encryption process.terminal controlsall interviewees agreed that preventing the display of passwords onscreens or reports should be essential. ninetyfive percent favored having anautomated logoff/timeout capability as a mandatory feature. a comment wasthat it should be possible to vary this feature by id.identification of terminals was a capability that 87 percent consideredessential, but only twothirds felt that a terminal lock should be included in theessential category.an additional comment was that a token port (for dynamic passwordinterface) should be a feature of terminals.telecommunications and networkingmore than 95 percent of the interviewees believed that network securitymonitoring; bridge, router, and gateway filtering; and dialin user authenticationshould be essential features. also, 90 percent wanted a modemlocking deviceas a mandatory feature. eightythree to eightyseven percent of intervieweeswanted security modems (callback authentication), data encryption, automatedencryption and decryption capabilities, and the ability to automaticallydisconnect an unneeded modem to be regarded as essential.additional comments in this area addressed the need for messageauthentication and nonrepudiation as security features.concepts of information security71computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.detection measuresall interviewees believed that audit trails identifying invalid accessattempts and reporting id and terminal source identification related to invalidaccess attempts were essential security measures. likewise, all agreed thatviolation reports (including date, time, service, violation type, id, data sets, andso forth) and the capability to query a system's log to retrieve selected data wereessential features.eightythree percent were in favor of network intrusion detection, arelatively new capability, as an essential item. however, everyone also agreedon the need for improved reporting of intrusions.general comments and summarygeneral suggestions made in the course of the interviews included thefollowing: make requirements general rather than specific so that they can apply toall kinds of systems. make security transparent to the user. make sure that ''mandatory" really means mandatory. seek opinions from those who pay for the systems.in summary, it was clearly the consensus that basic information securityfeatures should be required components that vendors build into informationsystems. some control of the implementation of features should be available toorganizations so that flexibility to accommodate special circumstances isavailable.interviewees indicated that listing essential (musthave and mustuse) andoptional security features in an accredited standards document would be veryuseful for vendors and procurement officers in the private sector. vendors coulduse the criteria as a measure of how well their products meet requirements forinformation security and the needs of the users. procurement officers could usethe criteria as benchmarks in evaluating different vendors' equipment during thepurchasing cycle. vendors could also use the criteria as a marketing tool, asthey currently use the orange book criteria. these comments are supportive ofthe gssp concept developed by this committee.notes1. some documentation can be found in the defense advanced research projects agency'scomputer emergency response team advisories, which are distributed to system managers and in avariety of electronic newsletters and bulletin boards.concepts of information security72computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.2. the mechanisms for carrying out such procedures are called mandatory access controls by thedod.3. such mechanisms are called discretionary access controls by the dod, and userdirected, identitybased access controls by the international organization for standards. also, the ownerbasedapproach stands in contrast with the more formal, centrally administered clearance or accessauthorization process of the national security community.4. there are many kinds of vulnerability. authorized people can misuse their authority. one usercan impersonate another. one breakin can set up the conditions for others, for example, byinstalling a virus. physical attacks on equipment can compromise it. discarded media can bescavenged. an intruder can get access from a remote system that is not well secured, as happenedwith the internet worm.5. although it might be comforting to commend the use of, or research into, quantitative riskassessment as a planning tool, in many cases little more than a semiquantitative or checklisttypeapproach seems warranted. risk assessment is the very basis of the insurance industry, which, it canbe noted, has been slow to offer computer security coverage to businesses or individuals (seechapter 6, appendix 6.2, "insurance"). in some cases (e.g., the risk of damage to the records of asingle customer's accounts) quantitative assessment makes sense. in general, however, riskassessment is a difficult and complex task, and quantitative assessment of myriad qualitativelydifferent, lowprobability, highimpact risks has not been notably successful. the nuclear industry isa case in point.6. the extent of interconnection envisioned for the future underscores the importance of planningfor interdependencies. for example, william mitchell has laid out a highly interconnected vision:through open systems interconnection (osi), businesses will rely on computer networks as much asthey depend on the global telecom network. enterprise networks will meet an emerging need: theywill allow any single computer in any part of the world to be as accessible to users as any telephone.osi networking capabilities will give every networked computer a unique and easily accessibleaddress. individual computer networks will join into a single cohesive system in much the same wayas independent telecom networks join to form one global service. (mitchell, 1990, pp. 69œ72)7. other federal privacy laws include the fair credit reporting act of 1970 (p.l. 91œ508), thefamily educational rights and privacy act of 1974 (20 u.s.c. 1232g), the right of financialprivacy act of 1978 (11 u.s.c. 1100 et seq.), the electronic funds transfer act of 1978 (15 u.s.c.1693, p.l. 95œ200), the cable communications policy act of 1984 (48 u.s.c. 551), the electroniccommunications privacy act of 1986 (18 u.s.c. 2511), and the computer matching and privacyprotection act of 1988 (5 u.s.c. 552a note) (turn, 1990). states have also passed laws to protectprivacy.8. this point was made by the congressional office of technology assessment in an analysis offederal agency use of electronic record systems for computer matching, verification, and profiling(ota, 1986b).9. recent cases about management perusing electronic mail messages that senders and receivers hadbelieved were private amplify that debate (communications week, 1990a).concepts of information security73computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.3technology to achieve secure computersystemsa reasonably complete survey of the technology needed to protectinformation and other resources controlled by computer systems, this chapterdiscusses how such technology can be used to make systems secure. it explainsthe essential technical ideas, gives the major properties of relevant techniquescurrently known, and tells why they are important. suggesting developmentsthat may occur in the next few years, it provides some of the rationale for theresearch agenda set forth in chapter 8.appendix b of this report discusses in more detail several topics that areeither fundamental to computer security technology or of special current interestšincluding how some important things (such as passwords) work and why theydo not work perfectly.this discussion of the technology of computer security addresses twomajor concerns:1. what do we mean by security?2. how do we get security, and how do we know when we have it?the first involves specification of security and the services that computersystems provide to support security. the second involves implementation ofsecurity, and in particular the means of establishing confidence that a systemwill actually provide the security the specifications promise. each topic isdiscussed according to its importance for the overall goal of providing computersecurity, and not according to how much work has already been done on thattopic.this chapter discusses many of the concepts introduced in chapter 2, butin more detail. it examines the technical process of relating computermechanisms to higherlevel controls and policies, a processtechnology to achieve secure computer systems74computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.that requires the development of abstract security models and supportingmechanisms. although careful analysis of the kind carried out in this chaptermay seem tedious, it is a necessary prerequisite to ensuring the security ofsomething as complicated as a computer system. ensuring security, likeprotecting the environment, requires a holistic approach; it is not enough tofocus on the problem that caused trouble last month, because as soon as thatdifficulty is resolved, another will arise.specification vs. implementationthe distinction between what a system does and how it does it, betweenspecification and implementation, is basic to the design and analysis ofcomputer systems. a specification for a system is the meeting point between thecustomer and the builder. it says what the system is supposed to do. this isimportant to the builder, who must ensure that what the system actually doesmatches what it is supposed to do. it is equally important to the customer, whomust be confident that what the system is supposed to do matches what hewants. it is especially critical to know exactly and completely how a system issupposed to support requirements for security, because any mistake can beexploited by a malicious adversary.specifications can be written at many levels of detail and with manydegrees of formality. broad and informal specifications of security are calledsecurity policies1 (see chapter 2), examples of which include the following: (1)"confidentiality: information shall be disclosed only to people authorized toreceive it." (2) "integrity: data shall be modified only according to establishedprocedures and at the direction of properly authorized people."it is possible to separate from the whole the part of a specification that isrelevant to security. usually a whole specification encompasses much morethan the securityrelevant part. for example, a whole specification usually saysa good deal about price and performance. in systems for which confidentialityand integrity are the primary goals of security policies, performance is notrelevant to security because a system can provide confidentiality and integrityregardless of how well or badly it performs. but for systems for whichavailability and integrity are paramount, performance specifications may berelevant to security. since security is the focus of this discussion,"specification" as used here should be understood to describe only what isrelevant to security.a secure system is one that meets the particular specifications meant toensure security. since many different specifications are possible,technology to achieve secure computer systems75computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.there cannot be any absolute notion of a secure system. an example from arelated field clarifies this point. we say that an action is legal if it meets therequirements of the law. since different jurisdictions can have different sets oflaws, there cannot be any absolute notion of a legal action; what is legal underthe laws of britain may be illegal in the united states.a system that is believed to be secure is called trusted. of course, a trustedsystem must be trusted for something; in the context of this report it is trusted tomeet security specifications. in some other context such a system might betrusted to control a shuttle launch or to retrieve all the 1988 court opinionsdealing with civil rights.policies express a general intent. of course, they can be more detailed thanthe very general ones given as examples above; for instance, the following is arefinement of the first policy: "salary confidentiality: individual salaryinformation shall be disclosed only to the employee, his superiors, andauthorized personnel people."but whether general or specific, policies contain terms that are notprecisely defined, and so it is not possible to tell with absolute certainty whethera system satisfies a policy. furthermore, policies specify the behavior of peopleand of the physical environment as well as the behavior of machines, so that itis not possible for a computer system alone to satisfy them. technology forsecurity addresses these problems by providing methods for the following: integrating a computer system into a larger system, comprising people anda physical environment as well as computers, that meets its securitypolicies; giving a precise specification, called a security model, for the securityrelevant behavior of the computer system; building, with components that provide and use security services, asystem that meets the specifications; and establishing confidence, or assurance, that a system actually does meet itsspecifications.this is a tall order that at the moment can be only partially filled. the firsttwo actions are discussed in the section below titled "specification," the last twoin the following section titled "implementation." services are discussed in bothsections to explain both the functions being provided and how they areimplemented.specification: policies, models, and servicesthis section deals with the specification of security. it is based on thetaxonomy of security policies given in chapter 2. there are only a few highlydeveloped security policies, and research is needed totechnology to achieve secure computer systems76computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.develop additional policies (see chapter 8), especially in the areas of integrityand availability. each of the highly developed policies has a corresponding(formal) security model, which is a precise specification of how a computersystem should behave as part of a larger system that implements a policy.implementing a security model requires mechanisms that provide particularsecurity services. a small number of fundamental mechanisms have beenidentified that seem adequate to implement most of the highly developedsecurity policies currently in use.the simple example of a traffic light illustrates the concepts of policy andmodel; in this example, safety plays the role of security. the light is part of asystem that includes roads, cars, and drivers. the safety policy for the completesystem is that two cars should not collide. this is refined into a policy thattraffic must not move in two conflicting directions through an intersection at thesame time. this policy is translated into a safety model for the traffic light itself(which plays a role analogous to that of a computer system within a completesystem): two green lights may never appear in conflicting traffic patternssimultaneously. this is a simple specification. observe that the completespecification for a traffic light is much more complex; it provides for the abilityto set the duration of the various cycles, to synchronize the light with othertraffic lights, to display different combinations of arrows, and so forth. none ofthese details, however, is critical to the safety of the system, because they donot bear directly on whether or not cars will collide. observe also that for thewhole system to meet its safety policy, the light must be visible to the drivers,and they must understand and obey its rules. if the light remains red in alldirections it will meet its specification, but the drivers will lose patience andstart to ignore it, so that the entire system may not support a policy of ensuringsafety.an ordinary library affords a more complete example (see appendix b ofthis report) that illustrates several aspects of computer system security in acontext that does not involve computers.policiesa security policy is an informal specification of the rules by which peopleare given access to a system to read and change information and to useresources. policies naturally fall into a few major categories:1. confidentiality: controlling who gets to read information;2. integrity: assuring that information and programs are changed only in aspecified and authorized manner; andtechnology to achieve secure computer systems77computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.3. availability: assuring that authorized users have continued access toinformation and resources.two orthogonal categories can be added:4. resource control: controlling who has access to computing, storage, orcommunication resources (exclusive of data); and5. accountability: knowing who has had access to information orresources.chapter 2 describes these categories in detail and discusses how anorganization that uses computers can formulate a security policy by drawingelements from all these categories. the discussion below summarizes thismaterial and supplements it with some technical details.security policies for computer systems generally reflect longstandingpolicies for the security of systems that do not involve computers. in the case ofnational security these are embodied in the information classification andpersonnel clearance system; for commercial computing they come fromestablished accounting and management control practices.from a technical viewpoint, the most fully developed policies are thosethat have been developed to ensure confidentiality. they reflect the concerns ofthe national security community and are derived from department of defense(dod) directive 5000.1, the basic directive for protecting classified information.2the dod computer security policy is based on security levels. given twolevels, one may be lower than the other, or the two may not be comparable. thebasic principle is that information can never be allowed to leak to a lower level,or even to a level that is not comparable. in particular, a program that has "readaccess" to data at a higher level cannot simultaneously have "write access" tolowerlevel data. this is a rigid policy motivated by a lack of trust inapplication programs. in contrast, a person can make an unclassified telephonecall even though he may have classified documents on his desk, because he istrusted to not read the document over the telephone. there is no strong basis forplacing similar trust in an arbitrary computer program.a security level or compartment consists of an access level (either topsecret, secret, confidential, or unclassified) and a set of categories (e.g., criticalnuclear weapon design information (cnwdi), north atlantic treatyorganization (nato), and so on). the access levels are ordered (top secret,highest; unclassified, lowest). the categories, which have unique access andprotection requirements, are not ordered, but sets of categories are ordered byinclusion: one set is lower than another if every category in the first is includedin the second. onetechnology to achieve secure computer systems78computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.security level is lower than another, different level if it has an equal or loweraccess level and an equal or lower set of categories. thus [confidential; nato]is lower than both [confidential; cnwdi, nato] and [secret; nato]. giventwo levels, it is possible that neither is lower than the other. thus [secret;cnwdi] and [confidential; nato] are not comparable.every piece of information has a security level (often called its label).normally information is not permitted to flow downward: information at onelevel can be derived only from information at equal or lower levels, never frominformation that is at a higher level or is not comparable. if information iscomputed from several inputs, it has a level that is at least as high as any of theinputs. this rule ensures that if information is stored in a system, anythingcomputed from it will have an equal or higher level. thus the classificationnever decreases.the dod computer security policy specifies that a person is cleared to aparticular security level and can see information only at that, or a lower, level.since anything seen can be derived only from other information categorized asbeing at that level or lower, the result is that what a person sees can depend onlyon information in the system at his level or lower. this policy is mandatory:except for certain carefully controlled downgrading or declassificationprocedures, neither users nor programs in the system can break the rules orchange the security levels. as chapter 2 explains, both this and otherconfidentiality policies can also be applied in other settings.integrity policies have not been studied as carefully as confidentialitypolicies, even though some sort of integrity policy governs the operation ofevery commercial dataprocessing system. work in this area (clark and wilson,1987) lags work on confidentiality by about 15 years. nonetheless, interest isgrowing in workable integrity policies and corresponding mechanisms,especially since such mechanisms provide a sound basis for limiting the damagecaused by viruses, selfreplicating software that can carry hidden instructions toalter or destroy data.the most highly developed policies to support integrity reflect theconcerns of the accounting and auditing community for preventing fraud. theessential notions are individual accountability, auditability, separation of duty,and standard procedures. another kind of integrity policy is derived from theinformationflow policy for confidentiality applied in reverse, so thatinformation can be derived only from other information of the same or a higherintegrity level (biba, 1975). this particular policy is extremely restrictive andthus has not been applied in practice.policies categorized under accountability have usually been formulatedtechnology to achieve secure computer systems79computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.as part of confidentiality or integrity policies. accountability has not receivedindependent attention.in addition, very little work has been done on security policies related toavailability. absent this work, the focus has been on the practical aspects ofcontingency planning and recoverability.modelsto engineer a computer system that can be used as part of a larger systemthat implements a security policy, and to decide unambiguously whether such acomputer system meets its specification, an informal, broadly stated policy mustbe translated into a precise model. a model differs from a policy in two ways:1. it describes the desired behavior of a computer system's mechanisms,not that of the larger system that includes people.2. it is precisely stated in formal language that resolves the ambiguities ofenglish and makes it possible, at least in principle, to give amathematical proof that a system satisfies the model.two models are in wide use. one, based on the dod computer securitypolicy, is the flow model; it supports a certain kind of confidentiality policy.the other, based on the familiar idea of stationing a guard at an entrance, is theaccess control model; it supports a variety of confidentiality, integrity, andaccountability policies. there are no models that support availability policies.flow modelthe flow model is derived from the dod computer security policydescribed above. in this model (denning, 1976) each piece of data in the systemvisible to a user or an application program is held in a container called anobject. each object has an associated security level. an object's level indicatesthe security level of the data it contains. data in one object is allowed to affectanother object only if the source object's level is lower than or equal to thedestination object's level. all the data within a single object have the same leveland hence can be manipulated freely.the flow model ensures that information at a given security level flowsonly to an equal or higher level. data is not the same as information; forexample, an encrypted message contains data, but it conveys no informationunless one knows the encryption key or can break the encryption system.unfortunately, data is all the computer can understand. by preventing an objectat one level from beingtechnology to achieve secure computer systems80computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.affected in any way by data that is not at an equal or lower level, the flow modelensures that information can flow only to an equal or higher level inside thecomputer system. it does this very conservatively and thus forbids many actionsthat would not in fact cause any information to flow improperly.a more complicated version of the flow model (which is actually the basisof the rules in the orange book) separates objects into active subjects that caninitiate operations and passive objects that simply contain data, such as a file, apiece of paper, or a display screen. data can flow only between an object and asubject; flow from object to subject is called a read operation, and flow fromsubject to object is called a write operation. now the rules are that a subject canonly read from an object at an equal or lower level, and can only write to anobject at an equal or higher level.not all possible flows in a system look like read and write operations.because the system is sharing resources among objects at different levels, it ispossible for information to flow on what are known as covert channels(lampson, 1973; ieee, 1990a). for example, a highlevel subject might be ableto send a little information to a lowlevel subject by using up all the disk spaceif it learns that a surprise attack is scheduled for next week. when the lowlevelsubject finds itself unable to write a file, it has learned about the attack (or atleast received a hint). to fully realize the intended purpose of a flow model, it isnecessary to identify and attempt to close all the covert channels, although totalavoidance of covert channels is generally impossible due to the need to shareresources.to fit this model of a computer system into the real world, it is necessaryto account for people. a person is cleared to some level of permitted access.when he identifies himself to the system as a user present at some terminal, hecan set the terminal's level to any equal or lower level. this ensures that the userwill never see information at a higher level than his clearance allows. if the usersets the terminal level lower than the level of his clearance, he is trusted not totake highlevel information out of his head and introduce it into the system.although not logically required, the flow model policy has generally beenviewed as mandatory; neither users nor programs in a system can break theflow rule or change levels. no real system can strictly follow this rule, sinceprocedures are always needed for declassifying data, allocating resources, andintroducing new users, for example. the access control model is used for thesepurposes, among others.access control modelthe access control model is based on the idea of stationing a guardtechnology to achieve secure computer systems81computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.in front of a valuable resource to control who has access to it. this modelorganizes the system into objects: entities that respond to operations by changing their state,providing information about their state, or both; subjects: active objects that can perform operations on objects; and operations: the way that subjects interact with objects.the objects are the resources being protected; an object might be adocument, a terminal, or a rocket. a set of rules specifies, for each object andeach subject, what operations that subject is allowed to perform on that object.a reference monitor acts as the guard to ensure that the rules are followed(lampson, 1985). an example of a set of access rules follows:subjectoperationobjectsmithread file''1990 pay raises"whitesend "hello"terminal 23process 1274rewindtape unit 7blackfire three roundsbow gunjonespay invoice 432567account q34there are many ways to express the access rules. the two most popular areto attach to each subject a list of the objects it can access (a capability list), or toattach to each object a list of the subjects that can access it (an access controllist). each list also identifies the operations that are allowed. most systems usesome combination of these approaches.usually the access rules do not mention each operation separately. insteadthey define a smaller number of "rights" (often called permissions)šforexample, read, write, and searchšand grant some set of rights to each (subject,object) pair. each operation in turn requires some set of rights. in this way anumber of different operations, all requiring the right to read, can readinformation from an object. for example, if the object is a text file, the right toread may be required for such operations as reading a line, counting the numberof words, and listing all the misspelled words.one operation that can be done on an object is to change which subjectscan access the object. there are many ways to exercise this control, dependingon what a particular policy is. when a discretionary policy applies, for eachobject an "owner" or principal is identified who can decide without anyrestrictions who can do what to the object. when a mandatory policy applies,the owner can make thesetechnology to achieve secure computer systems82computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.decisions only within certain limits. for example, a mandatory flow policyallows only a security officer to change the security level of an object, and theflow model rules limit access. the principal controlling the object can usuallyapply further limits at his discretion.the access control model leaves open what the subjects are. mostcommonly, subjects are users, and any active entity in the system is treated asacting on behalf of some user. in some systems a program can be a subject in itsown right. this adds a great deal of flexibility, because the program canimplement new objects using existing ones to which it has access. such aprogram is called a protected subsystem; it runs as a subject different from theprincipal invoking it, usually one that can access more objects. the securityservices used to support creation of protected subsystems also may be used toconfine suspected trojan horses or viruses, thus limiting the potential fordamage from such programs. this can be done by running a suspect program asa subject that is different from the principal invoking it, in this case a subjectthat can access fewer objects. unfortunately, such facilities have not beenavailable in most operating systems.the access control model can be used to realize both secrecy and integritypolicies, the former by controlling read operations and the latter by controllingwrite operations, and others that change the state. this model supportsaccountability, using the simple notion that every time an operation is invoked,the identity of the subject and the object as well as the operation should berecorded in an audit trail that can later be examined. difficulties in makingpractical use of such information may arise owing to the large size of an audittrail.servicesbasic security services are used to build systems satisfying the policiesdiscussed above. directly supporting the access control model, which in turncan be used to support nearly all the policies discussed, these services are asfollows: authentication: determining who is responsible for a given request orstatement,3 whether it is, "the loan rate is 10.3 percent," or "read file'memo to mike,'" or "launch the rocket.'' authorization: determining who is trusted for a given purpose, whether itis establishing a loan rate, reading a file, or launching a rocket. auditing: recording each operation that is invoked along with the identityof the subject and object, and later examining these records.given these services, it is easy to implement the access controltechnology to achieve secure computer systems83computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.model. whenever an operation is invoked, the reference monitor usesauthentication to find out who is requesting the operation and then usesauthorization to find out whether the requester is trusted for that operation. ifso, the reference monitor allows the operation to proceed; otherwise, it cancelsthe operation. in either case, it uses auditing to record the event.authenticationto answer the question, who is responsible for this statement?, it isnecessary to know what sort of entities can be responsible for statements; wecall these entities principals. it is also necessary to have a way of naming theprincipals that is consistent between authentication and authorization, so thatthe result of authenticating a statement is meaningful for authorization.a principal is a (human) user or a (computer) system. a user is a person,but a system requires some explanation. a system comprises hardware (e.g., acomputer) and perhaps software (e.g., an operating system). a system candepend on another system; for example, a userquery process depends on adatabase management system, which depends on an operating system, whichdepends on a computer. as part of authenticating a system, it may be necessaryto verify that the systems it depends on are trusted.in order to express trust in a principal (e.g., to specify who can launch therocket), one must be able to give the principal a name. the name must beindependent of any information (such as passwords or encryption keys) thatmay change without any change in the principal itself. also, it must bemeaningful, both when access is granted and later when the trust being grantedis reviewed to see whether that trust is still warranted. a naming system must be: complete: every principal has a name; it is difficult or impossible toexpress trust in a nameless principal. unambiguous: the same name does not refer to two different principals;otherwise it is impossible to know who is being trusted. secure: it is easy to tell which other principals must be trusted in order toauthenticate a statement from a named principal.in a large system, naming must be decentralized to be manageable.furthermore, it is neither possible nor wise to rely on a single principal that istrusted by every part of the system. since systems as well as users can beprincipals, systems as well as users must be able to have names.one way to organize a decentralized naming system is as a hierarchy,technology to achieve secure computer systems84computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.following the model of a treestructured file system like the one in unix or ms/dos, two popular operating systems. the consultative committee oninternational telephony and telegraphy (ccitt) x.500 standard for namingdefines such a hierarchy (ccitt, 1989b); it is meant to be suitable for namingevery principal in the world. in this scheme an individual can have a name like"us/gov/state/ jamesbaker." such a naming system can be complete; thereis no shortage of names, and registration can be made as convenient as desired.it is unambiguous provided each directory is unambiguous.the ccitt also defines a standard (x.509) for authenticating a principalwith an x.500 name; the section on authentication techniques below discusseshow this is done (ccitt, 1989b). note that an x.509 authentication mayinvolve more than one agent. for example, agent a may authenticate agent b,who in turn authenticates the principal.a remaining issue is exactly who should be trusted to authenticate a givenname. in the x.509 authentication framework, typically, principals trust agentsclose to them in the hierarchy. a principal is less likely to trust agents fartherfrom it in the hierarchy, whether those agents are above, below, or in entirelydifferent branches of the tree. if a system at one point in the tree wants toauthenticate a principal elsewhere, and if there is no one agent that canauthenticate both, then the system must establish a chain of trust throughmultiple agents.4often a principal wants to act with less than its full authority, in order toreduce the damage that can be done in case of a mistake. for this purpose it isconvenient to define additional principals, called roles, to provide a way ofauthorizing a principal to play a role, and to allow the principal to make astatement using any role for which it is authorized. for example, a systemadministrator might have a "normal" role and a "powerful" role. theauthentication service then reports that a statement was made by a role ratherthan by the original principal, after verifying both that the statement came fromthe original principal and that he was authorized to play that role. (it is criticalto ensure that the use of such roles does not prevent auditing measures fromidentifying the individual who is ultimately responsible for actions.)in general, trust is not simply a matter of trusting a single user or systemprincipal. it is necessary to trust the (hardware and software) systems throughwhich that user is communicating. for example, suppose that a user alicerunning on a workstation b is entering a transaction on a transaction server c,which in turn makes a network access to a database machine d. d'sauthorization decision may need to take account not just of alice, but also ofthe fact that b and c are involved and must be trusted. some of these issues donot arise in a centralized system, where a single authority is responsible for all thetechnology to achieve secure computer systems85computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.authentication and provides the resources for all the applications, but even in acentralized system an operation on a file, for example, is often invoked throughan application, such as a wordprocessing program, which is not part of the basesystem and perhaps should not be trusted in the same way.such rules may be expressed by introducing new, compound principals,such as "smith on workstation 4," to represent the user acting throughintermediaries. then it becomes possible to express trust in the compoundprincipal exactly as in any other. the name "workstation 4" identifies theintermediate system, just as the name "smith" identifies the user.how do we authenticate such principals? when workstation 4 says,"smith wants to read the file 'pay raises,'" how do we know (1) that the requestis really from that workstation and not somewhere else and (2) that it is reallysmith acting through workstation 4, and not jones or someone else?we answer the first question by authenticating the intermediate systems aswell as the users. if the resource and the intermediate are on the same machine,the operating system can authenticate the intermediate to the resource. if not,we use the cryptographic methods discussed in the section below titled "securechannels."to answer the second question, we need some evidence that smith hasdelegated to workstation 4 the authority to act on his behalf. we cannot ask fordirect evidence that smith asked to read the filešif we could have that, then hewould not be acting through the workstation. we certainly cannot take theworkstation's word for it; then it could act for smith no matter who is reallythere. but we can demand a statement that we believe is from smith, assertingthat workstation 4 can speak for him (probably for some limited time, andperhaps only for some limited purposes). given that smith says, "workstation 4can act for me," and workstation 4 says, "smith says to read the file 'payraises,'" then we can believe that smith on workstation 4 says, "read the file'pay raises.'"there is another authentication question lurking here, namely how do weknow that the software in the workstation is correctly representing smith'sintended action? unless the application program that smith is using is itselftrusted, it is possible that the action smith has requested has been transformedby this program into another action that smith is authorized to execute. suchmight be the case if a virus were to infect the application smith is running onhis workstation. this aspect of the authentication problem can be addressedthrough the use of trusted application software and through integritymechanisms as discussed in the section "secure channels" below.to authenticate the delegation statement from smith, "workstationtechnology to achieve secure computer systems86computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.4 can act for me," we need to employ the cryptographic methods describedbelow.the basic service provided by authentication is information that astatement was made by some principal. an aggressive form of authentication,called nonrepudiation, can be accomplished by a digital analog of notarizing, inwhich a trusted authority records the signature and the time it was made (see"digital signatures" in appendix b).authorizationauthorization determines who is trusted for a given purpose, usually fordoing some operation on an object. more precisely, it determines whether aparticular principal, who has been authenticated as the source of a request to doan operation on an object, is trusted for that operation on that object. (thisobjectoriented view of authorization also encompasses the more traditionalimplementations of file protection, and so forth.)authorization is customarily implemented by associating with the objectan access control list (acl) that tells which principals are authorized for whichoperations. the acl also may refer to attributes of the principals, such assecurity clearances. the authorization service takes a principal, an acl, and anoperation or a set of rights, and returns "yes" or "no." this way of providing theservice leaves the object free to store the acl in any convenient place and tomake its own decisions about how different parts of the object are protected. adatabase object, for instance, may wish to use different acls for differentfields, so that salary information is protected by one acl and addressinformation by another, less restrictive one.often several principals have the same rights to access a number ofobjects. it is both expensive and unreliable to repeat the entire set of principalsfor each object. instead, it is convenient to define a group of principals, give it aname, and give the group access to each of the objects. for instance, a companymight define the group "executive committee." the group thus acts as aprincipal for the purpose of authorization, but the authorization service isresponsible for verifying that the principal actually making the request is amember of the group.in this section authorization has been discussed mainly from the viewpointof an object, which must decide whether a principal is authorized to invoke acertain operation. in general, however, the subject doing the operation may alsoneed to verify that the system implementing the object is authorized to do so.for instance, when logging in over a telephone line, a user may want to be surethat hetechnology to achieve secure computer systems87computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.has actually reached the intended system and not some other, hostile system thatmay try to spoof him. this process is usually called mutual authentication,although it actually involves authorization as well: statements from the objectmust be authenticated as coming from the system that implements the object,and the subject must have access rules to decide whether that system isauthorized to do so.auditinggiven the reality that every computer system can be compromised fromwithin, and that many systems can also be compromised if surreptitious accesscan be gained, accountability is a vital last resort. accountability policies werediscussed abovešand the point was made that, for example, all significantevents should be recorded and the recording mechanisms should benonsubvertible. auditing services support these policies. usually they areclosely tied to authentication and authorization, so that every authentication isrecorded, as is every attempted access, whether authorized or not.in addition to establishing accountability, an audit trail may also revealsuspicious patterns of access and so enable detection of improper behavior byboth legitimate users and masqueraders. however, limitations to this use ofaudit information often restrict its use to detecting unsophisticated intruders. inpractice, sophisticated intruders have been able to circumvent audit trails in thecourse of penetrating systems. techniques such as the use of writeonce opticaldisks, cryptographic protection, and remote storage of audit trails can helpcounter some of these attacks on the audit database itself, but these measures donot address all the vulnerabilities of audit mechanisms. even in circumstanceswhere audit trail information could be used to detect penetration attempts, aproblem arises in processing and interpreting the audit data. both statistical andexpertsystem approaches are currently being tried, but their utility is as yetunproven (lunt, 1988).implementation: the trusted computing basethis section explores how to build a system that meets the kind of securityspecifications discussed earlier, and how to establish confidence that it doesmeet them. systems are built of components; a system also depends on itscomponents. this means that the components have to work (i.e., meet theirspecifications) for the system to worktechnology to achieve secure computer systems88computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.(i.e., meet its specification). note, however, that not all components of a systemhave to work properly in order for a given aspect of the system to functionproperly. thus security properties need not depend on all components of asystem working correctly; rather, only the securityrelevant components mustfunction properly.each component is itself a system with specifications and implementation,and so the concept of a system applies at all levels. for example, a distributedsystem depends on a network, workstations, servers, mainframes, printers, andso forth. a workstation depends on a display, keyboard, disk, processor,network interface, operating system, and, for example, a spreadsheetapplication. a processor depends on integrated circuit chips, wires, circuitboards, and connectors. a spreadsheet depends on display routines, anarithmetic library, and a macro language processor, and so it goes down to thebasic operations of the programming language, which in turn depend on thebasic operations of the machine, which in turn depend on changes in the state ofthe chips and wires, for example. a chip depends on adders and memory cells,and so it goes down to the electrons and photons, whose behavior is describedby quantum electrodynamics.a component must be trusted if it has to work for the system to meet itssecurity specification. the set of trusted hardware and software components iscalled the trusted computing base (tcb). if a component is in the tcb, so isevery component that it depends on, because if they do not work, it is notguaranteed to work either. as was established previously, the concern in thisdiscussion is security, and so the trusted components need to be trusted only tosupport security in this context.note that a system depends on more than its hardware and software. thephysical environment and the people who use, operate, and manage it are alsocomponents of the system. some of them must also be trusted. for example, ifthe power fails, a system may stop providing service; thus the power sourcemust be trusted for availability. another example: every system has securityofficers who set security levels, authorize users, and so on; they must be trustedto do this properly. yet another: the system may disclose information only toauthorized users, and they must be trusted not to publish the information in thenewspaper. thus when trust is assessed, the security of the entire system mustbe evaluated, using the basic principles of analyzing dependencies, minimizingthe number and complexity of trusted components, and carefully analyzing eachone.from a tcb perspective, three key aspects of implementing a securesystem are the following (derived from anderson, 1972):1. keeping the tcb as small and simple as possible to make it amenableto detailed analysis;technology to achieve secure computer systems89computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.2. ensuring that the tcb mediates all accesses to data and programs thatare to be protected; that is, it must not be possible to bypass the tcb;and3. making certain that the tcb itself cannot be tampered with, that is,that programs outside the tcb cannot maliciously modify the tcbsoftware or data structures.the basic method for keeping the tcb small is to separate out all thenonsecurity functions into untrusted components. for example, an elevator hasa very simple braking mechanism whose only job is to stop the elevator if itstarts to move at a speed faster than a fixed maximum, no matter what else goeswrong. the rest of the elevator control mechanism may be very complex,involving scheduling of several elevators or responding to requests from variousfloors, but none of this must be trusted for safety, because the brakingmechanism does not depend on anything else. in this case, the brakingmechanism is called the safety kernel.a purchasing system may also be used to illustrate the relative smallnessof a tcb. a large and complicated word processor may be used to prepareorders, but the tcb can be limited to a simple program that displays thecompleted order and asks the user to confirm it. an even more complicateddatabase system may be used to find the order that corresponds to an arrivingshipment, but the tcb can be limited to a simple program that displays thereceived order and a proposed payment authorization and asks the user toconfirm them. if the order and authorization can be digitally signed (usingmethods described below), even the components that store them need not be inthe tcb.the basic method for finding dependencies, relevant to ensuring tcbaccess to protected data and programs and to making the tcb tamperproof, iscareful analysis of how each step in building and executing a system is carriedout. ideally assurance for each system is given by a formal mathematical proofthat the system satisfies its specification provided all its components do. inpractice such proofs are only sometimes feasible, because it is hard to formalizethe specifications and to carry out the proofs. moreover, every such proof isconditioned on the assumption that the components work and have not beentampered with. (see the chapter 4 section "formal specification andverification" for a description of the state of the art.) in practice, assurance isalso garnered by relying on components that have worked for many people,trusting implementors not to be malicious, carefully writing specifications forcomponents, and carefully examining implementations for dependencies anderrors. because there are sotechnology to achieve secure computer systems90computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.many bases to cover, and because every base is critical to assurance, there arebound to be mistakes.hence two other important aspects of assurance are redundant checks likethe security perimeters discussed below, and methods, such as audit trails andbackup databases, for recovering from failures.the main components of a tcb are discussed below in the sections headed"computing" and "communications." this division reflects the fact that amodern distributed system is made up of computers that can be analyzedindividually but that must communicate with each other quite differently fromthe way each communicates internally.computingthe computing part of the tcb includes the application programs, theoperating system that they depend on, and the hardware (processing andstorage) that both depend on.hardwaresince software consists of instructions that must be executed by hardware,the hardware must be part of the tcb. the hardware is depended on to isolatethe tcb from the untrusted parts of the system. to do this, it suffices for thehardware to provide for a "user state" in which a program can access only theordinary computing instructions and restricted portions of the memory, as wellas a "supervisor state" in which a program can access every part of thehardware. most contemporary computers above the level of personal computerstend to incorporate these facilities. there is no strict requirement for fancierhardware features, although they may improve performance in somearchitectures.the only essential, then, is to have simple hardware that is trustworthy. formost purposes the ordinary care that competent engineers take to make thehardware work is good enough. it is possible to get higher assurance by usingformal methods to design and verify the hardware; this has been done in severalprojects, of which the viper verified microprocessor chip (for a detaileddescription see appendix b) is an example (cullyer, 1989). there is amechanically checked proof to show that the viper chip's gatelevel designimplements its specification. viper pays the usual price for high assurance: itis several times slower than ordinary microprocessors built at the same time.another approach to using hardware to support high assurance is toprovide a separate, simple processor with specialized software to implement thebasic access control services. if this hardware controlstechnology to achieve secure computer systems91computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.the computer's memory access mechanism and forces all input/output data to beencrypted, that is enough to keep the rest of the hardware and software out ofthe tcb. (this requires that components upstream of the security hardware donot share information across security classes.) this approach has been pursuedin the lock project, which is described in detail in appendix b.unlike the other components of a computing system, hardware is physicaland has physical interactions with the environment. for instance, someone canopen a cabinet containing a computer and replace one of the circuit boards. ifthis is done with malicious intent, obviously all bets are off about the security ofthe computer. it follows that physical security of the hardware must be assured.there are less obvious physical threats. in particular, computer hardwareinvolves changing electric and magnetic fields, and it therefore generateselectromagnetic radiation (often called emanations)5 as a byproduct of normaloperation. because this radiation can be a way for information to be disclosed,ensuring confidentiality may require that it be controlled. similarly, radiationfrom the environment can affect the hardware.operating systemthe job of an operating system is to share the hardware among applicationprograms and to provide generic security services so that most applications donot need to be part of the tcb. this layering of security services is usefulbecause it keeps the tcb small, since there is only one operating system formany applications. within the operating system itself the idea of layering orpartitioning can be used to divide the operating system into a kernel that is partof the tcb and into other components that are not (gasser, 1988). how to dothis is well known.the operating system provides an authorization service by controllingsubjects' (processes) accesses to objects (files and communication devices suchas terminals). the operating system can enforce various security models forthese objects, which may be enough to satisfy the security policy. in particularit can enforce a flow model, which is sufficient for the dod confidentialitypolicy, as long as it is able to keep track of security levels at the coarsegranularity of whole files.to enforce an integrity policy like the purchasing system policy describedabove, there must be some trusted applications to handle functions likeapproving orders. the operating system must be able to treat these applicationsas principals, so that they can access objects that the untrusted applicationsrunning on behalf of the same user cannot access. such applications areprotected subsystems.technology to achieve secure computer systems92computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.applications and the problem of malicious codeideally applications should not be part of the tcb, since they arenumerous, are often large and complicated, and tend to come from a variety ofsources that are difficult to police. unfortunately, attempts to build applications,such as electronic mail or databases that can handle multiple levels of classifiedinformation, on top of an operating system that enforces flow have had limitedsuccess. it is necessary to use a different operating system object forinformation at each security level, and often these objects are large andexpensive. and to implement an integrity policy, it is always necessary to trustsome application code. again, it seems best to apply the kernel method, puttingthe code that must be trusted into separate components that are protectedsubsystems. the operating system must support this approach (honeywell,1985œ1988).in most systems any application program running on behalf of a user hasfull access to all that the user can access. this is considered acceptable on theassumption that the program, although it may not be trusted to always do theright thing, is unlikely to do an intolerable amount of damage. but suppose thatthe program does not just do the wrong thing, but is actively malicious? such aprogram, which appears to do something useful but has hidden within it theability to cause serious damage, is called a trojan horse. when a trojan horseruns, it can do a great deal of damage: delete files, corrupt data, send a messagewith the user's secrets to another machine, disrupt the operation of the host,waste machine resources, and so forth. there are many places to hide a trojanhorse: the operating system, an executable program, a shell command file, or amacro in a spreadsheet or wordprocessing program are only a few of thepossibilities. moreover, a compiler or other program development tool with atrojan horse can insert secondary trojan horses into the programs it generates.the danger is even greater if the trojan horse can also make copies ofitself. such a program is called a virus. because it can spread quickly in acomputer network or by copying disks, a virus can be a serious threat(''viruses," in appendix b, gives more details and describes countermeasures).several examples of viruses have infected thousands of machines.communicationsmethods for dealing with communications and security for distributedsystems are less well developed than those for standalone centralized systems;distributed systems are both newer and more complex. theretechnology to achieve secure computer systems93computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.is no consensus about methods to provide security for distributed systems, but atcb for a distributed system can be built out of suitable trusted elementsrunning on the various machines that the system comprises. the committeebelieves that distributed systems are now well enough understood that thisapproach to securing such systems should also become recognized as effectiveand appropriate in achieving security.a tcb for communications has two important aspects: secure channels forfacilitating communication among the various parts of a system, and securityperimeters for restricting communication between one part of a system and therest.secure channelsthe access control model describes the working of a system in terms ofrequests for operations from a subject to an object and corresponding responses,whether the system is a single computer or a distributed system. it is useful toexplore the topic of secure communication separately from the discussionsabove of computers, subjects, or objects so as to better delineate thefundamental concerns that underlie secure channels in a broad range ofcomputing contexts.a channel is a path by which two or more principals communicate. asecure channel may be a physically protected path (e.g., a physical wire, a diskdrive and associated disk, or memory protected by hardware and an operatingsystem) or a logical path secured by encryption. a channel need not operate inreal time: a message sent on a channel may be read much later, for instance, if itis stored on a disk. a secure channel provides integrity (a receiver can knowwho originally created a message that is received and that the message is intact(unmodified)), confidentiality (a sender can know who can read a message thatis sent), or both.6 the process of finding out who can send or receive on asecure channel is called authenticating the channel; once a channel has beenauthenticated, statements and requests arriving on it are also authenticated.typically the secure channels between subjects and objects inside acomputer are physically protected: the wires in the computer are assumed to besecure, and the operating system protects the paths by which programscommunicate with each other, using methods described above for implementingtcbs. this is one aspect of a broader point: every component of a physicallyprotected channel is part of the tcb and must meet a security specification. if awire connects two computers, it may be difficult to secure physically, especiallyif the computers are in different buildings.technology to achieve secure computer systems94computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.to keep wires out of the tcb we resort to encryption, which makes itpossible to have a channel whose security does not depend on the security ofany wires or intermediate systems through which the messages are passed.encryption works by computing from the data of the original message, calledthe clear text or plaintext, some different data, called the ciphertext, which isactually transmitted. a corresponding decryption operation at the receiver takesthe ciphertext and computes the original plaintext. a good encryption schemereflects the concept that there are some simple rules for encryption anddecryption, and that computing the plaintext from the ciphertext, or vice versa,without knowing the rules is too difficult to be practical. this should be trueeven for one who already knows a great deal of other plaintext and itscorresponding ciphertext.encryption thus provides a channel with confidentiality and integrity. allthe parties that know the encryption rules are possible senders, and those thatknow the decryption rules are possible receivers. obtaining many securechannels requires having many sets of rules, one for each channel, and dividingthe rules into two parts, the algorithm and the key. the algorithm is fixed, andeveryone knows it. the key can be expressed as a reasonably short sequence ofcharacters, a few hundred at most. it is different for each secure channel and isknown only to the possible senders or receivers. it must be fairly easy togenerate new keys that cannot be easily guessed.the two kinds of encryption algorithms are described below. it isimportant to have some understanding of the technical issues involved in orderto appreciate the policy debate about controls that limit the export of popularforms of encryption (chapter 6) and influence what is actually available on themarket.71. symmetric (secret or private) key encryption, in which the same key isused to send and receive (i.e., to encrypt and decrypt). the key must beknown only to the possible senders and receivers. decryption of amessage using the secret key shared by a receiver and a sender canprovide integrity for the receiver, assuming the use of suitable errordetection measures. the data encryption standard (des) is the mostwidely used, published symmetric encryption algorithm (nbs, 1977).2. asymmetric (public) key encryption, in which different keys are usedto encrypt and decrypt. the key used to encrypt a message forconfidentiality in asymmetric encryption is a key made publicly knownby the intended receiver and identified as being associated with him,but the corresponding key used to decrypt the message is known onlyto that receiver. conversely, a key used to encrypt a message forintegrity (to digitally sign the message) in asymmetrictechnology to achieve secure computer systems95computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.encryption is known only to the sender, but the corresponding key usedto decrypt the message (validate the signature) must be publicly knownand associated with that sender. thus the security services to ensureconfidentiality and integrity are provided by different keys inasymmetric encryption. the rivestshamiradelman (rsa) algorithmis the most widely used form of publickey encryption (rivest et al.,1978).known algorithms for asymmetric encryption run at relatively slow rates(a few thousand bits per second at most), whereas it is possible to buy hardwarethat implements des at rates of up to 45 megabits per second, and animplementation at a rate of 1 gigabit per second is feasible with currenttechnology. a practical design therefore uses symmetric encryption forhandling bulk data and uses asymmetric encryption only for distributingsymmetric keys and for a few other special purposes. appendix b's"cryptography" section gives details on encryption.a digital signature provides a secure channel for sending a message tomany receivers who may see the message long after it is sent and who are notnecessarily known to the sender. digital signatures may have many importantapplications in making a tcb smaller. for instance, in the purchasing systemdescribed above, if an approved order is signed digitally, it can be storedoutside the tcb, and the payment component can still trust it. see theappendix b section headed "digital signatures" for a more careful definitionand some discussion of how to implement digital signatures.authenticating channelsgiven a secure channel, it is still necessary to find out who is at the otherend, that is, to authenticate it. the first step is to authenticate a channel fromone computer system to another. the simplest way to do this is to ask for apassword. then if there is a way to match up the password with a principal,authentication is complete. the trouble with a password is that the receiver canmisrepresent himself as the sender to anyone else who trusts the samepassword. as with symmetric encryption, this means that one needs a separatepassword to authenticate himself to every system that one trusts differently.furthermore, anyone who can read (or eavesdrop on) the channel also canimpersonate the sender. popular computer network media such as ethernet ortoken rings are vulnerable to such abuses.the need for a principal to use a unique symmetric key to authenticatehimself to every different system can be addressed by using a trustedtechnology to achieve secure computer systems96computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.third party to act as an intermediary in the cryptographic authentication process,a concept that has been understood for some time (branstad, 1973; kent, 1976;needham and schroeder, 1978). this approach, using symmetric encryption toachieve authentication, is now embodied in the kerberos authentication system(miller et al., 1987; steiner et al., 1988). however, the requirement that thistechnology imposes, namely the need to trust a third party with keys that maybe used (directly or indirectly) to encrypt the principal's data, may havehampered its widespread adoption.both of these problems can be overcome by challengeresponseauthentication schemes. these schemes make it possible to prove that a secret isknown without disclosing it to an eavesdropper. the simplest scheme to explainas an example is based on asymmetric encryption, although schemes based onthe use of symmetric encryption (kent et al., 1982) have been developed, andzeroknowledge techniques have been proposed (chaum, 1983). the challengerfinds out the public key of the principal being authenticated, chooses a randomnumber, and sends it to the principal encrypted using both the challenger'sprivate key and the principal's public key. the principal decrypts the challengeusing his private key and the public key of the challenger, extracts the randomnumber, and encrypts the number with his private key and the challenger'spublic key and sends back the result. the challenger decrypts the result usinghis private key and the principal's public key; if he gets back the originalnumber, he knows that the principal must have done the encrypting.8how does the challenger learn the principal's public key? the ccittx.509 standard defines a framework for authenticating a secure channel to aprincipal with an x.500 name; this is done by authenticating the principal'spublic key using certificates that are digitally signed. such a certificate, signedby a trusted authority, gives a public key, k, and asserts that a message signedby k can be trusted to come from the principal. the standard does not definehow other channels to the principal can be authenticated, but technology fordoing this is well understood. an x.509 authentication may involve more thanone agent. for example, agent a may authenticate agent b, who in turnauthenticates the principal. (for a more thorough discussion of this sort ofauthentication, see x.509 (ccitt, 1989b) and subsequent papers that identifyand correct a flaw in the x.509 threeway authentication protocol (e.g., burrowset al., 1989).)challengeresponse schemes solve the problem of authenticating onecomputer system to another. authenticating a user is more difficult, since usersare not good at doing encryption or remembering large, secret quantities. onecan be authenticated by what one knows (atechnology to achieve secure computer systems97computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.password), what one is (as characterized by biometrics), or what one has (a"smart card" or token).the use of a password is the traditional method. its drawbacks havealready been explained and are discussed in more detail in the section titled"passwords" in appendix b.biometrics involves measuring some physical characteristics of a personšhandwriting, fingerprints, or retinal patterns, for examplešand transmitting thisinformation to the system that is authenticating the person (holmes et al.,1990). the problems are forgery and compromise. it may be easy to substitute amold of someone else's finger, especially if the impersonator is not beingwatched. alternatively, anyone who can bypass the physical reader and simplyinject the bits derived from the biometric scanning can impersonate the person,a critical concern in a distributed system environment. perhaps the greatestproblem associated with biometric authentication technology to date has beenthe cost of equipping terminals and workstations with the input devicesnecessary for most of these techniques.9by providing the user with a tiny computer that can be carried around andwill act as an agent of authentication, a smart card or token reduces the problemof authenticating a user to the problem of authenticating a computer (nist,1988). a smart card fits into a special reader and communicates electricallywith a system; a token has a keypad and display, and the user keys in achallenge, reads the response, and types it back to the system (see, for example,the product racal watchword). (at least one token authentication system(security dynamics' secureid) relies on time as an implicit challenge, and thusthe token used with this system requires no keypad.) a smart card or token isusually combined with a password to keep it from being easily used if it is lostor stolen; automatic teller machines require a card and a personal identificationnumber (pin) for the same reason.security perimetersa distributed system can become very large; systems with 50,000computers exist today, and they are growing rapidly. in a large system no singleagent will be trusted by everyone; security must take account of this fact.security is only as strong as its weakest link. to control the amount of damagethat a security breach can do and to limit the scope of attacks, a large systemmay be divided into parts, each surrounded by a security perimeter. themethods described above can in principle provide a high level of security evenin a very large system that is accessible to many malicious principals. butimplementing these methods throughout a system is sure to be difficulttechnology to achieve secure computer systems98computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.and timeconsuming, and ensuring that they are used correctly is likely to beeven more difficult. the principle of "divide and conquer" suggests that it maybe wiser to divide a large system into smaller parts and to restrict severely theways in which these parts can interact with each other.the idea is to establish a security perimeter around part of a system and todisallow fully general communication across the perimeter. instead, carefullymanaged and audited gates in the perimeter allow only certain limited kinds oftraffic (e.g., electronic mail, but not file transfers). a gate may also restrict thepairs of source and destination systems that can communicate through it.it is important to understand that a security perimeter is not foolproof. if itallows the passing of electronic mail, then users can encode arbitrary programsor data in the mail and get them across the perimeter. but this is unlikely tohappen by mistake, for it requires much more deliberate planning than do themore direct ways of communicating inside the perimeter using terminalconnections. furthermore, a mailonly perimeter is an important reminder ofsystem security concerns. users and managers will come to understand that it isdangerous to implement automated services that accept electronic mail requestsfrom outside and treat them in the same fashion as communications originatinginside the perimeter.as with any security measure, a price is paid in convenience and flexibilityfor a security perimeter: it is difficult to do things across the perimeter. usersand managers must decide on the proper balance between security andconvenience. see appendix b's "security perimeters" section for more details.methodologyan essential part of establishing trust in a computing system is ensuringthat it was built according to proper methods. this important subject isdiscussed in detail in chapter 4.conclusionthe technical means for achieving greater system security and trust are afunction of the policies and models that have been articulated and developed todate. because most work to date has focused on confidentiality policies andmodels, the most highly developed services and the most effectiveimplementations support requirements for confidentiality. what is currently onthe market and known to users thus reflects only some of the need for trusttechnology. researchtechnology to achieve secure computer systems99computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.topics described in chapter 8 provide some direction for redressing thisimbalance, as does the process of articulating gssp described in chapter 1,which would both nourish and draw from efforts to develop a richer set ofpolicies and models. as noted in chapter 6, elements of public policy may alsoaffect what technology is available to protect information and other resourcescontrolled by computer systemsšnegatively, in the case of export controls, orpositively, in the case of federal procurement goals and regulations.notes1. terminology is not always used consistently in the security field. policies are often called"requirements"; sometimes the word "policy" is reserved for a broad statement and ''requirement" isused for a more detailed statement.2. dod directive 5200.28, "security requirements for automatic data processing (adp)systems," is the interpretation of this policy for computer security (encompassing requirements forpersonnel, physical, and system security). the trusted computer security evaluation criteria(tcsec, or orange book, also known as dod 5200.28std; u.s. dod, 1985d) specifies securityevaluation criteria for computers that are used to protect classified (or unclassified) data.3. that is, who caused it to be made, in the context of the computer system; legal responsibility is adifferent matter.4. the simplest such chain involves all the agents in the path, from the system up through thehierarchy to the first ancestor that is common to both the system and the principal, and then down tothe principal. such a chain will always exist if each agent is prepared to authenticate its parent andchildren. this scheme is simple to explain; it can be modified to deal with renaming and to allow forshorter authentication paths between cooperating pairs of principals.5. the government's tempest (transient electromagnetic pulse emanations standard) program isconcerned with reduction of such emanations. tempest requirements can be met by using tempestproducts or shielding whole rooms where unprotected products may be used. nsa has evaluatedand approved a variety of tempest products, although nonapproved products are also available.6. in some circumstances a third secure channel property, availability, might be added to this list. ifa channel exhibits secure availability, a sender can, with high probability, be confident that hismessage will be received, even in the face of malicious attack. most communication channelsincorporate some facilities designed to ensure availability, but most do so only under theassumptions of benign error, not in the context of malicious attack. at this time there is relativelylittle understanding of practical, generic methods of providing communication channels that offeravailability in the face of attack (other than those approaches provided to deal with natural disastersor those provided for certain military communication systems).7. for example, the digital equipment corporation's development of an architecture for distributedsystem security was reportedly constrained by the availability of specific algorithms:the most popular algorithm for symmetric key encryption is the des (data encryptionstandard). – however, the des algorithm is not specified by the architecture and, forexport reasons, ability to use other algorithms is a requirement. the preferred algorithmfor asymmetric key cryptography, and the only known algorithm with the propertiesrequired by the architecture, is rsa. – (gasser et al., 1989, p. 308)technology to achieve secure computer systems100computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.8. this procedure proves the presence of the principal but gives no assurance that the principal isactually at the other end of the channel; it is possible that an adversary controls the channel and isrelaying messages from the principal. to provide this assurance, the principal should encrypt someunambiguous identification of the channel with his private key as well, thus certifying that he is atone end. if the channel is secured by encryption, the encryption key identifies it. since the key itselfmust not be disclosed, a oneway hash (see appendix b) of the key should be used instead.9. another problem with retina scans is that individuals concerned about potential health effectssometimes object to use of the technology.technology to achieve secure computer systems101computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.4programming methodologythis chapter discusses issues pertinent to producing all highqualitysoftware and, in particular, issues pertinent primarily to producing softwaredesigned to resist attack. both application and systemlevel software areconsidered. although there are differences between how the two are produced,the similarities dominate the differences.of the several factors that govern the difficulty of producing software, oneof the most important is the level of quality to be attained, as indicated by theextent to which the software performs according to expectations. highqualitysoftware does what it is supposed to do almost all the time, even when its usersmake mistakes. for the purposes of this study, software is classified accordingto four levels of quality: exploratory, production quality, critical, and secure.these levels differ according to what the software is expected to do (itsfunctionality) and the complexity of the conditions under which the software isexpected to be used (environmental complexity).exploratory software does not have to work; the chief issue is speed ofdevelopment. although it has uses, exploratory software is not discussed in thisreport.productionquality software needs to work reasonably well most of thetime, and its failures should have limited effects. for example, we expect ourspreadsheets to work most of the time but are willing to put up with occasionalcrashes, and even with occasional loss of data. we are not willing to put up withincorrect results.critical software needs to work very well almost all of the time, andcertain kinds of failures must be avoided. critical software is used in trusted andsafetycritical applications, for example, medical instruments, where failure ofthe software can have catastrophic results.programming methodology102computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.in producing critical software the primary worries are minimizing bugs inthe software and ensuring reasonable behavior when nonmalicious users dounexpected things or when unexpected combinations of external events occur.producing critical software presents the same problems as producing productionquality software, but because the cost of failure is higher, the standards must behigher. in producing critical software the goal is to decrease risk, not todecrease cost.secure software is critical software that needs to be resistant to attack.producing it presents the same problems as does producing critical software,plus some others. one of the key problems is analyzing the kinds of attacks thatthe software must be designed to resist. the level and kind of threat have asignificant impact on how difficult the software is to produce. issues to considerinclude the following: to what do potential attackers have access? the spectrum ranges from thekeyboard of an automated teller machine to the object code of anoperational system. who are the attackers and what resources do they have? the spectrumranges from a bored graduate student, to a malicious insider, to aknowledgeable, wellfunded, highly motivated organization (e.g., aprivate or national intelligencegathering organization). how much and what has to be protected?in addition, the developers of secure software cannot adopt the variousprobabilistic measures of quality that developers of other software often can.for many applications, it is quite reasonable to tolerate a flaw that is rarelyexposed and to assume that its having occurred once does not increase thelikelihood that it will occur again (gray, 1987; adams, 1984). it is alsoreasonable to assume that logically independent failures will be statisticallyindependent and not happen in concert. in contrast, a security vulnerability,once discovered, will be rapidly disseminated among a community of attackersand can be expected to be exploited on a regular basis until it is fixed.in principle, software can be secure without being production quality. themost obvious problem is that software that fails frequently will result in denialof service. such software also opens the door to less obvious security breaches.a perpetrator of an intelligencegrade attack (see appendix e, "highgradethreats") wants to avoid alerting the administrators of the target system whileconducting an attack; a system with numerous lowlevel vulnerabilitiesprovides a rich source of false alarms and diversions that can be used to coverup the actual attack or to provide windows of opportunity (e.g., when thesystem is recovering from a crash) for the subversion of hardware or software.programming methodology103computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.lowquality software also invites attack by insiders, by requiring thatadministrative personnel be granted excessive privileges of access to manuallyrepair data after software or system failures.another important factor contributing to the difficulty of producingsoftware is the set of performance constraints the software is intended to meet,that is, constraints on the resources (usually memory or time) the software ispermitted to consume during use. at one extreme, there may be no limit on thesize of the software, and denial of service is considered acceptable. at the otherextreme is software that must fit into limited memory and meet "hard" realtimeconstraints. it has been said that writing extremely efficient programs is anexercise in logical brinkmanship. working on the brink increases theprobability of faults and vulnerabilities. if one must work on the brink, the goalsof the software should be scaled back to compensate.perhaps the most important factor influencing the difficulty of producingsoftware is size. producing big systems, for example, a global communicationsystem, is qualitatively different from producing small ones. the reasons forthis are well documented (nrc, 1989a).in summary, simultaneous growth in level of quality, performanceconstraints, functionality, and environmental complexity results in acorresponding dramatic increase in the cost and risk of producing, and the riskof using, the software. there is no technology available to avoid this, nor isresearch likely to provide us with such a technology in the foreseeable future. ifthe highest possible quality is demanded for secure software, something elsemust give. because security cannot be attained without quality and theenvironment in which a system is to run is usually hard to control, typically onemust either remove performance constraints (perhaps by allocating extraresources) or reduce the intended functionality.software is more than codegood software is more than good code. it must be accompanied by highquality documentation, including a requirements document, a design document,carefully written specifications for key modules, test plans, a maintenance plan,and so on.of particular importance for secure software is a guide to operations. morecomprehensive than a user's manual, such a guide often calls for operationalprocedures that must be undertaken by people other than users of the software,for example, by system administrators. in evaluating software one mustconsider what it will do if the instructions in the guide to operations arefollowed, and what it will do ifprogramming methodology104computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.they are not. one must also evaluate how likely it is that capable people withgood intentions will succeed in following the procedures laid down in the guideto operations.for critical and secure software, a guide to operations is particularlyimportant. in combination with the software it must provide for the following: auditing: what information is to be collected, how it is to be collected,and what is to be done with it must be described. those who havepenetrated secure software cannot be expected to file a bug report, and somechanisms for detecting such penetrations are needed. reduction of rawaudit data to intelligible form remains a complex and expensive process; aplan for secure software must include resources for the development ofsystems to reduce and display audit data. recovery: producing faultfree software of significant size is nearlyimpossible. therefore one must plan for dealing with faults, for example,by using carefully designed recovery procedures that are exercised on aregular basis. when they are needed, it is important that such proceduresfunction properly and that those who will be using them are familiar withtheir operation. if at all possible manual procedures should be in place tomaintain operations in the absence of computing. this requires evaluatingthe risk of hardware or software crashes versus the benefits wheneverything works. operation in an emergency mode: there may be provisions for bypassingsome security features in times of extreme emergency. for example,procedures may exist that permit "breaking in" to protected data in criticalcircumstances such as incapacitation or dismissal of employees withspecial authorizations. however, the system design should treat suchemergencies explicitly, as part of the set of events that must be managedby security controls.software should be delivered with some evidence that it meets itsspecifications (assurance). for noncritical software the good reputation of thevendor may be enough. critical software should be accompanied bydocumentation describing the analysis the software has been subjected to. forcritical software there must be no doubt about what configurations theconclusions of testing and validation apply to and no doubt that what isdelivered is what was validated. secure software should be accompanied byinstructions and tools that make it possible to do continuing quality assurance inthe field.software delivered without assurance evidence may provide only illusorysecurity. a system that is manifestly nonsecure will generally inspire caution onthe part of its users; a system that provides illusory security will inspire trustand then betray that trust when attacked.programming methodology105computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.arrangements should be made to have the assurance evidence reviewed bya team of experts who are individually and organizationally independent fromthe development team.software should be delivered with a plan for its maintenance andenhancement. this plan should outline how various expected changes might beaccomplished and should also make clear what kinds of changes mightseriously compromise the software.secure software must be developed under a security plan. the plan shouldaddress what elements of the software are to be kept confidential, how tomanage trusted distribution of software changes, and how authorized users canbe notified of newly discovered vulnerabilities without having that knowledgefall into the wrong hands.simpler is betterthe best software is simple in two respects. it has a relatively simpleinternal structure, and it presents a relatively simple interface to theenvironment in which it is embedded.before deciding to incorporate a feature into a software system, one shouldattempt to understand all the costs of adding that feature and do a careful costbenefit analysis. the cost of adding a feature to software is usuallyunderestimated. the dominant cost is not that of the feature per se, but that ofsorting out and controlling the interactions of that feature with all the others. inparticular, underestimating cost results from a failure to appreciate the effects ofscale. the other side of the coin is that the value of a new feature is usuallyoverestimated. when features are added, a program becomes more complex forits users as well as for its developers. furthermore, the interactions of featuresmay introduce unexpected security risks. it is axiomatic among attackers thatone does not break components but rather systems, by exploiting unanticipatedcombinations of features. it cannot be emphasized enough that truly securesystems are modest, straightforward, and understandable.the best designs are straightforward. the more intricate the design and thegreater the number of specialcase features to accomplish a given functionality,the greater the scope for errors. sometimes simple designs may be (or mayappear to be) unacceptably inefficient. this can lead developers to compromisethe structure or integrity of code or to employ intricate fast algorithms,responses that almost always make the software harder to produce and lessreliable, and often make it more dependent on the precise characteristics of theinput. better hardware and less ambitious specifications deserve strongconsideration before one ventures into such an exercise in softwareprogramming methodology106computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.virtuosity. such tradeoffs deserve special attention by designers of securesystems, who too often accept the almost impossible requirements to preservethe full performance, function, and hardware of predecessor systems.the role of programming languagesan important threat to all software is bugs that have been accidentallyintroduced by programmers. it has been clearly demonstrated that higherlevelprogramming languages tend to reduce the number of such bugs, for thefollowing reasons: higherlevel languages reduce the total amount of code that must bewritten. higherlevel languages provide abstraction mechanisms that makeprograms easier to read. all higherlevel languages provide procedures.the better languages provide mechanisms for data abstraction (e.g.,packages) and for control abstraction (e.g., iterators). higherlevel languages provide checkable redundancy, such as typechecking that can turn programs with unintended semantics into illegalprograms that are rejected by the compiler. this helps turn errors thatwould otherwise occur while the program is running into errors that mustbe fixed before the program can run. higherlevel languages can eliminate the possibility of making certainkinds of errors. languages with automatic storage management, forexample, greatly reduce the likelihood of a program trying to use memorythat no longer belongs to it. much useful analysis can be done by thecompiler, but there is usually ample opportunity to use other tools as well.sometimes these toolsšfor example, various c preprocessorsšmake upfor deficiencies in the programming language. sometimes they enforcecoding standards peculiar to an organization or project, for example, thestandard that all types be defined in a separate repository. sometimes theyare primitive program verification systems that look for anomalies in thecode, for example, code that cannot be reached.a potential drawback to using higherlevel programming languages inproducing secure software is that they open up the possibility of certain kinds of"tunneling attacks." in a tunneling attack, the attacker attempts to exploitvulnerabilities at a level of abstraction beneath that at which the systemdevelopers were working. to avoid such attacks one must be able to analyze thesoftware beneath the level of the source language. higherlevel languages oftenhave large runtime packages (e.g., the ada runtime support library). theseruntimeprogramming methodology107computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.packages are often provided as black boxes by compiler vendors and are notsubject to the requirements for independent examination and development ofassurance evidence that the rest of the software must satisfy. they are,therefore, often a weak link in the security chain.the role of specificationsspecifications describe software components. they are written primarily toprovide precise, easytoread, modulelevel documentation of interfaces. thisdocumentation facilitates system design, integration, and maintenance, and itencourages reuse of modules. the most vexing problems in building systemsinvolve overall system organization and the integration of components.modularity is the key to effective integration, and specifications are essentialfor achieving program modularity. abstraction boundaries allow one tounderstand programs one module at a time. however, an abstraction isintangible. without a specification, there is no way to know what theabstraction is or to distinguish it from one of its implementations (i.e.,executable code).the process of writing a specification clarifies and deepens understandingof the object being specified by encouraging prompt attention toinconsistencies, incompletenesses, and ambiguities. once written, specificationsare helpful to auditors, implementors, and maintainers. a specificationdescribes an agreement between clients and providers of a service. the provideragrees to write a module that meets a specification. the user agrees not to relyon any properties of the module that are not guaranteed by the specification.thus specifications provide logical firewalls between providers and clients ofabstractions.during system auditing, specifications provide information that can beused to generate test data, build stubs, and analyze information flow. duringsystem integration they reduce the number and severity of interfacing problemsby reducing the number of implicit assumptions.specifications are usually much easier to understand than areimplementationsšthus combining specifications is less work than combiningimplementations. by relying only on those properties guaranteed by aspecification, one makes the software easier to maintain because it is clear whatproperties must be maintained when an abstraction or its implementation ischanged. by distinguishing abstractions from implementations, one increasesthe probability of building reusable components.programming methodology108computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.one of the most important uses of specifications is design verification.getting a design "right" is often much more difficult than implementing thedesign.1 therefore, the ease and precision with which conjectures about adesign can be stated and checked are of primary importance.the kinds of questions one might ask about a design specification fall intoa spectrum including two extremes: general questions relevant to anyspecification and problemspecific questions dealing with a particularapplication. the general questions usually deal with inconsistency (e.g., doesthe specification contradict itself?) or incompleteness (e.g., have importantissues not been addressed?). between the two extremes are questions related toa class of designs, for example, generic security questions. design verificationhas enjoyed considerable success both inside and outside the security area. thekey to this success has been that the conjectures to be checked and thespecifications from which they are supposed to follow can both be written at thesame relatively high level of abstraction.relating specifications to programsthe preceding discussions of the roles of programming languages andspecifications have emphasized the importance of separately analyzing bothspecifications and programs. showing that programs meet their specifications isapproached mainly by the use of testing and verification (or proving). testing isa form of analysis in which a relatively small number of cases are examined.verification deals with a potentially unbounded number of cases and almostalways involves some form of inductive reasoning, either over the number ofsteps of a program (e.g., one shows that if some property holds after theprogram has executed n steps, it will also hold after n + 1 steps) or over thestructure of a data type (e.g., one shows that if some property holds for the firstn elements of an array, it will also hold for the first n + 1 elements).the purpose of both kinds of analysis is to discover errors in programs andspecifications, not to certify that either is errorfree. proponents of testing havealways understood this. testing cannot provide assurance that a property holdsšthere are simply too many cases to be examined in any realistic system. inprinciple, verification can be used to certify that a program satisfies itsspecification. in practice, this is not the case. as the history of mathematicsmakes clear, even the most closely scrutinized proofs may be flawed.testing techniques can be grouped roughly into three classes: (1) randomtesting involves selection of data across the environment, often with somefrequency distribution; (2) structural testing involvesprogramming methodology109computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.generating test cases from a program itself, forcing known behavior onto theprogram; and (3) functional testing uses the specified functions of a program asthe basis for defining test cases (howden, 1987; miller and howden, 1981).these techniques are complementary and should be used in concert.it is important that verification not be equated with formal proofs. informalbut rigorous reasoning about the relationships between implementations andspecifications has proved to be an effective approach to finding errors(solomon, 1982). people building concurrent programs frequently state keyinvariants and make informal arguments about their validity (lamport, 1989;wing, 1990).common sense and much empirical evidence make it clear that neithertesting nor verification by itself is adequate to provide assurance for critical andsecure software. in addition to being necessarily incomplete, testing is not acheap process, often requiring that months be spent in grinding out test cases,running the system on them, and examining the results. these tests must berepeated whenever the code or operating environment is changed (a processcalled regressions testing). testing software under actual operating conditions isparticularly expensive.2 verification relies on induction to address multiplecases at once. however, discovering the appropriate induction hypotheses canbe a difficult task. furthermore, unless the proofs are machine checked they arelikely to contain errors, and, as discussed in the following section, largemachinechecked proofs are typically beyond the current state of the art.many views exist on how testing and proving can be combined. the ibm''cleanroom" approach (linger and mills, 1988; selby et al., 1987) uses a formof design that facilitates informal proofs during an inspection process combinedwith testing to yield statistical evidence. some parts of a system may be testedand others proved. the basic technique of provingšworking a symbolicexpression down a path of the programšmay be used in either a testing orproving mode. this is especially applicable to secure systems when thesymbolic expression represents an interesting security infraction, such aspenetrating a communication system or faking an encryption key. inductivearguments may be used to show that certain paths cannot be taken, therebyreducing the number of cases to be analyzed.realtime systems pose special problems. the current practice is to useinformation gathered from semiformal but often ad hoc analysis (e.g., designreviews, summation of estimated times for events along specific program paths,and simulation) to determine whether an implementation will meet its specifiedtime deadlines with an acceptable degree of probability. more systematicmethods for analyzing functionalprogramming methodology110computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.and performance properties of realtime software systems are needed.formal specification and verificationin the computer science literature, the phrase "formal method" is oftenused to refer to any application of a mathematical technique to the developmentor analysis of hardware or software (ieee, 1990b,c). in this report, "formal" isused in the narrower sense of "subject to symbolic reasoning." thus, forexample, a formal proof is a proof that can, at least in principle, be checked bymachine.the process of formally verifying that a program is correct with respect toits specification involves both generating and proving verification conditions. averificationcondition generator accepts as input a piece of code and formalspecifications for that code, and then outputs a set of verification conditions,also called conjectures or proof obligations. these verification conditions areinput to a theorem prover in an attempt to prove their validity using theunderlying logic. if the conditions are all proved, then the program is said tosatisfy its specification.the security community has been interested for some time in the use offormal verification to increase confidence in the security of software (craigenand summerskill, 1990). while some success has been reported (haigh et al.,1987), on the whole formal program verification has not proved to be agenerally costeffective technique. the major obstacles have been the following(kemmerer, 1986): the difficulty of crossing the barrier between the level of abstractionrepresented by code and the level of abstraction at which specificationsshould be written. limits on theoremproving technology. given the current state of theoremproving technology, program verification entails extensive userinteraction to prove relatively simple theorems. the lack of wellengineered tools.the last obstacle is certainly surmountable, but whether the first two canbe overcome is subject to debate.there are fundamental limits to how good theorem provers can become.the basic problem is undecidable, but that is not relevant for most of the proofobligations that arise in program verification. a more worrisome fact is thatreasoning about many relatively simple theories is inherently expensive,3 andmany of the formulas that arise in practice take a long time to simplify. despitethese difficulties,programming methodology111computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.there has been enough progress in mechanical theorem proving in the lastdecade (lindsay, 1988) to give some cause for optimism.whether or not the abstraction barrier can be gracefully crossed is the mostcritical question. the problem is that the properties people care about, forexample, authentication of users, are most easily stated at a level of abstractionfar removed from that at which the code is written. those doing formal programverification spend most of their time mired in codelevel details, for example,proving that two variables do not refer to the same piece of storage, and intrying to map those details onto the properties they really care about.a formal specification is a prerequisite to formal program verification.however, as outlined above in the section titled "the role of specifications,"specifications have an important role that is independent of program verification.the potential advantages of formal over informal specifications are clear:formal specifications have an unambiguous meaning and are subject tomanipulation by programs. to fully realize these advantages, one must haveaccess to tools that support constructing and reasoning about formalspecifications.an important aspect of modern programming languages is that they arecarefully engineered so that some kinds of programming errors are detected byeither the compiler or the runtime system. some languages use "specs" or"defs" modules (mitchell et al., 1979), which can be viewed as a first step inintegrating formal specifications into the programming process. however,experience with such languages shows that while programmers are careful withthose parts (e.g., the types of arguments) that are checked by their programmingenvironment, they are much less careful about those parts (e.g., constraints onthe values of arguments) that are not checked. if the latter parts were checked aswell, programmers would be careful about them, too.designs are expressed in a formal notation that can be analyzed, andformal statements about them can be proved. the process of formal designverification can be used to increase one's confidence that the specifications say"the right thing," for example, that they imply some security property.organizations building secure systems have made serious attempts toapply formal specification, formal design verification, and formal programverification. this committee interviewed members of several suchorganizations4 and observed a consistent pattern: writing formal specifications and doing design verification significantlyincreased people's confidence in the quality of their designs. important flaws were found both during the writing of specifications andduring the actual design verification. although the majority ofprogramming methodology112computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.the flaws were found as the specifications were written, the "threat" ofdesign verification was an important factor in getting people to take thespecification process seriously. designlevel verification is far more costeffective than is programlevelverification. writing codelevel entry/exit assertions is useful even if they are notverified. although usable tools exist for writing and proving properties aboutspecifications, better specification languages and tools are needed. more attention needs to be devoted to formalizing a variety of generallyapplicable security properties that can be verified at the design level. little is understood about the formal specification and verification ofperformance constraints.hazard analysisfor critical and secure systems, hazard analysis is important. this involvesthe identification of environmental and system factors that can go wrong andthe levels of concern that should be attached to the results. environmentalevents include such actions as an operator mistyping a command or anearthquake toppling a disk drive. systematic hazard analysis starts with a list ofsuch events generated by experts in such domains as the application, the physicsof the underlying technology, and the history of failures of similar systems.each hazard is then traced into the system by asking pertinent questions: issystem behavior defined for this hazard? how will the system actually behaveunder these conditions? what can be done to minimize the effects of thishazard? thus hazard analysis is a form of validation in assuring that theenvironment is well understood and that the product is being built to respondproperly to expected events. many forms of security breaches can be treated ashazards (u.k. ministry of defence, 1989b).physical system safety engineers have long used techniques such as failuremode effects analysis and fault trees to trace the effects of hazards. software isalso amenable to analysis by such techniques, but additional problems arise(leveson, 1986). first, the sheer complexity of most software limits the depthof analysis. second, the failure modes of computercontrolled systems are notas intuitive as those for physical systems. by analogy, as radios with analogtuners age, the ability to separate stations slowly decreases. in contrast, radioswith digital tuners tend to work well, or not at all.programming methodology113computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.structuring the development processsome of the more popular approaches to software development haveaspects that this committee believes are counterproductive.some approaches encourage organizations to ignore what they alreadyhave when starting a new software project. there seems to be an almostirresistible urge to start with a clean slate. while this offers the advantage of nothaving to live with past mistakes, it offers the opportunity to make a host ofnew ones. most of the time, using existing software reduces both cost and risk.if software has been around for some time, those working with it already have aconsiderable investment in understanding it. this investment should not bediscarded lightly. finally, when the hazards of a system are well understood, itoften becomes possible to devise operational procedures to limit their scope.for similar reasons it is usually prudent to stick to established tools whenbuilding software that must be secure. not only should programmers useprogramming languages they already understand, but they should also look forcompilers that have been used extensively in similar projects. although this is aconservative approach that over the long haul is likely to impede progress in thestate of the art, it is clear that using new tools significantly increases risk.the development process should not place unnecessary barriers betweenthe design, implementation, and validation stages of an effort to producesoftware. particularly dangerous in producing critical or secure software areapproaches that rely primarily on ex post facto validation. software should beevaluated as it is being built, so that the process as well as the product can beexamined. the most reliable evaluations involve knowing what goes on whilethe system is being designed. evaluation by outsiders is necessary but shouldnot be the primary method of assurance.both software and the software development process should be structuredso as to include incremental development based on alternation betweenrelatively short design and implementation phases. this style of developmenthas several advantages, among them the following: it helps to keep designers in touch with the real world by providingfeedback. it tends to lead to a more modular design because designers areencouraged to invent coherent subsystems that can be implementedindependently of other subsystems. (that is not to say that the varioussubsystems do not share code.) it leads to designs in which piecewise validation (usually by somecombination of reasoning and testing) of the implementation isprogramming methodology114computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved. possible. at the same time it encourages designers to think of planning forvalidation as part of the design process. by encouraging designers to think of the design as something that changesrather than as a static entity that is done "correctly" once, it tends to leadto designs that can be more easily changed if the software needs to bemodified.managing software procurementcurrent trends in software procurement (particularly under governmentcontracts) are rather disturbing:1. it has become increasingly common for those buying software todevelop an adversarial relationship with those producing it. recentlegislation (the procurement integrity act of 1989, p.l. 100679,section 27) could be interpreted as virtually mandating such arelationship. if implemented, this act, which would stop the flow of"inside" information to potential vendors, might have the effect ofstopping the flow of all information to potential vendors, thussignificantly increasing the number of government softwareprocurements that would overrun costs or fail to meet the customer'sexpectations.52. purchasers of software have begun to take an increasingly narrow viewof the cost of software. procurement standards that require buyingsoftware from the lowest bidder tend to work against efforts toimprove software quality. likewise, the procurement of software byorganizations that are separate from the end users typically leads to anemphasis on reduction of initial cost, with a corresponding increase inlifecycle expense.3. contractors often use their most talented engineers to procure contractsrather than to build systems.the best software is produced when the customer and vendor have acooperative relationship. in the beginning, this makes it possible for thecustomer to be frank about his needs and the vendor to be frank about thedifficulty of meeting those needs. a negotiation can then follow as together thecustomer and vendor attempt to balance the customer's desires againstimplementation difficulties. as the project progresses, particularly if it is donein the incremental way suggested above, the vendor and customer must bothfeel free to revisit the definition of what the software is to do. such arelationship, while still possible in the private sector, could become difficult ingovernment procurements, owing to the difficulty of determining what is or isnot illegal under the procurement integrity act of 1989 (if it is actuallyimplemented). adaptation to changed circumstances andprogramming methodology115computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.redirection of contracts to incorporate lessons learned could be difficult,because the law makes even preliminary discussion of such issues betweencustomer and vendor a criminal offense. thus increasingly the emphasis in thecustomervendor relationship could be on satisfaction of the letter of thecontract. the sense of team ownership of a problem, so essential to success inan intangible field such as software development, would be lost completely.procurement standards that require software to be purchased from thelowest bidder often miss the point that the real cost of software is not the initialpurchase price. the costs of porting, supporting, maintaining, and modifyingthe software usually dominate initial production costs. furthermore the cost ofusing software that does not perform as well as it might can often outweigh anysavings achieved at the time it is purchased. finally, buying software from thelowest bidder encourages vendors to take a shortterm approach to softwaredevelopment. in a wellrun software organization, every significant softwareproject should have as a secondary goal producing components that will beuseful in other projects. this will not happen by accident, since it is more workand therefore more costly to produce components that are likely to be reusable.scheduling software developmentone of the reasons that software projects are chronically behind scheduleand over budget is that they start with unrealistic requirements, schedules, andbudgets. a customer's requirements are often vague wish lists, which arefrequently interpreted as less onerous than they in fact prove to be when theyare later clarified. the scheduled delivery date for software is often based onmarketing considerations (e.g., winning a contract), rather than on a carefulanalysis of how much work is actually involved. an unrealistically optimisticschedule has many disadvantages: decisions about what the software will do are made under crisisconditions and at the wrong time (near the end of a project) and for thewrong reasons (how hard something will be to implement given thecurrent state of the software, rather than how important it is or how hard itwould have been to implement from the starting point). programmers who have worked hard trying to meet an impossibleschedule will be demoralized when it becomes apparent that the schedulecannot be met. they will eventually begin to believe that missingdeadlines is the norm. the whole development process is distorted. people may spend inordinateamounts of care on relatively unimportant pieces of theprogramming methodology116computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.software that happen to be built early in the project and then race throughimportant pieces near the end. activities like quality assurance thattypically occur near the end of the process get compressed and slighted.scheduling the development of critical or secure software is somewhatdifferent from the scheduling for other kinds of software. extra time and moneymust be allocated for extensive review and analysis. if an outside review isrequired, this must be taken into account from the beginning, since extra timeand money must be allocated throughout the life of the project. oneconsequence of an extremely careful review process is the increased likelihoodof uncovering problems. time and money must be reserved for dealing withsuch problems prior to system delivery.education and trainingthere is a shortage of wellqualified people to work on productionqualitysoftware. there is a more serious shortage of those qualified to build criticalsoftware, and a dramatic shortage of people qualified to build secure software.a discussion of the general shortage of qualified technical people in thiscountry is beyond the scope of this report. however, a few comments are inorder about the narrower problems associated with the education and training ofthose working on critical and secure software.setting requirements for, specifying, and building critical software requirespecialized knowledge not possessed by typical software engineers. over theyears other engineering disciplines have developed specialized techniquesšhazard analysisšfor analyzing critical artifacts. such techniques are notcovered in most software engineering curricula, nor are they covered by mostonthejob training. furthermore, working on critical software requiresspecialized knowledge of what can go wrong in the application domain.working on secure software requires yet more skills. most notably, onemust be trained to understand the potential for attack, for software in generaland for the specific application domain in particular.this committee advocates a twopronged approach to addressing theshortage of people qualified to work on software: a new universitybasedprogram in combination with provisions for more onthejob education as a partof current and future software projects.the universitybased program would be aimed at returning, graduatelevelstudents who are already somewhat familiar with at least one application area.while the program would cover conventional software engineering, specialemphasis would be given to topics relatedprogramming methodology117computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.to critical and secure software. for example, different project managementstructures would be discussed in terms of their impact on both productivity andsecurity. discussions of quality assurance might emphasize safety engineeringmore than would be expected in a traditional software engineering program.although careful consideration should be given to the specific content of such acurriculum, it seems clear that at least a oneyear or perhaps even a twoyearprogram is needed. such a program could best be developed at universities withstrong graduate engineering and business programs.the committee envisions as an initial step approximately three suchprograms, each turning out perhaps 20 people a year. over time, it would benecessary (and probably possible) to increase the number of graduates.developing such a program would not be inexpensive: the committee estimatesthat the cost would be on the order of $1 million.given the current shortage and the time it will take to establish universityprograms that can increase the supply of qualified software engineers, managersof large securityrelated development efforts should deal explicitly with theneed to educate project members. both time and money for this should beappear in project budgets.management concerns in producing securesoftwaremanaging a project to produce secure software requires all the basic skillsand discipline required to manage any substantial project. however, productionof secure software typically differs from production of general highqualitysoftware in one area, and that is in the heavy emphasis placed on assurance, andin particular on the evaluation of assurance conducted by an independent team.perhaps the most difficult, and certainly the most distinctive, managementproblem faced in the production of secure software is integrating thedevelopment and the assurance evaluation efforts. the two efforts are typicallyconducted by different teams that have different outlooks and use differentnotations. in general, the assurance team has an analytical outlook that isreflected in the notations it uses to describe a system; the development teamfocuses on the timely production of software, and accordingly emphasizessynthesis and creativity.as a consequence it is very easy for an antagonistic relationship to developbetween the two teams. one result is that what is analyzed (typically adescription of a system) may bear little resemblance to the software that isactually produced. geographic and organizationalprogramming methodology118computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.separation of the assurance and development teams compounds this problem.ideally, the teams work side by side with the same material; as a practicalmatter, a jointly satisfactory "translation notation" may have to be devised sothat the assurance team does not have to work with actual source code (which istypically not processable by their tools) and the development team does nothave to program in an inappropriate language.scheduling of the various assurance and implementation milestones istypically a difficult process. assurance technology is considerably less maturethan implementation technology, and the tools it uses are often laboratoryprototypes rather than productionquality software. estimates of time and efforton the part of the assurance team are therefore difficult to make, and the variousassurance milestones often become the "gating factor" in maintaining a project'sschedule. managers must make it clear from the outset, and maintain theposture, that assurance is an important aspect of the project and not justsomething that causes schedule slips and prevents programmers from doingthings in otherwise reasonable ways. they must also recognize the fact thatassurance will be a continuing cost. when a software system is modified, theassurance evidence must be updated. this means more than merely runningregression tests. if, for example, assurance involves covert channel analyses,then those too must be redone.the project plan must include a long, slow startup in the beginning, with ahigher percentage of time devoted to specification and analysis than is devotedto design. this lead time is required because the typical design team can devisemechanisms at a rate that greatly exceeds the ability of the assurance team tocapture the mechanisms in their notations and to analyze them.managers should also cultivate a project culture in which assurance isviewed as everybody's problem and not just some mysterious process that takesplace after the software is done. it is particularly necessary that the developersappreciate an attacker's mindset, so that they themselves look at everythingthey do from the point of view of the threat. information security (infosec)attacks generally succeed because the attacker has embarked on an adventure,whereas the defenders are just working at a job. management must instill theprobing, skeptical, confident view of the attacker in each developer if thesoftware is to be secure in fact as well as on paper.what makes secure software differentfrom the perspective of programming methodology, the hardest part ofproducing secure software is producing good software. if oneprogramming methodology119computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.includes denial of service under the security rubric, producing secure softwareinvolves all the difficulties associated with building critical software, plus theadditional difficulties associated with assuring integrity and confidentialityunder the presumption of outside attack.some of the techniques generally considered useful in producing softwarehave additional benefits in the security realm. people in the programmingmethodology field have long stressed the importance of modularity. in additionto making software easier to build, modularity helps to limit the scope of bugsand penetrations. modularity may even be useful in reducing the impact ofsubverted developers.there are also some apparent tradeoffs between security concerns andother facets of good practiceš''apparent" because most of the time one shouldopt for good software practice; without it one will not have anything useful.attempts to provide protection from highgrade threats by strictly limitingthe number of people with access to various parts of the software may be selfdefeating. the social process of the interaction of professionals on a project,conducted formally or casually, is a powerful tool for achieving correctness infields like mathematics or software that deal with intangibles. secrecy stops thesocial process in its tracks, and strict application of the "needtoknow"principle makes it very likely that system elements are subject to scrutiny onlyby insiders with a vested interest in the success of the project. secrecy may alsohinder the technical evolution of countermeasures; individuals assigned to thedevelopment of a given device or subsystem may not be aware of even theexistence of predecessor devices, much less their specific strengths andweaknesses and mix of success and failure.the inherent mutability of software conflicts with the requirements forachieving security. consequently secure software is often deliberately madedifficult to modify, for example, by burning code into readonly memory. notonly does this make it hard for attackers to subvert the software, but it also,unfortunately, makes it hard to make legitimate changes, for example, fixing aknown vulnerability.in resourcelimited projects, any resources devoted to protecting thoseparts of a system deemed most vulnerable will detract from protecting otherparts of the system. one must be careful to ensure that other parts of the systemare not unduly impoverished.recommended approaches to sounddevelopment methodologythe recommendations that follow are broad directives intended to reflectgeneral principles. some are included in the fourth subset ofprogramming methodology120computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.the committee's recommendation 2, which calls for shortterm actions that buildon existing capabilities (see chapter 1). finding: what correlates most strongly with lack of vulnerabilities insoftware is simplicity. furthermore, as complexity and size increase, theprobability of serious vulnerabilities increases more than linearly.recommendation: to produce software systems that are secure, structuresystems so that securitycritical components are simple and small. finding: software of significant size must be assumed to have residualerrors that can compromise security. recommendation: reducevulnerability arising from failure of security. keep validated copies ofvital data offline. establish contingency plans for extended computeroutages. finding: extensive and extended use of software tends to reduce thenumber of residual errors, and hence the vulnerabilities.recommendation: encourage the development of generally availablecomponents with welldocumented programlevel interfaces that can beincorporated into secure software. among these should be standardizedinterfaces to security services. finding: designlevel verification using formal specifications has provedto be effective in the security area. recommendation: do more researchon the development of tools to support formal designlevel verification.emphasize as a particularly important aspect of this research theidentification of designlevel properties to be verified. finding: the most important bottleneck in reasoning about programs isthe difficulty of dealing with multiple levels of abstraction.recommendation: conduct research on program verification so as to putgreater emphasis on this problem. finding: software that taxes the resources of the computing environmentin which it is run is likely to be complex and thus vulnerable.recommendation: when building secure software, provide excessmemory and computing capacity relative to the intended functionality. finding: the use of higherlevel programming languages reduces theprobability of residual errors, which in turn reduces the probability ofresidual vulnerabilities. recommendation: when tunneling attacks are nota major concern, use higherlevel languages in building secure software. finding: using established software tends to reduce risk.recommendation: in general, build secure software by extending existingsoftware with which experience has been gained. furthermore, use maturetechnology, for example, compilers that have been in use for some time. finding: ex post facto evaluation of software is not as reliableprogramming methodology121computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.as evaluation that takes place during the construction of the software.recommendation: couple development of secure software with regularevaluation. if evaluation is to be done by an outside organization, involvethat organization in the project from the start. finding: there is a severe shortage of people qualified to build securesoftware. recommendation: establish educational programs thatemphasize the construction of trusted and secure software in the contextof software engineering. finding: adopting new software production practices involves asubstantial risk that cannot usually be undertaken without convincingevidence that significant benefits are likely to result. this greatly inhibitsthe adoption of new and improved practice. recommendation: establishan organization for the purpose of conducting showcase projects todemonstrate the effectiveness of applying wellunderstood techniques tothe development of secure software. finding: assurance is often the gating factor in maintaining a projectschedule for producing secure software. this is particularly true duringthe design phase of a project. recommendation: build into schedulesmore time and resources for assurance than are currently typical. finding: there is a tradeoff between the traditional security technique oflimiting access to information to those with a need to know and thetraditional software engineering technique of extensively reviewingdesigns and code. although there are circumstances in which it isappropriate to keep mechanisms secret, for most parts of mostapplications the benefits of secrecy are outweighed by the costs. when aproject attempts to maintain secrecy, it must take extraordinary measures,for example, providing for cleared "inspectors general," to ensure that theneed to maintain secrecy is not abused for other purposes, such asavoiding accountability on the part of developers. recommendation:design software so as to limit the need for secrecy.notes1. for example, jay crawford of the naval weapons center at china lake, california, reports thatthe majority of errors in the production versions of the flight software managed there were classifiedas specification and design errors rather than coding errors.2. the navy estimates that testing software in an operating aircraft costs $10,000 per hour.3. checking the satisfiability of simple boolean formulas, for example, is an npcomplete problem;that is, the worstcase time required (probably) grows exponentially in the size of the formula.4. morrie gasser and ray modeen, secure systems group, digital equipment corporation;timothy e. levin, gemini computers, inc.; j. thomas haigh, secure computingprogramming methodology122computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.technology corporation (formerly honeywell secure computing technology center); and georgedinolt, ford aerospace corporation.5. implementation of the procurement integrity act of 1989 was suspended through november 30,1990, and may be further suspended until may 31, 1991, to consider proposed changes by theadministration (see congressional record of june 21, 1990, and august 2, 1990).programming methodology123computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.5criteria to evaluate computer andnetwork securitycharacterizing a computer system as being secure presupposes somecriteria, explicit or implicit, against which the system in question is measured orevaluated. documents such as the national computer security center's(ncsc's) trusted computer system evaluation criteria (tcsec, or orangebook; u.s. dod, 1985d) and its trusted network interpretation (tni, or redbook; u.s. dod, 1987), and the harmonized information technology securityevaluation criteria (itsec; federal republic of germany, 1990) of france,germany, the netherlands, and the united kingdom provide standards againstwhich computer and network systems can be evaluated with respect to securitycharacteristics. as described below in "comparing national criteria sets," thesedocuments embody different approaches to security evaluation, and thedifferences are a result of other, perhaps less obvious purposes that securityevaluation criteria can serve.this chapter describes the competing goals that influence the developmentof criteria and how current criteria reflect tradeoffs among these goals. itdiscusses how u.s. criteria should be restructured to reflect the emergence offoreign evaluation criteria and the experience gained from the use of currentncsc criteria. while building on experience gained in the use of orange bookcriteria, the analysis contributes to the arguments for a new construct, generallyaccepted system security principles, or gssp. as recommended by thecommittee, gssp would provide a broader set of criteria and drive a moreflexible and comprehensive process for evaluating singlevendor (andconglomerate) systems.criteria to evaluate computer and network security124computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.security evaluation criteria in generalat a minimum, security evaluation criteria provide a standard language forexpressing security characteristics and establish an objective basis forevaluating a product relative to these characteristics. thus one can critique suchcriteria based on how well security characteristics can be expressed andevaluated relative to the criteria. security evaluation criteria also serve asframeworks for users (purchasers) and for vendors. users employ criteria in theselection and acquisition of computer and network products, for example, byrelying on independent evaluations to validate vendor claims for security and byusing ratings as a basis for concisely expressing computer and network securityrequirements. vendors rely on criteria for guidance in the development ofproducts and use evaluations as a means of product differentiation. thus it isalso possible to critique security evaluation criteria based on their utility tousers and vendors in support of these goals.these goals of security evaluation criteria are not thoroughlycomplementary. each of the national criteria sets in use (or proposed) todayreflects somewhat different goals and the tradeoffs made by the criteriadevelopers relative to these goals. a separate issue with regard to evaluatingsystem security is how applicable criteria of the sort noted above are tocomplete systems, as opposed to individual computer or network products. thisquestion is addressed below in "system certification vs. product evaluation."before discussing in more detail the goals for product criteria, it is useful toexamine the nature of the security characteristics addressed in evaluation criteria.security characteristicsmost evaluation criteria reflect two potentially independent aspects ofsecurity: functionality and assurance. security functionality refers to thefacilities by which security services are provided to users. these facilities mayinclude, for example, various types of access control mechanisms that allowusers to constrain access to data, or authentication mechanisms that verify auser's claimed identity. usually it is easy to understand differences in securityfunctionality, because they are manifested by mechanisms with which the userinteracts (perhaps indirectly). systems differ in the number, type, andcombination of security mechanisms available.in contrast, security assurance often is not represented by any uservisiblemechanisms and so can be difficult to evaluate. a product rating intended todescribe security assurance expresses an evaluator'scriteria to evaluate computer and network security125computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.degree of confidence in the effectiveness of the implementation of securityfunctionality. personal perceptions of "degree of confidence" are relative, andso criteria for objectively assessing security assurance are based primarily onrequirements for increasingly rigorous development practices, documentation,analysis, configuration management, and testing. relative degrees of assurancealso may be indicated by rankings based on the relative strength of theunderlying mechanisms (e.g., cryptographic algorithms).thus two products that appear to provide the same security functionality toa user may actually provide different levels of assurance because of theparticulars (e.g., relative strength or quality) of the mechanisms used toimplement the functionality or because of differences in the developmentmethodology, documentation, or analysis accorded each implementation. suchdifferences in the underlying mechanisms of implementation should berecognized in an evaluation of security. their significance can be illustrated byanalogy: two painted picnic tables may appear to be identical outwardly, butone is constructed of pressuretreated lumber and the other of untreated lumber.although the functionality of both with regard to table size and seating capacityis identical, the former table may be more durable than the latter because of thematerials used to construct (implement) it.another example illustrates more subtle determinants of assurance. aproduct might be evaluated as providing a high level of assurance because itwas developed by individuals holding u.s. government topsecret clearancesand working in a physically secure facility, and because it came with reams ofdocumentation detailing the system design and attesting to the rigorousdevelopment practices used. but an identical product developed by unclearedindividuals in a nonsecured environment and not accompanied by equivalentdocumentation, would probably receive a much lower assurance rating.although the second product in this example is not necessarily less secure thanthe first, an evaluator probably would have less confidence in the security of thesecond product due to the lack of supporting evidence provided by itsimplementors, and perhaps, less confidence in the trustworthiness of theimplementors themselves.1somewhat analogous is the contrast between buying a picnic table from awellknown manufacturer with a reputation for quality (a member of the "picnictable manufacturers of america") versus purchasing a table from someone whobuilds picnic tables as an avocation. one may have confidence that the formermanufacturer will use good materials and construction techniques (to protect hiscorporate image), whereas the latter may represent a greater risk (unless oneknows the builder or has references from satisfied customers), irrespective ofcriteria to evaluate computer and network security126computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.the actual quality of materials and workmanship. for computers and networks,the technology is sufficiently complex that users cannot, in general, personallyevaluate the security assurance and therefore the quality of the product as theymight the quality of a picnic table. even evaluators cannot thoroughly examineevery aspect of a computer system to the depth one would prefer, hence thereliance on evidence of good development practices, extensive documentation,and so on.security assurance is evaluated in these indirect ways in part becausetesting, specification, and verification technology is not sufficiently mature topermit more direct rankings of assurance. in principle one could begin byspecifying, using a formal specification language, the security policies that atarget product should implement. then one could use verification tools(programs) to establish the correspondence between this specification and aformal toplevel specification (ftls) for the product. this ftls could, in turn,be shown to match the actual implementation of the product in a (highlevel)programming language. the output of the compiler used to translate the highlevel language into executable code would also have to be shown to correspondto the highlevel language. this process could be continued to include firmwareand hardware modules and logic design if one were to impose even morestringent assurance standards.as described in chapter 4 of this report, stateoftheart specification andverification technology does not allow for such a thorough, computerdrivenprocess to demonstrate that a computer or network correctly supports a securitypolicy. experience has shown that there are numerous opportunities for humansubversion of such a process unless it is carried through to the step that includesexamination of the executable code (thompson, 1984), and unless extrememeasures, currently beyond the state of the art, are taken to ensure thecorrectness of the verification tools, compilers, and so on. testing is a usefuladjunct to the process, but the interfaces to the products of interest aresufficiently complex so as to preclude exhaustive testing to detect securityflaws. thus testing can contribute to an evaluator's confidence that securityfunctionality is correctly implemented, but it cannot be the sole basis forproviding a rating based on assurance as well. this explains, in large part, thereliance on indirect evidence of assurance (e.g., documentation requirements,trusted developers, and use of a secure development environment).assurance evaluationthere are actually two stages of assurance evaluation: design evaluationand implementation evaluation. design evaluation attempts to assurecriteria to evaluate computer and network security127computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.that a particular proposed system design actually provides the functionality itattempts rather than simply appearing to do so. some early systems wereconstructed that associated passwords with files, rather than with users, as aform of access control. this approach gave the appearance of providing therequired functionality but in fact failed to provide adequate accountability. thisis an example of a design flaw that would likely be detected and remedied by adesign evaluation process.design evaluation is insurance against making a fundamental design errorand embedding this error so deeply in a system that it cannot later be changedfor any reasonable cost. to support the requirement of confidentiality, thepossible mechanisms are well enough understood that design evaluation maynot be needed to ensure a good design. but for newer areas of functionality,such as supporting the requirement for integrity or secure distributed systems,there is less experience with design options.this committee considers explicit design evaluation to be very important.there are many ways to obtain such review, and vendor prudence may besufficient in some circumstances to ensure that this step is part of systemdesign. however, in general, the committee endorses design evaluation by anindependent team (involving personnel not employed by the vendor) as astandard part of secure system design and encourages that this step beundertaken whenever possible.implementation evaluation is also important, but generally is moredifficult, more time consuming, and more costly. for the level of assurancegenerally required in the commercial market, it may be sufficient to carry out aminimal implementation evaluation (as part of overall system quality assuranceprocedures, including initial operational or beta testing) prior to system releaseif a good design evaluation is performed. moreover, if the incident reportingand tracking system proposed in chapters 1 and 6 is instituted, implementationflaws can be identified and fixed in the normal course of system releases. (ofcourse, wellknown systems with wellknown design flaws continue to be used,and continue to be penetrated. but for systems with modest securitypretensions, many attacks exploit implementation flaws that could be correctedthrough diligent incident reporting and fixing of reported flaws.) by contrast thecurrent implementation evaluation process as practiced by ncsc is very timeconsuming, and because it must occur after implementation, it slows thedelivery of evaluated systems to the marketplace.2for systems attempting to conform to a baseline set of gssp asrecommended by the committee (see chapter 1, "overview andrecommendations," and chapter 2, "concepts of information security"),criteria to evaluate computer and network security128computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.the committee recommends that in the short term a process of evaluatinginstalled systems (field evaluation), rather than the a priori implementationevaluation now carried out by ncsc, be used to increase the level ofimplementation quality.this process of field evaluation, while it shares the basic goal of thecurrent ncsc process, differs from that process in several ways that thecommittee views as advantageous. first, because such field evaluation is lesstime consuming, it may be viewed as less onerous than the current method forimplementation evaluation. it should also be less costly, which would increaseits acceptability. one side effect is that the early customers of a system subjectto field evaluation would not have the full benefit of evaluated securitymechanisms, a situation that would prompt customers with relatively highconcern for security to delay purchase. in exchange for this limitation for earlycustomers, the system would reach the market promptly and then continue toimprove as a result of field experience. this process would also accommodatenew releases and revisions of a system more easily than the current ncscprocedure, the rating maintenance phase (ramp). new releases that revise thefunction of the system should receive an incremental design review. butrevisions to fix bugs would naturally be covered by the normal process of fieldtesting. indeed, it would be hoped that revisions would follow naturally fromthe implementation evaluation.this field evaluation process, if explicitly organized, can focus marketforces in an effective way and lead to the recognition of outside evaluation as avaluable part of system assurance. the committee is concerned that, outside ofthe dod, where the ncsc process is mandated, there is little appreciation ofthe importance of evaluation as an explicit step. instead, the tendency initially isto accept security claims at face value, which can result in a later loss ofcredibility for a set of requirements. for example, customers have confused abad implementation for a bad specification, and rejected a specification whenone system implemented it badly. thus the committee has linked itsrecommendation for the establishment of a broad set of criteria, gssp, with arecommendation to establish methods, guidelines, and facilities for evaluatingproducts with respect to gssp.the committee believes that the way to achieve a system evaluationprocess supported by vendors and users alike is to begin with a designevaluation, based on gssp itself, and to follow up with an implementationevaluation, focusing on field experience and incident reporting and tracking.incident reporting and tracking could have the added effect of documentingvendor attentiveness to security, educating customers, and even illuminatingpotential sources of legal liability. over time,criteria to evaluate computer and network security129computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.the following steps might be anticipated: if gssp were instituted, prudentconsumers would demand gsspconforming systems as a part of normalpractice. gssp would drive field evaluation. if vendors perceived fieldevaluation as helping them in the marketplace or reducing their liability, theywould come to support the process, and perhaps even argue for a strongerimplementation evaluation as a means to obtain a higher assurance rating forsystems. thus gssp could combine with market forces to promotedevelopment of systems evaluated as having relatively high assurance(analogous to the higher levels of the current orange book), a level ofassurance that today does not seem to be justified in the eyes of many vendorsand consumers. for this chain of events to unfold, gssp must be embraced byvendors and users. to stimulate the development of gssp, the committeerecommends basing the initial set of gssp on the orange book (specifically,the committee recommends building from c2 and b1 criteria) and possiblymaking conformance to gssp mandatory in some significant applications, suchas medical equipment or other lifecritical systems.tradeoffs in grouping of criteriain developing product criteria, one of the primary tradeoffs involves theextent to which security characteristics are grouped together. as noted above,aspects of security can be divided into two broad types: functionality andassurance. some criteria, for example, the orange book and the tni, tend to''bundle" together functionality and assurance characteristics to define a smallset of system security ratings. other criteria, for example, the proposed westgerman (zsi) set, group characteristics of each type into evaluation classes butkeep the two types independent, yielding a somewhat larger set of possibleratings. at the extreme, the originally proposed british (dti) criteria (a newevaluation scheme for both government and commercial systems has since beendeveloped (u.k. cesg/dti, 1990)) are completely unbundled, definingsecurity controls and security objectives and a language in which to formulateclaims for how a system uses controls to achieve the objectives. comparisonswith the successor harmonized criteria, the itsec, which builds on both thezsi and dti schemes, are amplified in the section below titled "comparingnational criteria sets."one argument in favor of bundling criteria is that it makes life easier forevaluators, users, and vendors. when a product is submitted for evaluation, aclaim is made that it implements a set of security functions with the requisitelevel of assurance for a given rating. the job of an evaluator is made easier ifthe security functions and assurance techniques against which a product isevaluated have been bundledcriteria to evaluate computer and network security130computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.into a small number of ratings (e.g., six, as in the orange book). becauseevaluators are likely to see many systems that have been submitted for the samerating, they gain experience that can be applied to later evaluations, thusreducing the time required to perform an evaluation.when completely unbundled criteria are used (e.g., the proposed dti set),the evaluators may have to examine anew the collection of security featuresclaimed for each product, since there may not have been previously evaluatedproducts with the same set of features. in this sense, evaluation associated withunbundled criteria would probably become more time consuming and moredifficult (for a system with comparable functionality and assurancecharacteristics) than evaluation against bundled criteria.bundled criteria define what their authors believe are appropriatecombinations of security functions and assurance techniques that will yielduseful products. this signaling of appropriate combinations is an especiallyimportant activity if users and vendors are not competent to define suchcombinations on their own. bundled criteria play a very powerful role inshaping the marketplace for secure systems, because they tend to dictate whatmechanisms and assurances most users will specify in requests for proposalsand what vendors will build (in order to match the ratings).a small number of evaluation ratings helps channel user demands forsecurity to systems that fall into one of a few rated slots. if user demands arenot focused in this fashion, development and evaluation costs cannot beamortized over a large enough customer base. vendors can then be faced withthe prospect of building customdesigned secure systems products, which canbe prohibitively expensive (and thus diminish demand). bundled criteria enablea vendor to direct product development to a very small number of rating targets.a concern often cited for unbundled criteria is that it is possible inprinciple to specify groupings of security features that might, in toto, yield"nonsecure" systems. for example, a system that includes sophisticated accesscontrol features but omits all audit facilities might represent an inappropriatecombination of features. if vendors and users of secure systems were to becomesignificantly more sophisticated, the need to impose such guidance throughbundled criteria would become less crucial. however, there will always be usersand vendors who lack the necessary knowledge and skills to understand howtrustworthy a system may be. the question is whether it is wise to rely onvendors to select "good" combinations of security features for systems and torely on users to be knowledgeable in requesting appropriate groupings ifunbundled criteria are adopted.criteria to evaluate computer and network security131computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.while bundled criteria may protect the naive vendor, they may also limitthe sophisticated vendor, because they do not reward the development ofsystems with security functionality or assurance outside of that prescribed bythe ratings. for example, recent work on security models (clark and wilson,1987) suggests that many security practices in the commercial sector are notwell matched to the security models that underlie the orange book. a computersystem designed expressly to support the clarkwilson model of security, andthus well suited to typical commercial security requirements, might not qualifyunder evaluation based on the orange book. a system that did qualify for anorange book rating and had added functions for integrity to support the clarkwilson model would receive no special recognition for the added functionalitysince that functionality, notably relating to integrity, is outside the scope of theorange book.3the governmentfunded lock project (see appendix b), for example, isone attempt to provide both security functionality and assurance beyond thatcalled for by the highest rating (a1) of the orange book. but because thisproject's security characteristics exceed those specified in the ratings scale,lock (like other attempts to go beyond a1) cannot be "rewarded" for thesecapabilities within the rating scheme. it can be argued that if lock were notgovernment funded it would not have been developed, since a vendor wouldhave no means within the evaluation process of substantiating claims ofsuperior security and users would have no means of specifying thesecapabilities (e.g., in requests for proposals) relative to the criteria (orange book).bundled criteria make it difficult to modify the criteria to adapt tochanging technology or modes of use. changing computer technology imposesthe requirement that security criteria must evolve. the advent of networkingrepresents a key example of this need. for example, as this report is prepared,none of the computers rated by the ncsc includes network interface softwarein the evaluated product, despite the fact that many of these systems will beconnected to networks. this may be indicative, in part, of the greatercomplexity associated with securing a computer attached to a network, but italso illustrates how criteria can become disconnected from developments in theworkplace. for some of these computers, the inclusion of network interfacesoftware will not only formally void the evaluation but will also introduceunevaluated, securitycritical software. this experience argues strongly thatevaluation criteria must be able to accommodate technological evolution so thatfielded products remain true to their evaluations.the discussion and examples given above demonstrate that constraints onthe evolving marketplace can occur unless evaluation criteria cancriteria to evaluate computer and network security132computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.be extended to accommodate new paradigms in security functionality orassurance. such problems could arise with unbundled criteria, but criteria likethe orange book set seem especially vulnerable to paradigm shifts becausetheir hierarchic, bundled nature makes them more difficult to extend.based on these considerations, the committee concludes that in the future asomewhat less bundled set of security criteria will best serve the needs of theuser and vendor communities. it is essential to provide for evolution of thecriteria to address new functions and new assurance techniques. the committeealso believes that naive users are not well served by bundled criteria, but ratherare misled to believe that complex security problems can be solved by merelyselecting an appropriately rated product. if naive users or vendors needprotection from the possibility of selecting incompatible features from thecriteria, this can be made available by providing guidelines, which can suggestcollections of features that, while useful, are not mandatory, as bundled criteriawould be.comparing national criteria setsthe orange book and its trusted network interpretation, the red book,establish ratings that span four hierarchical divisions: d, c, b, and a, inascending order. the "d" rating is given to products with negligible or nosecurity; the "c," "b," and "a'' ratings reflect specific, increasing provision ofsecurity. each division includes one or more classes, numbered from 1 (that is,stronger ratings correlate with higher numbers), that provide finergranularityratings. thus an evaluated system is assigned a digraph, for example, c2 or a1,that places it in a class in a division. at present, the following classes exist, inascending order: c1, c2, b1, b2, b3, and a1. a summary of criteria for eachclass, reproduced from the orange book's appendix c, can be found inappendix a of this report. there are significant, security functionalitydistinctions between divisionc and divisionb systems. in particular, the cdivision provides for discretionary access control, while the b division addsmandatory access control. a1 systems, the only class today within the adivision, add assurance, drawing on formal design specification andverification, but no functionality, to b3 systems. assurance requirementsincrease from one division to the next and from one class to the next within adivision. the orange book describes b2 systems as relatively resistant, and b3as highly resistant, to penetration. the robustness of these and higher systemscomes from their added requirements for functionality and/or assurance, whichin turn drive greater attention to security, beginningcriteria to evaluate computer and network security133computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.in the early stages of development. that is, more effort must be made to buildsecurity in, as opposed to adding it on, to achieve a b2 or higher rating.in these u.s. criteria, both the language for expressing securitycharacteristics and the basis for evaluation are thus embodied in therequirements for each division and class. this represents a highly "bundled"approach to criteria in that each rating, for example, b2, is a combination of aset of security functions and security assurance attributes.the information technology security evaluation criteria (itsec)štheharmonized criteria of france, germany, the netherlands, and the unitedkingdom (federal republic of germany, 1990)šrepresents an effort toestablish a comprehensive set of security requirements for widespreadinternational use. itsec is generally intended as a superset of tcsec, withitsec ratings mappable onto the tcsec evaluation classes (see below).historically, itsec represents a remarkably easily attained evolutionarygrafting together of evaluation classes of the german (light) green book(gisa, 1989) and the "claims language" of the british (dark) green books(u.k. dti, 1989). itsec unbundles functional criteria (f1 to f10) andcorrectness criteria (e0 as the degenerate case, and e1 to e6), which areevaluated independently.the functional criteria f1 to f5 are of generally increasing merit andcorrespond roughly to the functionality of tcsec evaluation classes c1, c2,b1, b2, and b3, respectively. the remaining functionality criteria address dataand program integrity (f6), system availability (f7), data integrity incommunication (f8), data confidentiality in communication (f9), and networksecurity, including confidentiality and integrity (f10). f6 to f10 may inprinciple be evaluated orthogonally to each other and to the chosen base level,f1, f2, f3, f4, or f5.the correctness criteria are intended to provide increased assurance. to afirst approximation, the correctness criteria cumulatively require testing (e1),configuration control and controlled distribution (e2), access to the detaileddesign and source code (e3), rigorous vulnerability analysis (e4), demonstrablecorrespondence between detailed design and source code (e5), and formalmodels, formal descriptions, and formal correspondences between them (e6).e2 through e6 correspond roughly to the assurance aspects of tcsecevaluation classes c2, b1, b2, b3, and a1, respectively.itsec's unbundling has advantages and disadvantages. on the whole it isa meritorious concept, as long as assurance does not become a victim ofcommercial expediency, and if the plethora of rating combinations does notcause confusion.a particular concern with the itsec is that it does not mandatecriteria to evaluate computer and network security134computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.any particular modularity with respect to system architecture. in particular, itdoes not require that the securityrelevant parts of the system be isolated into atrusted computing base, or tcb. it is of course possible to evaluate an entiresystem according to itsec without reference to its composability (e.g., as anapplication on top of a tcb), but this complicates the evaluation and fails totake advantage of other related product evaluations. the effectiveness of thisapproach remains to be seen.the initial itsec draft was published and circulated for comment in 1990.hundreds of comments were submitted by individuals and organizations fromseveral countries, including the united states, and a special meeting ofinterested parties was held in brussels in september 1990. in view of thevolume and range of comments submitted, plus the introduction of a differentproposal by eurobit, a european computer manufacturers' trade association,a revised draft is not expected before mid1991.the dynamic situation calls for vigilance and participation, to the extentpossible, by u.s. interests. at present, the national institute of standards andtechnology (nist) is coordinating u.s. inputs, although corporations andindividuals are also contributing directly. it is likely that the complete processof establishing harmonized criteria, associated evaluation mechanisms, andrelated standards will take some time and will, after establishment, continue toevolve. because the european initiatives are based in part on a reaction to thenarrowness of the tcsec, and because nist's resources are severelyconstrained, the committee recommends that gssp and a new organization tospearhead gssp, the information security foundation, provide a focus forfuture u.s. participation in international criteria and evaluation initiatives.reciprocity among criteria setsa question naturally arises with regard to comparability and reciprocity ofthe ratings of different systems. even though ratings under one criteria set maybe mappable to roughly comparable ratings under a different criteria set, themapping is likely to be imprecise and not symmetric; for example, themappings may be manytoone. even if there is a reasonable mapping betweensome ratings in different criteria, one country may refuse to recognize theresults of an evaluation performed by an organization in another country, forpolitical, as well as technical, reasons. the subjective nature of the ratingsprocess makes it difficult, if not impossible, to ensure consistency amongevaluations performed at different facilities, by different evaluators, in differentcountries, especially when one adds the differences in thecriteria to evaluate computer and network security135computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.criteria themselves. in such circumstances it is not hard to imagine how securityevaluation criteria can become the basis for erecting barriers to internationaltrade in computer systems, much as some have argued that internationalstandards have become (frenkel, 1990). reciprocity has been a thorny problemin the comparatively simpler area of rating conformance to interoperabilitystandards, where testing and certification are increasingly in demand, and thereis every indication it will be a major problem for secure systems.multinational vendors of computer systems do not wish to incur the costsand delay to market associated with multiple evaluations under differentnational criteria sets. equally important, they may not be willing to reveal toforeign evaluators details of their system design and their development process,which they may view as highly proprietary. the major u.s. computer systemvendors derive a significant fraction of their revenue from foreign sales and thusare especially vulnerable to proliferating, foreign evaluation criteria. at thesame time, the ncsc has interpreted its charter as not encompassing evaluationof systems submitted by foreign vendors. this has stimulated the developmentof foreign criteria and thus has contributed to the potential conflicts amongcriteria on an international scale.analyses indicate that one can map any of the orange book ratings ontoan itsec rating. a reverse mapping (from itsec to orange book ratings) isalso possible, although some combinations of assurance and functionality arenot well represented, and thus the evaluated product may be "underrated."however, the itsec claims language may tend to complicate comparisons ofitsec ratings with one another.products evaluated under the orange book could be granted itsec ratingsand ratings under other criteria that are relatively unbundled. this should begood news for u.s. vendors, if rating reciprocity agreements are enactedbetween the united states and foreign governments. of course, a u.s. vendorcould not use reciprocity to achieve the full range of ratings available tovendors who undergo itsec evaluation directly.even when there are correspondences between ratings under differentcriteria, there is the question of confidence in the evaluation process as carriedout in different countries.4 discussions with ncsc and nsa staff suggest thatreciprocity may be feasible at lower levels of the orange book, perhaps b1 andbelow, but not at the higher levels (committee briefings; personalcommunications). in part this sort of limitation reflects the subjective nature ofthe evaluation process. it may also indicate a reluctance to rely on "outside"evaluation for systems that would be used to separate multiple levels of dodclassified data. if other countries were to take a similar approach forcriteria to evaluate computer and network security136computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.high assurance levels under their criteria, then reciprocity agreements would beof limited value over time (as more systems attain higher ratings). anotherlikely consequence would be a divergence between criteria and evaluations forsystems intended for use in defense applications and those intended for use incommercial applications.system certification vs. product evaluationthe discussion above has addressed security evaluation criteria that focuson computer and network products. these criteria do not address all of thesecurity concerns that arise when one actually deploys a system, whether itconsists of a single computer or is composed of multiple computer and networkproducts from different vendors. procedural and physical safeguards, and othersfor personnel and emanations, enter into overall system security, and these arenot addressed by product criteria. overall system security is addressed byperforming a thorough analysis of the system in question, taking into accountnot only the ratings of products that might be used to construct the system, butalso the threats directed against the system and the concerns addressed by theother safeguards noted above, and producing a security architecture that addressall of these security concerns.the simple ratings scheme embodied in the orange book and the tnihave led many users to think in terms of product ratings for entire systems.thus it is not uncommon to hear a user state that his system, which consists ofnumerous computers linked by various networks, all from different vendors,needs to be, for example, b1. this statement arises from a naive attempt toapply the environment guidelines developed for the orange book to entiresystems of much greater complexity and diversity. it leads to discussions ofwhether a network connecting several computers with the same rating is itselfrated at or below the level of the connected computers. such discussions, byadopting designations developed for product evaluation, tend to obscure thecomplexity of characterizing the security requirements for real systems and thedifficulty of designing system security solutions.in fact, the term "evaluation" is often reserved for products, not deployedsystems. instead, at least in the dod and intelligence communities, systems arecertified for use in a particular environment with data of a specified sensitivity.5unfortunately, the certification process tends to be more subjective and lesstechnically rigorous than the product evaluation process. certification ofsystems historically preceded orange bookstyle product evaluation, andcertification criteria are typically less uniform, that is, varying from agency toagency.criteria to evaluate computer and network security137computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.nonetheless, certification does attempt to take into account the full set ofsecurity disciplines noted above and thus is more an attempt at a systemsapproach to security than it is product evaluation.certified systems are not rated with concise designations, and standards forcertification are less uniform than those for product evaluation, so that userscannot use the results of a certification applied to an existing system to simplyspecify security requirements for a new system. unlike that from productevaluations, the experience gained from certifying systems is not so easilycodified and transferred for use in certifying other systems. to approach thelevel of rigor and uniformity comparable to that involved in product evaluation,a system certifier would probably have to be more extensively trained than hiscounterpart who evaluates products. after all, certifiers must be competent inmore security disciplines and be able to understand the security implications ofcombining various evaluated and unevaluated components to construct a system.a user attempting to characterize the security requirements for a system heis to acquire will find applying system certification methodology a priori amuch more complex process than specifying a concise product rating based on areading of the tcsec environment guidelines (yellow book; u.s. dod,1985b). formulating the security architecture for a system and selectingproducts to realize that architecture are intrinsically complex tasks that requireexpertise most users do not possess. rather than attempting to cast systemsecurity requirements in the very concise language of a product ratings schemesuch as the orange book, users must accept the complexity associated withsystem security and accept that developing and specifying such requirementsare nontrivial tasks best performed by highly trained security specialists.6in large organizations the task of system certification may be handled byinternal staff. smaller organizations will probably need to enlist the services ofexternal specialists to aid in the certification of systems, much as structuralengineers are called in as consultants. in either case system certifiers will needto be better trained to deal with increasingly complex systems with increasedrigor. a combination of formal training and realworld experience areappropriate prerequisites for certifiers, and licensing (including formalexamination) of consulting certifiers may also be appropriate.increasingly, computers are becoming connected via networks and arebeing organized into distributed systems. in such environments a much morethorough system security analysis is required, and the product rating associatedwith each of the individual computers is in no way a sufficient basis forevaluating the security of the system as a whole. this suggests that it willbecome increasingly important tocriteria to evaluate computer and network security138computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.develop methodologies for ascertaining the security of networked systems, notjust evaluations for individual computers. product evaluations are not applicableto whole systems in general, and as "open systems" that can be interconnectedrelatively easily become more the rule, the need for system security evaluation,as distinct from product evaluation, will become even more critical.many of the complexities of system security become apparent in thecontext of networks, and the tni (which is undergoing revision) actuallyincorporates several distinct criteria in its attempt to address these variedconcerns. part i of the tni provides product evaluation criteria for networks,but since networks are seldom homogeneous products this portion of the tniseems to have relatively little direct applicability to real networks. part ii andappendix a of the tni espouse an unbundled approach to evaluation ofnetwork components, something that seems especially appropriate for suchdevices and that is similar to the itsec f9 and f10 functionality classes.however, many of the ratings specified in part ii and appendix a of the tniare fairly crude; for example, for some features only "none" or "present" ratingsmay be granted. more precise ratings, accompanied by better characterizationsof requirements for such ratings, must be provided for these portions of the tnito become really useful. appendix c of the tni attempts to provide genericrules to guide users through the complex process of connecting rated productstogether to form trusted systems, but it has not proven to be very useful. this isclearly a topic suitable for further research (see chapter 8).recommendations for product evaluation andsystem certification criteriathe u.s. computer industry has made a significant investment indeveloping operating systems that comply with the orange book. this realityargues against any recommendation that would undercut that investment orundermine industry confidence in the stability of security evaluation criteria.yet there are compelling arguments in favor of establishing lessbundledcriteria to address some of the shortcomings cited above. this situation suggestsa compromise approach in which elements from the orange book are retainedbut additional criteria, extensions of the tcsec, are developed to address someof these arguments. this tack is consistent with the recommendations for gsspmade in chapter 1, which would accommodate security facilities generallyregarded as useful but outside the scope of the current criteria, for example,those supporting the model for clarkwilson integrity (clark and wilson, 1987).criteria to evaluate computer and network security139computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.the importance of maintaining the momentum generated by the orangebook process and planning for some future reciprocity or harmonization ofinternational criteria sets makes modernization of the orange book necessary,although the committee anticipates a convergence between this process and theprocess of developing gssp. in both instances, the intent is to reward vendorswho wish to provide additional security functionality and/or greater securityassurance than is currently accommodated by the orange book criteria. thetni should be restructured to be more analogous to the itsec (i.e., with lessemphasis on parts i and ii and more on a refined appendix a). the tni is newenough so as not to have acquired a large industry investment, and it is nowundergoing revision anyway. thus it should be politically feasible to modify thetni at this stage.the itsec effort represents a serious attempt to transcend some of thelimitations in the tcsec, including the criteria for integrity and availability.however, it must be recognized that neither tcsec nor itsec provides theultimate answer, and thus ongoing efforts are vital. for example, a weakness ofitsec is that its extended functional criteria f6 through f10 are independentlyassessable monolithic requirements. it might be more appropriate if integrityand availability criteria were graded similarly to criteria fl through f5 forconfidentiality, with their own hierarchies of ratings. (the draft canadiancriteria work in that direction.)there is also a need to address broader system security concerns in amanner that recognizes the heterogeneity of integrated or conglomeratesystems. this is a matter more akin to certification than to product evaluation.to better address requirements for overall system security, it will benecessary to institute more objective, uniform, rigorous standards for systemcertification. the committee recommends that gssp include relevant guidelinesto illuminate such standards. to begin, a guide for system certification shouldbe prepared, to provide a more uniform basis for certification. a committeeshould be established to examine existing system certification guidelines andrelated documentationšfor example, password management standardsšfromgovernment and industry as input to these guidelines. an attempt should bemade to formalize the process of certifying a conglomerate system composed ofevaluated systems, recognizing that this problem is very complex and mayrequire a high degree of training and experience in the certifier. developmentand evaluation of heterogeneous systems remain crucial research issues.for systems where classified information must be protected, a further kindof criteria development is implied, notably development of ancriteria to evaluate computer and network security140computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.additional assurance class within the a division, for example, a2 (this isprimarily for government, not commercial, users),7 as well as functionalityextensions for all divisions of the orange book.the committee's conclusions and specific recommendations, which arerestated in chapter 1 under recommendation 1, are as follows:1. a new generation of evaluation criteria is required and should beestablished, to deal with an expanded set of functional requirements forsecurity and to respond to the evolution of computer technology, forexample, networking. these criteria can incorporate the securityfunctions of the existing tcsec (at the c2 or b1 level) and thuspreserve the present industry investment in orange bookratedsystems. the committee's proposed gssp are intended to meet thisneed.2. the new generation of criteria should be somewhat unbundled,compared to the current tcsec, both to permit the addition of newfunctions and to permit some flexibility in the assurance methodologyused. guidelines should be prepared to prevent naive users fromspecifying incompatible sets of requirements. the itsec represents areasonable example of the desirable degree of unbundled specification.3. systems designed to conform to gssp should undergo explicitevaluation for conformance to the gssp criteria. design evaluationshould be performed by an independent team of evaluators.implementation evaluation should include a combination of explicitsystem audit, field experience, and organized reporting of securityfaults. such a process, which should be less costly and less onerousthan the current ncsc process, is more likely to be costeffective tothe vendor and user, and is more likely to gain acceptance in the market.4. effort should be expended to develop and improve the organizedmethods and criteria for dealing with complete systems, as opposed toproducts. this applies particularly to distributed systems, in whichvarious different products are connected by a network.notes1. in the current environment, in which evaluations have been conducted by the ncsc, commercialsystem developers may face a greater challenge than those with defense contracting experience, whomay have both cleared personnel and a working understanding of the documentation requirements.this practical problem underscores the need for a more effective interface between the commercialand the national security or classified worlds.2. based on information obtained in a briefing from ncsc officials, the ncsc evaluation processconsists of five phases, including: (1) prereview phase, (2) vendorcriteria to evaluate computer and network security141computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.assistance phase (vap), (3) design analysis phase, (4) formal evaluation phase, and (5) ratingmaintenance phase (ramp).in the prereview phase vendors present the ncsc with a proposal defining the goals they expect toachieve and the basic technical approach being used. the prereview proposal is used to determinethe amount of ncsc resources needed to perform any subsequent evaluation. the vendorassistance phase, which can begin at any stage of product development, consists primarily ofmonitoring and providing comments. during this phase, the ncsc makes a conscious effort not to"advise" the vendors (for legal reasons and because it is interested in evolution, not research anddevelopment). the vendor assistance phase usually ends six to eight months before a product isreleased. the design analysis phase takes an indepth look at the design and implementation of aproduct using analytic tools. during this phase the initial product analysis report (ipar) isproduced, and the product is usually released for beta testing. the formal evaluation phaseincludes both performance and penetration testing of the actual product being produced. productsthat pass these tests are added to the evaluated products list (epl) at the appropriate level. usuallyvendors begin shipping their product to normal customers during this phase. the ratingmaintenance phase (ramp), which takes place after products are shipped and pertains toenhancements (e.g., movement from one version of a product to another), is intended for c2 and b1systems, to enable vendors to improve their product without undergoing a complete recertification.3. the ncsc has argued that it is premature to adopt criteria that address security features thatsupport clarkwilson integrity because formal models for such security policies do not yet exist. inthis way they justify the present bundled structure of the tcsec (committee briefing by nsa). thencsc continues to view integrity and assured service as research topics, citing a lack of formalpolicy models for these security services. however, it is worth noting that the orange book does notrequire a system to demonstrate correspondence to a formal security policy model until class b2,and the preponderance of rated systems in use in the commercial sector are below this level, forexample, at the c2 level. thus the ncsc argument against unbundling the tcsec to includeintegrity and availability requirements in the criteria, at least at these lower levels of assurance, doesnot appear to be consistent.4. in the future software tools that capture key development steps may facilitate evaluation andcrosschecks on evaluations by others.5. in the dod environment the term "accreditation" refers to formal approval to use a system in aspecified environment as granted by a designated approval authority. the term "certification" refersto the technical process that underlies the formal accreditation.6. the claims language of the itsec may be more amenable to system security specification.however, product evaluation and system certification are still different processes and should not beconfused, even if the ratings terminology can be shared between the two.7. proposals for an a2 class have been made before with no results, but lock and other projectssuggest that it may now be time to extend the criteria to provide a higher assurance class. this classcould apply formal specification and verification technology to a greater degree, require morestringent control on the development process (compare to the itsec e6 and e7), and/or call forstronger security mechanisms (e.g., the lock sidearm and bed technology, described inappendix b of this report). the choice of which additional assurance features might be included ina2 requires further study.criteria to evaluate computer and network security142computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.6why the security market has not workedwellcurrently available are a wide variety of goods and services intended toenhance computer and communications security. these range from accessorydevices for physical security, identification, authentication, and encryption toinsurance and disaster recovery services, which provide computer andcommunications centers as a backup to an organization's or individual's ownequipment and facilities. this chapter focuses on the market for secure ortrusted systems and related products, primarily software. it provides anoverview of the market and its problems, outlines the influences of the federalgovernment on this market, discusses the lack of consumer awareness andoptions for alleviating it, and assesses actual and potential governmentregulation of the secure system market. additional details on the export controlprocess and insurance are provided in two chapter appendixes.the market for trustworthy systemssecure or trusted information systems are supplied by vendors of generaland specialpurpose hardware and software. overall, the market for thesesystems has developed slowly, although the pace is picking up somewhat now.whereas the market in 1980 was dominated by commercial computer andcommunications systems with no security features, the market in 1990 includesa significant number of systems that offer discretionary access control and agrowing number from both major and niche vendors with both discretionaryand mandatory access control, which provides significant protections againstbreaches of confidentiality. notable is the trend to produce systems rated at theorange book's b1 level (see appendix a of this report), often bywhy the security market has not worked well143computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.adapting products that had had fewer security features and less assurance.according to vendors, consumers most frequently demand security inconnection with networked systems, which serve multiple users. one marketresearch firm (international resource development) has estimated that themarket for local area network (lan) security devices may grow up to sixfoldby the mid1990s; it also foresees significant growth in data and voiceencryption devices, in part because their costs are declining (brown, 1989a).other factors cited for growth in the encryption market are requirements forcontrol of fraud in financial services and elsewhere (datapro research, 1989a).prominent in the market has been host access control software for ibmmainframes, especially ibm's racf and computer associates' acf2 and topsecret. this type of addon software provides (but does not enforce) services,such as user identification, authentication, authorization, and audit trails, thatthe underlying operating systems lack. it was originally developed in the 1970sand early 1980s, driven by the spread of multiaccess applications (mainframebased systems were not originally developed with security as a significantconsideration). both ibm and computer associates plan to make theseproducts conform to orange book b1 criteria. although ibm intends now tobring its major operating systems up to the b1 level, it is reluctant to undertakedevelopment to achieve higher levels of assurance (committee briefing byibm). moreover, the market for host access control systems is growing slowlybecause those who need them generally have them already.1 one marketanalyst, datapro, notes that sales come mostly from organizations required byfederal or state regulations to implement security controls (datapro research,1990a).the most powerful alternatives to addon software, of course, are systemswith security and trust built in. in contrast to the mainframe environment, somevendors have been building more security features directly into midrange andopen systems, possibly benefiting from the more rapid growth of this part of themarket. even in the personal computer market, newer operating systems (e.g.,os/2) offer more security than older ones (e.g., ms/dos).multics, the first commercial operating system that was developed (by themassachusetts institute of technology, general electric, and at&t belllaboratories) with security as a design goal, achieved a b2 rating in 1985.while multics has a loyal following and is frequently cited as a prime exemplarof system security, its commercial history has not been encouraging. its pendingdiscontinuation by its vendor (now bull, previously honeywell, originallygeneral electric) apparently reflects a strategic commitment to other operatingsystems (datapro research, 1990b).why the security market has not worked well144computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.the history of unix illustrates the variability of market forces during thelifetime of a single product. originally unix had security facilities superior tothose in most commercial systems then in widespread use.2 unix wasenthusiastically adopted by the academic computer science community becauseof its effectiveness for software development. this community, where securityconsciousness was not widespread, created new capabilities, especially tointerface to darpasponsored networking (e.g., remote login and remotecommand execution).3 as unix spread into the commercial marketplace, thenew capabilities were demanded despite the fact that they undermined theability to run a tight ship from the security standpoint. subsequently, andlargely spurred by the orange book, various efforts to strengthen the unixsystem have been undertaken (including tmach, funded by darpa; lock,funded by the national security agency; the ieee posix 1003.6 standardsproposal; and various manufacturers' projects). but the corrections will not betotal: many customers still choose freedom over safety.the slow growth of the market for secure software and systems feedsvendor perceptions that its profitability is limited. both high development costsand a perceived small market have made secure software and systemdevelopment appear as a significant risk to vendors. moreover, a vendor thatintroduces a secure product before its competitors has only a year or two tocharge a premium. after that, consumers come to expect that the new attributeswill be part of the standard product offering. thus the pace of change andcompetition in the overall market for computer technology may be inimical tosecurity, subordinating securityrelevant quality to creativity, functionality, andtimely releases or upgrades. these other attributes are rewarded in themarketplace and more easily understood by consumers and even softwaredevelopers.while the overall market for computer technology is growing andbroadening, the tremendous growth in retail distribution, as opposed to customor lowvolume/highprice sales, has helped to distance vendors from consumersand to diminish the voice of the growing body of computer users in vendordecision making. although vendors have relatively direct communications withlargesystem customersšcustomers whom they know by name and with whomthey have individualized contractsšthey are relatively removed from buyers ofpersonal computer products, who may be customers of a retail outlet rather thanof the manufacturer itself. retail distribution itself may constrain the marketingof security products. vendors of encryption and access control products haveindicated that some retailers may avoid offering security products because ''theissue of security dampens enthusiasm," while some of these relatively smallvendors avoid retailwhy the security market has not worked well145computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.distribution because it requires more customer support than they can manage(datapro research, 1989a).many in the security field attribute the increased availability of moresecure systems to government policies stimulating demand for secure systems(see "federal government influence on the market" below). those policieshave led to a twotiered market: government agencies, especially those thatprocess classified information, and their vendors, are likely to demand orangebookrated trusted systems; other agencies, commercial organizations, andindividuals that process sensitive but unclassified information are more likely touse less sophisticated safeguards. this second market tier constitutes the bulk ofthe market for computerbased systems. the committee believes that, moreoften than not, consumers do not have enough or good enough safeguards, bothbecause options on the market often appear to be ineffective or too expensive,and because the value of running a safe operation is often not fully appreciated.since data describing the marketplace are limited and of questionable quality,the committee bases its judgment on members' experiences in major systemuser and vendor companies and consultancies. this judgment also reflects thecommittee's recognition that even systems conforming to relatively high orangebook ratings have limitations, and do not adequately address consumer needsfor integrity and availability safeguards.a soft market: concerns of vendorsvendors argue that a lack of broadbased consumer understanding ofsecurity risks and safeguard options results in relatively low levels of demandfor computer and communications security. for example, one survey of networkusers found that only 17 percent of fortune 1000 sites and 10 percent of othersites used network security systems (network world, 1990). thus, althoughmarket research may signal high growth rates in certain security markets, theabsolute market volume is small. to gain insight into the current market climatefor secure products, the committee interviewed several hardware and softwarevendors.vendors find security hard to sell, in part because consumers and vendorshave very different perceptions of the security problem.4 this situation calls forcreative marketing: one vendor stresses functionality in marketing operatingsystem software for singleuser systems and security in marketing essentiallythe same software for multiuser local area networked systems. a commonlyreported problem is limited willingness of management to pay for security,although the rise in expectations following publicity over major computercrimes suggestswhy the security market has not worked well146computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.that at least at the technical level, consumers are ready for more security. fromthe consumer's perspective, it is easy to buy something that is cheap; buyingsomething expensive requires risk assessment and an investment in persuadingmanagement of the need. vendors observed that they hear about whatconsumers would like, but they do not hear consumers say that they will notbuy products that lack certain security features.vendors differ in their attitudes toward the orange book as a stimulus tocommercial product security. some indicated that they saw the government asleading the market; others characterized the government as a force thatmotivates their customers but not them directly. vendors familiar with theorange book find it offers little comfort in marketing. for example, onecustomer told a sales representative that he did not need the capabilitiesrequired by the orange book and then proceeded to list, in his own words,requirements for mandatory access control and complete auditing safeguards,which are covered extensively in the orange book. overall, vendors maintainedthat the orange book has had limited appeal outside the governmentcontracting market, in part because it is associated with the military and in partbecause it adds yet more jargon to an already technically complex subject. thissentiment echoes the findings of another study that gathered inputs fromvendors (afcea, 1989). vendors also indicated that marketing a productdeveloped in the orange book environment to commercial clients requiredspecial tactics, extra work that most have been reluctant to undertake.vendors also complained that it is risky to develop products intended forgovernment evaluation (associated with the orange book) because theevaluation process itself is expensive for vendorsšit takes time and money tosupply necessary informationšand because of uncertainty that the desiredrating will be awarded. time is a key concern in the relatively fastpacedcomputer system market, and vendors complain about both the time to completean evaluation and the timing of the evaluation relative to the product cycle. thevendor's product cycle is driven by many factorsšcompetition, marketdemands for functionality, development costs, and compatibility and synchronywith other productsšof which security is just one more factor, and a factor thatis sometimes perceived as having a negative impact on some of the others.while vendors may have a product developmenttorelease cycle that takesabout three to six years, the evaluations have tended to come late in the productcycle, often resulting in the issuing of ratings after a product has beensuperseded by newer technology.the time to complete an evaluation has been a function of nationalwhy the security market has not worked well147computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.computer security center (ncsc) resources and practice. ncsc's schedule hasbeen driven by its emphasis on security, the perceived needs of its principalclients in the national security community, and the (limited) availability ofevaluation staff. by 1990, ncsc was completing evaluations at a rate of aboutfive per year, although the shift from evaluating primarily clevel systems toprimarily blevel systems was expected to extend the time required perevaluation (anthes, 1989d; committee briefing by nsa). the time involvedreflects the quality of the evaluation resources: individuals assigned to doevaluations have often had limited, if any, experience in developing oranalyzing complex systems, a situation that extends the time needed tocomplete an evaluation; both vendors and ncsc management have recognizedthis. further, as a member of the ncsc staff observed to the committee, "wedon't speed things up." as of late october 1990, 1 system had obtained an a1rating, none had been rated b3, 2 had been rated b2, 3 had been rated b1, 13had been rated c2, and 1 had been rated c1 (personal communication, nsa,october 26, 1990). prospects for future evaluations are uncertain, in view of therecent reorganization of the ncsc (see chapter 7).vendors have little incentive to produce ratable systems when the absenceof rated products has not detectably impaired sales. customers, evengovernment agencies that nominally require rated products, tend to buywhatever is available, functionally desirable, and or compatible with previouslypurchased technology. customer willingness to buy unrated products that comeonly with vendor claims about their security properties suggests possibilities forfalse advertising and other risks to consumers.consider the multilevel secure database management system released bysybase in february 1990 (danca, 1990a). the secure server, as it is called, wasdesigned and developed to meet b1level requirements for mandatory accesscontrol as defined in the orange book. the development for that product beganin 1985, with the initial operational (beta) release in the spring of 1989. the airforce adopted the secure server in its next version of the global decisionsupport system (gdss), which is used by the military airlift command tomonitor and control worldwide airlift capabilities. however, at the time of itsrelease, the secure server had not been evaluated against the orange bookcriteria because the relevant criteria, contained in the trusted databaseinterpretation (tdi), were still being reviewed. although the tdi is expected tobe released in late 1990 or early 1991, it will be at least six months (andprobably nine months) before any official opinion is rendered by ncsc. inshort, sybase will be marketing a secure product that took five years to developand the air force will be using thatwhy the security market has not worked well148computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.product for a full year before any evaluation information is released. both thevendors and consumers have proceeded with some degree of risk.federal government influence on the marketthe federal government has tried to influence commercialgrade computersecurity through direct procurement, research support, and regulatoryrequirements placed on the handling of data in the private sector. that influencehas been realized both directly through government actions (e.g., procurementand investment in research) and indirectly through regulations and policies thatprovide incentives or disincentives in the marketplaces.5 the influence of theorange book is discussed in chapters 2 to 5 and in appendix a. procurementand strategic research programs are discussed briefly below.procurementthe u.s. government has tried to suggest that a strong government andcommercial market would exist for security products were such productsavailable (eia, 1987). industry is skeptical of such promises, arguing that thegovernment does not follow through in its procurement (afcea, 1989), evenafter sponsoring the development of special projects for militarycriticaltechnology. however, one step the government has taken that has apparentlystimulated the market is known as "c2 by '92." a directive (ntissp no. 200,issued on july 15,1987) of the national telecommunications and informationsystems security committee (ntissc), the body that develops and issuesnational system security operating policies, required federal agencies and theircontractors to install by 1992 discretionary access control and auditing at theorange book c2 level in multiuser computer systems containing classified orunclassified but sensitive information. this directive is widely believed to havestimulated the production of c2level systems. however, its impact in the futureis in question, given the divergence in programs for protecting classified andsensitive but unclassified information that has been reinforced by the computersecurity act of 1987 and the revision of national security decision directive145 (see chapter 7). the computer security act itself has the potential forincreasing the demand for trusted systems, but the security assessment andplanning process it triggered fell short of expectations (gao, 1990c).concern for security is not a consistent factor in governmentprocurements. a small sample, compiled by the committee, of 30 recentwhy the security market has not worked well149computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.(1989) requests for proposal (rfps), 10 of which were issued by dodorganizations and 20 of which were issued by the civil agencies, presents apicture of uneven concern for security: five rfps had no stated securityrequirements. five dod and eight civil agency rfps specified adherence tostandards defined by the ncsc and the national institute of standards andtechnology (nist), although three of the dod rfps did not specify an orangebook level. two dod and three civil agency rfps indicated that unclassifiedbut protectable data would be handled. none of the dod rfps specifiedencryption requirements; three civil agency rfps required data encryptionstandard (des) encryption, and one required nsaapproved encryptiontechnology. access control features were required by 13 rfps. auditingfeatures were required by six.the procurement process itself provides vehicles for weakening thedemand for security. vendors occasionally challenge (through mechanisms forcomment within the procurement process) strong security requirements inrfps, on the grounds that such requirements limit competition. for example, ac2 requirement for personal computers was dropped from an rfp from the airforce computer acquisition command (afcac) because conforming systemswere not available (poos, 1990). budgetary pressures may also contribute toweakening security requirements. such pressures may, for example, result inthe inclusion of security technology as a nonevaluated option, rather than as arequirement, leading to a vendor perception that the organization is only payinglip service to the need for security.interestingly, dod itself is exploring novel ways to use the procurementprocess to stimulate the market beyond the orange book and military standards.in 1989 it launched the protection of logistics unclassified/sensitive systems(plus) program to promote standards for secure data processing and dataexchange among dod and its suppliers. plus complements other dod effortsto automate procurement procedures (e.g., electronic data interchange andcomputeraided acquisition and logistics support (cals) programs), helpingto automate procurement (kass, 1990). a subsidiary goal of plus is cheapercommercial security products (personal communication with plus staff).strategic federal investments in research and developmentthe government, especially through darpa funding, has contributed tocomputer technology through largescale strategic research and developmentprograms that supported the creation or enhancement of facilities such as the(recently decommissioned) arpanet networkwhy the security market has not worked well150computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.serving researchers, multics and adept 50 (operating systems with securityfeatures), mach (an extension of the unix operating system that fullyintegrates network capabilities and that has been championed by the industryconsortium open software foundation), and the connection machine (anadvanced parallel processor). each of these projectsšwhich were sponsored bydarpašhas moved the market into areas that are beneficial to bothgovernment and commercial computer users. the arpanet and multicsexperiences illustrate how very large scale, multifaceted, systemsorientedprojects can catalyze substantial technological advances, expand the level ofexpertise in the research community, and spin off developments in a number ofareas. scale, complexity, and systems orientation are particularly important forprogress in the computer and communications security arena, and thegovernment is the largest supporter of these projects. historically, security hasbeen a secondary concern in such projects, although it is gaining more attentionnow. the widespread impact of these projects suggests that similar initiativesemphasizing security could pay off handsomely.in the security field specifically, projects such as multics and adept 50(which provided strong access control mechanisms), lock (hardwarebasedintegrity and assurance), seaview (a secure database management system),tmach (a trusted or secure version of mach), and the ccep (commercialcomsec endorsement program for commercially produced encryptionproducts) are intended to stimulate the market to develop enhanced securitycapabilities by reducing some of the development risks. the lock program,for example, was designed to make full documentation and background materialavailable to major vendors so that they might profit from the lock experience;similar benefits are expected from the tmach development program.another example is nsa's stuiii telephone project, which involvedvendors in the design process. five prospective vendors competed to developdesigns; three went on to develop products. the interval from contract award tocommercial product was less than three years, although years of research anddevelopment were necessary beforehand. the stuiii has decreased the priceof secure voice and data communications from over $10,000 per unit to about$2,000 per unit, pleasing both government consumers and the commercialvendors. moreover, in 1990 the dod purchased several thousand stuiiiterminals for use not only in dod facilities but also for loan to qualifieddefense contractors; these firms will receive the majority of the purchased units.this program will help to overcome one obvious disincentive for commercialacquisition: to be of use, not only the party originating a call but also thereceiver must have a stuiii.why the security market has not worked well151computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.for national security reasons, programs that are sponsored by nsa confinedirect technology transfer to companies with u.s. majority ownership, therebyexcluding companies with foreign ownership, control, or influence (foci).while the united states has legitimate national interests in maintainingtechnological advantage, the increasingly international nature of the computerbusiness makes it difficult to even identify what is a u.s. company, much lesstarget incentives (nrc, 1990). another factor to consider in the realm ofstrategic research and development is the fact that, consistent with its primarymission, nsa's projects are relatively closed, whereas an agency like darpacan more aggressively reach out to the computer science and technologycommunity.the proposed federal highperformance computing program (ostp, 1989)could provide a vehicle for strategic research investment in system securitytechnology; indeed, security is cited as a consideration in developing thecomponent national research and education networkšand security wouldclearly be important to the success of the network. agencies involved ingenerating technology through this program include dod (with responsibilityconcentrated in darpa), the national science foundation (nsf), the nationalaeronautics and space administration (nasa), the department of energy(doe), and nist. however, funding uncertainty and delays associated with thehighperformance computing program suggest both that security aspects couldbe compromised and that additional but more modest largescale technologydevelopment projects that promote secure system development may be morefeasible. certainly, they would have substantial benefits in terms of advancingand commercializing trust technology. other governmentbacked researchprograms that focus on physical, natural, or biomedical sciences (e.g., theanticipated database for the mapping and sequencing of the human genome, orremoteaccess earth sciences facilities) also have security considerations thatcould provide useful testbeds for innovative approaches or demonstrations ofknown technology.export controls as a market inhibitorvendors maintain that controls on exports inhibit the development ofimproved commercial computer and communications security products.controls on the export of commercial computer security technology raisequestions about the kind of technology transfer that should be controlled (andwhy), whether security technologies aimed at the civilian market should beconsidered to have military relevance (dual use), whether control shouldcontinue under the provisions aimed atwhy the security market has not worked well152computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.munitions, and other considerations that affect how commercial and militaryperspectives should be weighed and balanced for these technologies. anoverview of the export control process is provided in chapter appendix 6.1.the challenge for policymakers is to balance national security and economicsecurity interests in drawing the line between technology that should becontrolled, because it compromises national security (in this case by hamperingintelligence gathering by government entities) and technology that need not be,and allowing that line to move over time.6the committee considered controls on the export of trusted systems and onthe export of commercialgrade cryptographic products. the current rulesconstraining the export of trusted (and cryptographic) systems were developedat a time when the u.s. position in this area of technology was predominant. asin other areas of technology, that position has changed, and it is time to reviewthe nature of the controls and their application, to assure that whatever controlsare in place balance all u.s. interests and thereby support national security inthe fullest sense over the long term. the emergence of foreign criteria andevaluation schemes (see "comparing national criteria sets" in chapter 5)makes reconsideration of export controls on trusted systems especially timely.balancing the possible temporary military benefit against the longruninterests of both national security applications and commercial viability, thecommittee concludes that orange book ratings, per se, do not signify militarycritical technology, even at the b3 and a1 levels. of course, specificimplementations of b3 and a1 systems may involve technology (e.g., certainforms of encryption) that does raise national security concerns, but suchtechnology is not necessary for achieving those ratings. nsa officials whobriefed the committee offered support for that conclusion, which is alsosupported by the fact that the criteria for achieving orange book ratings arepublished information. the committee urges clarifying just what aspects of atrusted system are to be controlled, independent of orange book levels, andtargeting more precisely the technology that it is essential to control. it alsourges reexamination of controls on implementations of the data encryptionstandard (des), which also derive from published information (the standard;nbs, 1977). issues in both of these areas are discussed below.technology transfer: rationale for controlling security exportscurrently, the military and intelligence communities provide the largestconcentration of effort, expertise, and resources allocated towhy the security market has not worked well153computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.ensuring information security. devoted to countering threats not likely to beexperienced by industry, much of this effort and expertise gives rise to special,often classified, products that are not and should not be commercially available.however, a strong commercial security effort would make it possible for thedefense sector to concentrate its development resources on militarycriticaltechnology. then the flow of technology for dualuse systems could besubstantially reversed, thus lessening concerns about the export of vital militarytechnology.exports of dualuse computer technologies are controlled largely fordefensive reasons, since those technologies can be used against u.s. nationalsecurityšto design, build, or implement weaponry or military operations, forexample. computer security presents offensive and defensive concerns.adversaries' uses of computer security technologies can hamper u.s.intelligence gathering for national security purposes (ota, 1987b). as a result,dod seeks to review sophisticated new technologies and products, to preventpotential adversaries of the united states from acquiring new capabilities,whether or not the dod itself intends to use them. another concern is thatinternational availability exposes the technology to broader scrutiny, especiallyby potential adversaries, and thus increases the possibility of compromise ofsafeguards.the need to minimize exposure of critical technology implies that certainmilitarycritical computer security needs will continue to be met throughseparate rather than dualuse technology (see appendix e, "highgradethreats"). as noted in this report's "overview" (chapter 1), national securitydictates that key insights not be shared openly, even though such secrecy mayhandicap the development process (see "programming methodology,''chapter 4). to maintain superiority, the export of such technology will alwaysbe restricted. thus the discussion in this chapter focuses on dualuse technology.export control of cryptographic systems and componentshistorically, because of the importance of encryption to intelligenceoperations and the importance of secrecy to maintaining the effectiveness of agiven encryption scheme, cryptographic algorithms and their implementationscould not be exported at all, even to other countries that participate in thecoordinating committee on multilateral export controls (cocom).restrictions on exports of des have been contested by industry because ofthe growing use of des. the restrictions were recently relaxed somewhat,allowing for export of confidentiality applications under the internationaltraffic in arms regulations (itar; office ofwhy the security market has not worked well154computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.the federal register, 1990) to financial institutions or u.s.companysubsidiaries overseas. des may also be exported for data integrity applications(nist, 1990b). that is, des may be used to compute integrity checks forinformation but may not be used to encrypt the information itself. private(vendorspecific) algorithms are generally approved for export following reviewby nsa (although that review may result in changes in the algorithm to permitexport). the department of commerce reviews export licenses for des andother cryptographic products intended for authentication, access control,protection of proprietary software, and automatic teller devices.because of current controls, computerbased products aimed at thecommercial market that incorporate encryption capabilities for confidentialitycan only be exported for limited specific uses. (ironically, encryption may evenbe unavailable as a method to assure safe delivery of other controlled products,including security products.) affected products include dbaseiv and othersystems (including pcoriented systems) with message and file securityfeatures. however, anecdotal evidence suggests that the regulations may not beapplied consistently, making it difficult to assess their impact.in some cases, the missing or disabled encryption function can be replacedoverseas with a local product; indigenous des implementations are availableoverseas. the local product may involve a different, locally developedalgorithm. it is not clear, however, that modular replacement of encryption unitswill always be possible. the movement from auxiliary blackbox units tointegral systems suggests that it will become less feasible, and there is somequestion about whether modular replacement violates the spirit if not the letterof existing controls, which may discourage some vendors from even attemptingthis option. vendors are most troubled by the prospect that the growingintegration of encryption into generalpurpose computing technology threatensthe large export market for computer technology at a time when some 50percent or more of vendors' revenues may come from overseas.much of the debate that led to the relaxation of export restrictions for descentered on the fact that the design of des is widely known, having beenwidely published for many years. similarly, the rsa publickey algorithm (see"selected topics in computer security technology," appendix b) is wellknown and is, in fact, not patented outside the united statesšbecause the basicprinciples were first published in an academic journal (rivest et al., 1978).consequently, there are implementations of des and rsa that have beendeveloped outside the united states and, as such, are not bound by u.s.restrictions.7 however, they may be subject to foreign export control regimes.withwhy the security market has not worked well155computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.u.s. vendors enjoined from selling des abroad, then foreign consumersand, more importantly, large multinational consumers will simply purchaseequivalent systems from foreign manufacturers.recognizing the demand for a freely exportable confidentiality algorithm,nist, in consultation with nsa, has announced plans to develop and certify anew algorithm for protecting sensitive but unclassified information, possiblydrawing on a published publickey system. a joint nistnsa committee isworking to develop a set of four cryptographic algorithms for use in thecommercial environment. one algorithm would provide confidentiality and thusis a des substitute. a publickey distribution algorithm would be used todistribute the keys used by the first algorithm. the last two algorithms would beused to provide digital signatures for messages: one would compute a onewayhash on a message and the other would digitally sign the hash. all of thealgorithms would, by design, be exportable, thus addressing a major complaintabout des. however, this process has been delayed, apparently because ofnsa's discomfort with nist's reported preference for using rsa, which itperceives as almost a de facto standard (zachary, 1990).the announced development of one or more exportable algorithms has notsatisfied vendors, who note that overseas competitors can offer localimplementations of des, which has become widely recognized as a standard.by contrast, the new algorithm, while promised to be at least as good as des,may be difficult to sell as it will be incompatible with des implementations inuse and may be tainted as u.s.governmentdeveloped. under thecircumstances, if national security objections to free des export continue, theyshould at the least be explained to industry. also, independent expert review ofthe new algorithm is desirable to elevate confidence to the level that des hasattained. note that there are other (nondes) commercially developedencryption algorithms that are licensed for export by the department of state.the united states is typically involved in their development, and some 98percent of the products implementing these algorithms are approved for export(committee briefing by nsa).export control of trusted systemstrusted systems that have been evaluated at the orange book's levels b3and above are subject to a casebycase review, whether or not they incorporatecryptography or other technologies deemed militarycritical.8 that is, thegovernment must approve the export of a given system to a given customer fora given application if it is, or could be, rated as b3 or above; products withlower ratings are not regardedwhy the security market has not worked well156computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.as militarycritical technology. the same rules extend to documentation andanalysis (e.g., for a technical conference or journal) of affected products. anaverage of 15 such license applications per year (covering five to seven items)have been reviewed over the past three years, and all have been granted.9 abouthalf have involved u.s. vendors providing technical data to their subsidiaries.in the case of software verification tools, which are used to develop trustedsystems, there is the added requirement that informal intergovernmentalagreements exist to monitor the tools' installation and operation. this issomewhat less restrictive than the treatment for supercomputers.note that in some respects trusted systems technology is very difficult tocontrol because it depends heavily on software, which is relatively easy to copyand transport (nrc, 1988a). as a result, such technology can never be the onlyline of defense for protection of sensitive information and systems.the commercial imperativebecause of the national security interests that dominate the itar, thecurrent export control regime for highlevel trusted systems and for mostencryption products does not contain mechanisms for addressing vendorconcerns about competitiveness. by contrast, commercial competitivenessconcerns affect both the evolution of the control list (cl) and the commoditycontrol list (ccl) associated with the export administration regulations (seechapter appendix 6.1) and the periodic reviews of dualuse technologies by theunited states and other participants in cocom. under the terms of the exportadministration act (50 u.s.c. app. §§ 2401œ2420, as amended), foreignavailability may also justify the relaxation of controls for particular products, asit did for atclass pcs in july 1989. foreign availability is not, however, afactor in administering controls on militarycritical technologies under the itar.the discussions of controls on dualuse technology exports in general drawon a broader range of perspectives than do the discussions of technologiescontrolled under the itar, in part because there is generally no argument overwhether a product is a munition or of fundamentally military value. as a resultthere is at least the potential for a greater balancing of policy interests in themaking of control decisions affecting nonitar technologies. the complaintsfrom industry surrounding controls on the export of des and rsa, algorithmsfor encryption that fall in part under itar rules, signal a larger problemdeveloping for exports of security technology. in today's global market forcomputer technology, commercial product line development,why the security market has not worked well157computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.production economics, and competitive strategy lead producers to want tomarket products worldwide. major vendors generally have a major share ofbusiness (often 50 percent or higher) from outside of the united states.industry has four key concerns: first, every sale is important forprofitability in a small market, such as the current market for securityratedsystems. this means that both actual disapproval of a given sale and the delayand uncertainty associated with the approval process are costly to vendors.(supercomputers are an extreme case of this problem.) second, the principalcommercial customers today for trusted systems (and commercialgradeencryption) are multinational corporations. this means that if they cannot use aproduct in all of their locations around the world, they may not buy from a u.s.vendor even for their u.s. sites. third, u.s. vendors have seen the beginningsof foreign competition in trust technology, competition that is being nurtured byforeign governments that have launched their own criteria and evaluationschemes to stimulate local industry (see "comparing national criteria sets" inchapter 5). these efforts may alter the terms of competition for u.s. vendors,stimulate new directions in international standards, and affect vendor decisionson where as well as in what to invest. fourth, as security (and safety)technology becomes increasingly embedded in complex systems, systemtechnology and users will come to depend on trust technology, and it willbecome more difficult to excise or modify in systems that are exportable. thislast problem has been cited by vendors as a source of special concern; a relatedconcern is providing interoperability if different standards are used in differentcountries or regions.the real difficulty arises if a vendor considers building security into a"mainstream" commercial product. in that event, the system's level of security,rather than its processing power, becomes its dominant attribute fordetermining exportability. a computer system that would export [sic] under acommerce department license with no delay or advance processing wouldbecome subject to the full state department munitions licensing process. novendor will consider subjecting a mainstream commercial product to suchrestrictions.10the push by industry for expanded export flexibility for securityratedsystems and lowgrade encryption units highlights the tension betweengovernment encouragement of the supply of computer security technology,notably through the orange book evaluation of commercial products, andpotential government restriction of the market for security products throughexport controls. the presence of an export control review threshold at b3,affecting b3 and a1 systems intended for other cocom countries, hasdiscouraged the enhancement of systemswhy the security market has not worked well158computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.to these levels, for fear of making products more difficult, if not impossible, toexport.since other factors, such as high development costs and softness ofperceived demand, discourage development of highly rated systems, it isdifficult to quantify the disincentive arising from export controls. however, thevery real pressure to export des and rsa does provide evidence of adeveloping international market for security technology beyond what maycurrently be exported. those and similar or successor technologies are not thetechnologies that are used for defense purposes, and it may be time to endorse anational policy that separates but mutually respects both national security andcommercial interests. those interests may overlap in the long run: as long aspolicy encourages use of commercial offtheshelf technology, a strongcommercial technology base is essential for feeding military needs. evenspecifically military systems profit from commercial experience. and thestrength of the commercial technology base today depends on the breadth of themarket, which has become thoroughly international.consumer awarenesseven the best product will not be sold if the consumer does not see a needfor it. consumer awareness and willingness to pay are limited because peoplesimply do not know enough about the likelihood or the consequences of attackson computer systems or about more benign factors that can result in systemfailure or compromise.11 consumer appreciation of system quality focuses onfeatures that affect normal operationsšspeed, ease of use, functionality, and soon. this situation feeds a market for inappropriate or incomplete securitysolutions, such as antiviral software that is effective only against certain virusesbut may be believed to provide broader protection, or password identificationsystems that are easily subverted in ordinary use.12further militating against consumer interest in newer, technicalvulnerabilities and threats is the experience of most organizations withrelatively unsophisticated abuses by individuals authorized to access a givensystem (often insiders), abuses that happen to have involved computers but thatneed not have. the breadandbutter work of the corporate computer securityinvestigator is mostly devoted to worrying about such incidents as the following:1. two members of management extract valuable proprietary data from acompany's computer and attempt to sell the data to a competitor;why the security market has not worked well159computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.2. an employee of company a, working on a contract for company b,uses a computer of company b to send a bomb threat to company c;3. an employee copies a backup tape containing confidential personnelinformation, which he then reveals to his friends;4. an employee uses his access to company billing information on acomputer to reduce the bills of certain customers, for which service hecollects a fee; and5. an employee uses company computer facilities to help him arrangeillegal narcotics transactions.all five of the above incidents are typical in a particular sense. in none ofthem did any single computer action of the perpetrator, as a computer action,extend beyond the person's legitimate authority to access, modify, transmit, andprint data. there was no problem of password integrity, for example, orunauthorized access to data, or trojan horses. rather, it was the pattern ofactions, their intent, and their cumulative effect that constituted the abuse.the kinds of incidents listed above consume most of the security officer'stime and shape his priorities for effective countermeasures. what the corporatecomputer and communications security specialist is most likely to want, beyondwhat he typically has, are better tools for monitoring and auditing the effects ofcollections of actions by authorized users: detailed logs, good monitoring tools,welldesigned audit trails, and the easy ability to select and summarize fromthese in various ways depending on the circumstances he is facing.13 thishistory in large measure accounts for the relatively low interest in thecommercial sector in many of the security measures discussed in this report.nevertheless, even attention to administrative and management controls,discussed in chapter 2, is less than it could or should be.enhancing security requires changes in attitudes and behavior that aredifficult because most people consider computer security to be abstract andconcerned more with hypothetical rather than likely events. very fewindividuals not professionally concerned with security, from top managementthrough the lowestlevel employee, have ever been directly involved in oraffected by a computer security incident. such incidents are reportedinfrequently, and then often in specialized media, and they are comprehensibleonly in broadest outline. further, most people have difficulty relating to theintricacies of malicious computer actions. yet it is understood that installingcomputer security safeguards has negative aspects such as added cost,diminished performance (e.g., slower response times), inconvenience in use,and the awkwardness of monitoring and enforcement, not to mention objectionsfrom thewhy the security market has not worked well160computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.work force to any of the above. the internet worm experience showed that evenindividuals and organizations that understand the threats may not act to protectagainst them.the sensational treatment of computer crimes in the press and in moviesabout computer hijacks may obscure the growing role of computer technologyin accomplishing more traditional and familiar crimes (e.g., fraud andembezzlement). in the public's eye, computer crimes are perpetrated byoverzealous whizkids or spies, not disgruntled employees or professionalcriminals; prosecutors also complain that the media portray perpetrators assmarter than investigators and prosecutors (comments of federal prosecutorwilliam cook at the 1989 national computer security conference). publicskepticism may be reinforced when, as in the case of recent investigations of thelegion of doom and other alleged system abusers (shatz, 1990), questions areraised about violation of first amendment rights and the propriety of searchand seizure techniquesšissues of longstanding popular concern.14inevitably, resources are invested in safeguards only when there is a netpayoff as measured against goals of the organizationšwhether such goals arechosen or imposed. it is notable that the banking industry's protection ofcomputer and communications systems was stimulated by law and regulation.in the communications industry, lost revenues (e.g., through piracy of services)have been a major spur to tightening security.insurance as a market leverinsurance can offset the financial costs of a computerrelated mishap. thedevelopment of the commercial market for computer insurance (described inchapter appendix 6.2) provides a window into the problems of achievinggreater awareness and market response.15the market for insurance against computer problems has grown slowly.insurance industry representatives attribute the slow growth to low levels ofawareness and concern on the part of organizations and individuals, plus unevenappreciation of the issues within the insurance industry, where underwriters andinvestigators may not fully understand the nature of the technology and itsimplications as used.16 insurance industry representatives also point to thereluctance of victims of computer mishaps to make their experiences public,even at the expense of not collecting on insurance.the process of determining whether coverage will be provided involvesassessing the controls provided by a prospect. somewhat like auditors,underwriters and carriers evaluate securityrelated safeguards in place byfocusing on physical and operational elements.why the security market has not worked well161computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.there is a concern for the whole control environment, including directlyrelevant controls and controls for other risks, which may indicate how well newrisks may be controlled.to the extent that premiums reflect preventive measures by anorganization (e.g., offsite periodic backup copies of data, highquality doorlocks, 24hour guard coverage, and sprinkler or other fire control systems),insurance is a financial lever to encourage sound security, just as the foreigncorrupt practices act (p.l. 95215) and a variety of accounting principles andstandards have encouraged stronger management controls in general (and, insome instances, stronger information security in particular (snyders, 1983)).education and incident tracking for security awarenessif some of the problems in the secure system marketplace are due to lackof awareness among consumers, options for raising consumer awareness ofthreats, vulnerabilities, and safeguards are obviously attractive. two options areraised here as conceptsšeducation and incident reporting and tracking. thecommittee's recommendation that incident tracking be undertaken by a neworganization is discussed in chapter 7.educationsociety has often regulated itself by promoting certain behaviors, forexample, taking care of library books. societal caretaking norms must now beextended to information in electronic form and associated systems. thecommittee believes that elements of responsible use should be taught along withthe basics of how to use computer and communication systems, much as peoplelearn how to be responsible users of libraries. building concern about securityand responsible use into computing and general curricula (where computers areused) may be more constructive in the long run than focusing efforts onseparate and isolated ethics units. this is not to discourage the many recentefforts among computerrelated professional societies, schools, and companiesto strengthen and discuss codes of ethics.17 however, today much of thesecurity training is funded by commercial companies and their employeestudents; that training, in turn, is focused on security officers and not end users.the committee underscores that the process becomes one to persuade, lead, andeducate, and when possible, to make the unacceptability of not protectingcomputer systems outweigh the cost of taking appropriate action.why the security market has not worked well162computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.incident reporting and trackingmore extensive and systematic reporting and tracking of security and othersystem problems could help to persuade decisionmakers of their value andpolicymakers of related risks. for example, investigation and prosecution ofcomputer crimes have proceeded slowly because of the uneven understandingwithin the legal community of the criminal potential as well as the relativelyhigh costs involved in computer crimes (conly, 1989; u.s. doj, 1989). at thistime there is little statistical or organized knowledge about vulnerabilities,threats, risks, and failures. (neumann and parker (1989) represent one attemptto characterize vulnerabilities.) what is known about security breaches islargely anecdotal, as many security events happen off the record; one source ofsuch information within the computer science and engineering community is theelectronic forum or digest known as risks.18 estimates of aggregate lossesvary widely, ranging from millions to billions of dollars, and estimates citedfrequently in news reports are challenged by prosecutors (comments of federalprosecutor william cook at the 1989 national computer security conference).the european community has begun to develop computer incident trackingcapabilities; the british and the french both have new programs (prefontaine,1990). a reliable body of information could be used to make the public and thegovernment more aware of the risks.a means is needed for gathering information about incidents,vulnerabilities, and so forth in a controlled manner, whereby information wouldactually be available to those who need itšvendors, users, investigators,prosecutors, and researchers. there are a number of implementation issues thatwould have to be addressed, such as provision for a needtoknow compartmentfor unclassified information that is considered sensitive because of the potentialimplications of its widespread dissemination. it would also be necessary tocouple reports with the caveat that yesterday's mode of attack may notnecessarily be tomorrow's. the incidentreporting system associated with thenational transportation safety board illustrates one approach to data collection(although the handling, storage, and retrieval of the data are likely to be differentšcomputer incident data are much more likely than transportation data to beexploited for copycat or derivative attacks).given the volume of transactions and activity that has occurred in theinformation systems of the private sector and occurs there each day, and giventhe decade or so during which numerous computer mishaps, intentional andaccidental, have been documented and recorded, the validated evidence that hasbeen accumulated remains minuscule by comparison to that of criminalincidents or accidents in other areaswhy the security market has not worked well163computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.of business risk, for example, fire, embezzlement, and theft. this situation mayreflect a relatively low incidence of problems to date, but there is strongevidence that available information is significantly underreported.19 the effortbegun by the darpa computer emergency response team to develop amechanism to track the emergency incidents to which it responds, and relatedplans at nist, are a step in the right direction that could provide the impetus fora more comprehensive effort.20 such an effort is discussed in chapter 7.technical tools to compensate for limited consumerawarenesslimited awareness of security needs or hazards can be offset in part bytechnical tools. properly designed technical solutions may serve to reinforcesafe behavior in a nonthreatening way, with little or no infringement of personalprivacy or convenience. impersonal, evenhanded technical solutions may wellbe better received than nontechnical administrative enforcement. the key is tobuild in protections that preserve an organization's assets with the minimumpossible infringement on personal privacy, convenience, and ease of use. as anexplicit example, consider the ubiquitous password as a personalidentificationsafeguard. in response to complaints about forgetting passwords and aboutrequirements to change them periodically, automated online promptingprocedures can be introduced; a questionandresponse process can beautomatically triggered by elapsed calendar time since the last passwordchange, and automated screening can be provided to deter a user from selectingan illconceived choice. concerted vendor action, perhaps aided by tradeassociations, and consumer demand may be needed to get such tools offeredand supported routinely by vendors.some issues pertaining to the proper use of such automated tools call forsensitivity and informed decision making by management. one concern is thepotential for loss of community responsibility. individual users no longer havethe motivation, nor in many cases even the capability, to monitor the state oftheir system. just as depersonalized ''renewed" cities of highrises and doormensacrifice the safety provided by observant neighbors in earlier, apparentlychaotic, gossipridden, ethnic neighborhoods (jacobs, 1972), so a system thatrelies on carefully administered access controls and firewalls sacrifices thesocial pressure and community alertness that prevented severe malfeasance inolder nonsecure systems. a perpetrator in a tightly controlled system knowsbetter who to look out for than one in an open system. furthermore, a tightlycontrolled system discourages,why the security market has not worked well164computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.even punishes, the simple curiosity of ordinary users that can spot unusual acts.wise management will avoid partitioning the community too finely lest thehuman component, on which all security ultimately rests, be lost. simply put,technological tools are necessary but should not be overusedregulation as a market influence: productquality and liabilityregulation is a policy tool that can compensate for consumer inability tounderstand a complex product on which much may depend. relatively littleabout computer systems is now regulated, aside from physical aspects ofhardware.21 although software is a principal determinant of the trustworthinessof computer systems, software has generally not been subject to regulation.however, regulations such as those governing export of technology, thedevelopment of safetycritical systems (recently introduced in the unitedkingdom), or the privacy of records about persons (as implemented inscandinavia) do have an immediate bearing on computer security andassurance. the issue of privacy protection through regulation is discussed inchapter 2, appendix 2.1.like other industries, the computer industry is uncomfortable withregulation. industry argues that regulations can discourage production, in partby making it more costly and financially risky. this is one of the criticismsdirected against export controls. however, regulation can also open up markets,when market forces do not produce socially desirable outcomes, by requiring allmanufacturers to provide capabilities that would otherwise be too risky forindividual vendors to introduce. vendors have often been put on an equalfooting via regulation when public safety has been an issue (e.g., in theenvironmental, food, drug, and transportation arenas). in the market for trustedsystems, the orange book and associated evaluations, playing the role ofstandards and certification, have helped to do the samešunfortunately, thatmarket remains both small and uncertain.22 as suggested above in "a softmarket," individual vendors find adding trust technology into their systemsfinancially risky because consumers are unable to evaluate security and trustand are therefore unwilling to pay for these qualities.23although in the united states regulation is currently a policy option of lastresort, growing recognition of the security and safety ramifications of computersystems will focus attention on the question of whether regulation of computerand communications software and system developers is needed or appropriate,at least in specific situationswhy the security market has not worked well165computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.(for example, where lives are at risk). the issue has already been broached in arecent congressional committee report (paul, 1989). although full treatment ofthat question is outside the scope of this report, the committee felt it necessaryto lay out some of the relevant issues as a reminder that sometimes last resortsare used, and to provide reinforcement for its belief that some incentives formaking gssp truly generally accepted would be of value.product quality regulationssystem manufacturers generally have much greater technical expertisethan system owners, who in acquiring and using a system must rely on thesuperior technical skill of the system vendor. the same observation, of course,applies to many regulated products on which the public depends, such asautomobiles, pharmaceuticals, and transportation carriers. similar motivationslie behind a variety of standards and certification programs, which may beeither mandatory (effectively regulations) or voluntary (ftc, 1983). whereasfailure of an automobile can have severe, but localized, consequences, failure ofan information system can adversely affect many users simultaneouslyšplusother individuals who may, for example, be connected to a given system orabout whom information may be stored on a given systemšand can evenprevent efficient functioning of major societal institutions. this problem ofinterdependence was a concern in recent gao inquiries into the security ofgovernment and financial systems (gao, 1989e, 1990a,b). the widespreadhavoc that various computer viruses have wreaked amply demonstrates thedamage that can occur when a weak spot in a single type of system is exploited.the accidental failure of an at&t switching system, which blocked anestimated 40 million telephone calls over a ninehour period on january 15,1990, also illustrates the kind of disruption that is possible even underconditions of rigorous software and system testing. the public exposure andmutual interdependence of networked computer systems make trustworthinessas important for such systems as it is for systems where lives or large amountsof money are at stake, as in transportation or banking. indeed, in settings asdiverse as the testing of pharmaceuticals, the design of automobiles, or thecreation of spreadsheet programs, results from programs and computers that arenot directly involved in critical applications ultimately wind up in just suchapplications.goods and services that impinge on public health and safety havehistorically been regulated. moreover, the direct risk to human life is a strongerand historically more successful motivation for regulationwhy the security market has not worked well166computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.than the risk to economic wellbeing, except in the case of a few key industries(e.g., banks and insurance carriers). this situation suggests that regulation ofsafety aspects of computers, a process that has begun in the united kingdom(u.k. ministry of defence, 1989a,b), has the best chance for success, especiallywith safetycritical industries such as medical devices and health care, or eventransportation. it also suggests that the case for securityrelated regulation willbe strongest where there are the greatest tieins to safety or other criticalimpacts. thus computer systems used in applications for which some form ofregulation may be warranted may themselves be subject to regulation, becauseof the nature of the application. this is the thinking behind, for example, thefood and drug administration's efforts to look at computer systems embeddedin medical instruments and processes (peterson, 1988). note, however, that it isnot always possible to tell when a generalpurpose system may be used in asafetycritical application. thus standardized ratings have been used in othersettings.24product liability as a market influencein addition to being directly regulated, the quality of software and systemsand, in particular, their security and safety aspects, may be regulated implicitlyif courts find vendors legally liable for safety or securityrelevant flaws. thoseflaws could be a result of negligence or of misrepresentation; the law involvedmight involve contracts, torts, or consumer protection (e.g., warranties). atpresent, there is some indication from case law that vendors are more likelynow than previously to be found liable for software or system flaws, and somelegal analysts expect that trend to grow stronger (agranoff, 1989; nycum,1989; boss and woodward, 1988). the committee applauds that trend, becauseit believes that security and trust have been overlooked or ignored in systemdevelopment more often than not. further, the committee believes that arecognized standard for system design and development, which could consist ofgssp, can provide a yardstick against which liability can be assessed.25depending exclusively on legal liability as a mechanism to stimulateimprovements in quality could backfire: it could inhibit innovation because offears linking legal risks and the development of new products. gssp could helpallay such fears and curb capricious litigation by clarifying general expectationsabout what constitutes responsible design and development.software plays a critical role in assuring the trustworthiness of computerand communications systems. however, the risk that software may not functionproperly is borne largely by the consumer, especiallywhy the security market has not worked well167computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.for offtheshelf software, which is typically obtained under licenses laden withdisclaimers. offtheshelf applications programs and even operating systems aretypically acquired by license with limited rights, under the terms specified bythe manufacturer, as opposed to direct sale (which would imply that the vendorforfeits control over the terms and conditions of its use) (davis, 1985). thepurchaser typically has no bargaining power with respect to the terms andconditions of the license.26 pcbased software licenses present the extremecase, since they are often sealed under shrinkwrap packaging whose openingsignifies acceptance of the license. typically, such licenses limit liability fordamages to replacement of defective media or documentation, repair ofsubstantial program errors, or refund of the license fee. from the vendor'sperspective, this is not surprising: the revenue from an individual "sale" of pcsoftware is very small, in the tens or hundreds of dollars; from the consumer'sperspective, the absence of additional protections contributes to relatively lowprices for packaged software. by contrast, customized applications systems,which may well be purchased rather than licensed, are developed in response tothe specifically stated requirements of the client. the terms and conditions arethose negotiated between the parties, the buyer has some real bargaining power,and the contract will reflect the intent and objectives of both parties.some consumer protection may come from the uniform commercial code(ucc). consumer protection may also come from the magnusonmosswarranty act (15 usc § 2301 et seq. (1982)), which provides standards for fullwarranties, permits limited warranties, and requires that warranties be expressedin understandable language and be available at the point of sale.the ucc is a uniform law, drafted by the national conference ofcommissioners on uniform state laws and adopted as law by 49 states, thatgoverns commercial transactions, including the sale of goods. while there is nolaw requiring express warranties in software licenses, the ucc addresses whatconstitutes an express warranty where provided, how it is to be enforced, andhow to disclaim implied warranties.27 the acquisition of a good by license is a"transaction" in goods and is generally covered by article 2 of the ucc,although some provisions of the code refer specifically to "sale" and may not beapplicable to licensed goods. the national conference of commissioners isexpected to clarify the issue of whether software is a "good" (and thereforecovered by the ucc) by including software within the definition of a "good." inany case, the state courts are quite familiar with the ucc and tend to apply itsprinciples to softwarewhy the security market has not worked well168computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.license transactions. note that a proposed extension to the ucc, section 4a,would impose liability on banks for errors in electronic funds transfers undercertain conditions. this provision is already seen as motivating greater wiretransfer network security among banks (datapro research, 1989b).the ucc provides a number of protections for the buyer of goods. inevery sale of a product by a seller that deals in goods of the kind sold, there isan implied warranty that the product is merchantable. the usual test formerchantability is whether the product is fit for the ordinary purposes for whichsuch products are used. the buyer can recover damages whether or not theseller knew of a defect, or whether or not the seller could have discovered sucha defect. the ucc also provides an implied warranty of fitness for a particularpurpose. this warranty provides damages where any seller, whether a dealer ingoods of the kind sold or not, has any reason to know the specific use to whichthe product will be put, and knows that the buyer is relying on the seller'ssuperior expertise to select a suitable product. these warranties may be, andalmost always are, disclaimed as part of pc software shrinkwrap licenses,often by conspicuously including such words as "as is" or "with all faults."the ucc does permit the vendor to limit or exclude consequential andincidental damages, unless such limitation is unconscionable (e.g., because it isoverly onesided). consequential damages are compensation for an injury thatdoes not flow immediately and directly from the action, but only from theconsequences or results of the action. for example, damages from a computerbreakin that exploited a flawed password mechanism would be deemedconsequential to the extent that the supplier of the password mechanism washeld responsible. recovery from suppliers can take other less farreaching (andmore plausible) forms, such as incidental damages. incidental damages includecommercially reasonable charges incurred incident to a breach, such as costsincurred to mitigate the damage.while disclaimers and standardform contracts or licenses are legal andhelp to keep prices down, as applied to software they raise questions aboutwhether consumers understand what is happening and what popular licensingpractices may mean. these questions were noted in a recent review of computercontract cases:since purchasers generally base their selection of equipment and software onthe sellers' representations as to the technical performance capabilities andreliability of equipment, the buyers often ignore the generally broaddisclaimers of express and implied warranties in standard vendor contracts.when they become disappointed and discover that disclaimers foreclose theircontract remedies, they turn to the law of misrepresentation for relief.why the security market has not worked well169computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.misrepresentation cases will continue to proliferate until the industry moreclosely aligns its express warranties with the reasonable expectations of itscustomers, who assume that the hardware and software they buy will performas described by the sellers' representatives who sold them the product. (bossand woodward, 1988, p. 1533)the vulnerability of consumers and the mismatch of expectations evenwhere individualized contracts are involved have been underscored by a fewrecent incidents involving vendor disabling of installed software in the courseof disputes with customers.28software and systems present special problemsit is clear from the foregoing discussion that a buyer of offtheshelfsoftware has extremely limited recourse should the licensed software notperform as expected. the major motivation for the vendor to producetrustworthy software is the desire to remain competitive. in the process,however, features for which customer demand is not high may receiveinadequate attention. for example, restraints to protect passengers and emissioncontrols to protect the public at large are now universally installed inautomobiles because they have been mandated by government action. althoughpublic interest groups helped spur government action, few individual consumersdemanded these features, perhaps because of the increased cost or theperception of reduced performance or the inability of an individual to bargainfor them effectively. yet few would argue that these impositions are not in thepublic interest; what does stimulate argument is the stringency of the safeguardrequired.unsafe or nonsecure software poses analogous risks to users and to othersexposed to it (see chapter 2's "risks and vulnerabilities"). more trustworthysoftware may, like safer and cleaner automobiles, carry a higher product pricetag and may also suffer from a perception of reduced performance. in theabsence of general consumer demand for more trustworthy software, shouldmanufacturers of offtheshelf software be subjected to governmental action? inparticular, should the government act to reduce a software vendor's ability todisclaim warranties and to limit damages?the software industry and software itself exhibit some characteristics thatlimit the scope for governmental action. on the one hand, complex softwarewill inevitably contain errors; no human being can guarantee that it will be freeof errors. imposition of strict liability (without a finding of malice ornegligence) for any error would clearly not be equitable, since the exercise ofeven an exceptionally high degree of care in software production would notguarantee an errorfree product.why the security market has not worked well170computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.on the other hand, tools and testing methods to reduce the probability of errorsare available. systematic use of such tools and methods prior to softwarerelease reduces the frequency and severity of errors in the fielded product. thecommittee believes that these tools and methods are not now in wide use bothbecause they are not well known (e.g., the forefront technology of automatedprotocol analysis, which can dramatically shorten the development cycle) orbecause, given the evolution of products and practices in the industry, theyappear to have been ignored by vendors (e.g., as has been the case for stronglytypechecked link editors).of course, licensees must accept many risks in using software. users musttrain themselves sufficiently in the proper operation of a computer system andsoftware before relying on them. a software vendor should not be held liablefor damage caused by users' gross ignorance.29 at the same time, the softwarevendor must bear a degree of responsibility in helping to properly train the userthrough adequate and clear documentation describing proper use of the product,and its limitations, including their bearing on security and safety. the superiorknowledge and skill of the software vendor itself should impose a duty of careon that vendor toward the unskilled licensee, who in purchasing the productmust rely on the vendor's representations, skill, and knowledge.30 at the sametime, any imposition of liability on the vendor must imply a concomitantimposition of responsibility on the user to make a reasonable effort to learn howto use the software properly.perhaps the most compelling argument against increasing product liabilityfor software and systems vendors is the potential for adverse impacts on thedynamic software industry, where products come quickly to the market andadvances are continually madešboth of which are major consumer benefits.innovation is frequently supported by venture capital, and imposition of heavywarranty liability can chill the flow of capital and restrict the introduction ofnew products or the proliferation of new ventures. even when raising capital isnot an issue, risk aversion itself can discourage innovation. in either case, theincreased business risk to the vendor is reflected in higher product prices to theconsumer, which in turn may mean that fewer consumers benefit from a givenpiece of software.toward equitable allocation of liabilitythe possible adverse consequences of holding software and systemvendors to a higher standard of care must be carefully weighed against thepotential benefits. as more powerful and more highlywhy the security market has not worked well171computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.interconnected systems become more widespread, there will be increasingconcern that the current allocation of the risk of software failure is too onesided for an information society, at least for offtheshelf software. the industryis sufficiently mature and verification tools and methodologies are sufficientlywell understood today that total insulation of the industry from theconsequences of software failure can no longer be justified. operating systemsoftware and the major offtheshelf applications software packages areproduced by companies with a business base substantial enough to supportquality assurance programs that would yield safer and more secure software;such programs could also reduce any liability risk to manageable proportions.as it is, vendors have already begun programs to make sure that their owndevelopment and production efforts are free of contamination from viruses.ibm, for example, set up its highintegrity computing laboratory for thispurpose (smith, 1989; committee briefing by ibm), and adapso, a tradeassociation, has been promoting such efforts for its constituent software andservices companies (landry, 1990). similarly, vendors do, to varying degrees,notify users of securityrelated flaws. for example, sun microsystems recentlyannounced the customer warning system for handling security incidents31(ulbrich and collins, 1990).shifting more (not all) risk to the vendors would result in greater carebeing taken in the production and testing of software. the british move torequire greater testing of safetyrelevant software illustrates that these concernsare not just local, but are in fact relevant to a worldwide marketplace. theresulting increased use of verification techniques would not only improve thelevel of software trustworthiness in the most general sense, but would alsonecessarily improve the level of trust in the specific information securitycontext. (see chapter 4's "relating specifications to programs" and "formalspecification and verification.")the national interest in the trustworthiness of software is sufficientlystrong that congress should review this question to determine (1) whetherfederal law is required (or whether state efforts are adequate) and (2) to whatextent risks that can be averted through safer software should be shifted fromuser to vendor. equitable risk allocation, which reasonably balances vendor anduser interests, is achievable and will advance the national interest.the development of gssp, as recommended in chapters 1 and 2, wouldprovide a positive force to balance and complement the negative force ofproduct liability. gssp would provide a clear foundation of expectation thatcustomers may count on as standards of performance and vendors may regardas standards of adequacy, against whichwhy the security market has not worked well172computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.legal claims could be judged. interestingly, a similar notion was expressed byinsurance industry representatives interviewed for this study, who suggestedthat some form of standard that could be harmonized with accounting standardswould be a potent mechanism to improve security controls in the businesscommunity. their rationale was that such standards would raise the profile ofthe issue with corporate directors and officers, who are liable to owners(stockholders, partners, and so on).32the committee recognizes that security is not the only property involved inthe issue of product liability; safety is obviously another such property.however, as security is a subliminal property of software, it is here that the gapbetween unspoken customer expectations and unarticulated vendor intentionslooms largest. advances in articulating gssp would go far toward clarifyingthe entire field. both customers and vendors stand to gain.appendix 6.1šexport control processnational security export controls (hereafter, "export controls") limit accessin other countries to technologies and products that could be valuable formilitary purposes. the control process, which varies by type of product,involves a list of controlled items and an administrative structure for enforcingcontrols on the export of listed items. controlled exports do not mean noexports. rather, these exports are controlled in terms of destination and, insome cases, volume or end use, with restrictions specified as part of the exportlicense. it should be noted that even the tightest export controls do not totallyblock access to protected technology.four organizations have been the principal influences on the export controlpolicy and process of the united states, namely the coordinating committee formultilateral export control (cocom), in which the united states participates,and the u.s. departments of state, commerce, and defense. each of theseorganizations has its own policies and jurisdictions for export control, but allthe organizations interact heavily with regard to common pursuits (nas, 1987).cocom, a multilateral effort to curb the flow of technology from the westto the soviet union and what have been its allies in the east bloc, has includedrepresentatives from japan, australia, and all nato countries except iceland.products controlled by cocom are listed on the industrial list (il). thedepartment of state administers the international traffic in arms regulations(itar; 22 cfr, parts 120œ130) through its center for defense trade (formerlythe office of munitions control) in consultation with the department of defense.why the security market has not worked well173computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.that office maintains the u.s. munitions control list, which includestechnologies and products representing an obvious military threat, such asweaponry. finally, the department of commerce administers the exportadministration regulations (ear; cfr parts 368œ399), in consultation withthe department of defense. commerce maintains the control list (cl), whichhas classified elements, and the commodity control list (ccl), which is notclassified. both of these lists contain dualuse technologies and products, whichhave both military and civilian/commercial value, and militarycriticaltechnologies that may be treated specially.recent developments in eastern europe have placed pressure on cocomas an institution and on the united states, which is generally more conservativethan other cocom nations about controlling exports of dualuse technology.even the topic of trade with other cocom countries has stirred substantialdebate within the u.s. government, some centering on how products are labeled(the most publicized controversy pertains to defining what is a supercomputer)and where they are listed, and much on whether a product should be listed at all.exports of general and specialpurpose computer systems are controlled ifthe systems offer one or more of three qualities: high performance (potentiallyuseful in such strategic applications as nuclear bomb development or wargaming), specific militarycritical functionality (e.g., radiation hardening andruggedness or applications like onboard fire control), or the capability toproduce highperformance or militarycritical computer systems (e.g.,sophisticated computeraided design and manufacturing systems). exports ofsupercomputers to countries other than canada and japan are subject to casebycase review, which can take months, and require special conditions associatedwith the sale, installation, and operation of the supercomputer, socalledsupercomputer safeguard plans.appendix 6.2šinsuranceinsurance is a means for sharing a risk. the insured pays the insurer (upfront, through a premium, and/or when receiving reimbursement, through adeductible or other copayment) to share his risks; if an adverse event takesplace, the insurance policy provides for payment to compensate for the damageor loss incurred. the business community already buys insurance for risksranging from fire to theft as well as for protection against employee dishonesty(bonding).to be insurable requires the following:why the security market has not worked well174computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved. a volume base for risk spreading (insurance on communication satelliteshas a very small volume, something that contributes to its cost); an establishable proof of loss; a quantifiable loss (e.g., the value of mailing lists and research datacannot be consistently and objectively quantified, according to insurancerepresentatives); an ability to tie a loss to a time frame of occurrence; an ability to credit responsibility for the loss; and a knowable loss base.with these elements, a purchaser of insurance can effectively transfer riskto a carrier and prove a loss. risks that do not satisfy these elements includeinherent business risks.another factor to consider is the nature of the consequences, whichinfluences the liability base: a computeraided manufacturing programcontrolling a robot may put lives at risk, whereas a numbercrunching generalledger program will not.the earliest insurance offerings covering computer environments weredirected at thirdparty providers of computer services (e.g., service bureaus)concerned about direct and contingent liability associated with losses to theircustomers. also leading the computer insurance market were banksšdriven bystate and federal auditors' concernsšand electronic funds transfer (eft)systems, ranging from those established by the federal reserve (e.g., fedwire)to the automated clearinghouses, for which there was legislative impetus behindthe establishment and use of insurance coverage. this governmental urging ofprovisions for insurance against computer system risks was initially resisted bythe insurance industry, which claimed not to understand the risks.insurance for banks and other financial services institutions is relativelywell developed, reflecting both the size of the potential loss, the ease withwhich the risk can be underwritten, and regulations requiring such protection.much computerrelated insurance for the banking industry, for example, buildson a historic base in bonds that protect against employee dishonesty, since mostcrimes against banks are perpetrated on the inside or with insider participation.outside of financial services, the insurance picture is mixed and lessmature. there is some coverage against computer system mishaps availablethrough employee bonding and property and casualty coverage. it is easiest toinsure the tangible elements of a computer system. by contrast, coverage maybe available for restoring a database, but not for reconstructing it from scratch.another basis for insurance is found in business interruption coverage. thusrecovery of costs for system downtime is available. a new development in thewhy the security market has not worked well175computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.1980s was the introduction of limited coverage against external intrusions andassociated offenses, including tampering, extortion, and others. although theinsurance described above protects the systemusing organization, insurancerepresentatives suggest there is a growing potential for coverage of errors andomissions on the part of the vendor, arising from the development of hardware,firmware, and software, to protect the vendor against liability claims. suchcoverage appears targeted to developers of such complex products asengineering design software.notes1. note that addon controls are futile unless the user has full control over all the software on amachine.2. a glaring example of a facility that can compromise security is ''object reuse," which never wasan issue in unix, because it could not happen. today's nonunix systems from digital equipmentcorporation and ibm still allow object reuse.3. as noted by one analyst, unix was originally designed by programmers for use by otherprogrammers in an environment fostering open cooperation rather than privacy (curry, 1990).4. the fact that consumers are preoccupied with threats posed by insiders and have problems todaythat could benefit from better procedures and physical security measures, let alone technicalmeasures, is discussed in the section titled "consumer awareness."5. for example, the most recent of a series of intragovernmental advisories is the office ofmanagement and budget's (omb's) guidance for preparation of security plans for federalcomputer systems that contain sensitive information (omb, 1990). this bulletin addresses thesecurity planning process required by the computer security act of 1987 (p.l. 100235). it isexpected to be superseded by a revision to omb circular number a130 and incorporated intofuture standards or guidelines from the national institute of standards and technology.6. an examination of this challenge for computing technologies generally can be found in aprevious computer science and technology board report, global trends in computer technologyand their impact on export control (nrc, 1988a).7. there may also have been instances in which software implementations of des or rsa were sentabroad by oversight or because the transmitter of the implementation was unaware of the law. thephysical portability of software makes such slips almost inevitable.8. note that the united kingdom and australia set the threshold at b2 or the equivalent.9. note that in this time period only one a1 product has been on the evaluated product list. theinformation on approval rates came from nsa briefings for the committee.10. this point was made by digital equipment corporation in july 1990 testimony before the housesubcommittee on transportation, aviation, and materials.11. for example, observers of the market for disaster recovery services have noted that until a 1986fire in montreal, a principal marketing tool was a 1978 study assessing how long businesses couldsurvive without their data processing operations; more recent fires (affecting the hinsdale, ill.,central office for telephone service and lowerwhy the security market has not worked well176computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.manhattan's business district) have also provided dramatic evidence of the consequences of systemmishaps (datamation, 1987).12. this situation and a variant, in which bad products effectively drive out good ones, is not unique(see akerlof, 1970).13. a security officer may even occasionally need to decrypt an encrypted file that was encryptedby a suspect using a key known only to the suspect; the security officer may have very mixedfeelings about the optimum strength of an encryption method that is available for routine use inprotecting the company's data.14. these issues have been actively discussed on electronic bulletin boards and forums (e.g.,risks, cud, the well) and in the general and business press with the publicized launch of theelectronic frontiers foundation in response to recent investigations and prosecutions.15. "insurance as a market lever" and chapter appendix 6.2 draw on discussions with insuranceindustry representatives, including carrier and agent personnel.16. insurance industry representatives voice concern about technology outpacing underwriting: if apolicy is written at one point in time, will the language and exclusions prove appropriate when aclaim is filed later, after new technology has been developed and introduced?17. indeed, there is some evidence that universities should do even more. for example, based on arecent survey, john higgins observed the following:it seems evident that a substantial majority of current university graduates in computer science haveno formal introduction to the issues of information security as a result of their university training.–while it is unlikely that every institution would develop a variety of courses in security, it isimportant that some institutions do. it establishes and helps to maintain the credibility of the subjectand provides a nucleus of students interested in security topics. the most favorable interpretation ofthe survey seems to suggest that at present there are at best only two or three such universities in thenation. (higgins, 1989, p. 556)18. risks, formally known as the forum on risks to the public in the use of computers andrelated systems, was established in august 1985 by peter g. neumann as chair of the associationfor computing machinery's (acm) committee on computers and public policy. it is an electronicforum for discussing issues relating to the use and misuse of computers in applications affecting ourlives. involving many thousands of people around the world, risks has become a repository foranecdotes, news items, and assorted comments thereon. the most interesting cases discussed areincluded in the regular issues of acm's software engineering notes (see neumann, 1989). anupdated index to about a thousand cases is under development.19. the relative reluctance of victims to report computer crimes was noted to the committee byprosecutors and insurance representatives.20. experience shows that many users do not repair flaws or install patches (software to correct aflaw) even given notification. since penetrators have demonstrated the ability to "reverse engineer"patches (and other remedies) and go looking for systems that lack the necessary corrections, theproper strategy for handling discovered flaws is not easy to devise.21. computer hardware, for example, must meet the federal communications commission'sregulations for electronic emanations, and european regulations on ergonomic and safety qualitiesof computer screens and keyboards have affected the appearance and operation of systemsworldwide.22. this point was made by digital equipment corporation in july 1990 testimony before the housesubcommittee on transportation, aviation, and materials.why the security market has not worked well177computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.23. vendors also argue that some consumers may prefer products with little security, but theprevalent lack of consumer understanding of the choices casts doubt on this explanation for theweak market.24. for example, rope manufacturers use a system of standardized strength ratings, since one cannottell at the point of manufacture whether a rope will be used to tie packages or to suspend objects, forexample. of course, some highly specialized rope, such as climbing lines, carries extra assurance,which comes with added cost.25. michael agranoff observes, "such standards would not eliminate computer abuse, especially by'insiders'; they would not eliminate computerrelated negligence. they would, however, provide a'curb on technology,' a baseline from which to judge both compensation for victims of computerabuse and the efficacy of measures to combat computer crime" (agranoff, 1989, p. 275).26. the terms and conditions governing the acquisition of operatingsystem and offtheshelfsoftware have many of the attributes of an adhesion contract (although whether there is a contract atall is open to debate). an adhesion contract is a standardized contract form offered on a "takeitorleaveit" basis, with no opportunity to bargain. the prospective buyer can acquire the item onlyunder the stated terms and conditions. of course, the "buyer" has the option of not acquiring thesoftware, or of acquiring a competing program that is most likely subject to the same or a similar setof terms and conditions, but often the entire industry offers the item only under a similar set ofterms and conditions.27. the ucc upholds express warranties in section 2313. an express warranty is created when theseller affirms a "fact or promise, describes the product, and provides a sample or model, and thebuyer relies on the affirmation, description, sample, or model as part of the basis of the bargain." bytheir very nature, express warranties cannot be disclaimed. the ucc will not allow a vendor tomake an express promise that is then disclaimed. language that cannot be reasonably reconciled isresolved in favor of the buyer.28. most recently, logisticon, inc., apparently gained telephone access to revlon, inc.'s computersand disabled software it supplied. revlon, claiming dissatisfaction with the software, had suspendedpayments. while logisticon argued it was repossessing its property, revlon suffered a significantinterruption in business operations and filed suit (pollack, 1990).29. although it would be inequitable to impose liability for clearly unintended uses in unintendedoperating environments, a vendor should not escape all liability for breach of warranty simplybecause a product can be used across a wide spectrum of applications or operating environments.30. that superior knowledge is an argument for promoting the technical steps discussed in thesection titled "consumer awareness," such as shipping systems with security features turned on.31. the customer warning system involves a point of contact for reporting security problems;proactive alerts to customers of worms, viruses, or other security holes; and distribution of fixes.32. the foreign corrupt practices act is one step toward linking accounting and informationsecurity practices; it requires accounting and other management controls that security expertsinterpret as including computer security controls (snyders, 1983). also, note that an effort is underway on the part of a group of security practitioners to address the affirmative obligations ofcorporate officers and directors to safeguard information assets (personal communication fromsandra lambert, july 1990).why the security market has not worked well178computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.7the need to establish an informationsecurity foundationin the preceding chapters, this report identifies factors contributing to lowlevels of computer security in commercial or nonmilitary systems, and itrecommends a variety of actions intended to promote security in the design,selection, and use of computer systems. this chapter argues that a neworganization should carry out many of those actions. in the discussion below,the proposed organization is called the information security foundation, or isf.mindful that u.s. efforts have been fragmented and inadequate whereas effortsin europe are gaining momentum and cohesion, this recommendation isintended to fill a troubling void. after reviewing the requirements and optionsfor such an organization, the committee concluded that the isf shouldessentially be a private, notforprofit organization, largely outside thegovernment once it is launched. it would need the highest level of support fromgovernment as well as industry; the strongest expression of such support wouldbe a congressional charter.actions needed to improve computer securityas documented in other chapters, several actions are necessary to improvecomputer security. these actions form the basis for the mission of the isf: defining requirements and evaluation criteria for users of commercialsystems, including private sector users and government processors ofsensitive but unclassified information. a major part of this effort is thedevelopment and promulgation of the generally acceptedthe need to establish an information security foundation179computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.system security principles (gssp), which would provide a set ofrequirements guidelines for trustworthy computer and communicationssystem design and use. conducting research and development, especially into criteria andevaluation procedures, in support of the above. evaluating the quality of security measures in industrydevelopedproducts during their development and throughout their life cycle, andpublishing evaluation results. in particular, evaluating products forconformance to gssp. eventually evaluations should also consider otheraspects of system trustworthiness, such as safety. (see "assuranceevaluation" in chapter 5.) developing and maintaining a system for tracking and reporting securityand safety incidents, threats, and vulnerabilities. promoting effective use of security and safety tools, techniques, andmanagement practices through education for commercial organizationsand users. brokering and enhancing communications between industry andgovernment where commercial and national security interests may conflict. focusing efforts to achieve standardization and harmonization ofcommercial security practice and system safety in the u.s. andinternationally.these actions are complementary and would be pursued most effectivelyand economically by a single organization. at present, some of these actions areattempted by the national security agency (nsa), the national institute ofstandards and technology (nist), and other organizations. however, currentefforts fall short of what is needed to accomplish the tasks at hand, and thedominant missions of existing agencies and organizations limit the scope oftheir involvement in addressing the issues of computer security andtrustworthiness. in particular, relevant government agencies are poorly suited torepresent the needs of nongovernmental system users (although they may takesome input from major system users and generate publications of interest tousers).attributes and functions of the proposed newinstitutionthe isf should have the following attributes and functions: it should be free from control by the computer and communicationvendors, but it must communicate and work effectively with them. thisquality is important to prevent the appearance or realitythe need to establish an information security foundation180computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.of bias or conflict of interest. vendors can be expected to be responsive toconsistent and credible user demand, but they have not shown (and cannotbe expected to show) leadership in defining and bringing to marketsystems with enhanced security. thus trade associations and conventionalindustry consortia are not credible vehicles for the needed activities,although they would be a valuable conduit for inputs and fordissemination of outputs such as gssp. it should have a strong user presence, through membership andparticipation in its governance. it must have defined relationships to existing governmental organizations,particularly nist and nsa, but also other organizations relevant to itsmissions, such as the defense advanced research projects agency(darpa) and the national science foundation (nsf). by charter and byaction, it must command the respect of both government and industry andmust seek open personal and institutional communications with both. itmust have ready access to technical assistance from government agencies.most importantly, because of existing agency activities there would haveto be a delineation of where the isf would have lead responsibility in theabove areas. industry, for example, would not tolerate a situation callingfor evaluations by both nsa and a new entityšbut it should findtolerable a situation involving nsa evaluations for militarycriticalsystems and isf evaluations for other, gsspcompliant systems, withcoordination between isf and nsa to minimize any duplication of effort. it must serve more than just a single industry or just the governmentalsector, to ensure the broad relevance of gssp and of the evaluations thatwould be performed to ensure conformance to gssp. it must strive to be at the forefront of the computer security field,attracting topnotch people to enable it to lead the field. staffing wouldtake time, but the opportunity to do research is necessary to attract themost talented candidates. it should address the broader problem of how to make computer systemstrustworthy, integrating security with related requirements such asreliability and safety. implementing these related requirements can benefitfrom similar techniques and mechanisms in many instances. while theisf should focus initially on security, it should consider related areas suchas safety and reliability from the start. although a security constituencyseems to be emerging outside of government, there is nothing analogousfor computer system reliability and safety. the isf could lead in helpingto establish a constituency for system trustworthiness. it should have a strong, diversified funding base. in particular, it must notdepend on government funding, although federal seedthe need to establish an information security foundation181computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.money would be appropriate. although government has much in commonwith the rest of the economy in terms of the kinds of computer systemsand applications it chooses, governmental priorities in system design, use,and management may differ from those found elsewhere, even forsystems processing sensitive but unclassified information. perhaps mostimportantly, government funding is unlikely to reach the levels or havethe stability necessary to sustain the isf. finally, policy independencemay be necessary in some cases, such as when the isf is called on to seeka middle ground between commercial and defense perspectives.the development and dissemination of gssp would be central functions ofthe isf. these activities would build on research and on consensus across avariety of stakeholding communities (vendors, commercial users, the generalpublic, and government). the goal is to achieve universal recognition along thelines that the financial accounting standards board (fasb) has for what havebeen called generally accepted accounting principles (gaap). although theanalogy to fasb is not perfect, it presents some notable parallels:the fasb plays a unique role in our society. it is a [de facto] regulator that isnot a government agency. it is an independent private foundation financed bycontributions and by revenues from the sale of its publications. contributionsare primarily from corporations and public accounting firms, but the fasb isindependent of the contributors by virtue of a carefully drawn charter. by thesame token, the fasb is independent of both the american institute of cpasand the securities and exchange commission, even though its "clout" comesfrom the fact that both institutions accept fasb pronouncements as the primeauthority for purposes of preparing financial statements in accordance withgenerally accepted accounting principles.–the fasb is the latest in a line of accounting standardsetting bodies that goback to the stock market crash of 1929 and the consequent securities acts of1933 and 1934. the stock market crash drove home the point that the u.s.economy depends greatly on a smoothly functioning capital market.– (mosso,1987)while fasb's gaap are intended to assure fair disclosure by companiesto investors and creditors, gssp are intended to protect companies andindividuals both inside and outside a computersystemusing entity. however,similar motivations inform the proposed isf and fasb. if industry does notpursue such an effort to protect itself and the public, there is a possibility ofgreater government regulation (see "regulation as a market influence" inchapter 6).the need to establish an information security foundation182computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.other organizations cannot fulfill isf's missiongovernment organizationsas noted above, the beginnings of the isf's mission can be found ingovernment. the history of government involvement in computer andcommunications security is outlined in chapter appendix 7.1. the forebearclosest to the proposed isf is the national computer security center (ncsc),which has supported the development of the orange book and performedevaluations of products against its criteria (see appendix a of this report). as isdiscussed in preceding chapters, the orange book criteria and the associatedevaluation process fall short of what vendors, users, and a wide range ofsecurity experts consider necessary. perhaps most important, the ncsc hasundergone a reorganization and downsizing that may severely limit its ability tomeet its old mission, let alone an expanded mission.a number of significant events have shaped the role of the ncsc incivilian computing. the promulgation of national security decision directive(nsdd) 145 in 1984 expanded the ncsc's scope to include civiliangovernment and some aspects of the private sector's concerns for protection ofsensitive unclassified information. subsequent passage of the computersecurity act of 1987 (p.l. 100œ235) and the july 1990 issuance of nsd 42,revising nsdd 145, substantially limited that scope to classified, nationalsecurityrelated activities. as a result, the ncsc's influence on commercial andcivilian government use of computers has been greatly reduced.starting in 1985, internal reorganizations within the nsa have merged theseparate and distinct charter of the ncsc with nsa's traditionalcommunications security role. most recently, the ncsc was reduced to a smallorganization to provide an external interface to product developers. the actualevaluations will be performed by nsa staff, sometimes assisted by specificoutsiders (e.g., mitre corporation and aerospace corporation), in directresponse to requirements of the national security community. althoughoutsourcing evaluation work is a practical solution to nsa's limited resources,it raises questions about the accountability of and incentives facing theevaluators. these questions are of great concern to industry, which hascomplained about the duration of evaluations and the lateness within theproduct cycle of the evaluation process. another issue raised by thereorganization is the extent to which nsa will remain concerned withevaluation of systems at the lower levels of the orange book, such as c2.1the need to establish an information security foundation183computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.the other major government player in this area is nist, which through thenational computer systems laboratory (ncsl) is concerned with computerand communications security. at present nist lacks the technical and financialresources to execute the agenda defined here for isf, and it also lacks thenecessary charter and organizational support. the recent move by nist tocoordinate a clearinghouse with industry focused on protections against virusesillustrates nist's opportunities for expansion, but it also illustrates nist'slimited resourcesšthis is a smallscale limitedfocus effort (danca, 1990e).in the computer security arena, nist has traditionally focused onsupporting technical standards (e.g., those related to open systemsinterconnection (osi) and integrated services digital networking) anddeveloping guidelines for system management and use. these activities aremore straightforward than articulating gssp and developing guidelines forassociated evaluations. evaluating the security functionality and assurance of acomputer system, for example, is more difficult than evaluating conformance tointeroperability standards. although nist has been involved with standardsconformance testing (and has begun a program to establish testing forconformance to certain des standards), it has so far not undertaken either tospecify evaluation criteria for the civil government or to evaluate commercialproducts against any criteria, or to offer guidelines for systemlevel evaluation.2such guidelines would have to describe how to judge the effectiveness ofsecurity safeguards against an anticipated threat.finally, its relations with nsa, on which it relies for technical assistanceand with which it has an agreement not to compete with the orange bookprocess, have not given nist the scope to act with substantial independence.the committee has doubts that nist's national computer systems laboratorycould play the role that is required, given its present charter and in particular thedifficulty it has in achieving satisfactory and consistent funding.private organizationsas banks, insurance companies, and business in general have becomeincreasingly interested in computer security, these organizations have found thattheir interests are not well served by the present activities of ncsc or nist.this situation is evidenced by either ignorance of or resistance to the orangebook (see chapter 6) and by observations on the inadequate budget andprogram of nist.but existing private organizations are also poorly suited to undertake theactions needed to improve computer security. currently, much activity in theprivate sector is driven by vendors, regulatedthe need to establish an information security foundation184computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.industries, and large computer and communications system users. they affectthe overall state of commercial security through the marketplace, tradeassociations, and relevant standardssetting ventures. as discussed in chapter 6,the influence is uneven and tends to be reactive rather than proactive.largely (but not exclusively) in the private sector are security specialists orpractitioners and their relatively new professional societies (discussed inchapter appendix 7.2). security practitioners are the principal force promotingcomputer and system security within organizations, but they operate under avariety of constraints. in particular, the voluntary nature of professionalsocieties for security practitioners limits their reach. also, professional societiestend to focus exclusively on security and show no signs of addressing broaderissues of system trustworthiness (in particular, safety).why isf's mission should be pursued outside ofthe governmentapart from the specific limitations of nist and the ncsc, there are moregeneral concerns about a governmental basis for the isf. the government has difficulty attracting and keeping skilled computerprofessionals. the ncsc, for example, appears to have been largelystaffed by young, recently graduated computer scientists who have littlepractical experience in developing complex computer systems. issues thatconstrain federal hiring include salary ceilings and limitations on thecapitalization available to technical personnel. the defense budget is shrinking. department of defense resources havesupported the activities in the ncsc and relevant activities elsewhere innsa, darpa, and research units of the armed services (e.g., the navalresearch laboratory). as noted in chapter 8, defense resources willcontinue to be valuable for supporting relevant research and development. the international standards arena may become a forum for the negotiationof standards for security and safety and for evaluation criteria. theamerican national standards institute (ansi) and other private u.s.standards organizations depend on voluntary contributions of time andtalent, and the role that nist and other agencies can play in contributingto international efforts is limited. the united states needs a strongpresence in these commercial standardssetting processes, complementingthe existing military standards process that to date has been a majorimpetus to development of trusted systems. government's necessary concern for national security sometimesthe need to establish an information security foundation185computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.obscures legitimate commercial interests, occasionally handicappingtechnology and market development that may be in the country's longterm economic security interests.the realities of the government environment suggest that accelerating thedevelopment and deployment of computer and communications securityrequires a greater role for the commercial sector.3a new notforprofit organizationgiven the limitations of private and public organizations, the committeeconcludes that the proposed information security foundation will be mostlikely to succeed as a private notforprofit organization. to assure that itsviability would not depend on specialinterest funding, multiple sources arenecessary.the isf would need the highest level of governmental support, and thestrongest expression of such support would be a congressional charter thatwould define its scope and, in particular, set parameters that would permit it towork with nsa, nist, and other agencies as appropriate. there are generalprecedents for government establishment of organizations acting in the publicinterest, including organizations that perform tasks previously performed bypublic or private entities.4 in all of these organizations, effective workingrelationships with government and operational flexibility, which would becritical for the isf, have been key.good working relationships with relevant agencies would be necessary sothat isf could contribute to satisfying government needs, especially indeveloping gssp and associated evaluations, and to avoid unnecessaryduplication of effort. for example, as noted above, there should be onerecognized source of evaluations for a given type of system. governmentrecognition of evaluations conducted by the isf would also be necessary tosupport international reciprocity in handling the results of evaluations indifferent countries (see chapter 5).one relatively new government initiative in computer security, theestablishment of computer emergency response teams (certs) to deal withthreatened or actual attacks in networks and systems, presents a specificopportunity for coordination between agencies and the isf. the isf could,building from the base already provided by darpa, provide a common pointfor collecting reports of security problems in vendor products and passing theseback to the vendor in a coordinated way. this function could be a part of thelarger action of providing an incident database (which would not be limited toemergency situations in large networked systems); the isf should bethe need to establish an information security foundation186computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.able to devote more resources to this important activity than does darpa ornist, although darpafunded cert activities could be an input into the isf.success for the isf would depend on strong participation by users andvendors. the appeal to users is that isf would provide, through the gssp andrelated evaluation processes, a mechanism for making vendors more responsiveto users' needs for systems that are more trustworthy and a forum designed toidentify and alleviate user problems. vendors would get a more responsiveevaluation mechanism and broader guidance for developing trusted systemsthan they have had in the ncsc. both vendors and users would gain fromhaving a single, wellendowed focal point for system security andtrustworthiness.critical aspects of an isf charterif the concept of establishing the isf is accepted, the details of the isf'sform and function will be discussed extensively. this report cannot offer toodetailed a vision of the isf, lest it prematurely overconstrain the approach.however, certain aspects of the isf seem critical. summarized here, theyshould be reflected in any legislation that might bring the isf into existence. the board of directors of the isf must include government, vendor, anduser representatives. the isf must be permitted to receive private funds as its major source ofincome. as discussed below, such funds would most likely be in the formof subscription fees and in charges to vendors for product evaluations. the isf must not have the salary levels of its employees tied togovernment scales but must be able to pay competitive rates. the natureof its work means that its most significant asset and the largest source ofexpense will be technical personnel. the isf must be able to solicit support from the government for specificactivities, such as research. it should be able to regrant such funds, underappropriate controls. the legal liability that the isf might incur by performing an evaluationmust be recognized and managed, given the necessarily subjective natureof evaluations. the goal is to facilitate evaluations to protect users andvendors; of course, the isf must be accountable in the event ofnegligence. this problem, which has been addressed for producttestingorganizations, might in isf's case best be handled by careful explanationof what an evaluation does and does not signify; for example, it mightsignify a given probability of resistance to certain types of attack,although no amount of testing and evaluationthe need to establish an information security foundation187computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.can ever guarantee that a system will be impervious to all attacks. it mightbe necessary for the isf to set up operating procedures to resolve disputesarising from evaluations; one option would be arbitration, which, unlikelitigation, would avoid introducing details of product design and strategyinto the public record.startup considerationsthe ncsc experience shows how difficult it can be to launch an effectiveevaluation program, in which success includes widespread industry awarenessand support as well as reasonable cost and time for evaluation. consequently,the committee believes it might take longer to inaugurate an effective isfevaluation program than to undertake other isf activities. the committeebelieves that gssp is a vital foundation for increasing customer awareness andvendor accountability, and by extension for building an effective evaluationprogram. a critical pacing factor would be vendor demand for evaluations. thismight be a function of true general acceptance for gssp, coupled with case lawtrends that might increase vendors' perceived liability for software and systemdefects. if prudent customers were to specify gssp, and vendors then usedcompliance with gssp in marketing, independent evaluation of gsspcompliance would protect both vendors and users. evaluation provides for truthin advertising from the customer's point of view, and it provides a mechanismfor the vendor to demonstrate good faith. note as a precedent that recentlyproposed legislation would ease the liability burden for vendors of productsevaluated by the food and drug administration (fda) and the federalaviation administration (crenshaw, 1990).selection of an appropriate initial leader for the organization would be acritical step; that person's job would involve not only developing a businessplan but also securing commitment from key stakeholders and recruiting astrong core staff. a parent organization should be designated to shelter the isfduring this first stage. although using a government agency would expose theisf to government politics during this first critical period, no obvious privategroup could play this role. a suitable ''launch site" would have to be soughtwhile the details of a charter, operating plan, and budget were being developed.funding the isfthis committee recommends a notforprofit consortium funded byconsumers and procurers of secure systems and functioning as a foundation.the most difficult aspect is to establish stable longtermthe need to establish an information security foundation188computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.funding to ensure the isf's effectiveness, enabling such a foundation to be acredible source for requirements and evaluation and to attract and keep a firstclass staff. the committee suggests that funding be derived from two sources:basic subscription fees, and usage fees from the computer manufacturers andcommercial users.5 also, the committee urges that the federal governmentprovide seed money to launch the operation and sustain it in the early stages.the overall budget for this kind of organization would likely be about $15million to $20 million. this assumes a budget devoted largely to costs fortechnical personnel, plus essential plant, equipment, and software tools. whileevaluations, which are laborintensive, might be the most expensive activity,they would be paid for by vendors.membership fees paid by private sector consumers of computer securityproducts should be the basic source of funds, since consumers rather thenvendors would be the main beneficiaries and would need a guarantee that theirinterests are paramount. for example, the first increment of funds could derivefrom basic subscription fees paid by all members. this funding would be usedto establish the base of research and criteria development needed for thefoundation to function efficiently. note that subscription fees for fortune 500companies of, for example, $50,000 per year per company would generate $10million annually if 200 participated. this seems to be a modest amount for a $5billion organization to spend. successful fundraising would likely hinge onobtaining commitments from industry clusters (i.e., multiple organizations ineach industry); this pattern has been observed in other consortia.system manufacturers might be asked to pay a subscription fee rangingfrom $50,000 to $500,000 based on their overall revenue. twenty vendorscontributing an average of $250,000 each would generate an additional $5million for the base fund. the basic subscription would entitle an organizationto participate in the foundation's research, evaluation, and education programs.as a reference point, note that membership in the corporation for opensystems, which promotes development of systems that comply with opensystems standards and conducts or supplies tools for conformance testing, costs$200,000 for vendors and $25,000 for users.contributions that range into six figures are difficult to obtain, especially ata time when computerrelated research and standards consortia haveproliferated (e.g., open software foundation, corporation for open systems,microelectronics and computer technology corporation, sematech, x/open)and when competitive considerations and the prospect of a recession promptbudget cutting. the mission of the proposed isf differs from that of any otherentity, but thethe need to establish an information security foundation189computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.combination of a government charter and an assured role in product evaluationswill be central for gaining the necessary corporate commitments. as notedabove, the impact of gaap comes not merely because a fasb exists butbecause the government, through the securities and exchange commission andother vehicles, has endorsed gaap (while industry has a strong voice in gaapdevelopment).the second source of funds could be fees for the evaluation of industrydeveloped products. this is analogous to other kinds of product testing, fromdrug testing (for which producers incur costs directly) to testing requested byvendors but carried out by independent laboratories (e.g., underwriterslaboratories, inc.). the actual cost incurred by the foundation for eachevaluation would be billed to the vendor. because the base of research andcriteria development activities would be funded by subscription fees, thefoundation could maintain a core staff to conduct evaluations and thus couldestablish its independence from vendors. the special nature of the isf wouldeliminate any prospect of competition with vendors and would be consistentwith the necessary protection of proprietary information. furthermore, thestability of the foundation would mean that evaluation fees could be held to aminimum. without the pool of subscription funds as general base funding, thecost of an evaluation might be prohibitive.it is critical that the evaluations be charged to the producer of the product.although it would be nice to imagine the government paying for this service,the committee concludes that this option (which is provided by the ncsctoday) is unrealistic. if the government pays, there is no way to adjust the levelof effort to meet vendor demands. if the vendor were to pay, the isf couldallocate funds to meet the product cycle of the vendor, and in this way theevaluation process could be more responsive to vendor needs. vendor fundingwould permit the organization to respond quickly with appropriate levels ofqualified individuals and would provide a critical incentive to complete theevaluation process expeditiously yet thoroughly by working with vendorsthroughout the entire development process. the evaluations could be completedand available as the products enter the marketplace (instead of years later). thegovernment could use the results of the isf directly in its own evaluation ofparticular systems.alternatives to the isfa number of alternatives to the isf, ranging from government centers toindustry facilities, must at least be considered. the base against whichalternatives should be measured is the present situationthe need to establish an information security foundation190computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.wherein the ncsc does detailed technical evaluations for the classified nationalsecurity community and nist serves in a limited advisory role to the civiliangovernment. the limitations of this situation have been discussed.one alternative is that nist develop its own computer security evaluationfacility comparable to the ncsc. the current nist course of (at least limited)endorsement of the orange book plus no direct involvement in actualevaluations argues against this alternative. without a significant change inoperational orientation and funding for nist, successfully implementing thisalternative is highly unlikely.an alternative considered in 1980, prior to the formation of the ncsc, wasthe establishment of a single federal computer security evaluation center for allof government, separate from the nsa but involving nsa, nist, and otherpersonnel representing other parts of government. the 1980 proposal wouldhave been funded jointly by the department of defense (dod) and thedepartment of commerce (doc), and it would have resulted in a center locatedat the national bureau of standards (now nist) and thus capable of operatingin an open, unclassified environment, but with the ability to deal with highlysensitive or classified issues as necessary.taking such an approach now would require major changes inmanagement philosophy and funding by dod and doc and would mostcertainly require legislative action crossing many firmly establishedjurisdictional boundaries. for these reasons and because this alternative echoesthe weaknesses of the nist alternative, the second alternative described isunlikely to succeed. however, if industry were to resist a nongovernmentalentity, then a single federal computer security evaluation organization wouldoffer improvements over what is currently available, and it could fulfill theadditional missions (development of gssp or broader educational efforts)proposed above.a third alternative that might avoid the staffing problems faced bygovernment agencies would be an independent laboratory involved in computersecurity technology development and funded by the government at a federallyfunded research and development center (ffrdc) such as mitre corporation,aerospace corporation, or the institute for defense analysis. suchorganizations already participate in ncsc evaluations on a limited basis andcan pay higher salaries and retain a core of knowledgeable experts, perhapseven rotating experts from industry. unfortunately, the experience gained todate with these organizations assisting the ncsc and the nature of thecontractual arrangement between them and ncsc have not providedopportunities for improving the existing process or for conducting research anddevelopment on the process of evaluation. also, thethe need to establish an information security foundation191computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.involvement of these groups in developing systems for the government mightcause vendors to perceive them as potential or actual competitors, therebyinspiring reluctance to divulge the proprietary information essential forthorough evaluation. this concern has been raised by u.s. vendors in responseto the u.k. plans to establish commercial licensed evaluation facilities (clefs).another approach is that taken by the fda, a government organizationthat reviews testing done inhouse by the producer of the product. in the case ofcomputer and communications systems, for which evaluation is of necessityrather subjective and the quality of assessments not easily quantified, it seemsunreasonable to expect that using vendor staff as evaluators could yield anunbiased result. there is no effective way for a government agency to controlthe process of evaluating computers and systems if it is limited to review of theresults of a vendor's evaluation.finally, note that the mission envisioned for the isf is not one that currentindependent testing laboratories can fill. evaluating trusted systems is muchmore difficult and timeconsuming than evaluating the performance of variousforms of hardware or conformance to existing technical standards.appendix 7.1ša history of governmentinvolvementthe dominant public institutions affecting computer and communicationssecurity in the united states are government agenciesšin particular, but farfrom exclusively, agencies within the department of defense (dod). driven bynational security concerns, the u.s. government has actively supported anddirected the advance of computer security since the dawn of computerdevelopment; its involvement with communications security dates back to therevolutionary war. the government's long history of involvement in computerand communications security illustrates how public institutions can nurture newtechnology and stimulate associated markets; it also shows where work remainsto be done.the national security agency and the dod perspectivethe government's involvement with computer security grew out of theevolving field of communications security in the early 1950s, when it wasdeemed necessary in the united states to establish a single organization, thethen very secret national security agency (nsa), to deal with communicationsecurity and related matters (e.g.,the need to establish an information security foundation192computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.signals intelligence) (kahn, 1967). the historical role of the dod and, inparticular, of the nsa, has been responsible for a longstanding tension betweenthe dod, which seeks to fulfill its mission of protecting national security, andcivilian agencies concerned with computer security, notably the nationalinstitute of standards and technology, together with the general vendorcommunity.the overall policy responsibility for communications security matters wasoriginally assigned to the u.s. communications security (comsec) board,consisting of cabinetlevel officials from all branches of the government, thatdealt with classified government information. this structure and nsa's highlyclassified responsibilities under that board existed from the early 1950s until themid1970s, when the issue of using encryption to protect other than classifiedinformation caused a division within the government. the publication of thedata encryption standard (des) in 1977 (nbs, 1977) (see discussion below)was a major triumph for both the civilian government and commercialcommunities (ibm contributed substantially to the development of des) buthas been regarded by some in the national security community as a majordisaster.6 up to that time, cryptography had remained largely a dark science,hidden in government secrecy. encryption systems were designed by and forthe government and were built and distributed under strict and highly classifiedgovernment control. there had also been some open research, particularly inpublickey cryptography.computer security does not have as extensive a history as doescommunications security. it has been recognized as a difficult issue needingattention for at least the past two decades. in the early 1970s, the dod fundedresearch into how to build computer systems that could be relied on to separateaccess to sensitive information in accordance with a set of rules. in themid1970s, several research projects (e.g., secure multics) were initiated todemonstrate such systems, and in 1978, the dod computer security initiativewas formed both to promote the development of such systems by industry andto explore how to evaluate them so that they could become widely available forboth government and commercial use. perhaps the most important result of thework during the 1970s was the formulation of a computerrelevant model ofmultilevel security, known as the bell and la padula model (bell and lapadula, 1976), which became the focal point of dod computer securityresearch and development. that model (discussed in chapter 3) formalizeddecades of dod policies regarding how information could be accessed, and bywhom, in manual paperbased systems.in 1981, the dod computer security evaluation center was establishedthe need to establish an information security foundation193computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.at nsa as an entity separate from the communications security structurealready in place. the reasons for this separation included the recognition thatwhile communications security had been largely a governmentowned functionin which nsa developed encryption algorithms, contracted for their production,and fully controlled their distribution and use throughout the government,computers were far more widely deployed even in the early 1980s and could notbe developed, produced, and controlled in the same way as encryption systems.a separate organization capable of working with industry, instead of directing itthrough procurement contracts, was needed.the dod computer security center, as it came to be called, published thetrusted computer system evaluation criteria (tcsec, or orange book) in1983 (superseded in 1985 by dod 5200.28std; u.s. dod, 1985d) and beganworking with industry to evaluate how well their products met the variouslevels of those criteria. it should be noted that the establishment of thecomputer security center as a separate function at nsa was opposed bothwithin and outside the agency at the time. the internal opposition stemmedfrom the perception that computer security was merely a subset ofcommunications security and should be handled in the same way by the sameorganization. the opposite view was that communications security wasbecoming increasingly dependent on computers, computer networks, andnetwork protocols, and required a new technology base managed by a neworganization. the external opposition derived from the negative concerns ofmany in the defense community, including other parts of dod and defensecontractors, that nsa's slowness to respond and dictatorial authority in thecommunications security arena would hamper the development of productsneeded to solve today's problems. these two opposing forces both within andoutside nsa continue today to influence the evolution of both computersecurity and communications security.up until the establishment of the computer security center, the precedingu.s. comsec board and another key policy group, the nationalcommunications security committee, largely ignored the computer securityproblem, lumping it, if considering it at all, into the communications securityarena. the 1977 presidential directive 24 (pd 24), which created the nationalcommunications security committee, split the responsibility forcommunications security, giving nsa authority over the protection of classifiedand national securityrelated information and the national telecommunicationsand information administration, a part of the department of commerce notrelated to the national bureau of standards (nbs), responsibility for protectingunclassified and nonnational security information. thisthe need to establish an information security foundation194computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.split in responsibility resulted in much confusion and was opposed by many inthe national security community.growing controversy over computer security led to intense pressure duringthe early days of the reagan administration to correct the situation. thoseefforts resulted in the publication in september 1984 of national securitydecision directive 145 (nsdd 145), the national policy ontelecommunications and automated information systems security, whichexpanded nsa's role in both communications and computer security andextended its influence to the national level, to the civilian government, and to alimited extent, to the commercial world. nsdd 145 required federal agencies toestablish policies, procedures, and practices to protect both classified andunclassified information in computer systems. it established the nationaltelecommunications and information systems security committee (ntissc)to develop and issue national system security operating policies.when nsdd 145 was emerging in 1983œ1984, computer security hadcome into its own with a separate organization at nsa. nsdd 145 swept thetwo forces together and elevated the dod computer security center to thenational computer security center (ncsc), giving it and the nsa's comsecboard roles in the civilian government as well as in the commercial world.in late 1985 a reorganization at nsa created the deputy directorate forinformation security, merging the comsec and computer security functionsand encompassing the ncsc. since it was becoming clear that the technologiesneeded to develop communications security systems and computer securitysystems were becoming inextricably linked, this merger was viewed by many asa positive force. others, however, viewed the expansion of nsa's role beyondthe defense and intelligence communities in a highly negative way, and effortsbegan in congress to redefine roles and limit the scope of nsa to its traditionalcommunities of interest. the computer security act of 1987 (u.s. congress,1987, p.l. 100235) defined the role of nbs (now nist) in protecting sensitiveinformation (see below), and limited nsa to its traditional responsibilities forthe protection of classified information.two recent developments have continued the withdrawal of nsa fromdirect and active involvement in the nondefense marketplace and its refocusingon the defense community and the protection of classified information andsystems generally. first, in mid1990, ncsc research and evaluation functionswere integrated with the nsa's communications security functions. officially,however, the restructuring was done to more effectively address network andsystemthe need to establish an information security foundation195computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.security issues and was prompted by "increasing recognition that current userapplications virtually eliminate traditional distinctions betweentelecommunications and information systems" (nsa, 1990a).second, nsdd 145 was revised in july 1990, resulting in nsd 42, so thatnsa no longer had responsibility for sensitive but unclassified information. incompliance with the computer security act of 1987, that responsibility wasassigned solely to nist, and all references to the private sector were removed.the ntissc became the national security telecommunications andinformation systems security committee (nstissc), under the new nationalsecurity council policy coordinating committee for national securitytelecommunications and information systems.the national institute of standards and technologythe other government agency with a longstanding interest in enhancingcomputer and communications security is the national institute of standardsand technology (nist; formerly the national bureau of standards, (nbs)),which serves all government unclassified, nonwarner amendment interests.involvement in computer and communication security began in the late 1970sand early 1980s at nist in what is now known as the national computersystems laboratory (ncsl) (formerly the institute for computer sciences andtechnology).the national institute of standards and technology's involvement incomputer security has most often resulted in the publication of federal standardsor guidelines on topics such as password protection, audit, risk analysis, andothers that are important to the use of computers but do not necessarily relate tothe technical aspects of protection within computer systems. these documents,formally known as federal information processing standards (fips)publications, are widely used within the civilian government as the basis forcomputer processing and computer system procurement. nist has also issuedother, tutorial publications to enhance awareness in government, in particular,of issues such as computer viruses. the fips publications provide valuableinformation to government computer managers who have little time to study thedetailed technical issues concerning computer systems, but who are responsiblefor their proper use. fips publications may also be valuable to industry, butthey are not widely known outside the government (although they arerecognized by many security practitioners).in 1972œ1973 interest in the establishment of an encryption algorithmsuitable for use by the nonclassified portions of the government and,potentially, the private sector, led to the des project at nbs. thethe need to establish an information security foundation196computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.issue of what constitutes "information related to national security" arose,perhaps not for the first time and definitely not for the last time, during thisperiod. the des controversy triggered the first in a series of actions intended toensure that public policy addressed the broader public interest in computer andcommunications security, not just the military interest. in particular, it helped tomotivate pd 24, discussed above. it is worth noting here that the number ofpeople involved in cryptography and its related activities at nbs during thistime frame never approached 1 percent of the number involved at nsa, andnbs's activities were substantially influenced on a continuous basis by theconstraints of nsa. nbs got by with few resources by leveraging investmentsby ibm, which was responsible for the technical development of thecryptographic algorithm that became the des.as noted above, the implementation of pd 24 contributed to the issuanceof nsdd 145, and concern about the associated expansion of nsa's role led tothe passage of the computer security act of 1987 (p.l. 100235), whichdefined specific informationprotection roles for nbs and thereby limitednsa's responsibilities. shortly thereafter, nbs was renamed the nationalinstitute of standards and technology (nist). although the renamedorganization has yet to be funded at a level commensurate with its current oranticipated mission, the intent was to strengthen the organization as a vehiclefor stimulating nondefense technology development. under p.l. 100235, nistis primarily responsible for establishment and dissemination of standards andguidelines for federal computer systems, including those needed "to assure thecosteffective security and privacy of sensitive information in federal computersystems." nist is also involved with other objectives of p.l. 100235 intendedto raise security awareness in the federal computing community: theestablishment of security plans by operators of federal computer systemscontaining sensitive information, and training of all persons associated withsuch systems.the complementary nature of the respective computer security missions ofnsa and nist as well as nsa's larger role in its national security arenanecessitates cooperation between the two. that cooperation has recently beenshaped by a memorandum of understanding (mou) developed to helpimplement p.l. 100235 and to assure national security review of areas ofmutual interest (nist/nsa, 1989). the computer security act of 1987 callsfor nist to draw on nsa for technical assistance (e.g., research, development,evaluation, or endorsement) in certain areas. the mou calls for nist to drawon nsa's expertise and products "to the greatest extent possible" in developingtelecommunications security standards for protecting sensitive but unclassifiedcomputer data, and to draw on nsa's guidelines forthe need to establish an information security foundation197computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.computer system security to the extent that they are ''consistent with therequirements for protecting sensitive information in federal computer systems."under the mou, a joint nsanist technical working group was established"to review and analyze issues of mutual interest" regarding the protection ofsystems processing sensitive information, especially those issues relating tocryptography.the national security agency as well as nist personnel are also involvedwith the nist computer and telecommunications security council and withthe computer systems security and advisory board organized by nist underp.l. 100235.according to the mou, nist is prevented from developing a competingset of ratings for security product evaluation.7 it plans instead to issue amanagement guide, aimed at civilian government, that will explain what trustedand evaluated systems are, and will point agencies toward evaluated systems asappropriate (this topic has already been treated in an ncsl bulletin). althoughnist does not give specific product ratings or endorsements, it is involved withdeveloping tests of products for conformance to its standards, and it has plans toaccredit other organizations to validate products for conformance to certainfips. nist does not appear likely to follow the nsa in publishing lists ofevaluated products such as ncsc's evaluated products list.unlike the nsa, nist has had only a small program in securityrelatedresearch. in particular, it has sponsored none of the fundamental operatingsystem research needed to develop or evaluate trusted computer systems,although nbs monitored the research and development activities of the 1970sand held an invitational rancho santa fe access control workshop in 1972.nist continues to participate in the dod computer security initiative throughjoint sponsorship of the "nbs" (now national) computer security conference,and nist has recently held a series of workshops aimed at generatingguidelines for integrity.observers suggest that nsa continues to have a substantial, although notalways direct, influence on nist's activities, drawing on nsa's nationalsecurity mission. while nist's computer security responsibilities grew as aresult of p.l. 100235, it was denied several budget increases requested by theadministration, and it remains funded in this area at the level (i.e., taking intoaccount growth in expenses like salaries) in place prior to the passage of thelaw. out of an appropriated nist budget of approximately $160 million (alevel almost matched by externally sponsored research), the appropriated fy1990 nist security program was $2.5 million; the nsa budget, the details ofwhich are classified, is on the order of $10 billion (lardner, 1990b).accordingly, the number of people involved in computerthe need to establish an information security foundation198computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.security at nbs/nist has always been relatively small compared with thenumber at nsa.other government agency involvementthe historic emphasis on the roles of nsa and nist makes it easy tooverlook the fact that other government agencies and groups are also involvedin promoting computer and communications security. as discussed inchapter 8, other dod agencies and the department of energy engage insecurityrelated research and development, although, with the exception ofdarpa, much of this work is tied to the operating mission of the relevantorganization; the national science foundation (nsf) funds basic research inmathematics and computer science that is relevant to the development of secureand trusted systems. note that while the dod's research and procurement haveemphasized a specific area of computer securityšnamely access control, whichhas a longestablished basis in manual systemsšit took almost two decades totransform research concepts into commercially produced, governmentevaluated products, which are only now beginning to satisfy dod applicationneeds. this lengthy gestation reflected the need to develop, and achieve someconsensus on, complex technology and an associated vocabulary.as recognized by p.l. 100235, the computerization of governmentactivities creates a need for computer and communications security in allgovernment agencies and organizations. for example, in an informal committeesurvey of 1989 government requests for proposals (rfps), some of the highestcomputer security requirements were stipulated for systems being procured bythe treasury department, the federal aviation administration, and the senate.across the government, security is one of many concerns captured in federalinformation resources management regulations (president's council onintegrity and efficiency, 1988; gsa, 1988), and p.l. 100235 mandatescomputer security planning and precautions for federal organizations. however,merely having a plan on paper is no guarantee that sound or effectiveprecautions have been taken. the gao has repeatedly raised this concern inconnection with government computer systems (gao, 1990c).two agencies, the general services administration (gsa; whichcoordinates government procurement) and the office of management andbudget (omb; which influences government procurement and has a generalinterest in the efficient use of information and systems), set the operatingclimate for computer and communications securitythe need to establish an information security foundation199computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.within civil government through circulars (e.g., a130) and other directives.despite this nominal breadth, defense agencies, which operate under a securityoriented culture and with a strong system of information classification, havebeen more active than most civilian agencies in seeking greater security. theyhave a relatively high degree of concern about unauthorized disclosure andaccess control, and they have been prodded by military standards (e.g., theorange book, which was made into a military standard) and by procurementrequirements for specific types of systems in certain applications (e.g., tempestunits that have shielding to minimize electronic emanations).federal concerns regarding protection of unclassified systems and datainclude protection against improper disclosure of personal data, as required bythe privacy act of 1974 (p.l. 93579), protection against fraud, and protectionof the availability and integrity of government systems (on which millionsdepend for a variety of payments and other services).although the scale of and public interest in government systems may beunique, the government shares many of the same problems found in commercialand other organizations, including inadequate awareness and inadequateprecautions. because of these commonalities, many of nist's activities, whilenominally aimed at meeting civilian government needs, are relevant to industry.a third group of government entities involved with computer andcommunications security are the investigating and prosecuting agencies,including the federal bureau of investigation (responsible for major federal lawenforcement and also for counterintelligence), the secret service (responsiblefor investigating computer crimes involving finance and communicationsfraud), the department of justice and the u.s. attorneys (both responsible forprosecuting federal cases), agencies with specialized law enforcementresponsibilities (e.g., u.s. customs service), and state and local lawenforcement entities (conly, 1989; cook, 1989). these agencies are concernedwith deterring and prosecuting computer crimes, which may result frominadequate computer and communications security. among the challenges theyhave faced are encouraging the development of laws that fit emerging andanticipated patterns of crime, and applying laws developed under differenttechnological regimes (e.g., laws against wire fraud) to computer crimes. (seebox 7.1 for a list of relevant laws.) these agencies report difficulties inachieving support from the public (computerrelated crimes often gounreported), difficulties in obtaining the necessary technical expertise, anddifficulties in obtaining management support for investigations of crimes that,compared to others, require a relatively large expenditure of resources forinvestigation relative to the nominal losses8 involved (conly, 1989; cook,1989).the need to establish an information security foundation200computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.box 7.1 legislative toolscongress has responded to the computer and telecommunication threatby providing federal investigators and prosecutors with impressive tools.18 u.s.c. §1029:prohibits fraudulent activity in connection with usingaccess devices in interstate commerce, includingcomputer passwords, telephone access codes, andcredit cards.18 u.s.c. §1030:prohibits remote access with intent to defraud inconnection with federal interest computers and/orgovernmentowned computers and prohibitsunauthorized computer access by companyemployees.18 u.s.c. §1343:prohibits the use of interstate communicationssystems to further a scheme to defraud.18 u.s.c. §2512:prohibits making, distributing, possessing, andadvertising communication interception devices andequipment.18 u.s.c. §2314:prohibits interstate transportation of stolen propertyvalued at over $5,000.17 u.s.c. §506:prohibits copyright infringement violationsšbut onlyif the copyright is actually on file.22 u.s.c. §2778:prohibits illegal export of department of defensecontrolled software and data.50 usca p. 2510:prohibits illegal export of department of commercecontrolled software and data.18 u.s.c. §793:prohibits espionagešincluding obtaining (and/orcopying) information concerning telegraph, wireless,or signal station, building, office, research laboratory,or stationšfor a foreign government, or to injure theunited states.18 u.s.c. §2701:prohibits unlawful access to electronically storedinformation.18 u.s.c. §1962:prohibits racketeering, which is in turn defined astwo or more violations of specific crimes, including18 u.s.c. §1029, §1343, and §2314.source:cook (1989).appendix 7.2šsecurity practitionersmany organizations rely on a security specialist or practitioner forguidance on computer and communications security problems and practices.most such individuals are associated with information systems planning andoperation units; others may be involved with the security of larger corporatefunctions (including physical facilities security as well as computer systemconcerns), with internal or external auditing responsibilities, or with an internalor external consulting service. as this range of roles suggests, securitypractitioners have athe need to establish an information security foundation201computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.variety of backgrounds and tend to be in staff positions. informalcommunication with such individuals revealed a shared perception amongsecurity practitioners that their job is often made difficult by management'sresistance to recommendations for greater securityrelated controls.nevertheless, while much of the debate about technology development has beendominated by technical (research, development, and evaluation) experts,security practitioners are a more prominent influence on the evergrowingsystemusing community. these are the individuals responsible for selecting,recommending, and implementing security technology and procedures.several professional societies provide guidelines, continuing education,and other tools and techniques to computer and communications securitypractitioners. they include, for example, the information systems securityassociation (issa), the computer security institute (csi), the special interestgroup for computer security (sigcs) of the data processing managementassociation (dpma), the american society for industrial security (asis), andthe edp auditors association. another such group has been organized by sriinternational, which offers a "continuing multiclient service" called theinternational information integrity institute (i4). the membership of i4 islimited, by membership decision, to approximately 50 firms that are typicallyrepresented by security practitioners (sri international, 1989). other groupsinclude largescale users groups like guide and share for ibm system users andindustryspecific associations like the bank administration institute.the need for professional certification has been a growing concern amongsecurity practitioners. by the mid1980s professional societies recognized thatcertification programs attesting to the qualifications of information securityofficers would enhance the credibility of the computer security profession.after attempting without success to associate with existing accreditedcertification programs, the information systems security association (issa)decided to develop its own. committees were formed to develop the commonbody of knowledge, criteria for grandfathering (to accommodate the transitionto the new regime of certification), and test questions. the common body ofknowledge refers to the knowledge deemed necessary to accomplish the tasksor activities performed by members in the field.elements of the common body of knowledge identified by a committee ofa new consortium of professional societies described below include thefollowing: access controlšcapabilities used by system management to achieve thedesired levels of integrity and confidentiality by preventing unauthorizedaccess to system resources.the need to establish an information security foundation202computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved. cryptographyšuse of encryption techniques to achieve dataconfidentiality. risk managementšminimizing the effects of threats and exposuresthrough the use of assessment or analysis, implementation of costeffective countermeasures, risk acceptance and assignment, and so on. business continuity planningšpreparation for actions to ensure thatprograms critical to preserving a business are run. data classificationšimplementation of rules for handling data inaccordance with its sensitivity or importance. security awarenessšconsciousness of the reality and significance ofthreats and risks to information resources. computer and systems securityšunderstanding computers, systems, andsecurity architectures so as to be able to determine the appropriate typeand amount of security appropriate for the operation. telecommunications securityšprotection of information in transit viatelecommunications media and control of the use of telecommunicationsresources. organization architecturešstructure for organization of employees toachieve information security goals. legal/regulatory expertisešknowledge of applicable laws and regulationsrelative to the security of information resources. investigationšcollection of evidence related to information securityincidents while maintaining the integrity of evidence for legal action. application program securityšthe controls contained in applicationprograms to protect the integrity and confidentiality of application dataand programs. systems program securityšthose mechanisms that maintain the securityof a system's programs. physical securityšmethods of providing a safe facility to support dataprocessing operations, including provision to limit (physical) access toauthorized personnel. operations securityšthe controls over hardware, media, and the operatorswith access privileges to the hardware and media. information ethicsšthe elements of socially acceptable conduct withrespect to information resources. security policy developmentšmethods of advising employees ofmanagement's intentions with respect to the use and protection ofinformation resources.in november 1988 a consortium of organizations interested in thecertification of information security practitioners began to forge a jointcertification program. in mid1989, the international information systemssecurity certification consortium or (isc)2 was establishedthe need to establish an information security foundation203computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.as a nonprofit corporation (under the provisions of the general laws, chapter180, of the commonwealth of massachusetts) to develop a certificationprogram for information systems security practitioners. participatingorganizations include the information systems security association (issa), thecomputer security institute (csi), the special interest group for computersecurity (sigcs) of the data processing management association (dpma),the canadian information processing society (cips), the internationalfederation of information processing, agencies of the u.s. and canadiangovernments, and idaho state university (which has developed computersecurity education modules). committees of volunteers from the variousfounding organizations are currently developing the products needed toimplement the certification program, such as a code of ethics, the common bodyof knowledge, an rfp for obtaining a testing service, a marketing brochure forfund raising, and preliminary grandfathering criteria. funds are being soughtfrom major computerusing and computerproducing organizations.according to (isc)2 literature, certification will be open to all who"qualify ethically" and pass the examinationšno particular affiliation with anyprofessional organization is a prerequisite for taking the test. the examinationwill be a measure of professional competence and may be a useful element inthe selection process when personnel are being considered for the informationsecurity function.9 recertification requirements will be established to ensurethat individual certifications remain current in this field that is changing rapidlyas technological advancements make certain measures obsolete and providemore effective solutions to security problems.the growth of security practitioner groups and activities is a positive force,one that can help to stimulate demand for trust technology. because thisprofession is new, still evolving, and diverse in composition, it is not clear thatit can have the impact on security that, say, certified public accountants have onaccounting. that assumption is based in part on the absence to date of generallyaccepted computer and communications security principles and maturestandards of practice in this arena, as well as the absence of the kind of legalaccountability that other professions have achieved.notes1. the concerns discussed focus on the ncsc's ability to reach out into the commercial world andinfluence the marketplace. the substantive thrust of the reorganized ncscša new emphasis onheterogeneous, networked systemsšshould generate valuable insights and techniques, althoughwho will benefit from them outside the government is not at all clear.the need to establish an information security foundation204computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.2. in september 1990, the computer system security and privacy advisory board established underthe computer security act of 1987 proposed that nist issue guidelines on civilian agencycomputer security analogous to the rainbow series and published as federal informationprocessing standards. however, it is not clear how or by whom such a document would bedeveloped, in part because nist lacks relevant funding (danca, 1990e).3. ironically, it was a similar recognition that led to the launch of the ncsc in the first place.4. note that the federal government already has a number of vehicles for action that do not involvedirect administration by federal employees, such as nonprofit federally funded research anddevelopment centers (ffrdcs), governmentowned/ contractoroperated (goco) industrial plants,and specially chartered quasipublic organizations such as federally sponsored financing agenciesthat conduct activities formerly conducted by the private sector. comsat is perhaps the most widelyrecognized example; it was specially chartered by congress, but it is profit making and is funded byselling shares. more relevant is the ffrdc concept, also involving congressional charters, which ingeneral does not, however, permit the flexibility in funding or in mission envisioned for the isf(musolf, 1983).5. another source of funds might eventually be sales of publications. such sales provide about $10million in revenue for fasb, for example (fasb, 1990).6. the emergence of des in the 1970s, its promotion by the then institute for computer sciencesand technology (icst) of the then national bureau of standards (nbs), and the role of the nsa inthat evolution, have been well publicized (ota, 1987b).7. the mou states that nist will "recognize the nsacertified rating of evaluated trusted systemsunder the trusted computer security evaluation criteria program without requiring additionalevaluation," and it also makes many references to coordination with nsa to avoid duplication ofeffort or conflict with existing technical standards aimed at protecting classified information.8. the nominal losses in a specific case are misleading. they signal a potential for greater lossthrough repetitions of undetected abuse.9. note that the movement toward certification among security practitioners contrasts with theongoing heated debate among systems developers and software engineers over certification.the need to establish an information security foundation205computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.8research topics and fundingearlier chapters of this report included discussions of the state of the art incomputer security that also addressed a variety of research activities. thischapter addresses the broader issue of the state and structure of the researchcommunity and also outlines some areas of research where the current level ofeffort seems insufficient. in addition, the committee also addresses directionsfor federally funded extramural research programs.the committee believes that there is a pressing need for a stronger programof universitybased research in computer security. such a program should havetwo explicit goals: addressing important technical problems and increasing thenumber of qualified people in the field. this program should be stronglyinterconnected with other fields of computer science and cognizant of trends inboth theory and uses of computer systems.in the 1970s the department of defense (dod) aggressively funded anexternal research program that yielded many fundamental results in the securityarea, such as the reference monitor and the bell and la padula model (bell andla padula, 1976). but with the establishment of the national computer securitycenter (ncsc) in the early 1980s, the dod shifted its emphasis from basicresearch to the development and application of evaluation criteria and thedevelopment of applications that meet mission needs. the specific focus ofmost dod funding for basic research has been related to nondisclosure ofinformation. furthermore, relatively little of the dodfunded research oncomputer security is currently being done at universities.the committee reviewed (unclassified) research on information securityconducted by the national security agency (nsa), and theresearch topics and funding206computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.ncsc in particular. now the research activities of the two are combined, owingto ncsc's recent reorganization, and the committee is not in a position tocomment on the newly structured program. although nsa supports activeresearch at several private centers (e.g., sri international and mitrecorporation), its support for academic research in computer security appears tohave been quite limited in scope and level. that support cannot be trackedstraightforwardly, because some of it is passed through other agencies and somerecipients have been asked not to divulge nsa's support. nsa has providedsome funding for programs, such as the outside cryptographic research program(ocreae) and dod's university research initiative (uri), that seek toincrease the pool of appropriately trained american graduates. in late august1990, nsa announced a new computer security university research program,a modest effort aimed at supporting university summer study projects (whichare inherently limited in scope and scale).at the same time, the other agencies with significant agendas related toresearch in computer security, such as the department of energy (doe), thenavy's office of naval research (onr), and the national institute of standardsand technology (nist), have had limited programs in funded externalresearch.1 in the area of information integrity, nist has attempted to establish arole for itself by holding a series of workshops, but no significant researchfunding has resulted.2notforprofit and vendor laboratories are pursuing a variety of projects,many of which are discussed elsewhere in this report (e.g., see chapter 4).however, support for these activities fluctuates with both government interestin security and shortterm business needs. although many of the topicsproposed below are relevant to industrial research conducted independently orin collaboration with universities, the committee focused on the need tostimulate academic research.universitybased research in computer security is at a dangerously lowlevel.3 whereas considerable research is being done on theoretical issues relatedto securityšfor example, number theory, cryptology, and zeroknowledgeproofsšfew research projects directly address the problem of achieving systemsecurity. this lack of direct attention to system security is particularly seriousgiven the ongoing dramatic changes in the technology of computing (e.g., theemergence of distributed systems and networks) that make it necessary torethink some of the current approaches to security. highrisk and longtermresearch, a traditional strength of universities, is essential. furthermore, thesmall number of academicians with research interests in the area of computersecurity makes it impossible to train a sufficient number ofresearch topics and funding207computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.qualified experts capable of participating in commercial research anddevelopment projects.various issues contribute to the lack of academic research in the computersecurity field. one is the occasional need for secrecy, which conflicts with thetradition of open publication of research results. another is the holistic nature ofsecurity. there is a risk in studying one aspect of security in isolation; theresults may be irrelevant because of changes or advances in some other part ofthe computer field. in many academic environments, it is difficult to do thelarge demonstration projects that provide worked examples (proofs of concepts)of total security solutions.meanwhile, evidence suggests a growing european research anddevelopment effort tied to national and regional efforts to develop the europeanindustrial base. although not focused specifically on security, several of theseprojects are developing advanced assurance techniques (e.g., formal methodsand safety analysis). the portable common tool environment (pcte)consortium of vendors and universities has proposed extensions to pcte thatallow programming tools to utilize common security functions, modeled afterbut more general than those outlined in the orange book (iepg, 1989;european commission, 1989a, p. 8). on another front, esprit funding isestablishing a pattern of collaboration that could pay off significantly insystemsoriented fields such as security and safety, as researchers learn to workeffectively in relatively large academic and industrial teams.4 although miti injapan is conducting a study of security problems in networks, the committeehas found no widespread japanese interest in developing indigenous securitytechnology at this time.a proposed agenda for research to enhancecomputer securitythe committee identified several specific technical issues currently ripe forresearch. it is expected that the issues described will have aspects that are bestaddressed variously by universities, contractors, nonprofit research laboratories,government laboratories, and vendor laboratories. the key is to develop a broadrange of system security expertise, combining the knowledge gained in bothacademic and industrial environments. the list that follows is by no meanscomplete (rather, a research agenda must always reflect an openness to newideas) but is provided to show the scope and importance of relevant researchtopics and to underscore the need to cultivate progress in areas that havereceived insufficient attention.research topics and funding208computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved. security modularity: how can a set of system components with knownsecurity properties be combined or composed to form a larger system withknown security properties? security models: the disclosure control problem has benefited from aformal model, the bell and la padula model, which captures some of thedesired functionality in an abstract manner. other security requirements,such as integrity, availability, and distributed authentication andauthorization, do not have such clean models. lacking a clean model, it isdifficult to describe what a system does or to confirm that it does so. forexample, models are needed that deal with separation of duty and withbelief and trust in situations of incomplete knowledge. efforts should bedirected at establishing a sound foundation for security models. themodels that have been used in the past lack, for the most part, any formalfoundation. the franconia workshops (ieee, 1988œ1990) have addressedthis issue, but more work is necessary. security models should beintegrated with other systems models, such as those related to reliabilityand safety. cost/benefit models for security: how much does security really cost, andwhat are its real benefits? both the cost of production and the cost of useshould be addressed. benefit analysis must be based on careful riskanalysis. this is particularly difficult for computer security becauseaccurate information on penetrations and loss of assets is often notavailable, and analyses must depend on expert opinion. therecommended reporting and tracking function envisioned for theinformation security foundation proposed in chapter 7 would facilitatemodel generation and validation. new security mechanisms: as new requirements are proposed, as newthreats are considered, and as new technologies become prevalent, newmechanisms will be required to maintain security effectively. recentexamples of such mechanisms are the challengeresponse devicesdeveloped for user authentication. among the mechanisms currentlyneeded are those to support critical aspects of integrity (e.g., separation ofduty), distributed key management on lowsecurity systems, multiwayand transitive authentication (involving multiple systems and/or users),availability (especially in distributed systems and networks), privacyassurance, and limitations on access in networks, to permitinterconnection of mutually suspicious organizations. assurance techniques: the assurance techniques that can be applied tosecure systems range from the impractical extremes of exhaustive testingto proofs of all functions and properties at all levels of a system. it wouldbe beneficial to know the complete spectrum of assurance techniques, thepracticality of their application, and to whatresearch topics and funding209computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.aspects of security they best apply. for instance, formal specification andverification techniques can be applied to some encryption protocols butmay be more useful for testing formal specifications in an effort todiscover design weaknesses (millen et al., 1987; kemmerer, 1989a).also, formally specifying and verifying an entire operating system maynot be costeffective, yet it may be reasonable to thoroughly analyze aparticular aspect of the system using formal specification and verificationtechniques. (this is one of the reasons for grouping the securityrelevantaspects of a secure operating system into a security kernel that is smallenough to be thoroughly analyzed.) identifying effective and easily usablecombinations of techniques, particularly ones that can be applied early insoftware production, is a current area of interest in the field of testing,analysis, and verification. in addition, attention must be given tomodernizing the existing technology base of verification and testing tools,which are used to implement the techniques, to keep pace with newtechnology. alternative representations and presentations: new representations ofsecurity properties may yield new analysis techniques. for example,graphics tools that allow system operators to set, explore, and analyzeproposed policies (who should get access to what) and systemconfigurations (who has access to what) may help identify weaknesses orunwanted restrictions as policies are instituted and deployed systems areused. automated security procedures: a practical observation is that many, ifnot most, actual system penetrations involve faults in operationalprocedures, not system architecture. for example, poor choice ofpasswords or failure to change default passwords is a common failuredocumented by stoll (1989). research is needed in automating criticalaspects of system operation, to assist system managers in avoidingsecurity faults in this area. examples include tools to check the securitystate of a system (baldwin, 1988), models of operational requirementsand desired controls, and threat assessment aids. faulttree analysis can beused to identify and assess system vulnerabilities, and intrusion detection(lunt, 1988) through anomaly analysis can warn system administrators ofpossible security problems. mechanisms to support nonrepudiation: to protect proprietary rights itmay be necessary to record user actions so as to bar a user from laterrepudiating these actions. research into methods of recording user actionsin a way that respects the privacy of users is difficult. control of computing resources: resource control is associated with theprevention of unauthorized use and piracy of proprietary software ordatabases owned or licensed by one party and legitimately installed in acomputing system belonging to another. it has attracted littleresearch topics and funding210computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.research and implementation effort, but it poses some difficult technicalproblems and possibly privacy problems as well, and it is, therefore, anarea that warrants further research. systems with security perimeters: most network protocol design effortshave tended to assume that networks will provide general interconnection.however, as observed in chapter 3, a common practical approach toachieving security in a distributed system is to partition the system intoregions that are separated by a security perimeter. this is not easy to do.if, for example, a network permits mail but not directory services(because of security concerns about directory searches), the mail may notbe deliverable due to the inability to look up the address of a recipient. toaddress this problem, research is needed in the area of network protocolsthat will allow partitioning for security purposes without sacrificing theadvantages of general connectivity.directions for funding security researchthere are several strategic issues basic to broadening computer securityresearch and integrating it with the rest of computer science: funding agencies'policies, crossfield fertilization, and the kinds of projects to be undertaken. theareas of study sketched above are suitable for funding by any agency with acharter to address technical research topics.the committee recommends that the relevant agencies of the federalgovernment (e.g., darpa and nsf) undertake funded programs of technologydevelopment and research in computer security. these programs should fosterintegration of security research with other related research areas, such aspromoting common techniques for the analysis of security, safety, andreliability properties. the committee recommends that nist, in recognition ofits interest in computer security (and its charter to enhance security for sensitivebut unclassified data and systems), work to assure funding for research in areasof key concern to it, either internally or in collaboration with other agenciesmore traditionally associated with research. nist may be particularly effective,under its current regime, at organizing workshops that bring togetherresearchers and practitioners and then widely disseminating the resultingworkshop reports.although federal agencies have traditionally been viewed as the primarysource of funding for computer science research, many states, such as texas,virginia, and california, have substantial funding programs geared towardregional industry and academic needs. the proposed research agenda should bebrought to the attention of state fundingresearch topics and funding211computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.agencies, especially in those states where industrial support and interaction arelikely.both the defense advanced research projects agency (darpa) and thenational science foundation (nsf) should proceed to justify a program inextramural computer security research. however, because of differences in thetraditional roles of darpa and nsf, this committee has identified specificactivities that it recommends to each.funding by the defense advanced research projects agencythe defense advanced research projects agency has traditionally beenwilling to fund significant systemdevelopment projects. the committeebelieves that this class of activity would be highly beneficial for securityresearch. security is a handson field in which mechanisms should be evaluatedby deploying them in real systems. some examples of suitable projects are thefollowing: use of stateoftheart software development techniques and tools toproduce a secure system. the explicit goal of this effort should be toevaluate the development process and to assess the expected gain insystem quality. development of distributed systems with a variety of security properties.a project now under way, and funded by darpa, is aimed at developingencryptionbased private electronic mail. another candidate for study isdecentralized, peerconnected name servers. development of a system supporting an approach to ensuring the integrityof data. there are now some proposed models for integrity, but withoutworked examples it will be impossible to validate them. this representsan opportunity for a cooperative effort by darpa and nist.funding by the national science foundationthe national science foundation has tended to fund smaller, lessdevelopmentoriented projects. a key role for nsf (and for darpa, as well),beyond specific funding of relevant projects, is to facilitate increased interactionbetween security specialists and specialists in related fields (such as distributedcomputing, safety, and faulttolerant computing). examples of areas in whichcreative collaboration might advance computer security include: safety: concern about the safetyrelated aspects of computer processing isgrowing both in the united states and internationally. great britain hasalready formulated a policy that requires the use ofresearch topics and funding212computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.stringent assurance techniques in the development of computer systemsthat affect the safety of humans (u.k. ministry of defence, 1989a,b).unfortunately, safety and related issues pertaining to computer systemsšunlike securityšhave no constituency in the united states. faulttolerant computing: over the years a great deal of research has beendirected at the problem of faulttolerant computing. most of this work hasaddressed problems related to availability and integrity; little attention hasbeen directed to the problems of malicious surreptitious attacks. anattempt should also be made to extend this work to other aspects ofsecurity. code analysis: researchers working on optimizing and parallelizingcompilers have extensive experience in analyzing both source and objectcode for a variety of properties. some of their techniques have been usedfor covert channel analysis (haigh et al., 1987; young and mchugh,1987). an attempt should be made to use similar techniques to analyzecode for other properties related to security. security interfaces: people experienced at writing careful specifications ofinterfaces and verifying highlevel properties from these specificationsshould be encouraged to specify standardized interfaces to securityservices and to apply their techniques to the specification and analysis ofhighlevel security properties. theoretical research: theoretical work needs to be properly integrated inactual systems. often both theoreticians and system practitionersmisunderstand the system aspects of security or the theoretical limitationsof secure algorithms. practitioners and theoreticians should be encouragedto work together.promoting needed collaborationboth darpa and nsf have a tradition of working with the broad sciencecommunity and should initiate programs to facilitate collaboration. somesuggestions for specific actions are the following: start a program aimed specifically at bringing together people withdifferent backgrounds and skills, for example, by providing grants tosupport visiting researchers for a period of one to two years. show a willingness to support research in computer security by peoplewith complementary expertise (in accounting or distributed systems, forexample), although they may have no track record in the security area. run a series of one or twoweeklong workshops for graduate studentswho are interested in doing research on problems related to computersecurity. prior experience in security should be secondaryresearch topics and funding213computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.to interest and evidence of accomplishment in related fields. workshopsshould, where possible, include laboratory experience with securityproducts and assurance technology.traditionally, computer security research has been performed in computerscience and engineering departments. however, another research approach thatseems relevant is the methodology of the business school. although businessschools have in the past shown little interest in security research, obvious studytopics include: value of security: a current research topic in business schools is assessinginformation technology's actual value to an organization. as a part ofthese studies, it might be possible to develop models for the value of thesecurity aspects of information technology from a business perspective,for example, drawing on the value of a corporate information base to beprotected. privacy in information systems: the use of a computer system in thecorporate environment will be influenced by the degree to which the usersperceive the information in the system as public or private. thesociological aspects of privacy may have a strong impact on the effectiveuse of information technology. a valuable contribution would be casestudies leading to a working model that relates perceived protection ofprivacy to an application's effectiveness. those involved in the emergingfield of computersupported cooperative work (also known ascollaboration technology or groupware) should be made aware of (1) theneed for security mechanisms when information is shared and (2) theinfluence of requirements for privacy on the processes being automated orcoordinated. in general, any study of information flow in an organizationshould also note and assess the security and privacy aspects of thatinformation flow.notes1. the office of naval research, however, has an ongoing internal program (at the naval researchlaboratory) in applied security research that includes such projects as methodologies for securesystem developers and tools for secure software development. the lack of appropriately trainedindividuals has been cited by onr as a major impediment to expanding their research efforts.the department of energy has responded to the recent spate of computer security breaches with aneffort centered at their lawrence livermore national laboratory to develop tools, techniques, andguidelines for securing computer systems. areas currently under investigation include viruses,intrusion detection systems, and security maintenance software tools. the doe also created acomputer incident advisory capability (ciac) similar to darpa's internet cert, but specificallyto support doe. further effort is being expended on developing guidelines for system securitytesting, incident handling, and others. doe is also supporting efforts to develop a universitybasedresearch capability.research topics and funding214computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.2. a limited computer security budget has hampered even internal nist efforts to date, althoughseveral programs are under development that would group funds from private industry or otherfederal agencies to address mutual security concerns (see chapter 7 for a more complete discussionof nist activities).3. consider, for example, the following indicators of low academic participation in the field ofcomputer security. at the january 1989 nist integrity workshop, of the 66 listed attendees, only 6were from u.s. academic institutions. at the 1988 institute of electrical and electronics engineerssymposium on security and privacy, a more general security conference with considerable attentionto dod interests, less than 6 percent were academic attendees out of an approximate total of 316. incontrast, at a broad conference on computer systems, the 1989 association of computingmachinery symposium on operating system principles, approximately 36 percent of the attendeeswere from u.s. academic institutions.4. examples include provably correct systems (procos), a result of basic research oriented towardlanguage design, compiler systems, and so on, appropriate for safetycritical systems; softwarecertification on programs in europe (scope), which will define, experiment with, and validate aneconomic european software certification procedure applicable to all types of software andacceptable and legally recognized throughout europe; and demonstration of advanced reliabilitytechniques for safetyrelated computer systems (darts), whose aim is to facilitate the selectionof reliable systems for safetycritical applications (european commission, 1989a, pp. 27 and 55;1989b).research topics and funding215computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.bibliographyadams, e. 1984. ''optimizing preventative service of software products," ibm journal of r&d,vol. 28, no. 1.adrion, w. r. 1989. testing techniques for concurrent and realtime systems, university ofmassachusetts, amherst.agranoff, michael h. 1989. "curb on technology: liability for failure to protect computerized dataagainst unauthorized access," computer and high technology law journal, vol. 5, pp.265œ320.akerlof, george a. 1970."the market for 'lemons': quality uncertainty and the market mechanism,"quarterly journal of economics, 87, pp. 488œ500.alexander, michael. 1989a. "computer crime fight stymied," federal computer week, october 23,pp. 43œ45.alexander, michael. 1989b. "business foots hackers' bill," computerworld , december 11.alexander, michael. 1989c. "trojan horse sneaks in with aids program," computerworld,december 18, p. 4.alexander, michael. 1990a. "biometric system use wideningšsecurity devices measure physicalbased traits to restrict access to sensitive areas," computerworld, january 8, p. 16.alexander, michael. 1990b. "hightech boom opens security gaps," computerworld, april 2, pp. 1,119.allen, michael. 1990. "identity crisis: to repair bad credit, advisers give clients someone else'sdata," wall street journal, august 14, p. al.allentonar, larry. 1989. "networked computers attract security problems abuse," networkingmanagement, december, p. 48.american bar association. 1984. report on computer crime, task force on computer crime,section on criminal justice, chicago, ill., june.american institute of certified public accountants (aicpa). 1984. report on the study of edprelated fraud in the banking and insurance industries, edp fraud review task force,aicpa, new york.anderson, j. p. 1972. computer security technology planning study, esdtr7351, vol. i,ad758 206, esd/afsc, hanscom afb, bedford, mass., october.anderson, j. p. 1980. computer security threat monitoring and surveillance , james p. andersonco., fort washington, pa., april.bibliography216computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.anthes, gary, h. 1989a. "acc tunes in to illicit hacking activityšfirm ferrets out threats," federalcomputer week, september 18, pp. 1, 53.anthes gary, h. 1989b. "u.s. software experts track british standards," federal computer week,september 18, pp. 3, 8.anthes, gary h. 1989c. "darpa response team spawns private spinoffs," federal computer week,december 11.anthes, gary h. 1989d. "vendors skirt ncsc evaluations: security system testing faulted forlength and cost in process," federal computer week, december 11, p. 4.anthes, gary h. 1990a. "nist combats confusion on encryption standard," federal computerweek, january 29, p. 7.anthes, gary h. 1990b. "oracle, af to build secure data base system: project will build operationalrelational dbms to meet al trust," federal computer week, march 12.armed forces communications and electronics association (afcea). 1989. information securitystudy, fairfax, va., april.bailey, david. 1984. "attacks on computers: congressional hearings and pending legislation,"proceedings of the 1984 ieee symposium on security and privacy, ieee computersociety, oakland, calif., april 29œmay 2, pp. 180œ186.baldwin, robert w. 1988. rule based analysis of computer security, technical report 401,massachusetts institute of technology, laboratory for computer science, cambridge,mass., march.beatson, jim. 1989. "is america ready to 'fly by wire'?" washington post, april 2, p. c3.becker, l. g. 1987. an assessment of resource centers and future requirements for informationsecurity technology, prepared for the national security agency, fort meade, md.,september.bell, elliott d. 1983. "secure computer systems: a retrospective," proceedings of the 1983 ieeesymposium on security and privacy, ieee computer society, oakland, calif., april 25œ27, pp. 161œ162.bell, elliot d. 1988. "concerning modeling of computer security," proceedings of the 1988 ieeesymposium on security and privacy, ieee computer society, oakland, calif., april 18œ21, pp. 8œ13.bell, elliott d. and l. j. la padula. 1976. secure computer system: unified exposition and multicsinterpretation, esdtr75306, mitre corp., bedford, mass., march.beresford, dennis r., et al. 1988. "what is the fasb's role, and how well is it performing?"financial executive, september/october, pp. 20œ26.berman, jerry and janlori goldman. 1989. a federal right of information privacy: the need forreform, american civil liberties union/computer professionals for social responsibility,washington, d.c.berton, lee. 1989. "audit firms are hit by more investor suits for not finding fraud," the wall streetjournal, january 24, pp. a1, a12.betts, mitch. 1989. "senate takes tentative look at virus legislation," computerworld, may 22.biba, k. j. 1975. integrity considerations for secure computer systems , report mtr 3153,mitre corp., bedford, mass., june.birrell, andrew d., b. w. lampson, r. m. needham, and m. d. schroeder. 1986. "a globalauthentication service without global trust," proceedings of the 1986 ieee symposium onsecurity and privacy, ieee computer society, oakland, calif., april 7œ9, pp. 223œ230.bloombecker, jay, esq. (ed). 1988. introduction to computer crime, 2nd ed., national center forcomputer crime data, los angeles, calif.bloomfield, r. e. 1990. safeit: the safety of programmable electronic systems, a governmentconsultation document on activities to promote the safety of computer controlledbibliography217computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.systems, volume 1: overall approach and volume 2: a framework for safety standards,icse secretariat, department of trade and industry, london, united kingdom, june.boebert, e. 1985. "a practical alternative to hierarchical integrity policies," proceedings of the 8thnational computer security conference , september 30, nist, gaithersburg, md.boebert, w. e., r. y. kain, w. d. young, and s. a. hansohn. 1985. "secure ada target: issues,system design, and verification," proceedings of the 1985 ieee symposium on securityand privacy, ieee computer society, oakland, calif., april 22œ24, pp. 176œ183.boss, a. h. and w. j. woodward. 1988. "scope of the uniform commercial code; survey ofcomputer contracting cases," the business lawyer 43, august, pp. 1513œ1554.bozman, jean s. 1989. "runaway program gores sabre," computerworld , may 22.brand, russell l. 1989. coping with the threat of computer security incidents: a primer fromprevention through recovery, july. available from the defense advanced researchprojects agency, arlington, va., or at the following address: 1862 euclid, department136, berkeley, ca 94709.branstad, d. 1973. "security aspects of computer networks," proceedings of the aiaa computernetwork systems conference, paper 73œ427, huntsville, ala., april, american institute ofaeronautics and astronautics (aiaa), washington, d.c.branstad, dennis k. and miles e. smid. 1982. "integrity and security standard based oncryptography," computers & security, vol. 1, pp. 225œ260.brewer, d. f. c. 1985. software integrity: (verification, validation, and certification), admiralcomputing limited, camberley, surrey, england, january, pp. 111œ124.brown, bob. 1989a. "security risks boost encryption outlays," network world, january 9, pp. 11œ12.brown, bob. 1989b. "co fire, virus attack raise awareness, not preparation," network world, july 3,p. 1.browne, malcolm w. 1988. "most ferocious math problem is tamed," new york times, october 12,p. a1.buckley, t. f. and j. w. wise. 1989. "tutorial: a guide to the viper microprocessor,"proceedings: compass '89 (computer assurance), ieee computer society, new york,june 23.burgess, john. 1989. "computer virus sparks a user scare," washington post, september 17, p. h3.burgess, john. 1990. "hacker's case may shape computer security law," washington post, january9, p. a4.burrows, m., m. abadi, and r. needham. 1989. a logic of authentication , digital systemsresearch center, palo alto, calif., february.business week. 1988. "is your computer secure," (cover story), august 1, pp. 64œ72.california, state of. 1985. informational hearing: computers and warranty protection forconsumers, sacramento, calif., october.canadian government, system security centre, communications security establishment. 1989.canadian trusted computer product evaluation criteria, version 1.0, draft, ottawa,canada, may.carnevale, mary lu and julie amparano lopez. 1989. "making a phone call might mean telling theworld about you," wall street journal, november 28, pp. a1, a8.casatelli, christine. 1989a. "smart signatures at fed," federal computer week, may 22.casatelli, christine. 1989b. "disaster recovery," federal computer week, december 11, pp. 28œ29,33.casey, peter. 1980. "proposals to curb computer misuse," jfit news , no. 8, november, p. 2.bibliography218computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.chalmers, leslie s. 1986. "an analysis of the differences between the computer security practices inthe military and private sectors," proceedings of the 1986 ieee symposium on securityand privacy, ieee computer society, oakland, calif., april 7œ9, pp. 71œ74.chandler, james p. 1977. "computer transactions: potential liability of computer users andvendors," washington university law quarterly , vol. 1977, no. 3, pp. 405œ443.chaum, david (ed.). 1983. advances in cryptology: proceedings of crypto 83, plenum, new york.chor, benzion. 1986. two issues in publickey cryptography: rsa bit security and a newknapsack type system, mit press, cambridge, mass.christian science monitor. 1989. "computer and spy: worrisome mix," march 7, p. 4.chronicle of higher education. 1988a. "virus' destroys campus computer data," february 3.chronicle of higher education. 1988b. "worries over computer 'viruses' lead campuses to issueguidelines," march 2.clark, d. d. and d. r. wilson. 1987. "a comparison of commercial and military computer securitypolicies," proceedings of the 1987 ieee symposium on security and privacy, ieeecomputer society, oakland, calif., april 27œ29, pp. 184œ194.cohen, fred. 1984. "computer viruses: theory and experiments," seventh dod/nbs conferenceon computer security, gaithersburg, md.cole, patrick and johathan b. levine. 1989. "are atms easy targets for crooks?" business week,march 6, p. 30.comer, douglas. 1988. internetworking with tcp/ip principles, protocols, and architectures,prenticehall, englewood cliffs, n.j.communications week. 1990a. "hack it through packet," april 16, p. 10.communications week. 1990b. "what's in the mail?" editorial, july 16, p. 20.computer and business equipment manufacturers association (cbema). 1989a. statement to u.s.congress (101st), senate, subcommittee on technology and the law, hearing oncomputer viruses, may 19.computer and business equipment manufacturers association (cbema). 1989b. statement to u.s.congress (101st), house of representatives, committee on the judiciary, subcommitteeon criminal justice, hearing on computer virus legislation, november 8.computer crime law reporter. 1989. "computer crime statutes at the state level," august 21update based on the "statenet" database and compiled and distributed by the nationalcenter for computer crime data, 2700 n. cahuenga blvd., los angeles, ca 90068.computer fraud & security bulletin. 1989œ1990. elsevier science publishing co., oxford, unitedkingdom.computer law associates annual meeting. 1978. unpublished proceedings: brooks, daniel j.,"natures of liabilities of software program suppliers"; derensis, paul r., "impact ofcomputer systems on the liabilities of various types of professionals"; hutcheon, peter d.,"computer system as means for avoidance of liability''; jenkins, martha m., "effects ofcomputersystem records on liabilities of suppliers, users, and others"; freed, roy n.,"how to handle exposures to, and impacts of, liability arising from computer use."washington, d.c., computer law association, fairfax, va., march 6.computer security journal. 1986œ1988. computer security institute, 500 howard street, sanfrancisco, ca 94105.computers & security. 1988. "special supplement: computer viruses," vol. 7, no. 2, elsevieradvanced technology publications, oxford, united kingdom, april.computers & security. 1988œ1990. elsevier advanced technology publications, oxford, unitedkingdom.computerworld. 1988a. "osi security system revealed," october 5, pp. 53, 58.bibliography219computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.computerworld. 1988b. "virus ravages thousands of systems," november 7, pp. 1, 157.conly, catherine h. 1989. organizing for computer crime investigation and prosecution, u.s.department of justice, national institute of justice, washington, d.c., july.consultative committee on international telephony and telegraphy (ccitt). 1989a. datacommunication networks message handling systems , vol. viii, fascicle viii.7,recommendations x.400x.420, ccitt, geneva, p. 272.consultative committee on international telephony and telegraphy (ccitt). 1989b. datacommunications networks directory, vol. viii, fascicle viii.8, recommendations x.500x.521, ccitt, geneva.cook, william j. 1989. "access to the access codes '88œ'89: a prosecutor's perspective,"proceedings of the 12th national computer security conference , national institute ofstandards and technology/national computer security center, baltimore, md., october10œ13.cooper, james arlin. 1989. computer & communications securitystrategies for the 1990s, mcgrawhill communications series, mcgrawhill, new york.cornell university. 1989. the computer worm. a report to the provost from the commission ofpreliminary enquiry, ithaca, n.y., february 6.cowan, alison leigh. 1990. "the $290,000 job nobody wants," new york times, october 11, d1,d9.craigen, d. and k. summerskill (eds.). 1990. formal methods for trustworthy computer systems(fm '89), a workshop on the assessment of formal methods for trustworthy computersystems, springerverlag, new york.crawford, diane. 1989. "two bills equal forewarning," communications of the acm, vol. 32, no.7, july.crenshaw, albert b. 1990. "senate panel approves liability bill," washington post, may 23.cullyer, w. 1989. "implementing high integrity systems: the viper microprocessor," ieee aesmagazine, may 13.curry, david a. 1990. improving the security of your unix system, itstd721fr9021,information and telecommunications sciences and technology division, sriinternational, menlo park, calif., april.cutler, ken and fred jones. 1990. "commercial international security requirements," unpublisheddraft paper, american express travel related services company, inc., phoenix, ariz.,august 3.danca, richard a. 1989. "lan group helps managers handle security risks," federal computerweek, july 10.danca, richard a. 1990a. "sybase unveils multilevel secure dbms," network world, february 19,pp. 1, 37.danca, richard a. 1990b. "ncsc decimated, security role weakened," federal computer week,july 16, pp. 1, 6.danca, richard a. 1990c. "bush revises nsdd 145," federal computer week, july 16, pp. 6, 41.danca, richard a. 1990d. "ncsc affirms shakeup in its structure," federal computer week,august 27, pp. 1, 4.danca, richard a. 1990e. "nist may issue civilian computer security guide: proposed documentcould become federal information processing standard," federal computer week,september 17, p. 60.danca, richard a. 1990f. "nist, industry team up for antivirus consortium," federal computerweek, october 8, p. 2.danca, richard a. 1990g. "torricelli charges nist with footdragging on security," federalcomputer week, october 8, p. 9.datamation. 1987. "disaster recovery: who's worried?" february 1, pp. 60œ64.bibliography220computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.datapro research. 1989a. "all about data encryption devices," datapro reports: informationsecurity, report no. is37001, mcgrawhill, delran, n.j., pp. 101œ109.datapro research. 1989b. "all about microcomputer encryption and access control," dataproreports: information security, report no. is31001, mcgrawhill, delran, n.j., pp. 101œ108.datapro research. 1989c. security issues of 1988: a retrospective, mcgrawhill, delran, n.j.,march.datapro research. 1990a. "host access control software: market overview," datapro reports:information security, report no. is52001, mcgrawhill, delran, n.j., pp. 101œ104.datapro research. 1990b. "bull security capabilities of multics," datapro reports: informationsecurity, report no. is56115, mcgrawhill, delran, n.j., pp. 101œ106.daunt, robert t. 1985. "warranties and mass distributed software," computers and hightechnology law journal, vol. 1, pp. 255œ307.davies, d. and w. price. 1984. security for computer networks: an introduction to data securityin teleprocessing and electronic funds transfers, wiley, new york.davis, bob. 1988. "a supersecret agency finds selling secrecy to others isn't easy," wall streetjournal, march 28, p. a1.davis, bob. 1989. "nasa discloses computer virus infected network," wall street journal, october18, p. b4.davis, g. gervaise, iii. 1985. software protection: practical and legal steps to protect and marketcomputer programs, van nostrand reinhold, new york.davis, otto a. and morton i. kamien. 1969. "externalities, information, and alternative collectiveaction," the analysis and evaluation of public expenditures: the ppb system,compendium of papers submitted to the subcommittee on economy in government of thejoint economic committee of the u.s. congress, washington, d.c., u.s. gpo, pp. 67œ86.davis, ruth m. 1989. "cals data protectionšcomputeraided acquisition and logistic support,data protection and security policy statement," the pymatuning group, arlington, va.,january.defense communications agency (dca). 1989. "ddn security coordination center operational,"defense data network security bulletin, ddn security coordination center, dca ddndefense communications system, september 22.denning, d. e. 1987. "an intrusiondetection model," proceedings of the 1986 symposium onsecurity and privacy, national bureau of standards, gaithersburg, md., september.denning, d. e., t. f. lunt, r. r. schell, w. r. shockley, and m. heckman. 1988. "the seaviewsecurity model," proceedings of the 1988 ieee symposium on security and privacy, ieeecomputer society, oakland, calif., april 18œ21, pp. 218œ233.denning, dorothy. 1976. "a lattice model of secure information flow," communications of theacm, vol. 19.denning, dorothy e., peter g. neumann, and donn b. parker. 1987. "social aspects of computersecurity," proceedings of the 10th national computer security conference, nationalbureau of standards/national computer security center, baltimore, md., september 21œ24, pp. 320œ325.dewdney, a. k. 1989. "of worms, viruses, and core war," scientific american, march, pp. 110œ113.dickman, steven. 1989. "hackers revealed as spies," nature, march 9, p. 108.didio, laura. 1989. "rash of viruses puts spotlight on security," network world, october 30, p. 19.didio, laura. 1990. "virus threat obscured by slow growth in early stages," network world, april23, p. 23.diffie, w. and m. hellman. 1976. "new directions in cryptography," ieee transactions oninformation theory, it22, november 16, pp. 644œ654.bibliography221computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.dillon, laura k. 1989. research on validation of concurrent and realtime software systems",university of california, santa barbara.dobson, j. e. and b. randell. 1986. "building reliable secure computing systems out of unreliableinsecure components," proceedings of the 1986 ieee symposium on security and privacy,ieee computer society, oakland, calif., april 7œ9, pp. 187œ193.early, peter. 1988. family of spies: inside the john walker spy ring , bantam books, new york.eason, tom s., susan higley russell, and brian ruder. 1977. systems auditability and controlstudy: data processing control practices report, vol. 1 of 3 volumes, institute ofinternal auditors, altamonte springs, fla.economist. 1988. "keeping out the kaos club," science and technology section, july 9, pp. 77œ78.electronic industries association (eia). 1987. proceedings: communications & computer security(comsec & compusec): requirements, opportunities and issues, eia, washington,d.c., january 14.emergency care research institute (ecri). 1985. "unauthorized use of computers: an oftenneglected security problem," issues in health care technology, ecri, plymouth meeting,pa., july, pp. 1œ6.emergency care research institute (ecri). 1988a. "legal implications of computerized patientcare," health technology, vol. 2, no. 3, may/june, pp. 86œ95, ecri, plymouth meeting,pa.emergency care research institute (ecri). 1988b. an election administrator's guide tocomputerized voting systems, vol. 1 and 2, ecri, plymouth meeting, pa.ernst & young. 1989. computer security survey: a report, cleveland, ohio.estrin, d. and g. tsudik. 1987. "visa scheme for interorganization network security,"proceedings of the 1987 ieee symposium on security and privacy, ieee computersociety, oakland, calif., april 27œ29, pp. 174œ183.european commission. 1989a. basis for a portable common tool environment (pcte), espritproject number 32, esprit, the project synopses, information processing systems, vol. 3of a series of 8, september.european commission. 1989b. basis for a portable common tool environment (pcte), espritproject number 32, basic research actions and working groups, vol. 8 of a series of 8,september.european computer manufacturers association (ecma). 1989. standard ecmaxxx security inopen systems: data elements and service definitions , ecma, geneva.falk, david. 1975. "building codes in a nutshell," real estate review , vol. 5, no. 3, fall, pp. 82œ91.federal computer week. 1988. "analysis, task forces work to keep internet safe," november 14,pp. 1, 49.federal computer week. 1989. "selling viruses," november 27, p. 25.federal republic of germany, ministry of interior. 1990. information technology securityevaluation criteria (itsec), the harmonized criteria of france, germany, thenetherlands, and the united kingdom, draft version 1, may 2, bonn, federal republic ofgermany.federal trade commission (ftc). 1983. standards and certification, final staff report, bureau ofconsumer protection, washington, d.c., april.fetzer, james h. 1988. "program verification: the very idea," communications of the acm, vol.31, no. 9, september, pp. 1048œ1063.financial accounting foundation (faf) (n.d.). "establishing standards for financial reporting,"fasb, norwalk, conn. [undated pamphlet]financial accounting foundation (faf). 1990. financial accounting foundation annual report1989, faf, norwalk, conn.bibliography222computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.financial accounting standards board (fasb). 1990. "facts about fasb," fasb, norwalk, conn.fitzgerald, karen. 1989. "the quest for intruderproof computer systems," ieee spectrum, august,pp. 22œ26.flaherty, david. 1990. protecting privacy in surveillance societies , the university of northcarolina press, chapel hill.florida state legislature. 1984. overview of computer security, a report of the joint committee oninformation technology resources, jacksonville, fla., january.forcht, karen a. 1985. "computer security: the growing need for concern," the journal ofcomputer information systems, fall.francett, barbara. 1989. "can you loosen the bolts without disarming the locks?" (executivereport: security in open times), computerworld , october 23.frenkel, karen a. 1990. "the politics of standards and the ec," communications of the acm, vol.33, no. 7, pp. 41œ51.galen, michele and jeffrey rothfeder. 1989. "is nothing private?" business week, september 4, pp.74œ77, 80œ82.gasser, morrie. 1988. building a secure computer system, van nostrand reinhold, new york.gasser, morrie, a. goldstein, c. kaufman, and b. lampson. 1989. "the digital distributed systemsecurity architecture," proceedings of the 12th national computer security conference,national institute of standards and technology /national computer security center,baltimore, md., october 10œ13, pp. 305œ319.gemignani, michael c. 1982. "product liability and software," rutgers journal of computers,technology and law, vol. 8, p. 173.general accounting office. 1980. increasing use of data telecommunications calls for strongerprotection and improved economies, washington, d.c.general accounting office (gao). 1987. space operations: nasa's use of informationtechnology, gao/imtec8720, washington, d.c., april.general accounting office (gao). 1988a. information systems: agencies overlook securitycontrols during development, gao/imtec8811, washington, d.c., may.general accounting office (gao). 1988b. information systems: agencies overlook securitycontrols during development, gao/imtec8811s, washington, d.c., may.general accounting office (gao). 1988c. satellite data archiving: u.s. and foreign activitiesand plans for environmental information , gao/rced88201, washington, d.c.,september.general accounting office (gao). 1989a. federal adp personnel: recruitment and retention,gao/imtec8912br, washington, d.c., february.general accounting office (gao). 1989b. electronic funds: information on three criticalbanking systems, washington, d.c., february.general accounting office (gao). 1989c. computer security: compliance with trainingrequirements of the computer security act of 1987, gao/imtec8916br, washington,d.c., february.general accounting office (gao). 1989d. computer security: virus highlights need for improvedinternet management, gao/imtec8957, washington, d.c., june.general accounting office (gao). 1989e. computer security: unauthorized access to a nasascientific network, gao/imtec902, washington, d.c., november.general accounting office (gao). 1990a. electronic funds transfer: oversight of criticalbanking systems should be strengthened, washington, d.c., january.general accounting office (gao). 1990b. financial markets: tighter computer security needed,gao/imtec9015, washington, d.c., january.general accounting office (gao). 1990c. computer security: government planning process hadlimited impact, gao/imtec9048, washington, d.c., may.bibliography223computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.general accounting office (gao). 1990d. justice automation: tighter computer security needed,gao/imtec9069, washington, d.c., july.general accounting office (gao). 1990e. computers and privacy: how the government obtains,verifies, uses, and protects personal data , gao/imtec9070br, washington, d.c.,august.general services administration (gsa). 1988. information technology installation security, officeof technical assistance, federal systems integration and management center, fallschurch, va., december.german information security agency (gisa). 1989. it security criteria: criteria for theevaluation of trustworthiness of information technology (it) systems, 1st version, koln,federal republic of germany.gilbert, dennis m. and bruce k. rosen. 1989. computer security issues in the application of newand emerging information technologies, a white paper, national institute of standardsand technology, gaithersburg, md., march.godes, james n. 1987. "developing a new set of liability rules for a new generation of technology:assessing liability for computerrelated injuries in the health care field," computer lawjournal, vol. vii, pp. 517œ534.government computer news. 1986. "dp courses don't include ethics study," july 4.government computer news. 1988. "gcn spotlight: security," april 29, pp. 35œ54.gray, j. 1987. "why do computers stop and what can we do about it?" 6th international conferenceon reliability and distributed databases , ieee computer society, engineering societieslibrary, new york.green, virginia d. 1989a. "overview of federal statutes pertaining to computerrelated crime,"(memorandum), reed, smith, shaw, and mcclay, washington, d.c., july 7.green, virginia d. 1989b. "state computer crime statutes and the use of traditional doctrines toprosecute the computer criminal," (memorandum), reed, smith, shaw, and mcclay,washington, d.c., july 7.greenberg, ross m. 1988. "a form of protection for you and your computer," 2600 magazine,summer.greenhouse, steven. 1990. "india crash revives french dispute over safety of airbus jet," new yorktimes, february 24.gregg, robert e. and thomas r. folk. 1986. "liability for substantive errors in computer software,"computer law reporter (washington d.c.), vol. 5, no. 1, july, pp. 18œ26.grimm, vanessa jo. 1989. "hill halves nist budget for security," government computer news,vol. 8, no. 22, october 30.gruman, galen. 1989a. "software safety focus of new british standard," ieee software, may.gruman, galen. 1989b. "major changes in federal software policy urged," ieee software,november, pp. 78œ80.haigh, j., r. a. kemmerer, j. mchugh, and b. young. 1987. "an experience using two covertchannel analysis techniques on a real system design," ieee transactions on softwareengineering, vol. se13, no. 2, february.hamlet, richard. 1988. "special section on software testing," communications of the acm, vol. 31,no. 6, june.hanna, keith, neil daeche, and mark longley. 1989. veritas+: a specification language basedon type theory, technical report, faculty of information technology, university ofkent, canterbury, united kingdom, may.harrison, warren. 1988. "using software metrics to allocate testing resources," journal ofmanagement systems, vol. 4, spring.helfant, robert and glenn j. mcloughlin. 1988. computer viruses: technical overview and policyconsiderations, science policy research division, congressional research service,washington, d.c., august 15.bibliography224computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.hellman, m. 1979. "the mathematics of publickey cryptography," scientific american, 241(2):146œ157.henderson, nell. 1989. "programming flaw, keyboard cited in airline delays twice in 2 weeks,"washington post, november 18, p. b4.higgins, john c. 1989. "information security as a topic in undergraduate education of computerscientists," proceedings of the 12th national computer security conference, nationalinstitute of standards and technology/national computer security center, baltimore,md., october 10œ13.hilts, philip j. 1988. "computers face epidemic of 'information diseases,'" washington post, may 8,p. a3.hoffman, lance j. 1988. making every vote count: security and reliability of computerized votecounting systems, george washington university, school of engineering and appliedscience, department of electrical engineering and computer science, washington d.c.,march.hollinger, richard c. and lonn lanzakaduce. 1988. "the process of criminalization: the case ofcomputer crime laws," criminology, vol. 26, no. 1.holmes, james p., r. l. maxwell, and l. j. wright. 1990. a performance evaluation of biometricidentification devices, sandia national laboratories, albuquerque, n. mex., july.honeywell, secure computing technology center. 1985œ1988. lock: selected papers,honeywell, st. anthony, minn.horning, james j., p. g. neumann, d. d. redell, j. goldman, and d. r. gordon. 1989. a review ofncic 2000: the proposed design for the national crime information center, americancivil liberties union, project on privacy and technology, washington, d.c., february.horovitz, bonna lynn. 1985. "computer software as a good under the uniform commercial code:taking a byte out of the intangibility myth," boston university law review, vol. 65, pp.129œ164.houston, m. frank. 1987. "what do the simple folks do? software safety in the cottage industry,"food and drug administration, center for devices and radiological health, rockville,md., pp. s/20s/24.houston, m. frank. 1989. designing safer, more reliable software systems, food and drugadministration, center for devices and radiological health, rockville, md.howden, william e. 1987. functional program testing and analysis, mcgraw hill, new york.independent european programme group (iepg), technical area 13 (ta13). 1989. "introducingpcte+," (april); and "rationale for the changes between the pcte+ specifications issue3 dated 28 october 1988 and the pcte specifications version 1.5 dated 15 november1988," (january 6), iepg, eurogroup of nato, brussels.information systems security association. 1988œ1990. issa access, newport beach, calif.info world. 1988. "what were simple viruses may fast become a plague," tech talk, may 2.institute for defense analyses (ida). 1987. ida memorandum reports: introduction to informationprotection (m379), operating systems security (m380), network security (m381),database system security (m382), formal specification and verification (m383), andrisk analysis (m384), ida, alexandria, va., october.institute of electrical and electronics engineers (ieee). 1984. ieee guide to softwarerequirements specifications, ansi/ieee std. 8301984, ieee, new york.institute of electrical and electronics engineers (ieee). 1988. proceedings: compass '88(computer assurance), june 27july 1, ieee, new york.institute of electrical and electronics engineers (ieee). 1988œ1990. proceedings of the computersecurity foundations workshop, franconia, n.h., ieee, new york.bibliography225computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.institute of electrical and electronics engineers (ieee). 1989a. proceedings: compass '89(computer assurance), june, ieee, new york.institute of electrical and electronics engineers (ieee). 1989b. cipher, newsletter of the technicalcommittee on security & privacy, ieee computer society, washington, d.c.institute of electrical and electronics engineers (ieee). 1990a. cipher, newsletter of the technicalcommittee on security & privacy, special issue, "minutes of the first workshop oncovert channels analysis," ieee computer society, washington, d.c.institute of electrical and electronics engineers (ieee). 1990b. ieee software (issue on formalmethods in software engineering), september.institute of electrical and electronics engineers (ieee). 1990c. ieee transactions on softwareengineering (issue on formal methods in software engineering), september.international standards organization (iso). 1989. "security architecture," part 2 of 4, informationprocessing systems open system interconnection basic reference model, iso74982,available from the american national standards institute, new york.jackson, kelly. 1989a. "plans grounded by faa computer glitches," federal computer week,november 20, p. 20.jackson, kelly. 1989b. "congress pushes computer crime law," federal computer week, november20, p. 23.jacobs, jane. 1972. the death and life of great american cities, penguin, harmondsworth, unitedkingdom.jaffe, matthew s. and nancy g. leveson. 1989. completeness, robustness, and safety in realtimesoftware requirements specification, technical report 8901, information and computerscience, university of california, irvine, february.japanese ministry of international trade and industry (miti). 1989. the present state andproblems of computer virus, agency of industrial science and technology, informationtechnology promotion agency , tokyo.johnson, david r. and david post. 1989. computer viruses, a white paper on the legal and policyissues facing colleges and universities, american council on education and wilmer,cutler & pickering, washington, d.c.johnson, william. 1989. "information espionage: an old problem with a new face," (executivereport: security in open times), computerworld , october 23.joseph, mark k. and algirdas avizienis. 1988. "a fault tolerance approach to computer viruses,"computer, ieee, may.juitt, david. 1989. "security assurance through system management," proceedings of the 12thnational computer security conference, national institute of standards and technology/national computer security center, baltimore, md., october 10œ13.kahn, david. 1967. the codebreakers: the story of secret writing, macmillan, new york.karger, p. 1988. "implementing commercial data integrity with secure capabilities," proceedings ofthe 1988 ieee symposium on security and privacy, ieee computer society, oakland,calif., april 18œ21, pp. 130œ139.karon, paul. 1988. "the hype behind computer viruses: their bark may be worse than their 'byte,'"pc week, may 31, p. 49.kass, elliot m. 1990. "data insecurity," information week, march 19, p. 22.keller, john j. 1990. "software glitch at at&t cuts off phone service for millions," wall streetjournal, january 16, p. b1.kemmerer, r. a. 1985. "testing formal specifications to detect design errors," ieee transactionson software engineering, se11(1), pp. 32œ43.kemmerer, r. a. 1986. verification assessment study final report, volume i, overview,conclusions, and future directions, library no. s228,204, national computer securitycenter, fort meade, md., march 27.bibliography226computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.kemmerer, r. a. 1989a. "analyzing encryption protocols using formal verification techniques,"ieee journal on selected areas in communications,vol. 7, no. 4., pp. 448œ457.kemmerer, r. a. 1989b. "integration of formal methods into the development process," ieeesoftware, september, pp. 37œ50.kent, stephen t. 1976. "encryptionbased protection protocols for interactive usercomputercommunication," technical report 162 (mitlcs tr162), laboratory for computerscience, massachusetts institute of technology , cambridge, mass., may.kent, stephen t. 1981. protecting externally supplied software in small computers, technicalreport 255, laboratory for computer science, massachusetts institute of technology,cambridge, mass.kent, stephen t., p. sevcik, and j. herman. 1982. "personal authentication system for accesscontrol to the defense data network," eascon '82š15th annual electronics andaerospace systems conference, 82ch182833, ieee washington section and ieeeaerospace and electronics systems society, washington, d.c., september 20œ22.king, julia. 1989. "executive tech briefing: network security," federal computer week, july 10,pp. 28œ35.kolkhorst, b. g. and a. j. macina. 1988. "developing errorfree software," ieee aes magazine,november.labaton, stephen. 1989. "rules weighed on transfer of big sums electronically," new york times,october 31, pp. d1, d8.lamport, leslie. 1989. "a simple approach to specifying concurrent systems," communications ofthe acm, vol. 32, no. 1, january, pp. 32œ45.lampson, butler. 1973. "a note on the confinement problem," communications of the acm , vol.16, no. 10, october, pp. 613œ615.lampson, butler. 1985. "protection," acm operating systems review, vol. 19, no. 5, december,pp. 13œ24.landry, john. 1990. statement of adapso, a computer software and services industry association,before the senate judiciary subcommittee on technology and the law, july 31.lardner, jr., george. 1990a. "cia director: e. european spies at work," washington post, february21, p. a15.lardner, jr., george. 1990b. "national security agency: turning on and tuning in," (twopartarticle), washington post, march 18œ19, p. a1.law commission. 1989. criminal law, computer misuse, hmso, london, united kingdom,october.leveson, nancy g. 1986. "software safety: why, what, and how," computer surveys, vol. 18, no.2, june, pp. 125œ164.lewis, peter h. 1989. "building a moat with software," the new york times, september 3, p. f7.lewis, peter h. 1990. "privacy: the tip of the iceberg," new york times, october 2, p. c8.lewyn, mark. 1989. "hackers: is a cure worse than the disease?" business week, december 4, p. 37.lindsay, peter. 1988. "survey of theorem provers," software engineering journal, ieee, january.linger, r. c. and h. d. mills. 1988. "a case study in cleanroom software engineering: the ibmcobol structuring facility," proceedings of compsac '88, ieee computer society,washington, d.c.linn, john. 1989. "privacy enhancement for internet electronic mail," (memorandumšemail),request for comments 1113, network working group, iab privacy task force, july 17.bibliography227computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.linowes, david f. 1989. privacy in americašis your private life in the public eye? university ofillinois press, urbana and chicago.lipner, s. b. 1982. "nondiscretionary controls for commercial applications," proceedings of the1982 ieee symposium on security and privacy, ieee computer society, oakland, calif.,april 26œ28, pp. 2œ10.lipton, r. j. 1989. a new approach to testing, princeton university, princeton, n.j.loew, sue j. 1989. "encrypted edi: scrambling to create a security productšsans standard," datacommunications, october, p. 50.luckham, david and sriram sankar. 1989. future directions in software analysis and testing,stanford university, stanford, calif.lunt, t. f. 1988. "automated audit trail analysis and intrusion detection: a survey," proceedings ofthe 11th national computer security conference , national institute of standards andtechnology/national computer security center, baltimore, md.lunt, t. f., r. r. schell, w. r. shockley, m. heckman, and d. warren. 1988. "a nearterm designfor the sea view multilevel database system," proceedings of the 1988 ieee symposiumon security and privacy, ieee computer society, oakland, calif., april, pp. 234œ244.lunt, teresa f. 1989. "aggregation and inference: facts and fallacies," proceedings of the 1989ieee symposium on security and privacy, ieee computer society, oakland, calif., may1œ3, pp. 102œ109.lyons, john. 1990. testimony before the subcommittee on transportation, aviation, and materials,u.s. house of representatives, national institute of standards and technology,gaithersburg, md.markoff, john. 1988a. "west german secretly gains access to u.s. military computers," new yorktimes, april 17.markoff, john. 1988b. "breach reported in u.s. computers," new york times, april 18, p. a1.markoff, john. 1989a. "virus outbreaks thwart computer experts," new york times, may 30.markoff, john. 1989b. "paper on codes sent to 8,000 computers over u.s. objection," new yorktimes, august 9, a1.markoff, john. 1989c. "computer virus cure may be worse than disease," new york times, october7, pp. a1, a35.markoff, john. 1990a. "breakdown's lesson: failure occurs on superhuman scale," new york times,january 16, p. a24.markoff, john. 1990b. "caller says he broke into u.s. computers to taunt the experts," new yorktimes, march 21, pp. a1, a21.markoff, john. 1990c. "arrests in computer breakins show a global peril," new york times, april4, pp. a1, a16.markoff, john. 1990d. "washington is relaxing its stand on guarding computer security," new yorktimes, august 18, pp. 1, 20.mcilroy, m. 1989. "virology 101," computing systems (usenix association, berkeley, calif.),vol. 2, no. 2, pp. 173œ181.mcloughlin, glenn j. 1987. computer crime and security, science policy research division,congressional research service, washington, d.c., january 3.meyer, c. and s. matyas. 1983. cryptography: a new dimension in computer data security,wiley, new york.microelectronics and computer technology corporation (mcc). 1989. spectra: a formalmethods environment, mcc technical report no. actilostp32489, mcc, austin,tex.millen, jonathan k. 1987. "covert channel capacity," proceedings of the 1987 ieee symposium onsecurity and privacy, ieee computer society, oakland, calif., april 27œ29, pp. 60œ66.bibliography228computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.millen, j. k., s. c. clark, and s. b. freedman. 1987. ''the interrogator: protocol security analysis,"ieee transactions on software engineering , vol. se13, no. 2, february.miller, donald v. and robert w. baldwin. 1989. "access control by boolean expressionevaluation," proceedings of the computer security application conference, tucson, ariz.,december 8, ieee computer society, washington, d.c.miller, edward and w. e. howden. 1981. software testing and validation techniques, 2nd rev. ed.,ieee computer society, washington, d.c.miller, s. p., c. neuman, j. i. schiller, and j. h. saltzer. 1987. "kerberos authentication andauthorization system," project athena technical plan, section e.2.1, massachusettsinstitute of technology, cambridge, mass., july.mitchell, j. g., w. maybury, and r. sweet. 1979. mesa language manual (version 5.0), csl793,xerox palo alto research center, palo alto, calif., april.mitchell, william. 1990. "enterprise networks: the multivendor networks of the 1990s,"networking management, vol. 8, no. 2, february, pp. 69œ72.moates, jr., william h. and karen a. forcht. 1986. "computer security education: are businessschools lagging behind?" data management, march.moeller, robert r. 1989. computer audit, control and security, john wiley & sons, new york.morris, r. and k. thompson. 1979. "unix password security: a case history," communications ofthe acm, vol. 22, no. 11, november, pp. 594œ597.mossbert, walter s. and john walcott. 1988. "u.s. redefines policy on security to place less stresson soviets," wall street journal, august 11.mosso, david. 1987. "public policy and the fasb: as seen by one of its board members,"bottomline, december.munro, neil. 1990. "nsa plan may stymie improved computer security," defense news, september10, pp. 3, 36.musolf, lloyd. 1983. uncle sam's private, profitseeking corporations: comsat, fannie mae,amtrak, and conrail, lexington books, d.c. heath and company, lexington, mass.national academy of sciences. 1987. balancing the national interest: u.s. national securityexport controls and global economic competition , (also known as the allen report),committee on science, engineering, and public policy, national academy press,washington, d.c.national aeronautics and space administration (nasa). 1984. nasa adp risk analysisguideline, (prepared by edp audit controls, inc.), automated information systemsdivision: nasa headquarters, july.national aeronautics and space administration (nasa). 1989a. automated information systemssecurity plan, johnson space center, april.national aeronautics and space administration (nasa). 1989b. automated information systemssecurity plan executive summary, goddard space flight center, july.national aeronautics and space administration (nasa). 1989c. assuring the security and integrityof the gsfc automated information resources , issuance information sheet gmi2410.6b, goddard space flight center, may.national aeronautics and space administration (nasa). 1989d. assuring the security andintegrity of nasa automated information resources , nmi: 2410.7a, nasa managementinstruction, information resources management office, washington, d.c.national bureau of standards (nbs). 1977. data encryption standard , federal informationprocessing standards publication 46, nbs, gaithersburg, md., january. reissued asfederal information processing standards publication 461, january 1988.national bureau of standards (nbs). 1978. considerations in the selection of security measuresfor automatic data processing systems, nbs, gaithersburg, md., june.national bureau of standards (nbs). 1980a. guidelines on user authentication techniques forcomputer network access control, federal information processing standardsbibliography229computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.publication 83, national technical information service, springfield, va., september 29.national bureau of standards (nbs). 1980b. des modes of operation, federal informationprocessing standards publication 81, national technical information service, springfield,va., december.national bureau of standards (nbs). 1981a. guidelines for adp contingency planning, federalinformation processing standards publication 87, national technical information service,springfield, va., march 27.national bureau of standards (nbs). 1981b. guideline on integrity assurance and control indatabase administration, federal information processing standards publication 88,national technical information service, springfield, va., august 14.national bureau of standards (nbs). 1982. executive guide to adp contingency planning, stuartw. katzke and james w. shaw, nbs special publication 50085, nbs, washington, d.c.,january.national bureau of standards (nbs). 1983. guideline for computer security and certification andaccreditation, federal information processing standards publication 102, nationaltechnical information service, springfield, va., september 27.national bureau of standards (nbs). 1984. security of personal computer systems: a growingconcern, nbs, gaithersburg, md., april.national bureau of standards (nbs). 1985a. security of personal computer systems: amanagement guide, nbs special publication 500120, nbs, gaithersburg, md., january.national bureau of standards (nbs). 1985b. security for dialup lines , nbs special publication500137, nbs, gaithersburg, md., may.national bureau of standards (nbs). 1986. work priority scheme for edp audit and computersecurity review, nbs, gaithersburg, md., march.national bureau of standards (nbs). 1988. guide to auditing for controls and security: a systemdevelopment life cycle approach, nbs special publication 500153, nbs, gaithersburg,md., april.national bureau of standards/national computer security center (nbs/ncsc). 1987. proceedingsof the 10th national computer security conference , nbs/ncsc, baltimore, md.,september.national bureau of standards/national computer security center (nbs/ncsc). 1988. proceedingsof the 11th national computer security conference , nbs/ncsc, baltimore, md., october.national center for computer crime data (ncccd) and rgc associates. 1989. commitment tosecurity, ncccd, los angeles, calif.national institute of standards and technology (nist). 1988. smart card technology: newmethods for computer access control, nist special publication 500157, nist,gaithersburg, md.national institute of standards and technology (nist). 1989a. report of the invitational workshopon integrity policy in computer information systems (wipcis), nist special publication500160, nist, gaithersburg, md., january.national institute of standards and technology (nist). 1989b. computer viruses and relatedthreats: a management guide, nist special publication 500166, nist, gaithersburg,md., august.national institute of standards and technology (nist). 1989c. report of the invitational workshopon data integrity, nist special publication 500168, nist, gaithersburg, md., september.national institute of standards and technology (nist). 1990a. secure data network systems(sdns) network, transport, and message security protocols (nistir 904250), securedata network systems (sdns) access control documents (nistir 904259), sebibliography230computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.cure data network systems (sdns) key management documents (nistir 904262),nist, gaithersburg, md.national institute of standards and technology (nist). 1990b. "data encryption standard factsheet," nist, gaithersburg, md., january.national institute of standards and technology (nist). 1990c. computer security publications,nist publication list 91, nist, gaithersburg, md., march.national institute of standards and technology (nist). 1990d. security requirements forcryptographic modules, draft, federal information processing standards publication1401, national technical information service, springfield, va., july 13.national institute of standards and technology (nist). 1990e. guidelines and recommendationson integrity, draft, nist, gaithersburg, md., july 23.national institute of standards and technology/national computer security center (nist/ncsc).1989. proceedings of the 12th national computer security conference, nist/ncsc,baltimore, md., october.national institute of standards and technology/national computer security center (nist/ncsc).1990. analysis and comments on the draft information technology security evaluationcriteria (itsec), nist, gaithersburg, md., august 2.national institute of standards and technology/national security agency (nist/nsa). 1989.memorandum of understanding between directors concerning the implementation ofpublic law 100235, washington, d.c., march 24.national research council (nrc). 1983. multilevel data management security, air force studiesboard, national academy press, washington, d.c.national research council (nrc). 1984. methods for improving software quality and life cyclecost, air force studies board, national academy press, washington, d.c.national research council (nrc). 1988a. global trends in computer technology and theirimpact on export control, computer science and technology board, national academypress, washington, d.c.national research council (nrc). 1988b. toward a national research network, computerscience and technology board, national academy press, washington, d.c.national research council (nrc). 1988c. selected issues in space science data management andcomputation, space sciences board, national academy press, washington, d.c.national research council (nrc). 1989a. scaling up: a research agenda for softwareengineering, computer science and technology board, national academy press,washington, d.c.national research council (nrc). 1989b. growing vulnerability of the public switched networks:implications for national security emergency preparedness, board ontelecommunications and computer applications, national academy press, washington,d.c.national research council (nrc). 1989c. nasa space communications r&d: issues, derivedbenefits, and future directions, space applications board, national academy press,washington, d.c., february.national research council (nrc). 1989d. use of building codes in federal agency construction,building research board, national academy press, washington, d.c.national research council (nrc). 1990. keeping the u.s. computer industry competitive:defining the agenda, computer science and technology board, national academy press ,washington, d.c.national security agency (nsa). 1985. personal computer security considerations, ncscwa002š85, national computer security center, fort meade, md., december.national security agency (nsa). 1990a. "press statement: ncsc's restructuring," nsa, fortmeade, md., august.national security agency (nsa). 1990b. "evaluated products list for trusted computerbibliography231computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.systems," information security products and services catalogue, national computersecurity center, fort meade, md.national security agency/central security service (nsa/css). 1986. software acquisitionmanual, nsam 812, fort meade, md., may 15.national security agency/central security service (nsa/css). 1987. software product standardsmanual, nsam 813/dodstd1703(ns), fort meade, md., april 15.national technical information service (ntis). january 1988/october 1989. u.s. department ofcommerce, published search. citations from the computer database: computer virusesand computer software vaccines for software protection, ntis, washington, d.c.needham, r. and m. schroeder. 1978. "using encryption for authentication in large networks ofcomputers," communications of the acm, vol. 21, no. 12, december , pp. 993œ998.network world. 1990. "network security still slack," (art captioned "computer intelligence"),february 5, p. 33.neumann, peter g. 1986. "on hierarchical design of computer systems for critical applications,"ieee transactions on software engineering , vol. 12, no. 9, september, pp. 905œ920.neumann, peter g. 1988. "a glitch in our computer thinking: we create powerful systems withpervasive vulnerabilities," los angeles times , august 2, p. 7.neumann, peter g. 1989. "risks: cumulative index of software engineering notesšillustrativerisks to the public in the use of computer systems and related technology," acm softwareengineering notes, vol. 14, no. 1, january, pp. 22œ26. (an updated index is to bepublished in the january 1991 issue, vol. 16, no. 1.)neumann, peter g. 1990a. "rainbows and arrows: how the security criteria address computermisuse," proceedings of the 13th national computer security conference, nationalinstitute of standards and technology/national computer security center, washington,d.c., october.neumann, peter g. 1990b. "a perspective from the risks forum," computers under attack:intruders, worms, and viruses, peter j. denning (ed.), acm press, new york.neumann, peter g. and d. b. parker. 1989. "a summary of computer misuse techniques,"proceedings of the 12th national computer security conference, national institute ofstandards and technology/national computer security center, baltimore, md., october10œ13, pp. 396œ407.new york state, committee on investigations, taxation, and government operations. 1989.beware computer 'virus attack', a staff report on the lack of security in state owned andoperated computers, albany, n.y., july 28.new york times. 1987. "german computer hobbyists rifle nasa's files," september 16.new york times. 1988. "computer systems under siege, here and abroad," january 31.new york times. 1988. "top secret, and vulnerable," april 15.new york times. 1988. "computer users fall victim to a new breed of vandals," may 19.new york times. 1988. "newspaper computer infected with a 'virus,'" may 25.new york times. 1988. "sabotage aimed at computer company destroys government computerdata," july 4.new york times. 1988. "programmer convicted after planting a 'virus,'" september 21, p. d15.new york times. 1988. "car computer inquiry begun," november 17.new york times. 1988. "cyberpunks seek thrills in computerized mischief," november 26.new york times. 1989. "2 accused of computer crimes in tv rivalry," may 11, p. a21.new york times. 1990. "g.a.o. study of computers," february 21, p. d4.newsweek. 1988. "is your computer infected?" february 1.bibliography232computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.nordwall, bruce d. 1989. "itt avionics emphasizes development of software, improves electronicsystems," aviation week & space technology , july 17, pp. 83, 85.norman, adrian r. d. 1983. computer insecurity, chapman and hall, new york.nycum, susan h. 1989. "legal exposures of the victim of computer abuse under u.s. law,"international bar association (iba) sbl conference, strasbourg, october 2œ6, iba,london, england.nycum, susan hubbell. 1976. "the criminal law aspects of computer abuse, part 1: state penallaws," journals of computers and law, vol. 5, pp. 271œ295.office of management and budget (omb). 1988. guidance for preparation of security plans forfederal computer systems containing sensitive information, omb bulletin no. 8816,washington, d.c., july.office of management and budget (omb). 1990. guidance for preparation of security plans forfederal computer systems that contain sensitive information, omb bulletin no. 9008,washington, d.c., july.office of science and technology policy (ostp). 1989. the federal highperformancecomputing program, washington, d.c., september 8.office of technology assessment (ota). 1985. federal government information technology:electronic surveillance and civil liberties, otacit293, october, u.s. gpo,washington, d.c.office of technology assessment (ota). 1986a. federal government information technology:management, security, and congressional oversight , otacit297, february, u.s.gpo, washington, d.c.office of technology assessment (ota). 1986b. federal government information technology:electronic record systems and individual privacy, otacit296, june, u.s. gpo,washington, d.c.office of technology assessment (ota). 1987a. the electronic supervisor: new technology, newtensions, otacit333, september, u.s. gpo, washington, d.c.office of technology assessment (ota). 1987b. defending secrets, sharing data: new locks andkeys for electronic information , otacit310, october, u.s. gpo, washington, d.c.office of technology assessment (ota). 1990. critical connections: communications for thefuture, otacit407, january, u.s. gpo, washington, d.c.office of the federal register, national archives and records administration. 1990. code offederal regulations, foreign relations, title 22, parts 1 to 299, subchapter mšinternational traffic in arms regulations, revised april 1, pp. 333œ390.parker, donn b. 1976. crime by computer, charles scribner's sons, new york.parker, donn b. 1983. fighting computer crime, charles scribner's sons, new york.parnas, david l., a. j. van schouwen, and s. p. kwan. 1990. "evaluation of safety criticalsoftware," communications of the acm, vol. 33, no. 6, june, pp. 636œ648.paul, bill. 1989. "electronic theft is routine and costs firms billions, security experts say," wallstreet journal, october 20, p. 1.paul, bill. 1990. "blackouts on east coast are called unavoidable," wall street journal, february28, p. b4.paul, james. 1989. bugs in the programšproblems in federal government computer softwaredevelopment and regulation, subcommittee on investigations and oversight, u.s. houseof representatives, september.paulk, mark c. 1989. "review of the computer virus crisis," ieee computer, july, p. 122.pc magazine. 1988a. "virus wars: a serious warning," february 29.pc magazine. 1988b. "why it's time to talk about viruses," june 28, pp. 33œ36.pearson, dorothy. 1988. "mis mangers launch counterattack to stem rising virus epidemic," pcweek, august 29, pp. 23œ24.bibliography233computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.pellerin, cheryl. 1990. "lightsout computing: agencies are discovering the benefits of unattendedcomputer centers," federal computer week , march 19.peterson, ivars. 1988. "a digital matter of life and death," science news, march 12, pp. 170œ171.pittelli, frank m. and hector garciamolina. 1989. "reliable scheduling in a tmr databasesystem," acm transactions on computer systems, vol. 7, no. 1, february.podell, harold j. and marshall d. abrams. 1989. "a computer security glossary for the advancedpractitioner," computer security journal , vol. iv, no. 1, pp. 69œ88.pollack, andrew. 1990. "revlon sues supplier over software disabling," new york times, october25, pp. d1, d4.ponting, bob. 1988. "some common sense about network viruses, and what to do about them,"(newsfront section), data communications, april, p. 60.poore, jesse h. and harlan d. mills. 1989. an overview of the cleanroom software developmentprocess, unpublished paper presented at the formal methods workshop, halifax, novascotia, july. available from the department of computer science, university oftennessee, knoxville.poos, bob. 1990. "af amends rfp to clarify security needs," federal computer week, february19, p. 4.potts, mark. 1989. "when computers go down, so can firms' bottom lines," washington post,november 2.prefontaine, daniel c., canadian department of justice. 1990. "future trends," presented at theforum on the international legal vulnerability of financial information, royal bank ofcanada, toronto, february 26œ28.president's council on integrity and efficiency. 1988. review of general controls in federalcomputer systems, u.s. gpo, washington, d.c., october.president's council on management improvement & president's council on integrity andefficiency. 1988. model framework for management control over automatedinformation systems, u.s. gpo, washington, d.c., january.privacy times (evan hendricks, ed.). 1989. vol. 9, no. 16, september 19, washington, d.c.rabin, michael o. and j. d. tygar. 1987. an integrated toolkit for operating system security,harvard university, cambridge, mass., may.reuter. 1990. "man faces charges of computer fraud," washington post , february 4, p. a18.richards, evelyn. 1989. "study: software bugs costing u.s. billions," washington post, october 17,pp. d1, d5.richardson, jennifer. 1990a. "federal reserve defends fedwire security," federal computer week,february 26, p. 4.richardson, jennifer. 1990b. "federal reserve adds security to fedwire," federal computer week,april 9.rinkerman, gary. 1983. "potential liabilities of independent software testing and certificationorganizations," computer law reporter, vol. 1, no. 5, march, pp. 725œ727.rivest, r., a. shamir, and l. adelman. 1978. "a method for obtaining digital signatures and publickey cryptosystems," communications of the acm, vol. 21, no. 2, february, pp. 120œ126.rochlis, jon a. and mark w. eichin. 1989. "with microscope and tweezers: the worm from mit'sperspective," communications of the acm, vol. 32, no. 6, june, pp. 689œ698.rothfeder, jeffrey, et al. 1990. "is your boss spying on you?" business week, january 15, p. 74.rumbelow, clive. 1981. "liability for programming errors," international business lawyer, vol. 9,(vii/viii), united kingdom.bibliography234computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.rutz, frank. 1988. "dod fights off computer virus," government computer news, vol. 7, no. 3,february 5, p. 1.safire, william. 1990. "spies of the future," new york times, march 16, p. a35.salpukas, agis. 1989. "computer chaos for air travelers," new york times, may 13, p. a1.saltman, roy. 1988. "accuracy, integrity and security in computerized votetallying,"communications of the acm, vol. 31, no. 10, october, pp. 1184œ1191.saltzer, j. and m. schroeder. 1975. "the protection of information in computer systems,"proceedings: ieee, vol. 63, no. 9, september, pp. 1278œ1308.savage, j. a. 1990. "apollo blasted by users over system security glitches," computerworld,october 8, p. 49.saydjari, o. sami, joseph m. beckman, and jeffrey r. leaman. 1987. "locking computerssecurely," proceedings of the 10th national computer security conference, nationalbureau of standards/national computer security center, baltimore, md., september 21œ24, pp. 129œ141.saydjari, o. sami, j. m. beckman, and j. r. leaman. 1989. "lock trek: navigating unchartedspace," proceedings of the 1989 ieee computer society symposium on security andprivacy, ieee computer society, oakland, calif., may, pp. 167œ175.scherlis, william l., stephen l. squires, and richard d. pethia. 1990. "computer emergencyresponse," computers under attack: intruders, worms, and viruses, peter denning (ed.),acm press, new york.schlichting, r. and r. schneider. 1983. "failstop processors: an approach to designing faulttolerant computing systems," acm transactions on computer systems, vol. 1, no. 3,august, pp. 222œ238.schmitt, warren. 1990. information classification and control, sears technology services,schaumburg ill., january.schultz, eugene. 1990. "forming and managing ciac: lessons learned," unpublished presentationat cert workshop, june 20, pleasanton, calif., lawrence livermore nationallaboratory, livermore, calif.schuman, evan. 1989. "never mind osf/1, here's osf/2," unix today, november 27, pp. 1, 26.selby, r. w., v. r. basili, and f. t. baker. 1987. "cleanroom software development: an empiricalevaluation," ieee transactions on software engineering, vol. se13, no. 9.selz, michael. 1989. "computer vaccines or snake oil?" wall street journal, october 13, p. b6.sennett, c. t. 1989. formal methods in the production of secure software , royal signals andradar establishment, malvern, united kingdom, pp. 1œ2.seymour, jim, and jonathan matzkin. 1988. "confronting the growing threat of computer softwareviruses," pc magazine, june 28, pp. 33œ36.shatz, willie. 1990. "the terminal men: crackdown on the 'legion of doom' ends an era forcomputer hackers," washington post, june 24, pp. h1, h6.shoch, john f. and jon a. hupp. 1982. "the 'worm' programsšearly experience with a distributedcomputation," computing practices, march, pp. 172œ180.shore, john. 1988. "why i never met a programmer i could trust," communications of the acm,vol. 31, no. 4, april, p. 372.simitis, s. (ed.). 1987. the hessian data protection act, editor: the hessian data protectioncommissioner, uhlandstrasse 4, 6200 wiesbaden, federal republic of germany.publisher: wiesbadener graphische betriebe gmbh, wiesbaden.simmons, g. 1988. "a survey of information authentication," proceedings: ieee, vol. 76, no. 5,may, pp. 603œ620.simpson, glenn. 1989. "can you count on the vote count?" insight, january 9, p. 23.sims, calvin. 1989. "not everyone applauds new phone services," new york times, december 13,p. 6.bibliography235computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.sims, calvin. 1990. "computer failure disrupts at&t long distance," new york times, january 16,pp. a1, a24.sloan, irving j. 1984. computers and the law, oceana publications, new york.smith, kerry m. l. 1988. "suing the provider of computer software: how courts are applyingu.c.c. article two, strict tort liability, and professional malpractice," willamette lawreview, vol. 24, no. 3, summer, pp. 743œ766.smith, tom. 1989. "ibm's new release of racf, other security tools bow," network world,october 30, pp. 4, 60.snyders, jan. 1983. "security software doubles your protection," computer decisions, vol. 15, no.9, september, pp. 46, 50œ56.solomon, j. 1982. "specificationtocode correlation," proceedings of the 1982 ieee symposium onsecurity and privacy, ieee computer society, oakland, calif., april.soma, john t. 1983. computer technology and the law, shepard's/mcgrawhill, coloradosprings, colo.soper, keith. 1989. "integrity vs. security: avoiding the tradeoff," computerworld, june 12, pp.79œ83.spafford, eugene h. 1989a. the internet worm program: an analysis, purdue technical reportcsdtr823, department of computer science, purdue university, west lafayette, ind.spafford, eugene h. 1989b. "crisis and aftermath," communications of the acm, vol. 32, no. 6,june, pp. 678œ687.specter, michael. 1990. "revenge on the nerds," washington post, february 11, p. c5.sprouse, robert t. 1987. "commentary: on the secfasb partnership," accounting horizons,december, pp. 92œ95.sri international. 1989. international information integrity institute (i4) annual report 1989,menlo park, calif.steiner, jennifer, c. neuman, and j. i. schiller. 1988. "kerberos: an authentication service for opennetwork systems," usenix dallas winter 1988 conference proceedings, usenixassociation, berkeley, calif., pp. 191œ202.stipp, david. 1990. "virus verdict likely to have limited impact," wall street journal, january 24,pp. b1, b7.stoll, clifford. 1988. "stalking the wily hacker," communications of the acm, vol. 31, no. 5,may, pp. 484œ497.stoll, clifford. 1989. the cuckoos's egg, doubleday, new york.strauss, paul r. 1989. "lesson of the lurking software glitch," data communications, june 21, p. 9.streitfeld, david. 1989. "personal data, on the record," washington post, september 26, p. d5.sweet, walter. 1990. "global nets elevate security concerns," network world, july 30, pp. 23œ24.tanebaum, a. 1981. computer networks, prenticehall, englewood cliffs, n.j.thackeray, gail. 1985. "computerrelated crimes: an outline," jurimetrics journal, spring, pp.300œ318.thompson, k. 1984. "reflections on trusting trust," (1983 turing award lecture), communicationsof the acm, vol. 27, no. 8, august, pp. 761œ763.time. 1988. "computer viruses," (cover story), september 26.toigo, jon william. 1990. "security: biometrics creep into business," computerworld, june 11,pp. 75œ78.tompkins, f. g. 1984. nasa guidelines for assuring the adequacy and appropriateness ofsecurity safeguards in sensitive applications, mtr84w179, the mitre corp., metrekdivision, mclean, va., september.turn, rein. 1980. "an overview of transborder data flow issues," proceedings of the 1980bibliography236computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.ieee computer society symposium on security and privacy, ieee computer society,oakland, calif., april 14œ16, pp. 3œ8.turn, rein. 1990. "information privacy issues for the 1990s," proceedings of the 1990 ieeecomputer society symposium on security and privacy , ieee computer society, oakland,calif., may 7œ8.turner, judith axler. 1988. "security officials ask researchers not to make 'virus' copies available,"the chronicle of higher education , no. 13, november 23, pp. 1, a12.tzu, sun. 1988. the art of war, (translated by thomas cleary), shambhala, boston.u.k. communicationselectronics security group/department of trade and industry (cesg/dti).1990. ukit security evaluation and certification scheme, publication no. 1: descriptionof the scheme, final draft version 2.3, uksp 01, cheltenham, england, july 13.u.k. department of trade and industry (dti). 1989. overview manual (v01), glossary (v02),index (v03), users' code of practice (v11), security functionality manual (v21),evaluation levels manual (v22), evaluation and certification manual (v23), vendors'code of practice (v31), version 3.0, commercial computer security centre, london,england, february.u.k. ministry of defence. 1989a. requirements for the procurement of safety critical software indefense equipment, interim defense standard 0055, glasgow, united kingdom, may.u.k. ministry of defence. 1989b. requirements for the analysis of safety critical hazards, interimdefense standard 00œ56, glasgow, united kingdom, may.ulbrich, b. and j. collins. 1990. "announcing sun microsystem's customer warning system forsecurity incident handling," xsunspotsdigest , vol. 9, no. 308, message 13.underwriters laboratories, inc. 1989. underwriters laboratories, inc. 1988 annual report,underwriters laboratories, inc., northbrook, ill.underwriters laboratories, inc. 1990a. the proposed first edition of the standards for safetyrelated software , ul1998, underwriters laboratories, inc., northbrook, ill., august 17.underwriters laboratories, inc. 1990b. ul yesterday today tomorrow , underwriters laboratories,inc., northbrook, ill.university of california, los angeles (ucla). 1989. sixth annual ucla survey of businessschool computer usage, john e. anderson graduate school of management, ucla, losangeles, calif., september.u.s. bureau of alcohol, tobacco and firearms. 1988. "explosive incidents report 1987,"washington, d.c.u.s. congress, house, committee on the judiciary, subcommittee on crime. 1983. counterfeitaccess device and computer crime: hearings on h.r. 3181, h.r. 3570, and h.r. 5112,98th cong., 1st and 2nd sess., september 29 and november 10, 1983, and march 28,1984,u.s. gpo, washington, d.c.u.s. congress, house, committee on the judiciary, subcommittee on crime. 1985. computercrime and computer security: hearing on h.r. 1001 and h.r. 930, 99th cong., 1st sess.,may 25, u.s. gpo, washington, d.c.u.s. congress, house. 1986. computer fraud and abuse act of 1986, public law 99œ474, h.r.4718, october 16, h. rept. 100œ153(i), u.s. gpo, washington, d.c.u.s. congress, house, committee on the judiciary. 1986. computer fraud and abuse act of 1986:report to accompany h.r. 4712, 99th cong., 2nd sess. , u.s. gpo, washington, d.c.u.s. congress, house, committee on the judiciary. 1986. computer fraud and abuse act of 1986:report to accompany h.r. 5616, 99th cong., 2nd sess., u.s. gpo, washington, d.c.bibliography237computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.u.s. congress, house, committee on government operations, legislation and national securitysubcommittee. 1987. computer security act of 1987: hearings on h.r. 145 before asubcommittee of the committee on government operations, 100th cong., 1st sess.,february 25 and 26 and march 17, u.s. gpo, washington, d.c.u.s. congress, house, committee on science, space, and technology. 1987. computer securityact of 1987: report to accompany h.r. 145, 100th cong., 1st sess., u.s. gpo,washington, d.c.u.s. congress, house, technology policy task force of the committee on science, space, andtechnology. 1987. communications and computers in the 21st century: hearing, 100thcong., 1st sess., june 25, u.s. gpo, washington, d.c.u.s. congress, house. 1989. computer protection act of 1989, h.r. 287, 101st cong., 1st sess.,january 3, u.s. gpo, washington, d.c.u.s. congress, house, committee on energy and commerce, subcommittee ontelecommunications and finance. 1989. hearing to examine the vulnerability of nationaltelecommunications networks to computer viruses, 101st cong., 1st sess., july 20, u.s.gpo, washington, d.c.u.s. congress, house. 1989. computer network protection act of 1989 , h.r. 3524, 101st cong.,1st sess., october 25, u.s. gpo, washington, d.c.u.s. congress, house. 1989. data protection act of 1989, h.r. 3669, 101st cong., 1st sess.,november 15, u.s. gpo, washington, d.c.u.s. congress, house. 1989. computer virus eradication act of 1989 , h.r. 55, 101st cong., 1stsess., u.s. gpo, washington, d.c.u.s. congress, house, committee on energy and commerce, subcommittee ontelecommunications and finance. 1990. oversight hearing to receive the findings of theu.s. general accounting office on the vulnerability of united states securities trading,electronic funds transfer, and financial message systems to computer viruses, 101stcong., 2nd sess., february 21, u.s. gpo, washington, d.c.u.s. congress, senate, committee on the judiciary. 1986. electronic communications privacy actof 1986: report to accompany s. 2575, 99th cong., 2nd sess., u.s. gpo, washington, d.c.u.s. congress, senate, judiciary subcommittee on patents, copyrights, and trademarks. 1989.computer software rental amendments act (s. 198): hearings, 101st cong., 1st sess.,april 19, u.s. gpo, washington, d.c.u.s. congress, senate, judiciary subcommittee on technology and the law. 1989. hearing oncomputer viruses, 101st cong., 1st sess., may 15, u.s. gpo, washington, d.c.u.s. congress, senate. 1990. computer abuse amendment act of 1990, s. 2476, 101st cong., 2ndsess., april 19, u.s. gpo, washington, d.c.u.s. department of defense (dod). 1985a. password management guideline , cscstd00285,also known as the green book, national computer security center, fort meade, md.,april 12.u.s. department of defense (dod). 1985b. technical rationale behind cscstd00385:computer security requirements, guidance for applying the department of defensetrusted computer system evaluation criteria in specific environments, also known as theyellow book, national computer security center, fort meade, md., june 25.u.s. department of defense (dod). 1985c. keeping the nation's secrets , commission to reviewdod security policies and practices, washington, d.c., november.u.s. department of defense (dod). 1985d. trusted computer system evaluation criteria, dod5200.28std, also known as the orange book, national computer security center, fortmeade, md., december (superseded cscstd00183 dated august 15, 1983).u.s. department of defense (dod). 1987. trusted network interpretation of the trustedbibliography238computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.computer system evaluation criteria, ncsctg005, version 1, also known as the redbook, or tni, national computer security center, fort meade, md., july 31.u.s. department of defense (dod). 1988a. ''improvements in computer security procedures,"office of assistant secretary of defense, public affairs, washington, d.c., january 6.u.s. department of defense (dod). 1988b. glossary of computer security terms, ncsctg004,version 1, national computer security center, fort meade, md., october 21.u.s. department of defense (dod). 1988c. "darpa establishes computer emergency responseteam," office of assistant secretary of defense, public affairs, washington, d.c.,december 6.u.s. department of defense (dod), defense acquisition board. 1990. department of defensesoftware master plan, draft, february 9.u.s. department of energy. 1985. sensitive unclassified computer security program compliancereview guidelines, doe/ma0188/1, assistant secretary, management andadministration, directorate of administration, office of adp management, washington,d.c., june (revised september 1985).u.s. department of energy, energy information administration. 1986. sensitive computerapplications certification/recertification policy and procedures, ei 5633.1, initiated byadp services staff, washington, d.c., october.u.s. department of energy. 1988. unclassified computer security program , doe 1360.2a,initiated by office of adp management, washington, d.c., may.u.s. department of justice (doj), national institute of justice. 1989. computer crime: criminaljustice resource manual, washington, d.c., august.u.s. department of the treasury. 1989. "reports of crimes and suspected crimes," federal register,vol. 54, no. 117, june 20.u.s. food and drug administration (fda). 1987. policy for the regulation of computer products,draft, fda, rockville, md., september 9.u.s. food and drug administration (fda). 1988. reviewer guidance for computercontrolledmedical devices, draft, fda, rockville, md., july 25.veterans administration, office of information, systems, and telecommunications. 1987.computer security: a handbook for va managers and endusers, july. available fromu.s. department of veterans affairs, washington, d.c.voelcker, john. 1988. "spread of computer viruses worries users," the institute (a publication of theinstitute of electrical and electronics engineers), vol. 12, no. 6, june, p. 1.wald, matthew l. 1990. "experts diagnose telephone 'crash'," new york times, january 16, p. a25.waldrop, mitchell m. 1989. "flying the electric skies," science, vol. 244, pp. 1532œ1534.walker, b. j., r. a. kemmerer, and g. j. popek. 1980. "specification and verification of the uclaunix security kernel," communications of the acm, vol. 23, no. 2, 1980, pp. 118œ131.walker, stephen t. 1985. "network security overview," proceedings of the 1985 ieee symposiumon security and privacy, ieee computer society, oakland, calif., april 22œ24, pp. 62œ76.wall street journal. 1988. "first computer message on stopping virus took 48 hours to reachtarget," november 8, p. b5.wall, wendy l. 1989. "few firms plan well for mishaps that disable computer facilities," wallstreet journal, may 31.washington post. 1988. "searching for a better computer shield," november 13, pp. h1, h6.bibliography239computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.washington post. 1989. "computer virus strikes michigan hospital," march 23.washington post. 1990. "man faces charges of computer fraud," february 4, p. a18.washington university law quarterly. 1977. "potential liability: conclusion," vol. 405, no. 3, p.433.webb, ben. 1989. "plan to outlaw hacking," nature, vol. 341, october 19, p. 559.weil, martin. 1989. "double malfunction grounds thousands," washington post, november 4, pp.b1, b4.williams, gurney iii. 1988. "ul: what's behind the label," home mechanix , pp. 78œ80, 87œ88.winans, christopher. 1990. "personal data travels, too, through agencies," wall street journal,march 27, p. b1.wines, michael. 1990. "security agency debates new role: economic spying," new york times,june 18, p. a1.wing jeannette. 1990. "a specifier's introduction to formal methods," ieee computer, september.wright, karen. 1990. "the road to the global village," scientific american, march, pp. 83œ94.young catherine l. 1987. "taxonomy of computer virus defense mechanisms," proceedings of the10th national computer security conference, national bureau of standards/nationalcomputer security center, baltimore, md., september 21œ24, pp. 220œ225.young w. d. and j. mchugh. 1987. "coding for a believable specification to implementationmapping," proceedings of the 1987 ieee symposium on security and privacy, ieeecomputer society, oakland, calif., april 27œ29, pp. 140œ148.youngblut, christine, et al. 1989. "sds software testing and evaluation," ida paper p2132,institute for defense analyses, alexandria, va., february.zachary, g. pascal. 1990. "u.s. agency stands in way of computersecurity tool," wall streetjournal, july 9, pp. b1, b3.zeil, steven j. 1989. constraint satisfaction and test data generation," old dominion university,norfolk, va.bibliography240computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.appendixesappendixes241computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.appendixes242computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.appendix athe orange bookthe department of defense's trusted computer system evaluationcriteria, or orange book, contains criteria for building systems that providespecific sets of security features and assurances (u.s. dod, 1985d; seebox a.1). however, the orange book does not provide a complete basis forsecurity: its origin in the defense arena is associated with an emphasis ondisclosure control that seems excessive to many commercial users ofcomputers. there is also a perception in the marketplace that it articulatesdefense requirements only. it specifies a coherent, targeted set of security functions that may not begeneral enough to cover a broad range of requirements in the commercialworld. for example, it does not provide sufficient attention to informationintegrity and auditing. it says little about networked systems (despite theattempts made by the current and anticipated versions of the trustednetwork interpretation, or red book (u.s. dod, 1987). also, it providesonly weak support for management control practices, notably individualaccountability and separation of duty. the orange book process combines published system criteria with systemevaluation and rating (relative to the criteria) by the staff of the nationalcomputer security center. this process provides no incentive or rewardfor security capabilities that go beyond, or do not literally answer, theorange book's specific requirements. familiarity with the orange book is uneven within the broadercommunity of computer manufacturers, managers, auditors, and insurers,and system users. its definitions and concepts have not been expressed inthe vocabulary typically used in general informationappendix a243computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.box a.1 summary of evaluation criteria classesthe classes of systems recognized under the trusted computer systemsevaluation criteria are as follows. they are presented in the order ofincreasing desirability from a computer security point of view.class (d): minimal protectionthis class is reserved for those systems that have been evaluated butthat fail to meet the requirements for a higher evaluation class.class (c1): discretionary security protectionthe trusted computing base (tcb) of a class (c1) system nominallysatisfies the discretionary security requirements by providing separation ofusers and data. it incorporates some form of credible controls capable ofenforcing access limitations on an individual basis, i.e., ostensibly suitable forallowing users to be able to protect project or private information and to keepother users from accidentally reading or destroying their data. the class (c1)environment is expected to be one of cooperating users processing data atthe same level(s) of sensitivity.class (c2): controlled access protectionsystems in this class enforce a more finely grained discretionary accesscontrol than (c1) systems, making users individually accountable for theiractions through login procedures, auditing of securityrelevant events, andresource isolation.class (b1): labeled security protectionclass (b1) systems require all the features required for class (c2). inaddition, an informal statement of the security policy model, data labeling,and mandatory access control over named subjects and objects must bepresent. the capability must exist for accurately labeling exportedinformation. any flaws identified by testing must be removed.class (b2): structured protectionin class (b2) systems, the tcb is based on a clearly defined anddocumented formal security policy model that requires the discretionary andmandatory access control enforcement found in class (b1) systems to beextended to all subjects and objects in the adp system. in addition, covertchannels are addressed. the tcb must be carefully structured intoprotectioncritical and nonprotectioncritical elements. the tcb interface iswelldefined and the tcb design and implementation enable it to besubjected to more thorough testing and more complete review.authentication mechanisms are strengthened, trusted facility management isprovided in the form of support for system administrator and operatorfunctions, and stringent configuration management controls are imposed.the system is relatively resistant to penetration.appendix a244computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.class (b3): security domainsthe class (b3) tcb must satisfy the reference monitor requirements thatit mediate all accesses of subjects to objects, be tamperproof, and be smallenough to be subjected to analysis and tests. to this end, the tcb isstructured to exclude code not essential to security policy enforcement, withsignificant system engineering during tcb design and implementationdirected toward minimizing its complexity. a security administrator issupported, audit mechanisms are expanded to signal securityrelevantevents, and system recovery procedures are required. the system is highlyresistant to penetration.class (a1): verified designsystems in class (a1) are functionally equivalent to those in class (b3) inthat no additional architectural features or policy requirements are added.the distinguishing feature of systems in this class is the analysis derivedfrom formal design specification and verification techniques and the resultinghigh degree of assurance that the tcb is correctly implemented. thisassurance is developmental in nature, starting with a formal model of thesecurity policy and a formal toplevel specification (ftls) of the design. inkeeping with extensive design and development analysis of the tcb requiredof systems in class (a1), more stringent configuration management isrequired and procedures are established for securely distributing the systemto sites. a system security administrator is supported.source: department of defense trusted computer system evaluation criteria, dod 5200.28std, december 1985, appendix c, pp. 93œ94. processing. it has been codified as a military standard, making it arequirement for defense systems, and its dissemination has been directedlargely to major vendors of centralized systems, notably vendors who areor who supply government contractors.because of its shortcomings, which have been debated in the computersecurity community for several years, the orange book must be regarded asonly an interim stage in the codification of prudent protection practices.appendix a245computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.appendix bselected topics in computer securitytechnologythis appendix discusses in considerable detail selected topics in computersecurity technology chosen either because they are well understood andfundamental, or because they are solutions to current urgent problems. severalsections expand on topics presented in chapter 3.orange book securitya security policy is a set of rules by which people are given access toinformation and/or resources. usually these rules are broadly stated, allowingthem to be interpreted somewhat differently at various levels within anorganization. with regard to secure computer systems, a security policy is usedto derive a security model, which in turn is used to develop the requirements,specifications, and implementation of a system.library examplea "trusted system" that illustrates a number of principles related to securitypolicy is a library. in a very simple library that has no librarian, anyone (asubject) can take out any book (an object) desired: no policy is being enforcedand there is no mechanism of enforcement. in a slightly more sophisticatedcase, a librarian checks who should have access to the library but does notparticularly care who takes out which book: the policy enforced is, "anyoneallowed in the room is allowed access to anything in the room." such a policyrequires only identification of the subject. in a third case, a simpleappendix b246computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.extension of the previous one, no one is allowed to take out more than fivebooks at a time. in a sophisticated version of this system, a librarian firstdetermines how many books a subject already has out before allowing thatsubject to take more out. such a policy requires a check of the subject's identityand current status.in a library with an even more complex policy, only certain people areallowed to access certain books. the librarian performs a check by name of whois allowed to access which books. this policy frequently involves thedevelopment of long lists of names and may evolve toward, in some cases, anegative list, that is, a list of people who should not be able to have access tospecific information. in large organizations, determining which users haveaccess to specific information frequently is based on the project they areworking on or the level of sensitivity of data for which they are authorized. ineach of these cases, there is an access control policy and an enforcementmechanism. the policy defines the access that an individual will have toinformation contained in the library. the librarian serves as the policyenforcing mechanism.orange book security modelsthe bestknown and most widely used formal models of computer securityfunctionality, the bell and lapadula model and its variants (bell and lapadula,1976), emphasize confidentiality (protection from unauthorized disclosure ofinformation) as their primary security service. in particular, these modelsattempt to capture the "mandatory" (what iso standard 74982 (iso, 1989)refers to as "administratively directed, labelbased") aspects of security policy.this is especially important in providing protection against "trojan horse"software, a significant concern among those who process classified data.mandatory controls are typically enforced by operatingsystem mechanisms atthe relatively coarse granularity of processes and files. this state of affairs hasresulted from a number of factors, several of which are noted below:1. the basic security models were accurately perceived to representdepartment of defense (dod) security concerns for protectingclassified information from disclosure, especially in the face of trojanhorse attacks. since it was under the auspices of dod funding that thework in formal security policy models was carried out, it is notsurprising that the emphasis was on models that reflected dodrequirements for confidentiality.2. the embodiment of the model in the operating system has beenappendix b247computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.deemed essential in order to achieve a high level of assurance and tomake available a secure platform on which untrusted (or less trusted)applications could be executed without fear of compromising overallsystem security. it was recognized early that the development oftrusted software, that is, software that is trusted to not violate thesecurity policy imposed on the computer system, is a very difficult andexpensive task. this is especially true if a security policy calls for ahigh level of assurance in a potentially "hostile" environment, forexample, execution of software from untrusted sources.the strategy evolved of developing trusted operating systemsthat could segregate information and processes (representing users)to allow controlled sharing of computer system resources. if trustedapplication software were written, it would require a trustedoperating system as a platform on top of which it would execute.(if the operating system were not trusted, it, or other untrustedsoftware, could circumvent the trusted operation of the applicationin question.) thus development of trusted operating systems is anatural precursor to the development of trusted applications.at the time this strategy was developed, in the late 1960s and inthe 1970s, computer systems were almost exclusively timesharedcomputers (mainframes or minis), and the resources to be shared(memory, disk storage, and processors) were expensive. with theadvent of trusted operating systems, these expensive computingresources could be shared among users who would develop andexecute applications without requiring trust in each application toenforce the system security policy. this has become an acceptedmodel for systems in which the primary security concern isdisclosure of information and in which the information is labeled ina fashion that reflects its sensitivity.3. the granularity at which the security policy is enforced is determinedlargely by characteristics of typical operating system interfaces andconcerns for efficient implementation of the mechanisms that enforcesecurity. thus, for example, since files and processes are the objectsmanaged by most operating systems, these were the objects protectedby the security policy embodied in the operating system. in support ofbelllapadula, data sensitivity labels are associated with files, andauthorizations for data access are associated with processes operatingon behalf of users. the operating system enforces the security policyby controlling access to data based on file labels and process (user)authorizations. this type of security policy implementation is thehallmark of highassurance systems as defined by the orange book.concerning integrity in the orange book, note that if an integrity policy(like clarkwilson) and an integrity mechanism (like type enforcementappendix b248computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.or rings) are then differentiated, an invariant property of mechanisms is thatthey enforce a "protected subsystem" kind of property. that is, they undertaketo ensure that certain data is touchable only by certain code irrespective of theprivileges that code inherits because of the person on whose behalf it isexecuting. thus a proper integrity mechanism would ensure that one's personalprivilege to update a payroll file could not be used to manipulate payroll datawith a text editor, but rather that the privilege could be used only to accesspayroll data through the payroll subsystem, which presumably performsapplicationdependent consistency checks on what one does.while the orange book does not explicitly call out a set of integritybasedaccess rules, it does require that b2level1 systems and those above execute outof a protected domain, that is, that the trusted computing base (tcb) itself be aprotected subsystem. the mechanism used to do this (e.g., rings) is usually, butnot always, exported to applications. thus an integrity mechanism is generallyavailable as a byproduct of a system operating at the b2 level.the orange book does not mandate mechanisms to support data integrity,but it easily could do so at the b2 level and above, because it mandates thatsuch a mechanism exist to protect the tcb. it is now possible to devisemechanisms that protect the tcb but that cannot be made readily available toapplications; however, such cases are in the minority and can be consideredpathological.hardware enforcement of security andintegritythe complexity and difficulty of developing secure applications can bereduced by modifying the hardware on which those applications run. suchmodifications may add functionality to the operating system or applicationsoftware, they may guarantee specific behavior that is not normally provided byconventional hardware, or they may enhance the performance of basic securityfunctions, such as encryption. this section describes two projects that serve asworked examples of what can be accomplished when hardware is designed withsecurity and/or integrity in mind, and what is gained or lost through such anapproach.viper microprocessorthe viper microprocessor was designed specifically for highintegritycontrol applications at the royal signals and radar establishmentappendix b249computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.(rsre), which is part of the u.k.'s ministry of defence (mod). viperattempts to achieve high integrity with a simple architecture and instruction setdesigned to meet the requirements of formal verification and to provide supportfor highintegrity software.viper 1 was designed as a primitive building block that could be used toconstruct complete systems capable of running highintegrity applications. itsmost important requirement is the ability to stop immediately if any hardwareerror is detected, including illegal instruction codes and numeric underflow andoverflow. by stopping when an error is detected, viper assures that noincorrect external actions are taken following a failure. such ''failstop"operation (schlichting and schneider, 1983) simplifies the design of higherlevel algorithms used to maintain the reliability and integrity of the entire system.viper 1 is a memorybased processor that makes use of a uniforminstruction set (i.e., all instructions are the same width). the processor has onlythree programmable 32bit registers. the instruction set limits the amount ofaddressable memory to 1 megaword, with all access on word boundaries. thereis no support for interrupts, stack processing or micropipelining.the viper 1 architecture provides only basic program support. in fact,multiplication and division are not supported directly by the hardware. thisapproach was taken primarily to simplify the design of viper, therebyallowing it to be verified. if more programming convenience is desired, it mustbe handled by a highlevel compiler, assuming that the resulting loss inperformance is tolerable.the viper 1a processor allows two chips to be used in tandem in anactivemonitor relationship. that is, one of the chips can be used to monitor theoperation of the other. this is achieved by comparing the memory and input/output (i/o) addresses generated by both chips as they are sent offchip. ifeither chip detects a difference in this data, then both chips are stopped. in thismodel, a set of two chips is used to form a single failstop processor making useof a single memory module and an i/o line.it is generally accepted that viper's performance falls short ofconventional processors' performance, and always will. because it is beingdeveloped for highintegrity applications, the viper processor must alwaysdepend on wellestablished, mature implementation techniques andtechnologies. many of the decisions about viper's design were made withstatic analysis in mind. consequently, the instruction set was kept simple,without interrupt processing, to allow static analysis to be done effectively.appendix b250computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.lock projectthe logical coprocessing kernel (lock) project intends to develop asecure microcomputer prototype by 1990 that provides a1level security forgeneralpurpose processing. the lock design makes use of a hardwarebasedreference monitor, known as sidearm, that can be used to build new, securevariants of existing architectures or can be included in the design of newarchitectures as an option. the goal is to provide the highest level of security ascurrently defined by national computer security center (ncsc) standards,while providing 80 percent of the performance achievable by an unmodified,insecure computer. sidearm is designed to achieve this goal by controllingthe memory references made by applications running on the processor to whichit is attached. assuming that sidearm is always working properly and hasbeen integrated into the host system in a manner that guarantees its controlscannot be circumvented, it provides high assurance that applications can accessdata items only in accordance with a wellunderstood security policy. thelock project centers on guaranteeing that these assumptions are valid.the sidearm module is the basis of the lock architecture and is itselfan embedded computer system, making use of its own processor, memory,communications, and storage subsystems, including a laser disk for auditing. itis logically placed between the host processor and memory, and integrated intothose existing host facilities, such as memory management units, that controlaccess into memory. since it is a separate hardware component, applicationscan not modify any of the security information used to control sidearmdirectly.security policy is enforced by assigning security labels to all subjects (i.e.,applications or users) and objects (i.e., data files and programs) and makingsecurity policy decisions without relying on the host system. the security policyenforced by sidearm includes typeenforcement controls, providingconfigurable, mandatory integrity. that is, "types" can be assigned to dataobjects and used to restrict access to subjects that are performing functionsappropriate to that type. thus typeenforcement can be used, for example, toensure that a payroll data file is accessed only by payroll programs, or thatspecific transforms, such as labeling or encryption, are performed on data priorto output. mandatory access control (mac), discretionary access control(dac), and type enforcement are "additive" in that a subject must pass all threecriteria before being allowed to access an object.the lock project makes use of multiple tepachebased typeiappendix b251computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.encryption devices to safeguard sidearm media (security databases andaudit) and data stored on host system media, and to close covert channels. assuch, lock combines aspects of both comsec (communications security)and compusec (computer security) in an interdependent manner. thesecurity provided by both approaches is critical to lock's proper operation.the lock architecture requires few but complex trusted softwarecomponents, including a sidearm device driver and software that ensuresthat decisions made by the sidearm are enforced by existing host facilitiessuch as a memory management unit. an important class of trusted softwarecomprises "kernel extensions," securitycritical software that runs on the host tohandle machinedependent support, such as printer and terminal securitylabeling, and applicationspecific security policies, such as that required by adatabase management system. kernel extensions are protected and controlledby the reference monitor and provide the flexibility needed to allow the locktechnology to support a wide range of applications, without becoming too largeor becoming architecturedependent.one of lock's advantages is that a major portion of the operating system,outside of the kernel extensions and the reference monitor, can be considered"hostile." that is, even if the operating system is corrupted, lock will notallow an unauthorized application to access data objects. however, parts of theoperating system must still be modified or removed to make use of thefunctionality provided by sidearm. the lock project intends to support theunix system v interface on the lock architecture and to attain certificationof the entire system at the a1 level.cryptographycryptography is the art of keeping data secret, primarily through the use ofmathematical or logical functions that transform intelligible data into seeminglyunintelligible data and back again. cryptography is probably the most importantaspect of communications security and is becoming increasingly important as abasic building block for computer security.fundamental concepts of encryptioncryptography and cryptanalysis have existed for at least 2,000 years,perhaps beginning with a substitution algorithm used by julius caesar(tanebaum, 1981). in his method, every letter in the original message, knownnow as the plaintext, was replaced by the letter thatappendix b252computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.occurred three places later in the alphabet. that is, a was replaced by d, b wasreplaced by e, and so on. for example, the plaintext "veni vidi vici" wouldyield "yhql ylgl ylfl." the resulting message, now known as theciphertext, was then couriered to an awaiting centurion, who decrypted it byreplacing each letter with the letter that occurred three places "before" it in thealphabet. the encryption and decryption algorithms were essentially controlledby the number three, which thus was the encryption and decryption key. ifcaesar suspected that an unauthorized person had discovered how to decryptthe ciphertext, he could simply change the key value to another number andinform the field generals of that new value by using some other method ofcommunication. although caesar's cipher is a relatively simple example ofcryptography, it clearly depends on a number of essential components: theencryption and decryption algorithms, a key that is known by all authorizedparties, and the ability to change the key. figure b.1 shows the encryptionprocess and how the various components interact.figure b.1 the encryption process.if any of these components is compromised, the security of the informationbeing protected decreases. if a weak encryption algorithm is chosen, anopponent may be able to guess the plaintext once a copy of the ciphertext isobtained. in many cases, the cryptanalyst need only know the type of encryptionalgorithm being used in order to break it. for example, knowing that caesarused only a cyclic substitution of the alphabet, one could simply try every keyvalue from 1 to 25, looking for the value that resulted in a message containinglatin words. similarly, many encryption algorithms that appear to be verycomplicated are rendered ineffective by an improper choice of a key value. in amore practical sense, if the receiver forgets the key value or uses the wrong one,then the resulting message will probably be unintelligible, requiring additionaleffort to retransmit the message and/or the key. finally, it is possible that theenemy will break the code even if the strongest possible combination ofalgorithms and key values is used. therefore, keys and possibly even thealgorithms need to be changed over a period of time to limitappendix b253computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.the loss of security when the enemy has broken the current system. the processof changing keys and distributing them to all parties concerned is known as keymanagement and is the most difficult aspect of security management after anencryption method has been chosen.2in theory, any logical function can be used as an encryption algorithm. thefunction may act on single bits of information, single letters in some alphabet,or single words in some language or groups of words. the caesar cipher is anexample of an encryption algorithm that operates on single letters within amessage. throughout history a number of "codes" have been used in which atwocolumn list of words is used to define the encryption and decryptionalgorithms. in this case, plaintext words are located in one of the columns andreplaced by the corresponding word from the other column to yield theciphertext. the reverse process is performed to regenerate the plaintext from theciphertext. if more than two columns are distributed, a key can be used todesignate both the plaintext and ciphertext columns to be used. for example,given 10 columns, the key [3,7] might designate that the third columnrepresents plaintext words and the seventh column represents ciphertext words.although code books (e.g., multicolumn word lists) are convenient for manualenciphering and deciphering, their very existence can lead to compromise. thatis, once a code book falls into enemy hands, ciphertext is relatively simple todecipher. furthermore, code books are difficult to produce and to distribute,requiring accurate accounts of who has which books and which parties cancommunicate using those books. consequently, mechanical and electronicdevices have been developed to automate the encryption and decryptionprocess, using primarily mathematical functions on single bits of information orsingle letters in a given alphabet.private vs. public cryptosystemsthe security of a given cryptosystem depends on the amount ofinformation known by the cryptanalyst about the algorithms and keys in use. intheory, if the encryption algorithm and keys are independent of the decryptionalgorithm and keys, then full knowledge of the encryption algorithm and keywill not help the cryptanalyst break the code. however, in many practicalcryptosystems, the same algorithm and key are used for both encryption anddecryption. the security of these symmetric cipher systems depends on keepingat least the key secret from others, making such systems privatekey cryptosystems. an example of a symmetric, privatekey cryptosystem is the dataencryption standard (des) (see below, "data encryption standard").appendix b254computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.in this case, the encryption and decryption algorithm is widely known andhas been widely studied; the privacy of the encryption and decryption key isrelied on to ensure security. other privatekey systems have been implementedand deployed by the national security agency (nsa) for the protection ofclassified government information. in contrast to the des, the encryption anddecryption algorithms within those cryptosystems have been kept classified, tothe extent that the computer chips on which they are implemented are coated insuch a way as to prevent them from being examined.users are often intolerant of private encryption and decryption algorithmsbecause they do not know how the algorithms work or if a "trapdoor" exists thatwould allow the algorithm designer to read the user's secret information. in anattempt to eliminate this lack of trust, a number of cryptosystems have beendeveloped around encryption and decryption algorithms based onfundamentally difficult problems, or oneway functions, that have been studiedextensively by the research community. another approach used in publickeysystems, such as that taken by the rsa (see the section below headed "rsa"),is to show that the most obvious way to break the system involves solving ahard problem (although this means that such systems may be broken simplermeans).for practical reasons, it is desirable to use different encryption anddecryption keys in a cryptosystem. such asymmetric systems allow theencryption key to be made available to anyone, while preserving confidencethat only people who hold the decryption key can decipher the information.these systems, which depend solely on the privacy of the decryption key, areknown as publickey cryptosystems. an example of an asymmetric, publickeycipher is the patented rsa system.digital signaturessociety accepts handwritten signatures as legal proof that a person hasagreed to the terms of a contract as stated on a sheet of paper, or that a personhas authorized a transfer of funds as indicated on a check. but the use of writtensignatures involves the physical transmission of a paper document; this is notpractical if electronic communication is to become more widely used inbusiness. rather, a digital signature is needed to allow the recipient of amessage or document to irrefutably verify the originator of that message ordocument.a written signature can be produced by one person (although forgeriescertainly occur), but it can be recognized by many people as belongingappendix b255computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.uniquely to its author. to be accepted as a replacement for a written signature, adigital signature, then, would have to be easily authenticated by anyone, but beproducible only by its author.a digital signature system consists of three elements, each carrying out aprocedure:1. the generator, which produces two numbers called the mark (whichshould be unforgeable) and the secret;2. the signer, which accepts a secret and an arbitrary sequence of bytescalled the input, and produces a number called the signature; and3. the checker, which accepts a mark, an input, and a signature and sayswhether or not the signature matches the input for that mark.the procedures have the following properties: if the generator produces a mark and a secret, and the signer produces asignature when given the secret and an input, then the checker will saythat the signature matches the input for that mark. if one has a mark produced by the generator but does not have the secret,then even with a large number of inputs and matching signatures for thatmark, one still cannot produce an additional input and matching signaturefor that mark. in particular, even if the signature matches one of theinputs, one cannot produce another input that it matches. a digitalsignature system is useful because if one has a mark produced by thegenerator, as well as an input and matching signature, then one can besure that the signature was computed by a system that knew thecorresponding secret, because a system that did not know the secret couldnot have computed the signature.for instance, one can trust a mark to certify an uninfected program if one believes that it came from the generator, and one also believes that any system that knows the corresponding secret isone that can be trusted not to sign a program image if it is corrupted.known methods for digital signatures are often based on computing asecure checksum (see below) of the input to be signed and then encrypting thechecksum with the secret. if the encryption uses publickey encryption, themark is the public key that matches the secret, and the checker simply decryptsthe signature.for more details, see chapter 9 in davies and price (1984).appendix b256computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.cryptographic checksumsa cryptographic checksum or oneway hash function accepts any amountof input data (in this case a file containing a program) and computes a smallresult (typically 8 or 16 bytes) called the checksum. its important property isthat it requires that much work be done to find a different input with the samechecksum. here "a lot of work" means "more computing than an adversary canafford." a cryptographic checksum is useful because it identifies the input: anychange to the input, even a very clever one made by a malicious person, is sureto change the checksum. suppose a trusted person tells another that the programwith checksum 7899345668823051 does not have a virus (perhaps he does thisby signing the checksum with a digital signature). one who computes thechecksum of file wordproc.exe and gets 7899345668823051 shouldbelieve that he can run wordproc.exe without worrying about a virus.for more details, see davies and price (1984), chapter 9.publickey cryptosystems and digital signaturespublickey cryptosystems offer a means of implementing digitalsignatures. in a publickey system the sender enciphers a message using thereceiver's public key, creating ciphertext1. to sign the message he enciphersciphertext1 with his private key, creating ciphertext2. ciphertext2 is then sent tothe receiver. the receiver applies the sender's public key to decrypt ciphertext2,yielding ciphertext1. finally, the receiver applies his private key to convertciphertext1 to plaintext. the authentication of the sender is evidenced by thefact that the receiver successfully applied the sender's public key and was ableto create plaintext. since encryption and decryption are opposites, using thesender's public key to decipher the sender's private key proves that only thesender could have sent it.to resolve disputes concerning the authenticity of a document, the receivercan save the ciphertext, the public key, and the plaintext as proof of the sender'ssignature. if the sender later denies that the message was sent, the receiver canpresent the signed message to a court of law where the judge then uses thesender's public key to check that the ciphertext corresponds to a meaningfulplaintext message with the sender's name, the proper time sent, and so forth.only the sender could have generated the message, and therefore the receiver'sclaim would be upheld in court.appendix b257computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.key managementin order to use a digital signature to certify a program (or anything else,such as an electronic message), it is necessary to know the mark that should betrusted. key management is the process of reliably distributing the mark toeveryone who needs to know it. when only one mark needs to be trusted, this isquite simple: a trusted person tells another what the mark is. he cannot do thisusing the computer system, which cannot guarantee that the informationactually came from him. some other communication channel is needed: a facetoface meeting, a telephone conversation, a letter written on official stationery,or anything else that gives adequate assurance. when several agents arecertifying programs, each using its own mark, things are more complex. thesolution is for one trusted agent to certify the marks of the other agents, usingthe same digital signature scheme used to certify anything else. consultativecommittee on international telephony and telegraphy (ccitt) standardx.509 describes procedures and data formats for accomplishing this multilevelcertification (ccitt, 1989b).algorithmsonetime padsthere is a collection of relatively simple encryption algorithms, known asonetime pad algorithms, whose security is mathematically provable. suchalgorithms combine a single plaintext value (e.g., bit, letter, or word) with arandom key value to generate a single ciphertext value. the strength of onetime pad algorithms lies in the fact that separate random key values are used foreach of the plaintext values being enciphered, and the stream of key values usedfor one message is never used for another, as the name implies. assuming thereis no relationship between the stream of key values used during the process, thecryptanalyst has to try every possible key value for every ciphertext value, atask that can be made very difficult simply by the use of differentrepresentations for the plaintext and key values.the primary disadvantage of a onetime pad system is that it requires anamount of key information equal to the size of the plaintext being enciphered.since the key information must be known by both parties and is never reused,the amount of information exchanged between parties is twice that contained inthe message itself. furthermore, the key information must be transmitted usingmechanisms different from those for the message, thereby doubling theresources required.appendix b258computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.finally, in practice, it is relatively difficult to generate large streams of"random" values effectively and efficiently. any nonrandom patterns thatappear in the key stream provide the cryptanalyst with valuable information thatcan be used to break the system.onetime pads can be implemented efficiently on computers using any ofthe primitive logical functions supported by the processor. for example, theexclusiveor (xor) operator is a convenient encryption and decryptionfunction. when two bits are combined using the xor operator, the result is 1 ifone and only one of the input bits is 1; otherwise the result is 0, as defined bythe table in figure b.2figure b.2 the xor function.the xor function is convenient because it is fast and permits decryptingthe encrypted information simply by "xoring" the ciphertext with the samedata (key) used to encrypt the plaintext, as shown in figure b.3.figure b.3 encryption and decryption using the xor function.data encryption standardin 1972, the national bureau of standards (nbs; now the nationalinstitute of standards and technology (nist)) identified a need for a standardcryptosystem for unclassified applications and issued a call for proposals.although it was poorly received at first, ibm proposed, in 1975, a privatekeycryptosystem that operated on 64bit blocks ofappendix b259computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.information and used a single 128bit key for both encryption and decryption.after accepting the initial proposal, nbs sought both industry and nsaevaluations. industry evaluation was desired because nbs wanted to provide asecure encryption that industry would want to use, and nsa's advice wasrequested because of its historically strong background in cryptography andcryptanalysis. nsa responded with a generally favorable evaluation butrecommended that some of the fundamental components, known as sboxes, beredesigned. based primarily on that recommendation, the data encryptionstandard (des; nbs, 1977) became a federal information processing standardin 1977 and an american national standards institute (ansi) standard (numberx3.921981/r1987) in 1980, using a 56bit key.the data encryption standard (des) represents the first cryptographicalgorithm openly developed by the u.s. government. historically, suchalgorithms have been developed by the nsa as highly classified projects.however, despite the openness of its design, many researchers believed thatnsa's influence on the sbox design and the length of the key introduced atrapdoor that allowed the nsa to read any message encrypted using the des. infact, one researcher described the design of a specialpurpose parallelprocessing computer that was capable of breaking a des system using 56bitkeys and that, according to the researcher, could be built by the nsa usingconventional technology. nonetheless, in over ten years of academic andindustrial scrutiny, no flaw in the des has been made public (although someexamples of weak keys have been discovered). unfortunately, as with all cryptosystems, there is no way of knowing if the nsa or any other organization hassucceeded in breaking the des.the controversy surrounding the des was reborn when the nsaannounced that it would discontinue the fs1027 des device certificationprogram after 1987, although it did recertify the algorithm (until 1993) for useprimarily in unclassified government applications and for electronic fundstransfer applications, most notably fedwire, which had invested substantially inthe use of des. nsa cited the widespread use of the des as a disadvantage,stating that if it were used too much it would become the prime target ofcriminals and foreign adversaries. in its place, nsa has offered a range ofprivatekey algorithms based on classified algorithms that make use of keysgenerated and managed by nsa.the data encryption standard (des) algorithm has four approved modesof operation: the electronic codebook, cipher block chaining, cipher feedback,and output feedback. each of these modes has certain characteristics that makeit more appropriate than the others forappendix b260computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.specific purposes. for example, the cipher block chaining and cipher feedbackmodes are intended for message authentication purposes, while the electroniccodebook mode is used primarily for encryption and decryption of bulk data(nbs, 1980b).rsathe rsa is a public key cryptosystem, invented and patented by ronaldrivest, adi shamir, and leonard adelman, that is based on large primenumbers (rivest et al., 1978). in their method, the decryption key is generatedby selecting a pair of prime numbers, p and q, (i.e., numbers that are notdivisible by any other) and another number, e, which must pass a specialmathematical test based on the values of the pair of primes. the encryption keyconsists of the product of p and q, which is called n, and the number e, whichcan be made publicly available. the decryption key consists of n and anothernumber, called d, which results from a mathematical calculation using n and e.the decryption key must be kept secret.a given message is encrypted by converting the text to numbers (usingconventional conversion mechanisms) and replacing each number with anumber computed using n and e. specifically, each number is multiplied byitself e times, with the result being divided by n, yielding a quotient, which isdiscarded, and a remainder. the remainder is used to replace the originalnumber as part of the ciphertext. the decryption process is similar, multiplyingthe ciphertext number by itself d times (versus e times) and dividing it by n,with the remainder representing the desired plaintext number (which isconverted back to a letter). rsa's security depends on the fact that, althoughfinding large prime numbers is computationally easy, factoring large integersinto their component primes is not, and it is computationally intensive.3however, in recent years, parallel processing techniques and improvements infactoring algorithms have significantly increased the size of numbers (measuredas the number of decimal digits in its representation) that can be factored in arelatively short period of time (i.e., less than 24 hours). seventydigit numbersare well within reach of modern computers and processing techniques, with 80digit numbers on the horizon. most commercial rsa systems use 512bit keys(i.e., 154 digits), which should be out of the reach of conventional computersand algorithms for quite some time. however, the best factoring approachescurrently use networks of workstations (perhaps several hundred or thousand ofthem), working parttime for weeks on end (browne, 1988). this suggests thatfactoring numbers up to 110 digits is on the horizon.appendix b261computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.protection of proprietary software anddatabasesthe problem of protecting proprietary software or proprietary databases isan old and difficult one. the blatant copying of a large commercial program,such as a payroll program, and its systematic use within the piratingorganization are often detectable and will then lead to legal action. similarconsiderations apply to large databases, and for these the pirating organizationhas the additional difficulty of obtaining the vendorsupplied periodic updates,without which the pirated database will become useless.the problem of software piracy is further exacerbated in the context ofpersonal computing. vendors supply programs for word processing,spreadsheets, gameplaying programs, compilers, and so on, and these aresystematically copied by pirate vendors and by private users. while largescalepirate vendors may eventually be detected and stopped, there is no hope ofpreventing, through detection and legal action, the mass of individual usersfrom copying from each other.various technical solutions have been proposed for the problem ofsoftware piracy in the personal computing world. some involve a machinecustomized layout of the data on a disk. others involve the use of volatiletranscription of certain parts of a program text. cryptography employingmachine or programinstance customized keys has been suggested, inconjunction with coprocessors that are physically impenetrable so thatcryptographic keys and crucial decrypted program text cannot be captured.some of these approaches, especially those employing special hardware, andhence requiring cooperation between hardware and software manufacturers,have not penetrated the marketplace. the safeguards deployed by softwarevendors are usually incomplete and after a while succumb to attacks by talentedamateur hackers who produce copyable versions of the protected disks. thereeven exist programs to help a user overcome the protections of many availableproprietary programs. (these thieving programs are then presumablythemselves copied through use of their own devices!) it should be pointed outthat there is even a debate as to whether the prevalent theft of proprietarypersonal computing software by individuals is sufficiently harmful to warrantthe cost of developing and deploying really effective countermeasures (kent,1981).the problem of copying proprietary software and databases, whileimportant, lies outside the purview of system security. software piracy is anissue between the rightful owner and the thief, and its resolution depends ontools and methods, and represents a goal, which are separate from thoseassociated with system security.appendix b262computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.there is, however, an important aspect of protection of proprietarysoftware and/or databases that lies directly within the domain of system securityas it is commonly understood. it involves the unauthorized use of proprietarysoftware and databases by parties other than the organization licensed to usesuch software or databases, and in systems other than within the organization'ssystem where the proprietary software is legitimately installed. consider, forexample, a large database with the associated complexquery software that islicensed by a vendor to an organization. this may be done with the contractualobligation that the licensee obtains the database for his own use and not formaking query services available to outsiders. two modes of transgressionagainst the proprietary rights of the vendor are possible. the organization itselfmay breach its obligation not to provide the query services to others, or someemployee who himself may have legitimate access to the database may provideor even sell query services to outsiders. in the latter case the licenseeorganization may be held responsible, under certain circumstances, for nothaving properly guarded the proprietary rights of the vendor. thus there is asecurity issue associated with the prevention of unauthorized use of proprietarysoftware or databases legitimately installed in a computing system. in thecommittee's classification of security services, it comes under the heading ofresource (usage) control. namely, the proprietary software is a resource and itsowners wish to protect against its unauthorized use (say, for sale of services tooutsiders) by a user who is otherwise authorized to access that software.resource control as a security service has inspired very few, if any,research and implementation efforts. it poses some difficult technical problems,as well as possible privacy problems. the obvious approach is to audit, on aselective and possibly random basis, access to the proprietary resource inquestion. such an audit trail can then be evaluated by human scrutiny, orautomatically, for indications of unauthorized use as defined in the presentcontext. it may well be that effective resource control will require recording, atleast on a spotcheck basis, aspects of the content of a user's interaction withsoftware and/or a database. for obvious reasons, this may provoke resistance.another security service that may come into play in this context ofresource control is nonrepudiation. the legal aspects of the protection ofproprietary rights may require that certain actions taken by a user in connectionwith the proprietary resource be such that once the actions are recorded, the useris barred from later repudiating his connection to these actions.it is clear that such measures for resource control, if properlyappendix b263computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.implemented and installed, will serve to deter the unauthorized use ofproprietary resources by individual users. but what about the organizationcontrolling the trusted system in which the proprietary resource is embedded?on the one hand, such an organization may well have the ability to dismantlethe very mechanisms designed to control the use of proprietary resources,thereby evading effective scrutiny by the vendor or its representations. on theother hand, the design and nature of security mechanisms are such that themechanisms are difficult to change selectively, and especially in a mannerensuring that their subsequent behavior will emulate the untamperedwithmode, thus making the change undetectable. thus the expert effort and peopleinvolved in effecting such changes will open the organization to danger ofexposure.there is now no documented major concern about the unauthorized use, inthe sense of the present discussion, of proprietary programs or databases. it maywell be that in the future, when the sale of proprietary databases assumeseconomic significance, the possibility of abuse of proprietary rights by licensedorganizations and authorized users will be an important issue. at that point anappropriate technology for resource control will be essential.use of passwords for authenticationpasswords have been used throughout military history as a mechanism todistinguish friends from foes. when sentries were posted, they were told thedaily password that would be given by any friendly soldier who attempted toenter the camp. passwords represent a shared secret that allows strangers torecognize each other, and they have a number of advantageous properties. theycan be chosen to be easily remembered (e.g., ''betty boop") without beingeasily guessed by the enemy (e.g., "mickey mouse"). furthermore, passwordsallow any number of people to use the same authentication method, and theycan be changed frequently (as opposed to physical keys, which must beduplicated). the extensive use of passwords for user authentication in humantohuman interactions has led to their extensive use in humantocomputerinteractions.according to the ncsc password management guideline, "a password isa character string used to authenticate an identity. knowledge of the passwordthat is associated with a user id is considered proof of authorization to use thecapabilities associated with that user id" (u.s. dod, 1985a).passwords can be issued to users automatically by a random generationroutine, providing excellent protection against commonly usedappendix b264computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.passwords. however, if the random password generator is not good, breakingone may be equivalent to breaking all. at one installation, a personreconstructed the entire master list of passwords by guessing the mapping fromrandom numbers to alphabetic passwords and inferring the random numbergenerator (mcilroy, 1989). for that reason, the random generator must base itsseed on a varying source, such as the system clock. often the user will not finda randomly selected password acceptable because it is too difficult to memorize.this can significantly decrease the advantage of random passwords, because theuser may write the password down somewhere in an effort to remember it. thismay cause infinite exposure of the password, thus thwarting all attempts tomaintain security. for this reason it can be helpful to give a user the option toaccept or reject a password, or choose one from a list. this may increase theprobability that the user will find an acceptable password.userdefined passwords can be a positive method for assigning passwordsif the users are aware of the classic weaknesses. if the password is too short,say, four digits, a potential intruder can exhaust all possible passwordcombinations and gain access quickly. that is why every system must limit thenumber of tries any user can make toward entering his password successfully. ifthe user picks very simple passwords, potential intruders can break the systemby using a list of common names or a dictionary. a dictionary of 100,000 wordshas been shown to raise the intruder's chance of success by 50 percent (mcilroy,1989). specific guidelines on how to pick passwords are important if users areallowed to pick their own passwords. voluntary password systems should guideusers to never reveal their password to other users and to change their passwordon a regular basis, a practice that can be enforced by the system. (the ncsc'spassword management guideline (u.s. dod, 1985a) represents such aguideline.)some form of access control must be provided to prevent unauthorizedpersons from gaining access to a password list and reading or modifying the list.one way to protect passwords in internal storage is by a oneway hash. thepasswords of each user are stored as ciphertext. if the passwords wereencrypted, per se, the key would be present and an attacker who gained accessto the password file could decrypt them. when a user signs on and enters hispassword, the password is processed by the algorithm to produce thecorresponding ciphertext. the plaintext password is immediately deleted, andthe ciphertext version of the password is compared with the one stored inmemory. the advantage of this technique is that passwords cannot be stolenfrom the computer (absent a lucky guess). however, a person obtainingunauthorized access could delete or change the ciphertextappendix b265computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.passwords and effectively deny service. the file of encrypted passwords shouldbe protected against unauthorized reading, to further foil attempts to guesspasswords.the longer a password is used, the more opportunities exist for exposing it.the probability of compromise of a password increases during its lifetime. thisprobability is considered acceptably low for an initial time period; after a longertime period it becomes unacceptably high. there should be a maximum lifetimefor all passwords. it is recommended that the maximum lifetime of a passwordbe no greater than one year (u.s. dod, 1985a).networks and distributed systemssecurity perimeterssecurity is only as strong as its weakest link. the methods described abovecan in principle provide a very high level of security even in a very large systemthat is accessible to many malicious principals. but implementing thesemethods throughout the system is sure to be difficult and time consuming.ensuring that they are used correctly is likely to be even more difficult. theprinciple of "divide and conquer" suggests that it may be wiser to divide a largesystem into smaller parts and to restrict severely the ways in which these partscan interact with each other.the idea is to establish a security perimeter around part of a system and todisallow fully general communication across the perimeter. instead, there aregates in the perimeter that are carefully managed and audited and that allowonly certain limited kinds of traffic (e.g., electronic mail, but not file transfersor general network "datagrams"). a gate may also restrict the pairs of sourceand destination systems that can communicate through it.it is important to understand that a security perimeter is not foolproof. if itpasses electronic mail, then users can encode arbitrary programs or data in themail and get them across the perimeter. but this is less likely to happen bymistake, and it is more difficult to do things inside the perimeter using onlyelectronic mail than to do things using terminal connections or arbitrarynetwork datagrams. furthermore, if, for example, a mailonly perimeter is animportant part of system security, users and managers will come to understandthat it is dangerous and harmful to implement automated services that acceptelectronic mail requests.as with any security measure, a price is paid in convenience and flexibilityfor a security perimeter: it is harder to do things across theappendix b266computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.perimeter. users and managers must decide on the proper balance betweensecurity and convenience.virusesa computer virus is a program that is hidden in another program (called its host) so that it runs whenever thehost program runs, and can make a copy of itself.when a virus runs, it can do a great deal of damage. in fact, it can doanything that its host can do: delete files, corrupt data, send a message with auser's secrets to another machine, disrupt the operation of a host, waste machineresources, and so on. there are many places to hide a virus: the operatingsystem, an executable program, a shell command file, or a macro in aspreadsheet or word processing program are only a few of the possibilities. inthis respect a virus is just like a trojan horse. and like a trojan horse, a viruscan attack any kind of computer system, from a personal computer to amainframe. (many of the problems and solutions discussed in this section applyequally well in a discussion of trojan horses.)a virus can also make a copy of itself, into another program or evenanother machine that can be reached from the current host over a network, or bythe transfer of a floppy disk or other removable medium. like a living creature,a virus can spread quickly. if it copies itself just once a day, then after a weekthere will be more than 50 copies (because each copy copies itself), and after amonth about a billion. if it reproduces once a minute (still slow for a computer),it takes only half an hour to make a billion copies. their ability to spreadquickly makes viruses especially dangerous.there are only two reliable methods for keeping a virus from doing harm: make sure that every program is uninfected before it runs. prevent an infected program from doing damage.keeping a virus outsince a virus can potentially infect any program, the only sure way to keepit from running on a system is to ensure that every program run comes from areliable source. in principle this can be done by administrative and physicalmeans, ensuring that every program arrives on a disk in an unbroken wrapperfrom a trusted supplier. inappendix b267computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.practice it is very difficult to enforce such procedures, because they rule out anykind of informal copying of software, including shareware, public domainprograms, and spreadsheets written by a colleague. moreover, there have beennumerous instances of virusinfected software arriving on a disk freshly shrinkwrapped from a vendor. for this reason, vendors and at least one tradeassociation (adapso) are exploring ways to prevent contamination at thesource. a more practical method uses digital signatures.informally, a digital signature system is a procedure that one can run on acomputer and that should be believed when it says, "this input data came fromthis source" (a more precise definition is given below). with a trusted sourcethat is believed when it says that a program image is uninfected, one can makesure that every program is uninfected before it runs by refusing to run it unless a certificate says, "the following program is uninfected," followed by thetext of the program, and the digital signature system says that the certificate came from the trustedsource.each place where this protection is applied adds to security. to make theprotection complete, it should be applied by any agent that can run a program.the program image loader is not the only such agent; others include the shell, aspreadsheet program loading a spreadsheet with macros, or a word processingprogram loading a macro, since shell scripts, macros, and so on are all programsthat can host viruses. even the program that boots the machine should apply thisprotection when it loads the operating system. an important issue is distributionof the public key for verifying signatures (see "digital signatures," above).preventing damagebecause there are so many kinds of programs, it may be hard to live withthe restriction that every program must be certified as uninfected. this means,for example, that a spreadsheet cannot be freely copied into a system if itcontains macros. because it might be infected, an uncertified program that isrun must be prevented from doing damageš leaking secrets, changing data, orconsuming excessive resources.access control can do this if the usual mechanisms are extended to specifyprograms, or a set of programs, as well as users. for example, the form of anaccess control rule could be "user a running program b can read" or "set ofusers c running set of programs d can read and write." then a set of uninfectedprograms can be defined, namelyappendix b268computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.the ones that are certified as uninfected, and the default access control rule canbe "user running uninfected" instead of "user running anything." this ensuresthat by default an uncertified program will not be able to read or write anything.a user can then relax this protection selectively if necessary, to allow theprogram access to certain files or directories.note that strong protection on current personal computers is ultimatelyimpossible, since they lack memory protection and hence cannot ultimatelyenforce access control. yet most of the damage from viruses has involvedpersonal computers, and protection has frequently been sought from socalledvaccine programs.providing and using vaccinesit is well understood how to implement the complete protection againstviruses just described, but it requires changes in many places: operatingsystems, command shells, spreadsheet programs, programmable editors, andany other kinds of programs, as well as procedures for distributing software.these changes ought to be implemented. in the meantime, however, variousstopgap measures can help somewhat. generally known as vaccines, they arewidely available for personal computers.the idea behind a vaccine is to look for traces of viruses in programs,usually by searching the program images for recognizable strings. the stringsmay be either parts of known viruses that have infected other systems, orsequences of instructions or operating system calls that are consideredsuspicious. this idea is easy to implement, and it works well against knownthreats (e.g., specific virus programs), but an attacker can circumvent it withonly a little effort. for example, many viruses now produce pseudorandominstances of themselves using encryption. vaccines can help, but they do notprovide any security that can be relied on. they are ultimately out of date assoon as a new virus or a strain of a virus emerges.application gatewayswhat a gateway isthe term "gateway" has been used to describe a wide range of devices inthe computer communication environment. most devices described as gatewayscan be categorized as one of two major types, although some devices aredifficult to characterize in this fashion.appendix b269computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved. the term "application gateway" usually refers to devices that convertbetween different protocol suites, often including applicationfunctionality, for example, conversion between decnet and snaprotocols for file transfer or virtual terminal applications. the term "router" is usually applied to devices that relay and route packetsbetween networks, typically operating at layer 2 (lan bridges) or layer 3(internetwork gateways). these devices do not convert between protocolsat higher layers (e.g, layer 4 and above).mail gateways, devices that route and relay electronic mail (a layer7application) may fall into either category. if the device converts between twodifferent mail protocols, for example, x.400 and smtp, then it is an applicationgateway as described above. in many circumstances an x.400 message transferagent (mta) will act strictly as a router, but it may also convert x.400electronic mail to facsimile and thus operate as an application gateway. themultifaceted nature of some devices illustrates the difficulty of characterizinggateways in simple terms.gateways as access control devicesgateways are often employed to connect a network under the control ofone organization (an internal network) to a network controlled by anotherorganization (an external network such as a public network). thus gateways arenatural points at which to enforce access control policies; that is, the gatewaysprovide an obvious security perimeter. the access control policy enforced by agateway can be used in two basic ways:1. traffic from external networks can be controlled to preventunauthorized access to internal networks or the computer systemsattached to them. this means of controlling access by outside users tointernal resources can help protect weak internal systems from attack.2. traffic from computers on the internal networks can be controlled toprevent unauthorized access to external networks or computer systems.this access control facility can help mitigate trojan horse concerns byconstraining the telecommunication paths by which data can betransmitted outside an organization, as well as supporting conceptssuch as release authority, that is, a designated individual authorized tocommunicate on behalf of an organization in an official capacity.both application gateways and routers can be used to enforce accesscontrol policies at network boundaries, but each has its own advantages anddisadvantages, as described below.appendix b270computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.application gateways as pac devicesbecause an application gateway performs protocol translation at layer 7, itdoes not pass through packets at lower protocol layers. thus, in normaloperation, such a device provides a natural barrier to traffic transiting it; that is,the gateway must engage in significant explicit processing in order to convertfrom one protocol suite to another in the course of data transiting the device.different applications require different protocolconversion processing. hence agateway of this type can easily permit traffic for some applications to transit thegateway while preventing the transit of other traffic, simply by not providingthe software necessary to perform the conversion. thus, at the coarsegranularity of different applications, such gateways can provide protection ofthe sort described above.for example, an organization could elect to permit electronic mail (email)to pass bidirectionally by putting in place a mail gateway while preventinginteractive login sessions and file transfers (by not passing any traffic otherthan email). this access control policy could be refined also to permitrestricted interactive login, for example, that initiated by an internal user toaccess a remote computer system, by installing software to support thetranslation of the virtual terminal protocol in only one direction (outbound).an application gateway often provides a natural point at which to requireindividual user identification and authentication information for finergranularity access control. this is because many such gateways require humanintervention to select services in translating from one protocol suite to another,or because the application being supported intrinsically involves humanintervention, for example, virtual terminal or interactive database query. in suchcircumstances it is straightforward for the gateway to enforce access control onan individual user basis as a byproduct of establishing a "session" between thetwo protocol suites.not all applications lend themselves to such authorization checks,however. for example, a file transfer application may be invoked automaticallyby a process during off hours, and thus no human user may be present toparticipate in an authentication exchange. batch database queries or updates aresimilarly noninteractive and might be performed when no "users" are present. insuch circumstances there is a temptation to employ passwords for useridentification and authentication, as though a human being were present duringthe activity, and the result is that these passwords are stored in files at theinitiating computer system, making them vulnerable to disclosure (see"authentication" in chapter 3). thus there are limitations on the use ofapplication gateways for individual access control.appendix b271computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.as noted elsewhere in this report, the use of cryptography to protect userdata from source to destination (endtoend encryption) is a powerful tool forproviding network security. this form of encryption is typically applied at thetop of the network layer (layer 3) or the bottom of the transport layer (layer 4).endtoend encryption cannot be employed (to maximum effectiveness) ifapplication gateways are used along the path between communicating entities.the reason is that these gateways must, by definition, be able to accessprotocols at the application layer, above the layer at which the encryption isemployed. hence the user data must be decrypted for processing at theapplication gateway and then reencrypted for transmission to the destination(or to another application gateway). in such an event the encryption beingperformed is not really endtoend.if an applicationlayer gateway is part of the path for (endtoend)encrypted user traffic, then one will, at a minimum, want the gateway to betrusted (since it will have access to the user data in clear text form). note,however, that use of a trusted computing base (tcb) for a gateway does notnecessarily result in as much security as if (uninterrupted) encryption were inforce from source to destination. the physical, procedural, and emanationssecurity of the gateway must also be taken into account as breaches of any ofthese security facets could subject a user's data to unauthorized disclosure ormodification. thus it may be especially difficult, if not impossible, to achieveas high a level of security for a user's data if an application gateway is traversedas the level obtainable using endtoend encryption in the absence of suchgateways.in the context of electronic mail the conflict between endtoendencryption and application gateways is a bit more complex. the securemassaging facilities defined in x.400 (ccitt, 1989a) allow for encrypted email to transit mtas without decryption, but only when the mtas areoperating as routers rather than as application gateways, for example, when theyare not performing "content conversion" or similar invasive services. theprivacyenhanced mail facilities developed for the tcp/ip internet (linn, 1989)incorporate encryption facilities that can transcend email protocols, but only ifthe recipients are prepared to process the decrypted mail in a fashion thatsuggests protocollayering violation. thus, in the context of email, only thosedevices that are more akin to routers than to application gateways can be usedwithout degrading the security offered by true endtoend encryption.routers as pac devicessince routers can provide higher performance and greater robustness andare less intrusive than application gateways, access controlappendix b272computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.facilities that can be provided by routers are especially attractive in manycircumstances. also, user data protected by endtoend encryption technologycan pass through routers without having to be decrypted, thus preserving thesecurity imparted by the encryption. hence there is substantial incentive toexplore accesscontrol facilities that can be provided by routers.one way a router at layer 3 (and to a lesser extent at layer 2) can effectaccess control is through the use of "packet filtering" mechanisms. a routerperforms packet filtering by examining protocol control information (pci) inspecified fields in packets at layer 3 (and perhaps at layer 4). the router acceptsor rejects (discards) a packet based on the values in the fields as compared to aprofile maintained in an accesscontrol database. for example, source anddestination computer system addresses are contained in layer3 pci, and thus anadministrator could authorize or deny the flow of data between a pair ofcomputer systems based on examination of these address fields.if one "peeks" into layer4 pci, an eminently feasible violation of protocollayering for many layer3 routers, one can effect somewhat finergrained accesscontrol in some protocol suites. for example, in the tcp/ip suite one candistinguish among electronic mail, virtual terminal, and several other types ofcommon applications through examination of certain fields in the tcp header.however, one cannot ascertain which specific application is being accessed viaa virtual terminal connection, and so the granularity of such access control maybe more limited than in the context of application gateways. several vendors oflayer3 routers already provide facilities of this sort for the tcp/ip community,so that this is largely an existing accesscontrol technology.as noted above, there are limitations to the granularity of access controlachievable with packet filtering. there is also a concern as to the assuranceprovided by this mechanism. packet filtering relies on the accuracy of certainprotocol control information in packets. the underlying assumption is that ifthis header information is incorrect, then packets will probably not be correctlyrouted or processed, but this assumption may not be valid in all cases. forexample, consider an accesscontrol policy that authorizes specified computerson an internal network to communicate with specified computers on an externalnetwork. if one computer system on the internal network can masquerade asanother authorized internal system (by constructing layer3 pci with incorrectnetwork addresses), then this accesscontrol policy could be subverted.alternatively, if a computer system on an external network generates packetswith false addresses, it too can subvert the policy.other schemes have been developed to provide more sophisticatedappendix b273computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.accesscontrol facilities with higher assurance, while still retaining most of theadvantages of routerenforced access control. for example, the visa system(estrin and tsudik, 1987) requires a computer system to interact with a routeras part of an explicit authorization process for sessions across organizationalboundaries. this scheme also employs a cryptographic checksum applied toeach packet (at layer 3) to enable the router to validate that the packet isauthorized to transit the router. because of performance concerns, it has beensuggested that this checksum be computed only over the layer3 pci, instead ofthe whole packet. this would allow information surreptitiously tacked onto anauthorized packet pci to transit the router. thus even this more sophisticatedapproach to packet filtering at routers has security shortcomings.conclusions about gatewaysboth application gateways and routers can be used to enforce accesscontrol at the interfaces between networks administered by differentorganizations. application gateways, by their nature, tend to exhibit reducedperformance and robustness, and are less transparent than routers, but they areessential in the heterogeneous protocol environments in which much of theworld operates today. as national and international protocol standards becomemore widespread, there will be less need for such gateways. thus, in the longterm, it would be disadvantageous to adopt security architectures that requirethat interorganizational access control (across network boundaries) be enforcedthrough the use of such gateways. the incompatibility between true endtoendencryption and application gateways further argues against such accesscontrolmechanisms for the long term.however, in the short term, especially in circumstances where applicationgateways are required due to the use of incompatible protocols, it is appropriateto exploit the opportunity to implement perimeter access controls in suchgateways. over the long term, more widespread use of trusted computersystems is anticipated, and thus the need for gatewayenforced perimeter accesscontrol to protect these computer systems from unauthorized external accesswill diminish. it is also anticipated that increased use of endtoend encryptionmechanisms and associated access control facilities will provide security forenduser data traffic. nonetheless, centrally managed access control forinterorganizational traffic is a facility that may best be accomplished throughthe use of gatewaybased access control. if further research can provide higherassurance packetfiltering facilities in routers, the resulting system, incombination with trusted computing systems forappendix b274computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.end users and endtoend encryption, would yield significantly improvedsecurity capabilities in the long term.notes1. see tcsec section 3.2.3.1.1 (u.s. dod, 1985d).2. to appreciate cryptography, note that we do not always understand what ''information" is.information, in the sense of semantic content, is always in the mind of the beholder and is acombination of ordinary symbols (e.g., "east wind, rain") or extraordinary ones (e.g., wehrmachtbeer orders) and some richer context. to differentiate, "data" is an encoding, and "information" isthe (always to some degree unknowable) meaning that the encoding may or may not convey to ahuman observer. with regard to automata, "information" refers to data that alters the behavior of therobots.for example, the string rdaqn qrhih fecca drswv kikss hspax cubs conveys 34characters of data to everyone who has "read" access to this transaction but conveys a significantamount of information only to those who know the richer context of cryptosystem and key. readersare invited to determine the key from the substantial hint that the plaintext is there are morethings in heaven and earth; solutions may be verified by transforming rcvqd alcfvcllll dlsck krvkt brvao avua from data to information.3. the security of rsa is not known to be provably equivalent to the problem of factoring themodulus, although that seems to be the best way to attack it.appendix b275computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.appendix cemergency response teamsin the aftermath of the internet worm incident has come a flurry ofattempts to anticipate the next occurrences of a virus, propagating trojan horse,or other widespread attack. as a result, several emergency response teamsoffering 24hour service have been established, including the following: the computer emergency response team (cert): formed by thedefense advanced research projects agency and centered at thesoftware engineering institute at carnegie mellon university, certprovides access to technical experts around the country. cert is intendedto provide both incidentprevention and incidentresponse services. it wasan outgrowth of the november 1988 internet worm incident, which wasmanaged and resolved by an informal network of internet users andadministrators. cert was established to provide the capability for a moresystematic and structured response; in particular, it is intended to facilitatecommunication during system emergencies. another role that has evolvedis communication with vendors about software weaknesses orvulnerabilities that have emerged through practical experience withattacks on systems. cert draws on the computer system user anddevelopment communities, and it also coordinates with the nationalinstitute of standards and technology and the national security agency.it sponsors workshops to involve its constituents in defining its role and toshare information about perceived problems and issues (scherlis et al.,1990). the defense data network (ddn) security coordination center (ssc):created by the defense communications agency at sri international toserve the (unclassified) ddn community as a clearinghouse for host anduser security problems and fixes, the ssc expands on theappendix c276computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.functions provided by sri through the network information center (nic)that has served milnet users but was not set up to address securityproblems. interestingly, the ssc was launched after darpa's cert inrecognition of the fact that there was no central clearinghouse tocoordinate and disseminate securityrelated fixes to milnet users (dca,1989). the computer incident advisory capability (ciac): this capability wasestablished by lawrence livermore national laboratory to providecerttype services for classified and unclassified computing within thedepartment of energy (doe). the scale of doe computer operations andattendant risks provided a strong motivation for an agencyspecificmechanism; the doe community has over 100,000 computers located atover 70 classified and unclassified sites. like the defensecommunications agency, doe saw that a "central capability foranalyzing events, coordinating technical solutions, ensuring thatnecessary information is conveyed to those who need such information,and training others to deal with computer security incidents is essential."doe was able to draw on an established research capability in thecomputer security arena, at lawrence livermore national laboratory(schultz, 1990).because of the rapidity with which computer pest programs can spreadboth within the united states and worldwide, it is vital that such efforts be wellinformed, coordinated with one another, and ready to mobilize rapidly inemergencies. note that none of these systems has yet been tested with a fullscale emergency on the scale of the internet worm.appendix c277computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.appendix dmodels for gsspthis section discusses three areas in which technical standards are set bythe kind of private sectorpublic sector interaction that this committee isrecommending for generally accepted system security principles (gssp): thebuilding codes, the underwriters laboratories, inc., and the financialaccounting standards board. the latter organization is responsible for whathave been called generally accepted accounting principles (gaap), a set ofstandards that provides a model for the gssp proposal.setting standardsšprecedentsbuilding codesbuilding codes endeavor to establish standards for safe construction. thefield is marked by extreme decentralization, with codes mandated and enforcedby local municipalities. the quality of code enforcement depends on theparticular code enforcement officials (falk, 1975). the codes themselves arebased on socalled model codes that are produced by a small number ofcompeting organizations. these codewriting organizations are associations ofenforcement officers and therefore can be thought of as representing thegovernment sector exclusively. there is, however, significant private sectorinput into the process from the various materials suppliers and their tradeassociations.building codes contain both performance and specification standards. apure performance standard would stipulate something like, "walls of residencesmust resist the spread of fire to the degree necessaryappendix d278computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.to allow occupants to escape." such standards, because they are so difficult toevaluate (the only true test of failure would be in an actual fire) are generallyrecast in a testable form, such as, "materials used in residence walls must resistan x degree fire for y minutes." upholding even this standard requires theexistence of testing capabilities that may be beyond the resources of anenforcement activity, and so the pressure from the evaluation community is forspecification standards, such as, "residence walls must be covered with adouble layer of 3/4inch sheetrock."performance standards are viewed as being fairer and as providing greaterroom for innovation, but they impose a much greater burden on the evaluators.building codes have been widely criticized as inhibiting innovation andraising construction costs by mandating outdated materials and labor practices.in part, this is a natural byproduct of the specification approach, which militatesagainst new technologies that deviate from the required specifications. in somecases the problem reflects local failures to adopt the latest revisions to modelcodes (falk, 1975).underwriters laboratories, inc.underwriters laboratories, inc. (ul) was established essentially by anentrepreneurial process because insurance companies could not rate the hazardsresulting from new technology, in this case, electric lighting. it began as apurely private sector activity and then, because of the quality of its work,became recognized by the government. it operates as both a standardsettingand an evaluation organization, issuing its famous "seal of approval" toequipment and components that meet its standards (underwriters laboratories,inc., 1989, 1990b). as described by one journalist,the ul mark – means that the equipment has been checked for potentialhazards, using objective tests laid out in detailed handbooks called standards.no federal law mandates such testing. but ul's clients, manufacturers whopay to have their products tortured and then listed by the lab, know that themark is an important selling point. (williams, 1988, p. 79)underwriters laboratories, inc., has developed a preliminary draft of asoftware safety standard, scheduled to be completed in 1990 (underwriterslaboratories, inc., 1990a). it is forming an industry advisory committee, opento interested parties, to assist it in drafting a formal ul standard. burglaryprotection systems, motor control mechanisms (e.g., for temperature, speed),industrial computers (i.e., programmable machines), "smart" appliances, andmedical devices have been identified by ul as having software that affectssafety andappendix d279computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.thus should be evaluated. note, however, that ul is a public safetyorganization. it does not necessarily deal with certification, verification, and soon, unless a device affects safety.financial accounting standards boardthe history of the financial accouting standards board (fasb) dates tothe stock market crash of 1929 and the entry of the government into the capitalmarkets through the establishment of the securities and exchange commission(sec). in the late 1930s, when sec activism was at a peak, the americaninstitute of certified public accountants formed a parttime and volunteeraccounting practices board to set accounting standards. the clear aim of thisactivity was to forestall governmentmandated standards; this aim persists infasb's own description of what causes a standard to be promulgated, wherepotential sec or congressional action is explicitly mentioned as a criterion indeciding whether a new standard is needed. overwhelmed by the changes in thefinancial markets in the 1960s, the accounting practices board instituted astudy in the early 1970s that led to the establishment of a fulltime independentinstitute, the financial accounting foundation (faf), to oversee the fasb andthe production of what have been referred to as generally accepted accountingprinciples (gaap) and other standards of financial accounting and reporting forprivate sector organizations. similar standards are established by a newer sisterunit of the fasb for the public sector, the government accounting standardsboard (gasb). according to its own literature,the mission of the financial accounting standards board is to establish andimprove standards of financial accounting and reporting for the guidance andeducation of the public, including issuers, auditors, and users of financialinformation.–the fasb develops broad accounting concepts as well as standards forfinancial reporting. it also provides guidance on implementation of standards.–the board's work on both concepts and standards is based on researchconducted by the fasb staff and by others. (fasb, 1990)the financial accounting foundation, fasb, and gasb serve tomaintain the independence of the accounting profession by providing aneffective alternative to government regulation. the effectiveness of thealternative rests on the use of standards to maintain what is called the "decisionusefulness" of accounting information. in simplified form, accountinginformation has decision usefulness if the standards under which it wasgenerated permit meaningful comparison of financial data from differentcompanies that are competing for capital (e.g., from potential purchasers ofcommon stock). accounting standardsappendix d280computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.differ from engineering standards in that they are not subject to verification byexperiment (e.g., failure of a beam under loading) and their wording balancesthe concerns of buyers and sellers in the capital markets.in order to achieve this balance, the fasb has established an elaborate dueprocess for the establishment of standards. the process appears to workreasonably well; the primary criticisms levied against the fasb are those of"standards overload," in which the establishment of a fulltime standardssettingbody has had the not surprising outcome that a large number of standards havebeen established. this prolificness combined with the large number ofpracticing accountants may be one reason why the faf has earned some $10million in revenue from sales of publications (faf, 1990). also, the fasb andgasb are independent of relevant professional organizations.at the end of its first decade the fasb received approximately 40 percentof its financial support from the accounting profession and 60 percent fromoutside sources such as financial institutions and banks. more recently, thefasb has run deficits, in part because it "has always had the delicate problemof having to seek contributions from the very companies it sometimes alienates"(cowan, 1990). the faf considers contributions as essential to its viability(faf, 1990).the fasb and the gaap can be viewed as a modified or hybrid form ofprofessional selfregulation, in which a professional community, under constantthreat of government intervention, prevents that intervention by satisfactorilyhandling the various problems themselves. the gaap have force of law in thattheir use is required for financial reporting by companies that raise capital in theregulated markets. they are recognized as authoritative by the sec (sprouse,1987). the sec and the general accounting office maintain liaison with boththe fasb and gasb.lessons relevant to establishing gsspeach of the undertakings discussed in this appendix offers lessons that arerelevant to the concept of gssp and the manner in which gssp may be definedand enforced.the experience with building codes indicates clearly that havingcompeting standards and decentralized evaluation and enforcement iscounterproductive; these factors inhibit technological progress. it is also clearthat any set of standards will always have some mix of performance andspecification requirements. it appears to be a fundamental principle of standardsand evaluation that performance standards permit more rapid evolution than dospecification standards,appendix d281computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.but at the cost of difficulty of evaluation. note that in both building code andcomputer security experience, major innovations have taken some ten years togo from concept to general acceptance.the ul experience shows that an evaluation process can be initiated in theprivate sector and then accepted by government, and that it is not necessary tobegin such an activity with a legal or administrative mandate. the fasb is alsoan example of a private effort that achieved government recognition.the fasb's history shows quite clearly that a forcing function is neededboth initially and in the long term. in the case of the fasb it is the threat ofgovernment regulation of a particular profession. the experience with thefasb, and to a lesser extent the building codes, shows the importance ofdetermining, by consensus, standards that balance the interests of all involvedparties, and of setting up those standards according to a due process. thefasb's history also illustrates the importance of institutional independence inbalancing pressures and criticisms from interested parties.those concerned with setting standards for computer security shouldnevertheless be cautious in drawing too close an analogy to the fasb.computer security does not involve an organized, recognized profession whoseprerogatives are threatened. much less money is involved (at least directly), anda clear forcing function, either in the form of an initiating incident or ongoingthreat of government action, is not present, although a liability crisis for systemvendors, were it to develop, could serve that purpose.appendix d282computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.appendix ehighgrade threatslt is impossible to build systems that are guaranteed to be invulnerable to ahighgrade threat, that is, a dedicated and resourceful adversary capable of andmotivated to organize an attack as an industrial rather than an individual orsmallgroup enterprise. such activities have historically been conducted by theintelligencegathering activities of governments and have generally posed athreat to the confidentiality of information. the rapidly decreasing cost ofcomputer resources, the rapid spread of computer technology, and the increasedvalue of informationbased assets make it likely that highgrade threats will beencountered from other sources and with aims other than traditional espionage.a highgrade threat is distinguished from the common "hacker" or criminal bythe following characteristics: the threat has extensive resources in money, personnel, and technology.in particular, the threat is able to construct or acquire, by legitimate orclandestine means, a duplicate of the system under attack. the attackteam can then conduct extensive analysis and experimentation without therisk that their activities will alert the administrators of the target system.the attacker may also have more powerful computer resources. the threat is patient and motivated. the attack resembles anentrepreneurial enterprise in that the equivalent to risk capital is raised inadvance and invested in anticipation of a major future reward. the attackis conducted as a fulltime, organized effort with a multidisciplinary staff,each of whom is eager to "break" the system. the threat is capable of exploiting a successful attack for maximum longterm gain. in particular, the attacking team is able to takeappendix e283computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.extraordinary measures to keep the existence of a successful attack secretfrom the target. the threat is adept in circumventing physical and procedural safeguardsand has access to clandestine technology. the threat will deliberately seek the most obscure vulnerability hidden inthe darkest corner of the systemšon the grounds that this is the one thatwill permit the maximum longterm exploitation.1the designers, implementors, and administrators of highgradecountermeasures must begin with the requirement that their system be safe fromhacker or criminal attacks and then work to counter the specialized threat oflargescale, longterm, highly covert assaults. hacker and criminal attacks mustbe prevented to preclude the highgrade attacker from obtaining "insideinformation" about the target system from cheap (if shortlived) penetrationsand to ensure that the operation of the system is as stable as possible.the functionality of system elements engineered to highgrade securitystandards must be even more modest than the functionality that is affordable forelements engineered to withstand hacker and criminal attacks. highgradecountermeasure engineering has traditionally been associated withcommunications security devices and subsystems; the committee anticipatesthat it will, in the future, be applied to selected computer security functions suchas reference monitors. in particular, this committee does not foresee that it willever be feasible to apply highgrade countermeasures to a multitude of systemelements, since technical advances that benefit the designer of countermeasuresoften benefit the attacker even more.2 this circumstance has importantimplications for the systemwide tradeoffs that have to be made when a highgrade threat is considered.the inevitability of "tunneling" attacks has to be taken into account and theanalysis and control carried down to the lowest possible layer of abstraction. atunneling attack attempts to exploit a weakness in a system that exists at a levelof abstraction lower than that used by the developer to design and/or test thesystem. for example, an attacker might discover a way to modify the microcodeof a processor that is used when encrypting some data, rather than attempting tobreak the system's encryption scheme. the requirement that tunneling attacksbe anticipated can substantially increase the cost of highgradecountermeasures, because it can preclude the use of offshore components (in thecase of national security systems) or components made by commercial rivals (inthe case of industrial systems.)a higher emphasis on reliability is required, because a highgrade threatmust be assumed to have the ability to monitor system behavior and takeadvantage of component failures. this raises cost andappendix e284computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.lengthens the schedule in several ways; for example, adding redundancyincreases both hardware and software costs.finally, the knowledge that a highgrade threat is waiting to attack asystem or component leads developers of highgrade countermeasures tosurround their system development with the most extreme forms of secrecy, soas to deny the attacker lead time in analyzing the design and developing attacks.because of the extreme cost, short ''security life," and difficult tradeoffsassociated with highgrade countermeasures, operations that assess a highgradethreat as possible but not likely should seriously consider strategies that focuson recovery from, rather than prevention of, attack.notes1. designers of countermeasures who anticipate hacker or common criminal attacks can ignore largeclasses of vulnerabilities on the grounds that there are easier ways to attack a system, because thelowgrade threat will look for the easiest way in.2. for example, as highspeed digital encryption system chips become more readily available, theymay be used to encrypt specific data channels within a computer system. however, they may also beused by attackers to build specialpurpose machines capable of breaking the encryption algorithmitself.appendix e285computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.appendix fglossaryaccessa subject's right to use an object. examples include read and write accessfor data objects, execute access for programs, or create and delete accessfor directory objects.access controlthe granting or denying to a subject (principal) of certain permissions toaccess an object, usually done according to a particular security model.access control lista list of the subjects that are permitted to access an object, and the accessrights of each subject.access labelsee label.access levela level associated with a subject (e.g., a clearance level) or with an object(e.g., a classification level).accountabilitythe concept that individual subjects can be held responsible for actions thatoccur within a system.accreditation1. the administrative act of approving a computer system for use in aparticular application. see certification. 2. the act of approving anorganization as, for example, an evaluation facility.appendix f286computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.administrativelydirectedaccess control (adac)access control in which administrators control who can access whichobjects. contrast with userdirected access control (udac). seemandatory access control.assuranceconfidence that a system design meets its requirements, or that itsimplementation meets its specification, or that some specific property issatisfied.auditingthe process of making and keeping the records necessary to supportaccountability. see audit trail analysis.audit trailthe results of monitoring each operation of subjects on objects; forexample, an audit trail might be a record of all actions taken on aparticularly sensitive file.audit trailanalysisexamination of an audit trail, either manually or automatically, possibly inreal time (lunt, 1988).authenticationproviding assurance regarding the identity of a subject or object, forexample, ensuring that a particular user is who he claims to be.authentication sequencea sequence used to authenticate the identity of a subject or object.authorizationdetermining whether a subject (a user or system) is trusted to act for agiven purpose, for example, allowed to read a particular file.availabilitythe property that a given resource will be usable during a given time period.bell and lapadulamodelan informationflow security model couched in terms of subjects andobjects and based on the concept that information shall not flow to anobject of lesser or noncomparable classification (bell and la padula, 1976).appendix f287computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.beta testinguse of a product by selected users before formal release.biba modelan integrity model in which no subject may depend on a less trusted object(including another subject) (biba, 1975).capabilityan authenticating entity acceptable as evidence of the right to performsome operation on some object.certificationthe administrative act of approving a computer system for use in aparticular application. see accreditation.cesgthe communicationselectronics security group of the u.k. governmentcommunications headquarters (gchq).challengeresponsean authentication procedure that requires calculating a correct response toan unpredictable challenge.checksumdigits or bits summed according to arbitrary rules and used to verify theintegrity of data.ciphertextthe result of transforming plaintext with an encryption algorithm. alsoknown as cryptotext.claims languagein the itsec, the language that describes the desired security features of a"target of evaluation" (a product or system), and against which the productor system can be evaluated.clarkwilsonintegritymodelan approach to providing data integrity for common commercial activities,including software engineering concepts of abstract data types, separationof privilege, allocation of least privilege, and nondiscretionary accesscontrol (clark and wilson, 1987).appendix f288computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.classification levelthe security level of an object. see sensitivity label.cleanroomapproacha software development process designed to reduce errors and increaseproductivity (poore and mills, 1989).clear textunencrypted text. also known as plaintext. contrast with ciphertext,cryptotext.clearancelevelthe security level of a subject.clefin the itsec, a commercial licensed evaluation facility.cocomcoordinating committee for multilateral export controls, which beganoperations in 1950 to control export of strategic materials and technology tocommunist countries; participants include australia, belgium, canada,denmark, france, germany, greece, italy, japan, luxembourg, thenetherlands, norway, portugal, spain, turkey, the united kingdom, andthe united states.compuseccomputer security.comseccommunications security.confidentialityensuring that data is disclosed only to authorized subjects.correctness1. the property of being consistent with a correctness criterion, such as aprogram being correct with respect to its system specification, or aspecification being consistent with its requirements. 2. in itsec, acomponent of assurance (together with effectiveness).countermeasurea mechanism that reduces the vulnerability of a threat.appendix f289computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.covertchannela communications channel that allows two cooperating processes totransfer information in a manner that violates a security policy, but withoutviolating the access control.criteriadefinitions of properties and constraints to be met by system functionalityand assurance. see tcsec, itsec.criticalitythe condition in which nonsatisfaction of a critical requirement can resultin serious consequences, such as damage to national security or loss of life.a system is critical if any of its requirements are critical.cryptokeyan input to an encryption device that results in cryptotext.cryptotextsee ciphertext.dataa sequence of symbols to which meaning may be assigned. uninterpretedinformation. data can be interpreted as representing numerical bits, literalcharacters, programs, and so on. (the term is used often throughout thisreport as a collective, singular noun.) see information.data encryptionstandard(des)a popular secretkey encryption algorithm originally released in 1977 bythe national bureau of standards.delegateto authorize one subject to exercise some of the authority of another.denial ofservicereducing the availability of an object below the level needed to supportcritical processing or communication, as can happen, for example, in asystem crash.dependabilitythe facet of reliability that relates to the degree of certainty that a systemwill operate correctly.appendix f290computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.dependencethe existence of a relationship in which the subject may not work properlyunless the object (possibly another subject) behaves properly. one systemmay depend on another system.digital signaturedata that can be generated only by an agent that knows some secret, andhence is evidence that such an agent must have generated it.discretionaryaccess control (dac)an accesscontrol mechanism that permits subjects to specify the accesscontrols, subject to constraints such as changes permitted to the owner of anobject. (dac is usually equivalent to ibac and udac, although hybriddac policies might be ibac and adac.)dtidepartment of trade and industry, u.kdualusesystema system with both military and civilian applications.effectiveness1. the extent to which a system satisfies its criteria. 2. in itsec, acomponent of assurance (together with correctness).emanationa signal emitted by a system that is not explicitly allowed by itsspecification.evaluation1. the process of examining a computer product or system with respect tocertain criteria. 2. the results of that process.feature1. an advantage attributed to a system. 2. a euphemism for a fundamentalflaw that cannot or will not be fixed.firmwarethe programmable information used to control the lowlevel operations ofhardware. firmware is commonly stored in readonly memorys (roms),which are initially installed in the factory and may be replaced in the fieldto fix mistakes or to improve system capabilities.appendix f291computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.formalhaving a rigorous respect for form, that is, a mathematical or logical basis.ftlsformal toplevel specification. (see "security characteristics" in chapter 5.)functionalityas distinct from assurance, the functional behavior of a system.functionality requirements include, for example, confidentiality, integrity,availability, authentication, and safety.gatewaya system connected to different computer networks that mediates transferof information between them.gchqgovernment communications headquarters, u.k.groupa set of subjects.identitybased access control(ibac)an access control mechanism based only on the identity of the subject andobject. contrast with rulebased access control. see discretionary accesscontrol.implementationthe mechanism that (supposedly) realizes a specified design.informationdata to which meaning is assigned, according to context and assumedconventions.informationflowcontrolaccess control based on restricting the flow of information into an object.see, for example, bell and la padula model.infosecinformation security. see also compusec and comsec.appendix f292computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.integritythe property that an object is changed only in a specified and authorizedmanner. data integrity, program integrity, system integrity, and networkintegrity are all relevant to consideration of computer and system security.integritylevela level of trustworthiness associated with a subject or object.integritypolicysee policy.itarinternational traffic in arms regulations (office of the federal register,1990).itsecthe information technology security evaluation criteria, the harmonizedcriteria of france, germany, the netherlands, and the united kingdom(federal republic of germany, 1990).kernela most trusted portion of a system that enforces a fundamental property,and on which the other portions of the system depend.keyan input that controls the transformation of data by an encryption algorithm.labela level associated with a subject or object and defining its clearance orclassification, respectively. in tcsec usage, the security label consists of ahierarchical security level and a nonhierarchical security category. anintegrity label may also exist, consisting of a hierarchical integrity level anda nonhierarchical integrity category (biba, 1975).letter bomba logic bomb, contained in electronic mail, that is triggered when the mailis read.appendix f293computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.level1. the combination of hierarchical and nonhierarchical components(tcsec usage). see security level, integrity level. 2. the hierarchicalcomponent of a label, more precisely referred to as "hierarchical level" toavoid confusion. in the absence of nonhierarchical categories, the twodefinitions are identical.logic bomba trojan horse set to trigger upon the occurrence of a particular logicalevent.mandatoryaccess control (mac)1. access controls that cannot be made more permissive by users orsubjects (general usage, roughly adac). 2. access controls based oninformation sensitivity represented, for example, by security labels forclearance and classification (tcsec usage, roughly rbac and adac).often based on information flow rules.modelan expression of a policy in a form that a system can enforce, or thatanalysis can use for reasoning about the policy and its enforcement.monitoringrecording of relevant information about each operation by a subject on anobject, maintained in an audit trail for subsequent analysis.mutual authenticationproviding mutual assurance regarding the identity of subjects and/orobjects. for example, a system needs to authenticate a user, and the userneeds to authenticate that the system is genuine.ncscthe national computer security center, part of the national securityagency, which is part of the department of defense.nodea computer system that is connected to a communications network andparticipates in the routing of messages within that network. networks areusually described as a collection of nodes that are connected bycommunications links.appendix f294computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.nondiscretionaryequivalent to mandatory in tcsec usage, otherwise equivalent toadministratively directed access controls.nonrepudiationan authentication that with high assurance can be asserted to be genuine,and that cannot subsequently be refuted.objectsomething to which access is controlled. an object may be, for example, asystem, subsystem, resource, or another subject.operatingsystema collection of software programs intended to directly control the hardwareof a computer (e.g., input/output requests, resource allocation, datamanagement), and on which all the other programs running on thecomputer generally depend. unix, vax/vms, and dos are all examplesof operating systems.orangebookcommon name for the department of defense document that is the basicdefinition of the tcsec, derived from the color of its cover (u.s. dod,1985d). the orange book provides criteria for the evaluation of differentclasses of trusted systems and is supplemented by many documents relatingto its extension and interpretation. see red book, yellow book.osiopen systems interconnection. a sevenlayer networking model.outsourcingthe practice of procuring from external sources rather than producingwithin an organization.passworda sequence that a subject presents to a system for purposes of authentication.patcha section of software code that is inserted into a program to correctmistakes or to alter the program.appendix f295computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.perimetera boundary within which security controls are applied to protect assets. asecurity perimeter typically includes a security kernel, some trustedcodefacilities, hardware, and possibly some communications channels.pinpersonal identification number. typically used in connection withautomated teller machines to authenticate a user.plaintextsee clear text.policyan informal, generally naturallanguage description of desired systembehavior. policies may be defined for particular requirements, such assecurity, integrity, and availability.principala person or system that can be authorized to access objects or can makestatements affecting access control decisions. see the equivalent, subject.private keysee secret key.protectedsubsystema program or subsystem that can act as a subject.public keya key that is made available without concern for secrecy. contrast withprivate key, secret key.publickeyencryptionan encryption algorithm that uses a public key to encrypt data and acorresponding secret key to decrypt data.ramprating maintenance phase. part of the national computer security center'sproduct evaluation process.receiverssubjects reading from a communication channel.appendix f296computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.red bookthe trusted network interpretation of the trusted computer system evaluation criteria, or tni (u.s. dod, 1987).referencemonitora system component that enforces access controls on an object.requirementa statement of the system behavior needed to enforce a given policy.requirements are used to derive the technical specification of a system.riskthe likelihood that a vulnerability may be exploited, or that a threat maybecome harmful.rsathe rivestshamiradelman public key encryption algorithm (rivest et al.,1978).rulebasedaccess control (rbac)access control based on specific rules relating to the nature of the subjectand object, beyond just their identitiesšsuch as security labels. contrastwith identitybased access control. see mandatory access control.safetythe property that a system will satisfy certain criteria related to thepreservation of personal and collective safety.secrecysee confidentiality.secretknown at most to an authorized set of subjects. (a real secret is possibleonly when the size of the set is one or less.)secret keya key that is kept secret. also known as a private key.secretkeyencryptionan encryption algorithm that uses only secret keys. also known as privatekey encryption.appendix f297computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.securechannelan information path in which the set of all possible senders can be knownto the receivers, or the set of all possible receivers can be known to thesenders, or both.security1. freedom from danger; safety. 2. computer security is protection of datain a system against disclosure, modification, or destruction. protection ofcomputer systems themselves. safeguards can be both technical andadministrative. 3. the property that a particular security policy is enforced,with some degree of assurance. 4. often used in a restricted sense to signifyconfidentiality, particularly in the case of multilevel security.securitylevela clearance level associated with a subject, or a classification level (orsensitivity label) associated with an object.securitypolicysee policy.sendera subject writing to a channel.sensitivitylabela security level (i.e., a classification level) associated with an object.separationof dutya principle of design that separates functions with differing requirementsfor security or integrity into separate protection domains. separation ofduty is sometimes implemented as an authorization rule specifying that twoor more subjects are required to authorize an operation.sharewaresoftware offered publicly and shared rather than sold.signaturesee digital signature.simple securitypropertyan informationflow rule stating that a subject at a given security level canread only from an object with a security label that is the same or lower(bell and la padula, 1976).appendix f298computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.smart carda small computer in the shape of a credit card. typically used to identifyand authenticate its bearer, although it may have other computationalfunctions.source codethe textual form in which a program is entered into a computer (e.g.,fortran).specificationa technical description of the desired behavior of a system, as derived fromits requirements. a specification is used to develop and test animplementation of a system.spoofingassuming the characteristics of another computer system or user, forpurposes of deception.statean abstraction of the total history of a system, usually in terms of statevariables. the representation can be explicit or implicit.state machinein the classical model of a state machine, the outputs and the next state ofthe machine are functionally dependent on the inputs and the present state.this model is the basis for all computer systems.stuiiia secure telephone system using endtoend privatekey encryption.stuban artifact, usually software, that can be used to simulate the behavior ofparts of a system. it is usually used in testing software that relies on thoseparts of the system simulated by the stub. stubs make it possible to test asystem before all parts of it have been completed.subjectan active entityše.g., a process or device acting on behalf of a user, or insome cases the actual useršthat can make a request to perform anoperation on an object. see the equivalent, principal.appendix f299computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.system1. a state machine, that is, a device that, given the current state and inputs,yields a set of outputs and a new state (see state machine). 2. aninterdependent collection of components that can be considered as a unifiedwhole, for example, a networked collection of computer systems, adistributed system, a compiler or editor, a memory unit, and so on.tcbsee trusted computing base.tcsecthe department of defense trusted computer system evaluation criteria(u.s. dod, 1985d). see orange book.tempestu.s. government rules for limiting compromising signals (emanations)from electrical equipment.threatthe potential for exploitation of a vulnerability.time bomba trojan horse set to trigger at a particular time.tokenwhen used in the context of authentication, a physical device necessary foruser identification.token authenticatora pocketsized computer that can participate in a challengeresponseauthentication scheme. the authentication sequences are called tokens.trapdoora hidden flaw in a system mechanism that can be triggered to circumventthe system's security.trojanhorsea computer program whose execution would result in undesired sideeffects, generally unanticipated by the user. a trojan horse program mayotherwise give the appearance of providing normal functionality.appendix f300computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.trustbelief that a system meets its specifications.trustedcomputingbase (tcb)a portion of a system that enforces a particular policy. the tcb must beresistant to tampering and circumvention. under the tcsec, it must alsobe small enough to be analyzed systematically. a tcb for security is partof the security perimeter.trustedsystema system believed to enforce a given set of attributes to a stated degree ofassurance (confidence).trustworthinessassurance that a system deserves to be trusted.tunnelingattackan attack that attempts to exploit a weakness in a system at a low level ofabstraction.user authenticationassuring the identity of a user. see authorization.userdirected accesscontrol(udac)access control in which users (or subjects generally) may alter the accessrights. such alterations may, for example, be restricted to certainindividuals by the access controls, for example, limited to the owner of anobject. contrast with administratively directed access control. seediscretionary access control.vaccinea program that attempts to detect and disable viruses.virusa program, typically hidden, that attaches itself to other programs and hasthe ability to replicate. in personal computers, ''viruses" are generallytrojan horse programs that are replicated by inadvertent human action. ingeneral computer usage, viruses are more likely to be selfreplicatingtrojan horses.vulnerabilitya weakness in a system that can be exploited to violate the system'sintended behavior. there may be security, integrity, availability, and othervulnerabilities. the act of exploiting a vulnerability represents a threat,which has an associated risk of being exploited.appendix f301computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.cscstd00385 to specific ˘0123computers at risk: safe computing in the information agecopyright national academy of sciences. all rights reserved.appendix glist of members of the formercommission on physical sciences,mathematics, and resourcesnorman hackerman, robert a. welch foundation, chairmanrobert c. beardsley, woods hole oceanographic institutionb. clark burchfiel, massachusetts institute of technologygeorge f. carrier, harvard universityralph j. cicerone, national center for atmospheric researchherbert d. doan, the dow chemical company (retired)peter s. eagleson, massachusetts institute of technologydean e. eastman, ibm t.j. watson research centermarye anne fox, university of texasgerhart friedlander, brookhaven national laboratorylawrence w. funkhouser, chevron corporation (retired)phillip a. griffiths, duke universityneal f. lane, rice universitychristopher f. mckee, university of california at berkeleyrichard s. nicholson, american association for the advancementof sciencejack e. oliver, cornell universityjeremiah p. ostriker, princeton university observatoryphilip a. palmer, e.i. du pont de nemours & companyfrank l. parker, vanderbilt universitydenis j. prager, macarthur foundationdavid m. raup, university of coloradoroy f. schwitters, superconducting super collider laboratorylarry l. smarr, university of illinois at urbanachampaignkarl k. turekian, yale universityappendix g303
detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/5769traffic management for highspeed networks: fourth lectureinternational science lecture series32 pages | 8.5 x 11 | paperbackisbn 9780309057981 | doi 10.17226/5769by h.t. kung, gordon mckay professor of electrical engineering and computerscience, harvard universitytraffic management for highspeed networks: fourth lecture international science lecture seriescopyright national academy of sciences. all rights reserved.traffic management forhighspeed networksbyh.t. kunggordon mckay professor of electrical engineering and computer scienceharvard universitynational academy presswashington, d.c. 1997itraffic management for highspeed networks: fourth lecture international science lecture seriescopyright national academy of sciences. all rights reserved.the national research council serves as an independent advisor to the federal government on scientific and technical questions ofnational importance. established in 1916 under the congressional charter of the private, nonprofit national academy of sciences, theresearch council brings the resources of the entire scientific and technical community to bear on national problems through its volunteeradvisory committees. today the research council stands as the principal operating agency of both the national academy of sciences and thenational academy of engineering and is administered jointly by the two academies and the institute of medicine. the national academy ofengineering and the institute of medicine were established in 1964 and 1970, respectively, under the charter of the national academy of sciences.the national research council has numerous operating units. one of these is the naval studies board, which is charged with conducting and reporting upon surveys and studies in the field of scientific research and development applicable to the operation and function of thenavy.a portion of the work done to prepare this document was performed under department of navy grant n000149410200 issued by theoffice of naval research and the air force office of scientific research under contract authority nr 201124. however, the content doesnot necessarily reflect the position or the policy of the department of the navy or the government, and no official endorsement should beinferred.the united states government has at least a royaltyfree, nonexclusive, and irrevocable license throughout the world for governmentpurposes to publish, translate, reproduce, deliver, perform, and dispose of all or any of this work, and to authorize others so to do.copyright 1997 by the national academy of sciences. all rights reserved.additional copies of this report available from:naval studies boardnational research council2101 constitution avenue, n.w.washington, d.c. 20418printed in the united states of americaiitraffic management for highspeed networks: fourth lecture international science lecture seriescopyright national academy of sciences. all rights reserved.naval studies boarddavid r. heebner, science applications international corporation (retired), chairgeorge m. whitesides, harvard university, vice chairalbert j. baciocco, jr., the baciocco group, inc.alan berman, applied research laboratory, pennsylvania state universitynorman e. betaque, logistics management institutenorval l. broome, mitre corporationgerald a. cann, raytheon companyseymour j. deitchman, chevy chase, maryland, special advisoranthony j. demaria, demaria electrooptics systems, inc.john f. egan, lockheed martin corporationrobert hummel, courant institute of mathematics, new york universitydavid w. mccall, far hills, new jerseyrobert j. murray, center for naval analysesrobert b. oakley, national defense universitywilliam j. phillips, northstar associates, inc.mara g. prentiss, jefferson laboratory, harvard universityherbert rabin, university of marylandjulie jch ryan, booz, allen and hamiltonharrison shull, monterey, californiakeith a. smith, vienna, virginiarobert c. spindel, applied physics laboratory, university of washingtondavid l. stanford, science applications international corporationh. gregory tornatore, applied physics laboratory, johns hopkins universityj. pace vandevender, prosperity institutevincent vitro, lincoln laboratory, massachusetts institute of technologybruce wald, center for naval analysesnavy liaison representativespaul g. blatch, office of the chief of naval operationsronald n. kostoff, office of naval researchstaffronald d. taylor, directorsusan g. campbell, administrative assistantchristopher a. hanna, project assistantmary (dixie) gordon, information officeriiitraffic management for highspeed networks: fourth lecture international science lecture seriescopyright national academy of sciences. all rights reserved.commission on physical sciences, mathematics, and applicationsrobert j. hermann, united technologies corporation, cochairw. carl lineberger, university of colorado, cochairpeter m. banks, environmental research institute of michiganlawrence d. brown, university of pennsylvaniaronald g. douglas, texas a&m universityjohn e. estes, university of california at santa barbaral. louis hegedus, elf atochem north america, inc.john e. hopcroft, cornell universityrhonda j. hughes, bryn mawr collegeshirley a. jackson, u.s. nuclear regulatory commissionkenneth h. keller, university of minnesotakenneth i. kellermann, national radio astronomy observatorymargaret g. kivelson, university of california at los angelesdaniel kleppner, massachusetts institute of technologyjohn kreick, sanders, a lockheed martin companymarsha i. lester, university of pennsylvaniathomas a. prince, california institute of technologynicholas p. samios, brookhaven national laboratoryl.e. scriven, university of minnesotashmuel winograd, ibm t.j. watson research centercharles a. zraket, mitre corporation (retired)norman metzger, executive directorivtraffic management for highspeed networks: fourth lecture international science lecture seriescopyright national academy of sciences. all rights reserved.prefacethe international science lecture series (isls) operates as a special project of the national researchcouncil's commission on physical sciences, mathematics, and applications. the series was established in 1990at the request of the office of naval research (onr) and joined in 1992 by the air force office of scientificresearch (afosr). the purpose of the series is to advance communication and cooperation within theinternational scientific community. a search committee established by the national research council (nrc)selects prominent u.s. scientists to lecture in three areas of basic scientific inquiry: ocean and meteorologicalsciences, materials science, and information science. the countries in which the lectures are to be given areselected on the basis of consultations with the international scientific community, with the science attache in u.s.embassies, with senior representatives of onrasia and onreurope, and with both onr and afosrrepresentatives in washington, d.c. wherever appropriate, each lecture in a host country is followed by formaland informal discussions with senior government, industrial, and academic representatives to expand the dialogueon research progress, problems, and areas of common interest in order to identify research opportunities that lendthemselves to greater cooperation and collaborative effort. following each tour, the formal lecture is published forwider international distribution.the fourth lecture of the series, which is presented here, is traffic management for highspeed networks byh.t. kung, gordon mckay professor of electrical engineering and computer science, harvard university. thefirst lecture in the series, the heard island experiment, was presented by walter h. munk, holder of the secretaryof the navy research chair at the scripps institution of oceanography, university of california at san diego, andthe second lecture, fountainhead for new technologies and new science, was presented by rustum roy, evanpugh professor of the solid state and professor of geochemistry, pennsylvania state university. the third lecturerwas john e. hopcroft, who is the joseph c. ford professor of computer science at cornell university and whogave the lecture, computing, communication, and the information age.professor kung's lecture tour consisted of two separate tripsšone in the far east and the other in siberia. hegave his lecture first at the chinese university of hong kong on june 5, 1995, to the computer sciencescommunity. while in hong kong, professor kung and the isls representatives from the nrc, onr, and afosralso visited the hong kong university of science and technology and the hong kong university. professor kungdelivered his lecture at these two institutions as well. on june 8 he presented his lecture at the sinoamericanjoint seminar on trends in information science held in beijing, china. discussions also were held with the staffsand faculties of tsing hua university and peking university. professor kung visited fudan university andshanghai jiaotong university on june 14 and 15 and zhejiang university in hanzhou on june 16. he gave hislecture at fudan university.the second tour took professor kung and the isls group to novosibirsk, siberia, in january 1996. they meton january 8, 1996, with the staff of the a.p. ershov institute of informaticsprefacevtraffic management for highspeed networks: fourth lecture international science lecture seriescopyright national academy of sciences. all rights reserved.systems and discussed future working relationships between the u.s. and russian information sciences researchcommunities. on january 9, the isls group met with the staffs of the institute of automation and electrometry aswell as the institute of computational technologies. professor kung presented his lecture there on january 10 andthen visited the institute for information systems and the novosibirsk state university.the national research council, the office of naval research, and the air force office of scientificresearch would like to express their appreciation to the many hostcountry representatives for their hospitality andtheir invaluable assistance in arranging professor kung's visits and the many discussions that followed the formallecture. the sponsors are also indebted to the american embassy representatives in each of the host countries andto the representatives of onrasia and onreurope for their tireless efforts to make the lecture tours a success.prefacevitraffic management for highspeed networks: fourth lecture international science lecture seriescopyright national academy of sciences. all rights reserved.contents abstract 1 why new control methods are needed 1 rapid increase in network speeds 1 network congestion problem 2 inadequacy of bruteforce approach to providing large buffers 2 use of flow control 4 control of congestion for atm networks 4 technical goals of flow control for supporting atm abr services 5 two traffic models 6 a flood control principle 6 creditbased flow control 6 credit update protocol 7 static vs. adaptive credit control 9 adaptive buffer allocation 9 receiveroriented adaptive buffer allocation 10 rationale for creditbased flow control 12 overallocation of resources to achieve high efficiency 12 linkbylink flow control to increase quality of control 13 pervc queueing to achieve a high degree of fairness 14 ratebased flow control 14 creditnet atm switch 16 experimental network configurations 18 measured performance on creditnet experimental switches 19 summary and concluding remarks 20 acknowledgments 21 references 21contentsviitraffic management for highspeed networks: fourth lecture international science lecture seriescopyright national academy of sciences. all rights reserved.contentsviiitraffic management for highspeed networks: fourth lecture international science lecture seriescopyright national academy of sciences. all rights reserved.traffic management for highspeed networksabstractnetwork congestion will increase as network speed increases. new control methods are needed, especiallyfor handling "bursty" traffic expected in very high speed networks such as asynchronous transfer mode (atm)networks. users should have instant access to all available network bandwidth when they need it, while beingassured that the chance of losing data in the presence of congestion will be negligible. at the same time, highnetwork utilization must be achieved, and services requiting guaranteed performance must be accommodated. thispaper discusses these issues and describes congestion control solutions under study at harvard university andelsewhere. motivations, theory, and experimental results are presented.why new control methods are neededrapid increase in network speedsover the past decade, the speed of computer and telecommunications networks has improved substantially.to wit: 1980s 1.5mbps (megabits per second) t1 4 or 16mbps token rings 10mbps ethernet 1990s 45mbps t3 100mbps ethernet 100mbps fddi 155mbps oc3 atm 622mbps oc12 atmwhy new control methods are needed1traffic management for highspeed networks: fourth lecture international science lecture seriescopyright national academy of sciences. all rights reserved.this rapid growth in speed is expected to continue over the next decade, because many new applications inimportant areas such as data and video will demand very high network bandwidths. these highspeed networks areintroducing major new challenges in network congestion control, as explained in the next two sections. that thehighspeed networks would make the solution for network congestion harder is contrary to what one's intuitionmight suggest.network congestion problemany network has bottlenecks or congestion points, i.e., locations where more data may arrive than thenetwork can carry. a common cause for congestion is a mismatch in speed between networks. for example, atypical highperformance local area network (lan) environment in the next several years may have thearchitecture shown in figure 1. while the servers will use new highspeed asynchronous transfer mode (atm)connections at the oc3 rate of 155 mbps, many clients will still depend on old, inexpensive but slower, 10mbpsethernet connections. data flowing from the servers at 155 mbps to the clients at 10 mbps will experiencecongestion at the interface between the atm and ethernet networks.congestion can also occur inside a network node that has multiple ports. such a node can be a switch such asan atm switch or a gateway such as a router. as depicted in figure 2, congestion arises when data, destined for asingle output port, arrive at many different input ports. the faster and more numerous these input ports are, theseverer the congestion will be.a consequence of congestion is the loss of data due to buffer overflow. for data communications in whichevery bit must be transmitted correctly, lost data will have to be retransmitted, and will result in degraded networkutilization and increased communications delay for end users.inadequacy of bruteforce approach to providing large buffersa universal solution to the problem of losing data because of congestion involves buffer memory in which acongested point can temporarily queue data directed at overloaded output ports. this use of buffer is illustrated infigure 2. however, simply providing large buffers would likely incur prohibitively high memory cost for highspeed networks, because as network speed increases, so also will the following factors: buffer overloading rate. suppose that data from multiple input ports feed to a single output port, and thatall the ports are of the same speed. if all these ports now increase their speed by a factor of x, then theoverloading rate to the node buffer will also increasefigure 1 congestion due to a mismatch in speed between 155mbps atm network and 10mbpsethernet.figure 2 congestion, in a switch or gateway, due to multiple arrivals at the same output.why new control methods are needed2traffic management for highspeed networks: fourth lecture international science lecture seriescopyright national academy of sciences. all rights reserved.by the same factor of x. if, to prevent buffer overflow, the same congestion control scheme is used aswas used before, then the feedback delay in the control system, which is a function of propagation delaysand largely independent of link speed, will remain essentially the same. an increase in link speed willtherefore demand an xfold increase in the required buffer size, if possible data loss is to be kept to thesame level as before. packet or burst size. for highspeed networks, highlevel protocols will use data packets with anincreased number of bytes, in order to reduce packet processing overhead at end systems, such as thepacket interrupt frequency at receiving hosts. these large packets introduce large bursts of data that mayarrive at congestion points at the same time. assuming the same average load as before, bursts ofincreased size imply increased overlapping of arriving bursts at congestion points. a larger buffer is thusneeded to accommodate these simultaneously arriving large bursts. transient traffic. typical transmission control protocol (tcp) sessions involve a few dozen kilobytes[19], and the required transmission time on an oc3 link at 155 mbps is only a few milliseconds. (asurvey of unix file sizes [7] has also shown a similar result for file sizes. that is, the average file lengthis only around 22 kbytes, and most files are smaller than 2 kbytes.) thus, for highspeed networks, thesesessions will not be long enough to achieve steadystate traffic flow beyond a local or metropolitan area.when facing this type of transient traffic over a wide area, traditional endtoend flow control methodssuch as tcp will incur relatively long feedback control delays, and thus such methods cannot be effectivein reducing buffer usage inside a network. bandwidth mismatch. as new networks are deployed, many of the relatively old, inexpensive, lowbandwidth networks will still be in use. as these new networks with higher and higher speeds emerge,gaps in speed between old and new networks will increase. for handling the same load, this greatermismatch in bandwidth again implies the need for larger buffers. load speed from computer sources. a single workstation or personal computer can now consume thewhole bandwidth of an oc3 link. highend computers such as servers tend to support highbandwidthnetwork interfaces that run as fast as the fastest computer networks available. one can expect that, at anypoint in time in the foreseeable future, several highperformance computers, if not just one, will alwaysbe able to saturate the fastest links in any network.to prevent data loss due to congestion, network buffers could be increased to accommodate the increase ineach of the above factors. but these factors increase independently, and the multiplicative effects of such increaseswill demand enormously large buffers. in addition, as network usage increases, so also will the expected numberof active sessions on the network and their peak bandwidths. for each session, a network node may have to bufferall the onthefly data from a distant sending host to itself when congestion occurs. the buffers occupied by thesession can be the entire tcp window if tcp is used. if there are n sessions, n times the size/capacity of thisbuffer will be needed.for all these reasons, bruteforce methods of using larger and larger buffers cannot solve the congestionproblems to be expected with highspeed networks.why new control methods are needed3traffic management for highspeed networks: fourth lecture international science lecture seriescopyright national academy of sciences. all rights reserved.use of flow controlwhen congestion persists, no amount of buffering is sufficient in the longer term; instead, each source oftraffic flowing through a bottleneck must be persuaded to send no more than its fair share of the bottleneck'scapacity. that is, proper flow control can bound the buffer requirement.this is fundamentally a feedback control problem, and many control ideas and principles apply. as depictedin figure 3, each network node, which can be switches or gateways, collects information about congestion, andinforms, directly or indirectly, the sources of data. this feedback is usually based on the amount of buffer spaceavailable or in use in the node. the sources act to control how much data they send. this control loop has a delayequal to at least twice the propagation delay between the switch and control point.control systems should seek to minimize this delay in feedback, since nodes will need to buffer any data thatarrive after the nodes signal the congestion status but before the end of the delay. moreover, the feedback controldelay should be sufficiently small so that the control system can respond in time to any changes in traffic load.control of congestion for atm networkscontrol of congestion for atm networks is of particular interest, because such networks support very highspeed connections and multimedia services. an atm network can simultaneously support multiple types ofservices of various qualities. as illustrated in figure 4, these include constant bit rate (cbr) services for voice andother fixedrate guaranteed traffic; variable bit rate (vbr) services for video; and available bit rate (abr) servicesfor data.being able to support abr for data communications represents a major advantage of atm networks overtraditional time division multiplexing (tdm) networks. under abr services, users can have instant access toavailable network bandwidth when they need it, and they do not have to hold onto unused bandwidth when they donot need it. these services are exactly what many computer users desire.in order to realize this potential of abr services for data applications, nodes or end systems in a networkneed to receive status information on buffer or bandwidth usages from downstream entities. that is, effective andefficient flow control is essential.figure 3 use of feedback control to handle. congestion.figure 4 atm network.use of flow control4traffic management for highspeed networks: fourth lecture international science lecture seriescopyright national academy of sciences. all rights reserved.technical goals of flow control for supporting atm abr servicesflow control mechanisms designed to support atm abr services should meet a variety of technical goals,including the following: data should rarely, if ever, be discarded due to exhaustion of node buffer memory. as mentioned above,such data may have to be retransmitted after a possibly lengthy timeout period, further contributing tonetwork congestion and the delay experienced by the user. network links should be used at full capacity whenever possible. for instance, if one connection sharing alink reduces the rate at which it sends, the others should increase their rates as soon as possible. inparticular, as illustrated in figure 5, the flow control mechanism should allow abr traffic to fill in,instantly, unused bandwidth left on the link after guaranteed traffic is served. all the connections that are constrained by a bottleneck link should get fair shares of that link. the flow control mechanism should be robust. loss or delay of control messages, and admission ofadditional connections while maintaining the total traffic load, for instance, should not cause increasedcongestion. the network administrator should not have to adjust any complex parameters to achieve highperformance. the flow control mechanism should have a cost commensurate with the benefits it provides.generally speaking, some existing lans such as ethernets have satisfied these goals. this explains at leastpartially why they have been used widely for data applications.new highspeed networks, such as atm networks and switched ethernets, use switches to achieve highperformance. they are unlike conventional ethernets, which use shared media. end systems on switchbasednetworks cannot monitor network congestion as easily as can end systems on sharedmedium networks. designingflow control schemes to satisfy the above technical goals for these new switchbased networksšespecially forwide area networks (wans)šis a significant challenge.figure 5 available bit rate (abr traffic filling in bandwidth slack left by guaranteed traffic, to maximize networkutilization).use of flow control5traffic management for highspeed networks: fourth lecture international science lecture seriescopyright national academy of sciences. all rights reserved.two traffic modelsany prediction of how well a flow control scheme will work requires a model for the behavior of networktraffic. a fullblown model might involve characteristics of applications and higherlevel protocols. for ourdiscussion here, it is enough to distinguish between smooth and ''bursty'' traffic.a smooth traffic source offers a constant and predictable load, or changes only on time scales that are largecompared to the amount of time the flow control mechanism takes to respond. such traffic is easy to handle well;the sources can be assigned rates corresponding to fair shares of the bottleneck bandwidth with little risk that someof them will stop sending and lead to underutilized links. furthermore, switches can use a small amount ofmemory, since bursts in traffic intensity are rare.sources of smooth traffic include voice and video with fixedrate compression. the aggregate effect of alarge number of bursty sources may also be smooth, particularly in a wan where the loads from a large numberof traffic streams are aggregated and the individual sources have relatively low bandwidth and arc uncorrelated.bursty traffic, in contrast, lacks any of the predictability of smooth traffic, as observed in some computercommunications traffic [8]. some kinds of bursts stem from users and applications. a world wide web browserclicking on a link, for instance, wants to see a page or image as soon as possible. the network cannot predict whenthe clicks will occur, nor should it smooth out the resulting traffic, since doing so would hurt the user's interactiveresponse.other sources of bursts result from network protocols that break up transfers into individual packets,windows, or rpcs, which are sent at irregular intervals. these bursts are sporadic and typically do not last longenough on a highspeed link to reach steady state over the link roundtrip time.designing flow control systems for bursty traffic is obviously much more difficult than designing controlsystems for smooth traffic. in supporting computer communications, which are generally bursty, we will have nochoice but to face the challenge of designing effective flow control for bursty traffic.a flood control principlean old principle for controlling floods suggests an approach to controlling network congestion. dams on ariver for holding floods are analogous to buffers in a network for holding excessive data.to control floods, dams are often built in series along a river, so that an upstream dam can share the load forany downstream dam. as depicted in figure 6, whenever a dam is becoming full, its upstream dams are notified tohold additional water. in this way, all the upstream dams can help reduce flooding at a downstream congestionpoint, and each upstream dam can help prevent flooding at all downstream congestion points. the capacity ofevery dam is efficiently used.creditbased flow controlan efficient way of implementing flowcontrolled atm networks is through the use of creditbased, linkbylink, pervc (virtual circuit) flow control [12, 14, 17]. as depicted in figure 7, creditbased control works likethe method of controlling floods described above. eachcreditbased flow control6traffic management for highspeed networks: fourth lecture international science lecture seriescopyright national academy of sciences. all rights reserved.downstreamtoupstream notification is implemented with a credit record message. the scheme generally worksover a vc link as follows. a link can be a physical link connecting two adjacent nodes, or a virtual circuitconnecting two remote nodes. before forwarding any data cell over the link, the sender needs to receive credits forthe vc from the receiver. at various times, the receiver sends credits to the sender indicating availability of bufferspace for receiving data cells of the vc. after having received credits, the sender is eligible to forward somenumber of data cells of the vc to the receiver according to the received credit information. each time the senderforwards a data cell of a vc, it decreases its current credit balance for the vc by one.figure 6 flood control analogy. all the dams (or buffers) on the path leading to the congestion point can helpprevent flooding (or cell loss). notifications are denoted by dashed arrows.figure 7 creditbased flow control. black dots stand for virtual circuit (vc) buffers.there are two phases in flow controlling a vc [15]. in the first buffer allocation phase, the vc is given anallocation of buffer memory in the receiver. in the second credit control phase, the sender maintains a nonnegative credit balance to ensure no overflow of the allocated buffer in the receiver.credit update protocolthe credit update protocol (cup) [12] is an efficient and robust protocol for implementing credit controlover a link. as depicted in figure 8, for each flowcontrolled vc the sender keeps a running total txcnt of all thedata cells it has transmitted, and the receiver keeps a running total fwdcnt of all the data cells it has forwarded.(if cells arc allowed to be dropped within the receiver, fwdcnt will also count these dropped cells.) the receiverwill enclose the uptodate value of fwdcnt in each credit record transmitted upstream via a credit cell. when thesender receives the credit record with value fwdcnt, it will update the credit balance, crdbal, for the vc:where bufalloc is the total number of cells allocated to the vc in the receiver.creditbased flow control7traffic management for highspeed networks: fourth lecture international science lecture seriescopyright national academy of sciences. all rights reserved.figure 8 credit update protocol (cup). the dotted arrow indicates a credit record, transmitted upstream, containingthe current value of fwdcnt at the receiver.note that the quantity computed by the sender, txcnt  fwdcnt, represents the "outstanding credits"corresponding to those cells of the vc that the sender has transmuted but the receiver has not founded. as depictedin figure 8, these cells are "inflight cells to arrive" and "cells in queue" at the time when the receiver sends creditrecord fwdcnt to the sender. thus crdbal computed by the sender using equation (1) is the proper new creditbalance, in the sense that, as long as the sunder transmits no more than crdbal cells, it will not overrun the vc'sallocated buffer in the receiver. see [12] for a scheme of using creditcheck cells periodically sent from the senderto the receiver, to recover from possible loss of data or credit cells due to link errors.the frequency at which the receiver sends credit records for a vc depends on the vc's progress. moreprecisely, each time after the receiver has founded a certain number of cells, "n2" cells [14] for some positiveinteger n2, the receiver will send a credit record upstream. the value of n2 can be set statically or adaptively (seeequation (4) below).the bufalloc value given to a vc determines the maximum bandwidth allowed to the vc by credit flowcontrol without loss of generality, we assume that the maximal peak bandwidth of any link is 1, and represent therate of a vc as a fraction of 1. for the rest of this section, we also make a simplifying assumption that all linkshave the same peak bandwidth of 1. let rtt be the roundtrip time, in cell transmission time, of the link betweenthe sender and the receiver (see figure 8), including both link propagation delays and credit processing time.assume that the receiver uses a fair scheduling policy between vcs with crdbal > 0, when forwarding cells outfrom its output link. then, if there are n active vcs competing for the same output link, the maximum averagebandwidth over rtt that the vc can achieve isnote that when there is only one vc using the output port, i.e., n = 1, the vc's bandwidth (bw) can be ashigh as bufalloc / (rtt + n2).the cup scheme is a lowerlevel and lighterweight protocol than are typical sliding window protocols usedin, e.g., x.25 and tcp. in particular, cup is not linked to retransmission of lost packets. in x.25 or tcp, loss ofany packet will stop the advancing window until the dropped packet has been retransmitted successfully. toimplement this, each data packet carries acreditbased flow control8traffic management for highspeed networks: fourth lecture international science lecture seriescopyright national academy of sciences. all rights reserved.sequence number. in contrast, in cup the sender does not retransmit lost data cells, the receiver does not reorderreceived cells, and data cells do not carry sequence numbers. this simplicity is made possible because cup needonly work for atm vcs that preserve cell order.it can be shown [14] that cup produces the same buffer management results as the wellknown"incremental" credit updating methods (see, e.g., [7, 9]). in these other methods, instead of sending fwdcntvalues upstream the receiver sends incremental credit values to be added to crdbal at the sender.static vs. adaptive credit controlwe call a creditbased flow control either static or adaptive depending on whether the buffer allocation isstatic or adaptive. in a static credit control, a fixed value of bufalloc is used for the lifetime of a vc. requiringonly the implementation of cup, or some equivalent protocol, the method is extremely simple.there are situations, however, where adaptive credit control is desirable. in order to allow a vc to operate at ahigh rate, equation (2) implies that bufalloc must be large relative to rtt + n2*n. allocating a small buffer to avc can prevent the vc from using otherwise available link bandwidth. on the other hand, committing a largebuffer to a vc can be wasteful, because sometimes the vc may not have sufficient data, or may not be able to getenough scheduling slots, to transmit at the desired high rate. the proper rate at which a vc can transmit dependson the behavior of traffic sources, competing traffic, scheduling policy, and other factors, all of which can changedynamically or may not be known a priori. in this case, adaptive credit control, which is static credit control plusadaptive adjustment of bufalloc of a vc according to its current bandwidth usage, can be attractive.generally speaking, for configurations where a large bufalloc relative to rtt + n2*n is not prohibitivelyexpensive, it may be simplest just to implement static credit control. this would give excellent performance.otherwise, some adaptive buffer allocation scheme, as described below, may be used to adjust bufallocadaptively. to maximize flexibility, the adaptation can be carried out by software.adaptive buffer allocationadaptive buffer allocation allows multiple vcs to share the same buffer pool in the receiver node adaptively,according to their needs. that is, bufalloc of a vc is automatically decreased if the vc does not have sufficientdata to forward, cannot get sufficient scheduling slots, or is backpressured due to downstream congestion. thefreedup buffer space is automatically assigned to other vcs that have data to forward and are not congesteddownstream.adaptive buffer allocation can be implemented at the sender or receiver node. as depicted in figure 9, in asenderoriented adaptive scheme [12, 17] the sender adaptively allocates a shared inputbuffer at the receiveramong a number of vcs from the sender that share the same buffer pool. the sender can allocate buffer for thevcs based on their measured, relative bandwidth usage on the output port p [12].receiveroriented adaptation [13] is depicted by figure 10. the receiver adaptively allocates a sharedoutputbuffer among a number of vcs from one or more senders that share the same buffer pool. the receiver canallocate buffer for the vcs based on their measured, relative bandwidth usage on the output port q [13].creditbased flow control9traffic management for highspeed networks: fourth lecture international science lecture seriescopyright national academy of sciences. all rights reserved.figure 9 senderoriented adaptation. the circles are switches. each darkened bar denotes a switch point.figure 10 receiveroriented adaptation.receiveroriented adaptation is suited for the case where a common buffer pool in a receiver is shared byvcs from multiple upstream nodes. figure 10 depicts such a scenario: the buffer pool at output port q of thereceiver switch rcv is shared by four vcs from two switches snd1 and snd2. note that the receiver (rcv forfigure 10) can observe the bandwidth usage of the vcs from all the senders (that is, snd1 and snd2). in contrast,each sender can observe the bandwidth usage only of those vcs going out from the same sender. therefore, it isnatural to use receiveroriented adaptation in this case.moreover, receiveroriented adaptation naturally supports the adaptation of n2 values for individual vcs, inorder to minimize credit transmission overhead and increase buffer utilization. since only the receiver needs to usen2 values, it can conveniently change them locally, as described in the next section.receiveroriented adaptive buffer allocationwe describe the underlying idea of the receiveroriented adaptive buffer allocation algorithm [13]. inreferring to figure 10, let rtt be the maximum of all the rtts and m be the size, in cells, of the common bufferpool in the receiver.for each allocation interval, which is set to be at least rtt, the receiver computes a new allocation and an n2value for each vc according to its relative bandwidth usage. over the allocation interval, let vu and tu be thenumber of cells forwarded for the vc and that for all the n active vcs, respectively. then for the vc, the newallocation is:and the new n2 value iswhere tq is the total number of cells currently in use in the common buffer pool at the receiver. forexposition purposes this section ignores floor and ceiling notations for certain quantities, such as those in therighthand sides of the above two equations. see [13] for precise definitions and analysis of all quantities.creditbased flow control10traffic management for highspeed networks: fourth lecture international science lecture seriescopyright national academy of sciences. all rights reserved.it is easy to see that the adaptive formula of equation (3) will not introduce cell loss. the equation says thatfor each allocation interval, the vcs divide a buffer of size m/2  tq  n according to their current relativebandwidth usage vu/tu. thus, the total allocation for all the vcs is no more than (m/2  tq  n) + n or m/2  tq,assuming that each of the n vcs is always given at least one cell in its allocation. since allocation intervals are atleast rtt apart, after each new allocation, the total number of inflight cells is bounded by the total previous allocation. note that the total previous allocation is no more than , where tqprev is the tq value used therein.therefore the total memory usage will never exceed (m/2  tq) + m/2 + tq or m. consequently, adaptive bufferallocation will not cause cell loss. this analysis also explains why m is divided by 2 in equation (3).equation (4) allows the frequency of transmitting credit cells of the vc, i.e., the n2 value, to adapt to thevc's current bufalloc, or equivalently, its relative bandwidth usage. that is, vcs with relatively large bandwidthusage will use large n2 values. this will reduce their bandwidth overhead in transmitting credit records upstream.(in fact, by adapting n2 values and by packing up to 6 credits in each transmitted credit cell, the transmissionoverhead for credit cells can be kept very low. simulation results in [13] show that this overhead is generally below afew percent, and sometimes below 1 percent.) on the other hand, an inactive vc could be given an n2 value assmall as 1. with a smaller n2 value, the receiver can inform the sender about the availability of buffer spacesooner, and thus increase memory utilization. the n2 value would increase only when the vc's bandwidth rampsup. thus the required memory for each vc could be as small as one cell.from equations (2), (3), and (4), we can show that the adaptive scheme guarantees that a vc will ramp up toits fair share. a sufficient condition is that a fair scheduling policy is employed, the switch buffer sizeor larger is used, and a significant portion of the switch buffer is not occupied, e.g.,the condition of equation (6) holds if those vcs that are blocked downstream do not occupy much bufferspace at the current node. the adaptive buffer allocation scheme is indeed designed in such a way that inactive orslow vcs will be allocated the very minimum or a small buffer space, respectively.assume that there are n  1 active vcs that in aggregate already get the full link bandwidth of an output portof the receiver. now a new vc using the same output port starts and wishes to get its fair share, i.e., 1/n, of thelink bandwidth. suppose that the vc's current buffer allocation x is insufficient for achieving this targetbandwidth. that is, by equations (2) and (4),or, equivalently,creditbased flow control11traffic management for highspeed networks: fourth lecture international science lecture seriescopyright national academy of sciences. all rights reserved.note that with the current allocation x, by equation (2) the relative bandwidth that the vc can achievesatisfies:since tq < 2*rtt/3, it follows from equation (5) and the last two inequalities above that:thus the new allocation for the vc computed by equation (3) will be strictly larger than x. in this way thebuffer allocation for the vc will keep increasing after each round of new allocation, as long as the achievablebandwidth allowed by the current bufalloc x is less than 1/n and the total queue length tq is less than2*rtt /3.in fact, the ramp up rate for a vc is exponential in number of allocations initially, when the bandwidthallowed by the credit control is small and when tq is small. we can easily explain this exponential ramp up, usingthe last inequality expression above, for the simplifying case that tq = 0. when rtt is large and x*n/4 is muchsmaller than rtt, the middle term is about a factoroftwo larger than the third term. that is, x is ramped uproughly by a factor of two for every new allocation. in general, from the inequality expression we see that if m =2**rtt + 2*n, then the ramp up factor for each allocation is about . therefore, the larger or m is, the fasterthe ramp up will be.rationale for creditbased flow controlwe discuss some key reasons behind the creditbased approach to flow control. the same rationale, perhapsformulated in a different form, is applicable to any flow control scheme.overallocation of resources to achieve high efficiencyfor reasons of efficiency, the size m of the total allocated buffer in the receiver generally needs to be largerthan rtt. this is overallocation in the sense that if traffic is 100 percent steady state, m need only be rtt forsustaining the peak bandwidth of the output link. however, for bursty traffic, m needs to be larger than rtt toallow high link utilization and reduce transmission time.first consider static credit control. if the required memory cost is affordable, we can let bufalloc be rtt +n2 for every one of the n active vcs. then by equation (2) the maximum bandwidth the vc can achieve is atleast 1/n for any value of n. when a scheduling slot for the output link becomes available, an "eligible" vc at thesender that has data and credit can transmit instantly at the peak link rate. when there are no other competingvcs, i.e., when n =creditbased flow control12traffic management for highspeed networks: fourth lecture international science lecture seriescopyright national academy of sciences. all rights reserved.1, any single vc can sustain the peak link rate by equation (2). thus, link utilization is maximized andtransmission time is minimized.now consider adaptive credit control. as in the static case, m needs to be large for increased link utilizationand reduced transmission time. for adaptive buffer allocation, m needs to be large also for fast ramp up [15] asexplained above.intuitively, receiveroriented adaptation needs more buffer than does senderoriented adaptation, becausereceiveroriented adaptation involves an extra roundtrip delay for the receiver to inform the sender of the newallocation. thus the minimum buffer size for receiveroriented adaptation is increased from rtt to 2*rtt.suppose that the total memory size is larger than the minimum 2*rtt, e.g., as given by equation (5). then thepart of the memory that is above the minimum 2*rtt will provide "headroom" for each vc to increase itsbandwidth usage under the current buffer allocation. if the vc does increase its bandwidth usage, then theadaptation scheme will notice the increased usage and will subsequently increase the buffer allocation for the vc[12].the receiveroriented adaptive buffer allocation scheme in [13] uses m given by equation (5). analysis andsimulation results have shown that with this choice of m the adaptive scheme gives good performance inutilization, fairness, and ramp up [13].linkbylink flow control to increase quality of controllinkbylink flow control has shorter and more predictable control loop delay than does endtoend flowcontrol. this implies smaller memory requirements for switching nodes and higher performance in utilization,transmission time, fairness, and so on.linkbylink flow control is especially effective for handling transient "cross" traffic. consider figure 11,where t is an endtoend flowcontrolled traffic using some endtoend transportlevel protocol such as tcp andx is highpriority cross traffic. if x uses the whole bandwidth of the switch3's output link, then the entire windowof t for coveting the endtoend roundtrip delay would have to be buffered to avoid cell loss. with linkbylinkflow control, all the buffers on the path from the source of t to switch3 can be used to prevent cell loss. incontrast, without linkbylink flow control, only the buffer at the congestion point (i.e., switch3 in this case) canbe used for this purpose. the argument for making efficient use of buffers is similar to that for making efficientuse of dams in the floodcontrol analogy described above.figure 11 (a) with linktolink flow control, all buffers on the path leading to the congestion point (switch3) wheretraffic t meets cross traffic x can be used for preventing cell loss; (b) without linkbylink flow control, only thebuffer in switch3 can help.creditbased flow control13traffic management for highspeed networks: fourth lecture international science lecture seriescopyright national academy of sciences. all rights reserved.moreover, sufficient predictability in the control loop delay is necessary for the receiver to perform policing.after issuing a flow control command to the sender, the receiver will need to start policing the traffic according tothe new condition only after the control loop delay. if control loop delay cannot be bounded, it is impossible forthe receiver to decide when to start policing.pervc queueing to achieve a high degree of fairnessto achieve fairness between bursty vcs sharing the same output, it is necessary to have separate queueingfor individual vcs. with a fair roundrobin scheduling policy among these queues, cells from different vcs willbe sent out in a fair manner.pervc queueing provides firewall protection against vcs interacting each other. technology advances havelowered the cost of implementing pervc queueing. there are more and more pervc queueing switches availableon the market. fore system's asx200wg is one example. pervc queueing will be the future trend for atmtechnology.ratebased flow controlit is instructive to consider ratebased flow control schemes [3, 4], in contrast to the creditbased approachdescribed above. ratebased flow control consists of two phases: rate setting by sources and network, and ratecontrol by sources. these two phases correspond to the buffer allocation and credit control phases in creditbasedflow control.rate control is a shaping function for which various implementations are possible. for example, when a cellof a vc with a given rate r arrives, the cell will be scheduled for output at time 1/r after the previous output of thesame vc. by sorting arriving cells into buckets according to their departure times, rate control can be implementedwithout pervc queueing (although perratebucket queueing may be needed).suppose that traffic is so smooth that it is possible to set the rate for each vc perfectly against someperformance criteria, and that these rates need not change over time to sustain the target performance. then, if thevcs are shaped at the sources according to the set rates, the ratebased flow control method should work perfectlywell. there would be no need for linkbylink flow control and pervc queueing in the network. the buffer in aswitch could also be kept at the minimum, almost as in a synchronous transfer mode (stm) switch.however, setting rates perfectly or near optimally is a complicated matter. consider, for example, theconfiguration in figure 12, known at the traffic management group of the atm forum in 1994 as genericfairness configuration (gfc) [20]. all traffic sources are assumed to be persistently greedy and can transmit atthe peak link rate when bandwidth is available. linksfigure 12 generic fairness configuration (gfc). [ ]: link bandwidth; link bandwidth = 1 if not indicated. (k):number of virtual circuits (vcs) in the vc group.creditbased flow control14traffic management for highspeed networks: fourth lecture international science lecture seriescopyright national academy of sciences. all rights reserved.have various propagation delays. the actual values of the propagation delays are not important to the discussionhere, and thus are not listed.note that both traffic b and e share the same link between s4 and s5, and the source of e is closer to the linkthan that of b. this is analogous to a parking lot scenario in which e starts from a position closer to the exit thanb. in a normal, realworld parking lot, e would have an unfair advantage over b by being able to move itself infront of b and get out first. however, in a good atm network with separate virtual circuits for b and e, theyought to share fairly the bandwidth of the link, as long as they are not bottlenecked elsewhere in the network.with this fairness objective in mind, we naturally consider the performance criterion described below, whichis sometimes called "maxmin" fairness [3, 4, 6] in the literature. first, the vcs on the most congested link willshare the link bandwidth equally, and this determines the rates to be set for these vcs. then, apply the procedureto the other vcs with the remaining bandwidth of the network. continue repeating the procedure until rates for allthe vcs have been assigned. table 1 shows the resulting rates assigned to individual vc groups.translating the above mathematical ratesetting procedure into an efficient and robust implementation is amajor challenge. first, with highly bursty abr traffic, because load changes rapidly, there would be no staticratesetting that could be ideal for any significant period of time. when traffic changes, "optimal" rates to beassigned to the affected vcs must change accordingly.for this reason, adaptive ratesetting is necessary for bursty traffic and has been the subject of intensiveresearch for many years. the enhanced proportional ratecontrol algorithm (eprca) [18], one of the schemesconsidered at the 1994 atm forum, represents the kind of adaptive ratesetting schemes this paper assumes.rate adaptation cannot be so precise that the newly derived rates will be exactly right with respect to currentload, for at least two reasons. first, information and measurements based on which particular adaptation isperformed cannot be totally complete or up to date due to various cost and implementation constraints. second, thefeedback control time that the adaptation takes to inform sources can vary because of disparities in propagationdelay and link speed, congestion conditions, scheduling policies, and many other factors.more interesting, perhaps, is that rate adaptation should not be precise either. to achieve high utilization withbursty traffic, it is necessary that the total assigned rate for all the vcs over a link be higher than the peak linkrate. consider the simple scenario shown in figure 13 involving only two vcs, a and b. assume that the twovcs share the same switch output linkcreditbased flow control15traffic management for highspeed networks: fourth lecture international science lecture seriescopyright national academy of sciences. all rights reserved.of bandwidth 1, and that each has a data burst that would take a unit time to transmit over a link of bandwidth 1.suppose that the b burst arrives 1 unit time later than the a burst. then as figure 13 depicts, in the precise ratesetting case where each vc is set with a rate of 0.5, it would take a total of 3 time units to complete thetransmission of both the a and b bursts. in contrast, in the overallocating ratesetting case where each vc is setwith a rate of 1, it would take only 2 time units to do the same. this need for overallocating resources is similar tothat discussed above for credit control.figure 13 both bursts a and b complete transmission earlier and make higher utilization of switch output link in theoverallocating case than in the precise case.since adaptation cannot and should not be precise, rates set by the adaptation may not be totally correct.bounding the liability of overrunning switch buffers is therefore a firstorder issue. this explains why creditbasedflow control has been desired to control buffer allocation and monitor the buffer usage, directly.creditnet atm switchto study atmlayer flow control, bnr and harvard university have jointly developed an experimental atmswitch [2], with both 622mbps (oc12) and 155mbps (oc3) ports. this effort is part of the creditnet researchproject supposed, in part, by the defense advanced research projects agency (darpa). under this project, bnrand hazard have developed the atm switch described here, whereas carnegie mellon university (cmu) andintel have developed an atmpci host interface at both oc3 and oc12 rates.this experimental creditnet switch has a number of unique features. these include atmlayer creditbasedflow control, pervc roundrobin cell scheduling, multicast support in hardware, highly programmablemicroprocessorbased switch port cards, and built in instrumentation for performance measurement.(independently, digital equipment corporation (dec) has also developed a creditbased atm network.)five of these experimental switches have been built; the first one has been operational since spring 1995.several atm host adapters have been used in conjunction with the switch. these include those from dec (forturbochannel), sun (sbus), intel (pci) and zeitmet (pci). both the oc3 and oc12 links have been used invarious experiments. in addition, a q93b signalingcreditnet atm switch16traffic management for highspeed networks: fourth lecture international science lecture seriescopyright national academy of sciences. all rights reserved.system has been successfully implemented on the switch. as of spring 1996, one of the switches now operates onsite at harvard, one is temporarily at a sprint site to support a wan congestion control trial, and others are at bnrand cmu.to implement creditbased flow control, the switch monitors the buffer use of each vc and provides feedbackto the immediately preceding switch or host along the vc's path. since each switch has precise knowledge of theresources a circuit is consuming, and the feedback loop is only one link long instead of the length of the entireendtoend connection, this flow control system allows much more efficient use of buffer memory and linkbandwidth.as shown in figure 14, the switch is physically organized as 16 modules that plug into a backplane andmemory buffer system. one of the modules is the switch control module for call processing using the q93bsignaling standard.the rest of the modules are port modules. each port module has two 960 microprocessors, one for schedulingmentioned above and one to handle realtime monitoring and control. these two microprocessors are not necessaryfor a minimum implementation of a creditbased switch, but they provide the programming flexibility necessary tostudy many research issues. for example, these processors provide the flexibility to experiment with differentways of observing and reacting to network load conditions. each port module also has a fiberoptic link interfaceusing synchronous optical network (sonet) framing. the cellhandling hardware is built from fieldprogrammable gate arrays for control, and from static random access memories (rams) for tables and queues.when a cell arrives at the switch, the input port broadcasts the cell's circuit identifier and address in thecommon memory on the arrival bus on the backplane. each output port monitors this backplane; when a portnotices that a cell has arrived for a circuit that leaves that port, it adds the cell's memory address to a queue.when a cell leaves an output port, its circuit identifier is broadcast on the departure bus on the backplane. bywatching the arrival and departure buses, each input port maintains a count of the number of cells buffered foreach circuit that enters that port. this count is used both to provide creditbased flowcontrol feedback and todecide which circuits are using so much memory that their data should be discarded.the common memory architecture allows the switch to support multicast in an efficient way. a commonmemory allocation engine maintains a list of free locations in the shared commonfigure 14 architecture overview of creditnet switch.creditnet atm switch17traffic management for highspeed networks: fourth lecture international science lecture seriescopyright national academy of sciences. all rights reserved.memory. entries from this list are allocated to cells as they arrive. when a multicast cell's information is broadcaston the arrival bus, more than one port module will enqueue this cell in its list of cells to send. however, the cellrequires only one common memory location.the allocation engine hands out addresses of free slots in the common memory to the ports on demand, sothat they can store incoming data. when it does this, it initializes a list of egress ports that must send this cell out.when a port sends out a cell, the presence of the cell's identifier on the departure bus tells the allocation engine toremove it from the list. when the list becomes empty, the common memory location is recycled for future use. allthis is done by efficient hardware: the allocation engine requires only four memory accesses per port per cellcycle.for most purposes, the ingress and egress sides of a port are effectively independent. however, they have animportant interaction required for the credit protocol. essentially, the credit protocol requires a sender to have acredit for a given vc, before sending cells on it. credit is granted by sending credit cells opposite the flow of data(from receiver to sender). thus, when the ingress side of a port realizes that a number of cells for that vc haveleft the switch, it notifies the egress side of the same port to send a credit cell.experimental network configurationsthe creditnet switch has been used to experiment with tcp performance over atm networks. theexperiments described below use two network configurations in a lan environment. the first, shown in figure 15(a), involves host a sending a continuous stream of data through the switch to host b. host a's link to the switchruns at 155 mbps, while host b's link runs at only 53 mbps, enforced by a properly programmed scheduler on thelink input. this is one of the simplest configurations in which congestion occurs. note that after sonet and atmoverhead, a 155mbps link can deliver roughly 134 mbps or 17 megabytes per second (mbyte/sec) of usefulpayload to a host. a 53mbps link can deliver about 5.7 mbyte/sec.the second configuration, shown in figure 15 (b), involves four hosts. host a sends data to host c, and host bto host d. the four host links run at 155 mbps, and the bottleneck link between the switches runs at 53 mbps. thepurpose of this configuration is to show how two conversations interact.figure 15 (a) network configuration for single tcp experiments on creditnet and (b) configuration for twocompeting tcps. the shaded circles represent switches. each darkened bar denotes a switch port.creditnet atm switch18traffic management for highspeed networks: fourth lecture international science lecture seriescopyright national academy of sciences. all rights reserved.the hosts in all these experiments are dec alpha 3000/400 workstations running osf/1 v3.0. the osf/1tcp implementation [5], used in all the experiments reported in this paper, is derived from 4.3reno [21]. thistcp tends to acknowledge, and thus transmit, pairs of packets. the tcp window size for these experiments islimited to no more than 64 kbytes, and the packet size to 9180, except when noted. the workstations use 155mbps otto turbochannel adapters provided by dec. the alphas can send or receive tcp using the ottos atabout 15 mbyte/sec. the otto drivers optionally implement creditnet's creditbased flow control partially insoftware; with credit turned on they can send and receive tcp at 13 mbyte/sec.the measurements are all directly derived from the instrumentation counters in the creditnet switchhardware. the hardware keeps track of the total number of cells sent by each vc and the number of cells bufferedfor each vc.measured performance on creditnet experimental switchesatmlayer creditbased flow control resolves some tcp performance problems over atm networks whenpackets are lost because of congestion [16]. the bottleneck switch no longer discards data when it runs out ofbuffer memory and possibly causes tcp to endure lengthy timeout periods. instead, it withholds credit from theswitches and/or hosts upstream from it, causing them to buffer data rather than sending it. this backpressure canextend all the way back through a network of switches to the sending host. the effect is that a congested switchcan force excess data to be buffered in all the upstream switches and in the source host. data need never be lostbecause of switch buffer overran. thus, if tcp chooses a window that is too large, the data will simply be bufferedin the switches and in the host; no data loss and retransmission timeouts will result.table 2 compares the useful bandwidths achieved with and without creditbased atmlayer flow control inthe configurations shown in figure 15. for the flowcontrolled cases, the switch has 100 cell buffers (4800payload bytes) reserved pervc. for the nonflowcontrolled cases, the switch has 682 (32 payload kbytes) cellsof buffering pervc. recall that for the configuration in figure 15 the slow link can deliver at most 5.7 payloadmbps, and the fast link 17. thus in both the one tcp and two tcps cases, tcp with creditbased flow controlachieves its maximumpossible bandwidth.using a configuration similar to that shown in figure 15 (b), experiments involving one tcp and one udp,instead of two tcps, have also been carded out. a typical measured result is as follows. when atmlayer creditbased flow control is used, udp gets its maximum bandwidth limited only by the source, while tcp getsessentially the remaining bandwidth of the bottleneck link between the two switches. however, when creditbasedflow control is turned off, tcp's throughput drops significantly and the total utilization on the bottleneck link byboth tcp and udp is reduced to less than 45 percent. thus, when competing with udp, tcp with atmlayerflow control can keep up its throughput even though udp does not reduce its bandwidth during networkcongestion.creditnet atm switch19traffic management for highspeed networks: fourth lecture international science lecture seriescopyright national academy of sciences. all rights reserved.figure 16 measured switch buffer occupancy when one tcp scuds into a slow link, as depicted in figure 15 (a), withcredit flow control turned on.figure 16 shows how much switch buffer space is used when one tcp sends into a slow link with credit flowcontrol turned on, for the configuration depicted in figure 15 (a). the flow control system makes sure that enoughcells are always buffered that it can keep the output link busy, but never much more than that. the largeoscillations correspond to packet boundaries.summary and concluding remarksalthough the cost of memory has been dropping over the years, the speed of networks and the potentialnumber of simultaneous applications sharing a network have also been increasing. the bruteforce way of simplyenlarging buffers to avoid data loss will quickly become technically and economically impractical.traffic management is therefore essential. for data applications, we need to ensure no loss due to congestion,high utilization, and fairness, regardless of traffic patterns. this kind of guarantee may be a requirement, not just aluxury, in order to provide acceptable service under harsh conditions expected in realworld data traffic.the most visible sign of network overload due to traffic bursts is usually buffer exhaustion. credit flowcontrol directly controls buffer allocation and monitors its usage.analysis, simulation, and actual experiments on switching hardware have shown that credit flow control canwork well over a wide range of network conditions. at one extreme, static allocation of buffers is simple andprovides the guarantee. the adaptive credit flow control system can reduce memory requirements to just a fewroundtrip times' worth of cells, while maintaining no loss and high performance. thus, a credit system canprovide good performance even if future networks are nothing like those currently predicted. credit flow control isan existence proof that control of congestion can enforce a guarantee of no data loss.as our field experience with atm networks expands, we will have much to learn, especially about theinteraction of atm flow control with higherlever protocols. future research in congestion control should explorethe patterns of real traffic on highspeed networks. working prototypes of the competing flow control systemsshould be compared. without such experience it is not possible to make proper tradeoffs between performanceand cost.summary and concluding remarks20traffic management for highspeed networks: fourth lecture international science lecture seriescopyright national academy of sciences. all rights reserved.acknowledgmentsthis research was supported in part by corporations including intel, nortel, and ascom nexion, and in part bythe defense advanced research projects agency (of the dod) monitored by darpa/cmo under contractmda97290c0035 and by afmc under contract f1vp962892c0116. parts of, the paper are excerpts fromearlier publications [11, 15, 16] by the author and his coauthors. quicktime movies capturing variousexperimental results of, the creditnet switch can be accessed at http://www.eecs.harvard.edu/cntraces.html.references[1] atm forum, ''atm usernetwork interface specification,'' version 3.0, prentice hall, englewood cliffs, new jersey, 1993.[2] t. blackwell et el., "an experimental flow controlled multicast atm switch," proceedings of the first annual conference ontelecommunications r&d in massachusetts, vol. 6, pp. 3338, october 25, 1994.[3] a. charny, "an algorithm for rate allocation in a packetswitching network with feedback," mit/lcs/tr601, laboratory forcomputer science, massachusetts institute of technology, cambridge, massachusetts, april 1994.[4] a. charny, d. clark, and r. jain, "congestion control with explicit rate indication," proceedings icc'95, p. 10, june 1995.[5] chranham chang et el., "highperformance tcp/ip and udp/ip networking in dec osf/1 for alpha axp," digital technical journal,winter 1993.[6] e.l. hahne, "roundrobin scheduling for maxmin fairness in data networks," ieee journal on selected areas in communications,vol. 9, no. 7, pp. 10241039, september 1991.[7] g. irlam, "unix file size surveyš1993," usenet comp.arch.storage, <url: http://www.base.com/gordoni/ufs93.html>, last updatedseptember 1994.[8] w.e. leland, m.s. taqqu, w. willinger, and d.v. wilson, "on the selfsimilar nature of ethernet traffic," proceedings of the acm sigcomm 1993 symposium on communications architectures, protocols, and applications, pp. 183193, september 1993.[9] m.g.h. katevenis, "fast switching and fair control of congested flow in broadband networks," ieee journal on selected areas incommunications , vol. sac5, no. 8, pp. 13151326, october 1987.[10] v. jacobson, "congestion avoidance and control," proceedings of the sigcomm 1988 symposium on communications architecturesand protocols, august 1988.[11] h.t. kung, "flowcontrolled atm switches for available bit rate services", proceedings of the 2nd international conference onmassively parallel processing using optical interconnections, pp. 176179, ieee computer society press, san antonio, texas,october 1995.[12] h.t. kung, t. blackwell, and a. chapman, "creditbased flow control for atm networks: credit update protocol, adaptive creditallocation, and statistical multiplexing," proceedings of the acm sigcomm 1994 symposium on communications architectures,protocols, and applications, pp. 101114, august 31september 2, 1994.[13] h.t. kung and k. chang, "receiveroriented adaptive buffer allocation in creditbased flow control for atm networks,"proceedings of infocom 1995, pp. 239252, april 1995.acknowledgments21traffic management for highspeed networks: fourth lecture international science lecture seriescopyright national academy of sciences. all rights reserved.[14] h.t. kung and a. chapman, "the fcvc (flowcontrolled virtual channels) proposal for atm networks," version 2.0, 1993. asummary appears in proceedings of the 1993 international conference on network protocols, pp. 116127, san francisco, october1922, 1993. (postscript files of this and other related papers by the authors and their colleagues are available via anonymous ftpfrom virtual.harvard.edu:/pub/htk/atm.)[15] h.t. kung and r. morals, "creditbased flow control for atm networks," ieee network magazine, pp. 4048, march/april 1995.[16] r. morris and h.t. kung, "impact of atm flow control on tcp performance: measurements on an experimental atm switch,"proceedings of globecom 1995, pp. 888892, stamford press, singapore, 1995.[17] c. ozveren, r. simcoe, and g. varghese, "reliable and efficient hopbyhop flow control," proceedings of the acm sigcomm 1994symposium on communications architectures, protocols, and applications, pp. 89100, august 31september 2, 1994.[18] larry roberts, "enhanced prca (proportional ratecontrol algorithm)," atmforum/940735r1, august 1994.[19] a. schmidt and r. campbell, "internet protocol traffic analysis with applications for atm switch design," acm sigcomm computercommunication review, vol. 23, no, 2, pp. 3952, april 1993.[20] r.j. simcoe, "configurations for fairness and other tests," atm forum/940557, 1994.[21] g. wright and w.r. stevens, tcp/ip illustrated, vol. 2, addisonwesley, new york, 1995.acknowledgments22
detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/1910the future of statistical software: proceedings of a forum100 pages | 8.5 x 11 | paperbackisbn 9780309045995 | doi 10.17226/1910panel on guidelines for statistical software, national research councilthe future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.the future of statisticalsoftwareproceedings of a forumpanel on guidelines for statistical softwarecommittee on applied and theoretical statisticsboard on mathematical sciencescommission on physical sciences, mathematics, and applicationsnational research councilnational academy presswashington, d.c. 1991ithe future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.notice: the project that is the subject of this report was approved by the governing board of the national research council, whose members are drawn from the councils of the national academy of sciences, the national academy of engineering, and the institute of medicine.the national academy of sciences is a private, nonprofit, selfperpetuating society of distinguished scholars engaged in scientific and engineering research, dedicated to the furtherance of science and technology and to their use for the general welfare. upon the authority of thecharter granted to it by the congress in 1863, the academy has a mandate that requires it to advise the federal government on scientific andtechnical matters. dr. frank press is president of the national academy of sciences.the national academy of engineering was established in 1964, under the charter of the national academy of sciences, as a parallel organization of outstanding engineers. it is autonomous in its administration and in the selection of its members, sharing with the national academyof sciences the responsibility for advising the federal government. the national academy of engineering also sponsors engineering programs aimed at meeting national needs, encourages education and research, and recognizes the superior achievement of engineers. dr. robertm. white is president of the national academy of engineering.the institute of medicine was established in 1970 by the national academy of sciences to secure the services of eminent members of appropriate professions in the examination of policy matters pertaining to the health of the public. the institute acts under the responsibility givento the national academy of sciences by its congressional charter to be an adviser to the federal government and, upon its own initiative, toidentify issues of medical care, research, and education. dr. stuart bondurant is acting president of the institute of medicine.the national research council was organized by the national academy of sciences in 1916 to associate the broad community of scienceand technology with the academy™s purposes of furthering knowledge and advising the federal government. functioning in accordance withgeneral policies determined by the academy, the council has become the principal operating agency of both the national academy of sciences and the national academy of engineering in providing services to the government, the public, and the scientific and engineering communities. the council is administered jointly by both academies and the institute of medicine. dr. frank press and dr. robert m. white arechairman and vice chairman, respectively, of the national research council.the national research council established the board on mathematical sciences in 1984. the objectives of the board are to maintain awareness and active concern for the health of the mathematical sciences and to serve as the focal point in the national research council for issuesconnected with the mathematical sciences. in addition, the board is designed to conduct studies for federal agencies and maintain liaison withthe mathematical sciences communities and academia, professional societies, and industry.support for this project was provided by the national science foundation and the national institute of environmental health sciences.library of congress catalog card no. 9166874international standard book number 0309045991additional copies of this report are available from:national academy press2101 constitution avenue, n.w.washington, d.c. 20418s467printed in the united states of americaiithe future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.panel on guidelines for statistical softwarewilliam f. eddy, carnegie mellon university, chairsally e. howe, national institute of standards and technologybarbara f. ryan, minitab, inc.robert f. teitel, abt associates, inc.forrest w. young, university of north carolinajohn r. tucker, staff officercommittee on applied and theoretical statisticswilliam f. eddy, carnegie mellon university, chairyvonne bishop, u.s. department of energydonald p. gaver, naval postgraduate schoolprem k. goel, ohio state universitydouglas m. hawkins, university of minnesotadavid g. hoel, national institute of environmental health sciencesjon kettenring, bellcorecarl n. morris, harvard universitykarl e. peace, biopharmaceutical research consultantsjayaram sethuraman, florida state universityjohn r. tucker, staff officeriiithe future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.board on mathematical sciencesshmuel winograd, ibm t.j. watson research center, chairronald douglas, state university of new yorkstony brook, vicechairlawrence d. brown, cornell universitysunyung a. chang, university of california at los angelesjoel e. cohen, rockefeller universityavner friedman, university of minnesotajohn f. geweke, university of minnesotajames glimm, state university of new yorkstony brookphillip a. griffiths, institute for advanced studydiane lambert, at&t bell laboratoriesgerald j. lieberman, stanford universityronald f. peierls, brookhaven national laboratoryjerome sacks, national institute of statistical sciencesex officio memberwilliam f. eddy, carnegie mellon university chair, committee on applied and theoretical statisticsstaffjohn e. lavery, directorjo neville, administrative secretaryruth e. o'brien, staff associatehans oser, staff officerjohn r. tucker, staff officerjames a. voytuk, senior staff officerscott t. weidman, senior staff officerbarbara wright, administrative assistantivthe future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.commission on physical sciences, mathematics, and applicationsnorman hackerman, robert a. welch foundation, chairpeter j. bickel, university of california at berkeleygeorge f. carrier, professor emeritus, harvard universitydean e. eastman, ibm t.j. watson research centermarye anne fox, university of texasaustinphillip a. griffiths, institute for advanced studyneal f. lane, rice universityrobert w. lucky, at&t bell laboratoriesclaire e. max, lawrence livermore laboratorychristopher f. mckee, university of california at berkeleyjames w. mitchell, at&t bell laboratoriesrichard s. nicholson, american association for the advancement of sciencealan schriesheim, argonne national laboratorykenneth g. wilson, ohio state universitynorman metzger, executive directorvthe future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.vithe future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.prefacethe panel on guidelines for statistical software was organized in 1990 by the national research council'scommittee on applied and theoretical statistics for the purpose of documenting, assessing, and prioritizing problem areas regarding the quality and reliability of statisticalsoftware; presenting to producers and users prototype guidelines in highpriority areas for the evaluation (based onestablished statistical principles) of statistical software packages; and making recommendations in the form of a plan for further discussion, research, testing, andimplementation of guidelines involving the statistical computing, user, and producer communities.the findings of the panel will be presented in a future report and at meetings of concerned groups, includingprofessional societies, to stimulate such further work. the panel's guidelines will be accompanied by benchmarktest data or descriptive material from which such data can be constructed. the panel will not endorse or censurespecific statistical software products, but rather will offer general guidelines and broad objectives and evaluationcriteria useful to statistical software users and developers, and designed to facilitate and focus further work on thesubject.on february 22, 1991, the panel held a public forum, ﬁthe future of statistical software,ﬂ so as to gathermaterial for its deliberations from a wide range of statistical scientists from academe, industry, and government.these proceedings have been compiled to document that input. however, the opinions expressed in this volumeare those of the speakers or discussants and do not necessarily represent the views of the panel on guidelines forstatistical software or of the national research council.prefaceviithe future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.prefaceviiithe future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.contents morning session opening remarksbarbara ryan, minitab, inc. 1 richness for the oneway anova layoutkeith e. muller, university of north carolina, chapel hill 3 serendipitous data and future statistical softwarepaul f. velleman, cornell university 15 morning discussion 23 afternoon session opening remarksforrest young, university of north carolina, chapel hill 33 an industry viewandrew kirsch, 3m 37 guidance for oneway anovawilliam dumouchel, bbn software products 43 incorporating statistical expertise into data analysis softwaredaryl pregibon, at&t bell laboratories 51 afternoon discussion 63 closing remarkswilliam eddy, carnegie mellon university 69 appendixes a. speakers 75b. position statements 77c. forum participants 85contentsixthe future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.contentsxthe future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.morning session opening remarksbarbara ryanminitab, inc.the question faced by the panel on guidelines for statistical software is how to provide effective guidance topeople involved with statistical software. the audience we are trying to address consists of people who have toreview software for one reason or anotherfor their own personal use, for use within a company or a university, orperhaps for presentation in a trade publication, professional journal, or popular magazine. we want to addresspeople who are trying to understand statistical software, recognize good software, and know how to look atsoftware to assess whether it will meet their needs.another group we expect the report to influence is the vendors who produce software. we want the vendorsto produce the best possible software for the users. in the late 1970s, when the american statistical associationgot involved in evaluating statistical software, there was some concern from the vendor community about howthat was done. but it had one major effect: it improved the quality of statistical software. very few vendors wantedto appear to be uninformed, and so they made sure their software met some of the criteria that the statisticiansconsidered very important. our panel hopes that its future report, to which this forum will provide input, will alsoinfluence vendors to improve their products in ways that will help people to do statistical work well.the panel has identified three qualities of statistical software as important: richness, exactness, and guidance.although others are also important, these were selected as a means to structure the panel's efforts. richness will beemphasized this morning and guidance will be featured this afternoon; exactness will arise throughout the day asappropriate. by richness, we mean: what does the software do, what type of analysis? does it have depth? does ithave breadth? those qualities are not necessarily always good things. for some people, too much depth and toomuch breadth are confusing. so richness is not automatically a good quality, but it is an important quality.morning session opening remarks1the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.morning session opening remarks2the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.richness for the oneway anova layoutkeith e. muller1university of north carolina, chapel hillstatement of the problemrationale of approachthe panel on guidelines for statistical software has organized its examination of statistical software aroundthe concepts of exactness, richness, and guidance. the oneway layout presents a ubiquitous task, and hence astimulating prototype for devising guidelines. this presentation covers richness in the oneway layout. in thesection below titled ﬁrichness dimensions,ﬂ a number of dimensions are proposed as the basis for evaluating therichness of statistical software. each of these is examined through the same three steps: (1) a brief definition isgiven, (2) richness is detailed for the oneway layout in the dimension under discussion, and (3) the specificdescription is followed by a discussion of general principles.creating general and useful principles will require continuing interactive discussion among many interestedobservers; the comments reported here are intended to stimulate further discussion. such discussion will helpavoid embedding statistical fads in guidelines. specific evaluations of software unavoidably depend on thephilosophy of the evaluator. hence evaluation guidelines must be ecumenical in coverage, yet able to be narrowedfor a particular task. undoubtedly the proposed structure and topics are not definitive. despite that, the presentauthor does believe that any alternate approach must cover all of the issues considered here.problem boundaries.a number of terms must first be defined, at least loosely. statistical software will be taken to be anycollection of computer programs and associated information intended to directly aid the production of any sort ofstatistical analysis. note that this definition includes software that itself may not produce statistical analysis. forexample, a program1 the author gratefully acknowledges the stimulation of summaries of meetings of the panel on guidelines for statisticalsoftware. the basic approach in which this discussion is embedded was determined by those deliberations. in addition, somespecific examples are taken from the summaries.richness for the oneway anova layout3the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.used to create a file used as input to an analysis program may fall under this definition. exactness consists of achoice of an acceptable algorithm and the accurate implementation of the algorithm. for example, asymptoticformulae for variance estimation or pvalue calculations should not be used in small samples whenever moreprecise calculations are available. the algorithm should either tolerate stressful data configurations, or detect themand gracefully report an inability to proceed. guidance consists of the help provided by the structure and featuresof the software in conducting a correct and effective analysis. this includes assistance in choosing appropriateinstructions, as well as assistance in deciding whether a particular approach is valid. richness consists of howfully the software can do the analysis. the term coverage may be preferred by some. throughout, the user will betaken to refer to the person executing the software.the definitions of exactness, guidance, and richness all overlap somewhat. distinctions between guidance andrichness are particularly important in defining the boundaries of the present discussion. at one extreme, providingmaximal guidance leads to an expertsystem approach. in that case, richness ceases to exist as a separate property,being determined by the range of tolerated inputs and guidance accuracy. at the other extreme, providing minimalguidance leads to documenting features of the software and nothing else. an intermediate amount of guidancewould be the automatic inclusion of diagnostics concerning the assumptions of the method of analysis. in contrast,richness describes the availability and convenience of creating such diagnostics. in considering guidance, thevalidity of the analysis approach for the data at hand is always in question. in contrast, when discussing richness itwill be assumed throughout that using the software for the data at hand is a valid endeavor.the oneway layout in analysis of variance (anova) is used as the basic example. for the sake of brevity,familiarity with traditional approaches will be assumed. kirk [1982] provided a comprehensive treatment of alarge range of anova models. in oneway anova, a continuous response variable is examined to assesswhether it is related to a categorical predictor variable. the predictor values may be strictly nominal, orderedcategories, or interval scale values. typically two to ten distinct predictor values are present. a number ofregularity conditions must be assumed for both nonparametric as well as parametric methods in order to ensurethe validity of statistical analysis. these may be loosely grouped into assumptions concerning (1) existence, (2)independence, (3) model, and (4) distribution. traditional parametric fixedeffect anova requires independentand identically distributed (i.i.d.) gaussian scores within each category and categories that differ only by expectedvalue.richness dimensions1. epistemological goalsdefinition the epistemological goal(s) for an analysis consists of the standards by whichrichness for the oneway anova layout4the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.truth is judged, the standards by which decisions are made, and the purpose of the analysis.oneway examples example 1: the user wishes to evaluate whether red, green, or blue text can be read mostrapidly on a computer screen. example 2: the user wishes to estimate the location and scale of the amount ofhypoxic cells in biopsies taken from a number of different types of cancerous tumors. example 3: the user wishesto discover whether a new drug regimen maintains kidney function better than current practice. example 4: theuser wishes to decide whether a single timerelease capsule has the same therapeutic effect as three smaller dosesdelivered once every eight hours. example 5: the user wishes to examine the effect on hypertension of the dietarypresence of one foodstuff from a large list.comments epistemology is the study of the basis and limits of knowledge. every user approaches an analysistask with a particular philosophy, whether explicit or not (even to the user). one's philosophy determines how todecide what is true, and what is worth deciding. statisticians' philosophies vary substantially on a number ofdimensions. for example, one may prefer a frequentist, bayesian, or decisiontheoretic approach. moregenerally, statisticians describe estimation and inference as separate activities. furthermore, one may be adamantabout distinguishing between confirmatory and exploratory analysis, or actively opposed to the distinction. onemay favor a parametric, robust, or nonparametric strategy.general principles users have widely varying philosophies and purposes in using software. software authorsand reviewers should report the epistemology upon which they based their work. it should be emphasized that thisdoes not demand statistical ecumenism. instead such clarification will allow users of software and readers ofsoftware reviews to recognize whether the bases for evaluation are shared.2. methodsdefinition methods consist of all the techniques and algorithms that can be implemented within particularsoftware.oneway examples first consider the traditional parametric model assuming gaussian errors, from afrequentist perspective. applications of estimation methods include estimation of primary parameters, such as cellmeans in a cell mean coding, and estimation of linear combinations of primary parameters (which are secondaryparameters, and contrasts), such as mean differences or trend contrasts. testing methods include the general linearhypothesis test and the many kinds of multiple comparison methods available [miller, 1981]. note that estimationof parameter variances and the specification of confidence intervals allow an (embedded) alternate approach forscalar parameters.richness for the oneway anova layout5the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.a bayesian perspective requires implementing different methods for some of the same tasks, but also methods fortasks not listed here (such as posterior density estimation).one may move away from the traditional model in many directions. concerns about the robustness of thetraditional model are implicit in all such moves. examples include methods for evaluating the validity ofassumptions [regression diagnostics; see belsley et al., 1980], semiparametric modifications such as downweighting extreme values, and ranktransformation [puri and sen, 1985].comments the effectiveness of the implementation depends on how conveniently the interface links thestatistical software with other software and the user. many of the methods can be implemented with graphicaldisplays. diagnostics and multiple comparison methods are especially appropriate. interfaces in general arediscussed in the sections below titled ﬁinputsﬂ and ﬁoutputs.ﬂrichness of methods unavoidably intertwines with guidance and exactness. consider the followingapplications, which were not given as examples: weighted least squares, both exact and approximate; estimatingnonlinear functions of parameters and associated confidence intervals; random effects model estimation andtesting, all subjects tested in all conditions (repeated measures); power calculation; and the analysis of dispersion.should these applications be covered by oneway layout software, nominally centered on anova? this issuewill be addressed in the discussion of structure below.general principles methods should be judged on (1) exactness, (2) breadth, and (3) interfaces. exactnessincludes numerical accuracy, numerical efficiency, and optimality of technique (for example, never usingasymptotic approximations in small samples when exact calculations are practical). breadth can be evaluated onlyafter having specified the target coverage desired. user interfaces should include communication from the user tothe software (control) and feedback to the user about, for instance, any branches taken by the software.3. inputsdefinition inputs are the statistical assumptions, data files, and control files. ﬁfilesﬂ include signals fromuser interfaces such as keys, lightpens, joysticks, and mouse keys, as well as information organized oncomputerreadable media.oneway examples the assumptions required for least squares optimality of the traditional fixedeffectmodel may be summarized as homoscedasticity of variance, existence of finite second moments, independence ofobservations, and model linearity (trivially met in this case). assuming gaussian errors leads to maximumlikelihood optimality of the least squares calculations, and allows closedform calculation of optimal likelihoodratio tests. one may wish to choose the parameter structure of the model, such as cell mean. one may also wish tochoose particular contrasts to estimate, such as trends. data arerichness for the oneway anova layout6the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.often stored in text format and also in formats created by various proprietary data management or data analysissoftware packages.comments assumptions must be treated somewhere, and so they are included here, although they may meritseparate treatment. attention to satisfying assumptions contributes strongly to good data analysis.the evaluation of software can be dominated by the quality of the input interface with the user. for example,software that depends on fielddependent references to variables may trip up even a sophisticated user. computerdesigners and scientists have focused on the control files. many convenient data entry packages are available,although unknown to most users. software structure has also been recognized as an important determiner of inputadequacy. relatively little effort has been expended on data file importing. this is often difficult even acrosssoftware, within a platform. crossing platforms and software can be extremely difficult.the great majority of data analysis methods are only defined for rectangular arrays of observations, allowingperhaps some missing data or other irregularities. notable exceptions center on the work in classification andtaxonomy. database management software necessarily supports a great variety of data arrangements andrelationship patterns. the conversion process may be inconvenient. furthermore, care must be taken to produce ananalysis file that allows the questions of interest to be addressed. these same comments are relevant also to thediscussion of structure below, and to the discussion of guidance.general principles considerations about the validity of assumptions should be embedded in inputrequirements of any statistical software. accepting a broad range of inputs may substantially enhance the utility ofthe software. users should be able to control, if they wish, the choice of algorithms, such as coding schemes.these desires must be balanced with the critical needs of efficiency and simplicity of use. input correctness shouldbe checked extensively. error messages should be selfcontained and indicate possible corrections. cleverstructuring may prevent many errors from occurring.4. outputsdefinition outputs are the collection of sensory displays provided immediately to the user, as well as anynontransitory record stored on paper, or on digital or other media.oneway examples for the traditional gaussian errors approach, example analysis outputs include ananova source table, parameter estimates and associated variance estimates, and plots of means. examplediagnostic outputs include tests of homogeneity or normality, and boxplots for each cell. all can be produced on acharacter printer or a graphics device, or stored on digital media.richness for the oneway anova layout7the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.comments most current presentations of statistics could be enhanced by the addition of carefully chosengraphics displays. currently, the depth of one's employer's pockets substantially determines the ability to rendergraphics with adequate resolution. software tends to be very platformspecific, with only the beginnings ofgraphics interchange standards. for static displays, some optimism may be appropriate in that systems of modestcost are rapidly approaching the limits of the human visual system. current dynamic displays are hardwareandcostlimited.a user may wish to validate a particular invocation of software, conduct an analysis not provided as anoption, document properly, ﬁcheckpointﬂ a procedure, or interface with other software. all require the ability todirect any output to digital media. conscientious data analysis, coupled with the power of computers, invites suchapproaches. digital file output enables extensibility, both within and between packages. many different platformsand types of software are used for data analysis. such diversity provides as many disadvantages as advantages.general principles the user may wish to direct any calculated result, including graphics and other displays, to astorage file for use at another time, on another computer, or in a different place. such files should be easilyportable and selfdocumenting. users will strongly prefer software that supports standard interchange formats:science operates on the basis of sharing information. software vendors, however, depend on secrecy ofinformation to allow them to stay in business and make a profit. vendors and scientists will need to cooperate andbe creative to meet the user preferences for openness and ease of interchange while simultaneously protectinglegitimate business interests.5. optionsdefinition options are the alternative epistemological goals, methods, inputs, outputs, structures, internalpaths, external paths, and documentation that may be invoked in addition to or in lieu of the default choices.oneway examples options that might be desired in the traditional oneway anova include choice ofcoding scheme, deletion of the intercept from the model, creation of confidence intervals for mean differences,specification of the categorical variable as a random effect, and diagnostics for homogeneity of distribution acrossgroups.comments the goals, structure, and audience of software mostly determine what options can and should beavailable. given a particular goal and audience, the structure of options will reward or frustrate, depending uponthe skills of the author. the availability of many options empowers the user who is able to control themefficiently. however, the same breadth may overwhelm and mislead less knowledgeable users.general principles the choice of options should be based on software goals. the choicerichness for the oneway anova layout8the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.of defaults should be based on the audience. layering options may be used to resolve the conflict between theneeds of the novice and the sophisticate. certain desirable options are discussed in other sections of this paper,including ﬁinputsﬂ and ﬁoutputsﬂ above.6. structuredefinition the structure of statistical software consists of the module definition and branching schemesemployed in design and execution.oneway examples currently, most anova software reports source tables by default and multiplecomparisons at the user's specific request. for software with diagnostics available, few packages report them bydefault.comments the structure of a program embodies the designer's philosophy about the analysis goals andguidance appropriate for the expected audiences. diversity of audiences appears to require layers of options.limited structures constrain richness, while complex structures reduce efficiency and user difficulties. designersmust understand the conceptual, analytical, and numerical tasks in any statistical method in order to produceappropriate and efficient structure.the description of the example being considered as ﬁthe oneway layoutﬂ may be contrasted with thedescription ﬁoneway fixedeffect anova.ﬂ the former describes the format of a collection of data values, whilethe latter fully specifies a model and associated analysis, including the required data format. the two descriptionscorrespond to radically different structures and labels for the elements of the structure. in turn, documentation andthe audiences that can be served are strongly affected.general principles ideal statistical software would provide seamless modularity. such software would alwayspresent an efficient and simpletouse interface with the user and other software. the methods and use of theproduct would derive from a consistent set of concepts, not a collection of tricks and gyrations. good structureincorporates principles of perception and learning from the behavioral sciences and principles of numericalanalysis and program design from the computational sciences.7. internal paths.definition the internal paths of statistical software are the branches that may be followed due either to thedictates of the algorithm and the data or to choices made by the user.oneway examples in a oneway anova, one may wish to evaluate diagnostics on residuals beforechoosing to examine any output involved with inference. much software always produces a source table. softwaremay allow the user to specify conditionalrichness for the oneway anova layout9the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.execution of stepdown tests, such as trend tests. depending upon the coding scheme, a program may use anorthogonalization scheme or a simple elimination algorithm.comments sophisticated users may desire control of branches internal to a single module. such control mayallow the naive user to bungle an analysis. such control may allow the sophisticated user to tune the performanceof the program to the application.general principles sophisticated users desire control of all internal paths. access to such control must beguarded with appropriate warning information. this recommended approach should be evaluated in light of theguidance standards and audiences.8. external pathsdefinition the external paths of statistical software are the branches that may be followed by the user anddata into, out of, and back into the software.oneway examples the user may wish to conduct diagnostic analysis on alternate transformations of thedata. the results may then be input to a summary analysis. in turn the user then needs to implement the preferredanalysis.comments one rarely uses software in isolation. convenient and efficient paths into and out of the softwaregreatly facilitate quality data analysis.statisticians will continue to create better methods that are computationally intensive for whatever computingmachinery becomes available. the ability to checkpoint such calculations would be advantageous. for example,some current iterative programs require manual intervention even to avoid most of the iterative calculations on asubsequent invocation.general principles interfaces (both input and output) with other modules in the software should be provided.convenient abilities to temporarily suspend execution, checkpoint, and conduct analysis recursively maysubstantially enhance the utility and convenience of the software.9. documentationdefinition documentation consists of all information intended to aid the use of statistical software.oneway examples traditionally paperback books designed to be reference manuals have been the primarydocumentation available to the user. such manuals focus on describing the vocabulary and grammar of thelanguage needed to control such things asrichness for the oneway anova layout10the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.the choice of response variable, the choice of the categorical predictor, labeling, and analysis options.comments one plausible standard for perfection of software would be the need for no documentation.extensive documentation may reflect either richness (and guidance) or awkwardness and unnecessary complexity.many types of documentation may be provided. softwarefocused information usually resides in manuals forlanguage reference, system management, and description of algorithms. tutorials, collections of examples, andstatistics texts focused on a particular piece of software assist the training of users in both the software and thestatistical methods. user groups and tollfree telephone numbers may be supported, reflecting either the vendor'ssensitivity or defensiveness.many formats may be used for documentation. the paperback book has been challenged by onlinedocumentation, at least for truly interactive software. the recent successful marketing of electronic books in japanprovides yet another step toward the handling of all information digitally. arguments over ringbound versusspinebound versus online manuals will eventually also involve new formats.documentation can make good software look bad and bad software look good. effective documentationrequires the same attention to structure as do the algorithms. the topmost layer of documentation may be thoughtof as metadocumentation, documentation of documentation. proper layering and branching can help the user.surprisingly, many existing manuals do not provide examples of actual code in all cases. in describing aparticular programming statement, or a sequence of clicks or keys, a template description may not suffice. anappendix of formulas and a list of algorithmic steps may greatly aid understanding and using software.professional standards may demand verifying the acceptability of the techniques relative to the data at hand.general principles software may be documented in many formats. metadocumentation can help the user takefull advantage of documentation. documentation should be structured, based on the same principles as thesoftware. a reference manual, although always necessary for the sophisticated user, does not, by itself, provideadequate documentation. algorithms and formulas should be detailed. truly complete examples should beincluded. the extent of tutorials and statistical information embedded in documentation depends on the guidancegoals and the audiences. sophisticated users may dislike the presence of statistical information and advice indocumentation, while novice users often crave it. system implementation documentation should be available.10. audiencesdefinition the audiences for statistical software are the user groups that may be distinguished from eachother because of their different approaches to and uses of the software.richness for the oneway anova layout11the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.oneway examples a piece of software may be used by a person with a doctorate in statistics and by aperson with literally no statistical training. the same software may be used by a person with a master's degree incomputer science and by a high school student frightened by computers.comments for statistical software, user sophistication varies in (1) knowledge of statistical theory, (2)knowledge of computing theory, (3) proficiency with data analysis, (4) facility with computing, and (5) experiencewith research data management. natural language sophistication and physiological limitations, such as colorblindness or response speed, may be relevant in some applications.general principles designers, programmers, documenters, and reviewers of statistical software need to beexplicitly and continuously sensitive to the audiences of interest. software and documentation structure shouldreflect the often disparate needs of the novice and the sophisticate.what next?step backthe basic position presented above is that richness varies on a large number of continuous, correlateddimensions. it is also argued that the creation and evaluation of statistical software should occur with respect toexplicit target goals and target audiences. this suggests that careful specification of the task, based on exactness,guidance, and richness, should always be the first step.jump in (continuous involvement)the process of creating and evaluating software improves with effective interaction between producers andconsumers. such continuous involvement will lead to the creation of good products. however, the products willnot be completely successful unless the decision makers and the ﬁfashionﬂ leaders of the user community can beeducated about general and specific guidelines for good statistical software. therefore, even after the panel onguidelines for statistical software releases its final recommendations in a future report, it will be necessary forthose of us interested in guidelines to stay involved to make the user community aware of the panel's guidelinesand encourage their acceptance.richness for the oneway anova layout12the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.referencesbelsley, d.a., e. kuh, and r.e. welsch, 1980, regression diagnostics, john wiley & sons, new york.kirk, r.e., 1982, experimental design, brooks/cole, belmont, calif.miller, r.g., 1981, simultaneous statistical inference, springerverlag, new york.puri, m.l., and p.k. sen, 1985, nonparametric methods in general linear models, john wiley & sons, new york.richness for the oneway anova layout13the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.richness for the oneway anova layout14the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.serendipitous data and future statistical software.paul f. vellemancornell universitymodern statistics is usually considered to have first appeared around the beginning of this century in a formthat resembled its scientific father more than its mathematical mother.as statistics matured during the middle of this century, statisticians developed mathematical foundations formuch of modern statistics. along the way they developed methods that were optimal in some sense. for example,maximum likelihood statisticswhich, when applied under the most commonly used assumptions, include themost commonly used methodshave many properties that make them the best choice when certain assumptionsabout the model and the data are true. data from suitably designed and suitably randomized studies were the focusof data analysis based on these insights.however, much realworld data is serendipitous. by that i mean that it arises not from designed experimentswith planned factors and randomization, nor from sample surveys, but rather as a byproduct of other activities.serendipitous data reside in databases, spreadsheets, and accounting records throughout business and industry.they are published by government and trade organizations. they arise as a byproduct of designed studies whenunexpected patterns appear but cannot be formally investigated because they are not part of the original protocol.serendipitous data forms the basis for some sciences and social sciences because it is, for the moment, the best wecan do.concern with optimal properties of statistics follows the traditions of mathematics in which elegant resultsare valued for their internal consistency and completeness but need not relate directly to the real world. bycontrast, a scientist would reject even the most elegant theory for the small sin of failing to describe the observedworld. traditional statistics are often inappropriate for serendipitous data because we cannot reasonably make theassumptions they require. much of the technology of classical statistics and of traditional statistics software isdesigned to analyze data from designed studies. this is a vital function, but it is not sufficient for the future.in his landmark paper, ﬁthe future of data analysis,ﬂ john tukey identified the difference betweenmathematical and scientific statistics, and called for a rebirth of scientific statistics. to quote lyle jones, the editorof volumes iii and iv of tukey's collected works [jones, 1986, p. iv],the publication was a major event in the history of statistics. in retrospect, it marked a turning point for statisticsthat elevated the status of scientific statistics and cleared the path for the acceptance of exploratory data analysis as alegitimate branch of statistics.serendipitous data and future statistical software.15the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.through tukey's work, and that of others, data analysis that follows the paradigms of scientific statistics hasbeen called exploratory data analysis (eda). recently, eda and the statistical graphics that often accompany ithave emerged as important themes of computerbased statistical data analysis. an important aspect of theseadvances is that, contrary to traditional methods, they do not require data from designed experiments or randomsamples; they can work with serendipitous data. by examining the differences in these two philosophies ofstatistical data analysis, we can see important trends in the future of statistics software.all statistical data analyses work with models or descriptions of the data and the data's relationship to theworld. functional models describe patterns and relationships among variables. stochastic models try to accountfor randomness and error in the data in terms of probabilities, and provide a basis for inference. traditionalstatistical analyses work with a specified functional model and an assumed stochastic model. exploratory methodsexamine and refine the functional model based on the data, and are designed to work regardless of the stochasticmodel.statistics software has traditionally supported mathematical statistics. the data analyst is expected to specifythe functional model before the analysis can proceed. (indeed, that specification usually identifies the appropriatecomputing module.) the stochastic model is assumed by the choice of analysis and testing methods. statisticspackages that support these analyses offer a large battery of tests. they avoid overwhelming the typical userbecause a typical path through the dense thicket of choices is itself relatively simple.the many branches in this design (figure 1) encourage modularity, which in turn encourages a diversity ofalternative tests and the growth of large, versatile packages. most of the conclusions drawn from the analysisderive from the hypothesis tests. printing (in figure 1) includes plotting, but simply adding graphics modules tosuch a program cannot turn it into a suitable platform for eda. for that we need a different philosophy of dataanalysis software design.software to support scientific statistics must support exploration of the functional model and be forgiving ofweak knowledge of the stochastic model. it must thus provide many plots and displays, offer flexible datamanagement, be highly interconnected, and depend on methods other than traditional hypothesis tests to revealdata structure. a schematic might look like figure 2. note that there is no exit from this diagram. most of thepaths are bidirectional, and many are omitted. the data analyst learns about the data from the process of analysisrather than from the ultimate hypothesis test. indeed, there may never be a hypothesis test.software to support scientific statistics is typically not modular because each module must communicate withall the others. the complexity can grow factorially. it is thus harder to add new capabilities to programs designedfor scientific statistics because they cannot simply plug in as new modules. however, additions that are carefullydesigned benefit from the synergy of all capabilities working together.the user interface of such software is particularly important because the data analyst must ﬁliveﬂ in thecomputing environment while exploring the data and refining theserendipitous data and future statistical software.16the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.figure 1: schematic of software to support mathematical statistics.figure 2: schematic of software to support scientific statistics.serendipitous data and future statistical software.17the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.functional model (rather than simply passing through on a relatively straight path, as would be typical oftraditional packages).the ideals of mathematical and scientific statistics are two ends of a continuum. good statistics software canfit almost anywhere along this continuum. however, it is probably impossible for any one program to serve bothends well. any program rich enough in statistics methods and options to meet the needs of classical statistics willfind it hard to offer the directness, speed, and integration required for data exploration.where is statistical software going?many programs are moving to fill the middle ranges of the continuum. programs such as spss, sas, andsystat that were once geared to the extreme mathematical statistics end of the spectrum now appear in versions fordesktop computers, and more flexible interfaces and better graphics have begun to be developed. programs such ass, data desk, and xlisp stat that pioneered in the data analysis end of the spectrum have had more capabilitiesadded so that they no longer concentrate only on data exploration and display.nevertheless, i do not believe that we will all meet in the middle.innovations in computing will offer new opportunitiescomputing power will continue to grow, and new data analysis and graphics methods will develop to takeadvantage of it. many of these will extend scientific statistics more than mathematical statistics, although bothkinds of data analysis will improve.as operating systems become more sophisticated, it will become increasingly common (and increasinglyeasy) to work with several programs, using each for what it does best and moving data and results among themfreely. thus, for example, one might open a favorite word processor, a presentation graphics program, atraditional mathematical statistics program, and a scientific data analysis program. one would then explore andgraph data in the data analysis program, copying results to and writing commentary in the word processor. onemight then move to the mathematical statistics program for specialized computations or specific tests availableonly there (again copying results to the word processor). finally, the presentation graphics program could be usedto generate a few displays to illustrate the major points of the analysis, and those displays again copied to the wordprocessor to complete the report. development in this direction will be goodserendipitous data and future statistical software.18the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.for statistical computing. there will be less pressure on statistics programs to be all things to all people, and moreencouragement to define a role and fill it well. it will be easier to develop new innovative software because thepractical barriers that make it difficult to develop and introduce new statistics software will be lower.networking will also improve dramatically, making it easier to obtain data from a variety of sources. muchof this data will be serendipitous, having been collected for other purposes. nonetheless, additional data are likelyto enhance analysis and understanding.challenges for statistical softwarethere are a number of clear challenges facing someone with the goal of producing better statistics software.first, it takes a long time to design and implement a statistics package. common wisdom is that 10 personyears isminimum, but the major packages have hundreds of personyears invested. second, the user community isdiverse. no matter what is in a package, someone will have a legitimate reason to want something slightlydifferent. many of these requests are reasonable, yet a package that meets all of them may satisfy nobody. third,full support of software is difficult and expensive. while documentation can now be produced with desktoppublishing, it is still very hard to design and write. finally, designing and programming for multiple platformsoften means designing for the lowest common capabilities. what may be worse, many of the difficulties arising inwriting portable software affect capabilities of particular interest to modern computerbased data analysis, such asthe speed and look of graphics, methods of efficient and effective data management, and the design andimplementation of the user interface.is the commercial marketplace the best source of statisticalsoftware innovation?i answer this question in the affirmative. software developed in academia or research facilities has rarelyreached usefulness or ready availability without being commercialized. commercial software continues to beinnovative (although it also tends to copy the innovations of others). commercial software is usually safer to usebecause the inevitable bugs and errors are more likely to get fixed. i also believe that the marketplace has shown ageneral ability to select statistics software. ﬁsurvival of the fittestﬂ has been known to kill off some goodprograms, but weak packages tend to survive only in niche markets even when large sums are spent on advertisingand marketing.nevertheless, commercial software development is a chancy business, demanding anserendipitous data and future statistical software.19the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.unusual combination of skill, knowledge, and luck. the most innovative interface designers may lack thespecialized knowledge to program a numerically stable least squares or to get the degrees of freedom right for anunbalanced factorial with missing cells. the best algorithms may be hidden behind an arcane command languageor burdened with awkward data management.we need to encourage innovation, but we also need to protect users from software that generates wrongresults or encourages bad data analyses.what can we do to encourage innovation?to encourage innovations, we should encourage small players. while i have great respect for the giants of thestatistics software industry, i think that monopoly rarely promotes innovation.developing standards that make it easier to work with several packages will also encourage innovation. forexample, we need a better standard for passing tables of data among programs while preserving backgroundinformation such as variable name, case label, units, and formats. some might argue that we should work toward aunified interface for working with statistics software, but i believe that this would stifle innovation.we can reduce the need to reinvent. for someone with a new idea about data analysis or graphics, it can takean inordinate amount of time and effort to implement standard algorithms that are already known. books such asnumerical recipes [press et al., 1986] help for some scientific applications, but have little for statistics. foradvanced statistical methods, heiberger's book computing for the analysis of designed experiments [heiberger,1990] stands alone.i support the trend toward using multiple programs. if a new concept can be proved in a relatively small,focused program and used readily with established programs, it will be easier to develop, distribute, and use. userscould then move data in and out of the program easily, continuing their analysis in their other preferred packages.programs focused on a particular approach or audience (e.g., time series, quality control, econometrics) couldintegrate in this way with more established programs.the pace of innovation could be speeded if we could convince academia that the creation of innovativesoftware is a legitimate intellectual contribution to the discipline. we can attract fresh talent to statisticalcomputing only if we provide publication paths and professional rewards. the journal of computational andgraphical statistics, begun recently by the american statistical association, the institute of mathematicalstatistics, and the interface foundation, and other new journals in this field may help here. some universitycopyright regulations also penalize software development relative to authoring books or papers, but there is noconsistency from school to school. we could work to establish standard policies that promote rather than stiflesoftware innovation.serendipitous data and future statistical software.20the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.what can we do to ensure quality?software reviewing is difficult and still haphazard. we need to recruit competent reviewers and encouragereviews that combine informed descriptions of a package's operation style, reasonable checks of its correctness,and welldesigned studies of its performance. all complex software has bugs and errors. responsible developerswant to hear of them so they can be fixed. but people tend to believe that if the program doesn't work, it must betheir fault (ﬁafter all, it's published, it's been out for so long someone must have noticed this before, computersdon't make mistakes –ﬂ). we need to encourage users to recognize, document, and report bugs and errors.a library of tested algorithms can help reduce errors. new programs always have errors. (so do old ones,butwe hopefewer and less serious ones.) by providing tested programs and stable algorithms we can reduce theincidence of bugs and errors. even if the algorithm is reprogrammed, it helps to have a benchmark for testing anddebugging.what can we do to promote progress? we must encourage those who have serendipitous data to examine it. (typical thinking: ﬁi don't have anydata – well, i do have personnel records, sales records, accounting records, expenses, materials costs –but that's not data.ﬂ) we must teach scientific statistics as well as mathematical statistics. we must encourage innovation.serendipitous data and future statistical software.21the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.references.heiberger, r., 1990, computing for the analysis of designed experiments, john wiley & sons, new york.jones, l. (ed.), 1986, the collected works of john w. tukey, vols. iii and iv, wadsworth advanced books & software, monterey, calif.press, w.h., b.p. flannery, s.a. teukolsky, and w.t. vetterling, 1986, numerical recipes, the art of scientific computing, cambridgeuniversity press, new york.serendipitous data and future statistical software.22the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.morning discussionturkan gardenier (equal employment opportunity commission): professor velleman's commentsreally spoke to the heart. after five or six years of dealing with military simulations for our designed experiments,where i had people run multivariance regression models in two weeks and analyze the nice multivarianceregression model, i'm now in a setting where all of the data is serendipitous.we held a seminar yesterday in which many attendees asked whether they could assume that a data set fromsome company was part of the universe of employee records. the statisticians looked at the data and said, ﬁnotreally; there are interventions in company records over a period of time.ﬂ by our not permitting that assumption,there was criticism expressed as to our not being real statisticians, because if it could not be assumed that thisyear's data was a random sample from a population, what were we doing there? thanks to what professorvelleman presented today, i am going back and reporting the innovations for serendipitous data that could beapplied.as a statistician working with serendipitous data, let me make another comment. we need lag time, both asdata analysts as well as statisticians. i am dealing with a lot of litigation cases as a statistical expert, tellingattorneys what data to collect. some people come to our office with partially collected data; such data is very hardto analyze and have the analyses stand up in court.within our organization, we write memoranda of understanding. it's part of a statistician's professionalresponsibility to work with the right types of data, to make the right assumptions before using a computer, and todo statistical significance tests. having lag time available ties in with ex post facto collection of the right types ofdata that could interface with interactive data analysis.paul velleman: i think you are right. i mentioned that we need to teach about scientific statistics. foryears we have taught the mathematical statistics approach as the approach in all introductory statistics courses. as aresult, the world is full of people who think that the way one does statistics is to test a hypothesis. many of ourclients know only that much statistics, and this has in effect made our lives more difficult. it will make our futurelives easier if we start teaching more about scientific statistics, rather than just hypothesis testing.clifton bailey (health care financing administration): the hcfa deals with all the medicare data. icertainly concur regarding serendipitous data. we try to analyze the 6 million persons who have a hospitalizationin each year, within a universe of 10 million hospitalizations from the 34 million beneficiaries. the data areprovided on bills, and one needs different kinds of tools and techniques to deal with many of the issues in thesetypes of analyses, e.g., the diagnostics.morning discussion23the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.i like paul velleman's example involving the pathway through an analysis. many times we want to makecomparisons, while taking one pathway, about what would have happened if another analysis, another pathway,had been used. when i look at standard statistical packages, i frequently see that they put in a log likelihood, butthey do not do it in the same way. many of them leave out the constants or formulate the likelihood differently andthen do not tell how it is done. i want to be able to compare across pathways in that larger perspective, to doexploratory analyses in that context.sally howe (national institute of standards and technology): one of the things that you face that otherpeople often don't face is that you have very large data sets. is that an obstruction to doing your work, that there isnot software available for very large data sets?clifton bailey: yes, the available software is very limited when you get into large data sets, unless youdeal with samples. but you want to be able to ask finer questions because you have all of that data, and so you donot want to merely use samplesor you want to use combinations of samples and then look at the subsets using thebase line against the sample.there are many complex issues involved in doing that. however, we can put a plot up and look at residualsfor our data sets that are generated in the psychology laboratory or in many of the nonserendipitous databasecontexts. we can look at that on a graph; we can scan down a column in a table. but we need other techniques andways of doing exploratory analyses with large data sets.another agency, the agency for health care policy research, is funding interdisciplinary teams to make useof these kinds of data. there are at least a dozen research teams that are focusing on patient outcomes. every oneof those teams is facing this problem, as is our agency, and i am sure that many others are also.sally howe: do you see any additional obstructions that the previous speakers have not yet mentioned?clifton bailey: i recently needed to have a userprovided procedure (or proc) in sas modified by oneof the authors, because the outputs would not handle the large volumes of numbers. when the number ofobservations went over 150,000, it would not run. many of the procs get into trouble when there are more than50,000 observations or some similar constraint.paul tukey (bellcore): this problem, dealing with these very large data sets, is one that more and morepeople are having to face, and we should be very mindful of it. the very fact that everything is computerized, withnetwork access to other computers and databases available, means that this problem of large data sets is going toarise more and more frequently. some different statistical and computational tools are needed for it. randomsampling is one approach, but an easy and statistically valid way to do themorning discussion24the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.random sampling is needed, and that is not always so simple to specify. people like dan carr [george masonuniversity], who is here today, and wes nicholson [battelle pacific northwest laboratory] have explicitlythought about the issue of how to deal with large data sets. there are ways to do it.when you have a lot of data points, one issue is that you can no longer do convenient interactive computing.it can take five minutes to run through the data set once, doing something absolutely trivial.there is also a statistical issue involved. when you have an enormous number of observations, you suddenlyconfront the horrifying fact that statistical significance is not what should be examined, because everything insight becomes statistically significant. when that is the case, what should replace statistical significance? otherways of determining what is of practical significance are needed. perhaps formal calculations of statisticalsignificance can still be used, but interpreted differently, e.g., used only as a benchmark for comparing situations. icompletely agree that this issue of large data sets is very important.forrest young: i too agree with what paul velleman is saying and with what clifton bailey hasbrought up. in exploratory methods, the definition of ﬁvery largeﬂ is a lot smaller than it is in confirmatorymethods, by their very nature. to handle 10,000 observations in exploratory methods is really difficult. to handle10 million, perhaps, in confirmatory methods is really difficult. that is their very nature.robert hamer (virginia commonwealth university): i agree with forrest that some exploratorymethods do not work well with large data sets. in fact, with sufficiently large data sets, almost any plottingtechnique will break down in a sense, because the result is a totally black page; every single point is darkened.paul tukey: see dan carr about that.mike guilfoyle (university of pennsylvania): i help people analyze data, and this problem about scaleintensiveness is very, very important. software sophistication has to be considered in light of the specific task inwhich the users are involved.many of these statistical software packages were involved at their origins with teaching. later they movedfrom teaching to research, and now from research the focus is turning to production. this is what clifton baileyfrom hcfa is saying. when large production runs are involved, the clever things performed at the touch of amouse on small research or educational data sets cannot be done. my large production runs are done with batchprocessing and involve multivolumes of tapes, a context in which the mouseclick cleverness is lost.i am surprised that no speakers have yet mentioned efron's work in jackknifing and bootstrapping [see, e.g.,efron and tibshirani (1991), or efron (1988)], which may be useful at least at the research level, to see if themodel does or does not work. it is also a computerintensive process, for which many people do not have theresources. but theremorning discussion25the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.are people thinking about such approaches, and the literature on those approaches might be of interest to statisticalsoftware producers.all of the speakers alluded to software packages as black boxes. but beyond that, one needs to think abouttechnology. people who use these clever little packages on workstations become very good at mastering thetechnology, but they then assume that they understand the underlying processes. however, it is more complicatedthan that, because those people may or may not understand statistics, and may or may not understand thecomputing involved. there is a nexus between computing and statistics, but you do not know whether it's a fiftyfifty split. one person may be reasonably competent in statistics and also reasonably competent at mastering thetechnology, while another is very good at handling the technology but knows nothing about the statistics. i worrythat these technological interfaces make things too easy; people can sit there and fake it. it goes beyond the oldregime of taking some data, running the regression, and asking if this is the best r2 that can be obtained. it is muchmore complicated.paul tukey: yes, you can fake it with a lot of these packages, and you could fake it with the oldpackages, too. part of the salvation might be what daryl pregibon will discuss this afternoon, an expert systemthat does not let you get away with doing something that is grossly inappropriate, or at least forces you to confrontit, and will not quietly acquiesce.in a way, a package cannot force a user to do something responsible. but at least it can responsibly bringthings to the user's attention.raymond jason (national public radio): i am not a degreed statistician; i am a user of statisticalsoftware. i have been assigned on occasion to design experiments, carry them out, and analyze them. the focusthis morning and for the program this afternoon seems to be on analysis and not on the design of experiments. ofcourse, a fully designed experiment gives data that are analyzable by any package. in my work, i have found that iwas not supported at all by software or its documentation in the design of the experiments. if a goal is to improvethe realworld utility of statistical software, experimental design is a necessary responsibility that needs to beaddressed.two ways of addressing it would be to specify standards for expert systems that do design of experiments, orat the very least to come up with some suitable warning to be applied to documentation. statistical software isadvertised and available to the general public through mail order outlets. you do not have to be a member of theamerican statistical association to be enticed by the software, and you do not have to be wealthy or associatedwith large companies to buy the software. it is definitely targeted at people such as myself. yet, i came very closeto a major statistical analysis disaster. only because of work with other experts, certainly not because of thedocumentation of themorning discussion26the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.software, was disaster prevented.2barbara ryan: those comments of raymond jason are excellent. i also deal with many people who arenot degreed statisticians. there is a real concern here because they are going to do statistical analyses. so unlesswe help those people, either through education or through software guidelines or other means, they are going tomake mistakes.regarding his other comment about designing experiments, it is true that most of the software available is foreither exploring or analyzing, not to help in designing. there are some attempts with diagnostics to warn whenthere are problems, but for upfront design, less software has been developed to help.keith muller: design is my favorite topic, but i find it the most fuzzily defined task that i face as astatistician. therefore, it would be the most difficult to implement in a program package. experimental design is acollection of heuristics at this stage, rather than an algorithm or a collection of algorithms.the dimension i identified under the ﬁaudiencesﬂ heading in my talk is that of user sophistication. statisticalsoftware ought to try to identify itself with respect to its target audience. that at least would be an attempt to beresponsible in distributing and marketing software.paul tukey: i also agree that the packages being designed now should have better design modules.people developing the software tend not to be doing a lot of design experiments, and so experimental design tendsto be a glaring omission.years ago, while i was a graduate student, i did some work with a russian statistician who was visiting inlondon. he had just written a book containing an idea that greatly impressed me. his idea was that design andanalysis are really one and the same, not two different things, and all should be integrated. he had some very goodideas on the ﬁhow to,ﬂ very practical ways of designing efficient experiments, not purely abstract mathematicalthings. we need to build on those kinds of ideas and get those things into our software packages. at least insituations where we have an opportunity to design experiments, we ought to take advantage of it. there are biggains to be had by doing so.herbert eber (psychological resources, inc.): the answer to the questions of what to do with huge datasets, and with the fact that everything is then significant, has been around for a while. it is called confidencelimits, power analysis, and effect size. there are packages available. i am aware of one in progress that will doaway with significance2 an article in the journal of quality technology by chris nachtsheim of the university of minnesota [nachtsheim, 1987]evaluates a number of software packages written to help people design experimentsed.morning discussion27the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.concepts almost completely and talk about confidence intervals. so alternatives do exist.william page (institute of medicine): concerning a previous point, we have not really talked aboutsampling yet. the way you collect the data has something to do with the way you analyze the data. either we areputting things in a simple random sample box, such as our anovas, or we're putting them in a serendipity box.there ought to be a way to handle something in between. do you have a complex sample survey? then you shouldbe using the anova box.this may pertain to the guidance issue of this afternoon. the first line produced by the package might ask,ﬁdo you have a simple random sample, yes or no?ﬂ if the user punches the help button, then a light flashes on andthe output reads, ﬁdo you have money to pay a statistician?ﬂkeith muller: on the issue of statistical significance, i try to teach my students the distinction betweenstatistical significance and practical or scientific importance, and that the latter is what the user is actuallyinterested in.richard jones (philip morris research center): there is an area of statistical software that is not beingaddressed here at all. many scientific instruments in laboratories have builtin software that does regressions, orhypothesis testing. some researchers, for instance, use these instruments and blindly accept whatever is produced. irecently had an opportunity, fortunately, to catch a chemist taking results from a totally automated analysis. hewas using a fourparameter equation that left me incredulous. when i asked why he used that, he said that it wasbecause it gave a correlation coefficient of .998. i said, ﬁwhy don't you just do a log transform?ﬂ he replied,ﬁbecause that correlation coefficient is only .994.ﬂ this gentleman was perfectly serious about this. this softwarebuilt into instruments is in general use. though not part of the big statistical packages that have been discussed, itis just as important to the scientific community and to their understanding of things.paul tukey: we have standards from ieee for how to do arithmetic computations to ensure that differentmachines get the same answers. an effort is needed to develop some standard statistical computing algorithms.some of these things already exist, of course. but specifying the actual code or pseudocode would allow thesealgorithms to be rendered in different languages, so that they can certifiably produce the same answers and do thebasic buildingblock statistical kinds of things from which one can build analyses. the major package vendorscould adopt these and replace whatever they are doing with software that adheres to the standards, and the peoplebuilding these instruments could, at least in the future machines, build in some coherence.paul velleman: i endorse that very strongly. there was a mention of providing test data sets. as acomparison, i think that test data sets are inevitably a failure, because the result is programs that get the rightanswer on the test data sets, but not necessarilymorning discussion28the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.anywhere else. standard algorithms and implementations of them would compel the same answer across a widevariety of data sets. if a particular implementation or use then fails to give the same answer as other methods, atleast the situation is well defined, and so it can be fixed. this is something that should be looked into.barbara ryan: i think the issue raised by richard jones is an issue of education. there are many waysto do statistical analysis. often the procedures are built into devices, as small packages that were developed inhouse. there's a huge amount of such statistical software currently in use that the ﬁprofessional statisticscommunityﬂ almost ignores. i have a sense that we dismiss it as being so simplistic and superficial that we are noteven going to look at it.the trouble is there are thousands of people who are using it. it is more an issue of education. maybe imisunderstood his question, but it's really what happens with a log transform to your r2. this is fundamentally anissue of education, not whether you are getting the right or wrong answers. so if a fairly naive user, as far as dataanalysis is concerned, gets involved in statistics but does not know what he or she is doing, how do you addressthat misuse of statistics, when such statistical software is so available to everyone?william eddy: i want to make a comment about standard algorithms versus standard data sets. if youread the ieee floatingpoint arithmetic standard 754, you will see that it does not specify how to do thearithmetic. what it specifies is what the results of arithmetic operations will be. therefore, the standard is actuallyarticulated in terms of standard data sets, rather than in terms of standard algorithms. if we are to emulate such anorganization, we have a difficult task ahead of us. algorithms are the easy way out.keith muller: i have a problem with an algorithm standard. let me give you an example in everyday lifewith which you are all familiarheadlights on your automobile. in the mid1930s there were all kinds of badheadlights available. therefore the united states created an equipment standard that said sealed beam headlightswere required. in europe, the standard used was a performance standard. the equipment standard held backdevelopment in the united states of quartz halogen bulbs, which are far superior in performance. it took a revisionof the law to permit their use here. consequently, i would urge us to specify performance standards rather thanalgorithmic standards.paul games (department of human development and family studies, pennsylvania state university):one of the things that i am most disturbed by in statistics is what some people promote as causal analysis. this iswhere they take what amounts to correlational data that has been collected in strange ways and, after merelygetting regression weights and using what they call ﬁthe causal path,ﬂ produce fantastic interpretations of theexperimental outcomes. is there anything being done in statistical packages that might induce a little sophisticationin those people?morning discussion29the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.forrest young: the recent development along that line is that those kinds of analyses have now beenadded to the major statistical packages.keith muller: this is an issue of philosophy and education. if you don't value such causal analyses, thenyou should teach your consultees accordingly. that is a statistical issue, and not a statistical computing issue, iwould argue.barbara ryan: there is a question of how much expertise and training we can build into software.people sometimes learn about statistics first from a software package manual. there is a philosophical issue of howmuch teaching you can provide through software vehicles. much is being provided by training courses. manypackages offer a lot of training with them, but provided through the software rather than through the university orindependent workshops run by institutes for professionals.it may also be a practical issue. if people keep learning from packages, perhaps the best way will then be toprovide more statistical education and guidance through a software vehicle.paul velleman: i see that more as an opportunity than a problem. the biggest problem in teachingstatistics is convincing your students that they really want to know this. when a person already has a statisticspackage in front of him and wants to understand it, that is the time to teach him. the minitab student handbook,which was really the first tutorial packagebased manual, was therefore an important innovation.barbara ryan: there is a problem when the people writing the manuals are not statisticians. it takes theeducational role away from people who are the real educators. one must find the right balance.keith muller: concerning the issue of large data sets that several members of the audience raisedearlier, i presented a paper with some coauthors a few years ago [muller, et al., 1982] in which we talked aboutthe analysis of ﬁnot smallﬂ data sets. i suggested there that one needed to take the base 10 logarithm of the numberof observations to classify the task that one faced. one could classify small data sets as those involving a hundredobservations or fewer, not small as those in the 1,000 to 50,000 or 100,000 range, large as 100,000 to 1 million,and very large as 10 million or more. if we were to classify statistical software according to the size of data set forwhich it does work, it would help the user because it is obvious that people run into problems and that softwaredoes not transport across those soft boundaries.also, i neglected to mention a paper by richard roistacher on a proposal for an interchange format[roistacher, 1978]. that appeared a few years ago, and there has been a thundering silence following itsappearance.william eddy: there's been about 15 years of silence following that.morning discussion30the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.robert teitel: at the interface conference that was held in north carolina in 1978 or 1979, roistachergot to the point of having most of the vendors accepting that standard in principle. but then, as i understand it, hisagency contract ran out of money, so that didn't go anywhere.the notion of small or medium or large cannot be done in absolute terms. ﬁsmallmediumlargeﬂ is relative tothe equipment you are using. many people would consider 10,000 observations on a hundred variables to beenormous, if you are trying to run it on a pc. i like to define ﬁlargeﬂ as any size data set that you have difficultyhandling on the equipment you are using.clifton bailey: if we had analog data on all of the medicare patients like that which is collected atbedside, our data sets would be very, very small.daniel carr (george mason university): i worked on the analysis of a large data set project years ago.leo breiman [university of california at berkeley] has raised the issue that complexity of the data is moreimportant than the large sample size. for instance, if someone said to me, ﬁi have 500 variables, what do i do?ﬂ, iwould say, ﬁthat is not the ‚large™ i like to work with. i like to have hundreds of thousands of observations withonly three variables.ﬂ complexity is an issue in the different types of data.i am very interested in interfaces to large databases. most of the data that is collected is never seen byhumans and i think that is a tragedy, because a lot of this data is very important. so i think statistical packagesneed more interfaces to standard government data sets. i went to the extreme of trying to interface gras andest; gras is a geographical information system. the neat thing about gras is, it already had the tools to readlots of government tapes. having that as a part of standard statistical packages would be great.at some point, i believe we are going to have to think differently about how we analyze large data sets. somethings are just too big to keep. in fact, a lot of data is summarized at the census level. so i think statisticians aregoing to have to think about flowthrough analysis at some point. it may take a new generation of people toactually do that.another topic brought up was what i call data analysis management. more and more, there is emphasis onquality assurance in analysis. most of that means proving what you did. it does not mean having to do it right, butat least proving what you did. i think that needs to be built into the software, so that we can keep a record ofexactly what we did and can clean up the record. when i do things interactively i keep a log of it, and then i goback and clean out the mistakes and rerun it. but i think we need to have this kind of record, and it needs to beannotated so that it is meaningful later on. a lot of times, i go back to my old analysis, look at it, and ask, ﬁwhatwas i doing?ﬂ i often think maybe i made a mistake. then two days later i realize i really did know what i wasdoing, so it was right. but i had forgotten in the meantime. so there's a need for this quality assurance feature, dataanalysis management with annotation, which may include dictation, voice, whatever is easy.morning discussion31the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.i would also like to see some highend graphics tools. i am envious of people right now who can makemovies readily. it is a very powerful communications medium that ought to be part of statistical software. i wouldlike to see more integration of mathematics tools like mathematica. somebody ought to be addressing memorymanagement; eight megabytes on a spark workstation may not be enough, depending on the software i am using.i would like to have more involvement with standards. for example, i am not sure that the standards that aredeveloped are always optimal for statistical analysis. for instance, there are some defaults on the iris workstationsfor projection that may not produce exactly the projections i would like for stereo. but at some point they even getbuilt into the hardware, and so i have to program around them. it would be nice if in some areas we could getinvolved with the standards, both for hardware and software, and that boundary is getting closer all the time.referencesefron, bradley, 1988, computerintensive methods in statistical regression, siam review, vol. 30, no. 3, 421œ449.efron, bradley, and robert tibshirani, 1991, statistical data analysis in the computer age, science, vol. 253, 390œ395.muller, k.e., j.c. smith, and j.s. bass, 1982, managing ﬁnot smallﬂ datasets in a research environment, sugi '82proceedings of the seventhannual sas user's group international conference.nachtsheim, christopher j., 1987, tools for computeraided design of experiments, journal of quality technology, vol. 19, no. 3, 132œ160.roistacher, r.c., 1978, data interchange file: progress toward design and implementation, in proceedings of the 11th symposium on theinterface: statistics and computing science, interface foundation, reston, va., pp. 274œ284.morning discussion32the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.afternoon session opening remarksforrest younguniversity of north carolina, chapel hillthere was talk at various times this morning about standardization and occasionally about certification. thepanel on guidelines for statistical software is about neither of those, and it is important to emphasize that. ourbusiness is guidelines, not issuing seals of approval.if you think particularly about the three topics of exactness, richness, and guidance, it is hard to know howone would decide for the last two, richness and guidance, that something deserves a seal of approval. making suchjudgments for exactness is a possibility, although i am not saying i think that is a good thing to do. the panel aimsonly to state guidelines, not to set standards or issue seals of approval. it is certainly possible, though, to setstandards for exactness. for richness or guidance, however, standardslet alone certificationmay not bepossible.another theme that came up several times in the morning was layering, that there should be different layersof the software system. i tend to see this as related to this afternoon's featured topic of guidance in that there canperhaps be an outer layer of a statistical system whose purpose is to guide the relatively unsophisticated user.in my ideal data analysis environment, such a layer would be there for the more naive user, but would nothave to be there; it need not be used by a more sophisticated user. there would be several layers. perhaps theinnermost layer would be just a language. a complete system would need to have more layers put on the outside tohelp people who are less sophisticated in terms of the data analysis, but who are very interested in the application.another theme from this morning was that of strategy, which is a central idea in guidance. paul tukeymentioned that one ought to have a strategy for doing regression modeling. also, paul velleman presented twostrategies. one is an original strategy for doing statistical analysis based on batch submission of analyses, whereone first reads in the data and then specifies the strategy, afterward producing output. that is a very linear strategywithout any choices in it. later, he presented a much more involved strategy, more in tune with exploratory dataanalysis, where data is read at the beginning and displayed, whereupon the user is faced with a lot of optionshaving to do with outliers, with diagnosing problems in the data, or with putting the data into subgroups andtransforming the data. basically, that is another idea of a strategy in data analysis. strategies are important forproviding guidance.in a paper i presented at the asa conference in august of 1990 on that topic [lubinsky and young, 1990],there were a couple of slides on guidance showing my ideas along this line. figure 3 is a mockup of a proofofconcept system that david lubinskyafternoon session opening remarks33the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.of at&t bell laboratories and i worked on. there is a window with a cyclic graph in it. as paul vellemanpointed out this morning, it has an entry point, but no exit. this represents the process of data analysis. it is neverfinished. but you can exit at any point you want. there is no specified plan of things that must be done before youcan quit. but when you do exit, the system would suggest a thing to do.figure 3: one possible way of guiding a data analysis. reprinted, with permission, from lubinsky and young[1990]. copyright © 1990 by american statistical association.for example, the grayedin box is suggesting that the first thing to do is to select the data. when that has beendone, a substrategy might be given, a recursive definition of a strategy. a new strategy box opens up that focusesboth on variables and observations or cases. when that is finished, that box closes.then the user goes to the next set of possible things that the strategy would suggest, either describing thedata, transforming the data, or defining a model. as the flow indicates, if you describe the data, you still can againtransform data or define the model, and conversely for transforming. but once you have a model defined, the onlything the strategies then suggest you do is to fit the model. fitting the model itself is recursively defined. withinthat one would see a more involved strategy depicting what to do.this is one possible way of guiding a data analysis. where does this strategy graph come from? it comes froman expert. somewhere, an expert at multiple regression must have sat down and created this graph. in fact, thisgraph was created by lubinsky and me after looking at the book by daniel and wood [1980], where such astrategy for doing multiple regression appears on the inside front cover. there are also analogous graphs presentedfor principal components in a factor analysis, for example. such sources forafternoon session opening remarks34the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.guidance strategies are available, and experts can certainly be consulted for strategies to guide data analyses.referencesdaniel, c., and f.s. wood, 1980, fitting equations to data, john wiley & sons, new york.lubinsky, d.j., and f.w. young, 1990, guiding data analysis, proceedings of section on computational statistics, american statisticalassociation, alexandria, va.afternoon session opening remarks35the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.afternoon session opening remarks36the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.an industry viewandrew kirsch3mintroductionfor over 30 years, substantial efforts have been made at 3m to transfer statistical skills and thinking fromprofessional statisticians to engineers, scientists, and business people. our goal is to decentralize statisticalknowledge and services, and our perspective on statistical software reflects this goal.statistical software plays a key role in the dissemination process. in training, software allows the students toconcentrate on concepts rather than calculations. in application, software provides a link to what was learned inclass and a fast means to apply it.the author is a member of a central group, with consulting, training, and software responsibilities, that acts as acatalyst for this process. knowledge of the (internal) clients' needs comes from consulting and training interactionsand from formal surveys. until recently this central group was also active in internal software development. suchexperience imparts an awareness of the formidable challenges that software providers face in serving the statisticalneeds of industrial users. this presentation focuses on requirements and critical success factors for statisticalsoftware intended for the nonstatistician, industrial user.requirements for ﬁindustrialgradeﬂ softwarethe requirements for ﬁindustrialgradeﬂ software are fairly demanding. in addition to such givens asacceptable cost and core statistical capabilities, it is critical that the software be available for a wide variety ofhardware environments. nearidentical versions must be available on commonly used microcomputers andmainframes to facilitate transparent data connectivity, communications on technical matters, and training. thealternative is another tower of babel. the nature of the users themselves imposes further requirements. a widerange of user skill levels must be accommodated, from novices to nearexperts. the basic user can be easilyoverwhelmed by too many options, while the advanced user can be frustrated by inflexibility.an industry view37the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.the nonstatistician sees statistical design and analysis as part of a larger task of product development,process control, or process management. such a user expects the software to perform the statistical tasks properlyand not to inhibit performance of the larger task. for this reason, it is important that the software should interfacewith commonly used database, spreadsheet, and word processing programs. graphical display of results inpresentationquality output (both on screen and as hard copy) is also important. beyond these, the user will bedelighted by other features, such as the automatic creation of data entry forms or uncoded response surface plotsthat go the extra mile to aid the solution of the larger task. strong prejudices regarding statistical software areapparent even among nonstatistician users, but this is more than just bullheadedness. in an atmosphere of stiffbusiness competition where time to market is vital to success, there is limited time available to learn new supporttools. the industrial user will be pleased by new capabilities but desires upwardly compatible releases.other software requirements include availability of support (either locally or by the vendor), complete andreadable documentation, and, in an age of increasingly global businesses, suitability for persons with limitedenglishlanguage skills.but the most crucial requirement for ﬁindustrialgradeﬂ statistical software, beyond core capabilities, is easeof use for infrequent users! this overriding need is suggested by an internal company survey that showed that,with the exception of basic statistical summaries, plots, and charts, most statistical methods were used by anengineer or scientist on a oncepermonth or onceperquarter basis. from a human factors perspective, thissuggests that most industrial users cannot be expected to remember a complex set of commands, protocols, ormovements in order to use the statistical software. appropriate software must rely on recognition (e.g., of icons orapplicationoriented menus) rather than recall (e.g., of commands or algorithmoriented menus) to satisfy theseusers.of course, ease of use means more than just recognition aids for the infrequent user. other important facetsare: wellplanned display of information; understandable and consistent terminology; reasonable response time; and helpful error handling and correction.but the author wants to stress sensitivity to the needs of the infrequent user, which are so often overlooked.while a visually attractive package will sell some copies, only those that genuinely meet the requirements of theinfrequent user will survive in the long run.an industry view38the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.implications for richness.given the wide range of user skill levels that must be accommodated by statistical software, creative methodsmust be employed to ensure the appropriate richness for each user. it is unacceptable to provide software that isdesigned to meet the needs of the advanced user and assume that the basic user can simply ignore any options oroutputs that are not needed. experience indicates that such a strategy will scare off many basic users. on the otherhand, providing different software for basic and advanced users induces an artificial barrier to learning andcommunication. an alternative is to provide software that can ﬁgrowﬂ as the user's skills grow. on output, forinstance, this means that different layers of output can be selected reflecting increasing degrees of richness.while configurable output is already available in many statistical packages, another kind of configuration hasyet to be widely exploited. this is the concept of configurable options. in a menudriven program, this means thatthe user (or local expert) can configure the menus to exclude certain options that are not needed and only addﬁclutterﬂ to the program. an example of a nonstatistical package employing this feature is microsoft word forwindows®. with configurable options, the user or local expert could eliminate unused or undesired options ofspecific output (e.g., durbinwatson statistic) or of whole methods (e.g., nonlinear regression).another aspect of richness that is important for the advanced user is the ability to add frequently usedcapabilities via some sort of ﬁmacroﬂ command sequence. as mentioned earlier, strong prejudices exist amongstatistical software users. a new package may well be rejected by a user or group of users because it lacks onlyone or two capabilities that the user(s) have come to depend on. the capability to build macros helps overcomesuch barriers. even better is the capability to incorporate such macros into the normal program flow as menu itemsor iconsa capability also available on word for windows. this is the ﬁflip sideﬂ of configurable options.with configurable options, statistical software can avoid the ﬁonesizefitsallﬂ fallacy, without inducingartificial barriers to learning or communications.implications for guidancethe novice user, limited by time constraints and capacity to recall, benefits greatly by appropriate guidance.such a user is not primarily concerned with guidance as to what the software is doing at a computational level; heor she will generally rely on the trainer or local expert to vouch for its validity and appropriateness. the noviceuser is more concerned about guidance on how to progress through a reasonable design or analysis from start tofinish and how to interpret the output.an industry view39the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.clearly it is easier to provide guidance for progressing through a specific analysis (e.g., regression modelfitting) than for a general exploratory analysis. the same can be said for software providing design capabilities:guidance on the particulars of a class of designs (e.g., plackettburman designs) will be easier to provide thanguidance on the selection of an appropriate class of designs. for guidance through these more specific kinds ofdesign or analysis problems, it is helpful to have the software options organized by user task rather than bystatistical algorithm. for example, it is preferable that algorithms for residual analysis be available within both thelinear regression and anova options rather than requiring the user to move to a separate option for residualanalysis from each starting point.another useful tool for guidance on a specific design or analysis is online help for interpreting the output. itis interesting that online help for command specification or option selection has been available for years, yet online help for output is much less widely used. it seems that many statisticians feel uncomfortable about the idea of acanned interpretation of the results of an anova or regression analysis. yet even a modest degree of memoryjogging would be enormously helpful. why should a nonstatistician be expected to remember that in a regressionanalysis, a small pvalue on the lackoffit test is ﬁbadﬂ (i.e., evidence of lack of fit), while a small pvalue on thetest for the model is ﬁgoodﬂ (i.e., evidence of a meaningful model)? will professional sensibilities be offended ifan online help statement simply reminds the user (upon request) of the definition of a pvalue and theimplications of a pvalue close to 0 for that particular test?the author, too, is wary of excessive guidance that would encourage a ﬁblack boxﬂ attitude toward design oranalysis, but feels that statisticians and other software developers must overcome a case of scruples in this regard. amiddle ground really is possible.guidance on the more general kinds of design and analysis issues invites the development of knowledgebased (expert) systems. a major difficulty here is the nature of statistical knowledgeit is a generic methodologyapplied to diverse subjectmatter areas. a knowledgebased system for, say, medical diagnosis could be relativelystand alone, but effective application of statistical methods requires the integration of subject matterconsiderations. any standalone system for statistical methods risks segregating statistics from subject matterknowledge in a dangerous way.for example, in a drying process, oven temperature and product mass can be controlled. without someincorporation of the basic physics regarding the multiplicative effect of these variables on heat transferred, thestandalone program might erroneously recommend a 2 × 2 design (where the ab and ab conditions are identicalfor heat transfer) or suggest fitting an additive model.one option is to create statistical expert systems to which subjectmatter knowledge can be appended. shortof that, statistical knowledgebased systems can exhibit an appropriate degree of modesty by proscribing ratherthan prescribing: pointing out options that are untenable and suggesting plausible options to explore further, ratherthan trying to identify one or a few solutions that are ﬁthe best.ﬂ the system can identify appropriate memoryjogging questions from a catalog of questions such as the following:an industry view40the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved. has an appropriate degree of replication been included in the design? was this data collected in a completely randomized fashion (as the type of analysis might suggest), orwas it collected in blocks?needless to say, all of these features would be available upon the user's request rather than imposed withoutrecourse by the program.conclusionfar too many software development dollars are being devoted to adding new capabilities and far too few toenhancing ease of use. it might shock some software providers to realize that the biggest competitor for therecommended statistical software at 3m is not another statistics package, but lotus 123®!menudriven software and windowed software for nonstatistical applications have raised the easeofuseexpectations of statistical software users. there is a substantial fraction of potential users in industry that will notﬁbuy inﬂ to a statistical software solution that does not combine stateoftheart ease of use with core capabilities,acceptable cost, and multiple hardware availability.the future of statistical software is not just a technical issue; it is also a business issue. the providers of moststatistical software are private, profitmaking companies. these firms often rely on new releases with addedcapabilities to produce current revenue. in the industrial market, far more revenue is available by providingenhanced ease of use than by adding noncore capabilities. it is hoped that future competition among softwareproducts will be more on the basis of guidance and configurability than on the basis of additional noncorecapabilities.an industry view41the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.an industry view42the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.guidance for oneway anovawilliam dumouchelbbn software productsgoals of guidancethe first goal of guidance is to permit the occasional, infrequent user whose main business is not statisticaldata analysis (such as a scientist or engineer) to achieve the benefits of using basic statistical procedures. thesebenefits are the ability to make comparisons, predictions, and so forth, with measures of uncertainty attached. oneof the key notions is to understand what is meant by ﬁmeasures of uncertaintyﬂ and how to convey them in thecomputer output, while avoiding the most common pitfalls and inappropriate applications that one can fall into.the second goal of guidance is to overcome the barriers that prevent technical professionals from usingstatistical models. such barriers were covered quite well by andrew kirsch in the preceding talk. one barrier is indealing with people who have not had courses in statistics or, worse yet, who have had a poorly taught statisticscourse. another is that nondeterministic thinking is just not the natural evolutionary way our brain seems to havedeveloped. so it is an unfamiliar and different concept to many.further, statistical jargon is quite alienating, in the same way that any jargon is alienating. statisticians inparticular, though, seem to have developed such a multitude of techniques that have different names. at first theyseem very arbitrary and unrelated. even if individuals can produce an analysis by working slowly through some ofthe software, they do not feel confident enough about the analysis to write a report or explain it to a supervisor.that could be a barrier to their attempting to do the analysis at all. moreover, in trying to overcome those sorts ofbarriers, there are many software design barriers. one must identify the motivating philosophy, in attempting todeal with these issues, because there are many potential solutions that might conflict with other perceived truths inthe statistics community.philosophy of guidancei was impressed by the degree to which all of the previous speakers seemed to embody the same kind ofphilosophy as mine. there seems to be a secular trend in theguidance for oneway anova43the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.philosophy of statistics, and the textbooks have not caught up with it. the kind of textbooks and the type ofstatistical teaching that were so prevalent in the 1950s and 1960s and perhaps even into the 1970s are no longeraccepted by expert users, as exemplified by today's speakers. unfortunately, there is a frankenstein monster outthere of hypothesis testing and p values, and so on, that is impossible to stop. most people think that statistics ishypothesis testing. there is a statistical education issue here for which i do not have a quick solution.so here are the principles of my philosophy of guidance. graphics should be somehow totally integrated, andone should not ever think of doing a data analysis without a graph. the focus should be on the task rather than thetechnique, emphasizing the commonality of different analysis problems. by keying on the commonality, what islearned in one scenario will help in another one. different sample sizes, designs, and distributions must besmoothly supported. merely having equal or unequal numbers in each group should not require that the usersuddenly go to a different chapter of the user's manual. an occasional or infrequent user who doesn't understandwhy that should be necessary will be totally alienated. as mentioned before, hypothesis testing should be deemphasized in favor of point and interval estimation. simple, easytovisualize techniques should be chosen.lastly, the statistical software should help as much as possible with the interpretation of the results and with theassembling of the report.recognizing the oneway anova problemwith these guidance ideas in mind, one of the first things to note is that ﬁoneway anovaﬂ is, of course,jargon. what does that really mean? how are people to know that they should use a program called onewayanova when they need to compare different groups?it is easy to give guidance if the scientific questions can be rephrased in terms of simple variables. astatistical variable is not a natural thing. in statistics training, a random variable is drummed into the studentearlier. it is better to phrase all of the statistical scientific questions in terms of questions about relations betweenvariables. instead of comparing the rats on this diet and the rats on that diet, one wants to know if there is arelationship in rats between weight gain and diet, with weight gain being one variable and diet another.that is not a natural use of the language for most people. yet software is much better used if the user has tothink about a database of random variables and relationships between those variables. forcing users to do that is,in a sense, a disadvantage of software, but also an ultimate advantage for users; if people understand and thinkabout random variables, it will greatly help them to think about statistical issues in the right manner. having usersthink in terms of variables may be doing them a favor.guidance for oneway anova44the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.metadata includes the description of variables in terms of their units, the types of the data, and so forth.software should include specific spots for that kind of data; it is the means by which guidance on softwarebecomes feasible. if one creates a data dictionary, each entry should include a variable name, a description, someindication as to whether it is categorical or a measurement scale, and what its range of values is.a partition is a quite handy further refinement of this, if a variable can have several values and there isinterest in a coarser strain of a given value. it may be easier for the software to address such subsets of values.with this situation, it is relatively easy to provide assistance. but a step must be taken to get the user to createthose kinds of databases. afterward, it is easy to talk about a response variable versus a predictor variable, or afactor versus a response. one might then ask, how is this categorical variable associated with that continuousvariable? short dialogues with the computer could address that. once that dialogue is completed, the softwareshould immediately display on the screen a graphical representation that has been integrated with the statisticalanalysis or inference procedure.i do not much mind if a student confuses the definition of a distribution with the definition of a histogram,since one is a picture of the other. a histogram is something one can study, draw, and get a feel for, whereas adistribution is more an abstract concept. i would not mind if that student confused the issue that a onewayanova is a method for looking at a set of box plots, namely, the representation of a continuous versus acategorical variable, and focused on the distributions in each category.i believe the idea that a oneway anova is an ftest is entirely wrong. a oneway anova is merely amethod for zeroing in on what a box plot might tell. of course, there are many different ways one can do this. oneway is to examine the plot and notice that there are a few outliers. there are quite a few directions one might wantto go in that case: some statistical model or analysis tack may be preferable, depending on the data, or the systemmight suggest using means as a representation rather than medians, since there are not too many outliers.adaptive fitting procedurewhere to go after looking at the box plot is rather data dependent and also dependent on any other goalsassociated with the given data. there should be some automatic screening or adaptive fitting procedure asguidance, in which the software itself does the kinds of things that most statisticians would recommend. thisincludes such things as recognizing horribly skewed distributions, or recognizing when a response variable onlytakes two or three values, examining whether or not there are differences between the spreads in each group. thestatistical software should then compose a model description and make some recommendations.guidance for oneway anova45the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.guidance for interpretation.there are many different problems in interpreting such data, even though oneway anova is thought of asone of the simplest of all statistical problems. but, as we heard this morning, even such a simple problem can bepartitioned into many different tests. one can give descriptions of the model and/or data, explore the residuals,explain the anova table, produce confidence intervalsto understand esoterica such as the simultaneous versusthe nonsimultaneous approach to estimating confidence intervalsand make confidence intervals for fittedvalues.again, software can help with that task. as an example, consider having software that produces a boilerplatedescription of the data being examined; for example,data are drawn from the nurturedat dataset. the variables gain vs diet are modeled with n = 45. diet isan unranked categorical variable. there are 3 levels of diet all with sample size 15. the means of gain range from81.1 (diet = control) to 152.1 (diet = liquid). there are three values of gain classed as extreme observations bythe boxplot criterion.why would one want software to produce such a simple boilerplate description? from many years of teachingin various universities, i have learned that it is amazingly hard to produce a student who can reliably write such aparagraph. in actual fact, it is hard to get students to focus on describing these quantitative issues. the same is truewith getting them to explain the single box plot and how to express in a couple of sentences a description of aconfidence interval for the mean. thus the software might also be capable of producing something such asif diet = liquid, half of the 15 values of gain are within the boxed area of the plot, an interquartile range (iqr) of28. there is 1 value classed as an extreme observation (more than 1.5 iqr from the nearest quartile). the groupmean is 152.1, and the true mean is between 139 and 165.3 with 95% confidence.these are the kinds of boilerplate descriptions that infrequent users especially, but even the people who areright in the middle of their course, have trouble producing.of course, that is even truer when it comes to explaining the results of an f test for anova. so again, onecould have a display such asthere is strong evidence that diet affects gain, since an f as large as 34.44 would only occur about 0% of the timeif there were no consistent differences between diet groups.guidance for oneway anova46the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.such a display assures that the user does not reverse the situation and say that the f test is not significantwhen in fact it is.many statisticians would feel unhappy about software that produces such sentences, saying that there are notenough qualifications. on the other hand, if too many sentences are produced in qualification for that boilerplate,people are going to write front ends for the system in order to screen off the first four sentences of everyparagraph, knowing those sentences will not say anything worthwhile.aside from the issue of data dependence interpretations, there are trickier issues, such as what one means by asimultaneous confidence interval. suppose the following standard sentence is produced describing how tointerpret a particular confidence interval:the true mean of gain where diet = liquid, minus the true mean where diet = solid, is approximately 10.9, andis between 14.7 and 36.7 using simultaneous 95% confidence intervals.what does ﬁsimultaneousﬂ mean? there needs to be at least some explanatory glossary, perhaps as an option,so that when a strange word is encountered a couple of sentences helping to define it can be given, such assimultaneous 95% (or 99%, etc.) confidence intervals are wider, and therefore more reliable, than 95%nonsimultaneous intervals, because they contain the true difference for every comparison in 95% of experimentswhile the later intervals are merely designed to be accurate in 95% of comparisons, even within one experiment. usenonsimultaneous intervals only if the comparisons being displayed were of primary interest at the design stage of theexperiment; otherwise you risk being misled by a chance result.one of the primary areas where guidance is needed is in residual analysis, something we are all supposed todo. for those infrequent users, it is again a little intimidating. one can have many plots available, but it is notexactly clear to those infrequent users which they should examine. part of the guidance can come from thestructure of the menu system. one can make it quite easy to look at residuals, rather than having to save residualsas a separate variable and leave the original analysis to start up another analysis where some residual plots aredone. by making residual analysis very easy, the user can be encouraged to try some of the menu items (e.g.,which graphs to look at, what to look for on the graph, point out features of this graph) and see what happens.guidance for oneway anova47the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.guidance for report writinganother issue that software needs to address is guidance for report writing. having the software keep somekind of log or diary is important. a log is a verbatim log, which has the beauty that when it is replayed, a repetitionof all the actions takes place. a diary is oriented more toward an end user, as a means of keeping track of what theuser has done. the diary needs to contain enough information to reproduce the analysis but not necessarily in thatlinear mode of the same steps that the user went through.in order to reproduce a given analysis, all one really needs is an object state record that encodes such thingsas which cases were included and which model was being used. one does not necessarily need to record all thesteps that led there. this kind of diary should collect interpretations produced by the system, as well as recordingtransformations and variable definitions, and it should also have a place to record the notes that the user might putin, along with references to tables and graphs that were saved. in the end, the user will have a compilation ofinformation that gives solid help in assembling a report. those boilerplate sentences have the right statisticaljargon and are used appropriately with the right parts of speech, so that at least they can be captured and put into areport.guidance regarding tacit technical assumptionsi now want to talk a bit more technically about some problems related to giving guidance. how can oneovercome what are major pitfalls, violations of the major statistical assumptions made in the statistical model, inthe oneway anova layout? one assumption is that the variances are supposed to be equal in each group, and theother is that the mean is a good summary because the data is not too outlier prone.adjusting for unequal dispersionsthe first problem concerns adjusting for unequal dispersions. when one looks for textbook advice onalternatives, the textbooks do not usually give very explicit advice, but rather provide implicit advice. often, onepart of a textbook will say that the variances are to be assumed equal, and it will not say what to do when they arenot equal. there may be an inference of ﬁfirst do this and then do that,ﬂ but not something explicitly stated.the difficulty with most comparisons of variances is that they are of very low power. the idea that oneshould always assume variances to be equal just because it cannot be proven otherwise is a tricky one. a largesample versus a small sample will yield radically different power. when there are many groups versus fewgroups, even theguidance for oneway anova48the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.comparison of variances becomes quite muddied. if the distribution is not an exactly normal distribution, thetypical outlier test for comparing variances is known to be biased. regarding testing versus estimation, if one hashuge sample sizes and one variance is proved significantly to be 20 percent bigger than another variance, that isnot at all relevant to the question of what kind of procedure one should use for oneway anova.the infrequent and nonstatistician user does not want to be concerned with all these technicalities. he or shewants software that will handle it all automatically.one approach is tantamount to applying, in the background, an empirically based shrinkage estimator to themeasures of dispersion, and using that estimator as the foundation of a guidance or automatic methodology. onedoes not want to be too sensitive to an outlierprone distribution; one must distinguish between having an outlierand having a wider dispersion.it is important that this estimator be based on the sample sizes. if there is a sample size 10 and a sample size100 for the two groups to be compared, it is certain that one of the interquartile ranges is estimated much moreaccurately than the other. on the other hand, if there are 10 groups each of size 4, the fact that a few of those haveinterquartile ranges quite different from the average must not be overemphasized.after obtaining such an estimator, one should, according to one perspective, do nothing unless the differencesin the estimated interquartile ranges are great. after having shrunk them toward the average, however, if one is,say, double the other, then a method that assumes unequal variances might be recommended. in that case, it ispresumed that the variances in each group are proportional to the squares of the shrinkage estimates of theinterquartile ranges.adjusting for outlierprone datathe second issue, adjusting for outlierprone data, raises the issue of how the software can be more robustwhen comparing measures of location. as there is a huge literature on robust estimations, one again faces thequestion of how much complication to include. most infrequent users, or scientists and industrial engineers,merely want the answer; they do not want to focus on the various techniques they could have chosen to get thatanswer. one must of course try to prevent misuses that can occur when the results of a computer package arereligiously applied, and to make sure that a few extreme responses do not distort the actual statistical estimatesthat are being presented. on the other hand, if the data are not very outlier prone, most people would prefer to usethe familiar leastsquares estimates, and techniques based on means. this avoids having the software usercontinually defend the fact that the answers differ a little from those given by some other package.when some nonclassical approach is warranted, it should not necessitate learning an entirely new softwareinterface. that is one of the biggest troubles regarding the socalled nonparametric techniques that weredeveloped and popularized in the 1950s and 1960s. they are accompanied by a whole range of limitationswhereby, although the sameguidance for oneway anova49the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.scientific problem is at issue, elegant solutions may not be available in some cases due to a technicality; verydifferentlooking computer output might attend problems that appear superficially similar to a casual user. tofacilitate focusing on the task instead of the technique, one might have to sacrifice some theoretical rigor that analternate technique might have. also, there is the question of how to choose between the moreand lessrigorousversions. an explicit threshold or criteria must be available.in the context of oneway anova, when the robust estimation is recommended, what might be done? thebox plot has been seen as a generic graph describing the oneway anova problem. in this robust version, the boxplot of course represents the median as one of the more prominent points for each group. so it would seem that themedian would be the natural thing to take as the alternative robust estimate, in order to have a tiein with the graphthat was being used to drive the analysis. the problem with using the median in a general linear model frameworkis that the median does not have an easily obtained sampling distribution. a more approximate approach is forced.one such approach is to have several samples rather than just one sample, and to use as a general measure ofdispersion a multiple of the interquartile range of the residuals, after subtracting off each group median.if the object is to provide an ease of use that allows the software user to focus on the task rather than thetechnique, some new techniques may have to be invented in conjunction with this. the users never see any of this;they just see a system suggestion that the confidence intervals for contrast be based on medians. they can overridethat suggestion if they want. then they merely get confidence intervals for contrasts and predictions withoutneeding to go into an entirely different software framework.in summary, for the oneway anova layout, insoftware guidance is possible. there are, however, morecomplicated scenarios that could be called oneway anova that are not covered by my remarks, e.g., issuesabout sampling methods and the validity of inference to target populations.there is no doubt that whenever a piece of software provides some kind of guidance, it will offend a certainfraction of the statistics community. this is because whenever you give a problem to several statisticians, theyeach will come back with different answers. statistics seems to be an art, and very hard to standardize.more than anything else, the means to providing better guidance is to make the entire data analysis processmore transparent. in the definition of oneway anova, one should be looking at a box plot and determining ifthere is more that can be used there. if a person can interactively point and click, and so really get hold of a boxplot, presumably more can be done with it. perhaps a manipulation metaphor can make the goals of statistics moretransparent and concrete, as well as the uncertainty measures produced by a statistical program. this is going toproduce more for the guidance and overall understanding than boilerplate dialogues that attempt to mimic thediscussion that an expert statistician might have with a client. still, in order to produce that transparency, therewill always have to be practical compromises with elegant theory before the everincreasing numbers of dataanalysts can have ready access to the benefits of statistical methods.guidance for oneway anova50the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.incorporating statistical expertise into data analysis softwaredaryl pregibonat&t bell laboratoriesoverviewnearly 10 years have passed since initial efforts to put statistical expertise into data analysis software werereported. it is fair to say that the ambitious goals articulated then have not yet been realized. the short history ofsuch efforts is reviewed here with a view toward identifying what went wrong. since the need to encode statisticalexpertise in software will go on, this is a necessary step to making progress in the future.statistical software is guided by statistical practice. the firstgeneration packages were aimed at datareduction. batch processing had no competition, and users had little choice. the late 1960s brought an era ofchallenging assumptions. robust estimators were introduced ﬁby the thousandsﬂ (a slight exaggeration), but thesewere slow to enter into statistical software, as emphasis was on numerical accuracy. by the late 1970s, diagnosticsfor and generalizations of classical models were developed, and graphical methods started to gain somelegitimacy. interactive computing was available, but statistical software was slow to capitalize on it. by the late1980s, computerintensive methods made huge inroads. resampling techniques and semiparametric models freedinvestigators from the shackles of normal theory and linear models. dynamic graphical techniques were developedto handle increasingly complex data.the capabilities and uses of statistical software have progressed smoothly and unerringly from data reductionto data production. accomplished statisticians argue that alternative model formulations, diagnostics, plots, and soon are necessary for a proper analysis of data. inexperienced users of statistical software are overwhelmed. initialefforts to incorporate statistical expertise into software were aimed at helping inexperienced users navigate throughthe statistical software jungle that had been created. the typical design had the software perform much of thediagnostic checking in the background and report to the user only those checks that had failed, possibly providingdirection on what might be done to alleviate the problem.not surprisingly, such ideas were not enthusiastically embraced by the statistics community. few of thecriticisms were legitimate, as most were concerned with the impossibility of automating the ﬁartﬂ of data analysis.statisticians seemed to be making a distinction between providing statistical expertise in textbooks as opposed tovia software. today the commonly held view is that the latter is no more a threat to one's individualincorporating statistical expertise into data analysis software51the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.methods and prejudices than is the former.given the weak support by peers in the field, and the difficulties inherent with trying to encode expertise intosoftware, some attempts were made to build tools to help those interested in specific statistical topics get started.these toolbuilding projects were even more ambitious than earlier efforts and hardly got off the ground, in partbecause existing hardware and software environments were too fragile and unfriendly. but the major factorlimiting the number of people using these tools was the recognition that (subject matter) context was hard toignore and even harder to incorporate into software than the statistical methodology itself. just how much contextis required in an analysis? when is it used? how is it used? the problems in thoughtfully integrating context intosoftware seemed overwhelming.there was an attempt to finesse the context problem by trying to accommodate rather than integrate contextinto software. specifically, the idea was to mimic for the whole analysis what a variable selection procedure doesfor multiple regression, that is, to provide a multitude of contextfree ﬁanswersﬂ to choose from. context guidesthe ultimate decision about which analysis is appropriate, just as it guides the decision about which variables touse in multiple regression. the separation of the purely algorithmic and the contextdependent aspects of ananalysis seems attractive from the point of view of exploiting the relative strengths of computers (bruteforcecomputation) and humans (thinking). nevertheless, this idea also lacked support and recently died of island fever.(it existed on a workstation that no one used or cared to learn to use.)so where does smart statistical software stand today? the need for it still exists, from the point of view of thenaive user, just as it did 10 years ago. but it is doubtful that this need is sufficient to encourage statisticians to getinvolved; writing books is much easier. but there is another need, this one selfish, that may be enough to effectincreased participation. specifically, the greatest interest in data analysis has always been in the process itself. thedata guides the analysis; it forces action and typically changes the usual course of an analysis. the effect of this oninferences, the bread and butter of statistics, is hard to characterize, but no longer possible to ignore. by encodinginto software the statistician's expertise in data analysis, and by directing statisticians' infatuation with resamplingmethodology, there is now a unique opportunity to study the data analysis process itself. this will allow theoperating characteristics of several tests applied in sequenceor even an entire analysis, as opposed to theproperties of a single test or estimatorto be understood. this is an exciting prospect.the time is also right for such endeavors to succeed, as long as initial goals are kept fairly limited in scope.the main advantage now favoring success is the availability of statistical computing environments with thecapabilities to support the style of programming required. previous attempts had all tried to access or otherwiserecreate the statistical computing environment from the outside. keeping within the boundaries of the statisticalcomputing environment eliminates the need to learn a new language or operating system, thereby increasing thechance that developers and potential users will experiment with early prototypes. both are necessary for thesuccessful incorporation of statistical expertise into data analysis software.incorporating statistical expertise into data analysis software52the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.what is meant by statistical expertise?what do i mean by statistical expertise? let me recommend how to solve it by george polya [polya, 1957].it is a beautiful book on applying mathematics to realworld problems. polya differentiates four steps in themathematical problemsolving process: (1) understanding the problem, (2) devising a plan to solve the problem,(3) carrying out the plan, and (4) looking back on the method of solution and learning from it.all four steps are essential in mathematical problem solving. for instance, devising a plan might consist of,say, using induction. carrying out the plan would be the actual technical steps involved. having proved a specificfact, one might look back and see it as a special case of something else and then be able to generalize the proof andperhaps solve a broader class of problems.what kind of expertise do i want to put into software? many of the steps that polya outlines are very contextdependent. knowledge of the area in which the work is being done is needed. i am not talking about integratingcontext into software. that is ultimately going to be important, but it cannot be done yet. the expertise of concernhere is that of carrying out the plan, the sequence of steps used once the decision has been made to do, say, aregression analysis or a oneway analysis of variance. probably the most interesting things statisticians do takeplace before that.statistical expertise needs to be put into software for at least three reasons. the first of course is to providebetter analysis for nonstatisticians, to provide guidance in the use of those techniques that statisticians think areuseful. the second is to stimulate the development of better software environments for statisticians. sometimesstatisticians actually have to stoop to analyzing data. it would be nice to have help in doing some of the things onewould like to do but has neither the time nor the graduate students for. the third is to study the data analysisprocess itself, and that is my motivating interest. throughout american or even global industry, there is muchadvocacy of statistical process control and of understanding processes. statisticians have a process they espousebut do not know anything about. it is the process of putting together many tiny pieces, the process called dataanalysis, and is not really understood. encoding these pieces provides a platform from which to study this processthat was invented to tell people what to do, and about which little is known.one of the most compelling reasons for resuming efforts to try to infuse guidance into statistical software, andto implement plans, is to better understand the process of analyzing data. but some of this is also motivated byfolly. part of my earlier career dealt with regression diagnostics, which is how to turn a smallsample problem into alargesample problem. one can increase the size of the data set by orders of magnitude. just 100 or 200 data pointscan easily be increased to thousands or tens of thousands. the larger set is highly correlated, of course, and may bereiterating the same information, but it can be produced in great quantity.in the good old days, there was data reduction. this is what analysis of varianceincorporating statistical expertise into data analysis software53the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.did. what began as a big body of data was reduced to mean and standard errors. today with all the computing andstatistics advances, the opposite end of the spectrum has been reached. ever more data is produced, overwhelmingthe users. some of that has to be suppressed.who needs software with statistical expertise?the audiences for statistical software are many and varied. infrequent users probably make up the majority ofusers of statistical software. they want active systems, systems that take control. in other words, they want a blackbox.most professional statisticians are probably frequent users. these users want to be in control, want passivesystems that work on command. one might call such a passive system a glass box, indicating that its users can seewhat it is doing inside and can understand the reasoning that is being used. if such users do not like what they seein the box, they will throw away the answer.but there is a range of things in between. problems are inevitable because, in having to deal with users withvery diverse needs and wants, it is hard to please all users. when building expertise into software it must beremembered that good data analysis relies on pattern recognition, and consequently graphics should be heavilyintegrated into the process. most of what is seen cannot be simply quantified with a single number. plots are donein order to see the unexpected.limitations to the incorporation of statistical expertisea lot of the experience that goes into data analysis cannot be very easily captured. moreover, good dataanalysis relies on the problem context; statistics is not applied in a void. it is applied in biology, as well as inbusiness forecasting. the context is very important, although it is very hard to understand when and where it isimportant. related to this is the difficulty of developing the sequence of required steps, the strategy. finally,implementing that strategy is hard to do. some hard tradeoffs must be made; some engineering decisions that arenot really statistically sound must be made. when a simulation is run, crude rules of thumb evolve and getimplemented. thus hard engineering decisions must be made in order to make any progress here.one guiding principle to follow consistently when incorporating expertise into software, and not merely forstatistical software, is this: whenever something is understoodincorporating statistical expertise into data analysis software54the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.well enough, automate it. matrix inversion has been automated, because it is believed to be well understood. noone wants to see how a matrix inversion routine works, and so it is automated.procedures that are not well understood require user interaction. in all of the systems described below, thereare various levels of this automationinteraction tradeoff.efforts to build data analysis softwarerexgoing back into recent history, there was a system that bill gale and i were involved with called rex (circa1982), an acronym for regression expert [gale, 1986a]. it was what one might call a front end to a statisticssystem, where the thing between the statistics system expertise and the user was an interface called rex. it was arulebased interpreter in which the user never talked directly to the statistics expertise, but only talked to it throughthis intermediary. the user would say something such as, ﬁregression y on x.ﬂ that was, in fact, the syntax, andwas approximately all that the user could say. then, if everything went right and this case was for the most parttextbook data that satisfied a whole bunch of tests, rex would come out with a report, including plots, on thelaser printer.if one of the tests failed, rex would say, ﬁi found a problem and here is how i would suggest you fix it.ﬂafter rex offered the suggestion, the user could pick one of five alternatives: implement that suggestion, showthe user a plot, explain why that suggestion was being made, describe alternatives (if any), or quit as a last resort.rex was fairly dogmatic in that respect. if it found a severe problem and if the user refused all thesuggestions, it would just say that it was not going to continue. such intransigence was fine; in an environmentwhere there were no users, it was easy to get away with that. if there had been any users, one can imagine whatwould have happened: the users would have simply used some other package that would have given them theanswers.what did rex do? it encoded a static plan for simple linear regression. it allowed a nonstatistician to get asingle good analysis of the regression, and it provided limited advice and explanation. it was an attempt to provide aplayground for statisticians to build, study, and calibrate their own statistical plans.on the subject of building, rex was actually built by a sort of bootstrapping. there was no history availableon how to do something such as this. instead, about a halfdozen examples were taken, all fairly small simpleregression problems. your speaker then did many analyses and kept a detailed diary of what was done, and why itwas done. by knowing when something was done in the analysis sequence, one could study thoseincorporating statistical expertise into data analysis software55the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.steps and try to extract what commonality there was. next, bill gale provided an architecture and a language toencode that strategy in the form of a fixed decision tree with ifthen rules associated with each node, such as isshown in figure 4. each procedure encoded one strategy based on about a halfdozen examples.figure 4: example of a fixed decision tree.it had been hoped that the language provided in rex would be a fertile playground for others to bootstraptheir own interests, whether in time series, cluster analysis, or whatever. we knew of no other way to buildstrategies.as to ﬁfeatures,ﬂ please notice the use of quotation marks. whenever a software developer mentions afeature, beware. features are often blunders or inadequacies. in the case of rex, i will even tell you which onesare which.rex had a variable automateinteract cycle. this is a positive feature whereby, if the data were clean, theuser really did not have to interact with the system. in that situation, the user encountered no bothers. it was notlike a menu system in which one has to work around the whole menu tree every time. but if the data set wasproblematic, rex would halt frequently and ask the user for more information so that it could continue.with rex, the user was insulated from the statistical expertise. that is one of those dubious features. manyusers are quite happy that they never have to see any underlying statistical expertise. others are crying to dosomething on their own. this is related to a third socalled feature, that rex was in control. if the user had adifferent ordering of how he or she would like to do regressione.g., looked at x first and then at y, and later justwanted to look at y first and then xthat could not be done. it was very pragmatic; one could not deviate from thepath. again, a certain class of users would be perfectly happy with that.another feature was that the system and language were designed to serve as anincorporating statistical expertise into data analysis software56the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.expert system shell that could be adapted for types of analyses other than regression.several things were learned from the work on rex. the first was that statisticians wanted more control.there were no users, rather merely statisticians looking over my shoulder to see how it was working.automatically, people reacted negatively. they would not have done it that way. in contrast, nonstatisticians towhom it was shown loved it. they wanted less control. in fact they did not want the systemthey wanted answers.to its credit, some of the people who did like it actually learned from rex. someone who did not knowmuch statistics or perhaps had had a course 5 or 10 years before could actually learn something. it was almost likean electronic textbook in that once you had an example, it could be an effective learning tool.the most dismaying discovery of all was that not only did the statisticians around me dislike it, but they werealso not even interested in building their own strategies. this was because the environment was a bit deficient, andthe plan formulationworking through examples and manually extracting commonality in analysesis a hard thingto do.rex died shortly thereafter. the underlying operating system kept changing, and it just became too painful tokeep it alive. there were bigger and better things to do. it was decided to next attack the second part of theproblem, to get more statisticians involved in building expertise into software. as it was known to be a painfultask, the desire was to build an expert system building tool, named student [gale, 1986b].studenti was fairly confident that the only way to end up with strategies and plans for understanding the data analysisprocess was by recording and analyzing the working of examples. yet taking the trees of examples, assimilatingwhat was done, and encoding it all into software was still a hard process. so student was an ambitious attempt tolook over the shoulder and be a big brother to the statistician. it was intended to watch the statistician analyzingexamples, to capture the demands that were issued, to maintain a consistency between what the statistician did forthe current example versus what had been done on previous examples, and to pinpoint why the statistician wasdoing something new this time. the statistician would have to say, e.g., ﬁthis case was concerned with timetheories,ﬂ or ﬁthere was a time component,ﬂ and student would thereby differentiate the cases and then proceed.the idea was that the statistician would build a system encapsulating his or her expertise simply by workingexamples, during which this system would encode the sequence of steps used into software. in terms of thearchitecture, everything resided under the watchful eye of student, with the statistician analyzing new data withinthe statistics package. student watched that person analyze the data and tried to keep him or her honest by saying,ﬁwhat you are doing now differs from what you did in this previous case; can you distinguish why you did it thisway this time and that way that time?ﬂ however, for any analysis there is always an analysis path. student wasmerely going to fill out the complete tree that may be involved in a more complicated problem.incorporating statistical expertise into data analysis software57the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.the student system also had features. statisticians did not have to learn an unfamiliar language orrepresentation, but simply used the statistics package that they ordinarily used, with no intermediate step (incontrast to rex, which had been written in lisp).ﬁknowledge engineeringﬂ was a buzz word 10 years ago in building such systems. a knowledge engineerwas someone who picked the brain of an expert and encoded that knowledge. with student, the program wasdoing that, and so one only needed to engineer a program, and it would subsequently do all the work for anynumber of problems.the system was again designed so that others could modify it, override it, and extend it to other analyses.basically, that ﬁfeatureﬂ did not work, and there are two reasons why. one is that the student system again waswritten in lisp. it actually ran on a different computer than that which was running the statistical software. with anetwork between the two, there were numerous logistical problems that got in the way and made things verytedious.perhaps the other reason it died was the gradual realization of the problem concerning context. it is just notsufficient to capture the sequences of commands that a statistician has issued in the analysis of data. context isnever captured in key strokes. there is almost always some ﬁaha!ﬂ phase in an analysis, when someone is workingat a knot and all of a sudden something pops out. almost always that ﬁaha!ﬂ is related to the context. beyond thetechnical troubles in building and debugging a system such as student was the realization that, in actuality, thewrong problem was being solved. that was what caused the project to be abandoned before progress was evermade.tessthat did not bring efforts to a complete halt, however. in 1987, a completely different approach was triednext with a system called tess. the realization that context is important, and that how to incorporate it intostrategy was unknown, led to the selection of an endaround approach. context would be accommodated ratherthan incorporated. in much the same way that computer chess programs worked, the accommodation would bedone by sheer bruteforce computation.most statisticians have used subset regression or stepwise regression. these programs are simply numbercrunchers and do not know anything about context. they do not know, for instance, that a variable a is cheaper tocollect than variable b or that the two variables are both measuring the same thing. statisticians think that theseregressions are useful, that they help to obtain a qualitative ordering on the variables and thereby perhaps help giveclues to which classes of models are interesting. after the subset selection is done and the class of models isconsidered, context is typically brought in to pick one of the models, after which that chosen model is used tomake inferences.the idea of tess (treebased environment for developing statistical strategy) was to expand that game (oflooking at the subset) to the overall selection of the model. forincorporating statistical expertise into data analysis software58the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.example, in performing a regression with a searchbased approach, one defines a space of descriptions d for aclass of regression data sets y, where those data sets are actually ordered pairs, y and x. the goal is to get ahierarchy of the descriptions. since some of the regressions are going to fit better than others and some will bemore concise than others, one tries to order them. after all possible regression descriptions have been enumerated,the space d is searched for good ones. the user interface of this system is radically different from those of theprevious two. the procedure is to tell the computer, ﬁgive me 10 minutes worth of regressions of y on x.ﬂ (twotransformations were involved to reexpress variables. often, samples or data would be split into two, and outlierswould be split off.) at the end of 10 minutes, a plot is made for each description in the space. there is a measureof accuracy, and also a measure of parsimony, and so a browsing list is started. this can be thought of as a cpplot. for each element of a cp plot, the accuracy is measured by cp, and the verbosity is measured by the numberof parameters.for tess, that concept was generalized. when attempting to instill guidance into software, this overallapproach will be important. for tess, it was necessary to develop a vocabulary of what seemed to be important indescribing regression data, i.e., the qualitative features of the data. these were not merely slopes and intercepts,but rather qualitative features closely linked with how data are represented and summarized. to organize all thesethings, a hierarchy was imposed and then a search procedure devised to traverse this hierarchy. the hope was tofind good descriptions first, because this search could in principle continue forever.tess had three notable ﬁfeaturesﬂ: it was coded from scratch outside of any statistical system; it had a singleautomateinteract cycle, which permitted context to guide the user in selecting a good description; and once again,there were broad aspirations that the design would stress the environment tools so as to allow others to imitate it.general observations on tess and studenttess and student were very different in how they dealt with the issue of context, but there were severalsimilarities. both embodied great hopes that they would provide environments in which statisticians couldultimately explore, build, and analyze their own strategies. ultimately, both used treelike representations forplans. they both used examples in the planbuilding process. and lastly, both are extinct.why did others not follow suit? there are a number of reasons. it was asking a lot to expect an individual todevelop a strategy for a potentially complex task, and to learn a new language or system in which to implement it.these plans with which student and tess were concerned are very different from what is ordinary, usualthinking. a statistician, by training, comes up with a very isolated procedure; for example when one looks fornormality, only a test for normality is applied, and it is assumed that everything else is out of the picture. once allthe complications are brought in, it is more than most people can sort out.the other barrier was always learning a new language or system to implementincorporating statistical expertise into data analysis software59the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.these things. even if we got student running, there was a problem in how we designed it from the knowledgeengineering point of view. it was always outside the statistical system; there was always some additional learningneeded.the implication of all of this is the need to aim at smaller and simpler tasks. also, an overriding issue is tostay within the bounds of the statistical system. there is a great deal of potential here that did not exist 5 yearsago. with some modern statisticalcomputing languages, one can do interesting things within a system and nothave to go outside it.miniexpert functionsa final type of dataanalysis software that possesses statistical expertise is a miniexpert function. thesecould pervade a statistical package, so that for every statistical function in the language (provided it is acommandtype language), many associated miniexpert functions can be defined.the first example of a miniexpert function is interpret(funcall, options), which provides interpretation of thereturned output from the function call; i.e., it postprocesses the function call's output. for instance, the regressioncommand regress(x, y) might produce some regression data strategy. applying the interpret function to thatoutput, via the command ﬁinterpret(regress(x, y)),ﬂ might result in the response ﬁregression is linear but there isevidence of an outlier.ﬂ the idea here is that ﬁinterpretﬂ is merely a function in the language like any otherfunction, with suitable options.the next(funcall, options) miniexpert function has much the same syntax. when it is given a function calland options, it will provide a suggestion for the next function call. it could serve as a postprocessor to theinterpret function, and entering ﬁnext(regress(x, y))ﬂ might result in the response ﬁtry a smooth of y on x sincethere is curvature in the regression.ﬂ options might include ranking, or memory to store information as to thesource of the data that it was given.let us give an example of how the user interface would work with miniexpert functions. in languages wherethe value of the last expression is automatically stored, by successively entering the commands ﬁsmooth(x, y)ﬂ andﬁinterpret(tutorial = t),ﬂ the interpreter would interpret the value of that smooth by default. it would not have to beexplicitly given the function call name itself. as a second example, if the value of whatever had been done wereassigned to a variable, say ﬁz tree(x, y),ﬂ a plot of the variable could be done, e.g., ﬁtreeplot(z).ﬂ then thevariable could be given to the function next via ﬁnext(z)ﬂ in order to have that miniexpert supply a messagesuggesting what to do next.for those who really want a black box, the system can be run in blackbox mode. if one enters ﬁregress(x, y)ﬂfollowed by ﬁnext(do it = t), next(do it = t), –,ﬂ the system will always do whatever was suggested. in this way,the system can accommodate a range of users, from those who want to make choices at every step of theincorporating statistical expertise into data analysis software60the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.data analysis to those who want a complete blackbox expert system.one point to note: these miniexpert functions have not even been designed yet. this is more or less a planfor the future. as to ﬁfeatures,ﬂ these functions possess a short automateinteract cycle, the user remains incomplete control (but can relinquish it if desired), they are suitable for novices and experts alike, they exist withinthe statistical system, they are designed for others to imitate, and they force the developer to think hard about what afunction returns.concluding observationsas to the future of efforts to incorporate statistical expertise into software, progress is being made, but it hasbeen very indirect and quite modest. work in this area may subtly influence designers of statistical packages andlanguages to change. effort in this direction parallels the contributions of artificial intelligence research in theareas of interactive computing, windowing, integrated programming environments, new languages, new datastructures, and new architectures.there are some packages available commercially that attempt to incorporate statistical expertise, if one isinterested in trying such software. since the particular systems that were described in this presentation do not existnow, anyone with such an interest would have to look for what is available currently.finally, let me mention some interesting work that may stimulate readers who feel that incorporating softwareinto data analysis plans is important. john adams, a recent graduate from the university of minnesota, wrote athesis directed at trying to understand the effects of all possible combinations of the things that are done inregression for a huge designed experiment [adams, 1990]. the design not only involved crossing all of the variousfactors, but also involved different types of data configurations, number of covariances, correlation structure, andso on. the goal was to learn how all those multiple procedures fit together.prior to any incorporation of statistical expertise into data analysis software, that sort of study must first bedone. in a way, it amounts to defining a metaprocedure: statistical expertise will not be broadly incorporated intosoftware until that kind of understanding becomes a standard part of the literature.incorporating statistical expertise into data analysis software61the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.references.adams, j.l., 1990, evaluating regression strategies, ph.d. dissertation, department of statistics, university of minnesota, minneapolis.gale, w.a., 1986a, rex review, in artificial intelligence and statistics, w.a. gale, ed., addison wesley, menlo park, calif.gale, w.a., 1986b, studentphase 1, in artificial intelligence and statistics, w.a. gale, ed., addison wesley, menlo park, calif.polya, g., 1957, how to solve it, 2nd edition, princeton university press, princeton, n.j.incorporating statistical expertise into data analysis software62the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.afternoon discussionturkan gardenier (equal employment opportunity commission): i totally agree with andrew kirschin his opinions on the need for configurable output. in litigation cases, an offensedefense strategy is played inwhich one has to selectively provide information to the opposing party without giving too much information. if toomuch is provided, not only might they fail to comprehend it, butattorneys have told meit could also be usedagainst the person providing it.andrew kirsch: the same risks are faced by engineers who must present their data analyses to others.turkan gardenier: it has sometimes been necessary to cut and paste the standard statistical output,which is not kosher. if asked, ﬁdid you present all the information the computer gave?ﬂ do you answer yes or no?if you reply that the omitted information is not relevant to the issue, they say, ﬁpresent everything,ﬂ and then theyask you to explain everything in depositions. the deposition consumes three hours on some trivial case in thecomputer output that is totally irrelevant to the issue, and frequently confuses the opposing party rather thanshedding light on the issue. there needs to be a road map for selecting information to be displayed, depending onthe capability of the user to either understand it or use it.andrew kirsch: that is as much an issue in learning as it is in litigation. i tell my students that there areprofessionals whose careers are based on adding new kinds of tests and checks on various kinds of statisticalanalyses. in order to know everything about the output, individuals have to make a career of it themselves.paul velleman: there seems to be a mild contradiction in this. i love andrew's conclusion that ease ofuse is more important. somehow, software vendors never hear ﬁplease give us more ease of useﬂ from users. theyhear ﬁplease give us another feature.ﬂ andrew began with a list of capabilities. if a vendor's package does not haveone of those capabilities, and the vendor wants 3m to use that package, that missing capability must be added,rather than the program being made easier to use. if greater ease of use is of prime importance, and i believe it is,then that is what the package developers need to hear from the user community.andrew kirsch: i agree. that very comment, on being willing to set aside adding new capabilities forthe sake of ease of use, has been made by 3m to some people present today.by the way, 3m does not try to meet all four of those categories (acceptable cost;afternoon discussion63the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.core statistical capabilities; availability for a variety of hardware environments, with interfaces to commondatabase, spreadsheet, word processing programs and ready availability of support along with complete, readabledocumentation; and lastly, ease of use for infrequent users) with one software. 3m has separate software foracceptance sampling, design of experiments, and so on. when going through an evaluation process, it is really hardto say what are core capabilities.eric stellwagen (business forecast systems, inc.): as a developer of software, we never hear fromour users concerning the vast majority of products that go out the door. the reason is that we strive very hard tomake them easy to use.now, we have fallen into the trap of having different products to aim at different markets. our mostsophisticated product, statistically speaking, is the one on which we get the greatest number of comments, such as,ﬁit does not have this test,ﬂ or ﬁdo you not know about this other technique?ﬂ but those comments are coming notfrom infrequent users, but from people who are using the product every day in their work.the vast majority of our clients are the ones who once a month take a program off the shelf, do their salesforecast, and put it back on the shelf until next month. so the points expressed at this forum mirror exactly myexperience.paul tukey: i think we have to make a clear distinction between ease of use and oversimplification of thetechnique. i, too, believe in ease of use. for instance, one of the reasons i like s is that it has highlevel objectsthat are easy to use, because they are selfdescribing. this means i do not have to be constantly reminding thesystem of all the attributes.but there is the danger of mixing up ease of use with pretending that the world is simple when it is not. ifnaive people do naive analyses, e.g., fit a straight line when a straight line is not appropriate to fit, they may missthe things that are happening, because they are using lotus. frankly, lotus is not all that easy to use for statistics.try to program a regression in the lotus programming language; it is horrendous.this is not a plea for oversimplified statistics. some may worry that if software is made too easy to use, itsweeps the hard problems under the rug and everybody is left to believe that the world is always simple, when thatis not necessarily so.andrew kirsch: i tried to mention the idea of ease of flow through the program as well as that oflimiting the amount of output generated. but certainly there is a danger in limiting generated output if you reducethe amount of data generated to the point that you are vastly oversimplifying. so users must keep focused on thosethings that are most important, because this large laundry list of output, of potential things to consider and act on,does not provide much focus.forrest young: returning to the topic of guidance, one way of making systems easier to use is to notﬁcrippleﬂ them by taking options out or perhaps hiding options, but to make suggestions to the user as to whichoptions, what kinds of analyses, are the onesafternoon discussion64the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.that are appropriate at this point. the software should not give the appearance that there is only one thing the usercan do, namely the thing that is being suggested by the system. rather, the program should identify the thingssome expert thinks would be reasonable things to do next. that would certainly make systems easier to use forrelatively new users.turkan gardenier: recently i received a demonstration copy of a menudriven tutorial calledstatistical navigator that explains when to use statistical tests. it encompasses most of the modules but does notanalyze data. it provides menus, with questions and answers and with information about the data it is given, andgives advice about what test to use.daryl pregibon: there is a publication that has probably been out about 10 years from the universityof michigan, i believe from the school of social sciences. it is a small monograph or paperback that basicallydoes what that software implements. this navigator tutorial is probably one of the first to implement the ideas intosoftware.paul tukey: daryl, your enthusiasm is clear about solving the expert system problem in the near future.what are your thoughts on extracting some of the lessons from that experience and incorporating pieces into somemore classical kinds of statistical software packages, so that one has a better understanding of what are the gooddiagnostics and so on? such programs would not necessarily always say what should be done next, but could atleast perform some operations in the background.daryl pregibon: bill dumouchel mentioned something related to that, the concept of metadata. theseare qualitative features, such as the units of measurements and the range of the variable, that can and should beintegrated into the software. one can go to great lengths to provide a certain amount of guidance, but it alsorequires some discipline. if you have a procedure to do a boxcox analysis, you want it to involve more than justthe numerical values of x and y; you also want this metadata. many statisticians have probably written softwareprocedures for boxcox, but how many of you have written any that truly ask or require this metadata to bepresent? yet in boxcox's paper [box and cox, 1964], approximately the first five pages were on the qualitativeaspects of data transformations. there were various guidelines there, and after following them one can then dooptimization.anyone who has created software that implements only the procedure is at fault. statisticians have factoredout all that qualitative metadata from their routines. there is much room for improvement in applying the lessonsthat have already been learned, to simply bring that existing guidance into analyses. that would drasticallyimprove the level of guidance available in statistical software and prohibit indiscriminate transforming of variableswhen there is no rationale for doing so on physical or other grounds.keith muller: this goes back to the ultimate expert question where, whenafternoon discussion65the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.individuals come to a statistical consultant, often they are asking the wrong question of the data. daryl's idea ofmetamethods, in which a shell is built around the little pieces to try to build a bit bigger piece, is very attractive.but the insolubility of that problem of asking the wrong question may be the reason that many of us wereoriginally so cynical about the feasibility of expert systems.daryl pregibon: i emphatically agree. the most interesting and difficult parts of an analysis problemare recognizing at the outset what the problem really is, and understanding what to disregard in order to reduce theproblem to something for which one can draw up a sequence of steps.carl russell (u.s. army operational test and evaluation command): when you mentioned ministeps, i expected you to discuss the kind of implementation that is similar to your ﬁnextﬂ function that is in bothjump and data desk. for those packages, whenever the user has an individual window, he or she can go up andessentially get it next. those little steps are already implemented, at least in a couple of packages.daryl pregibon: everyone should explore this approach in more detail. those vendors probably sawthe light before i did on this, which is why they were successful where i was not.william dumouchel: every menu system helps to some extent in that manner, because there is aparticular structure in which you want to do one set of analyses, while only in certain contexts have you completedother analyses.forrest young: what is your opinion about the future of expert systems?william dumouchel: that is a rather general question. i agree with daryl about how hard the problemis in the absence of making use of context. the best thing a software company can do is to provide tools for usersto design their own onsite expert environments. if a general tool is offered that has menubuilding tools in it aswell as other kinds of extensibility options, then a group at 3m who know what kind of data they usually use canfinetune it and put in expertise.the future effort toward incorporating expertise will be to try to make everything more concrete. desktopmetaphors seem to work wonders in many aspects of how to use computers quickly and easily. if we can think ofother metaphors that help in the data analysis software line, then somehow we can make this whole process moretransparent. expertise can be more an invisible rather than an explicit thing.clifton bailey: with expert systems, one is often asking to deal with models that are inherently not inthe class of models that are normally considered in everyday statistical work. also, in putting expertise into aproduct, one needs to be aware that there are certain symmetries and structures that would never be followed in aparticularafternoon discussion66the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.context.andrew kirsch: to reinforce a wellmade point of daryl's, the necessary antecedent to statisticiansmaking progress in the world of expert systems is to rethink and specify what it is that statisticians do. it is notjust automating some system, but is more akin to asking in advance whether it is laid out in the best possible way.simply automating may be an inferior practice. the problem here is not inferior practice, but that the practice hasnot been adequately specified.al best (virginia commonwealth university): these ideas are all very exciting, but clearly a lot moreneeds to be known before much can be done that is concrete in promulgating guidelines. should there beguidelines on whether there needs to be a ﬁnextﬂ function, or guidelines on whether a user should be allowed toget only a system table without a graph? there are many questions here.referencebox, g.e.p., and d.r. cox, 1964, an analysis of transformations, j. roy. stat. soc. b, vol. 26, 211œ252 (with discussion).afternoon discussion67the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.afternoon discussion68the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.closing remarkswilliam eddycarnegie mellon universitythis is a good moment for me to make a long series of rambling remarks and comments that may or may notaddress that last question.first, i want to thank all of you for coming and providing a number of very insightful dimensions that thepanel had not considered in their deliberations before today. i want to thank all the speakers and my colleagues onthe panel. i also want to thank the national science foundation (nsf) for its financial support, and i would like tothank the staff at the national research council who organized this forum.as was mentioned this morning, the panel welcomes additional input from you, preferably in writing. tocomment on the presentations starting from this morning, paul velleman made a statement to the effect that goodstatistical software drives bad statistical software out of the marketplace. i happen to not believe that. in fact, forquite a long time i have been saying there is an explicit gresham's law for statistical software, namely, the baddrives out the good. a1 thaler of the nsf recently pointed out to me that my view on this is not completelycorrect, that rather there is a modified law: if the software is good enough, it does not have to get any better. thatis certainly the case. as a supporting example, i am currently teaching a course to undergraduates. the particularstatistical package being used is identical to the one i used in 1972. this package has not changed an iota in 19years. it was good enough, and so it did not need to change.underneath all of these discussions is software. software is one of the most unusual commodities thathumans have ever ever encountered. it has a very unique feature in that if the tiniest change is made in the input, inthe program, in the controls, or in anything, there can be huge changes in what results; software is in this waydiscontinuous. this fact drives a great deal of work in software engineering and reliability, but it should also driveour thinking about statistical software.in relation to this, bill dumouchel's mention of switching from means to medians intrigued me because iknow that there exist sets of numbers that are epsilonindifferent, but which would produce substantially differentanswers in his system. that switch is not a smooth transition, but is instead a switch that discontinuously kicks in.the fact that it is such a switch troubles me, because i like things to vary in a smooth way. if there is one singlegoal that i would set for software, it is that ultimately the response would be a smooth function of the input. i donot see any way to achieve this, and cannot offer a solution.another thing that troubles me is that there are software systems available in the marketplace that do notmeet what i would consider minimally acceptable quality standards on any kind of measure you select. thesesystems are not the 6 or 10 or 20closing remarks69the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.big ones that everyone knows. there are 300 of them available out there. a lot of bad software is being foisted onunsuspecting customers, and i am deeply concerned about that.to put it in concrete terms, what does it take for the developer of a statistical package to be able to claim thatthe package does linear leastsquares regression? what minimal functionality has to be delivered for that? thereexists software that does not meet any acceptability criteriano matter how little you ask.another matter troubling me, even more so now that i am a confirmed unix user, is the inconsistentinterfaces in our statistical software systems. vendors undoubtably view this as a feature, it being whatdistinguishes package a from package b. but as a user, i cannot think of anything i hate more than having to readsome detestable manual to find out what confounded things i have to communicate to this package in order to dowhat i want, since the package is not the one i usually use. this interface inconsistency is not only in the humanside of things, where it is readily obvious, but also on the computer side. there is now an activity within theamerican statistical association to develop a way to move data systematically from package to package.users of statistical software cannot force vendors to do anything, but gentle pleas can be made to make life alittle easier. that is what this activity is really about, simplifying my life as well as yours and those of many othersout there.there was discussion this morning on standardizing algorithms or test data sets as ways to confirm theveracity of programs. that is important, but what is even more important is the tremendous amount of wastedhuman energy when yet another program is written to compute xtx1xty. there must be 400 programs out on themarket to do that, and they were each written independently. that is simply stupid. although every vendor will sayits product has a new little twist that is better than the previous vendor's package, the fact is that vendors arewasting their resources doing the wrong things.we heard several speakers this afternoon discuss the importance of simplifying the output and the analysis. ifvendors would expend effort there instead of adding bells and whistles or in writing another wretched regressionprogram, everyone would be much better off. the future report of the panel on guidelines for statistical softwarewill include, i hope, some indications to software vendors as to where they should be focusing their efforts.i do not know the solution to all these problems. standards are an obvious partial solution. when a minimumstandard is set everybody agrees on what it is and, when all abide by the standard, life becomes simpler. however,life does not become optimal. if optimality is desired, then simplicity must usually be relinquished. nevertheless,standards are important, not so much in their being enforced, but in their providing targets or levels of expectationof what is to be met.i hope this panel's future report will influence statistical research in positive ways, in addition to havingpositive effects on software development, and thereby affect the software infrastructure that is more and moreenveloping our lives at every turn.the critical thing that is most needed in software is adaptability. i loved daryl pregibon's ﬁdo.it = trueﬂcommand. that is an optimal command. from now on, thatclosing remarks70the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.is all my computer is going to do, so that i do not have to deal with it anymore. an afterthought: an essentiallysimilar function is needed in regard to documentation, namely, ﬁgo away!ﬂ why is it that documentation hasnot evolved very far beyond the old program textbooks? there is much room for improvement here, and it issorely needed.one interesting thing expressed today was that maybe it is statistics, and not just the software, that needs tobe changed. as said repeatedly by several speakers, statisticians tend to focus on the procedure rather than thetask. that is an educational problem that we as academics need to address in the way statistics is taught, and sothis relates to methodological weaknesses in statistical training. the numbers that come out of a statistical packageare not the solution; numbers are the problem. there is far too much focus on having the software produce sevendigit numbers with decimal points in the right place. statistical software must be made more sophisticated, to thepoint that it ﬁtalksﬂ to the user and facilitates correct and appropriate analyses. a paraphrase of an old aphorismsays that you can lead a fool to data, but you can't force him to analyze it correctly. future statistical softwareshould also guide the user to correct analyses.closing remarks71the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.closing remarks72the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.appendixesappendixes73the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.appendixes74the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.appendix aspeakerskeith e. mullerkeith e. muller is an associate professor of biostatistics at the university of north carolina in chapel hill.he earned b.s. and m.a. degrees in psychology from bradley university. he subsequently earned a ph.d. inquantitative psychology and an m.s. in statistics at the university of north carolina at chapel hill. his enthusiasmfor and training in quantitative methods began during his undergraduate years in the late 1960s. having growntired of earning money doing complex anovas on calculators, he took his first computing course, fortranand beginning numerical analysis. this was followed by a twosemester graduate sequence in numerical methodsfor computers (focusing on linear and nonlinear systems). other courses followed. as with many middleagedstatisticians, his statistical computing experiences encompass a range of equipment, software, and applications. heis a member of the american psychological association, the american statistical association, the psi chipsychology honorary, and the psychometric society, and he is an associate review editor for the journal of theamerican statistical association.paul f. vellemanpaul velleman teaches statistics at cornell university, where he chairs the department of economic andsocial statistics. he is a fellow of the american statistical association and past chair of the association's sectionon statistical computing. professor velleman is the developer of data desk®, one of the major statistics andgraphics programs on the macintosh. professor velleman has published research in modern data analysismethods, graphical data analysis methods, and statistical computing. his book (coauthored with david hoaglin),abc's of eda, made computer implementations of exploratory data analysis methods widely available.appendix a75the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.andrew kirschandrew kirsch is a statistical specialist with 3m in st. paul, minnesota. he holds an m.s. in statistics fromthe university of wisconsin and an m.a. in applied mathematics from the university of massachusetts. he hasworked with 3m plants and laboratories for the past 9 years on a wide variety of products and processes, fromelectronics to contact lenses. he also has responsibilities in the areas of software, education, supervision, andmethods research.william dumouchelwilliam dumouchel has been chief statistical scientist at the software products division of bolt beraneknewman inc. since 1987. he earned a ph.d. in statistics from yale university in 1971. before coming to bbnsoftware products he held faculty appointments at the university of california, berkeley, university of michigan,massachusetts institute of technology, and harvard university. he has been elected a fellow of the americanstatistical association and of the institute of mathematical statistics, and he is a member of the internationalstatistical institute. his research interests include statistical computing, bayesian statistics and metaanalysis, andenvironmental and insurance risk assessment.daryl pregibondaryl pregibon is head of the statistics and data analysis research department at at&t bell laboratoriesin murray hill, new jersey. he received his ph.d. in statistics from the university of toronto in 1979. dr.pregibon spent one postdoctoral year at princeton university and another at the university of washington. he hasbeen at at&t bell laboratories for the past 9 years. dr. pregibon has been involved in the development ofknowledgebased software for statistical data analysis since his coming to bell laboratories. he is a codeveloper,with w.a. gale, of the first prototype expert system for regression analysis, rex. he has contributed substantiallyto the theory, application, and computational aspects of generalized linear models. his current interests concerninteractive statistical graphics and treebased modeling. dr. pregibon is a fellow of the american statisticalassociation and a member of the international statistical institute.appendix a76the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.appendix bposition statements: additional material submitted bysymposium participantsappendix b77the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.r. clifton baileyhealth standards and quality bureauhealth care financing administrationat the symposium, andrew kirsch noted that looking at the physical context of some statistical problemssometimes exposes the fact that certain additive functional forms can be contrary to known physical relationships.in his example, a product was instead the natural construct. wilson (1952) discusses some examples such assymmetry (§11.5, pp. 308œ312), limiting cases (p. 308), and dimensional analysis (§11.12, pp. 322œ328). i wouldnote that such considerations quickly lead one to consider functional relationships that are not linear in theparameters. the supporting statistical software has been of limited utility in dealing with these functionalrelationships. the picture becomes more complex when one introduces stochastic components to the models andcomplex sample designs for collecting data.nonlinear regression deals nicely with the problems when the stochastic element is limited to a measurementerror. however, many formulations do not readily permit the use of implicitly defined functions. however, forthose of us who must perform operational analyses without the luxury of developing special tools to deal withdiverse problems, adequate tools have not been easily accessible.expert systems are designed to prevent the incorrect application of a procedure. in addition to asking whetherwe have correctly used a procedure according to accepted standards, we must also ask whether the methods areuseful for our solving problems. one of the very strengths of statistical methods derived for designed experimentsis the very weakness of these methods. the strength is to have methods that work without regard to the underlyingfunctional relationships. we really can design methods that work in this way by controlling the design. thesemethods answer the preliminary questions such as whether there is a difference or not. any subjectmatterspecialist quickly wants to go beyond these questions to understand the structure of a problem. furthermore, thereare many important data sources other than designed studies or experiments. graphical techniques are so appealingpartly because our analytical tools to reveal the more complex structures are so limited and underdeveloped. i hopethat graphical techniques will drive analysts to consider more complex structural relationships in the analyticalcontext.as i said following paul velleman's talk, it would be nice if statisticians would think more globally. forexample, in using the results of a survival analysis package for a specific analysis, it would be useful to know howthe likelihood was formulated. better yet, some agreement on the formulation would permit comparison acrossvarious models and packages. for example, a loglikelihood for a weibel, constant hazard, or other model formcould be compared if there were some agreement in how these are formulated. some packages do not provide thiscomparability, even within a given procedure. part of the reason is that some drop constants that are not relevantfor theappendix b78the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.function to be optimized, the loglikelihood. others use the density function in the formulation, while still othersconsider events to occur in an interval (which must be stated) and formulate the probability of the event occurringin that interval. i prefer this latter approach because of its generality. events need to have the same interval andcensored events are part of the same formulation.some problems fit nicely on personal computers, while others have large volumes of data that are centrallymanaged. successful interaction with these data depends on the links between the data sources and the tools foranalysis. many specialized and intriguing enhancements appear on a variety of the changing platforms. i think adiversity of procedures and implementations is useful. one implementation may provide a capability not availablein another, and results can be compared from various sources. however, it is difficult to keep track of these and tomake them work on evolving platforms and in evolving environments. the speakers indicated nicely that too mucheffort is devoted to reproducing the similar algorithms in every setting and not enough to the enrichment of ourtools.let me point out some of the problems faced by statisticians in operating agencies. we are not in the positionof spending long periods on research issues. furthermore, we are faced with managers of computing centers whowant to support only standard software. this applies to mainframe and personal computer software. the danger, inthe spirit expressed by leo breiman, is that we find nails to apply our hammers to. we do this by reformulatingour problem to fit one for which we know the solution. the result is a correct solution to the wrong problembecause our tools are oriented to solving the textbook problems.we also encounter difficulties applying many routines to larger data sets50,000 to 10 million cases.furthermore, the diagnostics for dealing with large data sets need to be very different from those envisioned forsmaller data sets for which we can easily scan residuals, plots and other simple displays, or tables for all of ourdata. for example, techniques for focusing on a few outlier observations need to be generalized to focus on outliersets. i have found that having a convenient means for constraining or fixing parameters can be used in manyinteresting ways to solve complex problems such as those faced when trying to study the effect of many (severalthousand) indicator variables in analyzing large data sets.the needs are different for exploratory work where we are trying to understand relationships, and forproduction activities such as making 6000 tables or graphs for a publication.the extensive use of contractingout for solution of problems fragments the system and tends to work againsthaving the technical expertise and resources appropriately concentrated to recognize and develop appropriatesolutions. (see the various presentations by w. edwards deming on the system and profound knowledge, e.g., hisfebruary 16, 1991, talk at the annual meeting of the aaas in washington, d.c.)a few years ago i found the available options were inadequate to the task of probability analysis. can youimage a procedure that produces the same graph no matter what the problem? actually the graphs were correct butnot useful since the scale on theappendix b79the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.graph changed with the problem. this does not facilitate graphical comparisons. the problem at hand required anelaborate workaround with the available package (see bailey and eynon, 1988).such creative use of imperfect tools is always required. i remember learning that a major government agencycomputer center (epa) was going to abandon support for spss on the mainframe. this was a managementdecision without contact with the statistical community in the agency. part of the problem arose because userscould not readily obtain manuals. at another agency, sas is the principal mainframe package available. whilethis is a powerful package suitable for many important statistical activities, other packages provide strengths notfound in sas.referencesbailey, r. clifton, and barrett p. eynon, 1988, toxicity testing of drilling fluids: assessing laboratory performance and variability, chemicaland biological characterization of sludges, sediments, dredge spoils, and drilling muds, astm stp 976, j.j. lictenberg, j. a.winter, c. i. weber, and l. fradkin, eds., american society for testing and materials, philadelphia, pp. 334œ374.wilson, e. bright, jr., 1952, an introduction to scientific research, dover, new york.appendix b80the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.michael p. cohen.national center for education statisticsi have a few comments on suggested standards and guidelines. these comments reflect my opinion only. myexperience has been in sampling, design, and estimation for large, complex statistical surveys, including the u.s.consumer price index and the integrated postsecondary education data system.1. while the forum emphasized the proliferation of software, there remains a dearth of statisticalprocedures for analyses of data from complex surveys within generalpurpose packages. i am awareof special purpose packages such as sesudan, wesvar, and pccarp.2. many statistical packages still do not treat weighted data very well. even procedures that allowweights often treat them as a way of indicating multiple occurrences of the same observed value.appendix b81the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.james r. knaub, jr.energy information administration department of energyin a letter to the american statistician [knaub, 1987], i made some comments on the practical interpretationof hypothesis tests which are pertinent to statistical software. i feel that such software, by ignoring type ii erroranalyses for simple alternatives, has helped make hypothesis testing disreputable, when it should be one of ourviable tools. i hope that future generations of statistical software will take into account the need for type ii erroranalysis.referenceknaub, james r., jr., 1987, practical interpretation of hypothesis tests, the american statistician, vol. 41, no. 3, p. 246.appendix b82the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.jay magidsonstatistical innovations, inc.overall, i believe that the panel members and invited speakers at the symposium represent a much too narrowrange of interests and opinions to adequately represent the majority of users of statistical software. in general, ibelieve that the interests of business users, less sophisticated users, and less frequent users are underrepresented.among more sophisticated users, the emphasis on exploratory data analysis (eda) of quantitative data, andapplications from the health sciences and quality control were overrepresented. in particular, i believe that theinterests of survey researchers from the social sciences who analyze primarily categorical data are being largelyneglected by the panel.most of the presentations point out the importance of eda methods in appropriately dealing with violationsof the assumptions of traditional techniques for quantitative analysis. however, there is a major revolution takingplace in the analysis of categorical data that was totally neglected at the symposium. the revolution in categoricalanalysis from simple crosstabulation software to loglinear models, latent class analysis, association models, andother chisquaredbased techniques is even more remarkable than eda techniques since it is based on a unifiedtheory. there is no need to adjust for the unrealistic assumptions of normality and linearity, since suchassumptions are not made.as interest in categorical methods continues to grow, in a few years these methods may well represent themajority of statistical applications. the choice of regression and oneway anova as examples at the seminarillustrates the overemphasis on eda and quantitative analysis. regression and anova are the same type oftechnique. oneway anova was chosen simply to illustrate box plots. crosstabulation was totally ignored.while my own academic background is econometrics, and while i have taught statistics at tufts and at bostonuniversity, i am currently president and chief statistician of a consulting firm where businesses are my primaryclients. i am also the developer of a statistical package that is selling at the rate of more than 50 copies a monthand has over 500 usersprimarily business users. thus, i am very familiar with business use of statistics.i have also been the software editor of the journal of marketing research, and head of the softwarecommittee at abt associates prior to forming statistical innovations inc. in 1981. i have published on bothquantitative and categorical multivariate techniques. i have also organized and conducted statistical workshopswith professor leo a. goodman, and through these courses have trained several hundred practicing researchers.thus, i speak from my experience when i strongly recommend that these underrepresentations present on yourpanel be corrected.appendix b83the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.appendix b84the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.appendix c 85the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.appendix c 86the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.appendix c 87the future of statistical software: proceedings of a forumcopyright national academy of sciences. all rights reserved.appendix c 88
detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/11827frontiers of engineering: reports on leadingedgeengineering from the 2006 symposium202 pages | 6 x 9 | hardbackisbn 9780309382571 | doi 10.17226/11827national academy of engineeringfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.the national academies press ¥ 500 fifth street, n.w. ¥ washington, d.c. 20001notice: this publication has been reviewed according to procedures approved by anational academy of engineering report review process. publication of signed worksignifies that it is judged a competent and useful contribution worthy of public consideration, but it does not imply endorsement of conclusions or recommendations by the nae.the interpretations and conclusions in such publications are those of the authors and donot purport to represent the views of the council, officers, or staff of the national academy of engineering.funding for the activity that led to this publication was provided by the air force officeof scientific research, defense advanced research projects agency, department of defenseðddr&eresearch, national science foundation, ford motor company, microsoftcorporation, cummins, inc., and john a. armstrong.international standard book number 13:9780309103398international standard book number 10:0309103398additional copies of this report are available from the national academies press, 500fifth street, n.w., lockbox 285, washington, dc 20001; (800) 6246242 or (202) 3343313 (in the washington metropolitan area); internet, http://www.nap.edu.copyright 2007 by the national academy of sciences. all rights reserved.printed in the united states of americafrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.the national academy of sciences is a private, nonprofit, selfperpetuating society ofdistinguished scholars engaged in scientific and engineering research, dedicated to thefurtherance of science and technology and to their use for the general welfare. upon theauthority of the charter granted to it by the congress in 1863, the academy has a mandatethat requires it to advise the federal government on scientific and technical matters. dr.ralph j. cicerone is president of the national academy of sciences.the national academy of engineering was established in 1964, under the charter of thenational academy of sciences, as a parallel organization of outstanding engineers. it isautonomous in its administration and in the selection of its members, sharing with thenational academy of sciences the responsibility for advising the federal government.the national academy of engineering also sponsors engineering programs aimed atmeeting national needs, encourages education and research, and recognizes the superiorachievements of engineers. dr. wm. a. wulf is president of the national academy ofengineering.the institute of medicine was established in 1970 by the national academy of sciencesto secure the services of eminent members of appropriate professions in the examinationof policy matters pertaining to the health of the public. the institute acts under the responsibility given to the national academy of sciences by its congressional charter to bean adviser to the federal government and, upon its own initiative, to identify issues ofmedical care, research, and education. dr. harvey v. fineberg is president of the instituteof medicine.the national research council was organized by the national academy of sciences in1916 to associate the broad community of science and technology with the academyõspurposes of furthering knowledge and advising the federal government. functioning inaccordance with general policies determined by the academy, the council has becomethe principal operating agency of both the national academy of sciences and the national academy of engineering in providing services to the government, the public, and thescientific and engineering communities. the council is administered jointly by both academies and the institute of medicine. dr. ralph j. cicerone and dr. wm. a. wulf are chairand vice chair, respectively, of the national research council.www.nationalacademies.orgfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.ivorganizing committeejulia m. phillips (chair), director, physical, chemical, and nano sciencescenter, sandia national laboratoriesapoorv agarwal, eva engine pmt leader, scientific researchlaboratories, ford motor companym. brian blake, associate professor, department of computer science,georgetown universitytejal a. desai, professor of physiology/bioengineering, university ofcalifornia, san franciscodavid b. fogel, chief executive officer, natural selection, inc.hiroshi matsui, associate professor, department of chemistry, cunygraduate center and hunter collegejennifer k. ryan, senior lecturer, management department, college ofbusiness, university college dublinwilliam f. schneider, associate professor, department of chemical andbiomolecular engineering, concurrent in chemistry, university of notredamejulie l. swann, assistant professor, school of industrial and systemsengineering, georgia institute of technologystaffjanet r. hunziker, senior program officervirginia r. bacon, senior program assistantfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.vprefacein 1995, the national academy of engineering (nae) initiated the frontiersof engineering program, which brings together about 100 young engineeringleaders for annual symposia to learn about cuttingedge research and technicalwork in a variety of engineering fields. the twelfth u.s. frontiers of engineering symposium, at ford research and innovation center in dearborn, michigan,was held on september 21ð23, 2006. speakers were asked to prepare extendedsummaries of their presentations, which are reprinted in this volume. the intentof this volume, which also includes the text of the dinner speech, a list of contributors, a symposium agenda, and a list of participants, is to convey the excitement of this unique meeting and to highlight cuttingedge developments in engineering research.goals of the frontiers of engineering programthe practice of engineering is continually changing. engineers today mustbe able not only to thrive in an environment of rapid technological change andglobalization, but also to work on interdisciplinary teams. cuttingedge researchis being done at the intersections of engineering disciplines, and successful researchers and practitioners must be aware of the many developments and challenges in areas that may not be familiar to them.every year at the u.s. frontiers of engineering symposium, 100 of thiscountryõs best and brightest engineers, ages 30 to 45, have an opportunity tolearn from their peers about pioneering work being done in many areas of engineering. the symposium gives young engineers working in academia, industry,and government in many different engineering disciplines an opportunity to makefrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.viprefacecontacts with and learn from individuals they might not meet in the usual roundof professional meetings. this networking may lead to collaborative work andfacilitate the transfer of new techniques and approaches. it is hoped that theexchange of information on current developments will lead to insights that maybe applicable in specific disciplines.the number of participants at each meeting is limited to 100 to maximizeopportunities for interactions and exchanges among the participants, who arechosen through a competitive nomination and selection process. the topics andspeakers for each meeting are selected by an organizing committee of engineersin the same 30 to 45yearold cohort as the participants. different topics arecovered each year, and, with a few exceptions, different individuals participate.speakers describe the challenges they face and the excitement of their workto a technically sophisticated audience with backgrounds in many disciplines.each speaker provides a brief overview of his/her field of inquiry; defines thefrontiers of that field; describes experiments, prototypes, and design studies thathave been completed or are in progress, as well as new tools and methodologies,and limitations and controversies; and then summarizes the longterm significance of his/her work.the 2006 symposiumthe four general topics for the 2006 meeting were: the rise of intelligentsoftware systems and machines, the nano/bio interface, engineering personalmobility for the 21st century, and supply chain management applications witheconomic and public impact. the rise of intelligent software systems and machines is based on attempts to model the complexity and efficiency of the humanbrain, or the evolutionary process that created it, with the goal of creating intelligent systems, that is, systems that can adapt their behavior to meet the demandsof a variety of environments. speakers in this first session addressed the creation, use, and integration of intelligent systems in various aspects of everydaylife in a modern society and suggested future capabilities of machine intelligence. the four talks covered the commercialization of auditory neuroscience,or the development of a machine that can hear; the creation of intelligent agentsin games; the coevolution of the computer and social sciences; and computational cognitive models developed to improve humanrobot interactions.the evolution in engineering that resulted from the harnessing of biomolecular processes, such as selfassembly, catalytic activity, and molecular recognition, was the topic of the session on the bio/nano interface. two speakersdescribed their work on using biotechnology to solve nanotechnology problems. their presentations covered biological and biomimetic polypeptide materials and the application of biomimetics to devices. the third and fourth speakers took the opposite approach. they described solving biotechnology problemsusing nanotechnology. the topics were optical imaging for the in vivo assessfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.prefaceviiment of tissue pathology and the commercialization and future developments inbionanotechnology.the papers in the session on engineering personal mobility for the 21stcentury were based on the premise that providing people in the developing worldwith the same level of personal mobility that people in the developed worldenjoy is one of the great challenges for the 21st century. mobility on that scalemust be cost effective, efficient, and environmentally sustainable. presentationsaddressed the history and evolution of the availability and expectations of personal mobility, the energy and environmental challenges for current forms ofpersonal mobility, and prospective technologies that could transform personalmobility for this and future generations.the last session was on supply chain management (scm) applications witheconomic and public impact. although effective scm is now a significantsource of competitive advantage for private companies (e.g., dell computerand walmart), researchers and practitioners have also begun to focus on thepublic impact of scm, for example, the relationship between scm and healthcare, housing policy, the environment, and national security. the presentationsin this session were indicative of the widespread applicability of scm, such asmanufacturing processes, military procurement systems, and public housingpolicy. the last presentation focused on strategies for dealing with supply chaindisruptions.in addition to the plenary sessions, the participants had many opportunitiesto engage in informal interactions. for example, they attended a ògetacquaintedsession,ó during which individuals presented short descriptions of their work andanswered questions from their colleagues. the entire group was also taken on aninformative tour of the ford rouge plant.every year, a dinner speech is given by a distinguished engineer on the firstevening of the symposium. the speaker this year was w. dale compton, lillianm. gilbreth distinguished professor of industrial engineering, emeritus, purdueuniversity. his talk, entitled, òthe changing face of industrial research,ó included a description of the current situation in industrial research and a discussion of the technical and nontechnical problems facing our country, particularlythe need to encourage innovative approaches, which are critical to u.s. competitiveness and national prosperity. the text of dr. comptonõs remarks is includedin this volume.nae is deeply grateful to the following organizations for their support ofthe 2006 symposium: ford motor company, air force office of scientificresearch, defense advanced research projects agency, u.s. department ofdefenseðddr&e research, national science foundation, microsoft corporation, cummins, inc., and dr. john a. armstrong. nae would also like to thankthe members of the symposium organizing committee (p. iv), chaired by dr.julia m. phillips, for planning and organizing the event.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.ixcontentsthe rise of intelligent softwaresystems and machinesintroduction3m. brian blake and david b. fogelcommercializing auditory neuroscience5lloyd wattscreating intelligent agents in games15risto miikkulainencoevolution of social sciences and engineering systems29robert l. axtellusing computational cognitive models to improve humanrobotinteraction37alan c. schultzthe nano/bio interfaceintroduction45tejal desai and hiroshi matsuibiological and biomimetic polypeptide materials47timothy j. demingfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.xcontentsapplications of biomimetics59morley o. stoneoptical imaging for in vivo assessment of tissue pathology65rebekah a. drezek, naomi j. halas, and jennifer westcommercialization and future developments in bionanotechnology73marcel p. bruchezengineering personal mobility for the 21st centuryintroduction83apoorv agarwal and william f. schneiderlongterm trends in global passenger mobility85andreas sch−ferenergy and environmental impacts of personal mobility99matthew j. barthnew mobility: the next generation of sustainable urban transportation107susan zielinskisupply chain management and applications witheconomic and public impactintroduction119jennifer k. ryan and julie l. swannsupply chain applications of fast implosion121brenda l. dietrichfrom factory to foxhole: improving the armyõs supply chain131mark y.d. wangmanaging disruptions to supply chains139lawrence v. snyder and zuojun max shenengineering methods for planning affordable housing and sustainablecommunities149michael p. johnsonfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.contentsxidinner speechthe changing face of industrial research161w. dale comptonappendixescontributors171program179participants183frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.the rise of intelligent software systemsand machinesfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.3introductionm. brian blakegeorgetown universitywashington, d.c.david b. fogelnatural selection, inc.la jolla, californiathe human brain is the most powerful computer not developed by man. thecomplexity, performance, and power dissipation of the human brain are all unmatched in traditional computer and software systems. for decades, scientistshave attempted to model the complexity and efficiency of the human brain, orthe evolutionary process that created it, in other words, to create intelligent systems that can adapt their behavior to meet goals in a variety of environments. inthis session, we examine the creation, use, and integration of intelligent systemsin our lives and offer insights into the future capabilities of machine intelligence.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.5commercializing auditory neurosciencelloyd wattsaudience inc.mountain view, californiain a previous paper (watts, 2003), i argued that we now have sufficientknowledge of auditory brain function and sufficient computer power to beginbuilding a realistic, realtime model of the human auditory pathway, a machinethat can hear like a human being. based on extrapolations of computationalcapacity and advancements in neuroscience and psychoacoustics, a realisticmodel might be completed in the 2015ð2020 time frame. this ambitious endeavor will require a decade of work by a large team of specialists and a networkof highly skilled collaborators supported by substantial financial resources. todate, from 2002 to 2006, we have developed the core technology, determined aviable market direction, secured financing, assembled a team, and developed andexecuted a viable, sustainable business model that provides incentives (expectedreturn on investment) for all participants (investors, customers, and employees),in the short term and the long term. so far, progress has been made on all ofthese synergistic and interdependent fronts.scientific foundationthe scientific foundation for audience inc. is a detailed study of the mammalian auditory pathway (figure 1), completed with the assistance of eight ofthe worldõs leading auditory neuroscientists. our approach was to build working,highresolution, realtime models of various system components and validatethose models with the neuroscientists who had performed the primary research.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.6frontiers of engineeringthe basic modelbuilding began in 1998 and continued through 2002, just whenpersonal computers crossed the 1 ghz mark, which meant that, for the first timein history, it was possible to build working, realtime models of real brainsystem components, in software, on consumer computer platforms. early demonstrations in 2001 and 2002 included highresolution, realtime displays of thecochlea; binaural spatial representations, such as interaural time differences(itds) and interaural level differences (ilds); highresolution, eventbasedfigure 1a highly simplified diagram of the mammalian auditory pathway. adaptedfrom casseday et al., 2002; ledoux, 1997; oertel, 2002; rauschecker and tian, 2000;and young, 1998.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.commercializing auditory neuroscience7correlograms; and a demonstration of realtime, polyphonic pitch detection, allbased on wellestablished neuroscience and psychoacoustic findings.market focus and product directionin the early years of the company, we explored many avenues for commercialization. after a twoyear sojourn (from 2002 to 2004) into noiserobust speechrecognition, we reassessed the market and determined that the companyõs greatest commercial value was in the extraction and reconstruction of the humanvoice, a technology that could be used to improve the quality of telephone callsmade from noisy environments. this insight was driven by the enormous sales inthe cellphone market and the need for cellphone users to be heard clearly whenthey placed calls from noisy locations. at that point, work on speech recognitionwas deemphasized, and the company began to focus in earnest on commercializing a twomicrophone, nonstationary noise suppressor for the mobile telephonemarket.technologyfigure 2 is a block diagram of audienceõs cognitive audio system, which isdesigned to extract a single voice from a complex auditory scene. the majorelements in the system are: fast cochlea transformª (fct), a characterizationprocess, a grouping process, a selection process, and inverse fct.¥fct provides a highquality spectral representation of the sound mixture,with sufficient resolution and without introducing frame artifacts, to allow thecharacterization of components of multiple sound sources.¥the characterization process involves computing the attributes of soundcomponents used by human beings for grouping and stream separation. theseattributes include: pitches of constituent, nonstationary sounds; spatial locationcues (when multiple microphones are available), such as onset timing and othertransient characteristics; estimation and characterization of quasistationary background noise levels; and so on. these attributes are then associated with the rawfct data as acoustic tags in the subsequent grouping process.¥the grouping process is a clustering operation in lowdimensionalityspaces to ògroupó sound components with common or similar attributes into asingle auditory stream. sound components with sufficiently dissimilar attributesare associated with different auditory streams. ultimately, the streams are trackedthrough time and associated with persistent or recurring sound sources in theauditory environment. the output of the grouping process is the raw fct dataassociated with each stream and the corresponding acoustic tags.¥the selection process involves prioritizing and selecting separate auditory sound sources, as appropriate for a given application.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.8fastcochleatransformgroupinversefast cochlea transformcharacterizepitchspaceonsettimeselectoversampled spectrummultifeature characterizationaudio innonstationary noiserandom, fast movingsirens, horns, musicpa systems, trainsstationary noisepatternedfans, crowds, windvoice hearddistinct source separation, identification and selectionfigure 2architecture of the cognitive audio system. source: audience inc.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.commercializing auditory neuroscience9¥inverse fct telephony applications involve reconstructing and cleaningup the primary output of the system to produce a highquality voice. inversefct converts fct data back into digital audio for subsequent processing, including encoding for transmission across a cellphone channel.technical detailsfast cochlea transformªfct, the first stage of processing, must have adequate resolution to supporthighquality stream separation. figure 3 shows a comparison of the conventionalfast fourier transform (fft) and fct. in many applications, fft is updatedevery 10 ms, giving it coarse temporal resolution, as shown in the right half ofthe fft panel. fct is updated with every audio sample, which allows for resolution of glottal pulses, as necessary, to compute periodicity measures on a performant basis as a cue for grouping voice components.because of the way fft is often configured, it provides poor spectral resolution at low frequencies; very often, the following processor (such as a backfigure 3comparison of fast fourier transform and fast cochlea transformª. source:audience inc.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.10frontiers of engineeringend speech recognizer) is only interested in a smooth estimate of the spectralenvelope. fct, however, is designed to give highresolution information aboutindividual resolved harmonics so they can be tracked and used as grouping cuesin the lower formants.high resolution is even more important in a multisource environment (figure 4). in this example, speech is corrupted by a loud siren. the low spectrotemporal resolution of the framebased fft makes it difficult to resolve andtrack the siren, and, therefore, difficult to remove it from speech. the highspectrotemporal resolution of fct makes it much easier to resolve and track thesiren as distinct from the harmonics of the speech signal. the boundaries between the two signals are much better defined, which results in high performance in the subsequent grouping and separation steps.characterization processthe polyphonic pitch algorithm is capable of resolving the pitch of multiplespeakers simultaneously and detecting multiple musical instruments simultafigure 4multistream separation demonstration (speech + siren). note that the fastcochlea transform creates a redundant, oversampled representation of the timevaryingauditory spectrum. we have found this is necessary to meet the joint requirements ofperfect signal reconstruction with no aliasing artifacts, at low latency, with a high degreeof modifiability in both the spectral and temporal domains. source: audience inc.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.commercializing auditory neuroscience11neously. figure 5 shows how the simultaneous pitches of a male and femalespeaker are extracted. spatial localization is valuable for stream separation andlocating sound sources, when stereo microphones are available. figure 6 showsthe response of binaural representations to a sound source positioned to the rightof the stereo microphone pair.figure 7 shows an example of stream separation in a complex audio mixture(voice recorded on a street corner with nearby conversation, noise from a passing car, and ringing of a cell phone) in the cochlear representation. after soundseparation, only the voice is preserved.inverse fast cochlea transformafter sound separation in the cochlear (spectral) domain, the audio waveform can be reconstructed for transmission, playback, or storage, using the inverse fct. the inverse fct combines the spectral components of the fct backinto a timedomain waveform.figure 5polyphonic pitch for separating multiple simultaneous voices. source: audience inc.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.12frontiers of engineeringfigure 6response of the cochlear model and computations of itd and ild for spatiallocalization. source: audience inc.figure 7separation of a voice from a streetcorner mixture, using a handset with realtime, embedded software. top panel: mixture of voice with car noise, another voice, andcellphone ringtone. bottom panel: isolated voice. source: audience inc.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.commercializing auditory neuroscience13product directionso far, the companyõs product direction has remained true to the originalgoal of achieving commercial success by building machines that can hear likehuman beings. along the way, we have found points of divergence betweenwhat the brain does (e.g., computes with spikes, uses slow wetware, does notreconstruct audio) and what our system must do to be commercially viable (e.g.,compute with conventional digital representations, use fast silicon hardware,provide inverse spectral transformation). in general, however, insights from ourstudies of the neuroscience and psychoacoustics of hearing have led to insightsthat have translated into improved signalprocessing capacity and robustness.product implementationin the early days of the company, i assumed it would be necessary to builddedicated hardware (e.g., integrated circuits or silicon chips) to support the highcomputing load of brainlike algorithms. therefore, i advised investors that audience would be a fabless semiconductor company with a strong intellectualproperty position (my catchphrase was òthe nvidia of sound inputó). becausethe project was likely to take many years and implementation technology changesquickly, paul allen, microsoft cofounder and philanthropist, advised us in 1998to focus on the algorithms and remain flexible on implementation technology(personal communication, 1998). eight years later, in 2006, his counsel continues to serve the company well.as we enter the market with a specific product, we are finding acceptancefor both dedicated hardware solutions and embedded software solutions, for reasons that have less to do with computational demands than with the details ofintegrating our solution into the existing cellphone platform (e.g., the lack ofmixedsignal support for a second microphone). so, the company is a fablesssemiconductor company after all, but for very different reasons than i expectedwhen the company was founded in 2000.referencescasseday, j., t. fremouw, and e. covey. 2002. pp. 238ð318 in integrative functions in the mammalian auditory pathway, edited by d. oertel, r. fay, and a. popper. new york: springerverlag.ledoux, j.e. 1997. emotion circuits in the brain. annual review of neuroscience 23: 155ð184.oertel, d. 2002. pp. 1ð5 in integrative functions in the mammalian auditory pathway, edited by d.oertel, r. fay, and a. popper. new york: springerverlag.rauschecker, j., and b. tian. 2000. mechanisms and streams for processing of òwható and òwhereóin auditory cortex. proceedings of the national academy of sciences 97(22): 11800ð11806.watts, l. 2003. visualizing complexity in the brain. pp. 45ð56 in computational intelligence: theexperts speak, edited by d. fogel and c. robinson. new york: ieee press/john wiley &sons.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.14frontiers of engineeringyoung, e. 1998. pp. 121ð158 in the synaptic organization of the brain, 4th ed., edited by g.shepherd. new york: oxford university press.further readingbregman, a. 1990. auditory scene analysis. cambridge, mass.: mit press.shepherd, g. 1998. the synaptic organization of the brain, 4th ed. new york: oxford universitypress.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.15creating intelligent agents in gamesristo miikkulainenthe university of texas at austingames have long been a popular area for research in artiþcial intelligence(ai), and for good reason. because games are challenging yet easy to formalize,they can be used as platforms for the development of new ai methods and formeasuring how well they work. in addition, games can demonstrate that machines are capable of behavior generally thought to require intelligence withoutputting human lives or property at risk.most ai research so far has focused on games that can be described in acompact form using symbolic representations, such as board games and cardgames. the socalled good oldfashioned artiþcial intelligence (gofai;haugeland, 1985) techniques work well with symbolic games, and to a largeextent, gofai techniques were developed for them. gofai techniques have ledto remarkable successes, such as chinook, a checkers program that became theworld champion in 1994 (schaeffer, 1997), and deep blue, the chess programthat defeated the world champion in 1997 and drew significant attention to ai ingeneral (campbell et al., 2002).since the 1990s, the field of gaming has changed tremendously. inexpensiveyet powerful computer hardware has made it possible to simulate complex physical environments, resulting in tremendous growth in the video game industry.from modest sales in the 1960s (baer, 2005), sales of entertainment softwarereached $25.4 billion worldwide in 2004 (crandall and sidak, 2006). videogames are now a regular part of many peopleõs lives, and the market continues toexpand.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.16frontiers of engineeringcuriously, very little ai research has been involved in this expansion. manyvideo games do not use ai techniques, and those that do are usually based onrelatively standard, laborintensive scripting and authoring methods. in this andother respects, video games differ markedly from symbolic games. video gamesoften involve many agents embedded in a simulated physical environment wherethey interact through sensors and effectors that take on numerical rather thansymbolic values. to be effective, agents must integrate noisy input from manysensors, react quickly, and change their behavior during the game. the ai techniques developed for and with symbolic games are not well suited to videogames.in contrast, machinelearning techniques, such as neural networks, evolutionary computing, and reinforcement learning, are very well suited to videogames. machinelearning techniques excel in exactly the kinds of fast, noisy,numerical, statistical, and changing domains that todayõs video games provide.therefore, just as symbolic games provided an opportunity for the developmentand testing of gofai techniques in the 1980s and 1990s, video games providean opportunity for the development and testing of machinelearning techniquesand their transfer to industry.artificial intelligence in video gamesone of the main challenges for ai is creating intelligent agents that canbecome more proficient in their tasks over time and adapt to new situations asthey occur. these abilities are crucial for robots deployed in human environments, as well as for various software agents that live in the internet or serve ashuman assistants or collaborators.although current technology is still not sufficiently robust to deploy suchsystems in the real world, they are already feasible in video games. modernvideo games provide complex artificial environments that can be controlled andcarry less risk to human life than any realworld application (laird and van lent,2000). at the same time, video gaming is an important human activity thatoccupies millions of people for countless hours. machine learning can makevideo games more interesting and reduce their production costs (fogel et al.,2004) and, in the long run, might also make it possible to train humans realistically in simulated, adaptive environments. video gaming is, therefore, an important application of ai and an excellent platform for research in intelligent, adaptive agents.current video games include a variety of highrealism simulations of humanlevel control tasks, such as navigation, combat, and team and individualtactics and strategy. some of these simulations involve traditional ai techniques,such as scripts, rules, and planning (agre and chapman, 1987; maudlin et al.,1984), and a large part of ai development is devoted to pathfinding algorithms,such as a*search and simple behaviors built using finitestate machines. ai isfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.creating intelligent agents in games17used to control the behavior of the nonplayer characters (npcs, i.e., autonomouscomputercontrolled agents) in the game. the behaviors of npcs, although sometimes impressive, are often repetitive and inflexible. indeed, a large part of thegameplay in many games is figuring out what the ai is programmed to do andlearning to defeat it.machine learning in games began with samuelõs (1959) checkers program,which was based on a method similar to temporaldifference learning (sutton,1988). this was followed by various learning methods applied to tictactoe,recently, machinelearning techniques have begun to appear in video games aswell. for example, fogel et al. (2004) trained teams of tanks and robots to fighteach other using a competitive coevolution system, and spronck (2005) trainedagents in a computer roleplaying game using dynamic scripting. others havetrained agents to fight in first and thirdperson shooter games (cole et al., 2004;hong and cho, 2004). machinelearning techniques have also been applied toother video game genres, from pacman (lucas, 2005) to strategy games (bryantand miikkulainen, 2003; yannakakis et al., 2004).nevertheless, very little machine learning is used in current commercialvideo games. one reason may be that video games have been so successful that anew technology such as machine learning, which would fundamentally changethe gaming experience, may be perceived as a risky investment by the industry.in addition, commercial video games are significantly more challenging than thegames used in research so far. they not only have large state and action spaces,but they also require diverse behaviors, consistent individual behaviors, fastlearning, and memory of past situations (gomez et al., 2006; stanley et al.,2005).neuroevolutionthe rest of this article is focused on a particular machinelearning technique, neuroevolution, or the evolution of neural networks. this technique notonly promises to rise to the challenge of creating games that are educational, butalso promises to provide a platform for the safe, effective study of how intelligent agents adapt.evolutionary computation is a computational machinelearning techniquemodeled after natural evolution (figure 1a). a population of candidate solutionsare encoded as strings of numbers. each solution is evaluated in the task andassigned a fitness based on how well it performs. individuals with high fitnessare then reproduced (by crossing over their encodings) and mutated (by randomly changing components of their encodings with a low probability). theoffspring of the highfitness individuals replace the lowfitness individuals in thepopulation, and over time, solutions that can solve the task are discovered.in neuroevolution, evolutionary computation is used to evolve neural netfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.18frontiers of engineeringevolvedtopologyleft/rightforward/backfireenemyradarsontargetobjectrangefinersenemylofsensorsbiasfigure 1a. evolving neural networks. solutions (such as neural networks) are encodedas chromosomes, usually consisting of strings of real numbers, in a population. eachindividual is evaluated and assigned a fitness based on how well it performs a given task.individuals with high fitness reproduce; individuals with low fitness are thrown away.eventually, nearly all individuals can perform the task. b. each agent in neuroevolutionreceives sensor readings as input and generates actions as output. in the nero videogame, the network can see enemies, determine whether an enemy is currently in its line offire, detect objects and walls, and see the direction the enemy is firing. its outputs specifythe direction of movement and whether or not to fire. in this way, the agent is embeddedin its environment and must develop sophisticated behaviors to do well. for generalneuroevolution software and demos, see http://nn.cs.utexas.edu.abfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.creating intelligent agents in games19work weights and structures. neural networks perform statistical pattern transformation and generalization, and evolutionary adaptation allows for learningwithout explicit targets, even with little reinforcement. neuroevolution is particularly well suited to video games because (1) it works well in highdimensional spaces, (2) diverse populations can be maintained, (3) individual networksbehave consistently, (4) adaptation takes place in real time, and (5) memory canbe implemented through recurrency (gomez et al., 2006; stanley et al., 2005).several methods have been developed for evolving neural networks (yao,1999). one particularly appropriate for video games is called neuroevolution ofaugmenting topologies (neat; stanley and miikkulainen, 2002), which wasoriginally developed for learning behavioral strategies. the neural networks control agents that select actions in their output based on sensory inputs (figure 1b).neat is unique in that it begins evolution with a population of small, simplenetworks and complexifies those networks over generations, leading to increasingly sophisticated behaviors.neat is based on three key ideas. first, for neural network structures toincrease in complexity over generations, a method must be found for keepingtrack of which gene is which. otherwise, it will not be clear in later generationswhich individuals are compatible or how their genes should be combined toproduce offspring. neat solves this problem by assigning a unique historicalmarking to every new piece of network structure that appears through a structural mutation. the historical marking is a number assigned to each gene corresponding to its order of appearance over the course of evolution. the numbersare inherited unchanged during crossover, which allows neat to perform crossover without expensive topological analysis. thus, genomes of different organizations and sizes remain compatible throughout evolution.second, neat speciates the population, so that individuals compete primarily within their own niches instead of with the population at large. in this way,topological innovations are protected and have time to optimize their structures.neat uses the historical markings on genes to determine the species to whichdifferent individuals belong.third, unlike other systems that evolve network topologies and weights,neat begins with a uniform population of simple networks with no hiddennodes. new structure is introduced incrementally as structural mutations occur,and only those structures survive that are found to be useful through fitnessevaluations. this way, neat searches through a minimal number of weightdimensions and finds the appropriate complexity level for the problem. thisprocess of complexification has important implications for the search for solutions. although it may not be practical to find a solution in a highdimensionalspace by searching that space directly, it may be possible to find it by firstsearching in lower dimensional spaces and complexifying the best solutions intothe highdimensional space.as is usual in evolutionary algorithms, the entire population is replaced withfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.20frontiers of engineeringeach generation in neat. however, in a realtime game or simulation, thiswould seem incongruous because every agentõs behavior would change at thesame time. in addition, behaviors would remain static during the large gapsbetween generations. therefore, in order to apply neat to video games, a realtime version of it, called rtneat, was created.in rtneat, a single individual is replaced every few game ticks. one of thepoorest performing individuals is removed and replaced with a child of parentschosen from among the bestperforming individuals. this cycle of removal andreplacement happens continually throughout the game and is largely invisible tothe player. as a result, the algorithm can evolve increasingly complex neuralnetworks fast enough for a user to interact with evolution as it happens in realtime. this realtime learning makes it possible to build machinelearning games.machinelearning gamesthe most immediate opportunity for neuroevolution in video games is tobuild a òmod,ó a new feature or extension, to an existing game. for example, acharacter that is scripted in the original game can be turned into an adaptingagent that gradually learns and improves as the game goes on. or, an entirelynew dimension can be added to the game, such as an intelligent assistant or toolthat changes as the player progresses through the game. such mods can make thegame more interesting and fun to play. at the same time, they are easy and safeto implement from a business point of view because they do not change theoriginal structure of the game. from the research point of view, ideas aboutembedded agents, adaptation, and interaction can be tested with mods in a rich,realistic game environment.with neuroevolution, however, learning can be taken well beyond gamemods. entirely new game genres can be developed, such as machinelearninggames, in which the player explicitly trains game agents to perform varioustasks. the fun and challenge of machinelearning games is to figure out how totake agents through successive challenges so that in the end they perform well intheir chosen tasks. games such as tamagotchi òvirtual petó and black & whiteògod gameó suggest that interaction with artiþcial agents can make for viableand entertaining games. in nero, the third such game, the artificial agentsadapt their behavior through sophisticated machine learning.the nero gamethe main idea of nero is to put the player in the role of a trainer or drillinstructor who teaches a team of agents by designing a curriculum. the agentsare simulated robots that learn through rtneat, and the goal is to train them formilitary combat.the agents begin the game with no skills but with the ability to learn. tofrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.creating intelligent agents in games21prepare them for combat, the player must design a sequence of training exercisesand goals. ideally, the exercises will be increasingly difficult so that the teambegins by learning basic skills and then gradually builds on them (figure 2).when the player is satisfied that the team is well prepared, the team is deployedin a battle against another team trained by another player, allowing the players tosee if their training strategies pay off.the challenge is to anticipate the kinds of skills that might be necessary forbattle and build training exercises to hone those skills. a player sets up trainingexercises by placing objects on the field and specifying goals through severalsliders. the objects include static enemies, enemy turrets, rovers (i.e., turrets thatmove), flags, and walls. to the player, the sliders serve as an interface for describing ideal behavior. to rtneat, they represent coefficients for fitness components. for example, the sliders specify how much to reward or punish agentsfor approaching enemies, hitting targets, getting hit, following friends, dispersing, etc. each individual fitness component is normalized to a zscore (i.e., thenumber of standard deviations from the mean) so all components can be measured on the same scale. fitness is computed as the sum of all componentsmultiplied by their slider levels, which can be positive or negative. thus, theplayer has a natural interface for setting up a training exercise and specifyingdesired behavior.agents have several types of sensors (figure 1b). although nero programmers frequently experiment with new sensor configurations, the standardsensors include enemy radars, an òon targetó sensor, object range finders, andlineoffire sensors. to ensure consistent evaluations, agents all begin in a designated area of the field called the factory. each agent is allowed to spend a limitedamount of time on the field during which its fitness can be assessed. when timescenario 1: enemy turretscenario 2: 2 enemy turretsscenario 3: mobile turrets & wallsbattlefigure 2a sample training sequence in nero. the figure depicts a sequence of increasingly difficult training exercises in which agents attempt to attack turrets withoutgetting hit. in the first exercise, there is only a single turret; additional turrets are added bythe player as the team improves. eventually walls are added, and the turrets are givenwheels so they can move. finally, after the team has mastered the hardest exercises, it isdeployed in a battle against another team. for animations of various training and battlescenarios, see http://nerogame.org.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.22frontiers of engineeringon the field expires, the agent is transported back to the factory, where anotherevaluation begins.training begins by deploying 50 agents on the field. each agent is controlled by a neural network with random connection weights and no hiddennodes, which is the usual starting configuration for neat. as the neural networks are replaced in real time, behavior improves, and agents eventually learnto perform the task the player has set up. when the player decides performancehas reached a satisfactory level, he or she can save the team in a file. savedteams can be reloaded for further training in different scenarios, or they can beloaded into battle mode.in battle mode, the player discovers how well the training has worked. eachplayer assembles a battle team of 20 agents from as many different trained teamsas desired, possibly combining agents with different skills. the battle beginswith two teams arrayed on opposite sides of the field. when one player presses aògoó button, the neural networks take control of their agents and perform according to their training. unlike training, however, where being shot does not causedamage to an agentõs body, agents in battle are destroyed after being shot severaltimes (currently five). the battle ends when one team is completely eliminated.in some cases, the surviving agents may insist on avoiding each other, in whichcase the winner is the team with the most agents left standing.torque, a game engine licensed from garagegames (http://www.garagegames.com/), drives neroõs simulated physics and graphics. an important property of torque is that its physics is slightly nondeterministic so that the samegame is never played twice. in addition, torque makes it possible for the playerto take control of enemy robots using a joystick, an option that can be useful intraining.behavior can be evolved very quickly in nero, fast enough so that theplayer can be watching and interacting with the system in real time. the mostbasic battle tactic is to seek the enemy aggressively and fire at it. to train for thistactic, a single static enemy is placed on the training field, and agents are rewarded for approaching the enemy. this training requires that agents learn to runtoward a target, which is difficult because they start out in the factory facing inrandom directions. starting with random neural networks, it takes on average99.7 seconds for 90 percent of the agents on the field to learn to approach theenemy successfully (10 runs, sd = 44.5 s).note that nero differs from most applications of evolutionary algorithmsin that the quality of evolution is judged from the playerõs perspective based onthe performance of the entire population, instead of the performance of the population champion. however, even though the entire population must solve thetask, it does not converge to the same solution. in seek training, some agentsevolve a tendency to run slightly to the left of the target, while others run to theright. the population diverges because the 50 agents interact as they move simultaneously on the field at the same time. if all of the agents chose exactly thefrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.creating intelligent agents in games23same path, they would often crash into each other and slow each other down, soagents naturally take slightly different paths to the goal. in other words, nero isa massively parallel, coevolving ecology in which the entire population is evaluated together.agents can also be trained to avoid the enemy, leading to different battletactics. in fact, rtneat is flexible enough to devolve a population that hasconverged on seeking behavior into its complete opposite, a population thatexhibits avoidance behavior. for avoidance training, players control an enemyrobot with a joystick and run it toward the agents on the field. the agents learn toback away to avoid being penalized for being too near the enemy. interestingly,they prefer to run away from the enemy backward so they can still see and shootat the enemy (figure 3a). as an interesting combination of conflicting goals, afigure 3behaviors evolved in nero. a. this training screenshot shows several agentsrunning away backward while shooting at the enemy, which is being controlled from afirstperson perspective by a human trainer with a joystick. this scenario demonstrateshow evolution can discover novel and effective behaviors in response to challenges set upby the player. b. incremental training on increasingly complex wall configurations produced agents that could navigate this complex maze to find the enemy. remarkably, theyhad not seen this maze during training, suggesting that they had evolved general pathnavigation ability. the agents spawn from the left side of the maze and proceed to anenemy at the right. notice that some agents evolved to take the path through the top,while others evolved to take the bottom path, suggesting that protecting innovation inrtneat supports a range of diverse behaviors with different network topologies. animations of these and other behaviors can be seen at http://nerogame.org.bafrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.24frontiers of engineeringturret can be placed on the field and agents asked to approach it without gettinghit. as a result, they learn to avoid enemy fire, running to the side opposite thebullets and approaching the turret from behind. this tactic is also effective inbattle.other interesting behaviors have been evolved to test the limits of rtneat,rather than specifically preparing troops for battle. for example, agents weretrained to run around walls in order to approach the enemy. as performanceimproved, players incrementally added more walls until the agents could navigate an entire maze (figure 3b). this behavior was remarkable because it wassuccessful without any path planning.the agents developed the general strategy of following any wall that stoodbetween them and the enemy until they found an opening. interestingly, differentspecies evolved to take different paths through the maze, showing that topologyand function are correlated in rtneat and confirming the success of realtimespeciation. the evolved strategies were also general enough for agents to navigate significantly different mazes without further training. in another example,when agents that had been trained to approach a designated location (marked bya flag) through a hallway were attacked by an enemy controlled by the player,they learned, after two minutes, to take an alternative path through an adjacenthallway to avoid the enemyõs fire. such a response is a powerful demonstrationof realtime adaptation. the same kind of adaptation could be used in any interactive game to make it more realistic and interesting.teams that were trained differently were sometimes surprisingly evenlymatched. for example, a seeking team won six out of ten battles, only a slightadvantage, against an avoidant team that ran in a pack to a corner of the fieldnext to an enclosing wall. sometimes, if an avoidant team made it to the cornerand assembled fast enough, the seeking team ran into an ambush and was obliterated. however, slightly more often the seeking team got a few shots in beforethe avoidant team could gather in the corner. in that case, the seeking teamtrapped the avoidant team and had more surviving numbers. overall, neitherseeking nor avoiding provided a significant advantage.strategies can be refined further by observing behaviors during battle andsetting up training exercises to improve them. for example, a seeking team couldeventually be made more effective against an avoidant team when it was trainedwith a turret that had its back against the wall. the team learned to hover nearthe turret and fire when it turned away and to back off quickly when it turnedtoward them. in this way, rtneat can discover sophisticated tactics that dominate over simpler ones. the challenge for the player is to figure out how to set upthe training curriculum so sophisticated tactics will emerge.nero was created over a period of about two years by a team of more than30 student volunteers (gold, 2005). the game was first released in june 2005 athttp://nerogame.org and has since been downloaded more than 100,000 times.nero is under continuing development and is currently focused on providingfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.creating intelligent agents in games25more interactive play. in general, players agree that the game is engrossing andentertaining. battles are exciting, and players spend many hours perfecting behaviors and assembling teams with just the right combination of tactics. remarkably, players who have little technical background often develop accurate intuitions about the underlying mechanics of machine learning. this suggests thatnero and other machinelearning games are viable as a genre and may evenattract a future generation of researchers to machine learning.games like nero can be used as research platforms for implementing novelmachinelearning techniques. for example, one direction for research is to incorporate human knowledge, in terms of rules, into evolution. this knowledge couldthen be used to seed the population with desired initial behaviors or to give realtime advice to agents during evolution (cornelius et al., 2006; yong et al., 2006).another area for research is to learn behaviors that not only solve a given problem, but solve it in a way that makes sense to a human observer. although suchsolutions are difficult to describe formally, a human player may be able to demonstrate them by playing the game himself or herself. an evolutionary learningsystem can then use these examples to bias learning toward similar behaviors(bryant, 2006).conclusionneuroevolution is a promising new technology that is particularly well suitedto video game applications. although neuroevolution methods are still beingdeveloped, the technology can already be used to make current games morechallenging and interesting and to implement entirely new genres of games.such games, with adapting intelligent agents, are likely to be in high demand inthe future. neuroevolution may also make it possible to build effective traininggames, that is, games that adapt as the traineeõs performance improves.at the same time, video games provide interesting, concrete challenges formachine learning. for example, they can provide a platform for the systematicstudy of methods of control, coordination, decision making, and optimization,within uncertainty, material, and time constraints. these techniques should bewidely applicable in other fields, such as robotics, resource optimization, andintelligent assistants. just as traditional symbolic games catalyzed the development of gofai techniques, video gaming may catalyze research in machinelearning for decades to come.acknowledgmentsthis work was supported in part by the digital media collaboratory of theuniversity of texas at austin, texas higher education coordinating board,through grant arp0036584762001, and the national science foundationthrough grants eia0303609 and iis0083776.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.26frontiers of engineeringreferencesagre, p.e., and d. chapman. 1987. pengi: an implementation of a theory of activity. pp. 268ð272in proceedings of the 6th national conference on artiþcial intelligence. los altos, calif.:morgan kaufmann.baer, r.h. 2005. videogames: in the beginning. springþeld, n.j.: rolenta press.bryant, b.d. 2006. evolving visibly intelligent behavior for embedded game agents. ph.d. thesis,university of texas at austin. technical report ai06334.bryant, b.d., and r. miikkulainen. 2003. neuroevolution for adaptive teams. pp. 2194ð2201 inproceedings of the 2003 congress on evolutionary computation. piscataway, n.j.: ieee.campbell, m., a.j. hoane, jr., and f.h. hsu. 2002. deep blue. artiþcial intelligence 134: 57ð83.cole, n., s.j. louis, and c. miles. 2004. using a genetic algorithm to tune firstperson shooterbots. pp. 139ð145 in proceedings of the 2004 congress on evolutionary computation. piscataway, n.j.: ieee.cornelius, r., k.o. stanley, and r. miikkulainen. 2006. constructing adaptive ai using knowledgebased neuroevolution. pp. 693ð708 in ai game programming wisdom 3, edited by s.rabin. revere, mass.: charles river media.crandall, r.w., and j.g. sidak. 2006. video games: serious business for americaõs economy.entertainment software association report. available online at: http://www.theesa.com/files/videogamefinal.pdf.fogel, d.b., t.j. hays, and d.r. johnson. 2004. a platform for evolving characters in competitivegames. pp. 1420ð1426 in proceedings of the 2004 congress on evolutionary computation.piscataway, n.j.: ieee.gold, a. 2005. academic ai and video games: a case study of incorporating innovative academic research into a video game prototype. pp. 141ð148 in proceedings of the ieee 2005symposium on computational intelligence and games. piscataway, n.j.: ieee.gomez, f., j. schmidhuber, and r. miikkulainen. 2006. efficient nonlinear control through neuroevolution. pp. 654ð662 in proceedings of the european conference on machine learning.berlin: springerverlag.haugeland, j. 1985. artiþcial intelligence: the very idea. cambridge, mass.: mit press.hong, j.h., and s.b. cho. 2004. evolution of emergent behaviors for shooting game charactersin robocode. pp. 634ð638 in proceedings of the 2004 congress on evolutionary computation.piscataway, n.j.: ieee.laird, j.e., and m. van lent. 2000. humanlevel aiõs killer application: interactive computergames. pp. 1171ð1178 in proceedings of the 17th national conference on artiþcial intelligence. menlo park, calif.: aaai press.lucas, s.m. 2005. evolving a neural network location evaluator to play ms. pacman. pp. 203ð210 in proceedings of the ieee symposium on computational intelligence and games. piscataway, n.j.: ieee.maudlin, m.l., g. jacobson, a. appel, and l. hamey. 1984. rogomatic: a belligerent expertsystem. in proceedings of the 5th national conference of the canadian society for computational studies of intelligence. mississagua, ontario: canadian society for computational studies of intelligence.samuel, a.l. 1959. some studies in machine learning using the game of checkers. ibm journal 3:210ð229.schaeffer, j. 1997. one jump ahead. berlin: springerverlag.spronck, p. 2005. adaptive game ai. ph.d. thesis, maastricht university, the netherlands.stanley, k.o., and r. miikkulainen. 2002. evolving neural networks through augmenting topologies.evolutionary computation 10(2): 99ð127.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.creating intelligent agents in games27stanley, k.o., b.d. bryant, and r. miikkulainen. 2005. realtime neuroevolution in the nerovideo game. ieee transactions on evolutionary computation 9(6): 653ð668.sutton, r.s. 1988. learning to predict by the methods of temporal differences. machine learning 3:9ð44.yannakakis, g.n., j. levine, and j. hallam. 2004. an evolutionary approach for interactive computer games. pp. 986ð993 in proceedings of the 2004 congress on evolutionary computation.piscataway, n.j.: ieee.yao, x. 1999. evolving artiþcial neural networks. proceedings of the ieee 87(9): 1423ð1447.yong, c.h., k.o. stanley, r. miikkulainen, and i. karpov. 2006. incorporating advice into evolution of neural networks. in proceedings of the 2nd artiþcial intelligence and interactive digital entertainment conference. menlo park, calif.: aaai press.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.29coevolution of social sciences andengineering systemsrobert l. axtellgeorge mason universityfairfax, virginiasince the birth of engineering, social and behavioral scientists have playedan important role in bringing new technologies to market and designing userinterfaces. many of these technologies have also proven to be invaluable tosocial and behavioral scientists in their efforts to understand people. in otherwords, there is a kind of coevolution of engineering systems and social sciences.multiagent systems (mas), in which a population of quasiautonomoussoftware objects (agents) interact directly with one another and with their environment for purposes of simulation, control, and distributed computation, ispoised exactly at this interface. mas can be considered social systems, in whicheach member of a heterogeneous population pursues its own objectives, constrained by its interactions with others. indeed, ideas from the social sciences,including game theory (e.g., mechanism design), economics (e.g., auctiontheory), and sociology (e.g., social networks), are increasingly being incorporated into such software systems.at the same time, social scientists are increasingly using software agents tomodel social processes, where the dominant approach is to represent each personby a software agent. such models yield highfidelity depictions of the origin andoperation of social institutions (e.g., financial markets, organizational behavior,and the structure of social norms). they can also be used to understand thedifferential effects of alternative policies on such institutions.in short, social systems are systems with multiple agents, and mas are(increasingly) social systems. the coevolution of social and technological sysfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.30frontiers of engineeringtems means that advances in one field lead to progress in the other, nucleatingfurther improvements in the original field, and so on. this interface betweenthese broadly defined fields of knowledge opens up many opportunities for newresearch.social systems as multiagent systemsbuilding models in which purposive software objects represent individualpeople is a way around two classical problemsñaggregation and the necessity toassume equilibriumñwithin conventional mathematical modeling in the socialsciences. because social systems are typically composed of a large number ofheterogeneous individuals, mathematical models in the social sciences have beenone of two types: (1) aggregate models, in which the heterogeneity of the population is either assumed away (e.g., representative agent models) or averagedaway by looking only at mean behavior (e.g., systems dynamics models); and (2)models written at the level of individuals, in which òsolutionó of the modelsinvolves all agents engaging only in equilibrium behavior (e.g., nash equilibriain game theory, walrasarrowdebreu equilibria in economics) and all dynamicpaths by which such equilibria might be achieved are neglected. it is clear howthe agent approach fixes aggregate models by fully representing individuals. theagentbased approach also remedies the second problem by letting agents interact directly (in general these are outofequilibrium interactions); equilibrium isattained only if a path to it is realized from initial conditions.mas grew up in the mid1990s and combined with socalled artificial life(alife) models, giving rise to agentbased approaches in the social sciences. asthe capacity of computer hardware increased exponentially, more sophisticatedagent models could be built, using either more cognitively complex agents or alarger number of simple agents, or both. thus, large agent populations were soonrealized in practice leading naturally to the metaphor of an artificial society(builder and bankes, 1991).in modeling an artificial society, a population of objects is instantiated andpermitted to interact. typically, each object represents one individual and hasinternal data fields that store the specific characteristics of that individual. eachobject also has methods of modifying its internal data, describing interactions,and assessing its selfinterest (i.e., it can rank the value to itself of alternativeactions). this quality of selfinterestedness, or purposefulness, makes the objectsinto agents.conventional mathematical models in the social sciences rely heavily on asuite of heroic assumptions that are false empirically and, arguably, do moreharm than good as benchmarks. there are four main ways agentbased computing can be used to relax these assumptions. first, mainstream economics makesmuch of a òrepresentative agent,ó conceiving the entire economy as simply ascaledup version of a single decision maker. this specification is easy to relaxcomputationally. second, economics models normally consider only rationalfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.coevolution of social sciences and engineering systems31agent behavior, whereby optimal behavior can be deduced by all agents for alltime. not surprisingly, in a mas of any complexity, such deductions are computationally intractable and cannot be implemented in practice. thus, modelsoften resort to bounded rationality. third, modeling conventions have oftendictated that agents not interact directly with other individuals but interact either indirectly through aggregate variables or perhaps through some idealizedinteraction topology (e.g., random graph, lattice). in agentbased computing,however, any topology, including empirically significant networks, can be easily implemented to mediate agent interactions. finally, equilibrium is the focalpoint for all solution concepts in the social sciences. whether equilibriumobtains or not in an agentbased system, the dynamics matter and are fullymodeled.all of the social sciencesñanthropology (axtell et al., 2002; diamond,2002, 2005; kohler and gumerman, 2000); geography (gimblett, 2002); socialpsychology (kennedy et al., 2001; latane et al., 1994; nowak et al., 2000);sociology (gilbert and doran, 1994; gilbert and conte, 1995; flache and macy,2002; macy and willer, 2002); political science (axelrod, 1984; kollman et al.,1992; cederman, 1997; lustick et al., 2004); economics (arifovic and eaton,1995; arifovic, 1996; kollman et al., 1997; tesfatsion, 1997; kirman andvriend, 2000; luna and stefansson, 2000; allen and carroll, 2001; arifovic,2001; bullard and duffy, 2001; luna and perrone, 2001; tesfatsion, 2002, 2003;arifovic and masson, 2004; axtell and epstein, 1999; axtell et al., 2001; young,1998); finance (palmer et al., 1994; arthur et al., 1997; lux, 1998; lebaron etal., 1999; lux and marchesi, 1999; lebaron, 2000, 2001a, 2001b, 2002, 2006);organizational science (carley and prietula, 1994; prietula et al., 1998); business(bonabeau and meyer, 2001; bonabeau, 2002); many areas of public policy(axtell and epstein, 1999; moss et al., 2001; saundersnewton, 2002; bourges,2002; janssen, 2002; rauch, 2002); transportation science and policy (nagel andrasmussen, 1994; nagel and paczuski, 1995; nagel et al., 1998; gladwin et al.,2003); public health/epidemiology (wayner, 1996); demography (kohler, 2001);and the military (ilachinski, 2004)ñhave more or less active research programsusing agent computing. although the nature of these applications is idiosyncraticwithin particular fields, they are unified methodologically in the search for agentspecifications that yield empirically observed (or at least empirically plausible)social behavior.multiagent systems as social systemsnot only has agent computing changed the practice of the social sciences,but the social sciences have altered the face of mas. certain social sciencemethods have been adopted by computer and information scientists not only atthe research frontier, but also in commercial systems. in the same way that socialscientists have reworked the mas paradigm for their own ends, developers haveadapted extant social science methods to specific problems in their domains.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.32frontiers of engineeringthe role of agents within computer and information science has been primarily to enhance the function of distributed, decentralized systems. for example,in designing a new computer network, each individual node in the network mightbe given the ability to manage its own resource allocation, based on informationabout the overall load on the network. this might be done cooperatively orcompetitively (huberman, 1987; miller and drexler, 1988). similarly, a welldesigned network should function properly regardless of the topology of how themachines are hooked together. thus, ideas from graph theory and social networktheoryñeach node can be thought of as socially interactiveñhave been relevantand put to good use.basic research on agent systems has been amplified and extended beyondthe academic community by the exigencies of ecommerce. the prospect ofautomated bargaining, contracting, and exchange among software agents hasdriven investigators to explore the implications of selfinterested agents actingautonomously in computer networks and information technology servers.because decentralization is an important idea within mas, ideas frommicroeconomics and economic general equilibrium theory that focus on decentralization were incorporated into mas under the rubric òmarketoriented programmingó (wellman, 1996). mechanism design is an approach to the synthesisof economic environments in which the desired performance of a mechanism isspecified, and one then figures out what incentives to give the agents in a waythat the equilibria (e.g., nash equilibrium) that are individually rational and incentive compatible achieve the objective. this formalism was developed largelyin the 1980s and is today viewed by some as a viable way to design mas (kfirdahav et al., 2000).in distributed control, market metaphors have been replaced with actualmarket models (clearwater, 1996). temperature control of a building is an example application (huberman and clearwater, 1995) of a mas that makes explicit use of concepts from economic general equilibrium (e.g., mascollel et al.,1995).in automated negotiation (e.g., rosenchein and zlotkin, 1994), mas researchers have made extensive use of game theory. in automated contracting, thecontract net protocol (weiss, 1999) was an early example of a highlevel protocol for efficient cooperation through task sharing in networks of problem solvers. since then, much more work of this type has been done. more recent workhas taken an explicitly social stance, looking for an emergent social order, forexample, through the evolution of social orders and customs, institutions, andlaws.agentbased technology and coevolutioni am aware that portraying agentbased computing as a bridge between engineering and the social sciences may be risky. by touting the apparent effectivefrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.coevolution of social sciences and engineering systems33ness of a new methodology, there is always a risk that òhypeó will overshadowsubstance and raise unrealistic expectations.the alternative approach is to paint an evolutionary picture, in which todayõsnew methodologies are seen as logical extensions of adequate but dated conventional methodologies. thus, progress appears to be natural, with no abruptchanges. this view can be òsoldó more easily to existing research communitiesand is easier to insinuate into conventional discourse.evolution or revolution? continuous change or abrupt change? smooth transition or phase transition? one is tempted to invoke kuhn (1962) at this point,but it may be enough to point out that the technical skills required for those whoare fomenting change are quite different from those of many current facultymembers and those who teach current graduate students. only a very small subset of social science researchers knows enough about computer science to perform agentbased modeling in their areas of expertise. this is also the majorbarrier to the systematic adoption of these new techniquesñand proof that agentbased modeling constitutes a discontinuous advance.assuming that mooreõs law will continue to hold true for the next generation (20 to 30 years), the capabilities of agent computing will double every 18 to24 months, increasing by an order of magnitude each decade. from the socialscience perspective, this technological revolution will permit the construction ofincreasingly large models involving greater numbers of progressively more complex agents. when one contemplates the possible desktop hardware of 2020, onecan imagine hundreds of gigabytes of ultrafast ram, fantastic clock and busspeeds, and enormous hard disks. the continuing computer revolution will fundamentally alter the kinds of social science models that can be built. it will alsoalter the practice of social sciences, as equations give way fully to agents, empirically tested cognitive models arise, and decision models grounded in neuroscience emerge.it is anyoneõs guess where coevolution will lead. coevolutionary systemshave the capacity to fundamentally alter one another and their environments innovel, creative ways. thus, speculations for the medium term and long run maylook and sound a lot like science fiction.referencesallen, t.m., and c.d. carroll. 2001. individual learning about consumption. macroeconomic dynamics 5(2): 255ð271.arifovic, j. 1996. the behavior of exchange rates in the genetic algorithm and experimental economies. journal of political economy 104(3): 510ð541.arifovic, j. 2001. evolutionary dynamics of currency substitution. journal of economic dynamicsand control 25: 395ð417.arifovic, j., and c. eaton. 1995. coordination via genetic learning. computational economics 8:181ð203.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.34frontiers of engineeringarifovic, j., and p. masson. 2004. heterogeneity and evolution of expectations in a model of currency crisis. nonlinear dynamics, psychology, and life sciences 8(2): 231ð258.arthur, w.b., j.h. holland, b. lebaron, r. palmer, and p. tayler. 1997. asset pricing underendogenous expectations in an artificial stock market. pp. 15ð44 in the economy as anevolving complex system ii, edited by w.b. arthur, s.n. durlauf, and d.a. lane. reading,mass.: addisonwesley.axelrod, r. 1984. the evolution of cooperation. new york: basic books.axtell, r.l., and j.m. epstein. 1999. coordination in transient social networks: an agentbasedcomputational model of the timing of retirement. pp. 161ð183 in behavioral dimensions ofretirement economics, edited by h.j. aaron. washington, d.c.: the brookings institutionpress.axtell, r.l., j.m. epstein, j.s. dean, g.j. gumerman, a.c. swedlund, j. harburger, s. chakravarty,r. hammond, j. parker, and m.t. parker. 2002. population growth and collapse in a multiagentmodel of the kayenta anasazi in long house valley. proceedings of the national academy ofsciences of the u.s.a. 99(supp 3): 7275ð7279.axtell, r.l., j.m. epstein, and h.p. young. 2001. the emergence of classes in a multiagentbargaining model. pp. 191ð211 in social dynamics, edited by s.n. durlauf and h.p. young.cambridge, mass./washington, d.c.: mit press/brookings institution press.bonabeau, e. 2002. agentbased modeling: methods and techniques for simulating human systems.proceedings of the national academy of sciences of the u.s.a. 99(supp 3): 7280ð7287.bonabeau, e., and c. meyer. 2001. swarm intelligence: a whole new way to think about business.harvard business review 79: 106ð114.bourges, c. 2002. computerbased artificial societies may create real policy. washington times,may 12, 2002.builder, c., and s. bankes. 1991. artificial societies: a concept for basic research on the societalimpacts of information technology. santa monica, calif.: rand corporation.bullard, j., and j. duffy. 2001. learning and excess volatility. macroeconomic dynamics 5(2): 272ð302.carley, k.m., and m.j. prietula. 1994. computational organization theory. hillsdale, n.j.: lawrenceerlbaum associates.cederman, l.e. 1997. emergent actors and world politics: how states and nations develop anddissolve. princeton, n.j.: princeton university press.clearwater, s.h. 1996. marketbased control. hackensack, n.j.: world scientific.diamond, j.m. 2002. life with the artificial anasazi. nature 419: 567ð569.diamond, j.m. 2005. collapse: how societies choose to fail or survive. new york: allen lane.flache, a., and m.w. macy. 2002. stochastic collusion and the power law of learning. journal ofconflict resolution 46: 629ð653.gilbert, n., and r. conte, eds. 1995. artificial societies: the computer simulation of social life.london: ucl press.gilbert, n., and j. doran, eds. 1994. simulating societies: the computer simulation of socialphenomena. london: ucl press.gimblett, h.r., ed. 2002. integrating geographic information systems and agentbased modelingtechniques for simulating social and ecological processes. santa fe institute studies in thesciences of complexity. new york: oxford university press.gladwin, t., c.p. simon, and j. sullivan. 2003. workshop on mobility in a sustainable world: acomplex systems approach, ann arbor, michigan, june 20ð22, 2003.huberman, b.a., ed. 1987. the ecology of computation. new york: northholland.huberman, b.a., and s.h. clearwater. 1995. a multiagent system for controlling building environments. first international conference on multiagent systems, san francisco, calif. cambridge, mass.: aaai press/mit press.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.coevolution of social sciences and engineering systems35ilachinski, a. 2004. artificial war: multiagentbased simulation of combat. singapore: world scientific publishing.janssen, m.a., ed. 2002. complexity and ecosystem management: the theory and practice ofmultiagent systems. northampton, mass.: edward elgar publishing inc.kennedy, j., r.c. eberhart, and y. shi. 2001. swarm intelligence. san francisco, calif.: morgankaufmann.kfirdahav, n.e., d. monderer, and m. tennenholtz. 2000. mechanism design for resource bounded agents. pp. 309ð315 in proceedings of the fourth international conference on multiagentsystems. boston, mass.: ieee computer society.kirman, a.p., and n.j. vriend. 2000. learning to be loyal: a study of the marseille fish market.pp. 33ð56 in interaction and market structure: essays on heterogeneity in economics, editedby d. delli gatti and a.p. kirman. berlin, germany: springerverlag.kohler, h.p. 2001. fertility and social interactions. new york: oxford university press.kohler, t.a., and g.j. gumerman, eds. 2000. dynamics in human and primate societies: agentbased modeling of social and spatial processes. santa fe institute studies in the sciences ofcomplexity. new york: oxford university press.kollman, k., j.h. miller, and s.e. page. 1992. adaptive parties in spatial elections. american political science review 86: 929ð937.kollman, k., j.h. miller, and s.e. page. 1997. political institutions and sorting in a tiebout model.american economic review 87(5): 977ð992.kuhn, t.s. 1962. the structure of scientific revolutions. chicago, ill.: university of chicago press.latane, b., a. nowak, and j.h. liu. 1994. measuring emergent social phenomena: dynamicism,polarization and clustering as order parameters of social systems. behavioral science 39: 1ð24.lebaron, b. 2000. agentbased computational fiannce: suggested readings and early research. journal of economic dynamics and control 24: 324ð338.lebaron, b. 2001a. a builderõs guide to agentbased financial martkets. quantitative finance 1:254ð261.lebaron, b. 2001b. evolution and time horizons in an agentbased stock market. macroeconomicsdynamics 5: 225ð254.lebaron, b. 2002. shortmemory traders and their impact on group learning in financial markets.proceedings of the national academy of sciences of the u.s.a. 99(supp 3): 7201ð7206.lebaron, b. 2006. agentbased financial markets: matching stylized facts with style. pp. 221ð238in post walrasian macroeconomics: beyond the dynamic stochastic general equilibrium model, edited by d.c. colander. new york: cambridge university press.lebaron, b., w.b. arthur, and r. palmer. 1999. time series properties of an artificial stock market.journal of economics dynamics and control 23: 1487ð1516.luna, f., and a. perrone, eds. 2001. agentbased methods in economics and finance: simulationsin swarm. boston, mass.: kluwer academic publishers.luna, f., and b. stefansson, eds. 2000. economic simulations in swarm: agentbased modelingand object oriented programming. advances in computational economics, volume 14. boston, mass.: kluwer academic publishers.lustick, i.s., d. miodownick, and r.j. eidelson. 2004. secessionism in multicultural states: doessharing power prevent or encourage it? american political science review 98(2): 209ð229.lux, t. 1998. the socioeconomic dynamics of speculative markets: interacting agents, chaos and thefat tails of return distributions. journal of economic behavior and organization 33: 143ð165.lux, t., and m. marchesi. 1999. scaling and criticality in a stochastic multiagent model of afinancial market. nature 397: 498ð500.macy, m.w., and r. willer. 2002. from factors to actors: computational sociology and agentbasedmodeling. annual review of sociology 28: 143ð166.mascollel, a., m.d. whinston, and j.r. green. 1995. microeconomic theory. new york: oxforduniversity press.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.36frontiers of engineeringmiller, m.s., and k.e. drexler. 1988. markets and computation: open agoric systems. pp. 231ð266in the ecology of computation, edited by b.a. huberman. new york: northholland.moss, s., c. pahlwostl, and t. downing. 2001. agentbased integrated assessment modelling: theexample of climate change. integrated assessment 2(1): 17ð30.nagel, k., and m. paczuski. 1995. emergent traffic jams. physical review e 51: 2909ð2918.nagel, k., and s. rasmussen. 1994. traffic at the edge of chaos. pp. 224ð235 in artificial life iv,edited by r.a. brooks and p. maes. cambridge, mass.: mit press.nagel, k., r. beckman, and c.l. barrett. 1998. transims for transportation planning. technicalreport. los alamos, n.m.: los alamos national laboratory.nowak, a., r.r. vallacher, a. tesser, and w. borkowski. 2000. society of self: the emergence ofcollective properties in selfstructure. psychological review 107: 39ð61.palmer, r.g., w.b. arthur, j.h. holland, b. lebaron, and p. tayler. 1994. artificial economic life:a simple model of a stock market. physica d 75: 264ð274.prietula, m.j., k.m. carley, and l. gasser, eds. 1998. simulating organizations: computationalmodels of institutions and groups. cambridge, mass.: mit press.rauch, j. 2002. seeing around corners. atlantic monthly 289: 35ð48.rosenschein, j.s., and g. zlotkin. 1994. rules of encounter: designing conventions for automatednegotiation among computers. cambridge, mass.: mit press.saundersnewton, d. 2002. introduction: computerbased methods: state of the art. social sciencecomputer review 20(4): 373ð376.tesfatsion, l. 1997. how economists can get a life. pp. 533ð564 in the economy as an evolvingcomplex system, volume ii, edited by w.b. arthur, s. durlauf, and d.a. lane. menlo park,calif.: addisonwesley.tesfatsion, l. 2002. agentbased eomputational economics: growing economies from the bottom up.artificial life 8(1): 55ð82.tesfatsion, l. 2003. agentbased computational economics: modeling economies as complex adaptive systems. information sciences 149(4): 262ð268.wayner, p. 1996. computer simulations: newmedia tools for online journalism. new york times,october 9, 1996.weiss, g., ed. 1999. multiagent systems: a modern approach to distributed artificial intelligence.cambridge, mass.: mit press.wellman, m. 1996. marketoriented programming: some early lessons. pp. 74ð95 in market basedcontrol: a paradigm for distributed resource allocation, edited by s.h. clearwater. hackensack, n.j.: world scientific press.young, h.p. 1998. individual strategy and social structure. princeton, n.j.: princeton universitypress.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.37using computational cognitive models toimprove humanrobot interactionalan c. schultznaval research laboratorywashington, d.c.we propose an approach for creating more cognitively capable robots thatcan interact more naturally with humans. through analysis of human team behavior, we build computational cognitive models of particular highlevel humanskills that we have determined to be critical for good peertopeer collaborationand interaction. we then use these cognitive models as reasoning mechanismson the robot, enabling the robot to make decisions that are conducive to goodinteraction with humans.cognitively enhanced intelligent systemswe hypothesize that adding computational cognitive reasoning componentsto intelligent systems such as robots will result in three benefits:1.most, if not all, intelligent systems must interact with humans, who arethe ultimate users of these systems. giving the system cognitive models canimprove the humansystem interface by creating more common ground in theform of cognitively plausible representations and qualitative reasoning. for example, mobile robots generally use representations, such as rotational and translational matrixes, to represent motion and spatial references. however, becausethis is not a natural mechanism for humans, additional computations must bemade to translate between these matrixes and the qualitative spatial reasoningfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.38frontiers of engineeringused by humans. by using cognitive models, reasoning mechanisms, and representations, we believe we can achieve a more effective and efficient interface.2.because the resulting system interacts with humans, giving the systembehaviors that are more natural and compatible with human behaviors can alsoresult in more natural interactions between human and intelligent systems. forexample, mobile robots that must work collaboratively with humans can haveless than effective interactions with them if their behaviors are alien or nonintuitive to humans. by incorporating cognitive models, we can develop systemswhose behavior can be more easily anticipated by humans and is more natural.therefore, these systems are more compatible with human team members.3.one key area of interest is measuring the performance of intelligent systems. we propose that the performance of a cognitively enhanced intelligentsystem can be compared directly to humanlevel performance. in addition, ifcognitive models of human performance have been developed in creating anintelligent system, we can directly compare the behavior and performance of thetask by the intelligent system to the human subjectõs behavior and performance.hide and seekour foray into this area began when we were developing computationalcognitive models of how young children learn the game of hide and seek (traftonet al., 2005b; trafton and schultz, 2006). the purpose was to create robots thatcould use humanlevel cognitive skills to make decisions about where to look forpeople or things hidden by people. the research resulted in a hybrid architecturewith a reactive/probabilistic system for robot mobility (schultz et al., 1999) anda highlevel cognitive system based on actr (anderson and lebiere, 1998)that made the highlevel decisions for where to hide or seek (depending onwhich role the robot was playing). not only was this work interesting in its ownright, but it also led us to the realization that òperspective takingó is a criticalcognitive ability for humans, particularly when they want to collaborate.spatial perspective takingto determine how important perspective and frames of reference are incollaborative tasks in shared space (and also because we were working on adarpafunded project to move these capabilities to the nasa robonaut), weanalyzed a series of tapes of a ground controller and two astronauts undergoingtraining in the nasa neutral buoyancy tank facility for an assembly task forspace station mission 9a. when we performed a protocol analysis of thesetapes (approximately 4000 utterances) focusing on the use of spatial languageand commands from one person to another, we found that the astronauts changedtheir frame of reference approximately every other utterance. as an example offrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.using computational cognitive models39how prevalent these changes in frame of reference are, consider the followingutterance from ground control:òé if you come straight down from where you are, uh, and uh, kind of peekdown under the rail on the nadir side, by your right hand, almost straight nadir,you should see theé.here we see five changes in frame of reference (highlighted in italics) in asingle sentence! this rate of change is consistent with the results found byfranklin et al. (1992). in addition, we found that the astronauts had to adoptothersõ perspectives, or force others to adopt their perspective, about 25 percentof the time (trafton et al., 2005a). obviously, the ability to handle changingframes of reference and to understand spatial perspective will be a critical skillfor the nasa robonaut and, we would argue, for any other robotic system thatmust communicate with people in spatial contexts (i.e., any construction task,direction giving, etc.).models of perspective takingimagine the following task, as illustrated in figure 1. an astronaut and hisrobotic assistant are working together to assemble a structure in shared space.the human, who because of an occluded view can see only one wrench, says tothe robot, òpass me the wrench.ó from the robotõs point of view, however, twofigure 1a scenario in which an astronaut asks a robot to òpass me the wrench.ófrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.40frontiers of engineeringwrenches are visible. what should the robot do? evidence suggests that humans,in similar situations, will pass the wrench they know the other human can seebecause this is a jointly salient feature (clark, 1996).we developed two models of perspective taking that could handle this scenario in a general sense. in the first approach, we used the actr/s system(harrison and schunn, 2002) to model perspective taking using a cognitivelyplausible spatial representation. in the second approach, we used polyscheme(cassimatis et al., 2004) to model the cognitive process of mental simulation;humans tend to simulate situations mentally to solve problems. using these models, we demonstrated a robot that could solve problems similar to the wrenchproblem.future workwe are now exploring other human cognitive skills that seem important forpeertopeer collaborative tasks and that are appropriate for building computational cognitive models that can be added to our robots. one new skill we areconsidering is nonvisual, highlevel focus of attention, which helps focus apersonõs attention on appropriate parts of the environment or situations based oncurrent conditions, task, expectations, models of other agents in the environment, and other factors. another human cognitive skill we are considering involves the role of anticipation in human interaction and decision making.conclusionclearly, for humans to work as peers with robots in shared space, the robotmust be able to understand the natural human tendency to use different frames ofreference and must be able to take the humanõs perspective. to create robotswith these capabilities, we propose using computational cognitive models, ratherthan more traditional programming paradigms for robots, for the following reasons. first, a natural and intuitive interaction reduces cognitive load. second,more predictable behavior engenders trust. finally, more understandable decisions by the robot enable the human to recognize and more quickly repair mistakes. we believe that computational cognitive models will give our robots thecognitive skills they need to interact more naturally with humans, particularly inpeertopeer relationships.referencesanderson, j.r., and c. lebiere. 1998. atomic components of thought. mahwah, n.j.: lawrenceerlbaum associates.cassimatis, n., j.g. trafton, m. bugajska, and a.c. schultz. 2004. integrating cognition, perception,and action through mental simulation in robots. robotics and autonomous systems 49(1ð2):13ð23.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.using computational cognitive models41clark, h.h. 1996. using language. new york: cambridge university press.franklin, n., b. tversky, and v. coon. 1992. switching points of view in spatial mental models.memory & cognition 20(5): 507ð518.harrison, a.m., and c.d. schunn. 2002. actr/s: a computational and neurologically inspiredmodel of spatial reasoning. p. 1008 in proceedings of the 24th annual meeting of the cognitive science society, edited by w.d. gray and c.d. schunn. fairfax, va.: lawrence erlbaumassociates.schultz, a., w. adams, and b. yamauchi. 1999. integrating exploration, localization, navigation andplanning through a common representation. autonomous robots 6(3): 293ð308.trafton, j.g., and a.c. schultz. 2006. children and robots learning to play hide and seek. pp.242ð249 in proceedings of acm conference on humanrobot interaction, march 2006. saltlake city, ut.: association for computing machinery.trafton, j.g., n.l. cassimatis, d.p. brock, m.d. bugajska, f.e. mintz, and a.c. schultz. 2005a.enabling effective humanrobot interaction using perspectivetaking in robots. ieee transactions on systems, man, and cybernetics, part a: systems and humans 35(4): 460ð470.trafton, j.g., a.c. schultz, n.l. cassimatis, l. hiatt, d. perzanowski, d.p. brock, m. bugajska,and w. adams. 2005b. communicating and collaborating with robotic agents. pp. 252ð278in cognitive and multiagent interaction: from cognitive modeling to social simulation, editedby r. sun. mahwah, n.j.: lawrence erlbaum associates.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.the nano/bio interfacefrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.45introductiontejal desaiuniversity of california, san franciscohiroshi matsuicity university of new york, hunter collegethe performance of natural systems in various aspects of engineering isoften superior to the performance of manmade technologies. hence, biomimeticsin nanoscale are being actively investigated to solve a variety of engineeringproblems. biology can provide tools for controlling material synthesis, physicalproperties, sensing, and mechanical properties at the molecular level. harnessingbiomolecular processes, such as selfassembly, catalytic activity, and molecularrecognition, can greatly enhance purely synthetic systems. therefore, the integration of these fields is a natural evolution in engineering.the speakers in this session update progress in this field, focusing on theinterface between biotechnology and nanotechnology. the first two speakerswill present approaches to using biotechnology to solve nanotechnology problems. the third and fourth speakers will describe approaches to usingnanotechnology to solve biotechnology problems.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.47biological and biomimeticpolypeptide materialstimothy j. deminguniversity of california, los angelesfibrous structures act as loadbearing components in vivo in many naturalproteins besides enzymes, which are soluble globular molecules. natural evolutionary processes have produced structural proteins that surpass the performanceof manmade materials (e.g., mammalian elastin in the cardiovascular systemthat lasts half a century without loss of function and spider webs composed ofsilk threads that are tougher than any synthetic fiber) (kaplan et al., 1994;mobley, 1994; viney et al., 1992). these biological polypeptides are all complex copolymers that derive their phenomenal properties from precisely controlled sequences and compositions of the constituent amino acid monomers,which, in turn, lead to precisely controlled selfassembled nanostructures. recently, there has been a great deal of interest in the development of syntheticroutes for preparing natural polymers, as well as de novo designed polypeptidesequences for applications in biotechnology (e.g., artificial tissues and implants),biomineralization (e.g., resilient, lightweight, ordered inorganic composites), andanalysis (e.g., biosensors and medical diagnostics) (capello and ferrari, 1994).to be successful, these materials must be òprocessible,ó or better yet, capable of selfassembly into precisely defined structures. peptide polymers havemany advantages over conventional synthetic polymers because they are able toassemble hierarchically into stable, ordered conformations (voet and voet,1990). this ability depends in part on the exact structures of protein chains (i.e.,defined molecular weight, monodispersity, stereoregularity, and sequence andfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.48frontiers of engineeringcomposition control at the monomer level). the inherent conformational rigidityof polypeptide chains also contributes to their ability to selfassemble. depending on the amino acid sidechain substituents, polypeptides are able to adopt amultitude of conformationally stable, regular secondary structures (e.g., helices,sheets, and turns), tertiary structures (e.g., the strandhelixstrand unit foundin barrels), and quaternary assemblies (e.g., collagen microfibrils) (voet andvoet, 1990). conformational rigidity and precise chain structures work togetherto produce hierarchically ordered, threedimensional materials from linear macromolecules.beyond laboratory replication of natural structural biopolymers, the synthesis of polypeptides that can selfassemble into nonnatural structures is an attractive challenge for polymer chemists. syntheticpeptidebased polymers are notnew materials; homopolymers of polypeptides have been available for manydecades, but their use as structural materials has been limited (fasman, 1967,1989). new methods in biological and chemical synthesis have made possiblethe preparation of increasingly complex polymer sequences of controlled molecular weight that display properties far superior to illdefined homopolymers.synthesis of biological polypeptidesthe advent of recombinant dna methodologies has provided a basis for theproduction of polypeptides with exact sequences that can be controlled by designof a suitable dna template. the most common technique for biosynthesis ofprotein polymers has been to design an artificial peptide sequence that can berepeated to form a larger polymer and then to construct the dna polymer thatencodes this protein sequence (mcgrath et al., 1990; tirrell et al., 1991). thedna sequences are then cloned and expressed, typically in yeast or bacterialhosts, to produce the designed polypeptides, which are then either secreted orisolated by lysing the microorganism cells. this methodology has been used forboth small and largescale production of biomimetic structural proteins, such assilks and elastins, as well as de novo sequences designed to fold into orderednanostructures (kaplan et al., 1994; mobley, 1994; viney et al., 1992).the steps required for synthesizing polypeptides using genetic engineeringare shown in figure 1. once a sequence of amino acids has been chosen, it isencoded into a complementary dna sequence, which must be chemically synthesized. solidphase synthesis has made significant advances in recent years,and it is now possible to synthesize polynucleotides that can encode chains ofhundreds of amino acids (mcbride and caruthers, 1983). by cloning these dnasequences into circular plasmid dna, the sequences can be incorporated intohost cells (e.g., bacteria). in these cells, the plasmids are amplified through replication and can then be sequenced to check for mutations or deletions before finalcloning into an expression plasmid. the expression plasmid contains a promotersequence that drives transcription. a strong promoter, such as the one in the petfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.biological and biomimetic polypeptide materials49expression vectors developed by studier et al. (1990), results in high levels ofdna transcription into messenger rna, which can result in high levelsof polypeptide production.once the material has been formed, the artificial protein must be isolatedfrom cellular byproducts and other proteins. in some cases, the polypeptidesform insoluble aggregates in the cells that can be purified by simply extractingall the cellular debris into suitable solvents and recovering the aggregates byfiltration. in cases where polypeptides are soluble after cell lysis, they can typically be purified using affinity chromatography by incorporating an affinitymarker into the polypeptide sequence (dong et al., 1994; zhang et al., 1992).recombinant dna methods have been used to prepare a variety of polypeptide materials. tirrellõs group has prepared lamellar crystals of controlled thickness and surface chemistry (creel et al., 1991; krejchi et al., 1994; mcgrath etal., 1992), smectic liquid crystals from monodispersed, rodlike helicalpolypeptides (zhang et al., 1992), and hydrogels from helixcoilhelix triblockcopolypeptides (petka et al., 1998). in related work, van hest has preparedpolypeptide sheet fibrils with the recombinant polypeptide chemically coupledmaterial concept inspired by natureprimary sequenceof amino acidsplasmidenzymemicrobial host complementary dnachemical synthesisamino acid polymersynthetic generecombinant plasmidfigure 1genetic engineering of polypeptides. source: van hest and tirrell, 2001.reprinted with permission from the royal society of chemistry.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.50frontiers of engineeringto synthetic poly(ethylene glycol) chains to improve solubility (smeenk et al.,2005). the genetically engineered polypeptide yields wellordered fibrils of controlled thickness and width.chaikof and coworkers have used recombinant dna methods to prepareproteinbased thermoplastic elastomers (nagapudi et al., 2005). they preparedtriblock copolymer sequences that mimic the natural protein elastin; the sequences were modified in such a way that the outer segments were plastic, andthe central segment was elastomeric, similar to purely synthetic thermoplasticelastomers, such as styreneisoprenestyrene rubber. overall, with genetic engineering we can prepare polypeptides with the complexity of natural proteins.hence, it is possible to prepare materials with polymer properties that can bemanipulated with exquisite control.synthesis of chemical polypeptidesto circumvent tedious protein purification, large investments of time ingene synthesis, and difficult artificial amino acid incorporation, it would be advantageous if we could synthesize complex copolypeptides chemically from inexpensive amino acid monomers. however, there is a large gap between biologically produced materials and the polypeptides that can be produced synthetically.the most common techniques used for polypeptide synthesis are solidphasesynthesis and polymerization of aminoacidncarboxyanhydrides (ncas)(kricheldorf, 1987, 1990). solidphase synthesis, which originated in the pioneering work of merrifield, involves the stepwise addition of nprotected aminoacids to a peptide chain anchored to a polymeric support (figure 2) (bodanzskyand bodanzsky, 1994; wunsch, 1971). the products of this method are sequencespecific peptides that can be isolated as pure materials. like solidphase synthesis of oligonucleotides, the number of peptide residues that can be correctlyadded to the chains depends on the efficiency of each individual step. however,the capability of strict sequential control in short sequences has made it possibleto prepare peptidebased materials.solidphase synthesis has been used to prepare a variety of polypeptidematerials. ghadiri and coworkers used the natural peptide gramacidina as amodel to develop cyclic peptides that selfassemble into hollow peptide nanotubes (ghadiri et al., 1993; khazanovich et al., 1994). the key component oftheir assembly is the pattern of alternating stereochemistry in the amino acidsequence, which leads to a barrellike helical structure. numerous other groupshave prepared short peptide sequences that assemble into welldefined nanofibers, either through sheet or helical coiledcoil assembly (aggeli et al.,1997; lashuel et al., 2000; niece et al., 2003; schneider et al., 2002; zimenkovet al., 2004).recently, woolfsonõs group has shown that welldefined kinks can be placedin these fibrils by careful design of the peptide sequence (ryadnov and woolfson,frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.biological and biomimetic polypeptide materials51nh2or1onhor2ohfmocnhor2fmocnhor1onh2or2nhor1onhorohn hoh+++byproducts (dcu)polymericsupportcoupling agent(e.g., dcc)1) wash2) piperidine(deprotection of amine)3) wash1) fmocamino acid2) repeat procedure for each residue3) hbrtfa or hf (cleavage from support)free peptidefigure 2solidphase peptide synthesis. fmoc = fluorenylmethoxycarbonyl, dcc =dicyclohexylcarbodiimide, dcu = dicyclohexylurea.2003). zhangõs group has also shown that small, amphiphilic peptides can bedesigned to form fibrils that create hydrogels that can be used as cell scaffolds(zhang et al., 2002), as well as membranes that can close off into spherical andtubular vesicles (vauthey et al., 2002). solidphase peptide synthesis has alsobeen used extensively in the preparation of hybrid copolymers, in which a smallpeptide sequence can have a big effect on overall materials properties (klok,2002).frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.52frontiers of engineeringthe most common technique currently used for largescale preparation ofpolypeptides is ncas (eq. 1). however, these materials are almost exclusivelyhomopolymers, random copolymers, or graft copolymers that do not have thesequence specificity and monodispersity of natural proteins (kricheldorf, 1987,1990). therefore, the physical properties of ncas do not meet the requirementsfor most applications, even though they have long been available and the monomers are inexpensive and easy to prepare. recently, a methodology has beendeveloped using transitionmetal complexes as active species to control the addition of nca monomers to polymer chain ends. the use of transition metals tocontrol reactivity has been proven to increase both reaction selectivity and efficiency in organic and polymer synthesis (boor, 1979; webster, 1991; wulff,1989).using this approach, substantial advances in controlled nca polymerization have been realized in recent years. highly effective zerovalent nickel andcobalt initiators (i.e., bpyni(cod) [deming, 1997] and (pme3)4co [deming,1999]) developed by deming allow the polymerization of ncas in a controlledmanner without side reactions into high molecular weight polypeptides via anunprecedented activation of the ncas into covalent propagating species (eq. 2).these cobalt and nickel complexes are able to produce polypeptides with narrowequation 1equation 2frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.biological and biomimetic polypeptide materials53chainlength distributions (mw/mn < 1.20) and controlled molecular weights (500< mn < 500,000) (deming, 2000). because this polymerization system is verygeneral, controlled polymerization of a wide range of nca monomers can beproduced as pure enantiomers (d or l configuration) or as racemic mixtures. byadding different nca monomers, the preparation of block copolypeptides ofdefined sequence and composition is also feasible (deming, 2000).polypeptide block copolymers prepared via transitionmetalmediated ncapolymerization are well defined, with the sequence and composition of blocksegments controlled by order and quantity of monomer, respectively, added toinitiating species. these block copolypeptides can be prepared with the samelevel of control as in anionic and controlled radical polymerizations of vinylmonomers, which greatly expands the potential of polypeptide materials. theunique chemistry of these initiators and nca monomers also allows nca monomers to be polymerized in any order, which has been a problem in most vinylcopolymerizations. in addition, the robust chain ends allow preparation ofcopolypeptides with many block domains (e.g., >4).the selfassembly of these block copolypeptides has also been investigated(e.g., to direct the biomimetic synthesis of ordered silica structures [cha et al.,2000], to form polymeric vesicular membranes [bellomo et al., 2004 (figure 3);holowka et al., 2005], and to prepare selfassembled polypeptide hydrogels[nowak et al., 2002]). furthermore, poly(llysine)bpoly(lcysteine) blockcopolypeptides have been used to generate hollow, organic/inorganic hybridmicrospheres composed of a thin inner layer of gold nanoparticles surroundedby a thick layer of silica nanoparticles (wong et al., 2002). using the sameprocedure, hollow spheres were also prepared; these consisted of a thick innerlayer of coreshell cdse/cds nanoparticles and a thicker outer layer of silicananoparticles (cha et al., 2002). the latter spheres are of interest, because theyallowed for microcavity lasing without the use of additional mirrors, substratespheres, or gratings.conclusionsmany approaches are being investigated for synthesizing new polypeptidematerials. biological approaches have been demonstrated to be extremely powerful for preparing materials with the precision of natural proteins. chemicaltechniques are being developed that might be used to prepare polypeptides ofany amino acid monomer. the two methodologies complement each other verywell. the biological approach is most useful for preparing polypeptides in whichmonomer sequence and composition must be controlled at the monomer level.the chemical approach is best suited for the preparation of high molecular weightpolypeptides in which sequence and composition must only be controlled onlength scales of many monomer units (i.e., homopolymer blocks). future advances in both the biological and chemical arenas are obviously targeted towardfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.54frontiers of engineeringa.b.figure 3(a) selfassembled vesicles from synthetic diblock copolypeptides. source:bellomo et al., 2004. reprinted with permission from macmillan publishers ltd.(b) silicacoated plates of helical polypeptide single crystals. source: bellomo anddeming, 2006. copyright 2006 american chemical society. reprinted with permission.conquering the shortcomings of each methodñincorporation of artificial aminoacids and simplification of preparation/purification for biological synthesis andcontrol of monomer sequence and composition in chemical synthesis. progressin these areas could expand both methodologies to the point where they mightmeet and overlap, thus providing scientists with the means of synthesizing anyconceivable polypeptide structure.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.biological and biomimetic polypeptide materials55referencesaggeli, a., m. bell, n. boden, j.n. keen, p.f. knowles, t.c.b. mcleish, m. pitkeathly, and s.e.radford. 1997. responsive gels formed by the spontaneous selfassembly of peptides intopolymeric sheet tapes. nature 386: 259ð262.bellomo, e.g., and t.j. deming. 2006. monoliths of aligned silicapolypeptide hexagonal platelets.journal of the american chemical society 128: 2276ð2279.bellomo, e., m.d. wyrsta, l. pakstis, d.j. pochan, and t.j. deming. 2004. stimuliresponsivepolypeptide vesicles by conformationspecific assembly. nature materials 3: 244ð248.bodanzsky, m., and a. bodanzsky. 1994. practice of peptide synthesis, 2nd ed. new york: springerverlag.boor, j. 1979. zeiglernatta catalysts and polymerizations. new york: academic press.capello, j., and f. ferrari. 1994. microbial production of structural protein polymers. pp. 35ð92 inplastics from microbes. cincinnati, ohio: hanser/gardner.cha, j.n., g.d. stucky, d.e. morse, and t.j. deming. 2000. biomimetic synthesis of ordered silicastructures mediated by block copolypeptides. nature 403: 289ð292.cha, j.n., m.h. bartl, m.s. wong, a. popitsch, t.j. deming, and g.d. stucky. 2002. microcavitylasing from block peptide hierarchically assembled quantum dot spherical resonators. nanoletters 3: 907ð911.creel, h.s., m.j. fournier, t.l. mason, and d.a. tirrell. 1991. genetically directed syntheses ofnew polymeris materials. efficient expression of a monodisperse copolypeptide containingfourteen tandemmly repeatedñ(alagly)4progluglyñelements. macromolecules 24: 1213ð1214.deming, t.j. 1997. facile synthesis of block copolypeptides of defined architecture. nature 390:386ð389.deming, t.j. 1999. cobalt and iron initiators for the controlled polymerization of amino acidncarboxyanhydrides. macromolecules 32: 4500ð4502.deming, t.j. 2000. living polymerization of aamino acidncarboxyanhydrides. journal of polymer science, part a: polymer chemistry 38: 3011ð3018.dong, w., m.j. fournier, t.l. mason, and d.a. tirrell. 1994. bifunctional hybrid artificial proteins.polymer preprints 35(2): 419ð420.fasman, g.d. 1967. poly amino acids. new york: dekker.fasman, g.d. 1989. prediction of protein structure and the principles of protein conformation. newyork: plenum press.ghadiri, m.r., j.r. granja, r.a. milligan, d.e. mcree, and n. khazanovich. 1993. selfassemblingorganic nanotubes based on a cyclic peptide architecture. nature 366: 324ð327.holowka, e.p., d.j. pochan, and t.j. deming. 2005. charged polypeptide vesicles with controllablediameter. journal of the american chemical society 127: 12423ð12428.kaplan, d., w.w. adams, b. farmer, and c. viney. 1994. silk polymers. american chemicalsociety symposium series 544. washington, d.c.: american chemical society.khazanovich, n., j.r. granja, d.e. mcree, r.a. milligan, and m.r. ghadiri. 1994. nanoscaletubular ensembles with specified internal diameters. design of a selfassembled nanotube witha 13† pore. journal of the american chemical society 116: 6011ð6012.klok, h.a. 2002. proteininspired materials: synthetic concepts and potential applications. angewandte chemie international edition 41: 1509ð1513.krejchi, m.t., e.d.t. atkins, a.j. waddon, m.j. fournier, t.l. mason, and d.a. tirrell. 1994.chemical sequence control of betasheet assembly in macromolecular crystals of periodicpolypeptides. science 265: 1427ð1432.kricheldorf, h.r. 1987. aminoacidncarboxyanhydrides and related materials. new york:springerverlag.kricheldorf, h.r. 1990. polypeptides. pp. 1ð132 in models of biopolymers by ringopening polymerization, edited by s. penczek. boca raton, fla.: crc press.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.56frontiers of engineeringlashuel, h.a., s.r. labrenz, l. woo, l.c. serpell, and j.w. kelly. 2000. protofilaments, filaments,ribbons, and fibrils from peptidomimetic selfassembly: implications for amyloid fibril formation and materials science. journal of the american chemical society 122: 5262ð5277.mcbride, l.j., and m.h. caruthers. 1983. an investigation of several deoxynucleoside phosphoramidites useful for synthesizing deoxyoligonucleotides. tetrahedron letters 24: 245ð248.mcgrath, k.p., d.a. tirrell, m. kawai, m.j. fournier, and t.l. mason. 1990. chemical and biosynthetic approaches to the production or novel polypeptide materials. biotechnology progress 6:188ð192.mcgrath, k.p., m.j. fournier, t.l. mason, and d.a. tirrell. 1992. genetically directed syntheses ofnew polymeric materials. expression of artificial genes encoding proteins with repeatingñ(alagly)3progluglyñelements. journal of the american chemical society 114: 727ð733.mobley, d. p. 1994. plastics from microbes. cincinnati, ohio: hanser/gardner.nagapudi, k., w.t. brinkman, j. leisen, b.s. thomas, e.r. wright, c. haller, x. wu, r.p. apkarian, v.p. conticello, and e.l. chaikof. 2005. proteinbased thermoplastic elastomers. macromolecules 38: 345ð354.niece, k.l., j.d. hartgerink, j.j.j.m. donners, and s.i. stupp. 2003. selfassembly combining twobioactive peptideamphiphile molecules into nanofibers by electrostatic attraction. journal ofthe american chemical society 125: 7146ð7147.nowak, a.p., v. breedveld, l. pakstis, b. ozbas, d.j. pine, d. pochan, and t.j. deming. 2002.peptides are a gelõs best friend. nature 417: 424ð428.petka, w.a., j.l. harden, k.p. mcgrath, d. wirtz, and d.a. tirrell. 1998. reverse hydrogels fromselfassembling artificial proteins. science 281: 389ð392.ryadnov, m.g., and d.n. woolfson. 2003. engineering the morphology of a selfassembling protetin fibre. nature materials 2: 329ð332.schneider, j.p., d.j. pochan, b. ozbas, k. rajagopal, l.m. pakstis, and j. gill. 2002. responsivehydrogels from the intramolecular folding and selfassembly of a designed peptide. journal ofthe american chemical society 124: 15030ð15037.smeenk, j.m., m.b.j. otten, j. thies, d.a. tirrell, h.g. stunnenberg, and j.c.m. van hest. 2005.controlled assembly of macromolecular bsheet fibrils. angewandte chemie international edition 44: 1968ð1971.studier, w.f., a.h. rosenberg, j.j. dunn, and j.w. dubendorf. 1990. use of t7 rna polymerase todirect expression of cloned genes. methods in enzymology 185: 60ð89.tirrell, d.a., m.j. fournier, and t.l. mason. 1991. genetic engineering of polymeric materials.materials research society bulletin 16: 23ð28.van hest, j.c.m., and d. tirrell. 2001. proteinbased materials, toward a new level of structuralcontrol. chemical communications 1897ð1904.vauthey, s., s. santoso, h.y. gong, n. watson, and s.g. zhang. 2002. molecular selfassembly ofsurfactantlike peptides to form nanotubes and nanovesicles. proceedings of the national academy of sciences 99: 5355ð5360.viney, c., s.t. case, and j.h. waite. 1992. biomolecular materials. materials research societyproceedings 292. pittsburgh, penn.: materials research society.voet, d., and j.g. voet. 1990. biochemistry. new york: john wiley and sons.webster, o. 1991. living polymers. science 251: 887ð893.wong, m.s., j.n. cha, k.s. choi, t.j. deming, and g.d. stucky. 2002. assembly of nanoparticlesinto hollow spheres using block copolypeptides. nano letters 2: 583ð587.wulff, g. 1989. mainchain chirality and optical activity in polymers consisting of cc chains.angewandte chemie international edition 28: 21ð37.wunsch, e. 1971. synthesis of naturally occurring polypeptides, problems of current research. angewandte chemie international edition 10: 786ð795.zhang, g., m.j. fournier, t.l. mason, and d.a. tirrell. 1992. biological synthesis of monodispersederivatives of poly(alphalglutamic acid): model rodlike polymers. macromolecules 25: 3601ð3603.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.biological and biomimetic polypeptide materials57zhang, s., d.n. marini, w. hwang, and s. santoso. 2002. design of nanostructured biologicalmaterials through selfassembly of peptides and proteins. current opinion in chemical biology6: 865ð871.zimenkov, y., v.p. conticello, l. guo, and p. thiyagarajan. 2004. rational design of a nanoscalehelical scaffold derived from selfassembly of a dimeric coiled coil motif. tetrahedron 60:7237ð7246.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.59applications of biomimeticsmorley o. stoneair force research laboratorywrightpatterson air force base, ohioat first glance, imitating nature via biomimetics seems to be a straightforward proposition. for example, if you are a roboticist, add legs to the platforminstead of wheels. unfortunately, as is often the case, the devil is in the details.after a short synopsis of examples of biomimetic material synthesis, sensing,and robotics, i will attempt to identify some lessons learned, some surprising andunanticipated insights, and some potential pitfalls in biomimetics. (for recentperspectives on combining biology with other disciplines, see naik and stone,2005.)òjust donõt add wateróoften, biologists and engineers speak completely different languages. perhaps the most graphic example of this is a comparison of the world of electricalengineers and sensor designers with the world of biologists. manipulation ofbiological macromolecules (i.e., nucleic acids and/or proteins) involves the useof buffered solutions (usually ph ~ 7), controlled salinity, and regulated temperatures. incorporating these biological salt solutions into electronics and sensor architectures seems like an oxymoron. however, the conversion of biologicalmaterials away from solution to solidstate processing has been a major objective in our laboratory.the key to overcoming this seemingly insurmountable incompatibility is theuse of òbridgingó materials systems, such as polymer host materials that capturefrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.60frontiers of engineeringand maintain biological functionality (brott et al., 2004). many polymer systems, such as poly(vinyl alcohol), qualify as hydrogels because they incorporateand maintain an enormous amount of water. while the biological side of thisequation can be satisfied via the incorporation of water, polymer systems can bespincoated, lithographically patterned, made conductive, and undergo a host ofother treatments that electrical engineers routinely use. thus, polymers representa truly bridging material system in making biological macromolecules meshwith synthetic technology.another recent example highlights the potential of biological materials thathave been integrated into a common electrical construct, such as a lightemittingdiode (led). to accomplish this, however, there must be a paradigm shift inmaterials thinkingñnamely, what would happen if dna were processed in gramand kilogram quantities, instead of the traditional microgram quantities.the fishing industry in japan, which processes tons of seafood yearly, alsothrows away tons of dna from fish gametes. researchers at the chitose institute of science and technology in japan, in partnership with our laboratory,have processed this discarded dna into a surfactant complex and scaled theprocess up to a multigram scale (wang et al., 2001). in this form and at thisscale, dna can be spincoated into traditional electronics architectures. recently,a dna electronblocking layer spindeposited on the holeinjection side of theelectronhole recombination layer greatly enhanced led efficiency and performance (hagen et al., 2006) (figure 1).in another approach, we have attempted to use biology indirectly in advanced material synthesis and devices. similar to the refrain from a commercialfor a popular chemical company, biology isnõt in the final material, but it makesthe final material better. researchers around the world racing to harness theincredible electronic, thermal, and mechanical properties inherent in singlefigure 1photographs of alq3 green emitting bioled and baseline oled devices inoperation.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.applications of biomimetics61walled carbon nanotubes (swnts) have run into a formidable obstacle. after atypical synthesis run, there is a large variety (both in length and diameter) ofcarbon nanotubes. this variety contributes to a number of chiralities, whichdictate the metallic or semiconducting nature of the swnt. much of this sizeheterogeneity arises from the heterogeneity of the starting metallic nanoparticlesused to catalyze the growth of swnts.ferritins and ferritinlike proteins sequester iron (in the form of iron oxide)in precisely defined cavities ranging from 8 nm to 4 nm for human and bacterialforms, respectively. we recently engineered a bacterial form called dps to produce uniform, monodisperse iron oxide particles (kramer et al., 2005). we reasoned that a monodisperse starting pool of nanoparticles would lead to a moremonodisperse population of swnts. after the bacterial protein was used toproduce the iron oxide particles, the biological shell was removed via sinteringin a reduced atmosphere and subjected to gasphase carbonnanotube growth.the resulting swnts adopted the monodisperse character of the starting catalyst particles. thus, biology was not in the final product but was used to make atechnologically promising material better.materials science and engineering overlap biologyfrom a materials science and engineering perspective, favorable electronicand structural properties usually emerge when the synthesis process can be controlled at finer and finer levels. hence the frenzy and hype over nanotechnology.as illustrated in the carbon nanotube example, biology can provide tools forcontrolling and/or synthesizing materials at the molecular level.an example of this control is unicellular algae, called diatoms, which makeexquisite cellular structures out of silica. thousands of species of diatoms existin salt and fresh water. each diatom species makes unique silica structures andpatternsñfrom hinges to intricate arrays of holes and spines. silica synthesisoccurs at ambient temperature and ph and has a complexity greater than anything we can make synthetically using solgel techniques.krıger and colleagues (1999) provided insight into the silicadepositionprocess of diatoms, which has led to a complete rethinking of the molecularevolution of this process (naik et al., 2002) and how it can be used in practicalapplications, such as enzymatic encapsulation (luckarift et al., 2004). based onwork by krıger and others, the field of biomineralization has expanded therange of materials synthesized via a biological route to encompass not onlyoxides, but also metals and semiconductors (slocik et al., 2005).specific peptides can now be used for the nucleation and growth of inorganic nanomaterials. when one considers that peptides specific for inorganicbinding and nucleation can be combined (i.e., genetically fused) with peptidesthat bind another moiety, endless possibilities begin to emerge. for example,biological macromolecules might be incorporated directly into electronic strucfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.62frontiers of engineeringtures/devices. one can imagine literally growing a fieldeffect transistor (fet)metaloxidemetal architecture via a biological route, rather than relying on standard topdown photolithographic processes.there might also be new approaches in optics and catalysis where there arenow significant challenges in assembling and interconnecting the building blocksof a nanoscale device. one might be able, for example, to address or measureresponses electronically at the molecular level. the very real scale gap betweenthe size of the molecule and the limits of lithography is shrinking. biobasedapproaches are being pursued to develop bottomup selfassembly techniquesthat provide specificity and versatility. peptides that recognize inorganic surfaces can be used as templates to organize and/or synthesize inorganic materials.by combining different functional peptides, we can create multifunctional polypeptides that can be used to synthesize and assemble hybrid materials. recently,we demonstrated that by growing bimetallic systems using a biobased approach,we can enhance catalytic activity of bimetallic materials (slocik and naik, 2006).in nature, the programmed assembly of amino acids in a polypeptide sequence gives rise to protein molecules that exhibit multifunctional properties.similarly, using protein engineering, inorganic binding peptide segments can befused to create multifunctional polypeptides to assemble and/or synthesize hybrid materials. by exploiting the interaction between peptides and inorganic materials, a peptide that contains two separate domains (each responsible for binding to a specific metal species) can be used to assemble hybrid materials (figure2). thus, we can control/program synthesis of bimetallic structures. the bimetallic nanoparticles made by using the designer peptides were found to be efficientcatalysts in the hydrogenation of 3buten1ol.this method represents a generalized approach to achieving hybrid structures by programming the amino acid sequence presented by the peptide interface. the peptide interface may be used to conjugate nanoparticle surfaces topolymers, organic molecules, or other biomolecules. however, to fully appreciate the potential of peptides and other biological building blocks as moleculartemplates, we will need a better understanding of the interaction betweendesignerpeptidem2(pd,pt)m1(au,ag)designerpeptidem2(pd,pt)m1(au,ag)figure 2assembly of hybrid materials using designer peptides.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.applications of biomimetics63biomolecules and inorganic surfaces. using computational modeling, we shouldget a much clearer understanding of the mechanism of peptideinorganic interactions. in the future, one should be able to create hybrid materials using proteinengineering by dialing in the sequence domains to direct their synthesis andassembly.bioinspired robotics:applied biology and engineeringthe combination of biological principles, mechanical engineering, and robotics has opened entirely new areas and possibilities. starting with the questionof why legs matter, the field of biodynotics is exploding to encompass whymaterials properties matter, why mechanics and architecture matter, and howbiological insights can lead to completely new capabilities. for example, entirelynew lessons and robotic capabilities have emerged, such as dynamic compliance,molecular adhesion, conformal grasping, and dynamic stability, to name just afew of the concepts that have been implemented into robotic platforms.the first contributions of biology to robotics were based on the insight that asprawled posture used opposing forces to achieve selfstabilization (dickensonet al., 2000; full and tu, 1991). much of this early work was focused on understanding the mechanics involved in legged locomotion. the springloaded invertedpendulum model has been accepted as an accurate model of biologicallocomotion independent of the number of legs or the biological platform (i.e.,horse or human or cockroach).recently, the cutkosky laboratory at stanford, where pneumatically drivenhexapod running robots were developed, has been challenged to build a wallclimbing platform capable of emulating geckolike behavior (clark et al., 2001).from a materials science perspective, the challenge has been to produce synthetic, selfcleaning hair arrays with a diameter of 200 nm at a density of 1ð2e9hairs/cm2.the field of robotics would also benefit immensely from the development oftunable (dynamic) modulus materials. today, compliance is usually tuned mechanically, which entails high costs in weight and power and produces less thansatisfactory performance. there are numerous biological models of tunablemodulus materials (e.g., the sea cucumber), and extrapolating these lessons torobotics could have a huge impact.conclusionin our research, we are framing future investments in an area we call biotronics, a term that encompasses both bioelectronics and biophotonics. as theled and fet examples described above suggest, this area is ripe for revolutionary breakthroughs. new capabilities, like tunable dielectrics, could revolutionizefrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.64frontiers of engineeringsensing and electronic readout. we believe that an integrated package of sensingand readout will emerge.biology may also enable us to fabricate materials, structures, and devicesfrom the bottom up. many believe that we will have to turn to biology forcommercially viable nanomanufacturing. catalysis and selfassembly have beenmastered by biological systems like enzymes and viruses, respectively. theselessons are being applied to traditional solidstate electronics, and engineers arebeginning to realize the possibilities.to continue advancements in biomimetics, we must include these principlesin undergraduate and graduate training programs. many other countries are alsoawakening to this realization. thus, the future technical base of our country willdepend on how well the science and engineering departments in our universitiesencourage this interdisciplinary training.referencesbrott, l.l, s.m. rozenzhak, r.r. naik, s.r. davidson, r.e. perrin, and m.o. stone. 2004. apoly(vinyl alcohol)/carbonblack composite film: a platform for biological macromolecule incorporation. advanced materials 16: 592ð596.clark, j.e, j.g. cham, s.a. bailey, e.m. froehlich, p.k. nahata, r.j. full, and m.r. cutkosky.2001. biomimetic design and fabrication of a hexapedal running robot. pp. 3643ð3649 inproceedings of the ieee international conference on robotics and automation, vol. 4. piscataway, n.j.: ieee.dickenson, m.h., c.t. farley, r.j. full, m.a.r. koehl, r. kram, and s. lehman. 2000. howanimals move: an integrative view. science 288: 100ð106.full, r.j., and m.s. tu. 1991. mechanics of a rapid running insect: two, four, and sixleggedlocomotion. journal of experimental biology 156: 215ð231.hagen, j.a., w. li, a.j. steckl, and j.g. grote. 2006. enhanced emission efficiency in organic lightemitting diodes using deoxyribonucleic acid complex as an electron blocking layer. appliedphysics letters 88: 171109ð171111.kramer, r.m., l.a. sowards, m.j. pender, m.o. stone, and r.r. naik. 2005. constrained ironcatalysts for singlewalled carbon nanotube growth. langmuir 21: 8466ð8470.krıger, n., r. deutzmann, and m. sumper. 1999. polycationic peptides from diatom biosilica thatdirect silica nanosphere formation. science 286: 1129ð1132.luckarift, h.r., j.c. spain, r.r. naik, and m.o. stone. 2004. enzyme immobilization in a biomimetic silica support. nature biotechnology 22: 211ð213.naik, r.r., and m.o. stone. 2005. integrating biomimetics. materials today 8: 18ð26.naik, r.r., l.l. brott, s.j. clarson, and m.o. stone. 2002. silicaprecipitating peptides isolatedfrom a combinatorial phage display peptide library. journal of nanoscience and nanotechnology 2: 1ð6.slocik, j.m., and r.r. naik. 2006. biologically programmed synthesis of bimetallic nanostructures.advanced materials 18: 1988ð1992.slocik, j.m., m.o. stone, and r.r. naik. 2005. synthesis of gold nanoparticles using multifunctionalpeptides. small 1(11): 1048ð1052.wang, l., j. yoshida, and n. ogata. 2001. selfassembled supramolecular films derived from marinedeoxyribonucleic acid (dna)cationic surfactant complexes: largescale preparation and optical and thermal properties. chemistry of materials 13: 1273ð1281.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.65optical imaging for in vivo assessment oftissue pathologyrebekah a. drezek, naomi j. halas, and jennifer westrice universityhouston, texasfor hundreds of years, optical imaging at both macroscopic and microscopiclevels has been used as a tool to aid clinicians in establishing a diagnosis. pathologists routinely use a simple compound microscope to examine stained andsectioned tissue at the microscopic level to determine a definitive diagnosis ofcancer. at a macroscopic level, clinicians often rely on observed colors as indicators of physiologic status, for instance, yellow skin is associated with jaundice,blue or purple hues with cyanosis, and red with inflammation. in each of theseexamples, the human eye gathers qualitative information about a patientõs statusbased on either the gross visual appearance of tissue or a microscopic evaluationof stained tissue sections or cytologic samples.despite the clear importance of these qualitative optical approaches in current medical practice, these strategies are only sensitive to a highly limitedsubset of the wide array of optical events that occur when light interacts withbiologic tissue. in fact, there is a compelling need for more quantitative opticalimaging strategies that can probe tissue physiology in vivo in real time withhigh resolution at relatively low cost. in this talk, i describe emerging technologies for quantitative optical imaging and the use of these technologies to diagnose and monitor cancer. particular emphasis is placed on how advances innanobiotechnology are leading to new approaches to in vivo medical diagnostics. because another talk in this session will consider luminescencebasednanomaterials (i.e., quantum dots), the discussion here is focused on nanomaterials, particularly goldbased materials, that provide a scatterbased or abfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.66frontiers of engineeringsorptionbased optical signal. the general biocompatibility of gold, coupledwith extensive prior medical applications of gold colloid, suggests a morestraightforward regulatory path toward ultimate clinical use than for many othernanomaterials currently under development.the role of nanotechnology inoptical imaging of cancerfor more than 50 years, cancer was the second leading cause of death in theunited states, accounting for more than 25 percent of deaths in the population.however, in the past two years, death from cancer has exceeded deaths fromheart attacks, and cancer has become the primary cause of deaths in the unitedstates. early detection is recognized as a highly effective approach to reducingthe morbidity and mortality associated with cancer. when diagnosed at an earlystage when the cancer is still localized and risk for metastasis is low, mostcancers are highly treatable and prognoses are favorable. however, if cancer isnot diagnosed until metastasis to distant sites has already occurred, fiveyearsurvival is poor for a wide variety of organ sites (table 1) (american cancersociety, 2006). thus, there is a significant clinical need for novel methods ofearly detection and treatment with improved sensitivity, specificity, and costeffectiveness.in recent years, a number of groups have demonstrated that photonicsbasedtechnologies can be valuable in addressing this need. optical technologies promise highresolution, noninvasive functional imaging of tissue at competitive costs.however, in many cases, these technologies are limited by the inherently weakoptical signals of endogenous chromophores and the subtle spectral differencesbetween normal and diseased tissue.in the past several years, there has been increasing interest in combiningemerging optical technologies with novel exogenous contrast agents designed toprobe the molecularspecific signatures of cancer to improve the detection limitsand clinical effectiveness of optical imaging. for instance, sokolov et al. (2003)recently demonstrated the use of gold colloid conjugated to antibodies to theepidermal growth factor receptor as a scattering contrast agent for biomoleculartable 1 cancer survival at five years as a function of stage atdiagnosisorganlocalizedregionaldistantprostate~100%>85%30%oral>80%50%25%breast>90%80%25%frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.optical imaging for in vivo assessment67optical imaging of cervical cancer cells and tissue specimens. in addition, opticalimaging applications of nanocrystal bioconjugates have been described by multiple groups, including bruchez et al. (1998), chan and nie (1998), and akermanet al. (2002). more recently, interest has developed in the creation ofnanotechnologybased platform technologies that can couple molecularspecificearly detection strategies with appropriate therapeutic intervention and monitoring capabilities.metal nanoshellsmetal nanoshells are a new type of nanoparticle composed of a dielectriccore, such as silica, coated with an ultrathin metallic layer, typically gold. goldnanoshells have physical properties similar to gold colloid, particularly a strongoptical absorption due to goldõs collective electronic response to light. the optical absorption of gold colloid yields a brilliant red color, which has been usedeffectively in consumerrelated medical products, such as home pregnancy tests.in contrast, the optical response of gold nanoshells depends dramatically on therelative size of the nanoparticle core and the thickness of the gold shell.by varying the relative thicknesses of the core and shell, the color of goldnanoshells can be varied across a broad range of the optical spectrum that spansthe visible and nearinfrared spectral regions (brongersma, 2003; oldenburg etal., 1998). gold nanoshells can be made either to absorb or scatter light preferentially by varying the size of the particle relative to the wavelength of the light attheir optical resonances. figure 1 shows a mie scattering plot of the nanoshellplasmon resonance wavelength shift as a function of nanoshell composition for a60 nm core gold/silica nanoshell. in this figure, the core and shell of the nanoparticles are shown to relative scale directly beneath their corresponding opticalresonances. figure 2 shows a plot of the core/shell ratio versus resonance wavelength for a silica core/gold shell nanoparticle (oldenburg et al., 1998). theextremely agile òtunabilityó of optical resonance is a property unique tonanoshellsñin no other molecular or nanoparticle structure can the resonance ofthe optical absorption properties be òdesignedó as systematically.halas and colleagues have completed a comprehensive investigation of theoptical properties of metal nanoshells (averitt et al., 1997) and achieved quantitative agreement between mie scattering theory and the experimentally observedopticalresonant properties. based on this success, it is now possible to designgold nanoshells predictively with the desired opticalresonant properties and thento fabricate the nanoshell with the dimensions and nanoscale tolerances necessary to achieve these properties (oldenburg et al., 1998). the synthetic protocoldeveloped for the fabrication of gold nanoshells is very simple in concept:1.grow or obtain silica nanoparticles dispersed in solution.2.attach very small (1ð2 nm) metal òseedó colloids to the surface of thefrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.68frontiers of engineeringfigure 1optical resonances of goldshell, silicacore nanoshells as a function of thecore/shell ratio. respective spectra correspond to the nanoparticles shown below them.source: loo et al., 2004. reprinted with permission.nanoparticles via molecular linkages; the seed colloids cover the dielectricnanoparticle surfaces with a discontinuous metal colloid layer.3.grow additional metal onto the òseedó metal colloid adsorbates viachemical reduction in solution.this approach has been used successfully to grow both gold and silvermetallic shells onto silica nanoparticles. various stages in the growth of a goldmetallic shell onto a functionalized silica nanoparticle are shown in figure 3.based on the core/shell ratios that can be achieved with this protocol, goldnanoshells with optical resonances extending from the visible region to approximately 3 µm in the infrared region can currently be fabricated. this spectral region includes the 800ð1,300 nm òwater windowó of the near infrared, aregion of high physiological transmissivity that has been demonstrated as thespectral region best suited for optical bioimaging and biosensing. the opticalfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.optical imaging for in vivo assessment691,0001001010246810wavelength (microns)core radius/shell thickness ratiofigure 2core/shell ratio as a function of resonance wavelength for gold/silicananoshells. source: loo et al., 2004. reprinted with permission.properties of gold nanoshells, coupled with their biocompatibility and ease ofbioconjugation, render them highly suitable for targeted bioimaging and therapeutic applications. by controlling the physical parameters of the nanoshells, itis possible to engineer nanoshells that primarily scatter light, which is desirablefor many imaging applications, or alternatively, to design nanoshells that arestrong absorbers, which is desirable for photothermalbased therapy applications.figure 3transmission electron microscope images of gold/silica nanoshells duringshell growth. source: loo et al., 2004. reprinted with permission.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.70frontiers of engineeringbecause the same chemical reaction is used to grow the metal layer of goldnanoshells as is used to synthesize gold colloid, the surfaces of gold nanoshellsare virtually chemically identical to the surfaces of the gold nanoparticles universally used in bioconjugate applications. gold colloid was first used in biological applications in 1971 when faulk and taylor (1971) invented theimmunogold staining procedure. since then, the labeling of targeting molecules,especially proteins, with gold nanoparticles has revolutionized the visualizationof cellular or tissue components by electron microscopy. the optical and electronbeam contrast qualities of gold colloid have provided excellent detectionqualities for immunoblotting, flow cytometry, hybridization assays, and othertechniques. conjugation protocols exist for the labeling of a broad range ofbiomolecules with gold colloid, such as protein a, avidin, streptavidin, glucoseoxidase, horseradish peroxidase, and igg.the vast prior history of goldcolloidbased materials has greatly facilitatedthe development of biomedical applications of newer goldbased nanoparticles.figure 4 shows one example of the type of medical application enabled by usingthis class of material. the figure shows an in vitro proofofprinciple example offigure 4dual imaging/therapy nanoshell bioconjugates. source: loo et al., 2005.copyright 2005 american chemical society. reprinted with permission.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.optical imaging for in vivo assessment71gold nanoshells designed to simultaneously scatter (for imaging) and absorb (forphotothermal therapy) near infrared light. here, scattering and absorbing nearinfrared nanoshells are conjugated to an antibody for a common breast cancersurface marker. this enables both òlighting upó and, if desired, destroying cellsthat express this marker without harming other cells. in figure 4, the top rowshows scatterbased imaging of carcinoma cells. by increasing the laser power,it is possible to destroy cells selectively as shown in the middle row, whichshows the viability of cells after laser irradiation. the left column shows a nonanoshell (cellsonly) control, and the middle column shows a nonspecific antibody control. the right column indicates successful imaging of cells followed byphotothermal destruction (black circle = laser irradiation spot) based on the presence of a chosen marker.although in vitro demonstrations can be completed using simple microscopes, in vivo use of this type of nanomaterial requires coupling the development of appropriate materials with the development of optical devices that enable imaging of these materials in tissue. by careful design of these opticalsystems, it is possible to generate multiple order of magnitude improvements inoptical contrast using nanomaterial imaging agents, which could potentially leadto the detection of much smaller lesions. in addition to examples from our owngroup, work is being done by other laboratories using a variety of other goldbased nanomaterials. in all cases, the move from in vitro cellbased demonstrations to in vivo clinical use is enabled by rapid developments in photonicsbasedstrategies for realtime, lowcost in vivo imaging.summarynumerous research groups throughout the country are leveraging emergingtechniques in optical imaging and nanotechnology to develop powerful new approaches for detecting molecularspecific signatures of precancers and early cancers. these groups are developing several classes of ultrabright contrast agentsthat strongly scatter and/or absorb at tunable wavelengths throughout the visibleand nearinfrared spectral bands, as well as methods of targeting these agents tomolecular markers of neoplasia. they are demonstrating the efficacy of theseagents in biological samples of progressively increasing complexity. these initial efforts will certainly be expanded in future studies.ultimately, the use of ultrabright contrast agents will extend the detectionlimits of optical technologies, increasing their sensitivity and specificity andpromoting improved screening and detection of early lesions. we believe thereis tremendous potential for synergy between the rapidly developing fields ofbiophotonics and nanotechnology. combining the tools of these fieldsñtogetherwith the latest advances in understanding of the molecular origins of cancerñwill offer a fundamentally new approach to the detection of cancer, a diseaseresponsible for more than onequarter of all deaths in the united states today.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.72frontiers of engineeringreferencesakerman, m.e., w. chan, p. laakkonen, s.n. bhatia, and e. ruoslahti. 2002. nanocrystal targetingin vivo. proceedings of the national academy of sciences 99: 12617ð12621.american cancer society. 2006. cancer facts and figures 2006. available online at: http://www.cancer.org/docroot/stt/content/stt1xcancerfactsfigures2006.asp.averitt, r.d., d. sarkar, and n.j. halas. 1997. plasmon resonance shifts of aucoated au2snanoshells: insights into multicomponent nanoparticles growth. physiology review letters 78:4217ð4220.brongersma, m.l. 2003. nanoshells: gifts in a gold wrapper. nature materials 2: 296ð297.bruchez, m., m. moronne, p. gin, s. weiss, and a.p. alivisatos. 1998. semiconductor labels asfluorescent biological labels. science 281: 2013ð2016.chan, w.c.w., and s. nie. 1998. quantum dot bioconjugates for ultrasensitive nonisotopic detection. science 281: 2016ð2018.faulk, w.t., and g. taylor. 1971. an immunocolloid method for the electron microscope. immunochemistry 8: 1081ð1083.loo, c., l. hirsch, j. barton, n. halas, j. west, and r. drezek. 2004. nanoshellenabled photonicsbased cancer imaging and therapy. technology in cancer research and treatment 3: 33ð40.loo, c., a. lowery, n. halas, j. west, and r. drezek. 2005. immunotargeted nanoshells for integrated imaging and therapy of cancer. nano letters 5: 709ð711.oldenburg, s.j., r.d. averitt, s.l. westcott, and n.j. halas. 1998. nanoengineering of opticalresonances. chemical physics letters 288: 243ð247.sokolov, k., m. follen, j. aaron, i. pavlova, a. malpica, r. lotan, and r. richardskortum. 2003.realtime vital optical imaging of precancer using antiepidermal growth factor receptor antibodies conjugated to gold nanoparticles. cancer research 63: 1999ð2004.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.73commercialization and futuredevelopments in bionanotechnologymarcel p. bruchezcarnegie mellon universitypittsburgh, pennsylvaniaperfection in nanotechnology has long been achieved by biological systems.an enzyme represents a nearly perfect robot, stamping out molecular patternsfrom unique templates designed to execute individual tasks with nearly perfectefficiency. evolution has driven these efficient designs to enable life forms tothrive in harsh environments. evolutionary improvements have developed over aperiod of at least 3.5 billion years, with impressive results. the fact that we arehere at all is a testament to the power and vast potential of nanotechnology.recently, we have made the first bluntfingered attempts to extend the capabilities of biological systems by harnessing innovations in materials chemistryand electronics coupled with biologically defined specificity for both magneticand fluorescent probes. in these cases, we have succeeded in introducing relatively limited new functionalities to existing biological systems. but we havebarely tapped the potential of engineering of these systems, and from here on,our efforts will undoubtedly expand dramatically.at the present time, we are guided by empirical observations and not by adetailed understanding of the interactions of biological systems with the materials and devices we are preparing. thus, not only are we bluntfingered, but weare also nearly blind. before we can realize substantial commercial rewards andbenefits in health and medicine, we will have to expand our efforts dramaticallyto develop new characterization methods and basic specifications and predictorsof biological performance.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.74frontiers of engineeringdefinition of bionanotechnologyat the present time, there is no consensus definition of bionanotechnology.to take advantage of the enthusiasm of funding agencies, a number of old (andimportant) areas, such as colloid science, molecular biology, and implantablematerials surface science, have been relabeled ònanotechnology.ó in fact, all ofthese fields, coupled with biological systems, should be included in bionanotechnology. in general, the idea of bionanotechnology is the engineering of interfaces between molecules or materials and biological systems. looking ahead,the key areas for commercialization will be bringing engineered systems intobiological contact and biological function.the version of bionanotechnology popularized in the media has been largelyoversold. the general idea, which was popular 20 years ago as the òmagicbulletó theory of biotechnology and has been adopted as the bionanotechnologytarget, can be described as the òdump truckó model of technology. in this conception, the technology components consist of a targeting moiety, either biological or nanotechnological, and one or more cargoes, which are envisioned assmall machines capable of specific destructive or corrective action.in reality, designing targeting molecules that are selective for diseased tissues and capable of delivering cargoes larger than a typical antibody has provenextraordinarily difficult, and molecular targeting of nanoscale devices greaterthan 5 nm outside the vascular space may prove to be prohibitively difficult.however, with no guiding principles for the effective biological direction ofnonbiological molecules, this is still an open question.in this paper, i describe three recent examples of commercialized bionanotechnology, beginning with the one that is the best characterized system. thethree are antibodydirected enzyme prodrug therapy (adept), superparamagnetic iron oxide particles for enhancing contrast on magnetic resonance images(mris), and quantumdot technology for biological detection. each of theseshows the potential power and some of the challenges of integrating technologies at the molecular level.antibodydirected enzyme prodrug therapyperhaps the most salient and relevant example of a bionanotechnology currently being commercialized is the adept method being investigated bygenencor and seattle genetics (figure 1). in this method, an antibodyenzymefusion is first prepared and isolated. this molecule can be designed with precisechemical (biological) composition, precise linkage geometry, and complete definition and characterization using standard molecularbiology techniques and biochemical methods. the antibody, linked to the enzyme, can be used to target theparticular antibodyenzyme conjugate to the site of interest. in this way, a smallantibody fragment is used to target a molecular machine (an enzyme) to a parfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.commercialization and future developments in bionanotechnology75ticular site of interest, and the machine is then used to generate a specific molecule at that site. in the clinic, a prodrug (a drug molecule modified to an inactive state that can be converted to an active state in situ) is administered to thepatient. after the antibodyenzyme construct reaches its target site, the prodrugis administered and is converted by the enzyme to an active state. the localconcentration of the active drug can be driven very high, even though the overallconcentration remains very low. thus, the therapy is both safer and more effective than a high dose of the toxic compound.adept is a highly characterized, highly effective example of bionanotechnology in action. however, even after 15 years of active research, thesetargeted prodrug strategies are still in the research or early clinical trial stageand not in general practice. this is a reflection of the complexities of developing biospecific performing technologies, which is likely to be a general problemfor the development of nanotechnologies with high clinical impact.superparamagnetic iron oxide particlesa second, more recognizable example of bionanotechnology in clinical useis ferridex and combidex superparamagnetic particles, marketed by advancedmagnetics, which are being commercialized for enhancing mri signals (figure2). the particles are modified with dextran (a polymerized sugar molecule) tocreate a biocompatible coating, which dramatically reduces nonspecific interactions in the body and increases the contrast of the instrument wherever the particles are present. when administered intravenously, they can easily be measuredin a standard clinical mri instrument. these materials are currently approvedfor imaging cancers of the liver and spleen.recently, the combidex agent was rejected by the food and drug administration (fda) because of safety concerns and a lack of efficacy data. questionsregarding safety had arisen when at least one patient died in a clinical trialinvestigating the use of the combidex agent for sentinelnode detection (findingtumor site normal tissuesinactive drugactive drugantibodyenzymeconjugatefigure 1adept uses antibodies to target particular cells with an enzyme that thenconverts prodrug molecules to an active drug at the target site.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.76frontiers of engineeringnearnodes that are most likely to contain cancerous cells), which is critical tothe grading, staging, and appropriate treatment of cancers. the fda did recommend, however, that with further appropriately designed trials, the compoundmay be approvable for specific indications.quantumdot technologyi have been extensively involved in work on the third exampleñthe use offluorescent quantum dots for biological detection in research and, ultimately,clinical applications (figure 3). semiconductor nanocrystals (i.e., quantum dots),specifically designed to have intense monochromatic emission spectra, arecoupled to biological targeting molecules, such as antibodies and nucleic acids.the conjugates can then be used to detect the presence of particular analytes inbiological samples. although these particles dramatically increase experimentalinformation and sensitivity, the clinical community has been slow to adopt thembecause of subtle protocol differences between these materials and the typicalfluorescent dyes and enzymatic methods used in detection. many of the protocoldifferences are thought to arise from distinct size differences between typicalprobes and nanotechnologybased probes. such idiosyncrasies are likely to beubiquitous in nanotechnologyenabled product commercialization.the technology for the use of quantum dots in biology was initially published in september 1998 in two simultaneous papers in science (bruchez et al.,1998; chan and nie, 1998). although these articles generated significant enthufe3o4dextran polymers dextran polymersfigure 2combidex particles, which enhance contrast on mris, consist of superparamagnetic iron oxide nanoparticles covered with dextran. because these particles are excluded from tumorbearing regions of lymph nodes, they can help identify tumorbearingnodes noninvasively.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.commercialization and future developments in bionanotechnology77siasm in the scientific community, biologically useful products were not launcheduntil november 2002. in the meantime, quantum dot corporation was accusedof hoarding the technology, stalling progress, and many other things.in fact, the reasons for the delay were hardly nefariousñwe have no rationalframework for òoptimizingó these materials. therefore, although we were working very hard to make products that could be used successfully by the averagebiologist, every time we made an improvement to any aspect of the system, theentire process had to be revalidated. this empirical approach to product development resulted in a very long development time.this delay was in addition to unavoidable delays in process development. ananoparticle designed for a particular application is a complex multilayer structure, shown schematically in figure 3. scaleup of the initial chemistries used tomake these nanoparticles (as published in science) was exceptionally dangerous;procedures involved pyrophoric precursors, flammable solvents, and rapid additions and releases of explosive gases. to develop safe, scalable procedures, ourscientists had to develop innovative techniques in all aspects of nanoparticlechemistry. again, every innovation had to be validated through to the utility oforganic coatinginorganic shell (zns)corenanoparticle(cdse)targetingmolecule (e.g. antibody)biocompatibility modifications(e.g. polyethylene glycol)figure 3quantumdot conjugates have bright emission and multicolor capability, allowing researchers to view many targets in a single sample and simplifying detectionstrategies. the overall structure of these conjugates is designed and optimized for performance in biological applications (e.g., stability, brightness, and specificity).frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.78frontiers of engineeringthe final material, making the development cycle exceptionally onerous. justimagine if, to validate a change in one hose of a car, you had to build an entirecar with only that change included.the lack of specifiability of our modules was a key challenge to commercialization. specification will require detailed basic investigations of the properties and chemistry of nanoparticle materials in biological systems. in addition,we will have to establish analytical tools and quantitative descriptors to detail thedistribution of properties present in a population of nanoparticles. this is categorically different from specification for organic molecules and proteins, inwhich properties can be effectively described by an average. in nanomaterials,performance properties may be dominated by a relatively small population ofparticles, so averaging cannot always be used.the challenge of characterizationdramatically different tools are necessary for characterization of the threeexamples i have described. adept, a fully biological system, can be characterized structurally, chemically, and on the basis of activity to ensure that eachcomponent of the system is capable of acting independently and that this behavior is preserved as the system components are brought together. nevertheless,for reasons related to biological complexity, the use of adept in the clinic hasnot yet proven beneficial. this problem gives some indication of the challengesahead for nanotechnology solutions.the second example, combidex technology, is a homogeneousparticle technology covered with a natural material, dextran, that minimizes the complexityof the system. in this case, the particle size and shape (which can be characterized by electron microscopy) dictate its magnetic properties. the interaction ofdextran on the surface dictates the in vivo behavior of these materials. althoughthe components can be characterized in great detail, the interaction of the dextran with the surface (e.g., the number of surface iron atoms that are actuallycovered) may be crucial to the fate of these materials in clinical use, an obstaclethat was not predicted.the interaction of molecules with surfaces in complex environments represents a critical area for analytical development. at the moment, many studies arecarried out by xray photoelectron spectroscopy (xps), a vacuum technique thatdoes not show many solution interactions in the normal biological environment.microrheology techniques might be valuable in addressing this issue.the final example, quantum dots, an entirely engineered material, presentsmany characterization challenges. first, the particle itself is a complex structure,and the best available tools for characterizing these materials are capital intensive and often inaccessible. essentially, methods such as energydependent xpsrequire a synchrotron source. other methods, such as zcontrast scanning transmissionelectron microscopy, require unique instrumentation that is availablefrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.commercialization and future developments in bionanotechnology79only in a few laboratories in the world. in addition, these tools are suited eitherfor measuring average properties or measuring singleparticle properties, but notboth. bridging the gap to a statistical method that shows singleparticle properties in a large population of particles would allow for discrimination of population properties from singleparticle properties.moving out in the structure, the surface is coated with ligands. thus, surfaceinteractions will cause the same problems as have arisen for combidexñroutinetools do not give a detailed molecular picture of interactions at the surface. theproblem is further complicated as particles are modified for biological applications, for instance by coupling polyethylene glycol molecules to the surface.the characterization of chemistry on the surface of these particles has notmatured to the level of typical organic chemical reactions. in fact, much of thecharacterization is still inferential (i.e., we analyze what does not react with theparticle to determine what does react with it). the tools we have today neitherdiscriminate between adsorbed and reacted materials nor determine whether thechemistry is homogeneous or heterogeneous from particle to particle. these distinctions will be critical for the development of nanoparticle tools with biomedical applications.outlookwhere, then, will bionanotechnology take us? as the examples i have described show, advances have progressed from adept, a completely characterized system with a defined molecular structure (still encountering difficulty inclinical acceptance), to a system in which components are well characterized(combidex), to quantum dots, a system we still cannot fundamentally characterize. chemists have tools like mass spectrometry and nuclear magnetic resonancespectroscopy to guide them. engineers have testing and measurement systemsfor validating systems as small as a few hundred nanometers. in the middlerange, however, nanoengineers (or nanochemists) still do not have the fundamental tools to determine how well they have done their jobs or in what directionthey should look for improvements.devices are synthesized on molar (~1026) scales, but the characterizationtools designed for molecules do not work effectively for bionanotechnology systems. clearly, the device characterization methods (typically single òdeviceócharacterization on enough devices to ensure a reliable measurement of productionrun statistics) are inappropriate, especially when a dose is 1013 devices and aminor population component can dominate the bad effects (for instance poreclogging).thus, we have an acute and growing need for specifiability in the design ofbionanotechnology tools. to achieve engineerable systems, a concerted effortmust be made to conduct a basic scientific investigation of the impact of materials properties on the biological behavior of bionanotechnology systems, comfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.80frontiers of engineeringbined with a physical scientific investigation of new methods to characterize thedetailed physical and population properties of nanometerscale materials andcomponents. specifiability will make predictability, falsifiability, and rapidprogress in commercial bionanotechnology feasible.further readingadept technologyalderson, r.f., b.e. toki, m. roberge, w. geng, j. basler, r. chin, et al. 2006. characterization ofa cc49based singlechain fragmentbetalactamase fusion protein for antibodydirected enzyme prodrug therapy (adept). bioconjugate chemistry 17(2): 410ð418.bagshawe, k.d., s.k. sharma, and r.h.j. begent. 2004. antibodydirected enzyme prodrug therapy(adept) for cancer. expert opinion on biological therapy 4(11): 1777ð1789.wu, a.m., and p.d. senter. 2005. arming antibodies: prospects and challenges for immunoconjugates. nature biotechnology 23(9): 1137ð1146.combidex technologyharisinghani, m.g., and r. weissleder. 2004. sensitive, noninvasive detection of lymph node metastases. plos medicine 1(3): 202ð209.harisinghani, m.g., j. barentsz, p.f. hahn, w.m. deserno, s. tabatabaei, c.h. van de kaa, et al.2003. noninvasive detection of clinically occult lymphnode metastases in prostate cancer.new england journal of medicine 348(25): 2491ð2495.quantumdot technologyalivisatos, a.p. 2004. the use of nanocrystals in biological detection. nature biotechnology 22(1):47ð52.bruchez, m.p., p. gin, m. morrone, s. weiss, and a.p. alivasotos. 1998. semiconductor nanocrystals as fluorescent biological labels. science 281(5385): 2013ð2016.chan, w.c.w., and s. nie. 1998. quantum dot bioconjugates for ultrasensitive nonisotopic detection. science 281(5385): 2016ð2018.michalet, x., f.f. pinaud, l.a. bentolila, j.m. tsay, s. doose, j.j. li, et al. 2005. quantum dots forlive cells, in vivo imaging, and diagnostics. science 307(5709): 538ð544.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.engineering personal mobility for the21st centuryfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.83introductionapoorv agarwalford motor companydearborn, michiganwilliam f. schneideruniversity of notre damenotre dame, indianahumans have historically spent roughly the same share of their time andincome traveling daily, but modern technology, especially the automobile, hasgreatly increased both the range and convenience of personal travel. personalmobility enabled by automobiles is closely related to a sense of personal freedom. current approaches to providing this mobility have had a major impact onthe landscape of our cities and suburbs, on the environment as a whole, and onenergy consumption. providing the same levels of personal mobility in the futurein a costeffective, energyefficient, and environmentally sustainable manner inboth the developing and developed worlds is one of the great challenges for the21st century.the speakers in this session explore the history and evolution of personalmobility, including its availability and the expectations it raises, the energyand environmental challenges of current forms of personal mobility, and prospective technologies that could transform personal mobility for us and futuregenerations.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.85longterm trends inglobal passenger mobilityandreas sch•fer*university of cambridgecambridge, united kingdomanticipating changes in travel demand on aggregate levels is critical forindustries making decisions about meeting the demand for vehicles and fuel andfor governments planning infrastructure expansions, predicting transportsector(greenhousegas) emissions, and evaluating mitigation policies. in this paper, ishow that only a few variables are necessary to explain past levels and projectinternally consistent future levels of aggregate, worldregional travel demandand transport modes. i then highlight the enormous challenges that must be metto reduce greenhousegas emissions, especially from passenger aircraft, the fastest growing transport mode.determinants of aggregate travel demand andtransport modesgrowth in per capita income and population are the two single most important factors in passenger mobility. during the past 50 years, global average percapita income has increased slightly more than threefold, and world populationhas more than doubled. this combined growth, by a factor of 7.4, has translatedinto a nearly proportional increase in passenger mobility. the nearly direct rela*the fundamental ideas underlying the model described in this paper were developed jointly withdavid g. victor, stanford university. see sch−fer and victor (2000).frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.86frontiers of engineeringtionship can be explained by socalled travel budgets, roughly constant averagesof expenditure shares of money and time.although the amount of time spent traveling is highly variable on an individual level, large groups of people spend about 5 percent of their daily timetraveling. the stability of the aggregate òtraveltime budget,ó first hypothesizedin similar form for urban travelers by the late yacov zahavi (1981), is illustratedin figure 1 for a wide range of income levels. on average, residents in africanvillages, the palestinian territories, and the suburbs of lima spend between 60and 90 minutes per day traveling, the same as for people living in the automobiledependent societies of japan, western europe, and the united states.a similar transition, from variability at disaggregate levels to stability ataggregate levels, can be observed for travelexpenditure shares (i.e., the percentage of income dedicated to travel). zahavi observed that households that relyexclusively on nonmotorized modes of transport and public transportation spendonly about 3 to 5 percent of their income on travel; that percentage rises to 10 to15 percent for people who own at least one motor vehicle. figure 2 shows thatthe aggregate òtravelmoney budget,ó here defined as total travel expendituresdivided by the gross domestic product (gdp), follows a similar pattern, risingfrom about 5 percent of gdp at motorization rates close to zero cars per 1,000capita (nearly all u.s. households in 1909 and the least developed countries0.00.51.01.52.02.53.03.54.0196019701980199020002010traveltimebudget(hours/capita/day)japanesecitieschinesecitiesothercitiescountriescitiescountriestravelsurveys:timeusesurveys:villagesinsouthwesttanzaniavillagesinghanapalestinianterritories131japanesecitiesparisparispariswarsawjapan14differentsettingsinsovietunion,unitedstates,lima(peru),eastandwestgermany,france,etc.saopaulosaopaulosouth africaunitedstatessingaporefigure 1average daily travel time as a function of per capita gdp.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.longterm trends in global passenger mobility87today) to around 10 percent of gdp at about 300 cars per 1,000 capita (industrialized countries today), an ownership level of about one car per household ofthree to four people. in addition, travel demand and choice of transport modedepend on average doortodoor speed and travel costs to the consumer. thedrastic decrease in the cost of air travel in the past decades has contributed to therising share of that transport mode.the past five decades in world travel demandto study the historical development and project the future development ofglobal travel demand, we estimated a unique data set of passenger mobility for11 world regions, covering passengerkilometers (km) traveled (pkt) using fourmajor modes of transport (lightduty vehicles, buses, railways, and aircraft) andspanning 51 years (1950 to 2000).1 the overall relationship between gdp percapita and per person pkt is shown in figure 3. the saturating travelmoneybudget, described above, helps explain how rising gdp translates into risingtravel demand.05101520250100200300400500600700800motorizationrate(lightdutyvehicles/1,000capita)percentageofgdpdedicatedtotravelu.s.:1909œ2001westerneurope:1963œ2003easterneurope:2000œ2002japan:1963œ2003turkey:1994southafrica:2000srilanka:2002individualeasterneuropeancountriesindividualwesterneuropeancountriesmexico:2000figure 2travel expenditures as a fraction of income for annual travel distance.1this data set is an update of an older data set for 1960 to 1990 by the same author. see sch−fer(1998).frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.88frontiers of engineering1001,00010,000100,0001,000,0001001,00010,000100,0001,000,000gdp/cap(us$[1996])pkt/cap(km)centrallyplannedasialatinamericamiddle east, north and south africaotherpacificasiasubsaharanafricanorthamericapacificoecdwesterneuropeeasterneuropeformersovietunionsouthasiaworldfigure 3passengerkm per capita by per capita gdp for 11 world regions and theentire world from 1950 to 2000.over the past five decades, earthõs inhabitants have increased their traveldemand from an average of 1,400 to 5,500 km, using a combination of automobiles, buses, railways, and aircraft. since the world population grew nearly 2.5fold during the same period, world pkt increased by one order of magnitude,from nearly 3.6 to some 33 trillion pkt. the biggest increase in pkt, by afactor of more than 20, occurred in the developing world, where the combinedgrowth in per capita gdp and population was largest.however, the òmobility gapó between developing and industrialized regionsremains substantial. in 2000, residents in north america, the pacific organisationfor economic cooperation and development (japan, australia, and newzealand), and western europe traveled 17,000 pkt per capita on average, fivetimes as much as people in the developing world. these differences are evenlarger on a worldregional scale. residents of north america, the region with thehighest level of mobility, traveled 25,600 km per year, while people in subsaharan africa (not including south africa) traveled just 1,700 km.gdp is the most important, but not the only, determinant of perpersonpkt. as figure 3 shows, the average travel per person differs significantly atdifferent income levels, mainly because of different costs for transportation, butalso because of the size of purchasing power parity (ppp) adjustments in thesocioeconomic data set (heston et al., 2002).while the travelmoney budget translates rising per capita gdp into risingfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.longterm trends in global passenger mobility89pkt per capita, the fixed traveltime budget requires that the increasing traveldemand be satisfied in the same amount of time. since each transport modeoperates within a known range of speeds, the increasing perperson travel demand can only be satisfied by shifting toward increasingly rapid transport.figure 4 shows the continuous shifts toward faster modes of transport, fromlowspeed public transportation (the aggregate of buses and lowspeed railways),to lightduty vehicles (automobiles and personal trucks, but, for simplicity, referred to as automobiles), to highspeed modes of transportation (aircraft andhighspeed rail), again for a 51year historical time horizon. three distinct phasesof dominance by a single transport mode can be seen. lowspeed public transportation is dominant for mobility levels of up to 1,000 km/cap; lightduty vehicles between 1,000 and 10,000 km/cap; and highspeed transport modes ateven higher levels of mobility.similar to differences in total mobility, differences in travel costs and inurban landuse characteristics can lead to different levels in shares for transportmodes at a given level of pkt per capita. however, the impact of policy measures on choice of transport mode is limitedñat least on the aggregate levelsshown. in eastern europe and the former soviet union, access to private automobiles was severely restricted until the transition toward a market economy inthe early 1990s. nevertheless, the modal trajectories have evolved largely withinthe shaded envelopes.the next five decades in world travel demandif travelexpenditure shares remain approximately stable, future increases inper capita gdp will continue to cause a rise in pkt. at the same time, the fixedtraveltime budget will continue to push travelers toward faster modes of transport. the highest level of travel demand would be achieved if travelers used thefastest mode of transport for their entire daily traveltime budget 365 days ayear. assuming that aircraft gradually increase their current average òdoortodooró speed from about 260 km/h (including transfers to and from the airport) to660 km/h, the current average airporttoairport speed for domestic flights in theunited states, and a traveltime budget of 1.2 h/d for 365 d/y, the annual perperson traffic volume would result in approximately 289,000 km. at that highmobility level, most travel would be international. prices would adjust, and sowould income levels.hence, regional differences in per capita traffic volume at a given gdp percapita, mainly resulting from differences in land use and prices, would decline,and the 11 trajectories would ultimately converge into a single point in the farfuture. given historical development, it is assumed that the gdp per capita valueof that òtarget pointó would correspond to us$(2000) 289,000. (sensitivity analyses show that the exact location of the target point has almost no impact on thelevels projected for 2050.)frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.90frontiers of engineering0.00.10.20.30.40.50.60.70.80.91.0percentageofpublictransportmodesinpktcentrallyplannedasialatinamericamiddle east, north andsouth africaotherpacificasiasubsaharanafricanorthamericapacificoecdwesterneuropeeasterneuropeformersovietunionsouthasia0.00.10.20.30.40.50.60.70.80.91.0percentageoflightdutyvehiclesinpkt0.00.10.20.30.40.50.60.70.80.91.01001,00010,000100,0001,000,000pkt/cap(km)percentageofhighspeedmodesinpktcentrallyplannedasiaotherpacificasianorthamericapacificoecdwesterneuropeformersovietunioncentrallyplannedasialatinamericamiddle east, northand south africaotherpacificasiasubsaharanafricanorthamericapacificoecdwesterneuropeeasterneuropeformersovietunionsouthasiafigure 4three stages in the evolution of motorized mobility (1950ð2000): the declining share of lowspeed public transport modes (top), the growth and relative decline of theautomobile (middle), and the rise of highspeed transportation (bottom).frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.longterm trends in global passenger mobility91this imaginary world of highspeed transportation helps in projecting futurelevels of pkt by approximating each of the 11 worldregional trajectories infigure 3 by one and the same type of regression equation and by constrainingone parameter of that equation so that the simulated trajectory must pass throughthe target point.2 future levels are then determined by the predicted levels ofgdp/cap and population. the worldregional gdp per capita projections usedhere are derived from recent reference runs of the mit joint program on thescience and policy of global change systems model, after slightly reducing thegrowth rates of industrialized regions and the reforming economies of easterneurope and the former soviet union and slightly increasing those of developingcountries to match the mean values of past projections more closely(nakicenovic, 2000; paltsev et al., 2005). overall, ppp adjusted gross worldproduct per capita is projected to nearly double from us$(2000) 7,500 in 2000to us$(2000) 14,200 in 2050. in addition to the 50 percent growth in worldpopulation, as suggested by the medium variant of the united nations population projections (2004), gross world product would rise by a factor of nearlythree.based on these changes in socioeconomic conditions, the stable relationshipbetween growth in gdp and traffic volume implies that worldtravel demandwill increase approximately in proportion to the projected level of income, from33 trillion passengerkm in 2000 to 105 trillion in 2050. because of their projected higher growth in income and population, developing regions will contribute a rising share, ultimately accounting for 60 percent of world passengertraffic volume in 2050, up from about 50 percent in 2000. (higher growth ratesof gdp in developing regions will further increase their absolute and relativeimportance in traffic volume.)given fixed travel time, future shares of lowspeed public transportationmodes, lightduty vehicles, and highspeed transportation systems must remainlargely within the shaded envelopes in figure 4. (the target point conditionrequires that highspeed transportation account for the entire traffic volume in ahypothetical world where the target point can be reached.) the precise shift inshares of transport modes, necessary to satisfy the projected travel demandthrough 2050, can be derived in a number of ways, but perhaps most convincingly by estimating the parameters of the functional form of statistical consumerchoice models. however, in this application, those models would require timeseries data (ideally for 1950 to 2000) on speeds and travel costs for each transport2the general form of the regression equation is pkt/cap = á (gdp/cap) á (p), with parameters being a constant, the income elasticity, and the price elasticity. however, because longterm historical data of travel costs (p) are available for very few countries, this factor is dropped.thus, includes the averages of travelmoney budget and price of travel in a particular region.imposing the target point condition leads to an estimate of .frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.92frontiers of engineeringmode. these data can be derived for the united states and, to a limited extent,for a few european countries and japan, but they are not available for mostcountries in the world.without these data, we can perform simplified projections by determiningplausible future shares in each modal envelope at the projected level of percapita pkt, depending on whether a particular region is an early adopter or alatecomer to the diffusion of automobiles. latecomers achieve lower shares oflightduty vehicle travel, here assumed to develop along the lower boundary ofthe automobile envelope in figure 4, as they have already òleapfroggedó intohighspeed travel and thus develop along the higher boundary of the highspeedtransportation envelope in the same figure. (for a general introduction to diffuby contrast, future shares of highspeed transportation in the three industrialized regions and two reforming economies are estimated as the mean value ofthe upper and lower boundary of the envelope at the projected level of per capitagdp. the projection for the industrialized regions are then retrospectively compared to estimates from more sophisticated statisticalchoice models, for whichmore complete speed and cost data are available. for example, the estimate of adetailed statisticalchoice model for north america yields a 2050 share of 55percent for highspeed transportation, which compares to 56 percent using thesimplified approach. the use of statisticalchoice models also allows us to conduct sensitivity tests (e.g., with regard to the stability of the traveltime budget).should the traveltime budget increase from 1.2 to 1.5 hours per person per day(a 25 percent increase), the projected 2050 share of highspeed transportationwould decline from 55 percent to 44 percent (a 20 percent decline). although thedecline in the 2050 share of highspeed transportation is significant, a 44 percentshare still corresponds to three times the share for that transport mode in 2000.in the industrialized world, lightduty vehicles and highspeed transportation modes will account for nearly the entire traffic volume in 2050 and forroughly equal shares. by contrast, in reforming economies and developing regions, automobiles will supply most of the pkt, followed by lowspeed publictransportation. in both metaregions, however, highspeed transportation is alsoon the rise, accounting for nearly 20 percent of the 2050 passengertraffic volume. globally, the traffic shares of automobiles and lowspeed public transportmodes will decline by about 6 and 12 percentage points, respectively, below the2000 level by 2050. at the same time, the relative importance of highspeedmodes will increase from 10 percent to about 30 percent. figure 5 summarizesthe global development in pkt by major mode of transport for 1950, 2000, and2050 (projected).the simplified model necessarily has a number of limitations. perhaps mostimportant, the mode shifts in figure 4 represent only the aggregate of two markets and do not capture substitutions in the urban and intercity transport segfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.longterm trends in global passenger mobility93lightdutyvehicleslowspeedpublictransportationhighspeedtransportation1950200020503.6billionpkt33.3billionpkt106billionpktfigure 5global passengerkm traveled, by major mode of transport, in 1950, 2000,and 2050 (projected). size of pies corresponds to pkt, which has been multiplied bynearly 10 times through 2000 and is likely to be multiplied by a factor of 30 by 2050. forcomparison, gdp has grown by factors of 7 and 20, respectively.ments. in the urban transportation segment, lightduty vehicles become moreimportant to the cost of lowspeed public transportation. by contrast, in intercitytransport, automobiles are displaced by highspeed transportation modes. bydisaggregating the data set into these two transportation markets and estimating(the functional form of) a nested, discretechoice model, we could project plausible levels of shares for transport modes over time periods of more than 50years. (whether these projections will ultimately be achieved in reality is a different subject, which raises questions about whether such models are more valuable as tools for understanding interactions between humans and technologythan for making exact predictions of the future.)another limitation is that the projection of future passenger mobility wasperformed in an unconstrained world. however, a separate analysis of potentially limiting factors, including the resource base of oil products, the need forhigher aircraft speeds, the potential substitution of travel by telecommunication,increasing travel congestion, and so on, suggests that none of these constraints islikely to be binding in the next five decades. at some point in the future, however, the finite characteristics of our planet will necessarily have an impact ontransportation systems.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.94frontiers of engineeringimplicationsthe growth in travel demand and the shift toward faster transport modeshave a number of implications. two of the most important are for the amount oftravel time spent in different transport modes and the impact on energy use andgreenhousegas emissions.figure 6 shows the daily perperson travel distance by mode of transport(left) and the associated daily travel time (right) for 1950, 2000, and 2050 (projected) in north america (essentially the united states and canada). over thepast 50 years, the daily travel distance has more than doubled, from 30 km to 70km, while perperson travel time has likely remained stable (no timeuse data areavailable for 1950). over the next five decades, based on our projection of percapita gdp, daily mobility will double again, to 140 km, with highspeed transportation accounting for 56 percent. however, despite the growing demand forhighspeed transport, travelers will continue to spend most of their travel time onthe road. although automobile travel time will decrease only slightly, the mainchange in traveltime allocation will be a net substitution of highspeed transportation for lowspeed public transportation. a traveler in 2050 will spend an average of 12 minutes per day in the air or on highspeed railways (compared to twominutes today). if the perperson traveltime budget increases to 1.5 hours perday, the average daily highspeed travel time will decrease to about 9 minutes.although total travel time may not be affected by the increase in traveldemand, energy use and greenhousegas emissions will change. all factors being020406080100120140195020002050195020002050highspeedtransportpublictransportlightdutyvehiclesnonmotorizedtransportpkt/cap/day(km)dailytraveltime(hoursperperson)0.00.20.40.60.81.01.21.4figure 6daily perperson travel distance by mode of transport (left) and associateddaily travel time (right) for 1950, 2000, and 2050 (projected) in north america.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.longterm trends in global passenger mobility95equal, rising travel demand will cause a proportional increase in passengertravelenergy use. given that (synthetic) oil products are likely to continue dominatingthe future fuel supply of the transportation system, over the next five decades,directly released greenhousegas emissions will also rise, roughly in proportionto travel demand. (the increase in lifecycle greenhousegas emissions could begreater if there is a significant shift toward unconventional oil, such as extraheavy oil, oil sands, shale oil, or synthetic oil from natural gas or, especially,coal.) changes in passengertravel energy intensity (i.e., energy use per pkt)will also influence levels of passengertravel energy use and greenhousegasemissions.in the absence of more fuelefficient transport modes, three trends will determine future levels of energy intensity. first, any increase in travel speed willcause an increase in energy intensity. based on current and average u.s. data, acomplete shift from lowspeed public transportation to lightduty vehicles inurban travel would increase energy intensity by 25 percent. for intercity travel, acomplete shift from lowspeed public transportation to lightduty vehicles wouldincrease energy intensity by almost 60 percent. a shift toward air travel wouldincrease it by another 40 percent. in the united states, most of these changeshave already taken place. if the ongoing shift from automobile intercity traveltoward aircraft continues, intercity passengertravel energy intensity will increaseby 15 to 20 percent by 2050.second, the change to air travel for intercity transport will also increase therelative importance of urban automobile travel, which is more energy intensethan intercity automobile travel because of varying engine loads and low occupancy rates. thus, this shift will cause an increase in average automobiletravelenergy intensity. however, even though the energy intensity of urban travel istwice that of intercity travel, it will probably only increase 10 percent or less,because nearly all pkt by automobiles already occurs over relatively short distances.third, the substitution of air transportation for intercity automobile travelmainly occurs at trip distances of less than 1,000 km, distances at which aircraftenergy use is mainly determined by the energyintensive takeoff and climb stages.aircraft energy intensity at such stage lengths can be twice as high as for trips ofmore than 1,000 km (babikian et al., 2002).in north america, the strong growth in air travel suggests that the combinedeffect of these three trends is determined mainly by the change in aircraft energyintensity resulting from the relative growth in different market segments. however, because the average energy intensity of total air travel is lower than forautomobiles and lowspeed public transport modes operating in urban areas, theoverall effect of these changes is likely to be negligible. thus, total energy useand greenhousegas emissions will rise roughly in proportion to the growth inpkt (i.e., by 2050, 130 percent over the 2000 level, based on our assumptions ofgdp growth). in western europe, the combination of these trends may result infrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.96frontiers of engineeringpassengertravel energy intensity rising by 2050 by as much as 20 percent abovethe level for 2000.the situation is fundamentally different, especially in developing countries,where the substitution of automobiles for lowspeed public transportation is justbeginning. combined with a future decline in vehicle occupancy rates (mainly aconsequence of increasing female participation in the labor force), the impact ofthese trends on passengertravel energy intensity may be 50 to 100 percent.compensating for this increase in energy intensity in developed countries already requires sophisticated fuelsaving technology.in the passengertransport sector, air travel accounts for the fastest growth inenergy use and greenhousegas emissions. in addition to the projected ninefoldincrease in global airtravel demand, future levels of airtravel greenhousegasemissions will depend on which technologies are used, assuming that (synthetic)oil products continue to fuel air transportation. table 1 shows the major opportunities for reductions in aircraft energy use for a given travel demand based onrecent studies. even if aircraft energy use is reduced by 33 to 56 percent by2050, the 2000 level of carbon dioxide emissions would still be multiplied by afactor of four to six. thus, controlling greenhousegas emissions from transportation will remain a significant challenge for generations to come.referencesbabikian, r., s.p. lukachko, and i.a. waitz. 2002. historical fuel efficiency characteristics of regional aircraft from technological, operational, and cost perspectives. journal of air transportmanagement 8(6): 389ð400.heston, a., r. summers, and b. aten. 2002. penn world table version 6.1, center for internationalcomparisons at the university of pennsylvania (cicup), october 2002. available online at:http://pwt.econ.upenn.edu/phpsite/pwtindex.php.jamin, s., a. sch−fer, m.e. benakiva, and i.a. waitz. 2004. aviation emissions and abatementpolicies in the united states: a city pair analysis. transportation research d 9(4): 294ð314.table 1 projected percentage change in aircraft energy use by 2050low estimatehigh estimateaircraft technologyð25ð45pax load factorð10ð10direct flights0ð11shift to highspeed railð1ð1totalsð33ð56note: estimates for highspeed rail are based on 50 percent market share in 10 u.s. highdensitycorridors, with a cumulative greatcircle distance of 16,700 km. sources: adapted from lee et al.,2001; jamin et al., 2004.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.longterm trends in global passenger mobility97lee, j.j., s.p. lukachko, i.a. waitz, and a. sch−fer. 2001. historical and future trends in aircraftperformance, cost, and emissions. annual review of energy and the environment 26: 167ð200.nakicenovic, n., ed. 2000. special report on emissions scenarios. intergovernmental panel onclimate change. cambridge, u.k.: cambridge university press.paltsev, s., j.m. reilly, h.d. jacoby, r.s. eckaus, j. mcfarland, m. sarofim, m. asadoorian, andm. babiker. 2005. the mit emissions prediction and policy analysis (eppa) model: version4. mit joint program on the science and policy of global change, report 125. cambridge,mass.: mit press. available online at: http://web.mit.edu/globalchange/www/reports.html#pubs.sch−fer, a. 1998. the global demand for motorized mobility. transportation research a 32(6): 455ð477.sch−fer, a., and d.g. victor. 2000. the future mobility of the world population. transportationresearch a 34(3): 171ð205.united nations. 2004. world population prospects: the 2004 revision population database. unitednations population division. available online at: http://esa.un.org/unpp/.zahavi, y. 1981. the umoturban interactions. dotrspadpb 10/7. washington, d.c.: u.s.department of transportation.further readingmarchetti, c. 1994. anthropological invariants in travel behavior. technological forecasting andsocial change 47: 75ð88.zahavi, y., and a. talvitie. 1980. regularities in travel time and money expenditures. transportation research record 750: 13ð19.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.99energy and environmental impacts ofpersonal mobilitymatthew j. barthuniversity of california, riversidepersonal mobility is critical for a progressive society. the freedom to travelanywhere at anytime with few restrictions is a precondition for a vibranteconomy. however, mobility is often restricted by limitations in the transportation infrastructure. for example, when many people use the infrastructure at thesame time, congestion invariably occurs. one can look at this phenomenon as aresourcemanagement problem. if resources (i.e., the transportation infrastructure) are limited and demand is high, congestion is likely to occur. two ways ofsolving this problem are (1) by providing additional resources and/or (2) reducing demand.in the united states, the transportation system has primarily been developedaround the automobile, and the majority of personal trips are made by drivingcars to various destinations. statistics show that only a small percentage of theu.s. travel demand is satisfied by public transit. instead, we have invested billions of dollars into building a large network of roadways that allow people todrive automobiles almost anywhere. since the major buildup of roads from the1950s through the 1990s, it has become significantly more difficult to constructnew roadways because of higher population densities and subsequent landuserestrictions. instead, transportation officials are now investigating intelligenttransportation systems and other means to increase the capacity of existing roadways through computer, communications, and control technologies (dot, 2001).the theory is that by improving overall capacity, congestion on the roadwayswould be reduced.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.100frontiers of engineeringnevertheless, studies have shown that roadway congestion continues to getworse. for example, the texas transportation institute (tti) conducts an annual mobility study that includes estimates of traffic congestion in many largecities and the impact on society (schrank and lomax, 2005). the study definescongestion as òslow speeds caused by heavy traffic and/or narrow roadways dueto construction, incidents, or too few lanes for the demand.ó because trafficvolume has increased faster than road capacity, congestion has gotten progressively worse, despite the push toward alternative modes of transportation, newtechnologies, innovative landuse patterns, and demandmanagement techniques.some of the major concerns raised by roadway congestion are impacts onenergy consumption and air quality. the tti annual mobility study estimatesthat billions of gallons of fuel are wasted every year because of congestion(schrank and lomax, 2005). in addition, heavy congestion often leads to greatermobilesource emissions. one way to estimate the energy and emissions impactsof congestion is to examine velocity patterns of vehicles operating under different levels of congestion. roadway congestion is often categorized by the òlevelof serviceó (los) (trb, 1994). for freeways (i.e., uninterrupted flow), los canbe represented as a ratio of traffic flow to roadway capacity. there are severaldifferent los values that are represented by the letters a through f. for eachlos, a typical vehiclevelocity trajectory will have different characteristics.examples of these velocity trajectories are shown in figure 1 (epa, 1997).under los a, vehicles typically travel near the highwayõs freeflow speed, withfew acceleration/deceleration perturbations. as los conditions get progressivelyworse (i.e., los b, c, d, e, and f), vehicles travel at lower average speeds withmore acceleration/deceleration events. for each representative vehiclevelocitytrajectory (such as those shown in figure 1), it is possible to estimate both fuelconsumption and pollutant emissions. for automobiles, we are most often concerned about emissions of carbon monoxide (co), hydrocarbons (hcs), oxidesof nitrogen (nox), and particulate matter.figure 2 shows examples of automobile fuel consumption and emissionrates that correspond to the average speeds of the representative velocity trajectories shown in figure 1. fuel consumption and emission rates are normalizedby distance traveled, given in units of grams per unit mile. as expected, whenspeeds are very low, vehicles do not travel very far; therefore, grams per mileemission rates are quite high. in fact, when a car is not moving, we get aninfinitedistance normalized emission rate. conversely, when vehicles travel athigher speeds, they experience higher engine load requirements and, therefore,have higher fuel consumption and emission rates. as a result, this type of speedbased emissionfactor curve has a distinctive parabolic shape, with high emission rates on both ends and a minimum rate at moderate speeds of around 45 to50 mph.figure 2 shows a fuelconsumption and emissions curve for a vehicle (anaverage òcompositeó vehicle representing the 2005 vehicle fleet in southern califrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.energy and environmental impacts of personal mobility10103060901200100200300400500600time (second)speed (km/h)los dlos e03060901200100200300400500600time (second)speed (km/h)los flos f03060901200100200300400500600time (second)speed (km/h)los a+los acfigure 1sample vehiclevelocity trajectories for different los on a freeway (based onepa facilitycycle development). source: epa, 1997.fornia) traveling at a perfectly constant, steadystate speed. of course, vehiclesmoving in traffic must do some amount of òstopandgoó driving, and the associated accelerations and decelerations lead to higher fuel consumption and emissions. the constant, steadystate speed line in figure 2 shows the lower bound offuel consumption and emissions for any vehicle traveling at that speed.several important results can be derived from this information:¥in general, whenever congestion brings the average vehicle speed below45 mph (for a freeway scenario), there is a negative net impact on fuel consumption and emissions. vehicles spend more time on the road, which results in lowerfuel economy and higher total emissions. therefore, in this scenario, reducingcongestion will improve fuel consumption and reduce emissions.¥if congestion brings average speeds down from a freeflow speed of aboutfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.102frontiers of engineering0501001502002503000102030405060708090speed (mphfuel (gram/mile)0.000.050.100.150.200.250.300.350.400102030405060708090speed (mph)hc (gram/mile)congestion driving patternssteadystate driving patternssteadystate driving patterns0.000.100.200.300.400.500.600.700.800102030405060708090speed (mph)nox (gram/mile)congestion driving patternssteadystate driving patterns01234567890102030405060708090speed (mph)co (gram/mile)congestion driving patternscongestion driving patternssteadystate driving patternsfigure 2fuel consumption and emissions based on average speed for typical passenger vehicles. a. fuel consumption. b. carbon monoxide. c. hydrocarbons. d. oxides ofnitrogen.abcdfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.energy and environmental impacts of personal mobility10365 mph to a slower speed of 45 to 50 mph, congestion can actually improve fuelconsumption and lower emissions. if relieving congestion increases average traffic speed to the freeflow level, fuel consumption and emissions levels will goup.¥if the realworld, stopandgo velocity pattern of vehicles could somehow be smoothed out so an average speed could be maintained, significant fuelconsumption savings and emissions reductions could be achieved.a similar analysis can be performed for roadway travel on arterials andresidential roads (i.e., interrupted flow patterns). these analyses are a bit morecomplicated, but they too show that any measure that keeps traffic flowingsmoothly for longer periods of time (e.g., operational measures, such as synchronization of traffic signals) improves overall fuel economy and lowers emissions.it is important to note that fuel/emissions congestion effects are much morepronounced for heavyduty trucks, which tend to have much lower powertoweight ratios than cars.three general areas can be addressed for decreasing congestion and improving mobility and accessibility: supply, demand, and land use. supply management techniques include adding resources and capacity to the transportation infrastructure. examples include building additional roads and adding lanes toexisting roads to increase roadway capacity; building bike paths or lanes andwalkways to promote these alternative modes of transportation; improving transit facilities and services, as well as intermodal facilities and services, to encourage people to use mass transit more often; improving overall system operations(e.g., responding quickly to roadway incidents); and implementing intelligenttransportation system techniques to improve travel efficiency.demand management could involve pricing mechanisms to limit the use ofresources; providing a much greater range of alternative modes of transportation;encouraging alternative work locations and flexible work schedules; and encouraging or even requiring employers to provide travelsupport programs. landusemanagement would require better urban design, mixeduse land development,increased housing and industry density, innovative planning and zoning, andgrowth management.it is important that these different areas should be addressed together, ratherthan separately. if supply alone is increased, this will likely induce additionaldemand. on the other hand, providing demand management without increasingsupply could limit economic growth. therefore, the best way to approach theproblem of traffic congestion is to address all three areas together.within these three general areas are several specific programs that can reduce congestion and also help reduce energy consumption and emissions. thefollowing list is not exhaustive, but it provides examples of some things that arebeing done today and some that could be done in the future.intelligent speed adaptation (isa) typically consists of an onboard systemfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.104frontiers of engineeringthat monitors the location and speed of a vehicle, compares it to a defined setspeed, and takes corrective action, such as advising the driver and/or limiting thetop speed of the vehicle. researchers in europe are actively investigating isasystems and are currently evaluating their effects on safety, congestion, andenvironmental impacts (servin et al., 2006).carsharing is a new mobility strategy that offers an alternative to individualvehicle ownership by providing a fleet of vehicles that can be shared throughoutthe day by different users. carsharing improves overall transportation efficiencyby reducing the number of vehicles required to meet total travel demand (e.g.,barth and shaheen, 2002).public transit is seldom used in the united states because it has typicallybeen inflexible and unreliable. however, new enhanced transit systems, suchas bus rapid transit (brt) and other systems that provide intermodal linkagesof standard transit routes, are becoming available (levinson et al., 2003).smart parking is a strategy that can lead to significant savings in fuel consumption and reductions in emissions. smart parking uses advanced technologies to direct drivers to available parking spaces at transit stations (and otherlocations). this encourages the use of mass transit, reduces driver frustration,and reduces congestion on highways and arterial streets (rodier et al., 2005).transitoriented developments (tods) promote the use of mass transit byintegrating multiple transit options in highdensity developments that includeresidential, commercial, and retail areas. tods have been demonstrated to increase the use of mass transit and pedestrian traffic and reduce the use of privatevehicles (cervero et al., 2004).innovative modes of transportation can be used in addition to automobilesto satisfy travel demand. new travel modes can include the segwayª humantransporter, electric bicycles, and neighborhood electric vehicles.summaryroadway congestion and associated environmental impacts will continue toget worse unless a number of alternatives are introduced. although a certainamount of congestion can have a positive impact on fuel consumption and vehicle emissions by slowing traffic, severe congestion has the opposite effect. anumber of transportation innovations can be implemented to improve overallpersonal mobility with minimal energy and environmental impacts.referencesbarth, m., and s. shaheen. 2002. shareduse vehicle systems: a framework for classifying carsharing, station cars, and combined approaches. transportation research record no. 1791: 105ð112.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.energy and environmental impacts of personal mobility105cervero, r., r. arrington, j. smithheimer, and r. dunphy. 2004. transitoriented development inthe united states: experiences, challenges, and prospects. washington, d.c.: transit cooperative research program.dot (u.s. department of transportation). 2001. the national its architecture: a framework forintegrated transportation into the 21st century. cd book from u.s. department of transportation, its joint program office. available online at: www.itsa.org/public/archdocs/national.html.epa (environmental protection agency). 1997. development of speed correction cycles. technical document #m6.spd.001, june 1997, prepared by sierra research. washington, d.c.: epa.levinson, h., s. zinnerman, j. clinger, s. rutherford, and r. smith. 2003. bus rapid transitsystems. transit cooperative research programs report #90, transportation research board,the national academies. washington, d.c.: transportation research board.rodier, c., s. shaheen, and a. eaken. 2005. transitbased smart parking in the san francisco bayarea, california: assessment of user demand and behavioral effects. transportation researchrecord 1927: 167ð173.schrank, d., and t. lomax. 2005. the 2005 urban mobility report. texas transportation institute,texas a&m university system. available online at: http://mobility/tamu.edu.servin, o., k. boriboonsomsin, and m. barth. 2006. an energy and emissions impact evaluation ofintelligent speed adaptation. pp. 1257ð1262 in proceedings of the 2006 ieee intelligent transportation systems conference, toronto, canada. piscataway, n.j.: ieee.trb (transportation research board). 1994. highway capacity manual. special report 209. washington, d.c.: national academy press.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.107new mobility: the next generation ofsustainable urban transportationsusan zielinskiuniversity of michiganann arbor, michiganin a classic 1950s photograph, a scientificlooking man in a light suit isdwarfed by a mammoth mainframe computer heõs programming. it is unlikelythat the idea of a ònanopodó would have entered his mind, let alone mesh networking, gis, or ògoogling.ó he wouldnõt have conceived of the connectivitythat a mere halfcentury later has brought these elements together, transformedthe world, and evolved into one of the fastest growing, most pervasive globalindustries.today, we are on the cusp of a comparable transformation for cities callednew mobility. accelerated by the emergence of new fuel and vehicle technologies; new information technologies; flexible and differentiated transportationmodes, services, and products; innovative land use and urban design; and newbusiness models, collaborative partnerships are being initiated in a variety ofways to address the growing challenges of urban transportation and to provide abasis for a vital new mobility industry (mte and icf, 2002).connectivityan early and very successful example of integrated innovation in new mobility is the hong kong octopus system, which links multiple transit services,ferries, parking, service stations, access control, and retail outlets and rewardsvia an affordable, contactless, storedvalue smart card. the entire system is defrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.108frontiers of engineeringsigned and engineered to support seamless, sustainable doortodoor trips (octopus, 2006).a more recent innovation, referred to as new mobility hub networks, beganin bremen, germany, and is evolving and spreading to a number of other european cities, as well as to toronto, canada (figure 1). new mobility hubs connect a variety of sustainable modes of transportation and services through anetwork of physical locations or òmobile pointsó throughout a city or region,physically and electronically linking the elements necessary for a seamless, integrated, sustainable doortodoor urban trip (mte, 2004). hubs are practical forcities in the developed or developing world because they can be customized to fitlocal needs, resources, and aspirations. hubs can link and support a variety ofdiverse elements:figure 1the new mobility hub concept. source: mte, 2004.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.new mobility109¥multiple transportation operators, modes, and services¥taxis and carsharing of a variety of vehicle types and sizes¥òsluggingó (sluglines.com, 2006)¥free or feeforuse bicycle sharing (bikeshare/cbn, 2006)¥walkable, bikable, and transitoriented spatial design and development(kelbaugh, 1997)¥cafes and meeting places¥wifi amenities¥electronic farepayment options and pricing mechanisms for all transportation modes and services¥satelliteenhanced, realtime, urban traveler information for all modes oftransportation provided at onstreet kiosks and by pda.factors driving the development of new mobilitythe evolution of new mobility is inspired by emerging innovations andpropelled by pressing needs, not the least of which is rapid urbanization. although a few cities are shrinking, especially in the developed world, by 2030more than 60 percent of the world population and more than 80 percent of thenorth american population will live in urban regions (un, 1996). with increasing motorization, traffic volume and congestion are already resulting in lostproductivity and competitiveness, as well as health and other costs related tosmog, poor air quality, traffic accidents, noise, and, more recently, climate change(wbcsd, 2001).at the same time, sprawling, carbased, urbandevelopment patterns canmean either isolation or chauffeur dependence for rapidly aging populations, aswell as for children, youths, and the disabled (aarp, 2005; hillman and adams,1995; oõbrien, 2001; wbcsd, 2001). in developing nations, aspirations towardprogress and status often translate into car ownership, even as the risks and costsof securing the energy to fuel these aspirations rise (gakenheimer, 1999; sperlingand clausen, 2002; wbcsd, 2001).engineering for new mobilitythe factors described above have created not only compelling challengesfor engineering, but also opportunities for social and business innovation. newmobility solution building is supported by new ways of thinking about sustainable urban transportation, as well as emerging tools and approaches for understanding, implementation, and commercialization. in this article, i focus on threefrontiers of thinking and practice for new mobility: complexity, accessibility,and new business models.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.110frontiers of engineeringcomplexitytools for understandinga variety of tools and approaches have been developed to support the analysis and modeling of complex urban transportation systems. at least three typesof complementary systems analysis (topdown, bottomup, and simulations) canbe applied to transportation and accessibility. topdown analyses generally startwith selfgenerated variables or hypotheses and develop a causalloop diagramusing software that highlights patterns, dynamics, and possible interventionpoints. once a basic analysis is complete, more indepth data gathering andmodeling can be done. some of the most extensive transportationrelated workof this kind has been undertaken by professor joseph sussman at mit (dodderet al., 2002; sussman, 2002; sussman and hall, 2004). figure 2 shows a passengertransportation subsystem for mexico city.bottomup, or agentbased, models are computerbased models that use empirical and theoretical data to represent interactions among a range of components, environments, and processes in a system, revealing their influence on theoverall behavior of the system (axelrod and cohen, 2000; miller and roorda,2006; miller and salvini, 2005; zellner et al., 2003). ethnographic research canalso be applied to transportation as a bottomup research tool. by giving subjectsdocumentation tools (e.g., cameras) over a fixed period of time, patterns of behavior can be observed without interference by researchers.simulations and scenariobuilding software can draw from and build uponboth topdown and bottomup analyses. simulations graphically depict and manipulate transportation and other urban dynamics to inform decision making andidentify opportunities for innovation. metroquest (2006) is a good example ofan effective urbantransportation simulation tool.sophisticated solution buildingcomplex transportation challenges call for sophisticated solutions. òsinglefixó approaches (e.g., alternative fuels alone, pricing mechanisms alone, orpolicy changes alone) cannot address the serious urban challenges and conditions noted above. informed by complex systems analysis, systemsbased solution building involves òconnecting the dots,ó that is, enhancing or transformingexisting conditions with customized, integrated innovations in products, services, technologies, financing, social conditions, marketing, and policies andregulations (ecmt, 2006; mte and icf, 2002; newman and kenworthy,1999). sophisticated solution building usually involves multisector interdisciplinary collaboration.a good example of systemsbased solution building is the new mobilityhub network described above. hub networks can catalyze engineering and busifrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.new mobility111electricpowerpolicyquality of lifemacroeconomicfactorsgdpper capita aggregatetransportationdemandhumanhealthautoownershipeconomicdevelopmentmodechoiceinvestmentforeignprivateproductivitymetroshareenvironmentprivateauto sharebus/taxi/colectivosharefleetfueli&mcongestionfleetfueli&mlanduseinvestmentpolicypopulationtransportationinvestmentfigure 2part of a larger analysis showing a passengertransportation subsystem formexico city. source: dodder et al., 2002.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.112frontiers of engineeringness opportunities related not only to the design and implementation of individual product and service innovations, but also to the engineering of physicaland digital connections between them.accessibilityover the past 50 years, measures of regional and economic success havebecome increasingly linked to (motorized) mobility and speed of travel (tti,2005). this association originated in the west and has been widely adopted incities of the developing world. however, transportation is only a means to anend, or a derived demand, so measures and applications of accessibility do notfocus on how fast or how far one can travel in a certain period of time. instead,they focus on how much can be accomplished in a given time frame and budgetor how well needs can be met with available resources. for example, on a typicalday in los angeles, you may drive long distances at high speeds to fit in threemeetings. in bremen, germany, a more accessible place, you may be able to fitin five meetings and a leisurely lunch, covering only half the distance at half thespeed and for half the price (levine and garb, 2002; thomson, 1977; zielinski,1995).accessibility can be achieved in at least three ways: wise land use anddesign, telecommunication technologies that reduce the need for travel, andseamless multimodal transportation. among other benefits, connected accessibility options can help address the demographic, equity, and affordability needsof seniors, children, the poor, and the disabled. at the same time, integratedaccessibility can help build more adaptable and resilient networks to meet thechallenges of climate change and emergency situations in cities. dynamic andflexible accessibility and communications systems can support quick responsesto unforeseen urban events.the university of michiganõs smart/carss project (2006) is currentlydeveloping an accessibility index to compare and rate accessibility in metropolitan regions as a basis for urban policy reform and innovation (see box 1).new business modelsin a 2002 study by moving the economy, the current value and future potential of new mobility markets were measured in billions of dollars (mte andicf, 2002). new mobility innovations and opportunities go beyond the sectoralbounds of the traditional transportation industry. they encompass aspects oftelecommunications; wireless technologies; geomatics; ebusiness and new media; tourism and retail; the movement of goods; supply chain management(zielinski and miller, 2004); the design of products, services, and technologies;real estate development; financial services; and more.new mobility innovations not only improve local competitiveness and qualfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.new mobility113box 1university of michigan smart/carss projectsmart (sustainable mobility and accessibility research and transformation), aninterdisciplinary initiative at the university of michigan in ann arbor, is grounded incomplexity theory and practice. the goal of the project is to move beyond purelytechnical and mobilitybased approaches to urban transportation to address challenges and opportunities raised by the complex interactions of social, economic,environmental, and policy factors. a project of carrs (center for advancing research and solutions for society), smart brings together experts on issues, theoretical approaches, and practical and policy applications to tackle the complexity,sophistication, impacts, and opportunities related to urban transportation and accessibility, particularly for growing urban populations worldwide. smart workscollaboratively across disciplines and sectors to:¥catalyze systemic and fundamental transformations of urban mobility/accessibility systems that are consistent with a sustainable human future¥harness emerging science on complex adaptive systems to meet futuremobility and accessibility needs in an ecologically and socially sustainable wayand identify òtipping pointsó to guide the evolution of such systems¥inform and develop integrated new mobility innovation and businessmodels¥provide diverse academic opportunities related to sustainable urban mobility and accessibility¥contribute to a growing multidisciplinary, multistakeholder, global network ofapplied learning in sustainable mobility and accessibility.ity of life (litman and laube, 2002; newman and kenworthy, 1999), they alsoprovide promising export and economic development opportunities for both mature and òbaseofthepyramidó markets (hart, 2005; prahalad, 2004). becauseurban transportation represents an increasingly urgent challenge worldwide, andbecause urban mobility and accessibility solutions can, in most cases, be adaptedand transferred, regions, nations, and enterprises that support new mobility (supplyside) innovation, as well as industry clustering and the development of newbusiness models, stand to gain significantly from transportation export marketsin the coming years (mte and icf, 2002).engineering and beyondnew mobility has the potential to revitalize cities and economies worldwideand can open up a wealth of engineering and business opportunities. but obstacles will have to be overcome, not all of them related to engineering. forexample, increased motorization and the high social status it represents in develfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.114frontiers of engineeringoping countries, along with seemingly unstoppable urban sprawl in the west, arechallenges that must be addressed on psychological and cultural levels, as wellas infrastructural and economic levels. progress toward a positive, integrated,and sustainable future for urban transportation will require more than movingpeople and goods. it will also involve the complex task of moving hearts andminds.acknowledgmentsthomas gladwin and jonathan levine, university of michigan, and moirazellner, university of illinois, chicago (all members of smart/carss), madehelpful contributions to this paper. background research was provided bysathyanarayanan jayagopi, a student in the masterõs program, university ofmichigan institute for global sustainable enterprise.referencesaarp (association for the advancement of retired people). 2005. universal village: livable communities in the 21st century. available online at: http://www.aarp.org/globalaging.axelrod, r., and r. cohen. 2000. harnessing complexity: organizational implications of a scientific frontier. new york: basic books.bikeshare/cbn (community bicycle network). 2006. available online at: http://communitybicyclenetwork.org/index.php?q=bikeshare.dodder, r., j. sussman, and j. mcconnell. 2002. the concept of clios analysis: illustrated by themexico city case. working paper series. cambridge, mass.: engineering systems division,mit. available online at: http://www.google.com/search?hl=en&q=sussman+%26+ dodder+the+concept+of+clios+analysis&btng=google+search.ecmt (european conference of transport ministers). 2006. implementing sustainable urban travel policies: applying the 2001 key messages. council of ministers of transport, dublin, may17ð18. available online at: http://www.cemt.org/council/2006/cm200603fe.pdf.gakenheimer, r. 1999. urban mobility in the developing world. transportation research part a(33): 671ð689.hart, s. 2005. capitalism at the crossroads: the unlimited business opportunities in solving theworldõs most difficult problems. philadelphia, pa.: wharton school publishing.hillman, m., and j. adams. 1995. childrenõs freedom and safety. pp. 141ð151 in beyond the car:essays in auto culture, edited by s. zielinski and g. laird. toronto: steel rail publishing.kelbaugh, d. 1997. common place: toward neighbourhood and regional design. seattle: university of washington press.levine, j., and y. garb. 2002. congestion pricingõs conditional promise: promotion of accessibilityor mobility. transportation policy 9(3): 179ð188.litman, t., and laube, f. 2002. automobile dependency and economic development. availableonline at: http://www.vtpi.org/ecodev.pdf.metroquest. 2006. available online at: http://www.envisiontools.com.miller, e.j., and m.j. roorda. 2006. prototype model of household activity travel scheduling.transportation research record 1831, paper 03.3272. washington, d.c.: transportation research board of the national academies.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.new mobility115miller, e.j., and p. salvini. 2005. ilute: an operational prototype of a comprehensive microsimulation model of urban systems. pp. 217ð234 in networks and spatial economics 5. thenetherlands: springer science and business media, inc.mte (moving the economy). 2004. bremen and toronto new mobility hub case studies and dayin the life scenario. available online at: http://www.movingtheeconomy.ca/content/cspdf/bremenvideosummaryaug2.pdf and http://www.movingtheeconomy.ca/content/mtehubabout.html and http://www.movingtheeconomy.ca/content/ditl.html.mte and icf (icf consulting). 2002. building a new mobility industry cluster in the torontoregion. available online at: http://www.movingtheeconomy.ca.newman, p., and p. kenworthy. 1999. sustainability and cities. washington, d.c.: island press.oõbrien, c. 2001. trips to school: childrenõs experiences and aspirations. york centre for appliedsustainability. available online at: http://plasma.ycas.yorku.ca/documents/ontariowalkabilitystudyrep.pdf.octopus. 2006. available online at: http://lnweb18.worldbank.org/external/lac/lac.nsf/sectors/transport/d5a576a039a802c0852568b2007988ad?opendocument and http://en.wikipedia.org/wiki/octopuscard.prahalad, c.k. 2004. fortune at the bottom of the pyramid. philadelphia, pa.: wharton schoolpublishing.sluglines.com. 2006. available online at: http: //www.sluglines.com/slugging/aboutslugging.asp.smart/carss. 2006. available online at: http://www.isr.umich.edu/carss.sperling, d., and e. clausen. 2002. the developing worldõs motorization challenge. availableonline at: http://www.issues.org/19.1/sperling.htm.sussman, j.m. 2002. collected views on complexity in systems. pp. 1ð25 in proceedings of theengineering systems division internal symposium. cambridge, mass.: engineering systemsdivision, mit.sussman, j.m., and r.p. hall. 2004. sustainable transportation: a strategy for systems change.working paper series. cambridge, mass.: engineering systems division, mit.thomson, j.m. 1977. great cities and their traffic. london: peregrine.tti (texas transportation institute). 2005. urban mobility report: 2005. available online at: http://tti.tamu.edu/documents/mobilityreport2005wappx.pdf.un (united nations). 1996. urban and rural areas. department of economic and social affairs,population division. available online at: http://www.un.org/esa/population/pubsarchive/ura/uracht1.htm.wbcsd (world business council on sustainable development). 2001. mobility 2001: world mobility at the end of the twentieth century and its sustainability. available online at: http://www.wbcsd.org/web/projects/mobility/englishfullreport.pdf.zellner, m., r. riolo, w. rand, s.e. page, d.g. brown, and l.e. fernandez. 2003. interactionbetween zoning regulations and residential preferences as a driver of urban form. availableonline at: http://www.caup.umich.edu/acadpgm/urp/utesymposium/publication/zellner.pdf.zielinski, s. 1995. access over excess. pp. 131ð155 in change of plans, edited by m. eichler.toronto: garamond press.zielinski, s., and g. miller. 2004. integration technologies for sustainable urban goods movement.moving the economy and canadian urban institute. available online at: http://www.tc.gc.ca/pol/en/report/urbangoods/report.htm.bibliographyburwell, d., and t. litman. 2003. issues in sustainable transportation. available online at: http://vtpi.org/susiss.pdf.jacobs, j. 1985. cities and the wealth of nations: principles of economic life. new york: randomhouse.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.116frontiers of engineeringkennedy, c., e.j. miller, a. shalaby, h. maclean, and j. coleman. 2005. the four pillars of sustainable urban transportation. transport reviews 25(4): 393ð414.levine, j. 1998. rethinking accessibility and jobshousing balance. journal of the american planning association 64: 133ð150.levine, j. 2005. zoned out: regulation, markets, and choices in transportation and metropolitanland use. washington, d.c.: resources for the future press.sterman, j. 2000. business dynamics: systems thinking and modeling for a complex world. newyork: irwin/mcgraw hill.sussman, j. 2000. introduction to transportation studies. boston: artech house.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.supply chain management applicationswith economic and public impactfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.119introductionjennifer k. ryanuniversity college dublindublin, irelandjulie l. swanngeorgia institute of technologyatlanta, georgiaa supply chain is a network that includes all facilities, materials, and activities necessary to bring a product or service to the enduser, or consumer. supplychain management (scm) is the process of planning, implementing, and controlling the operations of a supply chain, with the goal of satisfying customer requirements as efficiently as possible. for large multinational corporations thatmanufacture complex products, such as automobiles, electronics, or aircraft, supply chains are highly complex systems, and the management of these systems isa largescale problem involving many interrelated components, including facilitylocation and network design, production planning and scheduling, inventory control, transportation and vehicle routing, information systems, and so on.scm is further complicated because most supply chains operate in highlyvariable and uncertain environments with facilities or stages in the supply chainthat may be independently owned and/or operated. because of these complexities, scm relies heavily on methods developed by operations research, such asoptimization and stochastic processes, as well as an understanding of engineering, economic, and business processes.over the past two decades, effective scm has become a significant sourceof competitive advantage for private companies in both manufacturing industries(e.g., dell computer) and service industries (e.g., walmart). recently, researchers and practitioners have begun to focus on the public impact of scm, exploring the relationship between scm and health care, housing policy, the environment, and national security.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.120frontiers of engineeringthe goal of this session is to provide an introduction to the problems andmethods of scm, focusing on the matching of supply and demand in complex,variable, and/or uncertain environments. to illustrate the widespread applicability of scm, our speakers consider problems in a variety of settings, includingmanufacturing, the military, security, and public policy. these papers will provide examples of some of the work being done in scm. however, potentialapplications of scm methods could also include many other areas, such as waterresource management, certain nanoenvironments, network design of electrical systems, and so on.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.121supply chain applications offast implosion*brenda l. dietrichibm t.j. watson research centeryorktown heights, new york*an earlier version of this paper was published in dietrich et al. (2005). this version is includedhere with kind permission of springer science and business media.the chapter supply chain applications of fast implosion bybrenda l. dietrich is available only in the printed book.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.122frontiers of engineeringthe chapter supply chain applications of fast implosion bybrenda l. dietrich is available only in the printed book.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.supply chain applications of fast implosion123the chapter supply chain applications of fast implosion bybrenda l. dietrich is available only in the printed book.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.124frontiers of engineeringthe chapter supply chain applications of fast implosion bybrenda l. dietrich is available only in the printed book.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.supply chain applications of fast implosion125the chapter supply chain applications of fast implosion bybrenda l. dietrich is available only in the printed book.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.126frontiers of engineeringthe chapter supply chain applications of fast implosion bybrenda l. dietrich is available only in the printed book.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.supply chain applications of fast implosion127the chapter supply chain applications of fast implosion bybrenda l. dietrich is available only in the printed book.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.128frontiers of engineeringthe chapter supply chain applications of fast implosion bybrenda l. dietrich is available only in the printed book.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.supply chain applications of fast implosion129the chapter supply chain applications of fast implosion bybrenda l. dietrich is available only in the printed book.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.131from factory to foxhole:improving the armyõs supply chainmark y.d. wangrand corporationsanta monica, californiaat first glance, the endtoend supply chain by which repair parts are procured and moved to support u.s. army troops worldwide looks similar to commercial supply chains. both have suppliers, wholesale distribution centers, retailsuppliers, customers, and transportation carriers that move parts from point topoint. the main differences between military and commercial supply chainsrelate, not surprisingly, to the challenges the military faces and the way thosechallenges must be met.in 1999, a team of rand analysts was awarded al goreõs hammer awardfor support of the armyõs velocity management initiative, which dramaticallyimproved ordering and shipping times (ost) for repair parts. current efforts arenow focused further upstream in the supply chain to improve the armyõs purchasing and supply management (psm), integrate supplier management to increase stock availability, and lower total cost. as shown in figure 1, these initiatives span the entire army supply chain from factory to foxhole.military versus traditional supply chainstraditional commercial supply chains focus on physical efficiency, with theemphasis on operating at the lowest possible cost, minimizing investment ininventory, and maximizing capacity utilization. supply chains that support justintime manufacturing (e.g., the toyota production system) smooth the flow ofmaterial from supplier to manufacturing line (liker, 2003). management offrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.132frontiers of engineeringfigure 1 the armyõs factorytofoxhole supply chain and how velocity managementand purchasing and supply management can improve support for warfighters. source:wang, 2000.physically efficient supply chains may include active management of demand,(e.g., òeveryday low pricesó) to minimize surges and spikes and address inaccuracies in forecasting.in contrast, military supply chains focus on responsiveness and surge capabilities. the army must be able to deploy quickly anywhere in the world, and itssupply chain must be able to adapt and respond to unpredictable demands andrapidly changing environments. in preparation for operation iraqi freedom, theequivalent of more than ò150 walmart superstoresó was moved to kuwait tosupport 250,000 soldiers, sailors, airmen, and marines (walden, 2003).the nature of commodities, functional or innovative, dictates whether supply chains must be physically efficient or demand responsive (fischer, 1997).thus, industries that produce innovative products with very uncertain forecasts(e.g., hightech, highfashion, or even toy/entertainment industries) rely on demandresponsive supply chains (sheffi, 2005). the nature of the military mission requires a demandresponsive supply chain. in addition, the characteristicsof army repair parts add to the challenge.repair parts are not only highly specialized and weaponsystemspecific,but are also often produced by solesource suppliers who have no commercialmarket to fall back on. many parts, such as engines and transmissions, are òrepafrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.from factory to foxhole: improving the armyõs supply chain133rableó and must be overhauled as a source of future supply (diener, 2004). thus,the military not only has to manage a forward logistics pipeline, but must alsomanage an equally big reverse logistics, or òretrograde,ó pipeline in a òclosedloopó supply chain (blumberg, 2004). for every engine, transmission, or rotorblade replaced in the field, a carcass must be moved back to an army repairdepot or national maintenance program location. when you take into account thecommodity characteristics and a mission that must respond to unpredictablesurges and spikes in demand, the differences between the armyõs supply chainand the supply chains of commercial companies become readily apparent.velocity management to speed up flowthe purpose of the armyõs velocity management initiative, begun in 1995,was to improve the responsiveness, reliability, and efficiency of a logistics system based on massive stockpiles of supplies and weapon systems, many of themprepositioned òjust in caseó (dumond et al., 2001). although this was a worldclass system for supporting a cold war army, it has become increasingly lesseffective and unaffordable for the current forceprojection army.to measure the armyõs logistics performance, the velocitymanagementteam developed a percentile barchart presentation of ost that takes into account not only times for peak distribution, but also times for the tail end of thedistribution. figure 2 shows the time distribution of ost for moving instockfigure 2in 1994ð1995, lengthy ost times were combined with long, variable distribution times. source: wang, 2000.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.134frontiers of engineeringmateriel from wholesale defense distribution centers to the armyõs retail supplylocations. the horizontal axis shows ost measured in days, and the vertical axisshows the percentage of total requisitions. on the lower horizontal bar, the blackregion represents the time it took to receive half the requisitions for repair parts(17 days during the baseline period). the light (intermediate) and gray (final)regions show the time it took to receive 75 percent and 95 percent of the requisitions, respectively. the square marker shows the mean time (22.4 days duringthe baseline period). as this figure shows, the difference between the averagetime and the 95th percentile varied by a factor of two or three. thus, soldierswaiting for repair parts could not plan repair schedules or maintain the combatreadiness of their weapons systems. they simply had to wait, frustrated customers of an unreliable and unresponsive distribution system.the velocitymanagement team used a definemeasureimprove methodology to òwalk the process,ó following the flow of requisitions and materiel. anexcellent example of a winwin solution was the optimization of truck deliveries,which was accomplished by replacing a mix of delivery modes with a reliable,highvolume, highperforming distribution system based on scheduled deliveries. the army now has premiumlevel service that is faster, better, and cheaper.other improvements include better coordinated requisition processing and financial reviews, the adoption of simple rules to òclear the flooró daily, and theestablishment of a highlevel governance structure to measure performance andensure continuous improvement.as figure 3 shows, through velocity management, the army dramaticallyfigure 3army ost dropped dramatically during the implementation of velocity management. source: wang, 2000.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.from factory to foxhole: improving the armyõs supply chain135streamlined its supply process, cutting ost for repair parts by nearly twothirdsnationwide (wang, 2000). the greatest improvements, which cut ost by morethan 75 percent, were achieved at the major forces command (forscom) installations and other installations in the active army (figure 4). today, armycustomers nationwide and worldwide routinely receive the same quick, dependable service expected from a highperforming commercial supply chain.improving purchasing and supply managementdistribution improvements achieved through velocity management were focused on moving instock parts. more recent efforts have been focused on improving the armyõs psm processes to ensure that parts are kept in stock. duringoperation iraqi freedom, when both the operating tempo and demand for repairparts were consistently high, requisition backorders of armymanaged items atthe national wholesale level skyrocketed, reaching 35 percent for the active army(peltz et al., 2005). backorder rates are a key performance metric because theyindicate longer customer waiting times for parts, longer repaircycle times, and,ultimately, adverse impacts on the availability of weapons systems and unit readiness (folkeson and brauner, 2005).many factors were contributors to the armyõs stockavailability challenges.besides the contingency surge, they included financial delays and the underfunding of warreserve inventory prior to the war. the implementation of bestpsm practices, such as reducing lead times and total costs, could greatly improve future supply performance. in the commercial world, there has been afigure 4improvements in ost have been most dramatic at major forscom installations, such as fort bragg. source: wang, 2000.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.136frontiers of engineeringparadigm shift from managing items and contracts to managing suppliers andsupplier capacity. this has greatly reduced the òbullwhip effectóñwide variations in demand caused by a lack of coordination and information that cascadeback through a supply chain (lee et al., 1997). best psm practices call forcollaborative planning, forecasting, and replenishment by buyers and suppliers,which leads to better supplier management and more integrated supplier relationships. as the armyõs supply chain becomes more responsive to demand, itcontinues to move toward these psm goals.rand is currently performing highlevel analyses of the armyõs spendingfor goods and services, more than $300 billion in fy05, to identify opportunitiesfor improving purchasing (e.g., aggregating requirements when there are manycontracts or many suppliers for the same commodity). another important steptoward rationalizing the armyõs supply base will be the development of improved supply strategies. as longterm agreements are made with the best suppliers, overall supplier performance will improve, and the army and supplierscan work together to integrate business processes. army materiel command, theheadquarters organization responsible for psm, is planning to conduct severalpilot tests of psm principles in the coming year.summarythe armyõs supply chain faces unique challenges because it must operate inand provide support for highly unpredictable contingencies. as a result, it mustbe demandresponsive, that is, able to surge and adapt as conditions and demandchange. dramatic reductions in the armyõs ost have accelerated flow andstreamlined the armyõs supply chain. the current challenge is to leverage thedistribution improvements achieved by velocity management with higher andmore robust wholesale stock availability. efforts are under way to improve thearmyõs psm by adopting best practices in commercial psm to improve themanagement of suppliers and supplier capacity.referencesblumberg, d. 2004. introduction to reverse logistics and closedloop supply chain processes.boca raton, fla.: crc.diener, d. 2004. value recovery from the reverse logistics pipeline. rand mg238a. santamonica, calif.: rand corporation.dumond, j., m.k. brauner, r. eden, j. folkeson, and k. girardini. 2001. velocity management: thebusiness paradigm that has transformed u.s. army logistics. rand mr1108a. santamonica, calif.: rand corporation.fischer, m. 1997. what is the right supply chain for your product? harvard business review 75(2):105ð116.folkeson, j., and m.k. brauner. 2005. improving the armyõs management of reparable spare parts.rand mg205a. santa monica, calif.: rand corporation.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.from factory to foxhole: improving the armyõs supply chain137lee, h., v. padmanabhan, and s. whang. 1997. the bullwhip effect in supply chains. sloan management review 38: 93ð102.liker, j. 2003. the toyota way: fourteen management principles from the worldõs greatest manufacturer. new york: mcgrawhill.peltz, e., h.j. halliday, m.l. robins, and k.j. girardini. 2005. sustainment of army forces inoperation iraqi freedom: major findings and recommendations. mg342a. santa monica,calif.: rand corporation.sheffi, y. 2005. the resilient enterprise: overcoming vulnerability for competitive advantage.cambridge, mass.: mit press.walden, j. 2003. the forklifts have nothing to do: lessons in supply chain leadership. lincoln,nebr.: iuniverse.wang, m. 2000. accelerated logistics: streamlining the armyõs supply chain. rand mr1140a.santa monica, calif.: rand corporation.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.139managing disruptions to supply chainslawrence v. snyderlehigh universitybethlehem, pennsylvaniazuojun max shenuniversity of california, berkeleyfor as long as there have been supply chains, there have been disruptions,and no supply chain, logistics system, or infrastructure network is immune tothem. nevertheless, supply chain disruptions have only recently begun to receivesignificant attention from practitioners and researchers. one reason for this growing interest is the spate of recent highprofile disruptions, such as 9/11, the westcoast port lockout of 2002, and hurricanes katrina and rita in 2005.another reason is the focus in recent decades on the philosophy of òleanósupply chains, which calls for slimmeddown systems with little redundancy orslack. although lean supply chains are efficient when the environment behavesas predicted, they are extremely fragile, and disruptions can leave them virtuallyparalyzed. evidently, there is some value to having slack in a system.a third reason for the growing attention paid to disruptions is that firms aremuch less vertically integrated than they were in the past, and their supply chainsare increasingly global. a few decades ago, many firms manufactured productsvirtually from scratch. for example, ibm used to talk, only slightly hyperbolically, about sand and steel entering one end of the factory and computers exitingthe other. in contrast, todayõs firms tend to assemble final products from increasingly complex components procured from suppliers rather than produced inhouse. these suppliers are located throughout the globe, many in regions that areunstable politically or economically or subject to wars and natural disasters. inhis recent book end of the line, barry lynn (2006) argues that this globalizationhas led to extremely fragile supply chains.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.140frontiers of engineeringsupply chain disruptions can have significant physical costs (e.g., damageto facilities, inventory, electronic networks) and subsequent losses due to downtime. a recent study (kembel, 2000) estimates the cost of downtime (in terms oflost revenue) for several online industries that cannot function if their computersare down. for example, the cost of one hour of downtime for ebay is estimatedat $225,000, for amazon.com, $180,000, and for brokerage companies,$6,450,000. note that these numbers do not include the cost of paying employees who cannot work because of an outage (patterson, 2002) or the cost of losingcustomersõ goodwill. moreover, a company that experiences a supply chain disruption can expect to face significant declines in sales growth, stock returns, andshareholder wealth for two years or more following the incident (hendricks andsinghal, 2003, 2005a, 2005b).the huge costs of disruptions show that business continuity is vital to business success, and many companies are actively pursuing strategies to ensureoperational continuity and quick recovery from disruptions. for example, walmart operates an emergency operations center that responds to a variety ofevents, including hurricanes, earthquakes, and violent criminal attacks. this facility receives a call from at least one store with a crisis virtually every day(leonard, 2005). other firms have outsourced their business continuity and recovery operations. ibm and sungard, the two main players in this field, providesecure data, systems, networks, and support to keep businesses running smoothlyduring and after disruptions.supply chains are multilocation entities, and disruptions are almost neverlocalñthey tend to cascade through the system, with upstream disruptions causing downstream òstockouts.ó in 1998, for example, strikes at two general motors parts plants led to shutdowns of more than 100 other parts plants, whichcaused closures of 26 assembly plants and led to vacant dealer lots for months(brack, 1998). another, scarier, example relates to port security (finnegan 2006):nationalsecurity analysts estimate that if a terrorist attack closed new yorkharbor in winter, new england and upstate new york would run out of heatingfuel within ten days. even temporarily hampering the portõs operations wouldhave immeasurable cascading effects.nevertheless, very little research has been done on disruptions in multilocation systems. current research is focused mostly on singlelocation systemsand the local effects of disruptions. the research discussed below is a step toward filling this gap.underlying conceptssupply uncertainty (su) and demand uncertainty (du) have several similarities. in both cases, the problem boils down to having too little supply to meetdemand, and it may be irrelevant whether the mismatch occurs because of toofrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.managing disruptions to supply chains141much demand or too little supply. moreover, firms have used similar strategiesñholding extra inventory, using multiple suppliers, or improving their forecastsñto protect against both su and du.these similarities offer both good and bad news. the good news is thatsupply chains under du have been studied for decades, and we know a lot aboutthem. the bad news is that much of the conventional wisdom about du isexactly wrong for su. thus, we need research on supply chains under su todetermine how they behave and to develop strategies for coping with disruptionsin supply.related literaturein the early 1990s, researchers began to embed supply disruptions into classical inventory models, assuming that a firmõs supplier might experience a disruption when the firm wished to place an order. (see nahmias [2005] for anintroduction to inventory theory and zipkin [2000] for a more advanced treatment.) examples include models based on the economic order quantity model(berk and arreolarisa, 1994; parlar and berkin, 1991), the (r,q) model (gupta,1996; parlar, 1997), and the (s,s) model (arreolarisa and decroix, 1998). allof these models are generally less tractable than their reliable supply counterparts, although they can still be solved easily using relatively simple algorithms.more recent literature has addressed higher level, strategic decisions madeby firms in the face of disruptions. for example, tomlin (2006) explores strategies for coping with disruptions, including inventory, dual sourcing, and acceptance (i.e., simply accepting the risk of disruption and not protecting against it),and shows that the optimal strategy changes as the disruption characteristicschange (e.g., disruptions become longer or more frequent). tomlin and snyder(2006) examine how strategies change when a firm has advance warning of animpending disruption. lewis, erera, and white (2005) consider the effects ofborder closures on lead times and costs. chopra, reinhardt, and mohan (2005)evaluate the error that results from òbundlingó disruptions and yield uncertainty(another form of su) when making inventory decisions.only a very small body of literature is focused on disruptions in multilocation supply chains. hopp and yin (2006) investigated optimal locations forcapacity and inventory buffers in a multilocation supply chain and concludedthat as potential disruptions become more severe, buffer points should be locatedcloser to the source of disruptions. kim, lu, and kvam (2005) evaluated theeffects of yield uncertainty in a threetier supply chain. they addressed the consequences of the decision makerõs risk aversion, an important factor when modeling infrequent but highimpact events.a growing literature addresses disruptions in the context of facility location.here, the objective is to choose locations for warehouses and other facilities thatminimize transportation costs to customers and, at the same time, account forfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.142frontiers of engineeringpossible closures of facilities that would necessitate rerouting of the product.although these are multilocation models, they focus primarily on the local effects of disruptions (see snyder et al. [2006] for a review). we discuss thesemodels in greater detail below.supply uncertainty vs. demand uncertaintyin the sections that follow, we discuss the differences between su and duin multiechelon supply chains. (an echelon is a òtieró of a supply chain, such asa factory, warehouse, retailer, etc.) we consider several studies, each of whichexamines two possible answers to a question of supply chain design or management. each study demonstrates that one answer is optimal for su while theopposite answer is optimal for du. some of these results may be proven theoretically. others are demonstrated using simulation by snyder and shen (2006).1centralization vs. decentralizationconsider a system with one warehouse that serves n retailers (figure 1).under du, it is well known that if the holding costs are equal at the two echelonsand transportation times are negligible, then the optimal strategy is to hold inventory at the warehouse (a centralized system) rather than at the individualretailers (a decentralized system). this is because of the riskpooling effect,which says that the total mean cost is smaller in the centralized system becausecost is proportional to the standard deviation of demand. the standard deviation,in turn, is proportional to the square root of n in the centralized system but islinear in the decentralized system (eppen, 1979). although the assumptions ofequal holding costs and negligible lead times are unrealistic, the riskpoolingeffect and the insights that arise from it are applied widely in supply chainplanning and management.1although we use terminology suggestive of privatesector supply chains (e.g., òfirmsó and òretailersó), the results discussed in this paper are also applicable to noncommercial supply networks(e.g., military, health care, and humanitarian networks).figure 1onewarehouse, multiretailer system.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.managing disruptions to supply chains143now consider the same system under su (with deterministic demand). inthis case, if inventory sites are subject to disruptions, it may be preferable to holdinventory at the retailers rather than at the warehouse. under this decentralizedstrategy, a disruption would affect only a fraction of the retailers; under a centralized strategy, a disruption would affect the whole supply chain. in fact, themean costs of the two strategies are the same, but the decentralized strategyresults in a smaller variance of cost. this is referred to as the riskdiversificationeffect, which says that disruptions are equally frequent in either system, but theyare less severe in the decentralized system (snyder and shen, 2006).inventory placementin a serial system (figure 2), a common question is which stages shouldhold inventory. under du, the tendency is to push inventory as far upstream aspossible (to the left in figure 2), because the cost of holding inventory tends toincrease as one moves downstream in a supply chain. under su, however, thetendency is reversed. it is preferable to hold inventory downstream, where it canprotect against disruptions elsewhere in the supply chain. for example, this mightmean that a manufacturing firm should hold inventory of raw materials underdu but of finished goods under su.figure 3a. hubandspoke network. b. pointtopoint network. sites that hold inventory are shaded.figure 2serial system.hubandspoke vs. pointtopoint networksfigure 3 shows two possible networks for a firm with a single factory thatwants to distribute its product to multiple retailers. the network in figure 3a is afrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.144frontiers of engineeringhubandspoke network, with intermediate warehouses that hold inventory anddistribute it to retailers. the network in figure 3b is a pointtopoint network inwhich the warehouses are bypassed and retailers hold the inventory. many firmsoperate hubandspoke networks because of economies of scale and other savings from consolidating inventory locations. even absent economies of scale,however, the hubandspoke network is optimal under du because of the riskpooling effect (there are fewer inventorystocking locations, hence a smallertotal inventory requirement). under su, however, the pointtopoint network ispreferable because of the riskdiversification effect (increasing the number ofstocking locations reduces the severity of disruptions).a relevant analogy comes from the airline industry. large u.s. carriers haveprimarily adopted a hubandspoke model because of the economies of scale itoffers regarding airport infrastructure and the scheduling of flight connections.however, when a disruption (e.g., a thunderstorm) occurs at a hub, it can affectthe carrierõs entire domestic flight network. in contrast, smaller carriers havetended to adopt pointtopoint networks that allow flight schedules to be somewhat more flexible and reactive.supplier redundancyconsider a single firm with a single supplier trying to determine the value ofadding backup suppliers. suppose that each supplier has sufficient capacity tomeet the mean demand plus a few standard deviations. under du, backup suppliers have little value because they would fill in only when demand exceedscapacity, which happens infrequently. under su, however, backup suppliersplay a vital role because they provide capacity both to meet demand during adisruption to the primary supplier and to ramp up supply after a disruption.facility locationclassical facilitylocation models choose locations for plants, warehouses,and other facilities to minimize transportation cost or achieve some other measure of proximity to both suppliers and customers (daskin, 1995; drezner andhamacher, 2002), typically ignoring both du and su. a recent model finds thatunder du the optimal number of facilities decreases because of the riskpoolingeffect and economies of scale from consolidation (shen et al., 2003). conversely,when facilities face potential disruptions (i.e., under su), the optimal number offacilities increases because of the riskdiversification effect (snyder and daskin,2005). a model currently under development incorporates both du and su, thusbalancing these competing tendencies (jeon et al., 2006).frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.managing disruptions to supply chains145cost of reliabilitya firm that is used to planning primarily for du may recognize the importance of planning for su but may be reluctant to do so if it requires a large upfront investment in inventory or infrastructure. fortunately, a small amount ofextra inventory goes a long way toward protecting against disruptions. figure 4shows the tradeoffs between the vulnerability of a system to disruptions (on theyaxis, measured by the percentage of demands that cannot be met immediately)and the cost under du (on the xaxis, measured in the cost the firm is used toconsidering).each point in figure 4 represents a possible solution, with the leftmostpoint representing the optimal solution if there are no disruptions. this solutionis cheap but very vulnerable to disruptions. the lefthand portion of the curve issteep, suggesting that large improvements in reliability are possible with smallincreases in cost. for example, the second point shows 21 percent fewer stockoutsbut is only 2 percent more expensive. this trend is fairly common and has beenidentified in other contexts, including facility location with disruptions (snyderand daskin, 2005).conclusionsstudies of su and du in multiechelon supply chains show that the twotypes of uncertainty require different strategies in terms of centralization, inventory placement, and supply chain structure. in fact, the optimal strategy for dealing with su is, in many cases, the exact opposite of the optimal strategy for du.however, we are not suggesting that firms are currently doing everything wrong.rather, we are arguing that although du leads to certain tendencies in supplychain management (e.g., centralization), su suggests opposite strategies thatshould also be considered when making supply chain decisions. fortunately, it00.020.040.060.080.10.120.14020406080100120140ducostbackorderratefigure 4tradeoff curve.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.146frontiers of engineeringcan be relatively inexpensive to shift the balance enough to account for su, inthe sense that the tradeoffs between the two types of uncertainty are favorable.in virtually all practical settings, both du and su are present, and the optimal strategy must account for interactions between them. for example, sinceupstream inventory is cost effective under du but downstream inventory is mosthelpful under su, a firm may wish to adopt a hybrid strategy that combines theadvantages of both. for example, many firms hold inventory of both raw materials (upstream) and finished goods (downstream), with raw material inventoryaccounting for the bulk of the firmõs inventory holdings but finished goods inventory acting as a key buffer against uncertainty. alternately, a hybrid strategymay involve holding inventory near the middle of the supply chain (rather thanat both ends). for example, dell holds inventory of sophisticated components,assembling them into finished goods only after orders are placed.it is our hope that researchers will continue investigating the causes andeffects of supply chain disruptions, as well as strategies for coping with them.one important area for future research is the development of analytical tools forunderstanding the interdependence of risks faced by a supply chain. a singleevent (e.g., an economic downturn or a birdflu pandemic) might cause multipletypes of disruptions (e.g., a shortage of raw materials and absenteeism amongthe firmõs own workforce), and these risks may be subtly related. in other words,the supply chainõs total risk may not be a simple sum of its parts.another promising avenue for future research is to develop strategies fordesigning resilient supply chains. how can a supply chainõs infrastructure bedesigned so that buffers are located in the right places and in the right quantitiesto protect against disruptions and other forms of uncertainty? what forms shouldthese buffers take (e.g., inventory, capacity, redundant supply)?many of the analytical models for designing and managing supply chainsunder uncertainty assume that the decision maker has some knowledge of therisk of disruption, for example, the probability that a disruption will occur or theexpected duration of a disruption. in practice, these parameters can be very hardto estimate. therefore, we suggest, as a third area for future research, the development of models that are insensitive to errors in these parameters.acknowledgmentsthe authors gratefully acknowledge support from national science foundation grants #dmi0522725 and #dmi0348209.referencesarreolarisa, a., and g.a. decroix. 1998. inventory management under random supply disruptionsand partial backorders. naval research logistics 45: 687ð703.berk, e., and a. arreolarisa. 1994. note on òfuture supply uncertainty in eoq models.ó navalresearch logistics 41: 129ð132.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.managing disruptions to supply chains147brack, k. 1998. ripple effect from gm strike build. industrial distribution 87(8): 19.chopra, s., g. reinhardt, and u. mohan. 2005. the importance of decoupling recurrent and disruption risks in a supply chain. working paper, northwestern university, evanston, illinois.daskin, m.s. 1995. network and discrete location: models, algorithms, and applications. newyork: john wiley and sons.drezner, z., and h.w. hamacher, eds. 2002. facility location: applications and theory. new york:springerverlag.eppen, g.d. 1979. effects of centralization on expected costs in a multilocation newsboy problem.management science 25(5): 498ð501.finnegan, w. 2006. watching the waterfront. new yorker, june 19, 2006, pp. 52ð63.gupta, d. 1996. the (q,r) inventory system with an unreliable supplier. infor 34(2): 59ð76.hendricks, k.b., and v.r. singhal. 2003. the effect of supply chain glitches on shareholder wealth.journal of operations management 21(5): 501ð522.hendricks, k.b., and v.r. singhal. 2005a. association between supply chain glitches and operatingperformance. management science 51(5): 695ð711.hendricks, k.b., and v.r. singhal. 2005b. an empirical analysis of the effect of supply chaindisruptions on longrun stock price performance and equity risk of the firm. production andoperations management 14(1): 35ð52.hopp, w.j., and z. yin. 2006. protecting supply chain networks against catastrophic failures.working paper, northwestern university, evanston, illinois.jeon, h.m., l.v. snyder, and z.j.m. shen. 2006. a locationinventory model with supply disruptions. working paper, lehigh university, bethlehem, pennsylvania.kembel, r. 2000. the fibre channel consultant: a comprehensive introduction. tucson, ariz.:northwest learning associates.kim, h., j.c. lu, and p.h. kvam. 2005. ordering quantity decisions considering uncertainty insupplychain logistics operations. working paper, georgia institute of technology, atlanta.leonard, d. 2005. òthe only lifeline was the walmart.ó fortune 152(7): 74ð80.lewis, b.m., a.l. erera, and c.c. white. 2005. an inventory control model with possible borderdisruptions. working paper, georgia institute of technology, atlanta.lynn, b.c. 2006. end of the line: the rise and coming fall of the global corporation. new york:doubleday.nahmias, s. 2005. production and operations analysis. new york: mcgrawhill/irwin.parlar, m. 1997. continuousreview inventory problem with random supply interruptions. europeanjournal of operational research 99: 366ð385.parlar, m., and d. berkin. 1991. future supply uncertainty in eoq models. naval research logistics 38: 107ð121.patterson, d.a. 2002. a simple way to estimate the cost of downtime. pp. 185ð188 in proceedingsof lisa õ02: 16th systems administration conference. berkeley, calif.: usenix association.shen, z.j.m., c.r. coullard, and m.s. daskin. 2003. a joint locationinventory model. transportation science 37(1): 40ð55.snyder, l.v., and m.s. daskin. 2005. reliability models for facility location: the expected failurecost case. transportation science 39(3): 400ð416.snyder, l.v., and z.j.m. shen. 2006. disruptions in multiechelon supply chains: a simulationstudy. submitted for publication.snyder, l.v., m.p. scaparra, m.s. daskin, and r.l. church. 2006. planning for disruptions insupply chain networks. pp. 234ð257 in tutorials in operations research, edited by h. greenberg. baltimore, md.: informs.tomlin, b.t. 2006. on the value of mitigation and contingency strategies for managing supply chaindisruption risks. management science 52(5): 639ð657.tomlin, b.t., and l.v. snyder. 2006. inventory management with advanced warning of disruptions. working paper, lehigh university, bethlehem, pennsylvania.zipkin, p.h. 2000. foundations of inventory management. new york: mcgrawhill/irwin.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.149engineering methods forplanning affordable housing andsustainable communitiesmichael p. johnsoncarnegie mellon universitypittsburgh, pennsylvaniahousing is a key component of the u.s. economy. in 2001, housing comprised more than onethird of the nationõs tangible assets, and, in the form ofhome building and remodeling, housing consumption and related spending represented more than 21 percent of the u.s. gross domestic product. since 2001,home sales, prices, equity, and debt have all increased substantially, enablingmillions of americans to purchase goods and services (joint center for housingstudies of harvard university, 2006).decent, affordable housing (generally defined as housing that consumes lessthan 30 percent of a familyõs income) often enables families to enjoy stability,good health, employment, education, and recreation. decent, affordable housingalso contributes to the physical, economic, environmental, and social healthñthe sustainabilityñof communities (millennial housing commission, 2002).these impacts are especially important for lower income households and otherunderserved populations.despite the general strength of the u.s. housing market, the benefits ofhousing and stable, vibrant communities are not distributed equally. examples ofinequalities include: residential segregation, differences in homeownership ratesby race, sprawltype development patterns, and shortages of affordable housing.in the wake of hurricane katrina, for example, the challenges of securing basicshelter and rebuilding homes and communities have fallen disproportionately onminority and lowincome populations (de souza briggs, 2006; joint center forhousing studies of harvard university, 2006; millennial housing commission,frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.150frontiers of engineering2002). these and similar circumstances justify social intervention by government and nongovernmental organizations.the purpose of this paper is to highlight new, creative research in a varietyof disciplinesñespecially decision sciencesñthat can help determine when,where, what type, and by what means affordable housing and sustainable communities might be built, redeveloped, and maintained. as a prelude to the subject, it is useful to link housing planning and supply chain management, thetheme of this frontiers of engineering session.a supply chain is a network of facilities and modes of transportation thatuses production and logistics processes to transform inputs into finished goodsand services, thereby integrating supply and demand management. a centralfeature of supply chain management is temporal planningñstrategic, tactical,operational, and technical (e.g., the location of facilities at which operations areperformed). housing and community development (a social enterprise) are notliterally examples of supply chain management. however, facility locationñhere, the location of housingñis central to both, and the temporal scope ofhousing and community development planning spans strategic, tactical, and operational time horizons. finally, effective housing and community developmentplanning, like supply chain management, is an attempt to match supply anddemand for goods and servicesñin this case, affordable shelter and sustainablecommunities.initiatives to make affordable housing and sustainable communities moreaccessible must address the needs of stakeholders (e.g., employers, housing developers, citizens, government agencies); policy objectives (minimize housingcosts and environmental impacts, òdeconcentratingó poverty); and actions (thecreation of new housing alternatives, protection of current alternatives, changesin attitudes and preferences) (cf. de souza briggs, 2005).engineering and related disciplines can influence all of these dimensions ofhousing policy. civil, environmental, and mechanical engineering, for example,can generate methods of implementing housing initiatives with more efficientand effective construction. urban and regional planning, especially landuse andtransportation planning, in contrast, focus on social efficiency and equitable development outcomes, given current or bestpractice construction technologies.decision sciences (e.g., operations research and management science) representa link between engineering and planning methods; they generate specific, actionable strategies for optimizing social efficiency, effectiveness, and equity. decision sciences may take as given current or best practices in construction technologies or planning methods, or both, or neither.the remainder of this paper is focused on research results in engineeringconstruction methods and urban and regional planning methods related to thedevelopment of affordable housing and a discussion of the unique contributionsof decision sciences. we also identify a number of promising areas for continuedresearch.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.engineering methods for planning affordable housing151engineeringbased methods for housing constructiontraditional engineering is well suited to the efficient development of costeffective housing. improvements in construction technologies can result in increased affordability, energy efficiency, and structural integrity and decreasednegative environmental impacts. recent european research addressing òsustainableó development from an engineering perspective, focused mostly on minimizing negative environmental impacts, has shown that, even when constructiontechniques are modified to decrease the ecological impacts associated withòflowsó of energy, construction materials, and water, the resulting innovationsare often contradicted by increased resource usage by housing occupants andineffective national policies (e.g., priemus, 2005). ultimately, priemus argues,the policy with the greatest impact on sustainability may be a policy that discourages, or even decreases, the construction of new housing.other engineering approaches have focused on best practices for reducingenergy consumption through energyconserving materials, such as windows, insulation, and appliances; alternative energy sources, such as solar power; improved construction methods for foundations and walls; and more efficient heating and airconditioning systems (steven winter associates inc., 2001).buildingdesign strategies are based on advanced computer simulations comparing energy savings from novel designs with actual outcomes, as well as architectural choices, such as site selection and building orientation for maximum passive solar exposure, and compact floor plans. a specially designed house thatincorporated these technologies used 46 percent less energy than the averageu.s. house (balcomb et al., 1999).these technologies are also available for the rehabilitation of existing housing in lowincome areas through retrofitting, improved gas metering, and increased cooperation between stakeholders. estimated cost savings in energy fora lowincome family are on the order of one monthõs rent per year (katrakis etal., 1994).engineering methods also influence construction processes. examples include concurrent engineering to help meet customer requirements for industrialized housing (armacost et al., 1994) and knowledge management to improvecoordination between the owners, designers, and developers of affordable housing (ibrahim and nissen, 2003).urban planning for affordable housing andcommunity developmentamerican planners and analysts have been dealing, with limited success,with the problems of affordable housing and community design for more than 80years (von hoffman, 1996). in central cities, planners in the 1930s and 1940sembraced the idea of vertical towers grouped in communities distinct from surfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.152frontiers of engineeringrounding neighborhoods. these enclaves often resulted in social dysfunction andphysical decay, which have only been remedied in a substantial way in the pastdecade under the federal hope vi program. in contrast, postworld war iisuburbs were designed to be affordable, accessible to central cities via freeways,and uniform in appearance.in recent years, dense, transitfriendly, mixeduse developments in centralcities or nearby suburbs, often on land previously used for residential or industrial purposes, have converged with the redevelopment of distressed innercityneighborhoods into mixedincome, joint ventures (bohl, 2000). although u.s.consumers still overwhelmingly prefer the traditional suburban model of detached, singlefamily, owneroccupied housing, market demand is increasing forhousing units and communities that appear to be more sustainable socially andenvironmentally (myers and gearin, 2001).the impact of assisted housing development has been limited in recent yearsbecause of stagnant federal funding for subsidized and affordable housing. planning researchers are turning to decision models and geographic information systems to generate alternative strategies for optimizing social objectives (ayeni,1997). however, very little work in this area, or in traditional urban planning, isbeing done on decisionsupport models designed specifically for planning affordable housing.decisionscience methods for affordable housingpolicy and planningdecision models can help planners improve access to affordable housingand sustainable communities by simultaneously, and explicitly, addressing space,opportunity, design, and choice alternatives. space and opportunity are factors indecisions about the physical location of housing units and their proximity tocommunity amenities, which are important to improved quality of life. designdecisions are important to the development of policies that enable families toparticipate in housing programs, as well as in establishing development prioritiesand configuring mixed landuse and mixedhousing communities. choice decisions are essential to individuals choosing housing and neighborhood destinations that best meet their needs and preferences. in contrast to engineering construction and planning methods, decision models for housing development arequantitative, stylized, prescriptive, forwardlooking, and multiobjective.one type of strategic decision we address is choosing and evaluating housing and community development policies. a solution to this problem consists ofprogram types (e.g., housing subsidies) and intensities (e.g., funding levels ornumber of program participants). caulkins et al. (2005) developed a model topredict longterm population outcomes associated with stylized, largescale programs in which lowincome families use housing subsidies to relocate to lowpoverty neighborhoods. the purpose of the model is to identify the circumfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.engineering methods for planning affordable housing153stances under which a largescale housing program might preserve the health ofdestination communities. the authors model changes in the stock of middleclass families in a typical region as a result of (1) normal demographic changes,(2) a largescale housing mobility program resulting in lowincome families thatòassimilateó to the middle class, and (3) middleclass òflightó in response to inmovers. figure 1 shows that, for basecase values of structural parameters, equilibrium would be maintained over the long term (near x = 1) in a generic metropolitan area with a lowintensity housingmobility program; in the long term, thesize of middleclass communities would decrease only slightly.given support, in a strategic sense, for a particular housing policy, a tacticaldecision must be made about the amount and type(s) of housing to be providedin a specific region over a specific period of time. addressing this decisionrequires specifying program locations (municipalities, neighborhoods, or landparcels) and configurations (different numbers of differentsized rental orowneroccupied housing units). gabriel et al. (2006) developed a multiobjectiveoptimization model for identifying land parcels for development that balancesthe needs of planners, developers, environmentalists, and government.figure 1 dynamic optimization model solution for a housing mobility programñbasecase parameters. source: caulkins et al., 2005. reprinted with permission.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.154frontiers of engineeringfigure 2pareto frontiers for a case study of an affordablehousing location problem.source: johnson, 2006. reprinted with permission from pion limited, london.johnson (2006) solves two complementary optimization models specificallyfor affordable housing: (1) a longer range model for identifying regional investment levels that maximize social benefits and (2) a shorter range model foridentifying specific locations and development sizes that balances social benefitsand equity. figure 2 shows pareto frontiers associated with solutions to themultiobjective optimization problem for owneroccupied and renteroccupiedhousing using data for allegheny county, pennsylvania. the curves show that arange of policy alternatives can support a òmostpreferredó solution.the last decision problem considered here, operational in scope, is a clientõschoice of a mostpreferred housing program, neighborhood, or housing unit,within defined, affordable, housingpolicy priorities. solving this problem requires specifying detailed characteristics (attributes) of housing units and neighborhoods, decision models by which participants can rank potential destinations(alternatives), and information systems to help standardize and automate theprocess (decision support systems).johnson (2005) developed a prototype spatial decisionsupport system(sdss) for tenantbased subsidized housing that addresses qualitative concerns(which attributes of housing units and neighborhoods are important to the client)and quantitative concerns (how a client can rank a òshort listó of alternatives tofrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.engineering methods for planning affordable housing155maximize satisfaction and minimize the burden of the housing search). the sdssuses geographic information systems to illustrate neighborhood characteristics, arelational database to store information on specific housing units, and a multicriteria decision model to help clients make relocation decisions. figure 3 illustrates the spatialdata interface with fair housing data for allegheny county,pennsylvania.research now and in the futurea number of analytical methods can be used to make affordable housing andsustainable communities more accessible. in one stream of current research, civil,environmental, and mechanical engineering methods are being used to designhousing units that improve on current practices in terms of energy efficiency,cost, structural quality, and efficiency of construction processes. in anotherstream of current research, urban and regional planning are being used to helpstakeholders define development strategies that reflect best knowledge of socialsciencebased program evaluation, landuse and transportation planning standards, and communitylevel partnerships. decision sciences can provide opportunities to design housing and communitydevelopment policies that improveon current practices in constructionoriented engineering and planning in termsof social outcomes, multistakeholder negotiations, and housing program clientchoices.less than 224455779911riverspittsburgh city parksallegheny county parkspittsburgh neighborhoodsand suburban municipalitiesallegheny countyboundarypittsburgh city boundarytotal fair housingcomplaintsfigure 3spatialdata interface for counseling sdss. source: johnson, 2005. reprintedwith permission from elsevier. (figure can be viewed in color at http://www.andrew.cmu.edu/user/johnson2/searchpittsburghneighborhoods.jpg.)frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.156frontiers of engineeringbecause affordable housing and sustainable community development arenot currently top priorities for marketrate housing providers, government support for the engineering of residential housing may be necessary to increaseenvironmental sustainability and reduce user costs. however, housing policiesthat optimize various social criteria must also address technological aspects ofhousing and be based on best practices in urban and regional planning to beconsidered sustainable and affordable.the decisionsciences research described in this paper suggests a number ofpromising areas for future research. the most important is to provide evidencethat implementation of the decision models described above result in improvedoutcomes for communities and individuals. other areas for research include: (1)choices of housing design and construction strategies that balance housingunitand communitylevel sustainability measures; (2) the development of dynamicmodels for designing strategic housing policies to address placebased housingstrategies (i.e., new construction and rehabilitation of existing housing units);and (3) the design of realistic and tractable decision models to guide developersof affordable housing who must routinely choose a handful of sites to developfrom many alternatives, with limited funding, to maximize the probability ofneighborhood revitalization.as long as urban sprawl, environmental degradation, and geographical barriers to affordable housing and opportunity remain policy problems, researchershave an opportunity to devise novel and creative solutions at the nexus of engineering, planning, and decision sciences.acknowledgmentsmy thanks to jeannie kim and vincent chiou for assisting in this researchand to julie swann and jennifer ryan for encouraging me to participate in the2006 frontiers of engineering symposium.referencesarmacost, r.l., j. paul, m.a. mullens, and w.w. swart. 1994. an ahp framework for prioritizingcustomer requirements in qfd: an industrialized housing application. iie transactions 26(4):72ð80.ayeni, b. 1997. the design of spatial decision support systems in urban and regional planning.pp. 3ð22 in decision support systems in urban planning, edited by h. timmermans. london:e & f n spon.balcomb, j.d., c.e. hancock, and g. barker. 1999. design, construction, and performance of thegrand canyon house. national renewable energy laboratory, u.s. department of energy.available online at: http://www.nrel.gov/docsfy00osti/24767.pdf.bohl, c.c. 2000. new urbanism and the city: potential applications and implications for distressedinnercity neighborhoods. housing policy debate 11(4): 761ð801.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.engineering methods for planning affordable housing157caulkins, j.p., g. feichtinger, d. grass, m.p. johnson, g. tragler, and y. yegorov. 2005. placingthe poor while keeping the rich in their place: separating strategies for optimally managingresidential mobility and assimilation. demographic research 13(1): 1ð34. available online at:http://www.demographicresearch.org/volumes/vol13/1/default.htm.de souza briggs, x. 2005. politics and policy: changing the geography of opportunity. pp. 310ð341 in the geography of opportunity: race and housing choice in metropolitan america,edited by x. de souza briggs. washington, d.c.: brookings institution press.gabriel, s.a., j.a. faria, and g.e. moglen. 2006. a multiobjective optimization approach to smartgrowth in land development. socioeconomic planning sciences 40: 212ð248.ibrahim, r., and m. nissen. 2003. emerging technology to model dynamic knowledge creationand flow among construction industry stakeholders during the critical feasibilityentitlements phase. in information technology 2003: towards a vision for information technologyin civil engineering, edited by i. flood. reston, va.: american society of civil engineers.available on cdrom.johnson, m.p. 2005. spatial decision support for assisted housing mobility counseling. decisionsupport systems 41(1): 296ð312.johnson, m.p. 2006. planning models for affordable housing development. forthcoming in environment and planning b: planning and design.joint center for housing studies of harvard university. 2006. the state of the nationõs housing2006. available online at: http://www.jchs.harvard.edu/publications/markets/son2006/son2006.pdf.katrakis, j.t., p.a. knight, and j.d. cavallo. 1994. energyefficient rehabilitation of multifamilybuildings in the midwest. argonne national laboratory, decision and information sciencesdivision. available online at: http://www.eere.energy.gov/buildings/info/documents/pdfs/multigu.pdf.millenial housing commission. 2002. meeting our nationõs housing challenges: a report of thebipartisan millenial housing commission appointed by the congress of the united states.available online at: http://govinfo.library.unt.edu/mhc/mhcreport.pdf.myers, d., and e. gearin. 2001. current preferences and future demand for denser residential environments. housing policy debate 12(4): 633ð659.priemus, h. 2005. how to make housing sustainable?: the dutch experience. environment andplanning b 32(1): 5ð19.steven winter associates inc. 2001. building america field project: results for the consortium foradvanced residential buildings (carb), january to october 2001. national renewable energy laboratory, u.s. department of energy. available online at: http://www.nrel.gov/docs/fy03osti/31380.pdf.von hoffman, a. 1996. high ambitions: the past and future of american lowincome housing policy.housing policy debate 7(3): 423ð446.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.dinner speechfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.161the changing face of industrial researchw. dale comptonpurdue universitywest lafayette, indianagiving this talk is a great pleasureñnot only because i get to meet all of youand be part of this meeting, but also because this venue brings back many memories for me.you are meeting at the ford research laboratories during tumultuous timesfor the u.s. automotive industry. although i canõt offer insights into the currentstate of the industry or ford, i believe i can offer some insight into the factorsthat have led to the creation, and then the near demise, of some of this countryõsgreat industrial research establishments.major changes have occurred in recent years. bell labs is a distant memory.the size and focus of ibm research, ge research, westinghouse research, andxerox research have all been reduced, and research activities in many othercompanies have undergone similar changes. the focus of the industrial researchthat remains has changedñexcept for the pharmaceutical and biomedical industries, industrial research no longer includes basic research.why and how did these changes come about? letõs begin by examining thephilosophical justifications that made industrial research popular some 50 yearsago. this philosophy laid the groundwork for the great tide of industrial researchthat ultimately shaped the research posture today.the story begins with world war ii, when scientists and engineers from allover the country were called upon to leave their universities and come togetherto create the technologies and systems that were crucial to winning the war. thekey developments included radar, the proximity fuse, the atomic bomb, andmany others.in november 1944, following the cessation of hostilities, president franklinroosevelt wrote to dr. vannevar bush requesting his recommendations for postfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.162frontiers of engineeringwar activities. in rooseveltõs words, ònew frontiers of the mind are before us,and if they are pioneered with the same vision, boldness, and drive with whichwe waged this war, we can create a fuller and more fruitful employment and afuller and more fruitful life.ó dr. bush, who had been the director of the officeof scientific research and development during the war, responded with a report,scienceñthe endless frontier.a number of very important actions were taken in response to the recommendations in that report. first, in 1945, the office of naval research wasfounded, the first government agency responsible for funding research that didnot necessarily address immediate requirements for the military. this was thestart of federal funding for basic research in our universities. in january 1946,the research grants office was created at nih to administer projects of theoffice of scientific research and development and to operate a program ofextramural research grants and fellowship awards. a few years later, in 1950,the national science foundation (nsf) was created. nsf is still the principalsource of funding for basic research in the physical sciences, engineering, andsocial sciences. other federal departments soon followed suit and establishedtheir own extramural funding activities. many of you have participated in oneor more of these federal programs. in fact, nsf is providing partial support forthis conference.a number of actions were taken by industry soon after this. principally,industrial research was initiatedñprimarily in the physical sciences and engineering. back in 1951, ford created the ford research laboratories, as gerhardschmidt1 mentioned earlier. during the 50th anniversary celebration for the laboratories, ford characterized those 50 years in a very useful way. the period from1951 to 1970 was called the ògolden eraó of research, 1970 to 1985 the era ofredirection and regulation, and 1985 to 2001 the era of relevance. if we replacedregulation in the middle era with deregulation, as in the case of telecommunications, airlines, and energy, this characterization would be rather accurate forindustry as a whole.the golden erawhat was the emphasis during the golden era of research, the first 25 yearsafter world war ii? at that time, people looked at what scientists and engineershad done during the war and concluded that they could do the same for industry.the principal justification for supporting research was that good research wouldlead to good products and good profits. please note the absence of any mentionof the topics to be explored. the second justification was people. a research1vice president, research and advanced engineering, ford motor company.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.the changing face of industrial research163organization staffed by outstanding individuals would be available to consult oninternal company problems and would serve as the eyes and ears of the companyin the global world of science and technology.based on these two justifications, a wide variety of corporate laboratoriesflourished. great progress was made in science, publications were abundant, andopportunities for employment abounded. again, notice that the research outputwas not directly tied to the profits of the companies.the quality of science in physics, chemistry, and metallurgy at ford from1951 to 1970 was outstanding. if one were to compare the physics department atford with academic physics departments throughout the country, i think it is safeto say ford would have been in the top tenñmaybe even the top five.al overhauser at ford was the discoverer of the overhauser effect, whichhas been of enormous importance for solidstate physics. the first squidñanyof you in magnetic measurements will recognize the squid as the most sensitive detector of magnetic fields that has ever been developedñwas also inventedat ford. the first frequency doubler for the laser was demonstrated here. thescientists at ford were widely known and recognized as outstanding. scientistsin the ford chemistry and metallurgy departments were similarly talented.the era of redirection and regulationnow we enter the era of redirection and regulationñ1970 to 1986. allowme to share briefly some of my findings on joining ford in 1970. first, thepeople were outstanding. but, although they had great eyes and ears in the technical world, they had no effective communication with the operations sector ofthe company. in fact, they had little credibility with operations. programs in thephysics area were related to general relativity and solving math problems thatwere fun but had hardly any relevance to the companyõs operations. more important, i found no systems effort at all in the laboratory.during this era, we made serious efforts to refocus the laboratory on problems relevant to the company while keeping the research as long range and basicas possible. at the time, the company was beset by external demands that required new directions for research. the clean air act became law and emissionsregulations, fueleconomy regulations, and regulations limiting emissions frommanufacturing plants were passed.it was not hard to find research areas that could provide a fundamentalunderstanding that would be relevant to meeting these demands, but it took timefor people to reorient their thinking and embrace problems that were relevant tothe new needs of the company. in due course, however, a number of new activities were initiated.an atmosphericscience program was started, and a major effort in systemsengineering was undertaken. these soon began to provide dividends. ford wasthe first company to develop a simulation model for hybrid vehicles and then tofrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.164frontiers of engineeringdemonstrate the viability of a hybrid. ford developed the first flexiblefuel vehicles that could operate on 100 percent methanol, 100 percent gasoline, or anymixture of the twoñall automatically. we developed the first electronic enginecontrols in 1972. in 1975, they went into production as the first fully programmable electronic, adaptive control system in a production vehicle.that production program led to the largest offcampus reeducation of engineers up to that time. most of the mechanical engineers involved in the development of the engine and the controls needed uptodate training on digital electronics, so many of them pursued masterõs degree programs in electricalengineering at wayne state university. general courses were taught on thewayne state campus. courses based on proprietary information were taught atford.catalytic converters were critical to controlling emissions from vehicles.ford sponsored the development of the monolithic catalyst structure, which laterbecame the model for the industry. the key active ingredients in the catalystwere platinum and rodium, and reducing the amount of platinum became a longstanding goal. haren gandhi,2 who is sitting here in the front row, participated inthat effort. you may have noticed, on the outside wall in the large hall, themedal of technology that president bush presented to haren for his efforts toimprove the efficiency of catalysts, and, hence, reduce their cost by reducing theamount of platinum they required.despite the success of the catalyst, it was also an unfortunate example ofhow hard it is to communicate across òsilosó in a large company. while harenwas successfully reducing the amount of platinum in each catalyst, and thusreducing the cost of each unit, the financial sector of the company held a verylarge forward position in platinum. because there was no good mechanism fordiscussions between the companyõs research and financial sectors, ford lost a lotof money when the need for less platinum was announced and the value ofplatinum plummeted.research in many other areas was also ongoingñsensors, stamping dies,new paints, new highstrength alloys, and so on. basic research also continued,at a significantly lower level but at a high enough level to be effective. in otherwords, there was still an effective mass concentrated in areas that would logically support the large, more applied programs that, in turn, supported the operating divisions. i am sure gerhard will understand that maintaining basic research programs was possible only because we were able to hide them. theprograms directly related to the companyõs operating divisions were large enoughand important enough that management was not interested in asking questionsabout the other programs.the new regulatory environment had a profound impact on the company,2ford technical fellow and manager, chemical engineering department, ford motor company.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.the changing face of industrial research165which found itself working toward technical objectives with which it had noexperience and little knowledge. communication by researchers with brightpeople in operations who were willing to look at the needs of the company waspossible, but not easy. there were still many detractorsñpeople at high levels inthe company who wanted to reduce the research budget by as much as 30 percentin one year. fortunately, clearer heads prevailed, and the research laboratorieswere given enough time to make the necessary reorientation. if they had beenforced to do so in a very short time, it is unlikely that they would have continuedto exist.the era of relevancenow to the era of relevance, namely post1986. in this era, essentially allprograms must contribute either to the products or the processes used by thecompany that sponsors them. this is true not just at ford, but throughout industry. as companies have reduced the size, or even eliminated, their research laboratories or, at least, eliminated longterm research in their laboratories, manyhave increased their cooperation with universities, particularly in the bioscienceand engineering sectors. as a result, little basic research is being done today byindustry, although many industry segments continue to support some basic research, mostly in universities rather than in their own laboratories.but there are some problems with this arrangement. for example, graduatesinterested in pursuing careers in research have fewer opportunities. in addition,questions have arisen about ownership of intellectual property and, most important, about the funding philosophy of industry and federal agencies supportingresearch in universities. for example, how long a view can researchers take?and how many risks?lessons learnedlet me share with you my personal biases about what we have learned in thelast 60 years. good research can be done on relevant problems, but those problems frequently lead to questions that can only be answered by fundamentalresearch, which takes time. the reason the management of research in industry isso very difficult is that researchers must identify and be working on problemswell before the operations sectors even realize they have a problem. those problems cannot be solved immediately. research must be out in front, and that takestremendous foresight, which, in turn, requires that the research sector be in closetouch with operations. thatõs the only way these problems can be anticipatedand understood. only after that, can a company decide what can be done to solvethem and which problems will only be solvable through basic research.a research organization in industry is in a very fragile position. if it is tooclose to operations, the pressures for shortterm results may increase to the pointfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.166frontiers of engineeringthat longterm research is crowded out. if operations are held at armõs length,however, the research sector can lose its credibility and the support of the peoplewho would benefit the most from its results.those of you who work in universities may think none of this applies toyou. i hope that this is not the case, because choosing the right problem to workon at the right time is as critical to your success as to the success in industry. infact, it is critical that you get funding to pursue that research.the problems aheadjust as we learned that research in industry can only prosper in the long termwhen the research sector maintains contact with its customer, namely the company, so must we identify and tackle the really important problems confrontingnot only the company, but also the country and world markets. some of theseproblems are technical, and some are not. just for illustration, i will give youexamples of each.first, a nontechnical problemñthe lack of understanding of the consequences of political decisions, both local and national, related to technology. asearch of the congressional research office database for congressmen with òengineeró in their titles turned up only one. there may be a few more, but only onewas found in that search. that is pathetic, but it reflects how difficult it is fortechnical people to reach out and be part of the political system. we desperatelyneed to think about how to break down that barrier.we must address the whole issue of technical literacy, for both technical andnontechnical people. i might observe that one of the benefits of a symposiumlike this is that it increases your technical literacy in subjects that are not in yourspecial area of expertise. at my own institution, purdue university, there are nocourses that teach technology to nontechnical students. this is a travesty.now for the technical problems. a lot of things could be used as examples,but i will show my biases with two of themñenergy independence and healthcare delivery. first, energy independence. we have to find alternative fuels. wehave to find a way to become less dependent on the petroleum sources in thisworld. the conversion of cellulose to liquid fuel and coal to liquid are viablesources of liquidbased fuels, but the technology is not yet at a point that wouldmake these viable.my second example is a newly emerging research area for engineering. thenational academy of engineering and the institute of medicine recently published a study on the subject of engineering and health care delivery. the focuswas not just on bioengineering and biomedical engineering, as important as theyare, but also on the system by which care is provided to peopleñsuch as systemoptimization, sensors, remote communications, telemedicine, and making everyhospital room an intensive care unit. another question is how we can take adfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.the changing face of industrial research167vantage of the internet in the delivery of health care, the longterm role of whichwe cannot predict.major challengesat the beginning of this new century, the technical community is facing twomajor challenges. the first is ensuring the continuing availability of innovation,which is critical to our national prosperity. the second is supporting the necessary level of research to ensure that innovation continues.there is a tendency to think we have come full circle since 1944, fromresearch is golden, to research is unnecessary, to a realization that research iscritical to future innovation. however, we live in a time of globalization, whencompetition is fierce, money is limited, and expenditures that are not directlyrelevant to a companyõs mission must be justified. in fact, this is a more difficultenvironment than the environment of the 1970s when we were suffering theeffects of the oil embargo. we must find ways to meet these challenges throughboth technical and nontechnical means.i close with a quote from the recent national academies study, rising abovethe gathering storm. òthis nation must prepare with great urgency to preserveits strategic and economic security. because other nations have, and probablywill continue to have, the competitive advantage of a lowwage structure, theunited states must compete by optimizing its knowledgebased resources, particularly in science and technology, and by sustaining the most fertile environment for new and revitalized industries and the wellpaying jobs they bring.ói leave you with a big question. how will, or can, our institutions respond tothese challenges?frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.appendixesfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.171contributorsrobert l. axtell is an associate professor at the center for social complexityand computational social science at george mason university in fairfax, virginia. previously, dr. axtell was a senior fellow at the brookings institution inwashington, d.c., and he also served as a visiting or adjunct professor at thesanta fe institute, new school university, johns hopkins university, andgeorgetown university. he earned a ph.d. in engineering and public policyfrom carnegie mellon university, where he studied economics, computer science, game theory, operations research, and environmental science. his book,growing artificial societies: social science from the bottom up (mit press,1996), coauthored with j. epstein, was an early exploration of the potential ofmultiagent systems modeling in the social sciences. dr. axtellõs research hasbeen published in academic journals (e.g., science, proceedings of the nationalacademy of sciences, economic journal, computational and mathematical organization theory, journal of regulatory economics) and reprised in the popular science press (e.g., scientific american, science news, new scientist, discover, technology review), newspapers (e.g., wall street journal, los angelestimes, washington post), and magazines (e.g., atlantic monthly, new yorker).his latest book, artificial economies of adaptive agents: the multiagent systems approach to economics, was published by mit press in 2006. dr. axtellhas been a consultant to industry and government, through the former biosgroup,with nutech solutions, and most recently with bae systems.matthew j. barth is director of the college of engineering center for environmental research and technology at the university of california (uc), riverside. his transportation systems and vehicle technology research laboratoryfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.172frontiers of engineeringhas several fulltime staff members and provides research experience for undergraduate and graduate students; the research is focused on intelligent transportation systems and air quality. from 1985 to 1986, dr. barth was a member of thetechnical staff in the advanced technologies division of general research corporation, santa barbara. from 1986 to 1987, he was a visiting research studentat the university of tokyo. after completing his ph.d., he returned to japan as avisiting researcher at osaka university, where he conducted research in systemsengineering from 1989 to 1991. when he returned to the united states, he joinedthe faculty of the ucriverside college of engineering. dr. barth is a memberof the institute of electrical and electronic engineers, the air and waste management association, the transportation research board transportation and airquality committee and new technology committee, and the its america energy and environment committee. he has also served on several national research council committees. dr. barth received his m.s. and ph.d. in electricaland computer engineering from the university of california, santa barbara, in1986 and 1990, respectively.marcel bruchez is program manager at the technology center for networksand pathways and visiting associate research professor in the department ofchemistry at carnegie mellon university. from 1998 to 2005, at quantum dotcorporation in hayward, california, a company he cofounded, dr. bruchez wasfounding scientist and senior scientist in the chemistry division, principal scientist for labels product development, and director of marketing for labels products. he is the author of 12 manuscripts, 12 issued patents, and 20 publishedpatent applications. dr. bruchezõs other professional activities include reviewerfor journal of the american chemical society, advanced materials, angewandtechemie, nanoletters, nature materials, nature medicine, nature methods, andnature biotechnology (2002ð2003). he is also coeditor of methods in molecular biology: quantum dots in biological applications (humana press, 2006).dr. bruchez was the recipient of the rank prize optoelectronics award (2005)and mit tr100 award (2004). in 2003 he was recognized by science for one ofthe top ten scientific innovations of 2003ñquantum dots for biological detection. dr. bruchez received a ph.d. in physical chemistry from the university ofcalifornia, berkeley (1998).w. dale compton is the lillian m. gilbreth distinguished professor of industrial engineering, emeritus, at purdue university. his research interests includematerials science, automotive engineering, combustion engineering, materialsengineering, manufacturing engineering, and management of technology. from1986 to 1988, as the first national academy of engineering (nae) senior fellow, dr. compton directed activities related to industrial issues and engineeringeducation. he came to nae from the ford motor company, where he was vicepresident of research. before that, he was professor of physics and director of thefrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.contributors173coordinated sciences laboratory at the university of illinois. dr. compton hasserved as a consultant to numerous government and industrial organizations andis a fellow of the american physical society, american association for theadvancement of science, society of automotive engineers, and engineeringsociety of detroit. he has received the m. eugene merchant manufacturingmedal from the american society of mechanical engineers and the society ofmanufacturing engineers, the university of illinois college of engineeringalumni award for distinguished service, and the science trailblazers awardfrom the detroit science center and the michigan sesquicentennial commission. dr. compton was elected a member of nae in 1981 and is currently naehome secretary.timothy j. deming is a professor in the department of bioengineering at theuniversity of california, los angeles. previously, he held positions in the materials and chemistry departments and the interdepartmental program ofbiomolecular science and engineering at the university of california, santabarbara. dr. deming has received many awards and honors, including the international union of pure and applied chemistry (iupac) macromolecular division, samsungiupac young scientist award from the world polymer congress (2004), materials research society young investigator award (2003),camille dreyfus teacherscholar award (2000), beckman young investigatoraward (1998), alfred p. sloan research fellow (1998), and national sciencefoundation career award (1997). in 2002, he was a rothschildmayent foundation fellow at the institut curie in paris. dr. deming is currently a member ofthe editorial advisory boards of macromolecules, macromolecular bioscience,and biopolymers. he has also served on numerous professional society and faculty committees and worked with middleschool students. dr. deming has filed11 patent applications. he received a ph.d. in chemistry from the university ofcalifornia, berkeley (1993).brenda l. dietrich is director of mathematical sciences at the ibm thomas j.watson research center. her areas of research include manufacturing scheduling, services resource management, transportation logistics, integer programming, and combinatorial duality. she is a member of the advisory board of theindustrial engineering/management science department of northwestern university; a member of the industrial advisory board for both the institute formathematics and its applications and the center for discrete mathematics andtheoretical computer science at rutgers university; and ibmõs delegate to themassachusetts institute of technology supply chain 2020 program. she hasparticipated in numerous conferences of the institute for operations researchand the management sciences (informs), math programming, society forindustrial and applied mathematics, council of logistics management, and association for operations management. she holds a dozen patents, has coauthoredfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.174frontiers of engineeringnumerous publications, and has coedited mathematics of the internet: eauction and markets (springerverlag, 2002). dr. dietrich has been a member ofthe informs roundtable, served on the informs board as vice president forpractice, was chair of the advisory committee for the first two practice meetings,and is currently the presidentelect of informs. in addition, dr. dietrich hasserved on the editorial board of m&som and is currently on the editorial boardof logistics research quarterly. she received a b.s. in mathematics from theuniversity of north carolina and an m.s. and ph.d. in operations research/industrial engineering from cornell university.rebekah anna drezek, an associate professor of bioengineering and electricaland computer engineering at rice university, is affiliated with numerous institutes at rice, including the institute for biosciences and bioengineering, computer and information technology institute, rice quantum institute, center forbiological and environmental nanotechnology, and center for nanoscale science and technology. among her many awards are the mit tr100 award(2004), the beckman young investigator award (2005), the coulter foundationearly career translational research award (2005), and the american association for medical instrumentation becton dickinson career achievement award(2005). she has been an invited speaker or panelist at numerous colloquia, meetings, and workshops, including the american association for cancer researchannual meeting (2006), the 36th annual colloquium on the physics of quantum electronics (2006), and the ieee international biomedical imaging symposium (2006). dr. drezek received her m.s. and ph.d. in electrical engineeringfrom the university of texas at austin, in 1998 and 2001, respectively.michael p. johnson is an associate professor of management science and urbanaffairs at the h. john heinz iii school of public policy and management atcarnegie mellon university. his research interests are focused on publicsectorfacility location and service delivery, especially for affordable housing and sustainable community development. he has taught courses on operations research,decisionsupport systems, costbenefit analysis, and a capstone project coursefor public policy masterõs students. his extensive participation in academic service and community affairs has enriched his research and teaching. dr. johnsonrecently coedited a volume of tutorials in operations research and is currentlypresident of a professional society section on location analysis. he also foundedand codirected the carnegie mellon university/university of pittsburgh applied decision modeling seminar series. he recently evaluated plans by thepittsburgh public schools to open, close, and resize various public schools and iscurrently a member of a committee to redesign the districtõs program for giftedstudents. he has been a member of the board of the highland park communitydevelopment corporation and director of the development of a community planfor the highland park neighborhood of pittsburgh. dr. johnson has receivedfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.contributors175masterõs degrees in operations research from the university of california, berkeley, and in electrical engineering from the georgia institute of technology. hereceived a ph.d. from northwestern university in 1997. his previous professional experience includes consulting in logistics, operations management, andinformation systems.risto miikkulainen, professor of computer sciences at the university of texasat austin, has conducted recent research on methods of evolving neural networks and applying these methods to game playing, robotics, and intelligentcontrol. he is an author of more than 200 articles on neuroevolution,connectionist naturallanguage processing, and the computational neuroscienceof the visual cortex. dr. miikkulainen is an editor of the machine learningjournal and journal of cognitive systems research. he received an m.s. inengineering from the helsinki university of technology, finland (1986), and aph.d. in computer science from the university of california, los angeles (1990).andreas sch−fer is a lecturer (associate professor) in the department of architecture and a research associate with the institute for aviation and the environment at the university of cambridge. he is also a research affiliate with themassachusetts institute of technology (mit). previously, he spent five years atthe international institute for applied systems analysis in laxenburg, austria,and seven years at mit. dr. sch−fer has been working for more than 10 years inthe area of technology, human behavior, and the environment. his main areas ofinterest are modeling the demand for energy services, assessing characteristicsof greenhousegasemission technologies, and simulating the optimum technology dynamics in a greenhousegasconstrained energy system. he has publishedwidely on global traveldemand modeling, transportsystem technology assessment, and the introduction of technology. dr. sch−fer holds an m.sc. in aeronautical and astronautical engineering and a ph.d. in energy economics, both fromthe university of stuttgart, germany.alan schultz, director of the navy center for applied research in artificialintelligence at the naval research laboratory in washington, d.c., conductsresearch on humanrobot interaction, evolutionary robotics, learning in roboticsystems, and adaptive systems. the recipient of an alan berman research publication award, he has published more than 75 articles on machine learning androbotics. dr. schultz is currently cochair of the american association for artificial intelligence (aaai) symposium series and program chair of the 2007association for computing machinery (acm)/institute of electrical and electronics engineers international conference on human robot interaction. in 2006,he was program cochair of the 2006 acm international conference on humanrobot interaction. in 1999 and 2000, he chaired the aaai mobile robot competition and exhibitions.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.176frontiers of engineeringlawrence v. snyder is the frank hook assistant professor of industrial andsystems engineering at lehigh university and codirector of lehighõs centerfor value chain research. his research interests include modeling and solvingproblems in supply chain management, facility location, and logistics, especiallyunder the threat of disruptions or other sources of uncertainty. his research hasreceived awards from informs and the institute of industrial engineers. hehas worked as a supply chain engineer and consultant for major producers ofboth perishable and durable goods. dr. snyder received a ph.d. in industrialengineering and management sciences from northwestern university.morley o. stone is a former program manager in the defense sciences officeof the defense advanced research projects agency and principal research biologist and biotechnology lead for the air force research laboratory (afrl) atwrightpatterson air force base, where he has been a materials research engineer, research biologist, senior research biologist, and biotechnology groupleader. in addition, he is an adjunct faculty member in the department of materials science and engineering at ohio state university. his honors and awardsinclude fellow, afrl (2005); carnegie mellon alumni award (2005); vincentj. russo leadership excellence award (2003); and mit tr100 nominee (2003).he is a member of the american chemical society, american association forthe advancement of science, and materials research society. dr. stone received a ph.d. in biochemistry from carnegie mellon university (1997).mark y.d. wang is a senior physical scientist at the rand corporation insanta monica, california, where he works in the area of purchasing and supplychain management, including inventory and multimodal distribution logistics;business process reengineering; acquisition/purchasing strategy. recent researchprojects have included improving contracting at the city of los angeles airport,port, and department of water and power; reverse logistics; hightech manufacturing; and technology transfer from federally funded research and development.he was associate director of randõs national security research division andassociate director of randõs science and technology policy institute. dr.wang received an sc.d. in physics from the massachusetts institute of technology in 1994.lloyd watts is founder, chair, and chief technology officer of audience inc., aventurebacked company in silicon valley developing highperformance audiosignal processing systems for the telecommunications industry. he has workedat microtel pacific research, synaptics, arithmos, and interval research. hereceived a b.sc. in engineering physics from queenõs university (1984), anm.a.sc. in electrical engineering from simon fraser university (1989), and aph.d. in electrical engineering from the california institute of technology(1992).frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.contributors177susan zielinski is managing director of smart (sustainable transportationand access research and transformation) at the university of michigan in annarbor; smart is a project of the center for advancing research and solutionsfor society (carss). just before joining smart/carss, ms. zielinski spenta year as a harvard loeb fellow working on new mobility innovation andleadership. prior to 2004, she cofounded and directed moving the economy, aninnovative canadawide òlink tankó that catalyzes and supports new mobilityindustry development. for more than 15 years, she was a transportation plannerfor the city of toronto, where she worked on developing and leading transportation and airquality policies and initiatives, with a focus on sustainable transportation/new mobility. ms. zielinski has been an advisor to local, national, andinternational initiatives, including the national advisory committee on energyefficiency, transport canadaõs sustainable development advisory committee,gridlock panel of the ontario smart growth initiative, organisation for economic cooperation and development environmentally sustainable transportproject, the jury of the stockholm partnerships for sustainable cities, the european conference of transport ministers, the centre for sustainable transportation, and the kyoto cities initiative international advisory panel. after receiving her undergraduate degree from the university of toronto and a graduatefellowship to study for a year in france, she received a masterõs degree in environmental studies from york university. she is a registered professional plannerand a member of the canadian institute of planners.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.179programnational academy of engineering2006 u.s. frontiers of engineering symposiumseptember 21ð23, 2006chair: julia m. phillips, sandia national laboratorythe rise of intelligent softwaresystems and machinesorganizers: m. brian blake, georgetown university, and david fogel,natural selection, inc.commercializing auditory neurosciencelloyd wattscreating intelligent agents in gamesristo miikkulainencoevolution of the computer and social sciencerobert axtellusing computational cognitive models to build betterhumanrobot interactionalan schultz***frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.180frontiers of engineeringthe nano/bio interfaceorganizers: tejal desai, university of california, san francisco, andhiroshi matsui, cuny graduate center and hunter collegepart i: solving nanotechnology problems using biotechnologybiological and biomimetic polypeptide materialstimothy j. deming, university of california, los angelesbiomimetics and the application to devicesmorley o. stone, air force research laboratorypart ii: solving biotechnology problems using nanotechnologyoptical imaging for in vivo assessment of tissue pathologyrebekah a. drezek, rice universitycommercialization and future developments in bionanotechnologymarcel bruchez, carnegie mellon university***engineering personal mobility for the 21st centuryorganizers: apoorv agarwal, ford motor company, andwilliam schneider, university of notre damelongterm trends in global passenger mobilityandreas sch−fer, university of cambridgeenergy and environmental impacts of personal mobilitymatthew j. barth, university of california, riversidenew mobility: the next generation of sustainable urban transportationsusan zielinski, university of michiganadvancing the state of hybrid technologyandreas schell, daimlerchrysler corp.***frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.program181supply chain management and applications witheconomic and public impactorganizers: jennifer ryan, university college dublin, and julie swann,georgia institute of technologysupply chain applications of fast implosionbrenda l. dietrich, ibm thomas j. watson research centerfactory to foxhole: improving the armyõs supply chainmark wang, rand corporationsupply chain management under the threat of disruptionslawrence v. snyder, lehigh universityengineeringbased methods for affordable housing and sustainablecommunity developmentmichael p. johnson, carnegie mellon university***dinner speechthe changing face of industrial researchw. dale comptonfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.183participantsnational academy of engineering2006 u.s. frontiers of engineering symposiumseptember 21ð23, 2006alexis abramsonwarren e. rupp assistant professordepartment of mechanical andaerospace engineeringcase western reserve universitystephanie g. adamsassistant dean of researchaaas/nsf science and engineeringfellowuniversity of nebraskaðlincolnapoorv agarwaleva engine pmt leaderscientific research laboratoriesford motor companydavid v. andersonassociate professorschool of electrical and computerengineeringgeorgia institute of technologymark h. andersonassociate scientistcollege of engineeringuniversity of wisconsinðmadisonana i. antšnassociate professordepartment of computer sciencenorth carolina state universityrobert axtellassociate professordepartment of computational anddata sciencesgeorge mason universitygeorge d. bachandprincipal member of the technicalstaffsandia national laboratoriesqing baimember of technical staffagilent technologies, inc.frontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.184frontiers of engineeringmatthew barthdirectorcenter for environmental researchand technologyuniversity of california, riversidebradley r. bebeevice president, systems softwareinformation systems worldwidecorporationchandra r. bhatprofessordepartment of civil, architechtural,and environmental engineeringuniversity of texas at austinstephan billerlab group managergeneral motorsm. brian blakeassociate professor of computersciencegeorgetown universityrichard k. bogerapplication engineerabaqus, inc.marcel p. bruchezprogram manager, technology centerfor networks and pathwaysvisiting associate researchprofessor, department ofchemistrycarnegie mellon universitydavid bruemmerprincipal research scientistidaho national laboratorybryan cantrillsenior staff engineersun microsystemskimberly a. chaffinprincipal scientistmedtronic, inc.jane p. changassociate professordepartment of chemical andbiomolecular engineeringuniversity of california, los angelesjia chenresearch staff memberibm t.j. watson research centeraref chowdhurymember of technical staffbell laboratories, lucenttechnologieszissis dardasgroup leaderunited technologies research centerjarrett datcherengineer/scientistboeingðphantom worksmoses m. davidlead senior research specialistcorporate research process researchlaboratorymatthew delisaassistant professorschool of chemical and biomolecularengineeringcornell universityfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.participants185timothy demingprofessordepartment of bioengineeringuniversity of california, los angelestejal a. desaiprofessor of physiology/bioengineeringuniversity of california, sanfranciscobrenda l. dietrichdirector, mathematical sciencesibm t.j. watson research centeranne c. dillonsenior research scientistnational renewable energylaboratoryscott w. doeblingprogram managerlos alamos national laboratoryrebekah anna drezekassociate professordepartments of electrical andcomputer engineering andbioengineeringrice universityjohn dunaganresearchersystems and networking groupmicrosoft researchrichard elanderadvanced pretreatment team leaderbioprocess engineering groupnational bioenergy centernational renewable energylaboratoryye fangresearch associatescience and technology divisioncorning inc.andrei g. fedorovassistant professorwoodruff school of mechanicalengineeringgeorgia institute of technologydavid b. fogelchief executive officernatural selection, inc.suresh v. garimellaprofessorschool of mechanical engineeringpurdue universitymichael j. garvinassistant professordepartment of civil andenvironmental engineeringvirginia polytechnic institute & stateuniversityirene georgakoudiassistant professordepartment of biomedicalengineeringtufts universitycindie giummarrasenior engineeralcoa technical centerchristine s. grantprofessordepartment of chemical engineeringnorth carolina state universityfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.186frontiers of engineeringpaula hicksprincipal scientistcargillron hosenior research scientistsun microsystems research labsmani janakiramprincipal engineerintel corporationmichael p. johnsonassociate professorh. john heinz iii school of publicpolicycarnegie mellon universitypaul c. johnsonprofessor and executive deanira a. fulton school of engineeringarizona state universityruben juanesassistant professordepartment of civil andenvironmental engineeringmassachussetts institute oftechnologyjack w. judyassociate professor and director ofthe neuroengineering programdepartment of electrical engineeringuniversity of california, los angeleskrishna kalyanprincipal staff engineermotoroladeepak khoslasenior research scientisthrl laboratories, llcsteven kouassociate professor of industrialengineeringcolumbia universityolga anna kucharresearch scientist 4pacific northwest nationallaboratorydavid a. lavanassistant professordepartment of mechanicalengineeringyale universitychristian lebiereresearch facultypsychology departmentcarnegie mellon universityphilip leducassistant professordepartments of mechanicalengineering, biomedicalengineering, and biologicalsciencescarnegie mellon universitylorraine h. linsenior engineering specialistbechtel national, inc.melissa lundenstaff scientistlawrence berkeley nationallaboratoryteng maassociate professorfamufsu college of engineeringflorida state universityfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.participants187surya k. mallapragadaprofessordepartment. of chemical andbiological engineeringiowa state universityrajit manoharassociate professorschool of electrical and computerengineeringcornell universityhiroshi matsuiassociate professordepartment of chemistrycuny graduate center and huntercollegelawrence meganmanageradvanced control and optimizationr&dpraxairmatthew m. mehalikvisiting assistant professordepartment of industrial engineeringuniversity of pittsburghnafaa mekhilefsenior research scientistarkema, inc.sergey melnikresearchermicrosoft researchristo miikkulainenprofessordepartment of computer scienceuniversity of texas at austinjosh molhostaff r&d engineercaliper life sciencesjeffrey montanyeresearch and development directordow automotivedow chemical companykumar muthuramanassistant professorschool of industrial engineeringpurdue universitytimothy r. nolensenior research associate and labheadeastman chemical companyeric a. ottsenior engineerge aviationjulia m. phillipsdirectorphysical and chemical sciencescentersandia national laboratoriesjonathan prestontechnical fellowlockheed martin aeronauticscompanywilliam d. provineresearch managercentral research & developmentdupont companyadam rasheedaerospace research engineerge global researchfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.188frontiers of engineeringcarol regovice president cdmjennifer ryansenior lecturermanagement departmentuniversity college dublintodd salamonmember of technical staffmath and algorithmic sciencesresearch departmentbell laboratories, lucenttechnologiesandreas sch−ferassociate professordepartment of architectureuniversity of cambridgeandreas schelloffice of north american operationsdaimlerchryslerwilliam f. schneiderassociate professordepartment of chemical andbiomolecular engineering,concurrent in chemistryuniversity of notre damealan schultzdirectornavy center for applied research inartificial intelligencenaval research laboratorycorey j. schumachersenior research aerospace engineerair force research laboratorykenneth shepardassociate professordepartment of electrical engineeringcolumbia universityabhijit v. shevademember of engineering staffjet propulsion laboratorymichael siemerpresidentmydea technologiesvijay singhassistant professordepartment of agricultural andbiological engineeringuniversity of illinois at urbanaðchampaignsanjiv k. sinhacorporate directorenvironmental consulting &technology inc.lawrence snyderprofessordepartment of industrial and systemsengineeringlehigh universityhyongsok sohassistant professorbiomolecular science andengineering & mechanicalengineeringuniversity of california, santa barbaramorley o. stoneprincipal research biologist andbiotechnology leadmaterials and manufacturingdirectorateair force research labfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.participants189julie swannassistant professorindustrial and systems engineeringgeorgia institute of technologymargarita p. thompsonsenior research engineerdelphi research labssammy tinassociate professordepartment of mechanical, materials,and aerospace engineeringillinois institute of technologymanuel torressection manager and assistant vicepresidentadvanced concepts business unitsaicaditya tyagisenior water resources engineerch2m hillmichiel van nieuwstadttechnical leaderscientific research laboratoryford motor companymark wangsenior physical scientistrand corporationmichael l. washingtonindustrial engineernational immunization programimmunization services divisionu.s. centers for disease control andpreventionlloyd wattsfounder, ceo, and ctoaudience inc.paul k. westerhoffassociate professordepartment of civil andenvironmental engineeringarizona state universitycolin s. whelansenior principal engineerraytheon companychristopher wolvertontechnical leaderford motor companygerard wongassistant professordepartment of materials science andengineeringuniversity of illinois at urbanaðchampaignrobert wraychief scientistsoar technologyaleksey yezeretstechnical advisor and leadercatalyst technology groupcummins inc.chengxiang zhaiassistant professordepartment of computer scienceuniversity of illinois at urbanaðchampaignfrontiers of engineering: reports on leadingedge engineering from the 2006 symposiumcopyright national academy of sciences. all rights reserved.190frontiers of engineeringjingren zhouresearchermicrosoft researchsusan zielinskimanaging director, smartcenter for advancing research andsolutions for societyuniversity of michiganguestsadnan akaydivision directordivision of civil, mechanical, andmanufacturing innovationdirectorate for engineeringnational science foundationmarshall lihsenior advisordivision of chemical,bioengineering, environmental,and transport systemsdirectorate for engineeringnational science foundationmary lou maherprogram directorhumancentered computing clusterdirectorate for computer andinformation science andengineeringnational science foundationdinner speakerdr. w. dale comptonlillian m. gilbreth distinguishedprofessor of industrialengineering, emeritusschool of industrial engineeringpurdue universitynational academy of engineeringwm. a. wulfpresidentlance a. davisexecutive officerjanet hunzikersenior program officergin baconsenior program assistantford motor companygerhard schmidtvice president, research andadvanced engineeringharen gandhiford technical fellow and manager,chemical engineeringdepartmentbarb rutkowskiadministrative assistant to harengandhijohn markeemanager, business operationsjim bretzresearch technologist, businessoperationsrose gossmanbusiness operations
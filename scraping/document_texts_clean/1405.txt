detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/1405supercomputers: directions in technology and applications112 pages | 6 x 9 | paperbackisbn 9780309040884 | doi 10.17226/1405computer science and technology board, national research council, and academyindustry program, national academy of sciencessupercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.supercomputers: directions intechnology and applicationsacademy industry programnational academy of sciences/national academy of engineering/institute of medicineand thecomputer science and technology boardcommission on physical sciences, mathematics, and resourcesnational research councilnational academy presswashington, d.c. 1989isupercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.notice: this book is based on a symposium cosponsored by the academy industry program (ajoint project of the national academy of sciences, the national academy of engineering, and theinstitute of medicine) and the computer science and technology board of the national researchcouncil. it has been reviewed according to procedures approved by a report review committeeconsisting of members of the two academies and the institute of medicine.the national academy of sciences is a private, nonprofit, selfperpetuating society of distinguished scholars engaged in scientific and engineering research, dedicated to the furtherance of science and technology and to their use for the general welfare. upon the authority of the chartergranted to it by the congress in 1863, the academy has a mandate that requires it to advise the federal government on scientific and technical matters. dr. frank press is president of the nationalacademy of sciences.the national academy of engineering was established in 1964, under the charter of thenational academy of sciences, as a parallel organization of outstanding engineers. it is autonomousin its administration and in the selection of its members, sharing with the national academy of sciences the responsibility for advising the federal government. the national academy of engineeringalso sponsors engineering programs aimed at meeting national needs, encourages education andresearch, and recognizes the superior achievements of engineers. dr. robert m. white is presidentof the national academy of engineering.the institute of medicine was established in 1970 by the national academy of sciences tosecure the services of eminent members of appropriate professions in the examination of policy matters pertaining to the health of the public. the institute acts under the responsibility given to thenational academy of sciences by its congressional charter to be an adviser to the federal government and, upon its own initiative, to identify issues of medical care, research, and education. dr.samuel o. thier is president of the institute of medicine.the national research council was organized by the national academy of sciences in 1916 toassociate the broad community of science and technology with the academy's purposes of furthering knowledge and advising the federal government. functioning in accordance with general policies determined by the academy, the council has become the principal operating agency of both thenational academy of sciences and the national academy of engineering in providing services tothe government, the public, and the scientific and engineering communities. the council is administered jointly by both academies and the institute of medicine. dr. frank press and dr. robert m.white are chairman and vice chairman, respectively, of the national research council.support for this project was provided by the academy industry program and by the followingorganizations and agencies: apple computer, inc., control data corporation, cray research, inc.,the defense advanced research projects agency (grant no. n0001487j1110), the departmentof energy (contract no. defg0587er25029), digital equipment corporation, hewlett packard,ibm corporation, the national aeronautics and space administration (grant no. cda860535),the national science foundation (grant no. cda860535), and the office of naval research(grant no. n0001487j1110).cover: donna j. cox, scientist: charles evans, "neutron star collision," national center forsupercomputing applications, 1986library of congress catalog card number 8962945international standard book number 0309040884available from:national academy press2101 constitution avenue, n.w.washington, d.c. 20418printed in the united states of americas014first printing, december 1989second printing, june 1990iisupercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.academy industry programallan r. hoffman, directoredward abrahams, senior staff officerlois e. perrolle, staff officerdeborah faison, senior program assistantcomputer science and technology boardjoseph f. traub, columbia university, chairmanjohn seely brown, xerox parc corporationmichael l. dertouzos, massachusetts institute of technologysamuel h. fuller, digital equipment corporationjames freeman gilbert, university of california at san diegowilliam a. goddard iii, california institute of technologyjohn e. hopcroft, cornell universityrobert e. kahn, corporation for national research initiativessidney karin, san diego supercomputer centerleonard kleinrock, university of california at los angelesdavid j. kuck, university of illinois at urbanachampaignrobert langridge, university of california at san franciscorobert w. lucky, at&t bell laboratoriesraj reddy, carnegie mellon universitymary shaw, carnegie mellon universitywilliam j. spencer, xerox corporationivan e. sutherland, sutherland, sproull & associatesvictor vyssotsky, digital equipment corporationshmuel winograd, ibm corporationirving wladawskyberger, ibm corporationmarjory s. blumenthal, executive directordamian m. saccocio, staff officermargaret a. knemeyer, staff associatedonna f. allen, administrative secretarycatherine a. sparks, secretaryiiisupercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.ivsupercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.prefacetrends in supercomputing technologies and the use of supercomputers inthree innovative u.s. corporations are discussed by leading experts and byindustry representatives in these proceedings of a symposium on supercomputersheld at the national academy of sciences complex on september 8 and 9, 1988.the presentations that compose this report have been revised and updated in theinterval between the symposium and publication of this report. the symposiumwas the product of two groups, the academy industry program and the nationalresearch council's computer science and technology board.the academy industry program was created in 1983 to open a dialoguebetween the national research council and industry leaders. the program has atwopart purpose: (1) to make national research council studies, which numberabout 300 each year, available to industry decision makers and (2) to learn fromindustry how this country should best address its longterm needs in science andtechnology. sixtynine companies are currently members of this expandingprogram. industry, government, and academe provide the three legs of the u.s.science and technology base, and the academy industry program helps ensurethat industry's role in that triad is carefully considered within the nationalresearch council.the computer science and technology board, created in 1986, has anambitious agendašone focusing on research needs and public policies toenhance u.s. production and use of new computer technologies. the board'smembership, which is half corporate and half academic, reflects its belief in apartnership of the corporate and the academic sectors. inprefacevsupercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.addition, the board is an intentional mix of people who might identify themselvesas computer scientists and engineers or who would list one of the sciences astheir discipline and might also call themselves computational scientists. theboard is also somewhat unusual in that a very large proportion of its supportcomes from the corporate sector. ibm corporation, digital equipmentcorporation, hewlett packard, cray research, inc., control data corporation,and apple computer, inc. are all corporate sponsors.the board's most important activity by far is to study items of nationalinterest having something to do with computing, but there is one overarchingtheme, the competitiveness of the united states, that lies behind almost every oneof the studies.the board believes that it is not the manifest destiny of the united states toremain the leading computer country in the world. in fact, if we act complacent,it is assured that we will not be and that we deserve not to be. the reason thatleadership in computing is so important is that computing is the enablingtechnology. if we lose computing, we lose much more. in an information societywhere much of the industry is in the service sector, it is extremely easy forcompanies to go abroad. if we think that we saw an outflow in the manufacturingsector, we must realize how easy it would be for companies that do not have acapital base in this country to move their companies abroad.among the board's recent projects are the following: the national challenge in computer science and technology (nationalacademy press, washington, d.c., 1988), addresses nature and nurtureissues for the computer field. many of its major recommendations are inline with the remarks made by senator albert gore, jr., in his keynoteaddress for this symposium. a report, toward a national research network (national academypress, washington, d.c., 1988), written in response to a request fromthe office of science and technology policy for a review of a proposednational research network, which came out of what is sometimes calledthe gore initiative; a study, requested by the state department, of the technology thatmight affect u.s. policies on export control (global trends in computertechnology and their impact on export control, national academypress, washington, d.c., 1988); a review, requested by the national aeronautics and spaceadministration, of nasa's computer science research program; a colloquium, "keeping the u.s. computer industry competitive:defining the agenda," held in may 1989; andprefacevisupercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved. a survey of over 100 supercomputer users and developers that wasprepared by the board to help guide its future assessments of highperformance computing.these projects are just examples from the board's rich portfolio.supercomputers are prominent among the board's projects because scienceand technology advances make highperformance computing an increasinglyessential element of the u.s. scientific and industrial bases. how and why this isso are the focus of this symposium report.joseph f. traub, chairmancomputer science and technology boardprefaceviisupercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.prefaceviiisupercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.contentspart a opening remarks 1 welcomerobert m. white 32 supercomputers: vital tool for the nation's futurethe honorable albert gore, jr. 53 introductionlarry l. smarr 13part b the changing landscape of supercomputer technology 4 existing conditionsjack worlton 215 toward the futuresteve chen 51contentsixsupercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.part c existing applications of supercomputers in industry 6 deciding to acquire a powerful new research toolšsupercomputingbeverly eccles 737 using supercomputing to transform thinking about product designclifford r. perry 818 achieving a pioneering outlook with supercomputinglawrence g. tesler 90part d concluding remarks 9 summarydoyle d. knight 101contentsxsupercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.part aopening remarks1supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.2supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.1welcomerobert m. whitenational academy of engineeringladies and gentlemen, on behalf of the national academy of sciences, thenational academy of engineering, and the institute of medicine, it gives megreat pleasure to welcome you to our symposium on supercomputers.this symposium was organized by both our academy industry program,which seeks to strengthen the interactions between industry and the nationalresearch council, and our computer science and technology board, which, onbehalf of both academies and the national research council, is responsible forthe oversight of developments in computer science and technology and forproviding advice on computer activities to various groups in the federalgovernment and to others.certainly the digital computer and its applications have now becomeubiquitous. each generation has its own supercomputer. these, the mostpowerful computers produced by our industry, have characteristically opened newavenues for exploration in industry, government, research, and engineering.in welcoming you to this symposium, i would like to comment briefly on theapplication of supercomputers in three branches of geophysics that are among themost active users of supercomputers and have been since the advent of largescale digital computers. these are weather, atmospheric and ocean studies, andseismic analysis.i was privileged to go through the period of watching weather forecastingbeing transformed from an art to a science, beginning in about the mid1950s.today it is impossible to think about a weather forecast withoutwelcome3supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.thinking about the application of supercomputers to that activity.supercomputers, for purposes of weather forecasting, are now literally scatteredthroughout the world.it is also important, considering the growing national and internationalconcerns about the greenhouse problem, to recognize that the only way we havehad to simulate and experiment with the consequences of increasingconcentrations of infrared gases in the atmosphere has been by modelingatmospheric and oceanic systems with supercomputers. all our information, allour forecasts, and all our predictions about possible consequences of increasingamounts of greenhouse gases stem from the application of those mathematicalmodels and their integration on supercomputers.we have in this audience individuals who are deeply familiar with theapplications of supercomputers in a third area, seismic exploration.these are areasšaffecting industry, government, and researchšthat havebeen totally and utterly transformed by the supercomputer and the variousgenerations of the supercomputer, and these are fields that still remain limited bythe present capacity of supercomputers.we can use almost whatever capacity can be developed and provided for usto make better forecasts, understand the climate better, and model stratigraphy inthe earthšso please, keep at it. but supercomputers have transformed not onlythese fields but also many, many other fields, as this symposium's participantswill affirm. it is the ability of large computers to simulate large and complexsystemsšwhether they be physical, chemical, social, or economic systemsšthatmakes them so central to social and economic progress and to progress in ourunderstanding of nature.a concern shared by senator albert gore, jr., and the other participants inthis symposium is the challenge the u.s. computer industry faces from abroad.this is a serious and important challenge. it is a contest where we dare not comein second. we hope that this symposium will communicate at least in part what isat stake.welcome4supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.2supercomputers: vital tool for the nation'sfuturethe honorable albert gore, jr.u.s. senateintroduction of senator albert gore, jr., by frank press, chairman,national research council: i want to welcome everyone. this is an impressiveturnout from both academia and literally the most important companies inamerica interested in the opportunities available for supercomputers. all of usare fully aware of the exploding use of these machinesšthe locomotives of theinformation age. they can slow time in showing the excited states of atoms andions and chemical reactions or the behavior of quarks. they can quicken time byreshaping the earth's surface in minutes. they can help depict the unobservablesšthe propagation of cracks in a material under a load of stress, or electronscirculating around a neutron star. but that is science, and only part of the story.there is a bewildering and growing array of applications in the developmentof technology and in the creation of new products and industrial processes, andthat is the part of the story that this symposium addresses.in introducing senator albert gore, jr., i say with confidence that few incongress understand better the essential role of advancing technology in thisnation's future than does senator gore, and he has expressed that understandingby legislative leadership, by an informed and vigorous critique of federal scienceand technology policy, and most especially by his efforts to ensure that this nationmaintains and exploits its forefront position in supercomputers.as a member of the senate commerce committee, he has held numeroushearings and introduced important legislation related to the development andimplementation of supercomputers in the united states.supercomputers: vital tool for the nation's future5supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.recently, senator gore held a hearing on supercomputer networks.introducing that hearing, senator gore said, ''ensuring america's worldleadership in advanced computer technology may well be the most importanteconomic and technological challenge of the twentyfirst century.'' now he willtell us more about that.senator gore: thank you very much, frank. i am delighted to be here. thisis a crucial meeting, and i hope that those who might have come here with somequestions in their minds about where the nation will go, and where theircompanies or their institutions will go, will see this conference as a beginningpoint for the creation of a new national consensus about where america can go totake advantage of the revolution in supercomputing, with all it means for thefuture of our nation.of course a lot of those decisions will be made on capitol hill, and just asthe development of hardware always outpaces the development of software, sothe development of software almost always outpaces the development of publicpolicy. and true to form, we have been very slow to react.we are going to face many great challenges in the years to comešfromfinding shelter for 2 million homeless men, women, and children in this countryto giving the next generation of americans the best schools on earth. but i firmlybelieve that there may well be no more significant economic and technologicalchallenge than pushing forward to ensure america's leadership in advancedcomputer technology.we have come to a turning point as a nation. that is said so frequently italmost has become a cliche, but it really is incredible to live in the present time.we have discovered the ability to destroy human life. we have found theblueprint of life itself. we have invented artificial intelligence. we are creatingglobal environmental problems that sound like the plots of bad science fictionnovels, and in all of these fields our socalled common sense is challengedšbecause common sense is an accumulation of distilled experience, and we havemoved beyond historic experience.what is happening is not only new, it is not only unprecedented, but it isalso, in many cases, unimagined and is so different from what has happened incivilization up until now that we are jarred and knocked off balance and requiresome pause to collect ourselves and realize that this is indeed a turning point.those of us alive today must answer a fundamental question that underlies thenuclear arms race, the greenhouse effect, the epidemics, and the starvation in theworld. the underlying question is, are we as human beings capable of rising tothis unprecedented challenge?it is a question that i believe and hope will be answered affirmatively. in asense we are confronted in field after field not just with a crisis orsupercomputers: vital tool for the nation's future6supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.a problem, but with what yogi berra once described in his inimitable way whenhe said, "what we have here is an insurmountable opportunity."as we look at the possible solutions for so many of the problems that weface, we can see the emerging developments in supercomputer technology as afabulous opportunity that must not remain insurmountable.i think that the people attending this symposium therefore have a remarkableopportunity to guide this country's future. the supercomputer is not just anotheruseful invention. i do not know who came up with the analogy first, but i haveused it many times: the supercomputer is to the information revolution what thesteam engine was to the industrial revolution.we are ahead, we like to tell ourselves. we manufacture 72 percent of thesupercomputers in the world. but the benefits of supercomputing do not comefrom the creation of the machines; they come from the use of the machines. andwe are not using the machines. the companies that could be using them to opennew frontiers of science and technology and competition do not have the peoplewho can sit down and use supercomputers. partly as a result, the companies arenot buying supercomputers. and the people who do have supercomputers are notable to communicate with each other very effectively.so we do not imagine the new uses that are the most important ones, theones that we do not understand yet. make no mistake about it, this is a completelynew field of scientific inquiry. just as we have the two established methods ofcreating knowledge, inductive reasoning and deductive reasoning, so now wehave computing, a totally new avenue to knowledge. we must learn to understandit and use it.over the past year, i campaigned in every region of this country and saw thefoundations of solid economic progress. but when i went to larry smarr'snational center for supercomputing applications in urbana, illinois, and talkedwith others of you in this field, i really was inspired by the potential for what cantake place in america.but none of that will take place unless we solve the problems that make it aninsurmountable opportunity. frank press referred to my hearings last month.those hearings were part of a continuing involvement that has led me to theconviction that highperformance computing should be a top priority for thenation.we must launch an immediate assault on five fronts:1. we must create a national fiber optic network with high capacity forlinking supercomputing centers throughout the united states.2. we must stop chiseling at the margins where funding forsupercomputing centers is concerned. we must take advantage of theinvestment that has already been madešwhich is, after all, such aminor expendituresupercomputers: vital tool for the nation's future7supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.in the scheme of thingsšto allow this nation to take advantage ofwhat is already in place.3. we must put in place a special initiative to address the bottlenecks incomputer software development, where an extreme shortage ofspecialized software for important applications is impeding ourprogress.4. we must give the highest priority to educating and training youngpeople, graduate students, and postgraduate students so that we willhave the people who can help us participate in this revolution.5. finally, we must provide adequate funding for research anddevelopment in these related areas.six years ago, when i introduced the idea of a national fiber optic network, ispoke with people from corning glass works who were, understandably,enthusiastic supporters of the concept. but i could not find widespread supportfor the idea.i remember as a 10yearold child sitting in senate hearings where myfather, who was a senator at that time, introduced legislation creating theinterstate highway system. i remember listening as the problems wereconfronted and discussed, and then i remember not too many years later seeingthe bulldozers move the earth and seeing the drive from our farm in carthage,tennessee, to washington, d.c. cut from 17 hours to 9 hours, making it a 1daytrip instead of a 2day trip. and i remember watching the truck traffic andcommerce expand exponentially. the effects on the country have been sodramatic that they have never really been cataloged or even studied intensively. itis like the effect of the telephone system on the countryšit is so pervasive it isdifficult to study.i began with that model and then changed it significantly as i began toexplore highperformance computing. when we think and talk in the unitedstates particularly in my profession of politicsšabout infrastructure, we oftenmean highways, bridges, sewer lines, and water lines, and we need those things.but we are kidding ourselves if we think that that kind of infrastructure is the keyto competing with other countries in the future. trying to compete on that basis islike rebuilding world war i military hardware in preparation for world war ii.infrastructure is attractive to the political system because it is a function ofgovernment. it provides a role for government to play that liberals andconservatives can both accept. the benefits, generally speaking, are available toall. all boats are lifted, to use john kennedy's analogy to the rising tide. but inthinking about infrastructure, we have to expand our imaginations and realizethat the infrastructure we most need, if our objective is to increase our nation'scapacity to compete and to pursue knowledge, is going to be different from thekind of infrastructure on which we have concentrated.supercomputers: vital tool for the nation's future8supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.can we rely on the market system to give us that kind of infrastructure? forthe interstate highway system, we could not, yet private industry was theprincipal beneficiary after the government's investment was made. it was difficultto project the benefits that would generate user fees from trucks and cars payinggasoline taxes on interstate highways that did not exist, but the government madethe investment and did not add a penny to the national debt, because thecommerce generated by the investment was so vast as to generate user fees thathave created a huge surplus in the trust fund that was established.we must commit an act of faith once again and invest in new infrastructurethat we knowšwe knowšwill create benefits to private industry so vast as togenerate user fees or some other form of compensation to more than cover therelatively small investment that would be required to create this infrastructure.many people attending this symposium know a great deal about thisnetwork. for those who are hearing about it for the first time, let me brieflysketch the problem. private industry, whether it is the communications industryor any other industry requiring fiber optic cable, does not yet need the kind ofcapacity that supercomputers need. the numbers sound big to me as anonscientist, but 50 million bits per second is considered huge as a capacity, andthe communications companies in most cases are not driving the networks muchbeyond that capacity. it does not make economic sense for them to do so.supercomputers can usefully take advantage of 1 billion bits per second, or 3billion bits per second, or 10 billion bits per second. two years ago, with helpfrom the house commerce committee, i authored and steered to passagelegislation that authorized and kicked off a wideranging study of highperformance computing networks by the office of science and technologypolicy. there are already more than 100 major networks in the country, includingthe nsfnet, but coordination among them is limited.the study found, as i expected, that these superhighways for information arenow more like leftturn lanes at rush hour. they have low capacity. they areoverloaded. they are unable to keep pace with demand. this study also warnedthat, while the united states continues to develop the best supercomputertechnologies, we have been less than successful in applying them to address ourneeds. once again we are in danger of inventing a technology only to watch othernations apply the technology.as john young has said, silicon valley is not very different from detroit ifthe latest trade figures for electronics are examined. the japanese are not that farbehind, because although we invented electronic devices and created them first,we have not been using them.supercomputers: vital tool for the nation's future9supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.john connolly at the university of kentucky's center for computationalsciences said at the hearing that computer users will be able to send highdensityapplications such as highquality pictures and graphics through supercomputernetworks, but that demand for capacity far exceeds supply. he said that the nationmay soon find itself in a "graphic jam."a few years ago i noticed that the japanese entity for targeting keytechnologies for accelerated funding and attention had produced a list of 10 or 12toppriority projects. one of them was a 10gigabaudša 10billionbitspersecondšfiber optic network. what are we doing? we need to build on theadvantages that we have in this field before it is too late.it is getting to the point that it is almost too late. but it is not yet too late.that is the good news. we have the capacity to move quickly. but i want to argueto the national academy of sciences, to the commercial entities represented inthis symposium, and to scientists and researchers from fields other than computerscience that the creation of this national highvolume fiber optic network, asuperhighway for information linking supercomputing centers, ought to be thenumberone science priority for the united states of america.it is not just a computer science project. we all know from studying thehistory of science about the intimate link between communications capacity andscientific advance. why did scientific discovery explode after the invention of theprinting press? why do so many important advances occur almost simultaneouslyat different points around the world? it is, of course, because communicationmakes it possible to assemble all of the pieces of a mosaic that then becomesapparent to many people at the same time.now the communications technology of tomorrow, the computing capacityinherent in supercomputers, is virtually impossible for us to use because wecannot link supercomputer centers. think for a moment with me what it would belike if clusters of researchers in universities and commercial enterprises all overamerica could share the capacity of those machines and the infrastructureexisting at supercomputing centers, and then communicate on a regular basis withtheir counterparts all over the united states. the word synergy is inadequate todescribe the advances that would occur.imagine for a moment what it would be like if state governments competingone with another built interchanges to connect to this network, helping to createclusters of small businesses entering the information industry and able todownload their products for distribution to a hungry market connected to thenetwork at other points throughout the united states.there are few things we could do that would contribute more to this nation'sability to compete effectively in the future. many questions havesupercomputers: vital tool for the nation's future10supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.to be answered before the project i propose can become a reality. the questionsare being attacked right now, but the commitment must be there. i believe thatthose who are part of this field should turn their attention to this network as a highpriority.i believe that access to the incredible capabilities of supercomputers on abroad scale would of itself pull more bright young people into the field and intoprograms where they could acquire the skills that would enable them to then gointo industry and teach their employers how to revolutionize the particularbusinesses in which they were working.another vital concern that we must address is the bottleneck in softwaredevelopment. i have introduced legislation that is focused particularly oneducational software and again is designed to address a particular area thatmarket incentives are not solving. there are specialized applications for softwarethat have a relatively small market at this point, and the money to pay for thesoftware is really not there, but the payoff from that software is incredibly large.we ought to understand that, we ought to fill that gap, and we ought tocreate incentives where none now exist to produce that software. i have called forthe establishment of a publicprivate corporation that will evaluate particularlyhighpriority projects and then provide seed money that has to be leveraged byprivate investment with a large multiple to the public investment. but the privateinvestment will be pulled in by the imprimatur or seal of approval that comes fromthe selection of a particular software project that is greatly needed and has highpriority. i think the basic idea has worked in the past, and i think that it can workin the future.the inattention to education is an old story. i mentioned john young earlier;he and his fabulous commission have received virtually no attention. but theirreport (picking up the pace: the commercial challenge to american innovation,council on competitiveness, washington, d.c., 1988) is epochal, and itemphasizes educationšpre18 education, education in computer science andtechnology, and education in the traditional basic sciences. we must face up tothe problems in the educational system.we must also address the other problem that the young commission focusedon. that is, how do we fill the gap between research and applications? senatorfritz hollings, chairman of the commerce committee, has come up with someimaginative proposals, as have others. i intend to spend much of the next 2 yearsfocusing on that particular problem, and i welcome input from many here whohave thought a great deal about it.and then finally, we have to have adequate funding for the supercomputercenters and for research and development across the board.i believe we need leadership. i believe we need vision. i believe we needcommitment. because we know what can be done. we know what the payoff canbe. in a highspeed, highstakes competition with the japanese,supercomputers: vital tool for the nation's future11supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.mild words of encouragement are simply not good enough. you know that, and iwant you to feel one day soon that your government also knows that.each of you in your field is doing america a favor by pointing the way tothe future. the japanese have proved what a nation can accomplish withpowerful ideas and determination, and i believe it is america's turn to do thesame. after all, we were the ones who showed them how, and it is up to us torenew the american spirit. i believe we are up to the task.supercomputers: vital tool for the nation's future12supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.3introductionlarry l. smarrnational center for supercomputing applicationsuniversity of illinoiswe are gathered together at a moment in which the country is going to haveto make some critical decisions about its future, and many of the people attendingwill help make those decisions.i think the question we are all asking is, why supercomputers? we certainlyunderstand that the media like supercomputers. we read about supercomputing.we hear about it. and yet many of you here from corporations are wondering, do ineed to get involved in supercomputing? and if so, how? what are some of thereasons?it is not enough that supercomputers represent some of the most excitingtechnology today.senator gore in his keynote speech really put his finger on it. it is atechnology the use of which by american industry may very well determine thefuture of the u.s. economy in the global economy. the constant references to thesupercomputer as the steam engine of the information age or the machine tool ofthe 1990s are shorthand attempts to capture that idea. and yet probably not morethan 15 percent of the fortune 500 companies own supercomputers, sosupercomputers are not seen as being fundamental to all research, development,and manufacturing today.but if you are watching the trends, you will discover major changeshappening in corporations embracing this technology. we have here todayrepresentatives from three of those trendsetting corporationsšabbottlaboratories, eastman kodak company, and apple computer, inc. listen to whythey are using supercomputers and listen to the struggle they areintroduction13supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.going through internally to get their people, their scientists, and their engineers touse these machines. what i have foundšand i have probably talked to severalhundred industries in the last 2 yearsšis that in many ways the resistance iscoming primarily from the scientists and engineers themselves.as i have reflected on this phenomenon, i think that much of the difficultygoes back to the key role that universities play in their relationship to industry, aswell as to the critical role that federal funding for science plays in determining thepace of scientific progress and the future of this country. between 1970 and 1985there was a period that i have referred to as the "supercomputer famine inamerican universities."during this period federal funds were cut off for placing advancedcomputing equipment in our universities. for these 15 years, university studentsand professors were not doing their research on supercomputers. for 15 yearsindustry hired students from universities who did not bring those skills andattitudes into industry that would create a demand for supercomputing. now ourcountry has placed up to very high levels in industry a whole generation ofscientists, engineers, and managers who have never used, seen, or cared about asupercomputer.from this point of view, it is not difficult to understand why there has beenresistance to the use of supercomputers in industry and why america has missedits opportunity to take advantage of these machines and place them at the base ofthe american economy.fortunately this situation is changing extremely rapidly because of theforesight of the national science foundation (nsf) and the congress. theyundertook an initiative in 1984 to set up a national program for providingsupercomputing access to american universities. this consisted of creatingnational supercomputer centers and beginning to build what senator gorereferred to as the "superhighways of the information age." this proposed nationalnetwork would hook together the supercomputer centers with the researchuniversities in the country, thereby coupling the personal computers orworkstations on the investigators' desks with the remote supercomputers.that program now funds five supercomputer centers. doyle knight, amember of the steering committee for this meeting, is the director of the john vonneumann center in princeton, and i am the director of the national center forsupercomputing applications (ncsa) at the university of illinois; sid karin, thedirector of the san diego supercomputer center, is also participating in thissymposium. the other two centers are the pittsburgh supercomputer center andthe cornell supercomputer facility. all of the american supercomputermanufacturers (cray research, inc., control data corporation's eta systems(disbanded in april 1989), and ibm corporation) have been represented in thosecenters. three yearsintroduction14supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.ago, one had to leave the united states and go to europe to get access toamericanbuilt supercomputers to do basic research. now we have created anational infrastructure that, between the five centers, serves roughly 6000scientists at 200 universities in the united states. each scientist is allocated timethrough a peer review of proposed research to assure the quality of the projects.that is a rather miraculous discontinuous change by american time scales. whatwe are going to see now is that the graduates of these 200 universities will cometo industries wanting to know where their supercomputers are to do their work.this will be similar to what we saw when engineers, who previously hadused drafting tables, wanted to know where their cad/cam workstations werewhen they were hired. in a short period of time the whole notion of engineeringcad/cam changed in america, and both productivity and the complexity ofdesign increased.through the nsf supercomputing centers, the federal government isproviding universities with the kind of education and training necessary to bringnew blood into the national pool of individuals trained in advanced computing.unfortunately this does not directly help the vast current research communitywithin industry that is not going to go through that process. what we need to dois to create additional structures to expose key industry people to the sameopportunities, so that they develop the enthusiasm for advanced computing thatwe see among the bright young graduate students and professors in americanuniversities.each of the five nsf centers is pursuing industrial participation at theircenters in different ways. one model will be discussed in this symposium whencliff perry talks of kodak's participation in our center. over 60 researchers fromkodak have come to the ncsa interdisciplinary research center in the last 2years to convert codes that run on ordinary computers without visualization toones that work in a modern distributed environment of supercomputers networkedto mainframes and workstations. they can work with some of the world's expertsin visualization technologies to create visual interfaces to the massive numericdata fields they compute.the most important principle that i have learned as director of a center isthat if this country is really going to meet this crisis and take advantage of thisopportunity, teamwork is going to be the key idea. this means both the structuralteamwork between the federal government, industry, and universities and the kindof teamwork we see in our interdisciplinary research center between individualscientists, artists, computer scientists, and computer professionals workingtogether as small teams to take on problems of enormous complexity. teamworkis america's strong suit. it is what will pull us through this.a very good example of the results of teamwork can be seen in the artexhibit presented at this symposium by donna cox, an assistant professorintroduction15supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.of art and design at the university of illinois and an adjunct professor at ncsa.she is one of the most innovative computer artists today. she creates her art bytaking the numeric output of supercomputers, in areas of science, engineering,and mathematics, and then working with what she terms a ''renaissance team''šan artist, a scientist, and a computer scientistšto create beautiful visualizations.scientists are able to see their results through this teamwork in ways that they, asindividual scientists, would never have known how to do, and the visual resultends up as pure art that is being shown in galleries all over the united states, andnow internationally.i think you could hear in senator gore's voice, as he gave his keynoteaddress, a sense of urgency. i believe that he feels as many of us feel. i just cameback from 12 days in japan. if i felt an urgency before, i certainly feel a greatdeal more urgency now. this is very serious business. i think we have possibly a1 or 2year window as a country to take advantage of some of the lead we havein distributed computing, visualization, and our long tradition of usingsupercomputers in national laboratories.but it will not happen by the normal process of diffusion on the time scalethat we usually use in this country. it must be something more than that. i thinkthat making that extra effort is what senator gore was challenging us to do.the first session of this symposium, "the changing landscape ofsupercomputer technology," is a tutorial given by two of the leading experts onthe technology of supercomputers.first, jack worlton will talk about the various kinds of architectures onefinds in the supercomputer arena, how we have reached the point where we aretoday, and something about the computer industry itself. jack is a lifetimelaboratory fellow of los alamos national laboratory. he has spent 30 years inthe laboratory in a number of key positions. he is now president of worlton andassociates, and he consults and lectures worldwide. he is probably the mostsoughtafter speaker in the world today for teaching people about the technologyof supercomputers. he is also, by the way, one of the world's experts on theactual details of u.s. and japanese competition in this area.he will be followed by steve chen, who studied for his ph.d. from theuniversity of illinois with david kuck and represents one of the brilliant peoplewho have come out of that long tradition at illinois since world war ii inarchitectures and software engineering for supercomputers. he went, after beingat floating point systems, to cray research, inc. he was chief designer of thexmp, which has been one of the bestselling supercomputers to date. he thenbecame senior vice president at cray research, inc. recently, he moved on tobecome the president and chiefintroduction16supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.executive officer of supercomputer systems, inc. steve will be talking about thetechnologies that we should be tracking in the next 5 years and that will makesupercomputers even more super in the years to come.for the symposium's second session, "existing applications ofsupercomputers in industry," we have selected three different corporations thatare using supercomputers in rather different fashions to maintain and increasetheir competitive position in the world's marketplace. of the three, applecomputer, inc. is the one that actually owns its own supercomputer. eastmankodak company has access to supercomputing through the ncsa and is doinginteresting work onsite at kodak with different kinds of machines, and abbottlaboratories is in the process of deciding what to do about supercomputing.our first speaker in the second session is beverly eccles, a group leader incomputational chemistry at abbott laboratories who is on the project teamevaluating supercomputers for abbott. she obtained her ph.d. in theoreticalchemistry from the university of california at irvine, and she has been at abbottfor 2 years. previously she was at the beckman research institute of the city ofhope, where she did image analysis.cliff perry, who represents eastman kodak company in the second session,obtained his ph.d. from purdue university and then became a member of thefaculty at the university of minnesota. for the last 20 years he has been withkodak in a variety of very interesting positions and was until recently the directorof kodak's computational science laboratory. kodak is probably one of the fewcorporations in america that has a computational science laboratory. now he isthe director of the information and computing technologies division, whichoversees kodak's computational science laboratory.larry tesler, whose degree is from stanford university, will discusssupercomputer use at apple computer, inc. he was a member of the legendaryxerox palo alto research center group, that troop of very exceptional peoplewho generated many of the modern ideas about workstationhuman interfaces andhardware construction. he joined apple in 1980 and is currently vice presidentfor advanced technologies.introduction17supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.introduction18supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.part bthe changing landscape ofsupercomputer technology 19supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved. 20supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.4existing conditionsjack worltonlos alamos national laboratoryandworlton and associatescharacteristics of highperformance computerstypes of computersthe wide variety of computers used in computational science andengineering can be illustrated by plotting the performance and cost of all of thecomputers available from the computing industry. the resulting band ofproducts, as illustrated in figure 4.1, ranges over microcomputers,minicomputers, superminicomputers, minisupercomputers, highend mainframes,and supercomputers.there is a band rather than just a line because there is a range ofperformance that is available for a given cost and a range of costs for a givenperformance. in general, the lower edge of the band is represented by the olderproducts, and the upper edge of the band is represented by the newer products.that is, newer products have a higher performance for a given cost and a lowercost for a given performance than do older products.one of the tongueincheek definitions of supercomputers is that asupercomputer is any computer that costs $10 million. in fact, currentsupercomputer prices range from about $1 million to $20 million, but in terms ofconstant dollars, the $10 million average is a useful rule of thumb over about 3decades. for example, the ibm 704 in the mid1950s cost $2 million to $3million and operated at about 10,000 operations per second. however, if these1950s dollars are converted to 1980s dollars,existing conditions21supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.figure 4.1performance and cost of available types of computers. (reprinted, bypermission, from worlton and associates.)the cost of the ibm 704 is approximately $10 million. one way to view thisdevelopment is that over the past 30 years, for a constant cost the performance ofsupercomputers has increased by a factor of 10,000; or, put another way, for aconstant performance the cost has decreased by a factor of 10,000. this isequivalent to an annually compounded rate of improvement in the ratio ofperformance to cost of about 36 percent per year, a factor of 2 every 2.25 years,or a factor of 10 every 7.5 years.taxonomy of highperformance computersif we examine in greater detail the highperformance computers, wedistinguish at the first level three categories of computers: general purpose,special purpose, and research (table 4.1).general purposethe generalpurpose highperformance computers include thesupercomputers, highend mainframes, and minisupercomputers. thesupercomputers (such as the cray ymp and eta10) are distinguished by theirrelatively higher execution rates (sustained rates of about 108 to 109 operationsper second), their larger memory capacities (up to several hundredexisting conditions22supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.table 4.1 taxonomy of highperformance computerscategoryinstancesgeneral purposesupercomputershighend mainframesminisupercomputersspecial purposesinglepurposebit processorsmassively parallelsystolicresearchnationalindustryuniversitymillion 64bit words), their highspeed inputoutput systems, and their highprices. the prices of the mainframes (such as the ibm 3090, amdahl 5890, andcontrol data corporation 990) are about the same as those of thesupercomputers, ranging from about $1 million to $20 million, but theirperformance is typically about a factor of 5 below that of supercomputers; also,their memory capacities are typically lower and their inputoutput systems areslower. the minisupercomputers (such as the convex cseries, the alliant fx/8,and the scs40) have performances that are typically 5 to 10 times lower thanthose of supercomputers and prices in the range of $100,000 to $1 million,although some of the larger systems have prices in the $1 million to $2 millionrange.special purposethere is no clear line of distinction between general and specialpurposecomputers; rather, the distinction is relative. that is, the generalpurposecomputers have relatively more applications that they can execute with highefficiency than do the specialpurpose computers. it has been known for decadesthat a tradeoff is available between execution rate and generality; that is, for agiven component technology, specialpurpose computers can be made that arefaster than generalpurpose computers. however, the cost of a specialpurposecomputer is in addition to, not instead of, the cost of a generalpurpose computer,because users cannot afford to acquire a specialpurpose computer for everyapplication and therefore must always have generalpurpose computers that arespecialized for particular applications through application software. thus the roleofexisting conditions23supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.specialpurpose computers is to augment, not to replace, generalpurposecomputers.the most limited purpose of design is found in singlepurpose computerssuch as logic emulators. while limited in the range of problems they can solve,these are highly effective when the workload is highly specialized. bit processorsare designed for applications in which the data can be expressed by a single bit,such as image processing. massively parallel computers attempt to achieve highperformance by using a very large number of slow processors. finally, thesystolic computers implement an algorithm by pumping data through a series ofidentical functional units; for example, a systolic computer that implemented amatrix multiply would have an array of identical multiplyadd processors thatcommunicate their partial results to one another.it should be observed that the generality of many of the specialpurposeprocessors is being broadened by both hardware and software techniques in thenewer designs.researchresearch computers are designed to investigate how to make bettercomputers rather than for applications in science and engineering. some of theseare supported at the national level, the best known being japan's superspeedcomputer project, which combines the efforts of japan's six largest computercompanies to design a supercomputer that will operate at 1010 floatingpointoperations per second and will be ready for demonstration by march 1990. thisproject is funded at the $100 million level by the ministry of international madeand industry in japan. there are similar projects in europe, such as the alveyproject in great britain, the marisis project in france, and the suprenum projectin west germany. in the american scheme of things, these types of projects areconducted in universities; examples include the cedar project at the university ofillinois, the ultra project at new york university, and the hypercube project atthe california institute of technology. finally, industrial firms are also buildingresearch computers, including the rp3, the gf11, and the tf1 projects at ibm.the scientific computing environmentša matrixit is useful to think of the scientific computing environment in terms of amatrix in which we match the different types of computers against the genericinformation technologies, of which there are four: (1) processing, in which wetransform an input into an output according to an algorithm; (2) storage, in whichwe transfer information over time; (3) communications, in which we transferinformation over space; and (4) the human interface, inexisting conditions24supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.figure 4.2the scientific computing environment. note: lan, localarea network; wan,widearea network. (reprinted, by permission, from worlton and associates.)which we present information to and accept information from the user. wenow classify the types of computers by the level of sharing among the users: (1)the personal resources that are not shared (personal computers with prices lessthan $10,000 and workstations with prices between $10,000 and $100,000); (2)midrange computers that are typically shared by a few tens of users (minisupersand superminis with prices in the range of $100,000 to $1 million); and (3) thelargescale systems that are typically shared by a few hundred users(supercomputers and highend mainframes with prices in excess of $1 million).by matching the generic information technologies against these categories ofcomputers, we thereby create a 12way taxonomy that defines the scientificcomputing environment, as illustrated in figure 4.2. some major trends andcharacteristics of the technologies are described below.trends in the generic information technologiesprocessingfor all three types of computers, the processing power for a given cost isincreasing, and the cost of a given level of processing power is decreasing. thereare no fundamental, technological, or economical limits that will prevent thesetrends from continuing over at least the next decade. these trends may slow downsomewhat, but they will continue.existing conditions25supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.storagethere is no single storage technology that meets all requirements for fastaccess time and low cost, so a hierarchy of technologies is used, includingsemiconductors for the main storage, magnetic disks for secondary storage, andmagnetic tapes for archival storage. progress has been most rapid in the mainmemory, where access times are now just a few tens of nanoseconds (ananosecond is 109 s). the access time to magnetic disks is typically a few tens ofmilliseconds (a millisecond is 103 s, the time being constrained by the physicalrotation of the disks. thus there is a 6ordersofmagnitude gap between theaccess times of main memory and those of disks. this causes unacceptable delayswhen storage requirements exceed the capacity of main memory, so that a newlevel in the storage hierarchy has been created, often called the solidstate disk(ssd), with access times of tens of microseconds (a microsecond is 106 s). thethird level in the storage system, the archival level of storage, has been andremains a problem, because magnetic tape is not an ideal medium for very largearchives: it has low volumetric efficiency, its shelf life is limited, and it can beerased. optical storage technology is being developed in both disk and tapeformats, but so far there are no recording standards for optical media, and mostusers are unwilling to record their archives on a nonstandard medium; thusmagnetic tape will continue to be used for archival storage by most scientificcomputing centers until recording standards are developed for optical disks andtapes. an exception will be found in specialized applications where theadvantages of optical media exceed their disadvantages.communicationscommunication both within and among computers and their users continuesto grow in importance. three major trends are evident: (1) the rate of informationtransmission is increasing, (2) the cost per bit of information transmitted isdecreasing, and (3) the connectivityšthe number of users who have access todata communications portsšis increasing.the point about connectivity is a matter of urgent management concern. in1981 los alamos national laboratory conducted a strategic planning exercisethat led to the conclusion that data communications should be provided to allemployees as a utility comparable to heat, light, power, and telephones. thispolicy was called the data communication utility. at that time the laboratory hadonly about 1000 communications ports for a staff of about 7000, and new portswere being installed at the rate of only 200 per year, so that providing the fullstaff with data communications would have required about 30 years. however,the pace of port installation has been increased to 800 to 1000 per year and thereare now betweenexisting conditions26supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.5000 and 6000 ports for the staff, so that the data communication utility willsoon be a reality at los alamos. the data communication utility is a policy thatshould be considered by all organizations.the networks to which data communications ports provide access includelocalarea networks (lans) that span a building, site networks that span a wholesite, and widearea networks (wans) that span continents. these networkspermit transfer of information among computing resources and among an evergrowing set of scientists and engineers who communicate with each otherelectronically.interfacethe function of the human interface in information technology is twofold:(1) to provide transparent access to the information technologies, and (2) toprovide visualization tools that lead to the insights that are the very purpose ofcomputational science and engineering.it is a fundamental principle of technology development that all technologiesstrive toward the condition of transparency, that is, the ability to achieve thefunction of the technology without specifying how the function is achieved. forexample, the driver of an automobile is concerned primarily with just fourdevices, the steering wheel, the gear shift, the accelerator, and the brake pedal,but not with the thousands of parts these interface devices control. similarly, thecomputer user should be concerned only with the nature of the problem beingsolved and not with specifying how each step in the solution is to beaccomplished. many computer systems are deficient not because they lack thecomputational power the users require, but because they are not transparentštheyrequire the user to specify in great detail how the computation should proceed.future systems will increasingly allow users to specify what is to be done, nothow. for example, parallelprocessing computers are being developed today, butthese will not be mature until parallel execution is transparent to the user.visualization tools are now recognized as being as important as processing,storage, and communications in computational science and engineering. richardhamming's dictum that the purpose of computing is insight, not numbers, is therationale for visualization. the visualization technologies include graphicsterminals, microfilm, video tape, system software, and, more recently, theinterdisciplinary collaboration between computer scientists and artists to presentinformation in powerful and easily perceived forms.modes of operationmost science and engineering computer centers offer their users this wholeenvironment, and the users then choose from among the resourcesexisting conditions27supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.those that best meet their needs. for example, some users prefer to use aworkstation for development of programs, visualization of results, and executionof smallscale problems; when problems grow beyond the ability of theworkstation to provide an acceptable response time, the problems are thensubmitted to a supercomputer. other users find the local control and ease of useof midrange computers desirable as an intermediate step between workstationsand supercomputers. an important characteristic of the computing environmentshould be that users have a uniform interface across all three types of computers,so that they can move applications among the types of computers withoutsignificant conversion effort. this is the main reason for the growing popularityof the unix operating system: unix is available on all three generic types ofcomputing systems and hence can provide a relatively seamless interface amongthem.figure 4.3the rationale for highperformance computing. (reprinted, by permission, fromworlton and associates.)rationale for use of supercomputerswhy do scientists and engineers use powerful but expensive supercomputersinstead of less powerful but less expensive midrange and personal computers?and why do they use supercomputers in addition to midrange and personalcomputers? this discussion has to do with the rationale for supercomputing,which can be viewed from two perspectives: that of the user and that of themanager, as illustrated in figure 4.3.existing conditions28supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.requirements for supercomputing: the user perspectivefrom the point of view of the user, supercomputers are required to solvehighly complex problems in a timely manner. if the computing environment doesnot have complex problems to solve, then supercomputers are not required; also,if there are complex problems to be solved but there is no time urgency for theirsolution, then again, supercomputers are not required. however, if bothcomplexity and timeliness are important to an organization, then it is amanagement error not to provide the most powerful computing resourcesavailable, and these are by definition the supercomputers.we can analyze computing requirements of all kinds by using a simple butpowerful dimensional analysis in which we decompose execution rate into twoexplanatory variables, complexity and response time:complexity has units of operations per problem, where operations refers tothe ordinary things computers došadd, subtract, multiply, and so on. responsetime refers to the time required to solve a problem, in units of seconds perproblem. because this model is dimensionally correct, we can use it as analgebraic equation in which any two of the variables uniquely determine thethird. for example, we can specify the complexity of the problems we want tosolve and the response time required, and this determines the execution rate of thecomputer necessary to meet these requirements. alternatively, for a givencomputer we can specify either problem complexity or response time, but notboth.given this model we can see that there will be requirements for highexecution rates if the complexity of the problem is large, or if the response timerequired is small, or both. it is the need to solve problems with everlargercomplexities and evershorter response times that is driving the unrelentingdemands for higher execution rates in supercomputers. historians tell us it isinherent in the nature of science, technology, and engineering that they grow incomplexity. for example, a tour through the smithsonian's air and spacemuseum provides a perspective not only on the history of aerospace but also oncomplexity. the wright brothers' flyer was a relatively simple device comparedto the dc3, the dc3 was not as complex as jet aircraft, and jet aircraft are notas complex as aerospace vehicles. we can quantify this growing complexity: inthe decade from 1910 to 1920 it required only on the order of 10,000 engineeringhours toexisting conditions29supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.design large aircraft, but in the decade of the 1970s this metric had grown toabout 10,000,000 engineering hours.similarly, shorter response times are becoming more critical because ofintense international competition. in the september 1988 issue of ieee spectrum,an executive of a semiconductor firm is quoted as saying, ''the limit for the timeit takes to design an integrated circuit is a year. any longer and it will already beout of date when it is introduced'' (p. 33). success in the university, in industry,and in government is often determined not only by the results produced but alsoby the time scale on which the results are produced.we can quantify this relationship with a nomograph, that is, a diagram thatshows a correct relationship among the variables for any straight line drawnacross it, as illustrated in figure 4.4. the scale of response times decreases as wego higher in the diagram, the scale of complexity increases as we go higher, andthe sustained execution rate links the two other scales. suppose we want to solve aproblem that has a complexity of 109 operations, and we desire a 15min responsetime, which is about 1000 s. the required execution rate is then 109/103 = 106operations per second. however, if we required the solution on an interactivetime scale, for example 10 s, then the required execution rate would be 108,operations per secondš100 times faster. alternatively, if the 15min responsetime were satisfactory but the complexity of the problem were 1012 operations,then the required execution rate would be 109 operations per second. this lastexample is taken from the requirements at national aeronautics and spaceadministration ames for the numerical aerodynamic simulator. figure 4.4shows that it is the combination of high complexity and short response times thatforces the use of highperformance computers.the largest problems that are being solved on current supercomputers have acomplexity between 1013 and 1014 total operations. however, critical problems,such as longrange climate studies, that have complexities that are orders ofmagnitude more complex await the development of more powerfulsupercomputers.the dimensions of arithmetic complexitywe can analyze complexity in more detail by continuing our dimensionalanalysis; we decompose complexity into a 4factor formula:complexity (operations per problem) = avtgwhereexisting conditions30supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.figure 4.4nomograph of computational science and engineering. (reprinted, bypermission, from worlton and associates.)g = geometry [points per time step]t = time [time steps per problem]v = variables [number of variables computed per point]a = algorithm [operations per variable].geometrical complexity comes from the number of dimensions and theresolution (number of points) within each dimension, dimension meaning not onlythe space dimensions but also any variable that must be systematically variedover some range. for example, if we are tracking particles that do different thingsdepending on their energy and angle of flight, then energy and angularity aretreated as additional dimensions. if the problem is time dependent, thencalculations must be performed at all of the geometrical points for each time step.for each point in spacetime, a certain number of variables are computed, and foreach variable, a certain numberexisting conditions31supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.of operations must be performed in its computation. thus total complexity is theproduct of all of these 4factors.in practice, one of the difficult decisions in computational science andengineering is the tradeoff among these variables. there is a practical limit ontotal complexity, so that as we increase any one of these factors we must decreaseanother. for example, as we increase geometrical complexity, we may have touse simpler physics; or if we put in better physics, we may have to decrease thenumber of dimensions or the resolution.an example of complexitythe following example illustrates the structure of a typical supercomputercalculation:spatial resolution g = 104 mesh points (100 × 100)time resolution t = 4000 time stepsvariables v = 100 variables per pointalgorithm a = 30 operations per variable per time step per pointthe total complexity is the product of these factors, equal to 1.2 × 1011 totaloperations. the problem was executed on a cray1 supercomputer at los alamosnational laboratory in 1983 at an execution rate of about 20 million operationsper second, so the response time was 1.2 × 1011/2.0 × 107 = 6000 s = 1.67 h.the scale of response timesthere is a broad range of requirements for response times, from only a fewseconds up to about 100 h. problems exceeding 100 h are usually deemed to beintractable, both because they delay progress on the part of the user for so longand because they consume so much of the computing resource that they denyaccess to other users. a problem that requires 100 h of total execution timecannot be executed nonstop but must be executed at a rate of a few hours pernight, so that the realtime requirement for completion is weeks to months. thescale of response times can be thought of as follows: interactive. some response times must be only a few seconds, andideally the delay should be imperceptible to the user for such simpletasks as entering a line of instruction during code development. preproduction and postproduction. before a long calculation isexecuted, it is often submitted for a short run to determine if the input iscorrect, to avoid wasting time. also, after a longer production run ismade, it is often necessary to analyze the results in calculations that aretypicallyexisting conditions32supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.minutes to tens of minutes long and that are run during the daytime.smallscale production calculations are also of this size. production. problems that require hours of run time are run at nightduring the socalled production period. thus any problem of this size canyield only one result per day to the user. benchmark. at rare intervals, problems of higher complexity than theproduction runs are executed to check on the sensitivity of productionruns to higher resolution or better science. the run times of benchmarkcalculations are typically in the range of 10 to 100 h. intractable. calculations requiring over 100 h are so costly in both theexpenditure of resources and in delays in real time that they are rarely, ifever, performed.figure 4.5scale of response times. note: solid square, interactive; open triangle,preproduction and postproduction; solid triangle, production; open circle,benchmark; and closed circle, intractable. (reprinted, by permission, fromworlton and associates.)to understand the importance of more powerful computers, consider theeffect of executing the above scale of problems on a computer that is 100 timesfaster than the original computer, as illustrated in figure 4.5.for ease of reference, we refer to the original computer as 1x and to thefaster computer as 100x. problems that would require a full year of calculationon a 1x, and hence would be unthinkable, could be executedexisting conditions33supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.as benchmark calculations on the 100x in 87 h. intractable problems that wouldrequire nonstop runs of 1 month (720 h) on the 1x could be done as routineproduction jobs on the 100x in about 7 h. the 50h benchmark calculations onthe 1x could be completed in just 30 min on the 100x. fivehour production runson the 1x could be done in only 3 min on the 100x, and 15min calculations onthe 1x could be completed in just 10 s on the 100x. this comparison is relevantto the relative performance of midrange computers and supercomputers and tothe relative performance of current and future supercomputers.benefits of supercomputing: the managerial perspectivethe above discussion of requirements is of interest primarily to users whohave to solve complex problems in a timely manner, and this is, of course, animportant part of why supercomputers are used. however, the issues of problemcomplexity and solution time are only of peripheral interest to managers.managers are interested in the benefits to the organization of making thesubstantial investment in these computational resources. users sometimes makethe mistake of trying to explain to their managers why they need supercomputersin terms of problem complexity and response time, when what the manager wantsto know is, what can we do with supercomputers that we couldn't do withoutthem? what savings in time and money can we achieve? how will their useaffect the productivity of the organization? five benefits are briefly outlined asfollows: feasibility. the ability to overcome the limitations of experimental andtheoretical methods through computational science and engineeringmakes it possible to solve problems that are intractable by conventionalmethods alone. savings. by using computational science and engineering to guideexperimentation, costly and timeconsuming experiments can be focusedon the most productive areas, thereby economizing on manpower, time,and budgets. failure analysis. by identifying failure modes early in a project in theworld of information rather than later in the production or operationalphase, the consequences of failures can often be avoided. the spaceshuttle challenger is a tragic example, but failures of all kinds need tobe avoided to minimize delays, wasted resources, and embarrassment toan organization. quality assurance. if the maximum set of options possible within theconstraints of the time of a project are explored, optimum quality resultscan be produced. in the modern world of intense internationalcompetition, high quality is essential for survival.existing conditions34supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved. productivity. by increasing output and reducing time and cost, thesupercomputer increases the productivity of an organization.trends in supercomputer technologythis analysis of trends in supercomputer technology focuses primarily onsystem hardware, and within that topic, on trends in the technologies ofprocessing and storage. the equally important topics of system and applicationssoftware, and the technologies of communications and the human interface withinsystem hardware, are beyond the scope of this analysis.trend in execution rate at los alamos national laboratoryhighperformance computinglos alamos national laboratory has been, and continues to be, one of theworld's leading organizations in the application of highperformance computers toscience and engineering, and it is instructive to analyze the history of computingat los alamos for insights into how highperformance computer technology hasevolved. this history is illustrated in figure 4.6.when the laboratory was first established in 1943, there were no electroniccomputers, so punchedcard accounting machines were used in early r&defforts; these operated at about one operation per second. as electroniccomputers became available, the most powerful of these were installed at losalamos; figure 4.6 illustrates the approximate sustained execution rate of thefastest of these computers (in units of operations per second normalized to thecdc 7600 for administrative purposes). the fastest computer currently installedat los alamos is the cray ymp, which has a sustained execution rate on theorder of 109 operations per second. thus there has been an increase of some 8 to 9orders of magnitude in the execution rate of computers at los alamos in the past45 years. whether future improvements in the execution rate of supercomputerswill match that of the past decades is a matter of deep concern to computationalscientists and engineers.serial versus parallel processorsall of the computers installed at los alamos through the cray1, the first ofwhich was installed at los alamos in 1976, were serial processorsšthat is, theywere designed to execute only a single stream of instructions in sequential mode.it is clear from the shape of the trend line in figure 4.6 that there was a gradualslowing of the development of these serialexisting conditions35supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.designs, but in 1982 a new trend line began with the installation of the first of theparallelprocessor supercomputers, the cray xmp/2 with two vector processors,and later models in that line with four and eight vector processors. futureprospects for faster supercomputers will be based not only on improvements incomponent technology and the architecture of single processors, but also on theincreasing number of processors used in supercomputers. whereas the currentlyavailable supercomputers use up to 8 processors, supercomputers developedduring the period 1990 to 1995 will use from 16 to 64 processors. the dramaticincrease in execution rate for the projected points in figure 4.6 is expected tocome from both the increase in the number of processors and the development ofhigherperformance logic circuits (see below, "trend in highspeed logictechnologies").figure 4.6history and projection of execution rate at los alamos national laboratory.note: solid circle, serial; solid triangle, parallel; open triangle, projected.(reprinted, by permission, from worlton and associates.)existing conditions36supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.dimensional analysis of execution ratewe can systematically explore the prospects for designing fastersupercomputers by using a dimensional analysis of execution rate. wedecompose the units of execution rate (operations per second) into threeexplanatory variables: (1) the cycle time, in units of seconds per cycle; (2) designefficiency, in units of results per cycle per processing element (pe); and (3) thedegree of parallelism, in units of the number of pes.there are three methods of increasing overall execution rate: (1) decreasethe cycle time, (2) increase the singleprocessor efficiency, or (3) increase thenumber of processors. we will explore the trend for each of these variables.trend in component densitiesboth speed and cost improvements in both logic and memory componentsdepend on increases in the density of components per chip, and the general trendlines are shown in figure 4.7.during the period 1959 to 1972, the density of components per chipincreased by about a factor of 2 every 2 years, a trend first identified by gordonmoore of intel and known as moore's law. it is useful to track these trends interms of their quadrupling time, rather than their doubling time, because memorychip generations increase by a factor of 4. thus, in this early period, the densityof components per chip increased by a factor of 4 every 2 years. since 1972 thedensity of components per chip has been quadrupling about every 3 years and isexpected to slow down to a quadrupling every 4 years sometime in the early1990s. this pattern of change is a "piecewise exponential," that is, a series ofexponentials in which the successive exponents become gradually smaller,ultimately approaching a limit. this is a pattern commonly found in the evolutionof electronics and highperformance computing, including semiconductormemory, magnetic disks, and the execution rates of supercomputers. bothdynamic and static ram memories follow this pattern, with a quadrupling periodof 3 years, whereas the quadrupling period of magnetic disk density is muchlonger, about 8 years.trend in cycle timesa scatter diagram of the cycle times of leadingedge supercomputers sincethe mid1960s is shown in figure 4.8 there has been a decrease inexisting conditions37supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.supercomputer cycle time from 100 ns in the cdc 6600 in 1964, to 27.5 ns in thecdc 7600 in 1969, to 12.5 ns in the cray1 in 1976, to 4 ns in the cray2 in1985, and there is a projected decrease to the 2 to 3 ns range in severalcomputers scheduled for delivery in 1990. the recent trend in the leading edge ofcycletime development shows a decrease of about a factor of 2 in 4 to 5 years.this is a fairly slow rate of improvement compared to the improvements inearlier decades. for example, in the period 1955 to 1970, cycle times improvedfrom about 12 µs to 27.5 ns, a factor of 436; however, in the next 15 years, cycletimes improved from 27.5 ns to 4 ns, a factor of only about 7. this is a majorreason for the slowdown in the growth of serialprocessor execution rates. thosecomputers having cycle times that fall above the leading edge of this trend haveattempted to use architectural features to compensate for their slower cycle times.figure 4.7trend in component density. (adapted from james d. meindl, 1987, chips foradvanced computing, sci. am. 256:78œ88.)trend in highspeed logic technologiesthe prospects for faster logic technologies are illustrated in figure 4.9,which shows the gate delay (in nanoseconds) and the power per gate (inmilliwatts) for several highspeed logic technologies. technologies to bepreferred are those with both low gate delays and low power dissipation per gate.not shown in figure 4.9 are other important characteristics of logic technologiessuch as gate density and cost. the traditional logic technology used for highperformance computers has been bipolar emittercoupled logic (ecl), which isthe fastest of the silicon technologies and is used in most modernsupercomputers. two other logic technologies being usedexisting conditions38supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.in recent designs are complementary metaloxide semiconductor (cmos) andgallium arsenide (gaas).figure 4.8trend in cycle times of supercomputers. (reprinted, by permission, fromworlton and associates.)figure 4.9highspeed logic technologies. note: cmos, complementary metaloxidesemiconductor, ecl, bipolar emittercoupled logic; gaas, gallium arsenide;hemt, highelectron mobility transfer, jj, josephson junction. (courtesyhiroshi kashiwagi, electrotechnical laboratory, japan.)cmos. the cmos technology being used in the eta10 has two advantagesrelative to ecl: lower power dissipation and higher gate density. the lowerpower dissipation makes it possible to achieve high densities of gates per chip;the higher gate density makes it possible to incorporate more functionsexisting conditions39supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.per chip and therefore avoid transmission delays from chip to chip. cmos has amajor disadvantage as a logic technology for supercomputers: its gate delay isslower than that for ecl. however, its relatively slow speed can be partiallycompensated for by cooling it with liquid nitrogen to a nominal temperature of 77k, which lowers the gate delay. the cycle times of the eta10 range from 24 nsdown to 7 ns.gaas. gallium arsenide has two advantages relative to ecl: it has a lowergate delay and lower power dissipation. the lower power dissipation implies apotential for increases in packing density without causing heat dissipationproblems and therefore even further speed increases. this technology also hastwo disadvantages: higher cost and lower gate density per chip. the gate densityof gaas is projected to increase dramatically compared to the gate density oftraditional logic technologies, as illustrated in figure 4.10. if this projectionshould prove to be true or even approximately true, the gate density disadvantageof gaas would no longer exist. the gaas industry is maturing rapidly, and thecost per gate is projected to fall as increasing gate densities are achieved. galliumarsenide is being used in the design of the cray3 for delivery in about 1990, with aprojected cycle time of 2 ns.hemt. the highelectron mobility transistor (hemt) technology is analuminumdoped version of gaas that is cooled to liquid nitrogen temperatures.it is more complex than gaas and its development will take longer, probably intothe mid1990s. if it is developed successfully, it may offer cycle times evenlower than those offered by gaas.other logic technologies. josephson junction technology is asuperconducting technology that was investigated in the 1970s and early 1980sbut then was dropped by most companies because of its complexity and alsobecause its performance characteristics could be achieved with other technologiesthat were more tractable; however, japanese companies continue to study thistechnology. optical, molecular, and quantumtunneling technologies are beingstudied for their potential for highperformance computers, but they seem to offerfew prospects for the near term.summary. for the immediate future, it is expected that most supercomputersand other highperformance computers will use ecl and cmos logictechnology, but if the implementation of the cray3 in gaas is successful, itcould create a guidepost for others to follow. the leadingedge cycle time shouldbe about 2 ns around 1990, and cycle times of 1 ns should appear insupercomputers by 1995.existing conditions40supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.figure 4.10projected trends in gate density of highspeed logic circuits. (reprinted, bypermission, from bernard c. cole, gallium arsenide starts getting some respect,electronics, june 1988, p. 41.)milestones in processor architecturearchitectural efficiencyefficiency in computer architecture is measured in units of results generatedper clock cycle. improvements in computer architecture that have resulted inhigher efficiency are due largely to concurrency, that is, to doing more things atonce. some selected examples are illustrated in figure 4.11.in the mid1950s in computers such as the ibm 704, instructions wereexecuted in a sequential scalar mode; that is, they specified only one operation onone pair of operands, and the processing of instructions included a series ofsequential steps: fetching the instruction, decoding it, forming the effectiveaddress, fetching the operand, and then executing the operation. beginning inabout 1960 in computers like the ibm stretch, an instruction lookaheadprovided the ability to fetch and process instruction n + 1 while executinginstruction n. by the mid1960s in computers such as the cdc 6600, multiplefunction units were included that allowed several executions, such as add andmultiply, to be performed concurrently. by about 1970, the operations werespeeded up by pipelining, that is, subdividingexisting conditions41supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.the steps of the operations into substeps and overlapping these substeps for fasterexecution. the 1970s saw the development of vector processors in which thesame operation was applied to many operands or operand pairs, rather than tojust one pair as in the scalar designs. the major supercomputers today use vectorprocessing to achieve high speed. the first generation of vector processors hadmemorytomemory designs; that is, they fetched the vectors from memory andreturned the results to memory. however, this caused long startup times, and thesecond generation of vector processors followed the lead of the cray1 in usingvector registers that allowed faster startup times. most vector designs today usethe registertoregister design. the leading edge of supercomputer architecturetoday is found in designs that incorporate multiple vector processors, or parallelvector designs.figure 4.11some architectural contributions to higher performance. (reprinted, bypermission, from worlton and associates.)balance in vector processor designone of the key issues in vector processing is the balance between the scalarspeed (doing one operation per instruction) and the vector speed (doing manyrepetitive operations per instruction). figure 4.12 compares the merits of twodesigns, one with a relative performance of 10 in the scalar mode and 100 in thevector mode, and another with a relativeexisting conditions42supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.performance of 2.5 in the scalar mode and 400 in the vector mode. the overallperformance of the system is a function of the fractions of the work done in eachof the different modes. for practical applications, the fraction of vector work is inthe range 0.3 to 0.7, so that even though the second design has a higher peakperformance than does the first design, the first design, which is better balanced,is clearly to be preferred.figure 4.12balance in vector processor design. (reprinted, by permission, from worltonand associates.)parallel processingin addition to decreasing cycle time and increasing design efficiency, a thirdapproach to increasing computer speed is through the use of multiple processors,and there are many design issues for these socalled parallel computers: shouldthere be a few fast processors or many slow ones? should the memory be shared,attached to each processor, or both? what kind of interconnect network should beused to provide communications among the processors, and among the processorsand the memory? there are literally thousands of ways to design parallelcomputers, and at the moment there is wide disagreement in the industry aboutwhich are the bestexisting conditions43supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.choices. to analyze future prospects in these options, we turn to a taxonomy ofcomputer architecturesšan intellectual road map of possibilities.flynn's taxonomymichael flynn's taxonomy from the 1960s has been the most commonlyused guide to generic types of architectures. it matches the control concurrencyexpressed in the number of instruction streams (one or many) against theexecution concurrency expressed in the number of data streams that are beingprocessed (one or many), to generate the four types of architectures: (1) singleinstruction, single data (sisd); (2) single instruction, multiple data (simd); (3)multiple instruction, single data (misd); and (4) multiple instruction, multipledata (mimd). this has been a valuable guide for over 20 years, but it is nolonger an adequate guide because it is not comprehensive for all of thearchitectural design options being explored.an expanded taxonomy of architecturesin addition to the types of control concurrencyšserial and parallelšincluded in the flynn taxonomy, a third type is being used, called clustering. in aclustered design, clusters of multipleinstructionstream processors are connectedtogether with global control with access to global memory. also, the types ofinstructions employed in the 1960s were limited to scalar instructions, and thereare now several new options that need to be included in any classificationscheme.we can systematically explore the options for designing instruction types byconsidering a number pair (a,b), where a = the number of operations specified inthe instruction and b = the number of operands (or operand pairs) specified. thereare thus four options, as follows: the (1,1) option defines the scalar type of instruction, in which oneoperation is specified on one pair of operands. the (1,n) option defines the vector type of operation, in which oneoperation is specified, but the operation is applied to many operands orpairs of operands. the (m,1) option defines the systolic type of operation, in which manyoperations are performed on each operand that is fetched from memory. finally, the (m,n) option defines the horizontal type of operation, inwhich many operations are specified on many operands for eachinstruction. the horizontal category is also referred to as very longinstruction word (vliw).we can now create an expanded taxonomy of architectures that matches the 3control categories against the 4 instruction categories to create a 12waytaxonomy, as illustrated in figure 4.13.existing conditions44supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.figure 4.13an expanded taxonomy of computer architectures. notation (a,b): a, number ofoperations specified; b, number of operands (or operand pairs) specified.(reprinted, by permission, from worlton and associates.)historically, through the cdc 7600 in about 1970, most supercomputers hadserialscalar designs; that is, they executed one stream of scalar instructions.firstgeneration vector processors had serialvector designs; that is, they alsoexecuted a single stream of instructions, but the instructions specified vectoroperations. machines of this generation were typified by the cray1 and thecyber 205. in the 1980s, parallelvector computers have been developed in whichmultiple vector processors are interconnected through a communication network.the cray ymp and the cdc/eta10 are examples of computers in this category.the further development of vector designs is expected to be generalized toclusteredvector designs, with clusters of multiple vector processorsinterconnected with global networks. an example of this category is the cedarproject at the university of illinois that uses the alliant fx/8 as the cluster.the parallelscalar category represents those designs that use a large numberof relatively slow processors to attain high performance, such as the ncube/tenand the intel ipsc. these can also be designed with clustering, as found in themyrias2.systolic designs have been largely specialpurpose computers thatimplement a particular algorithm such as a matrix multiply or a fast fouriertransform, and they can be developed in serial, parallel, or clustered form. thewarp system developed by carnegie mellon university is the leading edge ofthis type of design. the horizontal or vliw designs attempt to execute multipleoperations per clock period, and they too can be developed in serial, parallel, orclustered form. examples of the vliw design are found in the cydra5 and themultiflow trace computers.existing conditions45supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.this taxonomy can be extended by subdividing each of the categories todevelop an even more detailed taxonomy.key issues in parallel processingthe trend toward parallel computation is perhaps the major trend to watch inthe next decade of computer architecture, and it is useful to identify some of thekey issues that are being explored in the dozens of commercial and researchparallelprocessor projects. these issues can be thought of in three categories:system software, system hardware, and applications.parallel processing will affect system software in language design,compilers, operating systems, and libraries. for example, should we continue touse fortran to specify parallel processing or should we abandon this old languageand adopt something new? fortran is being modified by adding new constructsthat allow the specification of parallel processing, and no doubt this languagewill continue to be used long into the future because of the huge commitment toapplications that exist for it. compilers are being influenced by parallelprocessing because they need to identify opportunities for parallel execution in atransparent manner, that is, so the user need not be concerned with the details ofhow the parallelism is achieved but only with what is to be done. operatingsystems are now more complex because they need to maintain control andprovide services for many streams of instructions rather than just for one. finally,system libraries are being adapted to use parallel processors.applications will be affected by parallel processing by the need to insertparallel control statements, to develop parallel algorithms, and to rethink themathematical models on which the algorithms are based.one of the key issues in the design of highperformance parallel computersis the tradeoff between speed and parallelism. that is, in principle we couldselect the number of processors to be from as few as one or two to as many asthousands (or indeed millions), and we could select the speed of the individualprocessors to be as slow as a bitserial microcomputer or as fast as asupercomputer, as illustrated in figure 4.14.however, a design with only a few slow processors would be too slow to beof any use as a highperformance computer, and a design with thousands ofsupercomputers in a single system would not be economically feasible, so that thepractical area of tradeoff lies in between. the evolution of parallel processing isfollowing three guideposts.first, the zone of ''largegrain'' parallelism is defined by the use of arelatively small number of fast processors, 1 < p < 100. this form of parallelprocessor design was pioneered by cray research, inc. in 1982 with theintroduction of the cray xmp product line, in which the individualexisting conditions46supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.processors are vector supercomputers in their own right and the number ofprocessors varies from the initial offering of 2 to the current offering of 8 in thecray ymp/8. other vendors have followed this design as a guidepost, includingparallelvector processors offered by control data corporation, ibmcorporation, convex, alliant, the japanese superspeed computer project, andwidely rumored introductions of similar designs from the japanesesupercomputer vendors. the number of processors in this domain is beingexpanded from 8 processors to 16 in the near term, and to 32 and 64 in thegenerations in development for delivery in the early 1990s.figure 4.14the tradeoff between speed and parallelism. (reprinted, by permission, fromworlton and associates.)second, the zone of "mediumgrain" parallelism is defined by the use of alarger number of slower processors, with the number of processors being in therange of 100 < p < 1000. this form of parallelism is typified by the bbnbutterfly, which has 256 processors, and by the myrias sps2, with 512processors.third, the zone of "smallgrain" parallelism is defined by the use of an evenlarger number of processors, p > 1000, each of which is even slower and istypically a bitserial microprocessor. here the number of processors varies from4096 in the amt610 dap, to 16,384 in the goodyear mpp, to 65,536 in theconnection machine.a taxonomy of limits to massive parallelismthe term massive parallelism is loosely applied to any of a variety ofcomputers in which the desired performance is achieved through a veryexisting conditions47supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.large number of relatively slow processors, with the threshold beyond 1000processors often used to mean massive.figure 4.15taxonomy of limits to massive parallelism. (reprinted, by permission, fromalex yuenwai kwok, center for supercomputer research and developmentreport no.679, august 1987.)the ability of a parallel computer to perform efficiently as the number ofprocessors is increased is referred to as scalability. a study of architecturalscalability published at the center for supercomputer research and developmentat the university of illinois presents a taxonomy of the limits to scalability, assummarized in figure 4.15.limits of the first kind are due to algorithmic constraints. for a system withp processors to perform at full effectiveness would require that the number ofavailable tasks equal or exceed the number of processors throughout thecalculation. however, not all algorithms can be so decomposed, so that there is adecomposition limit to scalability. when the number of tasks available is justone, this is referred to as the serial bottleneck, which has very seriousimplications for the effectiveness of massive parallelism.limits of the second kind are due to the implementation details, includinglatencies caused by communications, synchronization, and interference. as tasksare decomposed to finer levels of detail, the communications requirementsincrease in proportion to the level of decomposition, and the amount of actualcommunication latency is dependent on the system design. a secondimplementation limit is caused by the need to integrate the results of thedecomposed tasks through various synchronization techniques. atexisting conditions48supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.the synchronization points, the number of available tasks decreases, so that thenumber of idle processors increases, thus limiting performance. a thirdimplementation limit is caused by interference as the processors attempt to accessshared resources; these resources may include shared memory, sharedcommunication networks, or shared inputoutput systems.following hockney's n1/2 method, it can be shown mathematically that thefraction of parallelism that is required to achieve 50 percent efficiency for aparallelprocessor system is given bywhere p = the number of processors, and hence the fraction of serial workcannot exceedthus as p grows, the domain of applicability becomes inherently morelimited, because p1/2 approaches 1.0 and 1 p1/2 approaches 0 very rapidly. thisdoes not imply that it is impossible to use parallel processors effectively, but itdoes provide a guide to the domain of application, with a smaller number of veryfast processors being more useful for general purposes than a larger number ofslow processors.conclusionsit has been over 300 years since galileo unlocked the door to the world ofempirical science, using a telescope as a key, so to speak. in the year galileo died(1642), isaac newton was born, and he invented a key we call the calculus that heused to unlock the door to the world of theoretical science. and it has been ascant 4 decades since von neumann recognized that powerful electroniccomputers were the key to still another world of science, computational science.in the opening lines of man of la mancha, the author (dale wasserman)invites the audience, "come, enter into the world of my imagination." thesupercomputer, too, bids us enter into the world of imagination. in the real world,we are constrained by limits of time, space, energy, matter, and costs, but in theworld of the supercomputer, these constraints are merely bits of information thatwe can readily manipulate. in the world of the supercomputer, we canexperiment with new designs for such things as aircraft, automobiles,accelerators, weapons, and chemicals; we can experiment with these designsuntil they either perform as we desire or we discover why they fail. byexperimenting in the world of the supercomputer, we can avoid many of thecosts, delays, and failures that would haveexisting conditions49supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.occurred if we had experimented with these designs in the real world. in this newworld we can create previously unthinkable systems such as black holes, stand inspace to watch them perform, and create videotapes of this adventure to take backto the real world.the university, the industry, or the nation that would be a leader in themodern world of intense international competition must master the informationtechnologies, the leading edge of which are the supercomputing technologies. itis imperative for our future success as a nation that we accept the invitationoffered by the supercomputerš"come, enter into the world of . . . imagination."existing conditions50supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.5toward the futuresteve chensupercomputer systems, inc.if jack worlton is a lifetime fellowuser of supercomputers, i have become alongtime pursuer of a dream machine. i have chased this machine for more than10 years. i still have not found the perfect machine to fulfill the users' needs. thishas become very challenging but also very rewarding work.my hope is that some day we can come up with a machine that is about 100times faster than today's machines. this machine, as one of the fundamentaltools, will be used by scientists and engineers in many different disciplines tostudy things they cannot do today.i would like to share with you some of my thoughts on the futuredevelopments in supercomputing and their potential impact. i will speak onlyfrom a designer's point of view.the current stage in supercomputingsupercomputing has come a long way, when viewed from many angles: inspeed, the central processing units (cpus), memory size, input/output (i/o),peripherals, physical size, and software.speedyou have heard about machine clock rate coming down from 100 ns to 50ns, then to 25 ns, 12.5 ns, 6 ns, and 4 ns. and each time the clock rate is reducedby half, the underlying component technology becomes moretoward the future51supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.complex. furthermore, the requirements for data space increase. so the challengewe face in designing the machine gets worse.central processing unitsthe central processing unit (cpu) is the heart of the system. when wecannot get more speed out of a single cpu, we start combining more cpus. butthis is not an easy job either. we cannot just tie many boxes together and makethe machine faster. my favorite analogy: to build a faster racing car, we have todecrease the car size and at the same time have more engines in the chassis. wecannot put in larger engines because the car would become big and clumsy. sofor each generation, we have to invent a smaller engine that runs faster than theprevious one and link together as many engines as possible, such that the car canrun efficiently when all engine power is applied concurrently.we have seen the number of cpus increasing from 1 to 2, to 4, to 8 in amachine. but keep in mind that each cpu has to be faster than the previousgeneration. that makes the development work tough!memory sizewe start with 1 million words per cpu for data space. next we see thewords increasing to 4, then to 8, then to 16 megawords per cpu. the data spaceis increased to allow solving bigger problems as each generation's machinesharness more and faster cpus. we are trying to stay one step ahead of theapplication. unfortunately, sometimes we have felt that we are fighting a losinggame. the memory component designer can only give us a bigger memory chipwith very little improvement in speed. hence the data access time from memorybecomes slower relative to data compute time. we must now figure out all kindsof tricks to compensate for the gap between the memory chip and the cpu speed.input/outputmany years ago an input/output (i/o) channel could run about 1 megabyteper second. this was increased to 10 megabytes per second, and then to 100megabytes per second, which soon will become a standard rate for anythingusable. so the trend is clear. to solve bigger problems of the future, we cannotjust add memory size and cpu power without significantly increasing the i/otransfer rate.peripheralsperipherals are also a serious problem. advances in storage technology arefailing behind the cpu's improvement in terms of capacity and speed.toward the future52supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.ten years ago it was common to have disks with hundreds of megabytes and a1megabytepersecond transfer rate. today, we have gigabyte storage units witha 10megabytepersecond transfer rate. in the meantime, we still have to use asolidstate secondary memory device as a buffer to smooth out the speeddifference between cpus and peripherals.physical sizenot too many people recognize the changes in the physical size ofsupercomputers. many years ago the cdc 6600 filled about 500 square feet offloor space. the crayxmp occupied roughly 100 square feet. the cpumodule of the crayymp is suitcasesized. future products may shrink evenfurther. but that does not mean that such a cpu is easy to design. we cannot justsqueeze everything together. as each generation of machine comes down in size,the heat dissipation becomes harder to deal with. we can increase circuit densityin the chip, but we cannot proportionally reduce the power per gate.for example, a suitcasesized supercomputer may dissipate a couple ofthousand watts of power. we may be able to put it on the desktop, but we willhave an instant meltdown in case of a cooling malfunctionšit will go rightthrough the table. we are dealing with a fantastic problem. it's no small designchallenge to try to keep a supercomputer cool.softwareno one paid attention to software initially. most people were thinking aboutsupercomputers as just pieces of hardware. the user was forced to figure out howto use it and then handcode to optimize everything. later on we had a littleprimitive compiler software. then slowly, people started to recognize that thiswas not good enough anymore. productionquality compiler software wasdeveloped for vector processing over the past 10 years. user expectations forsoftware functionality and performance features continue to rise as more andmore supercomputers become available and are widely used.systemslet's view supercomputer development from a different perspective toappreciate how far we have come. when we look at the 10 year period from 1955to 1965, we can see that the cdc 6600 was a dominant factor in thesupercomputer arena, with 1 million to 10 million floatingpoint operations persecond.toward the future53supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.in the period 1965 to 1975, the cdc 7600, the ti asc, the burroughs bsp,and the illiac iv were developed. they reached from 10 million to 100 millionfloatingpoint operations per second. the cdc 7600 was the major workhorseduring this time period.from 1975 to 1985, thanks to seymour cray, a new machine took the lead.cray created the cray1 architecture to take advantage of extensive pipelinevector processing. in addition, supercomputer systems became more reliable. themean time between failures jumped from 10 hours to 100 hours and then to 1000hoursša viable product for use in commercial industry. after the crayxmpwas introduced, applications expanded rapidly, from pure laboratory research tovarious commercial product areas.during this time, more machines and manufacturers entered the market: thecray2, the cdc cyber205, and also, from overseas, the fujitsu, hitachi, andnec models. these machines generally reached from 100 million to 1 billionfloatingpoint operations per second. many more players have joined in becausethey see the importance of supercomputing, not only in the computer industryitself, but also in its wide effects on many key industry applications.personally, i have had the good fortune to work with two of the bestdesigners in the world, dave kuck and seymour cray. i have learned a lot fromthem. dave kuck inspired me with the illiac iv and with the followonburroughs bsp project. these projects gave me a deeper inside view of thesystem and software areas. i was also pleased to be able to join cray research.seymour cray was a good model of the best designer in the hardware andpackaging areas. finally, i was lucky to have the opportunity to participate indesigning the crayxmp and the ymp, to try my first foot in the water.the next stage in supercomputingwhat's in store in the next 10 years? definitely more companies will enterthe competition, but also some will fall out. the important thing is that speed willbe widespread. in the highestperformance arena, instead of going 10 timesfaster, the range will increase to 100 times faster. we will see machines with 32to 256 cpus in production use. machine speed will reach between 1 billion and100 billion floatingpoint operations per second. this is based on the technologyas far as we can see, barring any major breakthroughs.even this may not be fast enough. the director of the national center foratmospheric research, bill buzbee, once told me that the next generation ofocean problems may take about 100 to 1000 hours of current supercomputertime. i couldn't even comprehend the problem he wastoward the future54supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.describing. but the problem definitely cannot be solved today. we need tocontinue to push supercomputer technology forward in order to fulfill thoserequirements.my personal goal in the future is to develop such a computational engine forscientists and engineers to open new frontiers in science and industry, similar tothose made possible by the electron microscope and by steam and gaspoweredengines in earlier days.i have discovered that developing such a machine is not an easy jobanymore. no single person or single company can do it alone. we must dependon various technologiesšcomponent, software, and applicationšto advance in abalanced way. we need to take advantage of every technology we can get andstretch to move all these areas ahead.parallel processing environmentwe are going into the arena of parallel processing, and it is just a matter oftime before people will learn how to do it. i know it is painful. but we havemoved from assembly language to fortran. we took a long time to get there andnow fortran may never die. now we must move from fortran to parallel fortran.it took about 10 years to grow from serial fortran into vector fortran, and now itmay take another 10 years to go from vector to parallel fortran. but if we don'tstart now, we may never be able to take advantage of the performance of futuremachines.so we see where the train is going. today, and in the near future, we willhave in production from 1 to 16processor, highperformance machines. but wealso have seen experimental or developmental machines that have 32 to 256processors or even 1000 processors. right now such machines are in the researchand development stagešthe critical task is to study how to use them. becauseeach processor is quite slow, these machines are not used in production forgeneral applications.our goal is to move gradually toward more and faster processors, whilemaintaining a consistent system architecture. this approach will ensure that nousers will suffer a degradation of performance in running their existingproduction codes on the nextgeneration, more parallel machines when theybecome available. in the meantime, as users gain experience in developing moreparallel application algorithms, they will be able to explore higher performancethrough the added number of processors. i believe this is a sensible approach toprotect the users' software investment and, at the same time, induce the longtermdevelopment of parallel applications.next, let us focus on how the three key technology areasšcomponent,system, and applicationšmay proceed in developing a future highperformancesupercomputer.toward the future55supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.component technology developmentwe will stretch the currently available component technology. we mustcombine improvements in many elements to enhance the design of the machine.device speeddevice speeds have come down from 1 ns to 0.5 ns, and then to 250 ps and125 ps. they may even come down to the 50ps range. complementary metaloxide semiconductor (cmos), gallium arsenide (gaas), and bipolar devices areall viable. each has its own advantages and disadvantages.circuit densitydepending on the device type, today's circuit density is approaching the 1kgate level for gaas, the 10 kgate level for bipolar, and the 100 kgate levelfor cmos. in the future, we may see even largerscale integrated circuits. howusable are these big chips? bigger doesn't always mean better. the advantage ofthese superchips depends on the tradeoff of speed, power, circuit complexity,and overall system considerations.metal interconnectas circuit density increases, more transistors have to be connected in arelatively small and expensive silicon area. one way to keep the chip size downis to make the interconnect metal thinner, so that more signal lines can be placednext to each other. however, a thin metal line may degrade the signal speed andintegrity. as a result, the electronic signal may travel more slowly betweentransistors, even though each transistor's intrinsic switching speed is very fast.and, in the worse case, the signal may not travel far enough before it disappears.furthermore, very thin metal may cause an electromigration problem in ahighspeed (highcurrent) application. this is due to the loss of the electroncarrying property altogether inside the chip, leading to unreliable components.hence we have to develop a better metal interconnect system within theintegrated circuit to allow sufficient currentcarrying capability (for speed), whilemaintaining smaller physical size (for density). the balancing act between speedand density is among the most demanding requirements facing our componentdesigners in the future.substrate materialthe substrate material used to fabricate the printed circuit board is anothercritical factor. the traditional fiberglasslike material may not betoward the future56supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.sufficient for future highspeed and highdensity applications. the electricalproperty of the material may cause the signal to slow down and become noisy andlossy as speed increases. in addition, the mechanical and thermal properties of thematerial are also important in deciding the number of signal layers, the density ofsignal lines, and the compatibility between chip and substrate. we shouldcontinue to enhance current substrate materials and search for new ones to giveus the maximum component packaging density required for a highperformancesystem.power consumptionas i mentioned earlier, for a given technology, power per gate in a chip isnot coming down as fast as we would like it to. we have seen improvements from50 to 100 milliwatts per gate dropping to 10 to 20 milliwatts per gate (a factor of 5reduction), then to 5 to 10 milliwatts per gate (only a factor of 2 reduction). thispowerreduction trend appears to have flattened out. hence, while we areincreasing circuit density, the total power per chip is rising, causing difficultcooling problems at the component and system levels. this is a very critical area,and we need intensive cooperative research efforts with componentmanufacturers in the future.packagingmany of the integrated circuits we are using are getting faster.unfortunately, the performance gains at the component level are deratedsignificantly because of the packaging loss all the way up to the system level.multiple levels of interconnect media, such as printed circuit boards, chipattachments, connectors, backplane wires, and so on, all affect performance. asclock rate increases, component, module, and system packaging becomes a verycritical issue for the total system design.testing and measurementthe bigger the chip, the more pins there are to handle. future chips mighthave 250 to 1000 pins. in addition, they will operate at high speeds and highpower levels. as a result, the problem of testing chips becomes quite complex andexpensive. the same is true for highspeed measurement equipment for circuitboard and system checkout. because a piece of test equipment may cost up to $5million, the availability of costeffective, highperformance test equipment hasbecome a more visible concern.unfortunately, it is getting harder to find suppliers of advanced test andmeasurement equipment to satisfy the performance requirements. companies inthe united states keep dropping out of the market, and some equipment is onlyavailable from overseas. without such equipment, onetoward the future57supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.may have the best design, but one cannot build, test, and ship the machine. sothis is also a very important area to watch.system technology developmentarchitecture conceptsonce we have the best components, the next step is to put the systemtogether in the slickest way. there are many ways we can do this. we hear aboutmany different architectural concepts being explored: single versus multipleprocessors; system throughput versus processor speed; singlelevel versusmultiplelevel parallelism; loosely coupled versus tightly coupled systeminterconnects; monolithic versus distributed memory; and specialpurpose versusgeneralpurpose system design. if one looks underneath the design of futuremachines, it will have one or more of these architecture flavors. however, themost important thing is to design a balanced architecture and provide goodsoftware to support an application or marry applications. the user, in general,should be aware of but not be bothered with the complexity of system design.solution timeas i have mentioned before, the issue now is not how fast one can design amachine to do a + b; the real issue is solution time. in earlier days, peoplecompared different machines by counting how many millions of floatingpoint''add'' or "multiply" operations could be done in a second (mflops). thatmeasurement is similar to the rpm (revolutions per minute) rate of the wheels of aracing car. the rpm rate is not an indicator of how much usable horsepower isavailable when driving on a real road. similarly, the mflops of supercomputersbear no relation to the performance obtainable on real user applications.later, when performance was measured by how fast a machine couldcompute "livermore loops," some people could not differentiate between a realsupercomputing system and a "designer machine" targeted for livermore loops.we should raise ourselves to a higher level. it took me about 5 years ofpreachingši can tell you that's how long i've kept arguing the pointštoconvince users to find a new performance measurement yardstick. fortunately,now they have gone up one notch to use linpack, a set of mathematicalsubroutines for solving linear algebra that is, in general, more usable than just thelivermore loops rate or the peak mflops rate.even so, the performance numbers on linpack are still only an indicatorof the computation time for a small part of the total solution process. to besuccessful in future highperformance parallel processingtoward the future58supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.systems, we must strive for overall system performance and start to talk aboutsolution time. and we need the users' help to define what we mean by solutiontime rather than computation time.for example, threedimensional seismic processing may involve readingmore than 20,000 tapes of earth data before a machine begins to do a + b. theprocess starts by getting data into the machine with the 20,000 tapes and thengenerating the analysis and output to see exactly what is underneath the ground.the whole process may take 3 months of today's supercomputer time, duringwhich only a few days may be spent on numericintensive computational tasks.we need to define this whole process so that we can measure "total time to getresults." we want to make sure the scientists can do their thinking instead ofplaying around with the computer system, or running around the computer room.if i give a machine to an aircraft designer, that person should be able toconstruct a model, pick a grid point, describe the air foil, wing, and tail, and thensimulate it to see if the design is correct. the model should include structure, airflow, and control and other interdisciplinary conditions that have to be satisfied inone design. the designer should be able to define this design process frombeginning to end and measure the machine performance by the total time thatmust be spent completing this design process. this measurement is calledsolution time. the solution time includes all of the following elements: data acquisition/entry; data access/storage; data motion/sharing; data computation/process; and data interpretation/visualization.how to capture the raw and digitized design data, how to store it, and how tomove it efficiently in and out of the disk, solidstate secondary memory, and mainmemory during computation are all essential to the solution process.then, after all that has been done, how quickly can the results beinterpreted? when data can be generated very rapidly, a whole week may berequired to digest the numbers. i would rather see the visual: the undergroundpicture, or the heat flow on the surface of the integrated circuit chip. when thealpha particle hits the electronic device, i want to see the electromagnetic fieldmoving while i watch. i want to be able to start, or stop and restart again, thesimulation process any time i want. while i am simulating an air foil for anaircraft, i want to see if a particular region of the air foil is subject to highpressure or temperature. if i feel something is going wrong, i want to zoom into aparticular area to test it again or try out a different algorithm or analysis. i need tohave antoward the future59supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.interactive design or analysis capability on the system. and last but not the leastimportant of all, i want to be able to complete all this process without leaving myown design station.i hope these examples illustrate the important difference between thecomputation time and the solution time that involves the whole process. whoeverdesigns it, the machine with minimal solution time win be the best system in realapplication.exploitation of parallelismto achieve high performance on future parallel systems, we should workfrom two directions (see box). from the bottomup, we should continue toimprove the compiler techniques to exploit automatically the parallelism in userprograms. this includes extending vector detection capability to the detection ofparallel processable code. from the top down, we should provide system andapplications support in terms of libraries, utilities, and packages, all designed tohelp users prepare their applications to get the most performance out of theparallelism existing at the highest level.one way to think of a parallel application in the future is as a multipledomain approach. we have many, many processors at our disposal. how do wedecompose a problem and make it 99 percent parallel? it is not difficult. if welook at natural phenomena, most are parallel. unfortunately, we are trained tothink sequentially. take the aircraft design example again. we simulate onewing, then another wing, then the body, the tail. each part is called one domain.we can now simulate all domains at the same time.toward the future60supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.we can also think of a parallel application as a multiplestage pipelineapproach. take the seismic processing example. first we start with tape input,and then comes data verification and alignment. the next step is analysis andsimulation. the final step is data interpretation and visualization of theunderground picture. all stages of the whole process can be done concurrently onthe system. the first stage can be performed in groups of a few processors, withdata flowing continuously to support the next stage on another group ofprocessors, and so on.take this one step further. if we look at the future application development,we can bring different disciplines into one design solution, a multidisciplineapproach. for example, in the design of a space shuttle, materials, structure,aerodynamics, and control problems can all be evaluated at the same time withvarious design criteria. the analysis step of each discipline can be processed inparallel by different groups of processors.these examples are just a few of the ideas for exploring future parallelsystems to achieve much higher system performance through a topdownapplication decomposition than can be obtained only by the bottomup compilerapproach. the key to success is the adaptability of the system architecture. usersshould not have to change application algorithms when they migrate to higherparallel machines.application technology developmentmany examples indicate that supercomputers have proved very useful invarious industriesšin the defense, petroleum, aerospace, automotive,meteorological, electronic, and chemical segments. today, all the industrialcountries of the world are developing their own application techniques usingsupercomputers. these tools improve their competitiveness in creating newmaterials, developing better processes and products, or making new scientificdiscoveries. we see existing applications expanding to include more complexgeometry or more refined theory as machine capability and capacity keepimproving.we also see the potential in new areas, especially materials science. we needhelp to find new materials, whether we are designing integrated circuits forsupercomputers or developing industrial products. other emerging applicationareas include biomedical engineering, pharmaceuticals, and financial analysis.new applications will also evolve from interdisciplinary areas.we have to think about how to develop future application technology alongwith future system design. we must start earlier to interact with leadingapplication scientists and engineers to develop the next generation of algorithmsto make the greatest use of parallel processing. these effortstoward the future61supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.will also help to speed up the migration of existing application codes onto newmachines.our challenge is to start using these machines in production as soon as theybecome available. the worst thing we can do in this country is to design the bestmachines but then not use them. then some other country will jump in to makeuse of them ahead of us. we have already seen this happening in some industrieswith the current generation of supercomputers. we certainly want to keep ourleadership position in application technology development for future machines.summarynew directionsin summary, i will point out a few new directions that may evolve insupercomputing: comprehensive support for parallel processing; development of open systems that enhance productivity andcompetition; total system design to minimize solution time; seamless services environment and distribution of functions; and wider applications in scientific, engineering, and commercial fields.in the future there will be more comprehensive support for parallelprocessing from very primitive to very sophisticated levels. this means that morecompiler and system software features will be made available for supportingusers in parallelizing their application algorithms as well as developing anddebugging parallel programs.the open system concept is spreading rapidly. participants are working frommany directions to exchange ideas and codes. an open system environment willallow us to concentrate our development and application resources only on thoseextension areas related to performance or functionality. this will prevent the"reinventing the wheel" syndrome and enhance our productivity in deliveringcompetitive products.a total system design that minimizes solution time is an important key. wewill measure machines by solution time instead of by computation time.the user will see a seamless services environment with distribution offunctionsšthe supercomputer merged with mainframes and workstations. userswon't have to tackle different kinds of environments. instead, an integrateddesign, engineering, and manufacturing computing environment will emerge,greatly enhancing user productivity and industry efficiency.toward the future62supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.we will also see a broad expansion of applications for science, engineering,and commercial endeavors. scientists and engineers will explore the unknownand develop new technologies. industry will be more competitive and productivethrough its development of new products or processes.potential impactdevelopments in supercomputing technology strongly influence not only thecompetitiveness of key industries in our national economy but also the vitality ofthe computing industry itself. this influence on the computer industry can beshown in a simple triangle (figure 5.1). the base of the triangle representspersonal computers and workstations. the middle section contains mainframe ormidrange computers. at the top is the supercomputer. all three levels oftechnology are interacting heavily. for example, the basic componenttechnology, parallel architecture concepts, and software and hardware designexploited in the supercomputer arena will trickle down to the mainframe andworkstation level; viceversa, the user interface software and application toolscommonly seen at the workstation level will be introduced at the supercomputerlevel. as a result, the supercomputing technology pulls the computer industryupward, creating new market opportunities and enhancing user productivity.need for technological leadershipi used to say, "how do we stay there?" i have changed my mind. now i say,"how do we get there?" the race is too close to call at this time.i don't think we have too much leadership in component technology. i haveworked on this problem for many years. each year i become more humble when isee how difficult it is to build this kind of machine without a competitive andsustainable technology base.we are losing by months from many points of view. we are starting to losesome of the critical components. we have tried to help u.s. companies, to workwith them, to drive their capability forward to meet with us. but sometimes it islike wrestling with a big boat.our competitors have the advantage. their work is integrated. they canfocus on something and stay in there for a long time. they can sacrifice onesegment of their industry to pay for another one as long as it is strategicallyimportant to their longterm technology objectives. in the past, we in the unitedstates seemed not to be able to do that no matter how hard we tried. thus, toreverse this trend, some component and computer industry leaders need to worktogether intensively to develop and maintain a strong component technology basein this country.toward the future63supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.figure 5.1impact of supercomputing technology. (note: manufacture of supercomputersby cdc was discontinued in april 1989.)fortunately, we still have some lead in software and application technology,especially with respect to parallel processing. my hope is to combine ourresources with those of government, universities, and industry. it is important forus to keep this cooperative development effort moving. in 5 years, we can design amachine that is 100 times faster than today's, but nobody will be able to use itunless we ship it with good software and application tools.toward the future64supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.we must start working with users today. it may take 5 years to develop anapplication. beginning now, while users are developing their nextgenerationapplications for a highperformance parallel machine, we can be developing ournextgeneration system software and application libraries and tools for a highefficiency user environment. we are entering a new paradigm of supercomputingin which user application (and productivity) is in the center, instead of hardware(peak rate) as in the last decade.that is my goal. we have to keep this technology leadership. we canaccomplish it as long as we have a common view of the future. in order todevelop and sustain supercomputing technology, we must take a longterm view.we must be willing to take risks. we have learned from our past experience.also, most importantly, we should have a focus. we have many resources in thiscountry, but they are scattered and never focused enough. that is why we arelosing step by step in some areas.these are just some of my personal observations and experiences that iwould like to share with you. certainly, i am not done yet. i am still chasing thatdream machine!toward the future65supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.discussionmichel gouilloud: steve chen, you have come with a long list of challengesand problems. can you suggest some priorities, in other words, some of theproblems you see as the most critical for you in the path of developing your nextgeneration of machines?steve chen: i think the underlying component technology is the mostcritical problem. for example, in silicon technology i see a plateau for speed andpower. the nextgeneration chip we see is denser but not faster, and it requiresmore power. we certainly don't want to have a machine that is 100 times fasterbut needs 100 times more power. we may have to build a power substation nextto the computer room. that problem is real. we need a breakthrough in this area.another critical area is highdensity cooling. we have to be able to cool asmall area that has very dense heat dissipation, e.g., 10,000 to 20,000 watts.the next area is application. we need to work with users to design machinesthat are balanced, while at the same time preparing their future applications totake full advantage of parallel processing.michael teter: we from corning glass are interacting fairly heavily withthe cornell supercomputer facility. we seem to notice that, independent of thesize of the supercomputer there, as soon as users start competing for time, theamount that any individual scientist has for his own research becomes essentiallynegligible, and he would almost be better off buying a vax and working byhimself.larry smarr: the largest university user of the ncsa has received 10,000hours in the last year. several users have used over 1000 hours. kodak uses morethan 100 hours a month. it is management of the allocation of time that isimportant. the national centers are still learning how to do this. in fact, it is onlywithin the past several months that the blue ribbon peer review boards for eachcenter have taken over completely the allocation of time. previously, individualprogram officers at the nsf simply forwarded any good proposal they received,and that caused some real saturation problems.our goal is certainly to upgrade the facilities as rapidly as we can. thatrequires leadership and support from congress and the nse i believe we are allnow beginning to pull together on that. our goal is to give to those users who areon the machine both supercomputer response time and supercomputer power,even if that means that we have to limit by strict peer review the number of userson the system.arthur freeman: i would like to add to the discussion about whether ittoward the future66supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.is better to use a vax. if you can use a vax, don't go to a supercomputer. onething that is very clear is that 100,000 vaxs don't add up to a supercomputer interms of capability, just as 100,000 volkswagen engines don't add up to a saturnengine. supercomputers are very different from vaxs. i think people have tounderstand this difference between capacity and capability. capability just is notthere on a vax. it is there on a supercomputer. we want to increase thatcapability all the time.george kuper: my question addresses a concern outside the operationaldiscussion that has just been going on. steve chen, you very accurately describedthat one of the major challenges you face is decomposing problems andunderstanding how to think differently about solution sets. you said that we needto increase the demand function, because we are possibly at a stage now in oursociety where our supply of supercomputing capacity exceeds our ability to use itwisely.i wonder if you think that we are facing a major intellectual challenge, acomputational mechanics challenge that is even greater than the technicalchallenge of building faster machines?steve chen: yes, we face a psychological challenge. i was joking with jackworlton. for many years, every time i spoke with him, he always said he needed amachine 100 times faster. now i say, "i will give you that machine, but tell mehow to use it." each time i have given him a machine at los alamos, it wasalready too slow. but, at the same time, the machine was not used to exploit itsfull performance features. we had a fourprocessor system for more than 5 years.but the users were still using the system as a throughput machine without goingto parallel processing. this was because it was so easy to port all the existingapplication codes onto the new system, to run it as a fourway throughputmachine instead of a fourway parallel machine.in contrast, the overseas users are more aggressive. a good example is theeuropean consortium for mediumrange weather forecasting. in anticipatingthe future performance required by finerresolution forecasting models andupcoming parallel machines, they have already decomposed their problems with ageneral nway parallel approach where n is greater than 1. they had demonstratedtheir parallel algorithms in a research model before the next machines arrived.hence they were able to continue to upgrade their production forecast model from1 processor to 2 processors, to 4 processors today; next they will have 8, 16, andeven higher numbers of processors as soon as those become available. theirtransition from a research to a production model has been quick and successful,because they took a longterm view and broke that psychological barrier veryquickly. we in the united states are behind in this respect. we have got to catchup in this area.toward the future67supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.john riganati: steve, in the earliest days of vector architecture, seymourcray made a presentation to lawrence livermore national laboratory. at theend of the presentation he was asked what made him believe that the vectorarchitecture he was discussing was really a generalpurpose machineit didn'texist at the timeand whether the problems at livermore would be able to mapinto those.the way harry nelson tells the story, seymour just smiled enigmatically andsaid, ''we'll see.'' well, we did see, and the vector architecture has proven to bequite general purpose. but the architectures that are evolving now are one stepmore difficult to understand. can you help us, especially from a user point ofview, to understand why the parallel architectures, the cluster architectures, reallywill be general purpose in the sense that they will be able to map generalapplications onto those architectures?steve chen: yes. let's refer to my earlier remarks. you can think about yourapplications and decompose them from the top down, e.g., using the multipledomain approach, the multiplestage pipeline approach, or the multidisciplineapproach. these are natural approaches by which you can easily map manyapplications on to the parallel architecture. you get the best performance thatway. with the proper tool set, the user should be able to exploit this highlevelparallelism in a simple and general way without entanglement with the lowestlevel machine complexity.mark karplus: you gave us the hope that in 5 or 6 years you might have amachine that is 100 times faster and that will combine some improvements intechnology, plus minor parallelism. what many people wonder about is doingmuch better. i think there will be people who will very easily figure out how touse a machine that is 100 times faster and who will want more. but there is verylittle discussion of massive parallelism, and many people say from the computerpoint of view that the future is to get machines that are 1000 or 10,000 timesfaster.steve chen: i can only give you my personal viewpoint. i think those areworthwhile research activities at this moment. i would like to see that effortmoving forward. but as far as putting 1000 microprocessors together, i don'tthink you can achieve the same capability we're talking about in solving generalapplications problems. i would rather evolve from the currently available smallerparallel machines to larger parallel machines, step by step. we have to move thewhole community, instead of just one or two very bright scientists. a few peoplemight be able to sit down at a terminal and decompose a problem into 1000parallel tasks. that would be very good. but i don't think we can bring in thewhole community that way in a short period of time. however, i do see thepossibility of specialpurpose massively parallel machines cooperating with thegeneralpurpose supercomputer.edward mason: at amoco corporation we use supercomputers andtoward the future68supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.massive computation for geophysics, but we also have a chemical company and arefining company. one of the biggest problems is retraining or educating peoplewho are very good in particular fields of science but who have not usedsupercomputers, to solve problems by taking advantage of the opportunitiesprovided by computational science and, when appropriate, by supercomputers.visualization and transparency are crucial. parallel computing has beendiscussed a lot here, but the biologist or the chemist could care less how it isdone. the concern is what can be done. and the problem is to have those expertsin chemistry, biology, and other fields become familiar with how to simply, fromtheir point of view, exploit supercomputers.larry smarr: critical to the success of that education and training, which ithink is issue number one, is having the industrial users live and work in theuniversity environment where, because of the nsf initiative, we have such a vastnumber of faculty and students who are not having to relearn but are veryenergetically going directly into using supercomputers. having them workshoulder to shoulder with the people from industry is proving to be very effectivein bringing about that technology transfer. i would very much like to see moresupport from the government for this education and training part of the program.toward the future69supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.toward the future70supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.part cexisting applications ofsupercomputers in industry 71supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved. 72supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.6deciding to acquire a powerful newresearch toolšsupercomputingbeverly ecclesabbott laboratoriesabbott laboratories is a health care company that has five major divisions.each division behaves somewhat as a separate entity. i belong to thepharmaceutical products division, and i provide support to the computationalchemistry area of "discovery" research.using computation to produce pharmaceuticalsin the pharmaceutical products division, computation is done in twodistinct areas that are functionally, physically, and managerially separate. thefirst is corporate computation, which supports payroll and production and salesand is primarily ibm based. the second is r&d computation, which is doneprimarily on digital equipment corporation vax computers.research and development computational support spans several areas. anetwork of vax systems forms the backbone for generalpurpose supportfunctions: electronic mail, graphics, word processing, and so on. clinical andtoxicological data processing is another area of support. government regulationsrequire that these data be kept in archives and that statistical analyses be done onthe data to prove abbott's claims of drug effectiveness and safety. laboratoryautomation systems include many specialpurpose instruments that connect tocomputers for data collection, analysis, and possibly delivery via the computernetwork to a central processor or database. computational chemistry, the area inwhich i am directly involved, will be the focus of the remainder of thisdiscussion.deciding to acquire a powerful new research toolšsupercomputing73supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.computational chemistry spans three primary areas that utilize computationallyintensive methods to solve problems:1. computerassisted molecular design. this is the computer version ofputting together the ballandstick plastic models of chemicalcompounds. the computer further enables the researcher to visualizea molecular structure, examine it, compute its theoretical properties,and attempt to determine what it is about the molecule that makes itparticularly useful for a drug application.2. xray crystallography. this area deals with analysis of the crystallineform of materials to determine the structure of the molecules of achemical substance. again, chemical structure is very important.3. nuclear magnetic resonance spectroscopy. this involves anotherphysical measurement that helps the chemist determine the structureof a molecule, the spatial relationships between portions ofmolecules, and the relationships between a molecule (a drug) and asubstrate.the common theme is to try to understand the structures of molecules andhow they relate to the chemical actions of these molecules. these computationalactivities are far from what would commonly be considered wet chemistry.although computational chemists do perform experiments, the context of theirexperiments is the computational domain. modeling is their world of chemicalreality. how well a model matches observed chemical behavior is a good measureof the usefulness of the model.the current methods used in computational chemistry give a broad picture.the goal is not to model every minute detail of a molecule and its behavior, butrather to gain an understanding of how structure and function are related. twomethods can be used to perform experiments to gain such understanding. thefirst involves interactive processing, which may include querying a database orlooking at a graphic display of the model of a moleculeša threedimensionaldepiction of the structurešon a highresolution, highperformance computergraphics workstation and interacting with the structure in view (using techniquesof rotating, zooming, color coding, and so on). rather than physically holding aplastic model of a structure, one can turn dials and cause the structure to move on adisplay screen to try to answer a question such as, how do these two moleculesfit together? this is the inspection and manipulation stage, the manual part ofgetting acquainted with these chemical structures.the second computational method involves batch processing. acomputational chemist who has an interesting structure to investigate can do anyof a number of things that come under the classification of batch computations.these do not necessarily take very long to run. however, many suchcomputations do. some of the computations running on vax/785typeequipment can take from a week to several months of elapsed timeshareddeciding to acquire a powerful new research toolšsupercomputing74supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.computing time. such time requirements are not uncommon. basically, ourcomputers at abbott are kept running night and day, every day, doing these sortsof computations.batch computations permit the computational chemist to evaluate thetheoretical physical properties of compounds of interest and to simulate thebehavior of these compounds, both individually and in interaction with othermolecules. the ultimate goals of interactive and batch processing are to elucidatechemical structures, derive theoretical physical properties, relate these toobserved chemical behavior, and deduce what aspects of a chemical compoundresult in a desired drug action. with the understanding gained from suchinformation, it may perhaps be possible to develop a better drug, a drug thatperhaps is more easily absorbed by the body, or will not break down in bodytissues, or will not have serious side effects, or will be more specific in itsbehavior.the computational method is a cyclical one of interactive processinginterleaved with batch processing. the computational chemist views the resultsof the batch processing and may make adjustments in the structure, test newhypotheses, or simply resume the computation where it left off, all in the spirit ofexperimentation. the computational chemist works in collaboration with themore traditional bench chemist, aiding in developing rational approaches to drugdesign based on physical and chemical principles.the computational chemist uses the computer daily in experimental workand conducts computational experiments on computers of varying power andfunction. much of the work proceeds in a direction based on chemical intuitionfrom accumulated years of experience. this science of computer modeling andsimulation of chemical behavior is still something of an art. one cannot computewith great precision everything about a reasonably sized molecule because thereare not enough computer resources in the world to do so. one has to makeapproximations involving adjustable parameters. one must sacrifice accuracy forfeasibility of computation. in the course of experimentation, the computationalchemist must manipulate many interrelated experimental parameters according toa "tryitandsee" methodology.assessing advantages of supercomputingnow the question is, with this cyclical methodology to performingcomputational experiments, what can a supercomputer do for us? the simplestadvantage is that a supercomputer can speed up most calculations. computationsthat take 30 days can possibly be accomplished in 1 day, or even in hours. butsimply speeding up a calculation is not enough. we want to look for new ways ofanalyzing our data.deciding to acquire a powerful new research toolšsupercomputing75supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.immediate feedback for rapid resultsif we can tighten the loop between the experimental design (the interactivestep) and the experimental outcome (the batch step), thus shortening the timebetween asking the question and seeing the answer, then we can very quickly askthe next question, and the next, thus making creative thought flow more easily. itis similar to the difference between writing a letter and making a telephone call.the feedback is immediate, and channels are followed that perhaps would not beif results came back in a week. when results come more slowly, we are moreconservative in the questions we can ask, and we leave many more stonesunturned. when the answers come back while the questions are still fresh, thetrain of thought can continue, and less time is spent trying to remember whereone left off and what line of reasoning was being explored. a very importantthing that a supercomputer can do is to tighten the loop between inception of anexperiment and the final outcome.increased human inputa second important thing that a supercomputer can do is to put a human intothe loop in some of what is now batch, iterative computation. right now thehuman element in the process exists only at the initial point when the input dataare assembled for a batch computation. at this point a decision must be made asto how far to carry an iterative calculation, and often this decision is based onarbitrary criteria. then one must wait until the batch calculation is completed,only to find, perhaps, that some of the computational time was wasted onexploring an unfruitful avenue. a human can get into the loop if the time it takesto calculate each iterative step is reduced to the point that a batch calculationbecomes interactivešto the point that the iteration interval is short enough that ahuman can monitor the progress of the iterations continuously and can interveneto take a corrective or exploratory action. there are several examples of batchactivities that can profit from this. an additional benefit of continuous feedbackof results is that the computational chemist can gain new chemical insight whenthe temporal aspect of a computation is compressed to the point that newprinciples can be inferred that could not otherwise have been inferred from apostprocessing review of the results of a batch computation.visualization of resultsa third advantage of a supercomputer is that it can enable the visualizationof scientific results. presentday computational chemists aredeciding to acquire a powerful new research toolšsupercomputing76supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.accustomed to viewing the ballandstick models, both in real plastic and incomputerproduced graphic displays. this is not the only way to view amolecular structure. an atom is not a ball, nor the bond between atoms a stick.on the contrary, a molecule is more correctly viewed as a malleable cloud ofelectrons surrounding a number of atomic nuclei that are constrained to move insome rather preferred distances and orientations with respect to one another. fromthese fundamental particles, physics allows us to derive some physical propertiesthat vary continuously throughout the volume of the molecular structure. we arelooking for new ways to visualize these physical properties to gain new insights.the computational chemist who can view the data in just the right way canperhaps discern new patterns, derive new hypotheses, and explore newdirections. we at abbott are looking forward to the enhanced visualizationpossibilities that a supercomputer will afford us in the display of derived physicaldata to lead us to the development of new algorithmic approaches.creating an environment for supercomputingat abbott laboratories we have convinced ourselves that we do need asupercomputer for all of the reasons i have listed. however, we are alsoconvinced that we cannot just put a supercomputer on the floor and turn the usersloose on it. we need the whole integrated supercomputer environment. theenvironment must take into account the fact that if it is too much trouble to put asupercomputer to work for the scientists, they will not use it. they are used tositting down to their computer terminals every day. they read their electronicmail. they work comfortably at graphics workstations. they manage input andoutput data sets stored in data files and databases on disk. if they are handed asupercomputer, they must be able to make use of it in the same way that theycurrently use existing computer resources. it must fit seamlessly into theirenvironment. this means that we need to be able to supply interactive access tothe supercomputer. it cannot just be a batch machine.with the coming of age of unix in the supercomputer and workstationmarketplace and, more importantly, in the physical and chemical sciences, astandard interactive operating system is now a reality. but we want to do morethan just interactively submit batch jobs. we also want to run those jobsinteractively. we want to be able to tie the computation in progress on thesupercomputer into the graphics workstation at our desk, permitting a twowayflow of data: a realtime display of intermediate results of calculations to monitorprogress, and interactive input from the user to modify the course of thecalculation. we want the flexibility of being able to derive new ways to visualizethe data. we need the ability to program the environment to suit our evolvingneeds and experiments;deciding to acquire a powerful new research toolšsupercomputing77supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.hence we need a development platform. the supercomputer is a new kind of tool.it is going to stimulate new thinking if it is put into the hands of people who canuse it easily, and this will lead to the development of new computationalalgorithms.gaining acceptanceat abbott laboratories we have had to deal with two obstacles to bringing in asupercomputer. the first, selling it to upper management, was, surprisingly, theeasier to overcome. to a great degree this was due to our ability to easilydemonstrate the supercomputer's usefulness in a particular pilot application. wewere able to show how a supercomputer could save approximately 1 year of laborfor a crystallographer in doing a refinement for determining a crystal structure.management can see the dollars and cents of that. we were able to assert thecompetitive advantage to being the first pharmaceutical company to purchase asupercomputer. also significant was the fact that we already had somechampions in the ranks of upper management who had drawn their ownconclusions, early on, about the scientific promise of a supercomputingenvironment.the second, and greater, obstacle to acquiring a supercomputer has beengaining acceptance by the user community. we have heard of this difficulty anumber of times from other organizations trying to accomplish the same thing.the problem is that the users do not see a costjustifiable way of owning thistechnology and incorporating it into their research. they have shortrange goalsthat involve a slight profit from the application of a supercomputer. their longrange goals do not include the supercomputing environment as a necessity. asupercomputing environment is new and unfamiliar to them. we need to educatethe users as to what they can do; we need to open up the creative flow. the usersperceive a supercomputer not as an opportunity but as a responsibility and aburden thrust upon them. they feel that if the supercomputer is not fully utilizedor that if no great breakthroughs come because of its presence, they will beresponsible for a perceived failure. they feel that the expectations ofmanagement will have to be high to match the capital outlay and operatingexpenses of a supercomputer. they are researchers and cannot predict orengineer breakthroughs, so it is very difficult for them to stand up and say thatthey need and can justify the acquisition of a supercomputer. we have yet onlypartially sold acceptance of supercomputers to the users.defining computing requirementsat abbott we have gone through the standard steps for acquiring any kind ofcomputer system, starting with defining our computing requiredeciding to acquire a powerful new research toolšsupercomputing78supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.ments. interviews with the computational chemists and other researchersindicated the need for a machine with a performance class well into the range ofsupercomputers. given this, the next step was to define other measurables anddeliverables that would affect vendor selection. for abbott, the first of these was alist of a number of turnkey applicationsšthree or four thirdparty packages thatwe said we absolutely must have. these codes are our bread and butter inpharmaceutical computational research; they already exist commercially; weknow how to put them to work in our research today. next, we specified that wemust have a strong program development platform: an interactive operatingsystem (that works), a compiler (that works) with the capability to optimize codeto suit the computer architecture, file management and data integrity, programoptimization tools, subroutine libraries, and so forth. making use of thisdevelopment platform will allow abbott to realize a competitive advantage in newcomputational methods. next, we insisted on connectivity, the ability to makethis machine talk to all of the computer hardware we have on our site.protecting corporate investmentfinally, we laid out the specifications for things that are not so easilyquantifiable but that have played a very large role in our decisionmaking processfor vendor selection. to try to help protect our investment and achieve ourdesired goals, we considered the following:1. upgrade path. what new hardware will be developed, and, moreimportantly, how will everything done this year move to that newpiece of hardware? how long will a machine be down while it isbeing upgraded?2. support. will the vendor help researchers who encounter problems?will help be available when a machine's performance is less thanoptimal?3. does the vendor understand the purchaser's business? this isextremely important. a vendor that does not understand yourbusiness will not understand what you need, and the vendor'sdevelopment strategies may not support your strategies. assessingvendor understanding and support has been very important to thoseof us in the computational chemistry area.4. do the vendor and the purchaser share a common vision? there areso many pieces of hardware available now, including the wholehierarchy from personal computers to the supercomputer, networks,and so forth. does the vendor share your vision of what you aretrying to put in place in your company? every company has differentattitudes about departmental computing versus central computing,operating systems, graphics requirements, and so on. making surethat the vendor and purchaser share a common vision ensures agreater ability to achieve desired goals.deciding to acquire a powerful new research toolšsupercomputing79supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.making a decisionafter this whole process at abbott of evaluating computers, evaluatingvendors, and evaluating whether or not we really want to acquire our ownsupercomputer (or whether we want simply to get some timesharing on anothercomputer)šafter all this, we have decided that the time is now. the simplereason is summed up in an adage that applies to everything from personalcomputers to supercomputers: there will always be something better next .we at abbott have made that decision even though there are several players inthe picture, each with distinct advantages, whether available today or promisedfor the future. there are the crays that have been available for a long time andsome machines that are just 80 percent developed now. in selecting a vendor it isnecessary to consider the whole environment, the whole picture, and to keep inmind that as soon as a piece of hardware is bought, next month, or next year,there will be something better that can be taken advantage of when fiscalpossibilities allow that in the next round.deciding to acquire a powerful new research toolšsupercomputing80supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.7using supercomputing to transformthinking about product designclifford r. perryeastman kodak companyi am delighted to share with you why eastman kodak company scientistsare using the awesome power of supercomputers and associated visualizationsystems. i will also discuss some recent applications by kodak scientists inmultiple disciplines throughout the kodak research laboratories. but i will beginby briefly discussing the issue of communication and assimilation of the use ofsupercomputers within our industrial sector, because i believe that our failure tocommunicate how we assimilate the use of computer technology is principallyresponsible for the very slow rate of application of supercomputers to industrialr&d problems.understanding the need for communicationour numberone priority, in my view, is to create better ways ofcommunicating not only how we use supercomputers but also how we encourageassimilation of their use by potential practitioners.although i will be sharing with you what we have been doing at kodak,much more needs to be done. communication and assimilation are industrywideproblems. we cannot continue to learn on our own in this highly competitiveglobal marketplacešwhich brings me to a very brief, personal story.some 30 years ago when i was a college freshman, i accepted a job thatrequired only that i ''play'' with the university's newly acquired ibm650computer, a socalled firstgeneration computer. my job was to inventapplications and to prove the computer's usefulness to the university'susing supercomputing to transform thinking about product design81supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.research community. they had been given the ibm650 but had few ideas aboutwhat problems to apply it to. these people were scientists interested in scienceand not in tools that had not yet proved their direct and useful applications inhelping them pursue their science.this was an example of computer technology preceding useful application,and there was no known way to assimilate its use into its intended environment.there was no form of communication and no way to learn from others. we eachhad to learn on our own. some 7 years later i accepted a job at the generalmotors technical center's computer technology division, where i was again toplay the role of a researcher using computers: i was to find useful applicationswithin general motors' r&d community for their newly installed ibm360computer, a secondgeneration computer. but the firstgeneration problem stillexisted: the latest computer technology again had preceded development of aprocess to assimilate its use into its intended environment. there was no form ofcommunication and no way to learn from others. we each still had to learn onour own.some 21 years after that i was asked to facilitate the use of supercomputingwithin kodak's r&d community. we were in the process of signing a 3yearcontract with the national center for supercomputing applications (ncsa) withlarry smarr at the university of illinois. the readiness for supercomputing atkodak, as i initially had surmised, was once again an example of availabilitypreceding a process for complete implementation. there was still no known orestablished process to assimilate the use of supercomputing into its intendedenvironment, and there was no form of communication and no way to learn fromothers.would we still, after 30 years, have to learn on our own? in 30 yearscomputing technology had advanced tremendously, but there was still noorganizational process or procedural framework for making its applications clear.how then was supercomputing to fulfill its promise as a problem killer and as atool with the potential to transform thinking? there was still no approach torapidly and effectively making its enormous capacity understandable to itspotential users, and hence there was little hope of making its use pervasive withinits potential market.i believe that this past and present bumbling about is a direct result of ourfailure to communicate within our own organizations and with one another.perhaps the greatest failure to communicate is between the practitioners and thelaity, who fail to understand the promise, the payoff, the problems, and the limitsof computational science and who, more importantly, fail to understandšbecauseof the lack of effective ways to communicatešthe synergy achieved in attackingproblems with the combined methods of theory, experiment, and computation.in this current era of intense worldwide competition, we cannot affordusing supercomputing to transform thinking about product design82supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.to learn alone, organization by organization, one at a time. although we mayhave communicated results obtained from applying supercomputer technology,we have not communicated approaches that deal with the sociology or socialpsychology of scientists who are the potential practitioners of supercomputing.we have been given a remarkable tool, the supercomputer, that holds greatpromise for us in industry, but we cannot assume that accommodating thesupercomputer requires only minor changes in the way we assimilate newtechnology into our r&d activities. how do we help foster the cultural changethat is required?i cannot speak of applications without also speaking of communications thatare both internal and external to our organizations. we must, as supercomputerstakeholders, grow to understand the commonality of our endeavors throughcommunication.using supercomputers to increase productivityi think it is important to realize that supercomputers do not requirevisualization systems, and visualization systems do not require supercomputers.however, we have found that when they are combined into a system that includesthe scientist or engineer, we have new opportunities to transform the potential ofour r&d opportunities for significant discoveries and breakthroughs.visualization as a stimulus for creativitythis synergistic system, visualization and the interpretation of what wevisualize, can lead to new theories and scientific paradigms, that is, the set ofbeliefs, values, and techniques shared by members of the scientific communityand new tools for the advanced engineering sciences.i think that the power of computergenerated scientific visualization is bestsummarized by herbert butterfield as quoted in thomas s. kuhn's the structureof scientific revolutions (university of chicago press, 1970). butterfielddescribed science's reorientation by a change in paradigm as "picking up theother end of the stick, a process that involves handling the same bundle of data asbefore but placing it in a new system of relations with one another by giving it adifferent framework."supercomputer visualization is that system. it fosters differentinterpretations that can lead not only to insight and understanding but also to ashift in paradigms. it has been said that when aristotle and galileo looked atswinging stones, aristotle observed a falling object that was constrained,using supercomputing to transform thinking about product design83supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.whereas galileo saw a pendulum. priestley and lavoisier both saw oxygen, butthey interpreted their observations differently.the richness of visualization as part of the process of scientific inquiry isthat it allows us to differ in our interpretations of what we have seen. thesedifferent interpretations can lead us to new theories and new scientificparadigms, and then to new technologies and new products.supercomputing and visualization systems at kodak have not caused shiftsin paradigm as significant as those caused by the newtonian or einsteinianrevolutions, nor have they caused relatively small changes in the paradigmsgenerated by the wave theory of light, the dynamical theory of heat, or maxwell'selectromagnetic theory. supercomputing has, however, allowed us to transformthe way we think about problems involving polymers, crystalline compound dies,photographic imaging systems, and manufacturing technology development.for example, using supercomputer simulation, our engineers at the kodakpark division were able to test and then to redesign the delivery system used in acritical photographic film manufacturing process. this eliminated the need tobuild a prototype of the first flawed design and resulted in cost savings ofhundreds of thousands of dollars. these new theories and processes willaccelerate kodak's ability to continually enhance the quality of its products.i will now discuss a few of the many examples that illustrate how thesupercomputer has allowed us to change the way we think about specificproblems and has led to new solutions that would not have been possible withoutsupercomputing and visualization systems. i hope that these examples will serveas evidence that we are making progress through the effective use ofsupercomputing to enhance the quality of our products and our manufacturingprocesses.visualization and simulation of physical processesthe following three examples of supercomputerassisted science involvesimulations of physical processes. the graphics allow us to visualize thegigabytes of scientific data generated by the supercomputer simulations. thesecomputergenerated images illustrate physical qualities as well as quantities.the first example illustrates how we use supercomputers in fundamentalresearch. at kodak, we synthesize polymers for many purposes. thus we need tobetter understand the properties of these complex molecules in order to designnew and better plastic materials. supercomputers and visualization systems allowour researchers to study the physics and chemistry of basic polymers in new anddifferent ways. for example, these researchersusing supercomputing to transform thinking about product design84supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.are investigating how a polymeric network behaves when stressed, themechanisms of selfdiffusion, and the effects of polymer blending. results fromthese studies will enable prediction of polymeric behavior in the design ofmaterials with specific performance criteria. in a real sense, these polymerresearchers are designers of new materials.the goal of another investigation is to understand the diffusion of a polymerchain when it is entangled with other chains. the interactions and intraactions ofthe chains significantly influence the ability of a polymer chain to diffuse. it ispossible to see on videotape one polymer entangled in a network of otherpolymers. we can then follow the path the polymer takes while wandering aboutthe material. the sooner the polymer is able to diffuse away from its initialconfiguration, the sooner it is able to relieve the stress. this importantinformation will enable scientists at kodak to infer the behavior of the materialwhen it is under stress as well as its viscosity and other viscoelastic properties.developing this knowledge is a challenge. although the questions appear tobe simple, obtaining answers requires wellcrafted computer simulationsinvolving many hours of computer time. visualization is again required to gaininsights from the gigabytes of data generated by the simulation, and it is oftenvery helpful in determining the validity of the models.visualization applied to the manufacture of productsa second example illustrates the use of supercomputers at kodak in appliedresearch at the engineering level. it is no surprise that kodak has a very keeninterest in the manufacturability of plastics. plastics can be found in many of ourproducts, from cameras and film spools to copiers and mass memory products.we also produce bulk plastics such as acetate fibers and pet products that areused extensively by the bottling industry. we are always looking for new plasticsto enhance product quality and to reduce costsšboth the costs of materials andthe costs of manufacturing.a simple example illustrates how supercomputing and visualization canhelp. suppose we want to make an improved plastic part for one of our products,such as a camera body. we also desire increased productivity in the process usedto manufacture the parts of the camera body. we assume that we will use a totallynew plastic, but first we must familiarize ourselves with its characteristics. whathappens when the plastic flows into the mold? how long will it take to fill themold? how long will it take to cool the plastic and to eject the plastic part fromthe mold?we also need to know about the heat conductionšthe temperature, thepressure, and the velocity at every point of the mold. prior to the useusing supercomputing to transform thinking about product design85supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.of supercomputers and visualization systems, we had little idea how to even thinkabout these questions, other than to build a physical prototype that was extremelyexpensive and timeconsuming. it was virtually impossible for mold designers tophysically see into the mold and observe the physics at work.now for the first time our designers can see these phenomena viasupercomputing and visualization. a new vocabulary is emerging based onvisualization of the computer simulation. more importantly, new insights areemerging. as a result, manufacturing productivity is increasing significantly.a third example of the application of supercomputing involves research oncolor, a vital element in many kodak imaging products. our photographicscientists are keenly interested in matching physics with perception. this isimportant because it can lead to increased flexibility in developing colorreproduction processes. color reproduction almost always involves creating colorin a constrained way. since only three dies are used in a photographic paper, orthree phosphors in a cathoderay tube, it is important that these choices be madeso that the image quality and color rendition are not compromised in the eye ofthe observer.color theory attempts to describe causal relationships between physicalcolor stimuli from the environment and psychological color sensations evoked bythese stimuli. color stimuli are radiations within the visible spectrum and aredescribed by radiometric functions, whereas color sensations are subjective andare described by words such as red, blue, or green.kodak research scientists, in collaboration with faculty at the university ofillinois through kodak's partnership in the ncsa, are exploring an elegantmathematical color theory to enable the computation of all possible ways ofevoking a given color sensation. the key to this theory is the mapping of colorstimuli to the color sensations they evoke. with the supercomputer, researcherscan determine how different dies can be used in color reproduction by simulating astandard observer's response to the colors produced by the die mixtures. thisflexibility could lead to more costeffective and higherquality colorreproductions. although it is still in the early stages, this theory is showing muchpromise.assimilating supercomputing into the r&dculturelacking an existing framework for assimilating supercomputing technologyinto our scientific and industrial culture, a situation that i described earlier, we atkodak created an approach or a process. while we believe it is a model and notthe model, it is an approach that has served us well and one that we seek tocontinually improve.using supercomputing to transform thinking about product design86supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.obviously we realize that we cannot displace whole cultures overnight, sowe initiated what might be called a supercomputer assimilation program. westarted by organizing a program of onthejob seminars on the latest applicationsand developments in supercomputing. in short, we communicate how otherscientists use supercomputers. these computational science seminars havefeatured such distinguished experts as kenneth wilson, a nobel laureate formerlyfrom cornell university and currently director of a supercomputing center inohio, and other leading scientists from supercomputing centers such as ncsa,our valued partner. donna cox, for example, has visited kodak three times in thelast 18 months and has held seminars with literally hundreds of eastman kodakcompany scientists and engineers.we are also taking advantage of a longstanding forum of exchange withinthe kodak research laboratories, the interplant technical conference series.kodak scientists and technicians gather three times annually for theseconferences to foster and to profit from a greater sharing of ideas, techniques, andapplications across a wide variety of scientific disciplines and technologicalfrontiers. the 77th interplant technical conference, titled supercomputing andscience and engineering, will focus on the developing role of supercomputingtechnology in r&d at kodak and will communicate how kodak scientists andothers use supercomputers.in addition to seminars and conferences, kodak has another mechanism tokeep r&d personnel involved. we disseminate in the r&d community the manytechnical reports that have been written by those who have used thesupercomputer, and we have had over 55 r&d personnel that have traveled fromrochester, new york, to urbana, illinois, who have written trip reports abouttheir experiences at the center. we have since installed a t1 link, but the tripreports have provided valuable insight into how we can facilitate the use of thesupercomputer by other kodak scientists. these reports document the results ofthe r&d activities and the techniques and computer tools used to generate thoseresults. the terms supercomputing and visualization are appearing more and morein our everyday vocabulary.we also have a supercomputer technology planning process, a participatoryplanning process that facilitates communication and involvement. we recentlycompleted and disseminated to several hundred people, including topmanagement throughout the company, a kodak supercomputing requirements andprovision plan that profiles our past needs and projects our future needs inspecific areas of application. this report, which also details a plan to supplycomputing capacity and visualization systems to the worldwide kodak r&dcommunity, involved the direct participation of more than 50 kodak scientistsand engineers covering a broad spectrumusing supercomputing to transform thinking about product design87supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.of r&d from life sciences to photographic imaging systems to manufacturingtechnology. this kodak r&d activity worldwide involves some 8000 scientistsand technicians working in the united states, england, france, germany,australia, and japan.by definition, the supercomputer represents the state of the art. as such, it isused by an elite set of pioneers, people willing to put in the tough work requiredto tame the tool in return for major payoffs. we call these pioneers computationalscientists. they are rolemodel supercomputer users who communicate anddemonstrate the usefulness of the supercomputer. thus as an additional step,kodak research management also established the computational sciencelaboratory within the information and computing technology division.this laboratory provides oneonone collaborative assistance to facilitate theeffective integration of supercomputing technology into r&d activities. whilewe do research using computational science methods, we also serve as inhouseadvocates for the use of supercomputing. our mission is to be a catalyst. wepromote advanced r&d computing technologies and systems that will helppeople increase their ability to do creative, costeffective, businessfocusedresearch. and we follow up by keeping another of our important stakeholders, topmanagement, involved.because it is especially important to communicate supercomputing benefitsto our management stakeholders, we formed a supercomputing technologyboard, consisting of the directors of several research and engineering divisions, toinform management of the experiences gained and the successes realized throughthe use of our supercomputing program. as kodak people become more involvedwith supercomputers, management is continually made aware of the tangiblesuccesses resulting from the company's investment in advanced computertechnology.we have made extraordinary progress in supercomputing in a relativelyshort time. there have been both a remarkable symbiosis and a synergy amongscientists and technologists in their applying of computational techniques to theirrespective disciplines. on balance what has come out of this symbiosis has beenbeneficial to each faction, be it academic or industrial. yet there has not been auniversal benefit. such a benefit can be achieved only by an informed and selfconfident scientific and technological population that can leverage the kind ofcultural change necessary for growth. and that change in growth is predicated onimproving communication at all levels.in closing, i leave you with the word communication, becausecommunication is the keystone. we must communicate outside of ourorganizations, as we are communicating and learning from one another in thissymposium, and we must orchestrate change within our organizations by creatingnew processes of internal communication. i believe that the meansusing supercomputing to transform thinking about product design88supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.to achieving sound communications are forums such as this and theestablishment of processes and organizational advocates that facilitate the use ofsupercomputing technology within our organizations.let us have more of these forums. the results will be the increasedefficiency and enhanced effectiveness of supercomputing scientists and engineersin the american workplace.using supercomputing to transform thinking about product design89supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.8achieving a pioneering outlook withsupercomputinglawrence g. teslerapple computer, inc.apple purchased a crayxmp/48 computer a little more than 3 years ago.it has taken a while for us to develop a range of applications. currently about 20different projects are using the cray, and they involve about 50 engineers. mostof the applications that we have are proprietary and cannot be talked about, butfortunately there are some recent applications that i can discuss for thissymposium.extending the range of applicationsthe main reason we bought a cray was to make applications possible thatwere previously impossible because of the time they took to run. we had circuitsimulations, for example, that would have run for 2 months, and it was easier toactually build the circuit and try it out than to wait the 2 months to run thesimulation on, say, a vax. the cray has helped us a lot, because now we can runthose simulations in 1 day.we had applications that would run overnight, and you had to really planthem well, start them running, and then come back the next day to see the results.if there was a mistake, you had to make a little change and run the applicationagain. those we can now do in a few minutes, and we can try many variationsquickly, as the other speakers have mentioned.more importantly, there were things that previously had to be done in thebatch mode that we can now do interactively, a requirement that beverly ecclesof abbott laboratories talked about very well. but the mainachieving a pioneering outlook with supercomputing90supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.reason we bought the cray is that we ourselves are a computer company, and ourjob is to create the computers of the future.the group that i run, the advanced technology group, is not designingproducts for apple. we are doing research on and prototyping of technologiesthat will apply to future products. so we are looking 3, 5, or 10 years ahead forwhat might be applicable in future apple products. for us, the supercomputer is away to experience the kinds of speed that will be on the desktop in the $1,000 to$10,000 range in several years.to make that possible, we have created a somewhat unusual setup. thenetwork that we have at apple includes the crayxmp. we recently upgradedthe disk storage on that to about 30 gigabytes. in addition we have an en641that connects us to ethernet, and we have vaxs and many sun and apolloworkstations on ethernet as gateways into the appletalk network, which allowsus to connect up to the many thousands of macintoshes that are all around theapple campus, including more than 1000 in engineering.on the macintoshes we have software, for example, ncsatelnet, aswell as a product from pacer software that allows us to do terminal emulation, filetransfer, and so on, by using convenient menus. we are able to produce not onlytext but also graphic displays on the macintosh to access the power of thesupercomputer.we also have, in addition to the standard 50megabyte hyperchannel, an800megabytepersecond ultrachannel that gives us very high bandwidth videoout, essentially, to a number of highresolution monitors, so that we can getdirect interaction with the cray. when we do that, we are temporarily tryingdown the entire machine for one user. if someone is rotating an image in threedimensions, the machine is dedicated as a personal computer to that user for a fewseconds while that is going on. some of the applications i will discuss rely onthat capability.we use the cray both in product development and in research. we use it forcircuit design simulations, and that has saved a lot of time in proving designs.disk head design is an application i will discuss; industrial design is another.the disk head design project is an interesting one. the goal is to make therecording head fly at a constant height over the disk surface, on the order of 10microinches. the shape of the head and the shape of the medium and theaerodynamics all interact. if the system is not set up well, then the head willcrash, or the head will be too high above the disk to get a clean signal. we wantedto know what the effects of various parameters were.an interesting problem we ran into was that if the head itself, the airbearing, has a resonant frequency that corresponds to the frequency of therotation, oscillations result. to understand that better, we worked with jimachieving a pioneering outlook with supercomputing91supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.white from the university of santa clara. the disk head can have little levels andother shapes that affect the aerodynamics. our engineers came up with a set ofequations based on the geometry of the head.one of the problems is that the medium itself is not completely flat. it can be athousandth of an inch off flatness, and when the head is flying a few hundredthousandths of an inch over the disk, that variation can cause a problem becauseessentially, the head is like a cruise missile trying to go over peaks.in a simulation, the head can be shown as flying between 150 and 350 or sonanometers over the surface. by varying the shape of the head, the engineers canrun the simulation over and over. the simulation takes only 20 minutes to run,and the engineers can keep playing with it until they achieve satisfactory results.another concern is that there may be a problem caused by a slight bump inthe medium. a little of the oxide may have a small bump in it, and a result maybe that the head can really bounce. and of course if it bounces too much, it willcrash.another area to explore is what happens if there is a jolt to the head, whichcan happen because someone moves the drive while it is running. after a seek,when the head comes to a stop, there is a similar jolt. we need to know how longit will take the oscillation of the head to settle down so that we can actually startto do a read or write.why is apple studying all these things when in fact we do not manufacturedisk mechanisms? the reason is that we work with vendors of heads and media,and vendors of drives, and they come to us with claims of why the nextgeneration of disks is going to be so much better than the last. we need to be ableto evaluate their claims, because if we simply go along with them and somethingdoes not work well, then we might have to shut down our production, and that is aserious consequence.product designwe have also used the cray for product design, or packaging. we have usedthree different applications: (1) thermal analysis, (2) structural analysis, and (3)mold flow similar to that discussed by cliff perry.for thermal analysis we have used a package called ansys, which is afinite element program displaying the output graphically on a macintosh ii. forexample, we have modeled a personal computer board, with major heat sourcesdisplayed as small blue areas. a simulation is run until it settles into a steadystate, which occurs after the computer has been on for a while. then the task is tosee what temperatures the various components have reached.achieving a pioneering outlook with supercomputing92supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.it is possible to tune the heat flow in the systemšby adjusting the coolingair flow and the layoutšso that it is all in the range of about 143 to 152°f, butsome hot spots may remain. by playing with the parameters, engineers can try toget the temperatures within an acceptable range for the components.another application, called nektonics, is a finite element program that isused for structural analysis related to the cooling problem. a small objectrepresents the edge of a cooling vent. just above and below that object is a vent; apiece of plastic separates the vents. as air is drawn in through the package, theflow is depicted. the question is, what will the temperature be after a certainperiod of time, given certain assumptions about air flow and the initialconditions?this program can show potential problems. for example, if air flows past aparticular point and loses velocity, it also loses its ability to cool. it is possible toplay with the shape of a particular edge and solve the problem of reduced airflow. by manipulating with a computeraided design (cad) program the shapeof a vent edge, a different flow pattern was obtained, and the result was that thevelocity loss was reduced.a third application is used for a mold flow problem. what is interesting inthis example is that the product we used this application for was the largesize,extended keyboard for the macintosh ii. a keyboard for a macintosh has variousplaces on the surface that, if looked at in just the right way, are small dark areas.these are weld lines where the plastic has come through the mold and weldedtogether, and they are not very good to look at. the problem was to try toimprove the keyboard's appearance so that people would stop telephoningcustomer support to ask why they couldn't clean their keyboards.the approach to this problem was to break the keyboard's surface down intovery small polygons and then to run a simulation that showed the filling of theplastic in the mold. the point at which the plastic in two paths merges togetherbecomes a weld line. the idea is to try to control conditions so that thetemperature of the two is about the same and the weld occurs in a place where itwill not be noticed by the user. this is the case in the newly designed keyboard,not in our original design.in a twodimensional display of mold flow, different colors representdifferent time periods so that it is possible to see the history of the flow. nowthere is an interactive program that shows the process in real time. the engineercan use a mouse to select a specific part of the picture and then can view ablownup zoomedin view of just that part. this gives the engineer the ability tofocus on parts of the process. our application does not have the aesthetics of thevisualization that cliff perry described or the ability to show multiple parametersat once. instead, we traded that off to be able to get interactive capability for theengineer.achieving a pioneering outlook with supercomputing93supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.we also can do pressure and temperature plots, and so on. what is importantis that in the end, the engineer gives to the plastic maker a drawing that shows thekey things that have to be done. basically the approach to solving the problem ofgetting the plastic to flow at the rate that we wanted was to indicate places wherethe inside of the mold was narrower, which slowed down the plastic flow so thatwe could catch up in other places. this approach gave the result we wanted; theweld lines were exactly where we wanted them. the illustration was done with aprogram called pixelpaint on the macintosh ii, by starting with the caddiagram and simply taking the data that came out of the simulation.the benefits of using the supercomputer for product design are increasedsavings of time and moneyšhundreds of thousands of dollarsšmade possible byfewer tooling runs plus the much greater advantage of getting products to marketfaster. we can get products to market months faster because we know that thelikelihood that the first mold is going to work is much higher. we also do nothave to wait months for another mold if there is something wrong with the firstone, and we can improve the various parameters of the design and get betterquality.researchnow we also use our supercomputer in research. at apple, we have beendoing neural network simulations to better understand how to use different neuralnet models for learning. in addition, we have simulated a cochlear model that isused in a speech recognition project. the idea is that, to be usable, any speechrecognition system has to be able to work in a noisy room. one approach toachieving that is to try to actually model the human ear, which has a comb ofhairs that is able to sort out different frequencies and to measure, essentially, theenergy at each different frequency.some work had been done at schlumberger research by dick lyon, whorecently came to apple. what we decided to do was to take the same type ofmodel that he had implemented, implement it on the cray, and then animate theresults.the result is a plot, called a correlogram, that shows low frequencies at thetop and high frequencies at the bottom. viewed from left to right, it showsvarious correlations of different timing sets. essentially it enables the engineer to''see'' what the ear "sees" when it hears a sound. an utterance can be visualized aspillarshaped forms that represent the main frequency and as other forms thatrepresent other, weaker frequencies. if there is noise in the room, the backgroundbecomes fuzzy, but it is still possible to see a pattern of frequencies standing out.this is the beginning of a veryachieving a pioneering outlook with supercomputing94supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.long research project to try to emulate the power of the human ear to sort outnoise.for neural net simulation we have been able to get very high rates on thecray. using even only one processor on the xmp, we have been able to doabout 10 million connection updates per second and also to animate the results toget a feel for how a neural net learns.so the benefits for research are, again, more rapid prototyping. we can trymany alternatives. people are willing to try things if they can get results in a fewminutes, or even interactively, but the main thing is that we are now muchbolder. we will try things that we would not have tried before. one of the chipsthat we are designing currently is one we probably would not have attempted todesign previously because people thought it would not work. when we simulatedit, we found that it would work, and we went ahead and built it and in fact it didwork so i would say that the main impact of the supercomputer is that it makesus more comfortable with taking bigger risks.adding supercomputing capabilityvisualization, as everyone participating in this symposium has explained, isan absolutely key capability. having a fast network is very, very important, andwe continue to upgrade the speed of our network so that people can get higherbandwidth between the user interface and the supercomputer. one big problem issimply operating the supercomputer center. it accounts for a major portion of ourbudget, and we are always under pressure to add new power to it. it is competingalways with other needs such as upgrading the network and addingminisupercomputers and workstations. the operations end is something thatanyone thinking of buying a supercomputer really must consider.two years ago we brought in a person from our management informationsystems department to manage our supercomputer center, and we have hiredseveral people who are expert in using the cray and other supercomputerengineering systems to work there.the last hurdle, as other people have mentioned, has been to get the users touse the supercomputer. the way we do it is that we have a small group of peoplewe call cray evangelists. they do not appear on television; they walk around.they go to engineers and try to find out what those engineers do, and then theytry to match them up with applications on the cray or help them write their ownapplications on the cray. all of the applications i have discussed in thissymposium have come from that effort, which is very similar to what wasdescribed as the effort that goes on at kodak also.achieving a pioneering outlook with supercomputing95supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.discussionedward abrahams: have you who have tackled this problem of convincingusers to use the supercomputer found some techniques that were not soproductive? presumably you have mentioned some of the ones that areproductive. what techniques did not work, so we can avoid them?lawrence tesler: trying to convince somebody who is very negative isprobably the one thing that isn't worth doing. in other words it's important to findpeople who immediately see the benefits of supercomputing and to get them tostart using it. then their colleagues will realize that they can use supercomputingalso.beverly eccles: yes. seek the champions for the cause.clifford perry: i don't have too many keys to failure, but one key to successis involving the users from the very beginning in participative planning. weactually sent out letters to literally hundreds of the heavy users of our traditionalhighend mainframe computing facility, asking them to participate in an idealizeddesign of a supercomputing facility and to think about what the attributes of thatparticular center should be. would it offer oneonone collaborative assistance?would it offer transparency visavis using that computer or the highendmainframe? how would it be administered? how would it be charged out? whathelp would be rendered to the users?when only topdown decisions are made, people don't use the computers.the decisionmaking process has to be topdown, bottomup, middleout. wefocused on the bottomup and middleout, and then when larry smarr came andmapped what he had to offer against the idealized design that was documentedand was formulated by the participation of those whose lives would be affectedby the advent of the supercomputing facility, we found that we had animmediate, captured market.generally, it takes about 2 years to justify the use of a supercomputer onsite,and it takes upwards of $250,000 to $300,000, as has been published by theminnesota supercomputing consortium. it has to be done in a participativemanner, in my opinion, or it won't work.mel schmidt: could all the panelists briefly describe how they determineallocations within their organizations? are there any mechanisms for billing theusers?beverly eccles: within abbott, computer resources are basically free. theresources are supplied and the expense goes into the budget, but individuals arenot concerned about how much disk space or how much of the central processingunit they are using. those resources are simply available for us, and the scientistsfeel very comfortable in that environment.achieving a pioneering outlook with supercomputing96supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.clifford perry: at kodak we have an arrangement with ncsa that everyuser who logs onšand we have an administrative procedure to do thatšis billeddirectly in their division. we have allocated, if you will, $100,000 chunks to setsof people.lawrence tesler: at apple as at abbott, all the shared computer resourcesthat are used by more than one department are essentially free. on our financialreports from the apple product division, we break out the entire budget for thiscomputer operation. it is weighed as a whole as a percent of the entire r&dbudget.achieving a pioneering outlook with supercomputing97supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.achieving a pioneering outlook with supercomputing98supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.part dconcluding remarks 99supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved. 100supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved.9summarydoyle d. knightconsortium for scientific computingthe john von neumann national supercomputer centerthese proceedings of the symposium on supercomputers have focused ontwo major topics. first, jack worlton and steve chen described the evolution ofsupercomputer technology over the past 20 years and projected future trends inimproved processor performance, increased memory, and rapidly expandingparallelism. they also emphasized the role of algorithm development, noting thatimprovements in performance associated with the development of newalgorithms are increasingly important.second, three presentations focused on current applications ofsupercomputers in industry. beverly eccles (abbott laboratories), clifford perry(eastman kodak company), and larry tesler (apple computer, inc.) provided aseries of examples of applications of supercomputers in their corporations. twokey points were emphasized: supercomputers provide the opportunity to design new products, rangingfrom film emulsions to computer keyboards, with a degree of accuracyheretofore unachievable with conventional computers and at a costoftentimes far lower than that associated with experimental methods(e.g., development of prototypes). supercomputers can improve the productivity of designers bysignificantly reducing the time required to evaluate a new idea. the rapidfeedback of results enhances creativity.these three researchers also addressed the issue of integration ofsupercomputer technology into industry. four important points werestressed:summary101supercomputers: directions in technology and applicationscopyright national academy of sciences. all rights reserved. industry must first recognize the potential benefits of supercomputertechnology in research and development. a core of supercomputer "evangelists" must be established initiallywithin a corporation. this core groupšpeople who are dedicated tousing supercomputing as well as to explaining and communicating itsadvantages to other scientists in industryšwill provide the leadershipand incentive for the adoption of the technology by the larger group. "bottomup" planning is necessary for successful incorporation ofsupercomputers into industry. the current computer "habits" ofresearchers and designers must first be understood before any majorchanges can be implemented. close collaboration between academia and industry is needed to provideimproved software tools and training for students, who will become theresearchers in the industry of tomorrow.summary102
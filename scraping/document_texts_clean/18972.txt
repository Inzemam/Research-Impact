detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/18972future directions for nsf advanced computing infrastructureto support u.s. science and engineering in 20172020:interim report48 pages | 6 x 9 | paperbackisbn 9780309313797 | doi 10.17226/18972committee on future directions for nsf advanced computing infrastructure tosupport u.s. science in 20172020; computer science and telecommunicationsboard; division on engineering and physical sciences; national research councilfuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in interim...copyright national academy of sciences. all rights reserved.committee on future directions for nsf advanced computing infrastructure  to support u.s. science in 20172020computer science and telecommunications boarddivision on engineering and physical sciencesto support interim reportnsf advanced computinginfrastructurefuture directions for u.s. science and engineering in 20172020future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.the national academies press 500 fifth street, nw washington, dc 20001notice: the project that is the subject of this report was approved by the governing board of the national research council, whose members are drawn from the councils of the national academy of sciences, the national academy of engineering, and the institute of medicine. the members of the committee responsible for the report were chosen for their special competences and with regard for appropriate balance.this project was supported by the national science foundation, award oci1344417. any opinions, ndings, or conclusions expressed in this publication are those of the author(s) and do not necessarily re˚ect the view of the organizations or agencies that provided support for this project.international standard book number13: 9780309313797international standard book number10: 0309313791additional copies of this report are available from the national academies press, 500 fifth street, nw, keck 360, washington, dc 20001; (800) 6246242 or (202) 3343313; http://www.nap.edu.copyright 2014 by the national academy of sciences. all rights reserved.printed in the united states of americafuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.the national academy of sciences is a private, nonprot, selfperpetuating society of distinguished scholars engaged in scientic and engineering research, dedicated to the furtherance of science and technology and to their use for the general welfare. upon the authority of the charter granted to it by the congress in 1863, the academy has a mandate that requires it to advise the federal government on scientic and technical matters. dr. ralph j. cicerone is president of the national academy of sciences.the national academy of engineering was established in 1964, under the charter of the national academy of sciences, as a parallel organization of outstanding engineers. it is autonomous in its administration and in the selection of its members, sharing with the national academy of sciences the responsibility for advising the federal government. the national academy of engineering also sponsors engineering programs aimed at meeting national needs, encourages education and research, and recognizes the superior achievements of engineers. dr. c. d. mote, jr., is president of the national academy of engineering.the institute of medicine was established in 1970 by the national academy of sciences to secure the services of eminent members of appropriate professions in the examination of policy matters pertaining to the health of the public. the institute acts under the responsibility given to the national academy of sciences by its congressional charter to be an adviser to the federal government and, upon its own initiative, to identify issues of medical care, research, and education. dr. victor j. dzau is president of the institute of medicine.the national research council was organized by the national academy of sciences in 1916 to associate the broad community of science and technology with the academy™s purposes of furthering knowledge and advising the federal government. functioning in accordance with general policies determined by the academy, the council has become the principal operating agency of both the national academy of sciences and the national academy of engineering in providing services to the government, the public, and the scientic and engineering communities. the  council is administered jointly by both academies and the institute of medicine. dr. ralph j. cicerone and dr. c. d. mote, jr., are chair and vice chair, respectively, of the national research council.www.nationalacademies.orgfuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.vcommittee on future directions for nsf advanced computing infrastructure to support u.s. science in 20172020william d. gropp, university of illinois, urbanachampaign, cochairrobert harrison, stony brook university, cochairmark r. abbott, oregon state universitydavid arnett, university of arizonarobert l. grossman, university of chicagopeter m. kogge, university of notre damepadma raghavan, pennsylvania state universitydaniel a. reed, university of iowavalerie taylor, texas a&m university katherine a. yelick, university of california, berkeleyjon eisenberg, director, computer science and telecommunications board, and study directorshenae bradley, senior program assistantfuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.vicomputer science and telecommunications boardrobert f. sproull, university of massachusetts, amherst, chair luiz andré barroso, google, inc.steven m. bellovin, columbia universityrobert f. brammer, brammer technology, llcedward frank, brilliant lime and cloud parityseymour e. goodman, georgia institute of technology laura haas, ibm corporationmark horowitz, stanford universitymichael kearns, university of pennsylvaniarobert kraut, carnegie mellon university susan landau, worcester polytechnic institutepeter lee, microsoft corporationdavid e. liddle, us venture partners barbara liskov, massachusetts institute of technologyjohn stankovic, university of virginiajohn a. swainson, dell, inc.ernest j. wilson, university of southern californiakatherine yelick, university of california, berkeleystaffjon eisenberg, director virginia bacon talati, program ofcershenae bradley, senior program assistantrenee hawkins, financial and administrative manager herbert s. lin, chief scientistlynette i. millett, associate director eric whitaker, senior program assistantfor more information on cstb, see its website at  http://www.cstb.org; write to cstb, national research council, 500 fifth street, nw, washington, dc 20001; call (202) 3342605; or email cstb at cstb@nas.edu.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.viiadvanced computing, a term used in this report to include both  compute and dataintensive capabilities, is used to tackle a rapidly growing range of challenging science and engineering problems. the national science foundation (nsf) requested that the national research council (nrc) carry out a study examining anticipated priorities and associated tradeoffs for advanced computing in support of nsfsponsored science and engineering research. the study encompasses advanced computing activities and programs throughout nsf, including but not limited to, those of its division on advanced cyberinfrastructure. the statement of task for the full nrc study is given in box p.1. in response to this request, the nrc established the committee on future directions for nsf advanced computing infrastructure to support u.s. science in 20172020 (see appendix a). as part of the study, the sponsor also requested an interim report in 2014 that identies key issues and discusses potential options. the committee has begun its work by gathering and reviewing relevant materials, receiving testimony and comments from individuals, and identifying additional experts to receive testimony from and additional sources of information. the information collection is still incomplete, but some important issues have begun to come into focus. mindful that nsf seeks timely input for its budget process and that the issues raised in the study merit broad input from the science and engineering communities that use, develop, and provide advanced computing capabilities, the study committee offers this interim report to frame issues it believes that prefacefuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.viii prefacensf and the committee itself need to consider, and to stimulate discussion and encourage feedback to the committee on these issues.what follows is an initial compilation of issues that the com mittee believes will need to be considered as future nsf strategy, budgets, and programs for advanced computing are developed, together with key issues on which the committee invites comment. this list is preliminary, and the committee anticipates adding to and rening this list as it prepares its nal report. appendix b provides a supplemental set of questions focused on the needs of users of advanced computing on which the committee also invites comment. box p.1  statement of taska study committee will examine anticipated priorities and associated tradeoffs for advanced computing in support of national science foundation (nsf)sponsored science and engineering research. advanced computing capabilities are used to tackle a rapidly growing range of challenging science and engineering problems, many of which are compute, communications, and dataintensive as well. the committee will consider:1. the contribution of highend computing to u.s. leadership and competiveness in basic science and engineering and the role that nsf should play in sustaining this leadership; 2. expected future nationalscale computing needs: highend requirements, those arising from the full range of basic science and engineering research supported by nsf, as well as the computing infrastructure needed to support  advances in modeling and simulation as well as data analysis;3. complementarities and tradeoffs that arise among investments in supporting advanced computing ecosystems; software, data, communications;4. the range of operational models for delivering computational infrastructure, for basic science and engineering research, and the role of nsf support in these various models; and 5. expected technical challenges to affordably delivering the capabilities needed for worldleading scientic and engineering research.an interim report will identify key issues and discuss potential options. it might contain preliminary ndings and early recommendations. a nal report will include a framework for future decision making about nsf™s advanced computing strategy and programs. the framework will address such issues as how to prioritize needs and investments and how to balance competing demands for cyberinfrastructure investments. the report will emphasize identifying issues, explicating options, and articulating tradeoffs and general recommendations.the study will not make recommendations concerning the level of federal funding for computing infrastructure.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.preface ixsome issues will require further input and deliberation before the committee comments on them. for example, the committee has not yet devoted much attention to item 1 in the statement of task, regarding the contribution of highend computing to u.s. leadership and competiveness and the role that nsf should play in sustaining this leadership. it has also not addressed issues around data curation, access, and sustainability, which, although not central to the committee™s task, will nonetheless be important elements of nsf™s future strategy for advanced computing. we invite your feedback on this report and, more generally, your comments on the future of advanced computing at nsf. you may provide feedback by email to <sciencecomputing@nas.edu> or via the project™s public feedback page at <www.nas.edu/sciencecomputing>.william d. gropp and robert harrison, cochairscommittee on future directions for nsf advanced computing infrastructure to  support u.s. science in 20172020future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.xithis report has been reviewed in draft form by individuals chosen for their diverse perspectives and technical expertise, in accordance with procedures approved by the national research council™s report review committee. the purpose of this independent review is to provide candid and critical comments that will assist the institution in making its published report as sound as possible and to ensure that the report meets institutional standards for objectivity, evidence, and responsiveness to the study charge. the review comments and draft manuscript remain condential to protect the integrity of the deliberative process. we wish to thank the following individuals for their review of this report:amy w. apon, clemson university,daniel e. atkins iii, university of michigan,thom h. dunning, northwest institute for advanced computing,susan l. graham, university of california, berkeley,laura haas, ibm research,tony hey, microsoft research,michael l. klein, temple university, andlinda petzold, university of california, santa barbara.although the reviewers listed above have provided many constructive comments and suggestions, they were not asked to endorse the conclusions or recommendations, nor did they see the nal draft of the report before its release. the review of this report was overseen by elsa m. garmire, acknowledgment of reviewersfuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.xii acknowledgment of reviewersdartmouth college. appointed by the national research council, she was responsible for making certain that an independent examination of this report was carried out in accordance with institutional procedures and that all review comments were carefully considered. responsibility for the nal content of this report rests entirely with the authoring committee and the institution.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.xiiisummary 11 the role of advanced computing in science  7 and engineering complementary roles of simulation and dataintensive computing, 92 challenges 11 responding to growing demand, 11 growing demand, 11 the potential of dataintensive computing for nsf science and engineering and the corresponding requirements, 11 work˚ow, 12 technology challenges, 12 computeintensive challenges, 12 dataintensive challenges, 13 serving both data and computeintensive workloads, 14 software and algorithms for next generation cyberinfrastructure, 15 training the next generation of scientists, 15 demand and resource allocation, 16 demand for capacity, 16 demand for capability, 16contentsfuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.xiv contents the role of the private sector in providing advanced computing, 17 the role of other federal agencies in providing advanced computing, 17 allocation of research funding and computing  resources, 183 possible nsf responses 19 better understanding of science and engineering  opportunities, priorities, and requirements for  advanced computing, 19 functional rather than technologyfocused or structural approach to understanding requirements and  establishing priorities, 20 enhanced organizational stability and flexibility of  nsffunded advanced computing centers, 21 enhanced strategic planning and internal coordination, 21appendixesa biographies of committee members 25b questions on directions and needs for advanced  32cyberinfrastructure future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.1the national science foundation (nsf) asked the national research council to study anticipated priorities and associated tradeoffs for advanced computing in support of nsfsponsored science and engineering research. (see box p.1 in the preface for the complete statement of task.) this interim report contains a preliminary set of issues the committee on future directions for nsf advanced computing infrastructure to support u.s. science in 20172020 believes that nsf, the science and engineering research community, and the committee itself need to consider. it is intended to stimulate discussion and prompt feedback that the committee will consider in preparing its nal report. (see the preface for how to provide feedback to the study committee.)building advanced computing infrastructure to support integrated discoveryadvanced computing in this context refers to the technical capabilities that support compute and dataintensive research across the entire science and engineering spectrum and that are so expensive that they are shared among multiple researchers, institutions, and applications. computeintensive modeling and simulation, the historical focus of highperformance computing systems and programs, is an established peer, standing beside theory and experimentation, in the scientic process. dataintensive computing is emerging as a ﬁfourth paradigmﬂ for scientic discovery, complementing theory, experiment, and simulation, and summaryfuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.2 future directions for nsf advanced computing infrastructuremay require new technical and programmatic responses. compute and dataintensive approaches are increasingly used in combination: data is used to validate models, simulations are used to quantify uncertainty or ll in for incomplete theory, and stochastic models link modeling and data analytics. dataintensive computing is becoming more important as the volume of data grows, as new analytical techniques are adopted, and as some elds move from being primarily computeintensive to being much more dataintensive. for its nal report, the committee will explore and seeks comment on  1. how to create advanced computing infrastructure that enables integrated discovery involving experiments, observations, analysis, theory, and simulation.technology challengesunfavorable trends in power consumption and interchip communications are forcing consideration of new system architectures, the development of new algorithms and software approaches to use them, and more attention to redundancy and fault tolerance. absent new technology, the anticipated end of sustained reductions in the ratio of price to performance (a benet of moore™s law) portends stagnation in computer performance improvement. for dataintensive systems, variability in  storage hardware performance and failure rates constrain the performance and practical size of verylargescale systems. also, it will not be straightforward in all cases to keep scaling up system and scientic software to meet growing needs. the resulting uncertainty about technical direction complicates planning for future extremeperformance computers. today™s approach of federating distributed compute and data intensive resources to meet the increasing demand for combined computing and data capabilities is technically challenging and expensive. new approaches that colocate computational and data resources might reduce costs and improve performance. recent advances in cloud data center design may provide a viable integrated solution for a signicant fraction of (but not all) data and computeintensive and combined workloads. new algorithms and software approaches will be needed to effectively use systems with new architectures, and they can also play an important role in continuing to improve the performance of scientic codes and the productivity of researchers. some developments may best take place within individual research areas and disciplines, but others may benet from common, coordinated efforts.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.summary 3new knowledge and skills will be needed to effectively use these new advanced computing technologies. ﬁhybridﬂ disciplines such as computational science and data science and interdisciplinary teams may come to play an increasingly important role. for its nal report, the committee will explore and seeks comments on  2. technical challenges to building future, more capable advanced computing systems and how nsf might best respond to them.responding to growing demanddemand for advanced computing has been growing for all types and capabilities of systems, from large numbers of singlecommodity nodes to jobs requiring thousands of cores; for systems with fast interconnects; for systems with excellent data handling and management; and for an increasingly diverse set of applications that includes data analytics as well as modeling and simulation. anecdotal reports point to a low and perhaps declining rate of success for obtaining allocation of time on existing machines. given the ﬁdouble jeopardyﬂ that arises when researchers must clear two hurdlesšrst, to obtain funding for their research proposal and, second, to be allocated the necessary computing resourcesšthe chances that a researcher with a good idea can carry out the proposed work under such conditions is diminished.since the advent of its supercomputing centers, nsf has provided its researchers with stateoftheart computing systems. but it is unclear, given their likely cost, whether nsf will be able to invest in future highesttier systems in the same class as those being pursued by the department of energy, department of defense, and other federal mission agencies and overseas. options for providing highesttier capabilities that merit further exploration include purchasing computing services from federal agencies (thus increasing access beyond that driven by direct mission interests) or by making arrangements with commercial services (rather than more expensive purchases by individual researchers). more broadly, across a wide spectrum of system capability, the growth of new models of computing, including cloud computing and publically available but privately held data repositories, opens up new possibilities for nsf. access to these commercial facilities could widen access to largescale capabilities for computation and data analytics, but the cost tradeoffs are complicated and need to be looked at carefully.it is becoming increasingly difcult to balance investments in advanced computing facilities, given the large and growing aggregate future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.4 future directions for nsf advanced computing infrastructuredemand, the steep cost of the highestend systems, growing demand for dataintensive as well as computeintensive systems, and the constant or shrinking nsf resources. compounding the challenge is the wide variety of computing needs, the state of scientic data and software, and wide variation in ability to effectively use advanced computing across scientic disciplines. moreover, the range of science and engineering research sponsored by nsf involves a diverse set of work˚ows, including those that involve primarily compute or dataintensive processing and ones that involve combinations of both. it is thus harder than ever to understand the expanding and diverse requirements of the science and engineering community; explain the importance of a new, broader range of advanced computing infrastructure to stakeholders, including those that set budgets; explore nontraditional approaches; and manage the advanced computing portfolio strategically.  3. the committee will review data from nsf and the advanced computing programs it supports and seeks input, especially quantitative data, on the computing needs of individual research areas. for its nal report, the committee seeks comment on  4. the match between resources and demand for the full spectrum of systems, for both compute and dataintensive applications, and the impacts on the research community if nsf can no longer provide stateoftheart computing for its research community. 5. the role that private industry and other federal agencies can play in providing advanced computing infrastructurešincluding the opportunities, costs, issues, and service models, as well as balancing the different costs and making tradeoffs in accessibly (e.g., guaranteeing ondemand access is more costly than providing besteffort access). 6. the challenges facing researchers in obtaining allocations of computing resources and suggestions for improving the allocation and review processes for making advanced computing resources available to the research community.possible nsf responsesbetter understanding of science and engineering opportunities, priorities, and requirements for advanced computing not all research areas or programs have dened their requirements for advanced computing or established processes for regularly updating and rening them, such as by constructing roadmaps that describe science or engineering goals and advanced computing resources needed. future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.summary 5such analyses may provide useful information for understanding aggregate capability and capacity needs and expected trends in these needs, for understanding overall nsf resource requirements, for prioritizing investments, and for better aligning research programs and supporting advanced computing investments. because scientists can effectively use infrastructure only when it is presented as an integrated wholeš encompassing appropriate hardware, software, data, networking, technical services, and so forth; it may be most productive to use a functional rather than a technologyfocused or structural approach focused on individual elements. for its nal report, the committee will explore and seeks comment on  7. whether wider collection and more frequent updating of requirements for advanced computing could be used to inform strategic planning, priority setting, and resource allocation; how these requirements might be used; and how they might best be developed, collected, aggregated, and analyzed.enhanced organizational stability and flexibility of nsffunded advanced computing centers although nsf™s use of frequent open competitions has stimulated intellectual competition and increased nsf™s nancial leverage, it has also impeded collaboration among frequent competitors, made it more difcult to recruit and retain talented staff, and inhibited longerterm planning. for its nal report, the committee seeks comment on 8. the tension between the benets of competition and the need for continuity as well as alternative models that might more clearly delineate the distinction between performance review and accountability and organizational continuity and service capabilities. enhanced strategic planning and internal coordination advanced computing receives less attention in the current nsf  strategic plan than might be expected, given its vital role in science and engineering, although it is the subject of a separate strategy focused on cyberinfrastructure. decision making about advanced computing is distributed across the division for advanced cyberinfrastructure, other divisions and division programs, the major research instrumentation program, and individual research institutions. both coordination and future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.6 future directions for nsf advanced computing infrastructurestrategic decision making seem especially important in an era of growing demand and cost. topdown mandates often prove ineffective, even when the coordination is very much needed, and reaching consensus through ﬁgrassrootsﬂ efforts may be too slow. both topdown and bottomup processes require mechanisms for identifying detailed needs of the directorates and their programs and for ensuring adequate community input.for its nal report, the committee seeks comment on 9. how nsf might best coordinate and set overall strategy for advanced computingrelated activities and investments as well as the relative merits of both formal, topdown coordination and enhanced, bottomup process. future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.71the role of advanced computing in science and engineeringmany past reports have underscored the integral role of advanced computing in science and engineering, including but not limited to the role of computing in addressing scientic and engineering ﬁgrand challengesﬂ vital to our nation™s welfare, security, and competitiveness. over time, and especially in recent years, advanced computing has become relevant to an expanding set of scientic problems and disciplines. the term advanced computing is used in this report to refer to the full complement of capabilities that support compute and dataintensive research across the entire science and engineering spectrum, which are too expensive to be purchased by an individual research group or department and perhaps too expensive even for an individual research institution. the term also encompasses higherend computing for which there are economies of scale in establishing shared facilities rather than having each institution acquire, maintain, and support its own systems. for computeintensive research, it includes not only today™s ﬁsupercomputers,ﬂ which are able to perform more than 1015 ˚oating point operations per second (known as ﬁpetascaleﬂ) but also ﬁhighperformance computingﬂ platforms that share the same technologies as supercomputers but may have lower levels of performance. the terms capability and capacity are sometimes used to refer to the low and high end of the spectrum of computing performance of a single application. capability computing is computing that stretches the limits of available resources. for example, for computeintensive applications, it is the capability to run the largest possible tightly coupled computafuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.8 future directions for nsf advanced computing infrastructuretions (i.e., which can only be run practically on a single computer system). the concept of capability computing also applies to dataintensive applications, although how this might best be dened is an open question. faster machines have been deployed by the department of energy and elsewhere; these very fastest machines might be thought of as the ﬁextreme end.ﬂ capacity computing, by contrast, provides large amounts of computing but lower peak performance. what was capability computing 2 decades ago is capacity computing today, both in terms of individual computing needs and the number of jobs being run. the distinction is arguably somewhat articial: highperformance computers cover a wide range of performance characteristics, and large machines can be used to provide either capacity or capability. the need for researchers to reach solutions in a reasonable amount of time means that any large problem or large ensemble of problems, even those that do not require a capability machine to reach a solution, can be in some sense a capability problem. highthroughput computing refers to systems that provide large amounts of processing capacity over long periods of timešthat is, the number of operations available per month or year rather than per secondšbut not as high peak performance.since the beginning of the national science foundation™s (nsf™s) supercomputing centers program in the 1980s, nsf™s division of advanced cyberinfrastructure (aci) and its predecessor organizations have supported computational research across nsf with both supercomputers and other highperformance computers and provided services to a user base that spans work sponsored by all federal research agencies. modeling and simulation has for some time been seen as a true peer, standing beside theory and experiment, in the scientic process. it is used at a wide range of scales, as measured by the number of parallel cores needed. some problems in astrophysics, cosmology, or biomolecular model ing use massively parallel simulations and run on machines with tens to hundreds of thousands (or more) cores. other problems, in elds such as materials, climate simulation, and earthquake modeling, use large volumes of computation on ﬁmidscaleﬂ machines with a thousand or more cores, as do a wide array of applications of uncertainty quantication and other techniques for robust design and decision making. in addition, massive volumes of highthroughput simulations are used in combinatorial chemistry, drug design, design of functional materials, and systems design. dataintensive computing is beginning to emerge as a separate discipline and is being viewed by some as a ﬁfourth paradigmﬂ for scientic discovery, complementing discoveries made by theory, experiment, and simulation. in some disciplines, such as astronomy and biology, the perfuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.the role of advanced computing in science and engineering 9centage of research papers that are primarily based on data from data repositories of previously collected data, versus new experimental data, is increasing and reaching a point that this mode of discovery is now a signicant driver of research. networked sensors are increasingly embedded in urban and civil infrastructure, and sensors are widely used to capture research data in a growing number of elds. new algorithms for analyzing data sets that are large, complex, noisy, or unstructured allow automatic discovery of patterns within data that were previously unknown. web search engines, online shopping recommendations, and face recognition software are some wellknown applications of such algorithms, but these techniques are also increasingly valuable in science and engineering. internet companies, such as google, yahoo!, facebook, and amazon, have introduced new software and hardware platforms and new programming models for dataintensive computing, and these platforms and models are increasingly being used for scientic research. dataintensive research may require highperformance input/output (i/o) systems, access to very large storage systems using systems with different architectures than traditional highperformance computing systems, and new approaches to data visualization.complementary roles of simulation and dataintensive computingnsf™s historical emphasis on advanced computing for modeling and simulation is sometimes viewed as being in competition with the more recent interest in dataintensive computing. the relative need for one or the other is important when future advanced computing investments are considered, because the types of computer systems, storage systems, networks, software, usage models, stafng and support, industry partners, and organizational structures may be different (and possibly quite different) across these two broad categories of use. the needs of users in the two categories and the appropriate technical and organizational responses to those needs both require future study. it is misleading, however, to think of these two categories as competing in science and engineering, because modeling and data analysis are often used in concert. in cosmology, computational models are used to ll in missing or incomplete data; in imagebased scientic instruments such as synchrotrons, simulation may be used to ﬁinvertﬂ the observation into a particular crystal structure; in climate analysis, reconstruction of historical data is critical to the validation of models; and in the materials genome initiative, the results of millions of simulations are being stored and shared for community analysis. large experiments in elds like highenergy physics use simulation to design devices and to set up individual future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.10 future directions for nsf advanced computing infrastructureexperiments to maximize the likelihood of success. simulations produce large data sets that are part of the data challenge, and sophisticated simulations incorporate observational data to quantify uncertainty or ll in for incomplete theory. advanced computational models and algorithms are being fused with observational data and with more sophisticated and expensive techniques to accommodate and quantify uncertainty. fundamentally, scientic discovery goes beyond identifying patterns in data to discovering models that explain and predict those patterns. as these examples suggest, the fusion of computational modeling and data analytics pervades all of science and engineering. this is true of both large scientic collaborations and the work of individual investigators.modeling and largescale data analysis is also driving the development of a new class of stochastic models in many areas of research, such as earth system modeling. the present class of dynamically based models are being stretched to their limits because there is often little knowledge of the model parameters, let alone the dynamical form of critical processes such as cloud formation and rainfall. rather than continuing to improve model resolution and add more features, some climate researchers are advocating for a new approach based on stochastic models that will link models and largescale data analytics.thus, for many scientic disciplines, the issue is not whether to use data or simulation, but how the two will be used together. the need for advanced computing is important throughout disciplines, as the models, data, and types of scientic inquiry grow in sophistication. the increasing number of uses that combine computational models and observational data suggests that facilities supporting both are needed, and there may be value in colocating data and compute capabilities. although the technical challenges are many, the social, organizational, and funding challenges may be equally crucial.  1. for its nal report, the committee will explore and seeks comments on how to create advanced computing infrastructure that enables integrated discovery involving experiments, observations, analysis, theory, and simulation. future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.11responding to growing demandgrowing demanddemand for capacity and capability computing has been growing both in terms of computing requirements and the number of scientists and researchers involved. it is becoming increasingly difcult to balance investments, given the large and growing aggregate demand, the high cost of highend facilities, and the constant or shrinking national science foundation (nsf) resources. compounding the challenge is the wide variety across scientic disciplines in terms of computing needs, the state of scientic data and software, and the ability of researchers to effectively use advanced computing.these developments present new challenges for nsf as it seeks to understand the expanding requirements of the science and engineering community; explain the importance of a new broader range of advanced computing infrastructure to stakeholders, including those that set its  budget; explore nontraditional approaches; and manage the advanced computing portfolio strategically. the potential of dataintensive computing for nsf  science and engineering and the corresponding requirementsmultiple elds (e.g., materials science) are also transitioning from being primarily computeintensive (e.g., ab initio simulations in material 2challengesfuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.12 future directions for nsf advanced computing infrastructurescience) to being much more dataintensive (e.g., due to the rapid growth of experimental data, growing use of data analytics, and automation of calculations searching for materials with desired properties) and may not be prepared for this transition. some communities may lack sufcient national or communal hardware or software infrastructure to facilitate development of new work˚ows or to realize economies of scale and may not have leveraged best practices and investments established by other communities.workowwork˚ow refers to the series of computational steps required to yield a research result from the experimental and/or simulation results and to the tools and processes used to manage them and record the provenance of results. the range of science and engineering research sponsored by nsf involves a diverse set of work˚ows, including those that involve primarily compute or dataintensive processing or combinations of both. additionally, the compute and data capacities and the scale of parallelism required by these work˚ows can vary greatly by several orders of magnitude. the shift from generalpurpose central processing units (cpus) to more specialized architectures, such as hybrids of generalpurpose processors and graphical processing units, which have a much more highly parallel structure, further exacerbates the challenges of aligning the work˚ows with available computing capabilities. technology challengesa number of technology challenges will affect the ability of nsf and others to deliver the desired advanced computing capabilities to the science and engineering communities. they will require adaptations, such as recoding existing software and writing new software in new ways, while providing new opportunities for advanced computing users to make the necessary adaptations. computeintensive challengesit is an accepted truth today that moore™s law will end sometime in the next decade, causing signicant impact to highend systems. we have already transitioned through a major technologydriven change in 2004, driven by hitting a ﬁpower wallﬂ in our ability to cool processor chips, which has already rewritten the architectural landscape. already, graphics processing units (gpus) are providing a signicant increase in computing power per chip and per unit energy, but often at the cost future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.challenges 13of needing new algorithms and new software. data exchange capabilities between  processors are also under increasing pressure, because as the number of transistors and cores on a die continues to explode, the number of paths and rate at which we can signal over such paths is growing, at best, slowly. these trends are forcing consideration of new architecturesšpossibly distinct from the ones used to build conventional midrange systemsšand new software approaches in order to use them effectively. indeed, future growth in capabilities may come from an explosion of specialized hardware architectures that exploit the growth in the number of transistors on a chip. the transition implied by the anticipated end of moore™s law will be even more severešabsent development of disruptive technologies; it could mean, for the rst time in over three decades, the stagnation of computer performance and the end of sustained reductions in the priceperformance ratio. redundancy and fault tolerant algorithms are also likely to become more important. lastly, power consumption (and its associated costs) is now a signicant factor in the design of any large data center. for example, simple extrapolation of existing climate models to resolve processes such as cloud formation quickly lead to a computer that requires costly and possibly impractical amounts of electrical power. these challenges and the associated uncertainty pose signicant challenges when contemplating future investment in extreme performance computers. dataintensive challengesbuilding dataintensive systems that provide the needed scale and performance will require attention to several technical challenges. these include the following: managing variability and failure in storage components. verylargescale, dataintensive computing consists of large numbers of storage devices (typically disks), which are often commodity components and not the higherquality storage devices generally used in highperformance computing. although the probability of failure of any single device is low, the aggregate number of failures is high, as is the variability in time required for a device to perform a computation (those that take longer than would be expected from the performance of their peers are sometimes called stragglers). for example, a large part of the complexity of systems like hadoop is the result of dealing with failures and stragglers. research is needed to more efciently manage failure and variance, especially for a broader range of programming models.verylargescale scientic data management and analysis. although this is an active research area, it is still a challenge to manage data at future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.14 future directions for nsf advanced computing infrastructurethe petabyte (pb) to exabyte scale. file systems, data management systems, data querying systems, provenance systems, data analysis systems, statistical modeling systems, work˚ow systems, visualization systems, collaboration systems, and data sharing must all scale together. data analysis is typically an iterative process, and traditional scientic computing approaches often rely on software that was never designed to work at this scale. as a simple example, there is no opensource le or storage system that scales to 100 pb. on the other hand, the commercial sector has developed data management infrastructure over distributed le systems, which has produced a variety of new data management systems, sometimes called nosql (not only sql) systems. we are moving into an era of data access through a set of application programming interfaces (apis) rather than discrete les. adapting scientic software will be a challenge in this new environment.at scale interoperability of geographically distributed data centers. verylargeœscale, dataintensive computing relies more on external data resources than is usually the case with highperformance computing. some of the most interesting discoveries in data science have been made by integrating thirdparty and external data. analysis that uses data distributed across multiple locations requires costly, highcapacity network links, and its performance will in any event suffer compared to computation that uses data in a single location. for this reason, datacenterscale computing platforms benet by integrating at scale with other such facilities and the data repositories they contain.serving both data and computeintensive workloadsas discussed above, research increasingly involves both compute and dataintensive computing. what technical and system architectural approaches are best suited to handling this mix is an open question. federating distributed compute and dataintensive resources has repeatedly been found to present multiple additional costs and challenges, including, but not limited to, network latency and bandwidth, resource scheduling, security, and software licensing and versioning. overcoming these challenges could increase participation and diversify resources and might be essential to realizing new science and engineering frontiers by coupling capability computing with experiments producing large data. avoiding unnecessary federation by consciously colocating facilities might yield signicant cost savings and enhancements to both performance and capability. an additional complication is that many important scientic data collections are not currently hosted in existing scientic computing centers. recent advances in cloud data center design (including commodity processors and networks and virtualization of these resources) may make future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.challenges 15it costeffective for data centers to serve a signicant fraction of both dataintensive and computeintensive workloads. such an approach might also support different use models, such as access via cloud apis, that complement traditional batch queues. this may prove essential to opening nsf resources to use by new communities and enabling greater utilization. colocation of computing and data will be an important aspect of these new environments, and such approaches may work best when the bulk of the data exchange can be kept inside a data center.software and algorithms for next generation cyberinfrastructureas described above, most experts believe that the coming end of moore™s law and the long domination of complementary metaloxidesemiconductor (cmos) devices in computing will force signicant changes in computer architecture. successful exploitation of these new architectures will require the development of new software and algorithms that can use them effectively. new software and algorithms will also be needed for computation that uses cloud computing architectures. new algorithms and software techniques can also help improve the performance of codes and the productivity of researchers. adoption of either will depend on establishing incentives for their adoption into existing applications and use of appropriate metrics to evaluate the effectiveness of applications in context. for example, a code that only needs to run for, at most, a few hundred hours may not need to be very efcient, but one that will run for a million hours should be demonstrably efcient in terms of total run time, not ˚oatingpoint operations per second (flops). relaxing the ﬁnearperfectﬂ accuracy of computing may usher in a new era of ﬁapproximate computingﬂ to address system failures, including data corruption, given the massive scale of these new systems. any investment in cyberinfrastructure will need to take into account the need to update, and in many cases redevelop, the software infrastructure for research that has been developed over the past few decades. under these conditions, innovations in algorithms, numerical methods, and theoretical models may play a much greater role in future advances in computational capability. training the next generation of scientistsnew knowledge and skills will be needed to make effective use of new system architectures and software. ﬁhybridﬂ disciplines such as computational science and data science and interdisciplinary teams may come to play an increasingly important role. keeping abreast of a rapidly evolving suite of relevant technologies is challenging for many computer future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.16 future directions for nsf advanced computing infrastructurescience programs, especially those with limited partnerships with the private sector. most domain scientists rely on traditional software tools and languages and may not have ready access to knowledge or expertise about new approaches.  2. the committee will explore and seeks comments on the technical challenges to building future, more capable advanced computing systems and how nsf might best respond to them.demand and resource allocationdemand for capacitycomments from the science and engineering communities anecdotally suggest a pentup demand for advanced computing resources, such as unsatised extreme science and engineering discovery environment (xsede) allocation requests for already peerreviewed and nsffunded research. this need is across all types and capabilities of systems, from large numbers of singlecommoditynodes to jobs requiring thousands of cores, fast interconnects, and excellent data handling and management.  demand for capabilitysince the beginnings of the nsf supercomputing centers, nsf has provided its researchers with stateoftheart capability computing systems. today, the blue waters system at illinois and stampede at texas represent signicant infrastructure for capability computing, augmented by other systems that are part of xsede. today, it is unclear whether nsf will be able to invest in future highesttier capability systems. missionoriented agencies in the united states, such as the department of energy, as well as international research organizations, such as the partnership for advanced computing in europe or the ministry of science and technology in china, are pursuing systems that are at least an order of magnitude more powerful, for both computation and data handling, than current nsf systems. similarly, commercial cloud systems, while not an alternative for the kinds of applications that require tightly coupled capability systems, have massive aggregate computing and datahandling power.  3. the committee will review data from nsf and the advanced computing programs it supports and seeks input, especially quantitative data, on the computing needs of individual research areas. future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.challenges 17 4. the committee seeks comments on the match between resources and demand for the full spectrum of systems, for both compute and dataintensive applications, and the impacts on the research community if nsf can no longer provide stateoftheart computing for its research community. the role of the private sector in providing advanced computinghistorically, nsf has supported the acquisition of specialized research infrastructure through a variety of processes, including major research equipment and facilities construction and major research instrumentation programs, support for major centers, and individual grants. in many cases, the private sector has provided equipment and expertise, but the private sector has not provided nsf researchers with a signicant source of computing cycles or resources. the growth of new models of computing, including cloud computing and publically available but privately held data repositories, opens up new possibilities for nsf. for example, by supporting some footprint in commercial cloud environments, many more nsf researchers could have the ability to access compute and data capabilities at a scale currently only available to a few researchers and commercial users. for some elds, this could be transformative. one of the benets of cloud computing is the ˚exible way in which resources are provided on demand to the users. evidence from several studies suggests that this ˚exibility comes with a monetary cost (which may not be competitive with nsfsupported facilities) that must be balanced against the opportunity cost, in terms of scientic productivity, in the conventional model of allocations and jobs queues. on the other hand, virtualization, the implied ability to migrate work, and limited oversubscription can work to decrease overall costs, increase overall system throughput, and increase the ability of the system to meet ˚uctuating workloads, although perhaps at the expense of the performance of an individual job. the cost tradeoffs are complicated and need to be looked at carefully. the role of other federal agencies in providing advanced computingresearchers funded by one agency sometimes make use of computing resources provided by other federal agencies. today, allocations are made by the agency that operates the advanced computing system on the basis of scientic merit and alignment with agency mission. other arrangements are possible. nsf could directly purchase advanced computing services from another federal agency. it could also join with other agencies future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.18 future directions for nsf advanced computing infrastructureto contract from a commercial provider or coordinate with other agencies on specifying services and costs in developing requests for proposals for commercial services. 5. the committee seeks comments on the role that private industry and other federal agencies can play in providing advanced computing infrastructurešincluding the opportunities, costs, issues, and service models. it also seeks input on balancing the different costs and on making tradeoffs in accessibly (e.g., guaranteeing ondemand access is more costly than providing besteffort access).allocation of research funding and computing resourcesa particular issue that has surfaced in the committee™s work so far is the ﬁdouble jeopardyﬂ that arises when researchers must clear two hurdles: getting their research proposals funded and getting their requests for computing resources allocated. given the modest acceptance rates of both processes, such a process necessarily diminishes the chances that a researcher with a good idea can in fact carry out the proposed work. relatedly, researchers also do not know in advance on what machine they will be granted an allocation, which may cause them to incur the cost and delay needed to ﬁportﬂ data and code to a new system (and possibly new system architecture) in order to use the allocation.  6. the committee seeks comments on the challenges facing researchers in obtaining allocations of computing resources and suggestions for improving the allocation and review processes for making advanced computing resources available to the research community. future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.193possible nsf responsesbetter understanding of science and engineering opportunities, priorities, and requirements for advanced computing although the critical role of advanced computing in science and engineering is well understood and a number of reports have been prepared to address foundationwide or disciplinary requirements, not all research areas or programs have dened their requirements for advanced computing or established processes for regularly updating and rening them, such as by constructing roadmaps that describe science and engineering goals and advanced computing resources needed. one example is the report of the snowmass 2013 computing frontier working group on lattice field theory.1 such analyses may provide additional useful information for understanding aggregate capability and capacity needs and expected trends in these needs, for understanding overall national science foundation (nsf) resource requirements, for prioritizing investments, and for aligning research program and supporting advanced computing investments. such communityled efforts seem a natural t for nsf. 1 t. blum, r.s. van de water, d. holmgren, r. brower, s. catterall, n. christ, a. kronfeld, et al., lattice eld theory for the energy and intensity frontiers: scientic goals and computing needs, report of the snowmass 2013 computing frontier working group on lattice field theory, arxiv:1310.6087v1, submitted october 23, 2013.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.20 future directions for nsf advanced computing infrastructure 7. for its nal report, the committee will explore and seeks comment on whether wider collection and more frequent updating of requirements for advanced computing could be used to inform strategic planning, priority setting, and resource allocation; how these requirements might be used; and how they might best be developed, collected, and aggregated. functional rather than technologyfocused or structural approach to understanding requirements and establishing prioritiesin 2009, the nsfwide advisory committee for cyber infrastructure (acci) established six task forces to investigate longterm cyberinfrastructure issues with a focus on its major elements, including high performance computing, campus bridging, grand challenges, data, and software. their nal reports in 20112 provide detailed descriptions of the state of the major elements of nsf™s advanced cyberinfrastructure ecosystem and recommendations for advancing them in support of research. new initiatives aligned with these recommendations have resulted in advances in the state of key elements of cyberinfrastructure, including software, data, hardware, and networking. at rst glance, it is once again tempting to seek a structural approach similar to the one used in the acci task forces whereby one considers prioritizing investments for the major elements of the ecosystem, such as hardware, software, storage, etc. such a structural approach could potentially lead to optimal solutions for each element by resolving the complex tradeoffs that arise when these elements contend for resources. however, none of these elements can be directly utilized by the science and engineering user community. in fact, scientists can effectively utilize infrastructure only when it is presented to them as an integrated whole encompassing appropriate hardware, software, data, networking, technical services, etc. additionally, customizations to meet the requirements of work˚ows along broad thematic areas can further enhance utility to catalyze the next generation of scientic outcomes. in this approach, requirements can be used to understand the needed functional capabilities. the latter, in turn, could be used to inform how strategic investments should be made.2 see national science foundation, ﬁaccištask forces,ﬂ http://www.nsf.gov/cise/aci/taskforces (accessed september 25, 2014).future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.possible nsf responses 21enhanced organizational stability and flexibility of nsffunded advanced computing centersany funding and organizational structure must balance organizational stability and sustainability against responsiveness to technological change and customer needs. nsf has long supported leadingedge cyberinfrastructure via a series of solicitations and open competitions. although this has stimulated intellectual competition and increased nsf™s nancial leverage, it has also made deep and sustainable collaboration difcult among frequent competitors. individual awardees, quite rationally, often focus more on maximizing their longterm probability of continued funding, rather than adapting and responding to community needs.frequent competitions can also make it more difcult for nsffunded service providers to recruit and retain talented staff when the horizon for funding is only 25 years. this is especially true when the competition for information technology and computational science expertise with industry is so great. in contrast, longer horizons could also let nsf and its service providers evolve services and stafng in response to changing community needs and business partnerships. in turn, major research equipment and facilities construction (mrefc) projects could coordinate and plan computing support and data analysis needs with nsf™s cyberinfrastructure providers. longerterm funding horizons could also allow service providers to work more collaboratively with nsf on responses to community needs, encourage interorganizational collaboration, and facilitate longerterm budget planning and staged equipment acquisitions across multiple sites. 8. the committee seeks comments on the tension between the benets of competition and the need for continuity as well as alternative models that might more clearly delineate the distinction between performance review and accountability and organizational continuity and service capabilities. enhanced strategic planning and internal coordinationdespite its vital role in science and engineering, the committee observes that advanced computing receives relatively little attention in the current nsf strategic plan, and decision making about advanced computing is distributed across the division for advanced cyberinfrastructure, other divisions and division programs, the major research instrumentation program, and individual research institutions. both coordination and strategic decision making seem especially important in an era of growing demand and cost, and place a premium on shared solutions where possible. future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.22 future directions for nsf advanced computing infrastructuretopdown mandates often prove ineffective, even when the coordination is very much needed, and reaching consensus through ﬁgrassrootsﬂ efforts may be too slow. both topdown and bottomup processes require mechanisms for identifying detailed needs of directorates and their programs and for ensuring adequate community input; the committee will be exploring and seeks comment on ways this might be done. 9. the committee seeks comments on how nsf might best coordinate and set overall strategy for advanced computingrelated activities and investments as well as the relative merits of both formal, topdown coordination and enhanced, bottomup process. future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.appendixesfuture directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.25abiographies of committee memberswilliam gropp, cochair, is the thomas m. siebel chair in computer science at the university of illinois, urbanachampaign, where he is also founding director of the parallel computing institute. he held the positions of assistant (19821988) and associate (19881990) professor in the computer science department at yale university. in 1990, he joined the numerical analysis group at argonne, where he was a senior computer scientist in the mathematics and computer science division, a senior scientist in the department of computer science at the university of chicago, and a senior fellow in the argonnechicago computation institute. from 2000 through 2006, he was also deputy director of the mathematics and computer science division at argonne. in 2007, he joined the university of illinois, urbanachampaign, as the paul and  cynthia saylor professor in the department of computer science. in 2008, he was appointed deputy director for research for the institute of advanced computing applications and technologies at the university of illinois. his research interests are in parallel computing, software for scientic computing, and numerical methods for partial differential equations. he has played a major role in the development of the mpi messagepassing standard, is one of the designers of the petsc parallel numerical library, and has developed efcient and scalable parallel algorithms for the solution of linear and nonlinear equations. dr. gropp is a fellow of the association for computing machinery (acm), the institute of electrical and electronics engineers (ieee), and the society for industrial and applied mathematics (siam), and a member of the national academy future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.26 future directions for nsf advanced computing infrastructureof engineering. he received the sidney fernbach award from the ieee computer society in 2008 and the technical committee on scalable computing award for excellence in scalable computing in 2010. dr. gropp received his b.s. in mathematics from case western reserve university, an m.s. in physics from the university of washington, and a ph.d. in computer science from stanford university.robert harrison, cochair, is the director, institute of advanced scientic computing, at stony brook university and director, computational science center, brookhaven national laboratory. the core mission of the new stony brook institute is to advance the science of computing and its applications to solving complex problems in the physical sciences, the life sciences, medicine, sociology, industry, and nance. it works closely with the brookhaven center, which specializes in dataintensive computing. dr. harrison™s research interests are focused on scientic computing and the development of computational chemistry methods for the world™s most technologically advanced supercomputers. from 2002 to 2012, he was director of the joint institute of computational science, professor of chemistry and corporate fellow at the university of tennessee and oak ridge national laboratory. prior positions were at the environmental molecular sciences laboratory, pacic northwest laboratory, and argonne national laboratory. he has a prolic career in highperformance computing with more than 100 publications on the subject, as well as extensive service on national advisory committees. he received his b.a. from churchill college, university of cambridge, and his ph.d. in organic and theoretical chemistry from the university of cambridge.mark abbott is dean of the college of earth, ocean, and atmospheric sciences at oregon state university (osu). dr. abbott has been at osu since 1988 and has been dean of the college since 2001. prior to his appointments at osu, he served as a member of the technical staff at the jet propulsion laboratory (jpl) and as a research oceanographer at scripps institution of oceanography. dr. abbott™s research focuses on the interaction of biological and physical processes in the upper ocean and relies on both remote sensing and eld observations. he is a pioneer in the use of satellite ocean color data to study coupled physical/biological processes. as part of a nasa earth observing system interdisciplinary science team, dr. abbott led an effort to link remotely sensed data of the southern ocean with coupled ocean circulation/ecosystem models. his eld research included the rst deployment of an array of biooptical moorings in the southern ocean as part of the u.s. joint global ocean flux study. dr. abbott was a member of the national science board from 2006 to 2012 and served as a consultant to the board until 2013. he is the future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.appendix a 27vice chair of the oregon global warming commission. he is currently a member of the board of trustees for the consortium for ocean leadership and the board of trustees of neon, inc. his past advisory posts include chairing the coastal ocean applications and science team for noaa and chairing the u.s. joint global flux study science steering committee. he has also been a member of the director™s advisory council for the jpl and nasa™s modis and seawifs science teams and the earth observing system investigators working group. he was the 2011 recipient of the jim gray escience award, presented by microsoft research. dr. abbott is a national associate member of the national academies and is currently a member of the national research council™s (nrc™s) space studies board, chair of the committee on earth science and applications from space, a member of the committee to advise the u.s. global change research program, and a member of the panel on the review of the draft 2013 national climate assessment (nca) report. among his prolic nrc service, dr. abbott served on the nrc™s committee on evaluating nasa™s strategic direction, the committee on the assessment of nasa™s earth science programs, the committee on the role and scope of missionenabling activities in nasa™s space and earth science missions, and the panel on landuse change, ecosystem dynamics and biodiversity for the 2007 earth science and applications from space decadal survey. dr. abbott received his b.s. in conservation of natural resources from the university of california, berkeley, and his ph.d. in ecology from the university of california. david arnett is professor of astrophysics at the steward observatory of the university of arizona. he is a theoretical astrophysicist who rst demonstrated how explosive nucleosynthesis in supernovae produces the elements from carbon through iron and nickel. he constructed quantitative theoretical models of evolving massive stars and showed that the ejecta produce a good t to the abundance of heavy elements in the galaxy. his research interests include nuclear astrophysics, formation of neutron stars and black holes, highperformance computers, theoretical physics, hydrodynamics, thermonuclear burning, stellar evolution, computer graphics, and computer modeling. dr. arnett is a member of the national academy of sciences. dr. arnett received his ph.d. in physics from yale university. robert grossman is a faculty member at the university of  chicago. he is the director of the center for data intensive science, a senior  fellow and core faculty in the computation institute and the institute for  genomics and systems biology, and a professor of medicine in the section of genetic medicine. he also serves as the chief research informatics future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.28 future directions for nsf advanced computing infrastructureofcer for the biological sciences division. his research group focuses on data intensive computing, data science, and bioinformatics. he is the founder and a partner of open data group, which provides analytic services to help companies build predictive models over big data, and is the director of the notforprot open cloud consortium, which provides cloud computing infrastructure to support the research community. he was elected a fellow of the aaas in 2013. dr. grossman earned his ph.d. in applied mathematics at princeton university and an a.b. in mathematics from harvard university.peter kogge is a professor of computer science and engineering and concurrent professor of electrical engineering at the university of notre dame. dr. kogge was with ibm, federal systems division, from 1968 until 1994, and was appointed an ieee fellow in 1990, and an ibm fellow in 1993. in 1977 he was a visiting professor in the ece department at the university of massachusetts, amherst. from 1977 through 1994, he was also an adjunct professor in the computer science department of the state university of new york at binghamton. in 1994, he joined the university of notre dame as rst holder of the endowed mccourtney chair in computer science and engineering (cse). starting in the summer of 1997, he has been a distinguished visiting scientist at the center for integrated space microsystems at jpl. he is also the research thrust leader for architecture in notre dame™s center for nano science and technology. for the 20002001 academic year, he was the interim schubmehlprein chairman of the cse department at notre dame. from august 2001 until december 2008, he was the associate dean for research, college of engineering; since fall 2003, he has been a concurrent professor of electrical engineering. his current research areas include massively parallel processing architectures, advanced vlsi and nano technologies and their relationship to computing systems architectures, non von  neumann models of programming and execution, parallel algorithms and applications, and their impact on computer architecture. while at ibm, one of his groups designed the rst multiprocessor pim device with signicant dram memory that may also, arguably, be the world™s rst multicore chip. a paper on its architecture received the daniel  slotnick award at the 1994 international conference on parallel processing. he also designed and built the rtais parallel processor. prior parallel machines included the ibm 3838 array processor, and the space shuttle input/output processor (iop), which probably represents the rst true parallel processor to ˚y in space and is one of the earliest examples of multithreaded architectures. dr. kogge received the ieee seymour cray award in 2012 and the ieee charles babbage award in 2014. he received his b.s. in electrical engineering from the university future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.appendix a 29of notre dame, his m.s. in systems and engineering from syracuse university, and his ph.d. in electrical engineering from stanford university. padma raghavan is the associate vice president for research and director of strategic initiatives at the pennsylvania state university, where she is also a distinguished professor of computer science and engineering. dr. raghavan is the founding director of the penn state institute for cyberscience, the coordinating unit on campus for developing interdisciplinary computation and dataenabled science and engineering. prior to joining penn state in 2000, she served as an associate professor in the department of computer science at university of tennessee. her research is in the area of highperformance computing and computational science and engineering. she has more than 95 peerreviewed publications in three major areas, including scalable parallel computing; energyaware supercomputing, i.e., performance and power scalability of advanced computer systems; and computational modeling, simulation, and knowledge extraction. dr. raghavan currently serves on the editorial boards of the siam book series computational science and engineering and software, environments and tools, the journal of parallel and distributed computing, the journal of computational science, and ieee transactions on parallel and distributed systems. she serves on the program committees of major conferences sponsored by acm, ieee, and siam, and she cochaired technical papers for supercomputing 2012 and the 2011 siam conference on computational science and engineering. dr. raghavan also serves on various advisory and review boards, including the nrc panel on digitization and communication science, the network for earthquake engineering simulation, and the computer research association™s committee on the status of women in computing research. she is a fellow of the ieee, and she received an nsf career award and the maria goeppertmayer distinguished scholar award from the university of chicago and the argonne national laboratory for her research on parallel sparse matrix computations. dr. raghavan received her ph.d. in computer science from penn state. daniel a. reed is currently vice president for research and economic development, as well as a professor of computer science, electrical and computer engineering, and medicine at the university of iowa. he also holds the university computational science and bioinformatics chair at iowa. dr. reed was a corporate vice president at microsoft from 2009 to 2012, responsible for global technology policy and extreme computing, and director of scalable and multicore computing at microsoft from 2007 until 2009. prior to microsoft, he was the founding director of the  renaissance computing institute at the university of north carolina, future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.30 future directions for nsf advanced computing infrastructurechapel hill, where he also served as chancellor™s eminent professor and vice chancellor for information technology. before joining the university of north carolina, chapel hill, in 2003, dr. reed was director of the national center for supercomputing applications (ncsa), gutgsell professor and head of the department of computer science at the university of  illinois, urbanachampaign. he was appointed to the president™s council of advisors on science and technology (pcast) by president bush in 2006 and served on the president™s information technology advisory committee (pitac) from 20032005. as chair of pitac™s com putational science subcommittee, he was lead author of the report  computational science: ensuring america™s competitiveness. on pcast, he cochaired the networking and information technology subcommittee (with george scalise of the semi conductor industry association) and coauthored a report on the networking and information technology research and development (nitrd) program called leadership under challenge: information technology r&d in competitive world. dr. reed is the past chair of the board of directors of the computing research association (cra) and currently serves on its government affairs committee. cra represents the research interests of the university, national laboratory, and industrial research laboratory communities in computing across north america. he received his b.s. from the university of  missouri, rolla, and his m.s. and ph.d. degrees from  purdue university, all in computer science. valerie taylor is the senior associate dean of academic affairs in the dwight look college of engineering and the regents professor and royce e. wisenbaker professor in the department of computer science and engineering at texas a&m university. in 2003, she joined texas a&m as the department head of computer science and engineering, where she remained in that position until 2011. prior to joining texas a&m, dr. taylor was a member of the faculty in the electrical engineering and computer sciences department at northwestern university for 11 years. she has authored or coauthored more than 100 papers in the area of highperformance computing. she is also the executive director of the center for minorities and people with disabilities in it. dr. taylor is an ieee fellow and has received numerous awards for distinguished research and leadership, including the 2001 ieee harriet b. rigas award for a woman with signicant contributions in engineering education, the 2002 outstanding young engineering alumni from the university of california, berkeley, the 2002 cra nico habermann award for increasing the diversity in computing, and the 2005 tapia achievement award for scientic scholarship, civic science, and diversifying computing. dr. taylor is a member of the acm. she earned her b.s. in electrical and computer engineering and m.s. in computer engineering from purdue future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.appendix a 31university and a ph.d. in electrical engineering and computer sciences from the university of california, berkeley.katherine yelick is a professor of electrical engineering and computer sciences at the university of california, berkeley, and the associate laboratory director for computing sciences at lawrence berkeley national laboratory. dr. yelick is known for her research in parallel languages, compilers, algorithms, and libraries. she coinvented the upc and  titanium languages and developed analyses, optimizations, and runtime systems for their implementation. she has also done research on memory hierarchy optimizations, communicationavoiding algorithms, and automatic performance tuning, including developing the rst autotuned sparse matrix library. in her current role as associate laboratory director, she manages an organization that includes national energy research scientic computing center (nersc), the energy science network (esnet), and the computational research division. she was the director of nersc from 2008 to 2012. dr. yelick has received multiple research and teaching awards, including the athena award, and she is an acm fellow and an ieee senior member. she is a member of the california council on science and technology, the nrc computer science and telecommunications board (cstb), and the science and technology committee overseeing research at los alamos and lawrence livermore national laboratories. she earned her ph.d. in electrical engineering and computer science from the massachusetts institute of technology. future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.32bquestions on directions and needs for  advanced cyberinfrastructurethe committee seeks input from the community on the directions and needs for cyber infrastructure and provides a list of key issues in the body of the report. this appendix contains additional issues and questions on which the committee will be asking input. the committee seeks both responses to these questions and suggestions for other issues on which to request input.general issues foundational advances in science and engineering.advanced by largescale data analytics and data mining not currently possible in research infrastructures.emergent technologies and algorithms, balance between experimental and ﬁproductionﬂ systems, education and workforce development, community software) required to support sustained advances in u.s. science.(e.g., campus, regional, national; problemfocused or multipurpose) to the items above, identifying those that can be most positively affected by the national science foundation (nsf). these should encompass economic, crossagency, and international considerations.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.appendix b 33questions for users of advanced computing infrastructure1. research needs/opportunities a. what are some of the open problems in your eld that require largescale simulation to solve? which might lead to fundamental or foundational advances? why are these problems not being solved today? b. what are some of the open problems in your eld that require dataintensive computing, such as largescale data analytics and data mining? why are these problems not being solved today? c. are there plans or roadmaps that characterize future computing needs in your eld?  d. what types of new work˚ows are emerging that require complex access pathways between data sets, computation, and storage?2. advanced computing capabilities, facilities, requirements e. what forms of computing are used in your eld? for example, how does your eld make use of laptop/desktops, research group clusters, department or campus commodity cluster systems, mid to largescale, shared capacity systems such as xsede, leadershipclass capability systems such as blue waters (nsf) or mira (department of energy), or commercial cloud services such as amazon ec2? how would you characterize the importance of access to each typešrequired, desirable, or unnecessary? how might these needs change in the future, and why?  f. how are data sets evolving in terms of variety and distribution? do you access tens to hundreds of nearrealtime data sets? do you rely on a few large repositories? g. with computer hardware and software evolving more rapidly than in the recent past, what impacts do you see for your eld? for example, what role will new hardware such as accelerators (gpus or intel xeon phi), fpgas, new memory systems, or new i/o systems play? are there barriers to their adoption, such as challenges making necessary modications to software? h. what software does your eld depend on? who develops and maintains this code, and how is this work supported? i. is your eld keeping up the technical skills needed to use new technical capabilities?3. challenges and suggestions j. what are the biggest challenges that your eld faces in using computation? consider access to systems with sufcient capability and capacity; productivity of environments; algorithms; workforce; stability of software and hardware; and the ability to use systems efciently, including parallelism and scalability.future directions for nsf advanced computing infrastructure to support u.s. science and engineering in ...copyright national academy of sciences. all rights reserved.34 future directions for nsf advanced computing infrastructure k. what investments would have the greatest positive impact on your research eld? for example, this could be more computer systems to increase access, different kinds of systems with a different balance of capability, support for community software, development of new algorithms, or a workforce with better training in computational science. l. what other elements of national cyber infrastructure would signicantly advance the pace of discovery or expand participation? examples might include shared le systems or standard services and application program interfaces.
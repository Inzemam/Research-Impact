detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/1982computing the future: a broader agenda for computer scienceand engineering288 pages | 6 x 9 | paperbackisbn 9780309047401 | doi 10.17226/1982committee to assess the scope and direction of computer science and technology,national research councilcomputing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.computing thefuturejuris hartmanis and herbert lin, editorscommittee to assess the scope and direction of computer scienceand technologycomputer science and telecommunications boardcommission on physical sciences, mathematics, and applicationsnational research councilnational academy presswashington, d.c. 1992icomputing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.national academy press 2101 constitution avenue, nw washington, dc 20418notice: the project that is the subject of this report was approved by the governing board of thenational research council, whose members are drawn from the councils of the national academyof sciences, the national academy of engineering, and the institute of medicine. the members ofthe committee responsible for the report were chosen for their special competences and with regardfor appropriate balance.this report has been reviewed by a group other than the authors according to proceduresapproved by a report review committee consisting of members of the national academy of sciences, the national academy of engineering, and the institute of medicine.support for this project was provided by the following organizations and agencies: nationalscience foundation (grant no. cda9012458), office of naval research (contract no.n0001487j1110), air force office of scientific research (n0001487 j1110), and the association for computing machinery, inc., under an unnumbered contract.library of congress catalogingin publication datacomputing the future : a broader agenda for computer sc ience and engineering / committee toassess the scope and direction of computer science and technology, computer science andtelecommunications board, commission on physical sciences, mathematics, and applications,national research council ; juris hartmanis and herbert lin, editors.p. cm.includes bibliographical references and index.isbn 03090474041. computer science. 2. engineering. i. hartmanis, juris. ii. lin, herbert. iii.national research council (u.s.). committee to assess the scope and direction of computer science and technology.qa76.c5855 1992004`.0973šdc20 9219571cipcopyright 1992 by the national academy of sciencesthis book is printed with soy ink on acid free recycled stock. printed in the united states of america first printing, july 1992 second printing, october 1992iicomputing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.committee to assess the scope and directionof computer science and technologyjuris hartmanis, cornell university, chairmanruzena bajcsy, university of pennsylvaniaashok k. chandra, ibm t.j. watson research centerandries van dam, brown universityjeff dozier, university of california at santa barbarajames gray, digital equipment corporationdavid gries, cornell universitya. nico habermann,* carnegie mellon universityrobert r. johnson, university of utahleonard kleinrock, university of california at los angelesm. douglas mcilroy, at&t bell laboratoriesdavid a. patterson, university of california at berkeleyraj reddy, carnegie mellon universityklaus schulten, university of illinois at urbanachampaigncharles seitz, california institute of technologyvictor vyssotsky, digital equipment corporationstaffmarjory s. blumenthal, directorherbert s. lin, senior staff officerdonna f. allen, administrative assistant*resigned from the committee on october 1, 1991, in order to become assistant directorof the computer and information science and engineering directorate of the nationalscience foundation.iiicomputing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.computer science and telecommunicationsboardjoseph f. traub, columbia university, chairmanalfred v. aho, at&t bell laboratoriesruzena bajcsy, university of pennsylvaniadavid j. farber, university of pennsylvaniasamuel h. fuller, digital equipment corporationjohn l. hennessy, stanford universitymitchell d. kapor, on technology, inc.sidney karin, san diego supercomputer centerken kennedy, rice universityleonard kleinrock, university of california at los angelesrobert l. martin, bell communications researchabraham peled, ibm t.j. watson research centerwilliam press, harvard universityraj reddy, carnegie mellon universityjerome h. saltzer, massachusetts institute of technologymary shaw, carnegie mellon universityedward shortliffe, stanford university school of medicineivan e. sutherland, sun microsystemslawrence g. tesler, apple computer, inc.george l. turin, teknekron corporationwillis h. ware, the rand corporationwilliam wulf, university of virginiamarjory s. blumenthal, directorherbert s. lin, senior staff officermonica krueger, staff officerrenee a. hawkins, staff associatedonna f. allen, administrative assistantarthur l. mccord, project assistantivcomputing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.commission on physical sciences,mathematics, and applicationsnorman hackerman, robert a. welch foundation, chairmanpeter j. bickel, university of california at berkeleygeorge f. carrier, harvard universitygeorge w. clark, massachusetts institute of technologydean e. eastman, ibm t.j. watson research centermarye anne fox, university of texasphillip a. griffiths, institute for advanced studyneal f. lane, rice universityrobert w. lucky, at&t bell laboratoriesclaire e. max, lawrence livermore laboratorychristopher f. mckee, university of california at berkeleyjames w. mitchell, at&t bell laboratoriesrichard s. nicholson, american association for the advancement ofsciencealan schriesheim, argonne national laboratorykenneth g. wilson, ohio state universitynorman metzger, executive directorvcomputing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.the national academy of sciences is a private, nonprofit, selfperpetuating society of distinguished scholars engaged in scientific and engineering research, dedicated to the furtherance of science and technology and to their use for the general welfare. upon the authority of the charter granted to it by the congress in 1863, the academy has a mandate that requires it to advise the federal government on scientific and technical matters. dr. bruce m. alberts is president of the national academy of sciences. the national academy of engineering was established in 1964, under the charter of the national academy of sciences, as a parallel organization of outstanding engineers. it is autonomous in its administration and in the selection of its members, sharing with the national academy of sciences the responsibility for advising the federal government. the national academy of engineering also sponsors engineering programs aimed at meeting national needs, encourages education and research, and recognizes the superior achievements of engineers. dr. wm. a. wulf is president of the national academy of engineering. the institute of medicine was established in 1970 by the national academy of sciences to secure the services of eminent members of appropriate professions in the examination of policy matters pertaining to the health of the public. the institute acts under the responsibility given to the national academy of sciences by its congressional charter to be an adviser to the federal government and, upon its own initiative, to identify issues of medical care, research, and education. dr. harvey v. fineberg is president of the institutedicine. the national research council was organized by the national academy of sciences in 1916 to associate the broad community of science and technology with the academy™s purposes of furthering knowledge and advising the federal government. functioning in accordance with general policies determined by the academy, the council has become the principal operating agency of both the national academy of sciences and the national academy of engineering in providing services to the government, the public, and the scientific and engineering communities. the council is administered jointly by both academies and the institute of medicine. dr. bruce m. alberts and dr. wm. a. wulf are chair and vice chair, respectively, of the national research council. www.nationalacademies.orgvicomputing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.prefacein april 1990, the computer science and technology board (now thecomputer science and telecommunications board (cstb)) of the nationalresearch council formed the committee to assess the scope and direction ofcomputer science and technology. composed of 16 individuals from industryand academia, the committee was charged with assessing how best to organizethe conduct of research and teaching in computer science and engineering(cs&e) in the future. the committee took a broad outlook on its charge butchose to focus its efforts primarily on academic cs&e, which is both a majorsource of trained personnel at all levels (for itself and for industry and commerce)and a very important performer of research in the field. this dual role suggeststhat positive changes in academic cs&e will have high leverage throughoutindustry and academia.the committee addressed four questions in its deliberations:1. what is cs&e? what characterizes the intellectual content of thefield? how is it different from other fields? what are theimplications of rapid technological change for the field? what is thescience that underpins hardware and software computer technology?2. how is the field doing? what are the accomplishments of the field?what is the impact of the field on society? what is the demographicprofile of the field?3. what should the field be doing? to what extent and in whatdirections should the field change its educational and researchagenda?prefaceviicomputing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.how can the academic and industrial sectors work together moreeffectively?4. what does the field need in order to prosper? are current fundingemphases appropriate? what structural or institutional changes (ifany) are necessary to support academic cs&e as it evolves into thenext century?these questions are particularly appropriate given the circumstances oftoday. from its beginnings as an organized and independent academic disciplinein the 1960s, academic cs&e has been quite successful. it has witnessed rapidgrowth in demand for computer scientists and engineers, and it has worked handin hand with the computer industry, demonstrating the remarkably rich interactionpossible between academic and industrial cs&e research. indeed, togetheracademic and industrial cs&e research have in a few short decades laid theintellectual foundation and created the scientific base for one of the mostimportant technologies of the future.but today, both the intellectual focus of academic cs&e and theenvironment in which academic cs&e is embedded are in the midst ofsignificant change. the traditional intellectual boundaries of academic cs&e areblurring with the rise of indepth programs and activities in computationalscience. universities themselves are retrenching; the computer industry isundergoing substantial and rapid restructuring; and the increasingly apparentutility of computing in all aspects of society is creating demands for computingtechnology that is more powerful and easier to use. such changes motivate theforwardlooking assessment of the field that this report attempts to provide.given the increasing pervasiveness of computerrelated technologies in allaspects of society, the committee believes that several key groups will benefitfrom an assessment of the state of academic cs&e: federal policy makers, who have considerable influence in determiningintellectual directions of the field through their control of researchbudgets and funding levels; academic computer scientists and engineers, who are the individualsthat do the real work of selecting research topics, performing research,and teaching students; university administrators, who play key roles in influencing theintellectual tone of the academic environment; and industry, which is by far the major employer of cs&e baccalaureateholders, one of the major employers of cs&e ph.d. recipients, and (inthe computer industry) a key player in cs&e research.prefaceviiicomputing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.each of these groups has a different perspective on the intellectual, fiscal,institutional, and cultural influences on the field, and the committee devotedconsiderable effort to forging a consensus on what should be done in the face ofthe different intellectual traditions that characterize various subfields of cs&eand of different views on the nature of the problems that the field faces.this report does not address international dimensions of cs&e in any detailor depth, other than to note that the importance of cs&e as an area of research isrecognized all over the world. although the committee believes strongly thatinternational aspects of the field are worth considering, it had neither theexpertise nor the resources to focus on such aspects. appropriate sponsoringagencies and the computer science and telecommunications board may wish toconsider a study that addresses international dimensions of the field.the report is divided into two parts. part i addresses in broad strokes thefundamental challenges facing the field and what the committee believes is anappropriate response to these challenges. part ii elaborates on certain issues ingreater detail. in particular, the reader unfamiliar with cs&e as an intellectualdiscipline will find the necessary background in chapter 6. readers unfamiliarwith the institutional infrastructure of academic cs&e or the demographics of thefield will find additional detail in chapters 7 and 8, respectively.a variety of previous studies have addressed important aspects of the field.the taulbee surveys1 of the past several years have reported on human resourceissues in cs&e, cstb's report the national challenge in computer science andtechnology2 discussed research opportunities in the field, and the hopcroftkennedy report3 described1 david gries, "the 1984œ1985 taulbee survey," communications of the acm,volume 29(10), october 1986, pp. 972œ977; david gries, "the 1985œ1986 taulbeesurvey," communications of the acm, volume 30(8), august 1987, pp. 688œ694; davidgries and dorothy marsh, "the 1986œ1987 taulbee survey," communications of the acm,volume 31(8), august 1988, pp. 984œ991; david gries and dorothy marsh, "the 1987œ1988 taulbee survey,'' communications of the acm, volume 32(10), october 1989, pp.1217œ1224; david gries and dorothy marsh, "the 1988œ1989 taulbee survey,"communications of the acm, volume 33(9), september 1990, pp. 160œ169; david griesand dorothy marsh, "the 1989œ1990 taulbee survey," computing research news,volume 3(1), january 1991; and david gries and dorothy marsh, "the 1990œ1991taulbee survey," computing research news, volume 4(1), january 1992, pp. 8 ff.2 computer science and technology board, national research council, the nationalchallenge in computer science and technology, national academy press, washington,d.c., 1988.3 john e. hopcroft and kenneth w. kennedy, eds., computer science achievements andopportunities, society for industrial and applied mathematics, philadelphia, 1989.prefaceixcomputing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.the scientific contributions of cs&e. the 1989 acmcra conference, publishedas strategic directions in computer research,4 discussed structural and longrange issues for the field. these studies provided a strong foundation on whichthe committee built its comprehensive and integrated assessment.in addition, the cstb, in cooperation with the office of scientific andengineering personnel at the national research council, conducted a companionproject on human resources concurrently with this project. the key activity ofthis project, a workshop on human resources in computer science and technologyheld on october 28œ29, 1991, addressed the utility of current and proposed newtaxonomies for classifying computing professionals and considered present andfuture supplyanddemand issues for the labor market for computer specialists.participants in the workshop included experts in computer science andtechnology, labor market analysis, and the administration of human resources.while certain insights of this workshop have been incorporated into this report, afull report based on this workshop is expected to be released in the summer of1992.the committee to assess the scope and direction of computer science andtechnology met in june and september of 1990 and in february, june, andseptember of 1991. it received input through briefings and interviews with avariety of federal government officials and representatives from the computerindustry, from several major commercial users of computer and informationtechnology, and, through the computing research association, from heads ofdepartments granting ph.d.s in cs&e.the committee appreciates the time and thoughtful attention provided bynumerous individuals, who are listed in the appendix; in particular the commentsand criticisms of reviewers of early drafts of this report are gratefullyacknowledged. of course, the findings, conclusions, and judgments of this reportare solely the responsibility of the committee.a variety of government agencies that sponsor computer research andprofessional organizations in the computer field were interested in conducting abroadranging assessment of the health of the field. some of them generouslyprovided funding for this project; they include the national science foundation,the office of naval research, the air force office of scientific research, and theassociation for computing machinery, inc.4 association for computing machinery (acm) and the computing researchassociation (cra), strategic directions in computing research, acm press, new york,1990.prefacexcomputing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.contents executive summary 1part i 1 computingsignificance, status, challenges 13 computing in society 13 scope and purpose of this report 18 computer science and engineering 19 contributions of cs&e to computing practice 24 computing as a twoedged sword 26 the relationship between the federal government andcs&e research 28 the relationship between cs&e and the computerindustry 38 the changing environment for academic cs&e 45 summary and conclusions 49 notes 492 looking to the future of cs&e 55 broadening the field 55 a historical perspective 60 research opportunities in broadening 64 a broader research agendašsome illustrations 69 broadening educational horizons in cs&e 85 a special role for universityindustrycommerceinteraction 86contentsxicomputing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved. prerequisites for broadening 87 summary and conclusions 90 notes 903 a core cs&e research agenda for thefuture 95 processor capabilities and multipleprocessor systems 97 data communications and networking 101 software engineering 103 information storage and management 107 reliability 110 user interfaces 111 summary and conclusions 113 notes 1144 education in cs&e 116 undergraduate education in cs&e 118 the master's degree in cs&e 130 the ph.d. degree in cs&e 131 employment expectations for holders of cs&edegrees 133 continuing education 133 precollege cs&e education 135 summary and conclusions 136 notes 1365 recommendations 139 overall priorities 139 recommendations regarding research 143 recommendations regarding education 151 conclusions 157 notes 158part ii 6 what is computer science and engineering? 163 computer science and engineering 163 abstractions in computer systems 168 selected accomplishments 174 synergy leading to innovations and rapid progress 212 intellectual and structural characteristics of cs&e asa discipline 213 notes 214contentsxiicomputing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.7 institutional infrastructure of academic cs&e 217 federal agencies funding computer science andengineering 217 private nongovernmental organizations 231 notes 2378 human resources 239 baccalaureate and postbaccalaureate degree production 239 composition of academic cs&e 246 notes 259 appendix contributors to computing the future 261 index 265contentsxiiicomputing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.contentsxivcomputing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.executive summaryas an academic discipline, computer science and engineering (cs&e) hasbeen remarkably successful in its first decades of existence. but both theintellectual focus of academic cs&e and the environment in which the field isembedded are today in the midst of significant change. accordingly, a proactivelook forward will better prepare the field to evolve into the 21st century. thecomputer science and telecommunications board's committee to assess thescope and direction of computer science and technology was asked to takesuch a look, examining how best to organize the conduct of research and teachingin cs&e for the future.the backdropcomputers and computing are ubiquitous in modern society. in nearly everypart of modern life, the hardware and software of computer technology enable thedelivery of services and products of higher quality to more people in less timethan would otherwise be possible. indeed, computing and increasingly powerfulcomputers are the driving force behind the movement of society into theinformation age, affecting transportation, finance, health care, and most otheraspects of modern life; computing technology and related services account forabout 5 percent of the gross national product.executive summary1computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.what has led to the unprecedented expansion of computational power? thecontributions of those who have made successive generations of electroniccomponents smaller, faster, lighter, and cheaper are undeniable. but theorganization of these components into useful computer hardware (e.g.,processors, storage devices, displays) and the ability to write the softwarerequired to exploit this hardware are primarily the fruits of cs&e. furtheradvances in computer power and usability will also depend in large part onpushing back the frontiers of cs&e and will be motivated by a myriad ofapplications that can take advantage of these advances.cs&e research, in which the academic cs&e community has played amajor role, has made enormous contributions to computing practice, and insightsderived from such research inform the approach of programmers and machinedesigners at all levels, from those designing a stillfaster supercomputer to thoseprogramming a small personal computer. techniques and architectural themesdeveloped or codified by cs&e are familiar to every developer of software andhardware, concepts like programming languages, compilers, relational databases,reducedinstructionset computing, and so on. moreover, as the complexity ofcomputing has grown, so also has the need for wellunderstood concepts andtheories with which to manage this complexity. indeed, entirely new cs&eresearch problems and opportunities today are created by rapid technologicaladvances in computing. whereas intuitively grounded insight was oftensufficient to lead to substantial progress in the earliest days of the field, asystematic approach has become increasingly important. thus the importance ofcs&e research to computing practice can only be expected to increase in thefuture.federal support for cs&e research has been critical. from its initial supportof cs&e research for strictly military purposes, the federal government nowinvests considerable amounts ($680 million in fy 1991) in basic and appliedcs&e research for both military and civilian purposes; about 46 percent of this$680 million went to academic research. such support is a strong indication thatthe federal government recognizes the importance of cs&e research to themissions of many government agencies as well as to the welfare of the nation.however, growth in funding, substantial though it has been in recent years, hasnot kept pace with the growing need for a science base to create, control, andexploit the potential of ever more powerful computer systems. nor has fundingkept pace with the growth in the number of academic cs&e researchers; in theacademic comexecutive summary2computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.munity, the ratio of funding per researcher has dropped by over 20 percent since1985. such trends have led to substantial concern within this community thatresources are inadequate to support a research agenda vigorous enough to exploitadvances and address problems as they arise.decreasing per capita amounts of federal research funding are only oneaspect of a new environment for academic cs&e. assumptions of the 1940s and1950s regarding the positive social utility of basic research (i.e., research withoutforeseeable application) are being questioned increasingly by the federalgovernment, and justifications for research may well in the future requireconcrete demonstrations of positive benefit to the nation. an illustration of thispossible trend is that the highperformance computing and communicationsprogram, a program initiated in fy 1992, calls for cs&e research specifically inthe context of solving "fundamental problem[s] in science and engineering, withpotentially broad economic, political, and/or scientific impact, that could beadvanced by applying highperformance computing resources."1in addition, another major influence on academic cs&e, the computerindustry, is undergoing massive change as it shifts from sales based on largemainframe computers affordable by only a few institutions to 11 computers forthe masses," i.e., smaller computer systems that are increasingly portable andinterconnectable to each other or to information service providers, and mostprobably embodying new computing styles such as penbased computing. such atrend will increase the importance, already considerable, of being able tointroduce new products on a much shorter time scale. at the same time,customers are demanding greater degrees of functionality from their computersystems. new computing technology will have to be fitted to customer needsmuch more precisely, thus placing a premium on knowledge of the customer'sapplication. new applications of computing will also lead to new cs&e researchproblems.finally, computing has resulted in costs to society as well as benefits.amidst growing concern in some sectors of society with respect to issues such asunemployment, invasions of privacy, and reliance on fallible computer systems,the computer is no longer seen as an unalloyed positive force in society.these changes in the environment for academic cs&e mark a criticaljuncture for the discipline. it is rapidly becoming clear that, although academiccs&e has enjoyed remarkable success in the last several decades, the ways of thepast will not necessarily lead to success in the future.executive summary3computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.judgments and prioritiesin considering appropriate responses of cs&e for the future, the committeeexamined the current state of the field and made several important judgments thatguided its work.the first and foremost judgment was that cs&e is coming of age. althoughas an organized and independent intellectual discipline the field is less than 30years old, it has established a unique paradigm of scientific inquiry that isapplicable to a wide variety of problems. indeed, the committee believes that thishistory and resulting strength should enable academic cs&e to recognize thatintellectually substantive and challenging cs&e problems can and do arise in thecontext of problem domains outside cs&e per se. cs&e research can be framedwithin the discipline's own intellectual traditions but also in a manner that isdirectly applicable to other problem domains, as illustrated in table es.1. cs&ecan thus be an engine of progresstable es.1 importance of selected core subfields of cs&e to variousapplicationsapplicationcore subfieldglobalchangeresearchcomputationalbiologycommercialcomputingelectroniclibrarymultipleprocessorsveryimportantcentralimportantveryimportantdatacommunicationsand networkingcentralimportantcentralcentralsoftwareengineeringimportantvery importantcentralimportantinformationstorage andmanagementcentralvery importantveryimportantcentralreliabilityveryimportantimportantveryimportantimportantuser interfacesveryimportantvery importantcentralcentralnote: the core subfields listed above are parts of a future research agenda for cs&e (seechapter 3). as significantly, they are important to, and can derive inspiration and challengingproblems from, these selected application domains. these core subfields correspond to areas in whichmajor qualitative and quantitative changes of scale are expected. these areas are processorcapabilities and multipleprocessor systems, available bandwidth and connectivity for datacommunications and networking, program size and complexity, management of large volumes of dataof diverse types and from diverse sources, and the number of people using computers and networks.understanding and managing these changes of scale will pose many fundamental problems in cs&e,and using these changes of scale properly will result in more powerful computer systems that willhave profound effects on all areas of human endeavor.executive summary4computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.and conceptual change in other problem domains, even as these domainscontribute to the identification of new areas of inquiry within cs&e.second, the strong connections between cs&e research and computingpractice led the committee to conclude that at least within cs&e, the traditionalseparation of basic research, applied research, and development is dubious. giventhe way research in cs&e is practiced, distinctions between basic and appliedresearch are especially artificial, since both call for the exercise of the samescientific and engineering judgment, creativity, skill, and talent.finally, the committee concluded that the growing ubiquity of computingwithin society places a premium on the largest possible diffusion of cs&eexpertise to all endeavors in society whose computing applications stress theexisting state of the art. however, the primary vehicle for such diffusionšundergraduate cs&e programsšis highly variable in content and quality, largelydue to rapid advancements in the field. it is imperative that undergraduate cs&eeducation reflect the best knowledge and insight that the field has to offer ifcomputing is to reach its full potential within society.these judgments led to the committee's formulation of a set ofcorresponding overall priorities. the first priority is to sustain the core effort in cs&e, i.e., the effortthat creates the theoretical and experimental science base on whichcomputing applications build. this core effort has been deep, rich, andintellectually productive and has been indispensable for its impact onpractice in the last couple of decades. the second priority is to broaden the field. given the manyintellectual opportunities available at the intersection of cs&e and otherproblem domains and a solid and vigorous core effort in cs&e, thecommittee believes that academic cs&e is well positioned to broadenits selfconcept. such broadening will also result in new insights withwide applicability, thereby enriching the core. furthermore, given thepressing economic and social needs of the nation and the changingenvironment for industry and academia, the committee believes thatacademic cs&e must broaden its selfconcept or risk becomingincreasingly irrelevant to computing practice. the third priority is to improve undergraduate education in cs&e.the quality of undergraduate cs&e education is inextricably tied to thestate of computing practice in all sectors of society. moreover, betterundergraduate education is necessary for better research, since it isnecessary for transmitting recently developed core knowledge to thenext generation and for providing the intellectual basis in cs&e forindividuals pursuing a broader research agenda.executive summary5computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.recommendations (a summary)in the interests of brevity, this summary of recommendations omits manysubstantive details. readers are urged to read the full text of therecommendations in chapter 5.to federal policy makers regarding researchrecommendation 1. the highperformance computing andcommunications (hpcc) program should be fully supported throughoutthe planned fiveyear program.the hpcc program is of utmost importance for three reasons. the first isthat highperformance computing and communications are essential to thenation's future economic strength and competitiveness, especially in light of thegrowing need and demand for ever more advanced computing tools in all sectorsof society. the second reason is that the program is framed in the context ofscientific and engineering grand challenges. thus the program is a strong signalto the cs&e community that good cs&e research can flourish in an applicationscontext and that the demand for interdisciplinary and applicationsoriented cs&eresearch is on the rise. and finally, a fully funded hpcc program will have amajor impact on relieving the funding stress affecting the academic cs&ecommunity. consistent with priority 1, the committee believes that the basicresearch and human resources component of the hpcc program is critical,because it is the component most likely to support the research that will allow usto exploit anticipated technologies as well as those yet to be discovered throughsuch research.the committee is concerned about the future of the hpcc program after fy1996 (the outer limit on current plans). if the effort is not sustained after fy 1996at a level much closer to its planned fy 1996 level than to its fy 1991 level of$489 million, efforts to exploit fully the advances made in the preceding fiveyears will almost certainly be crippled. in view of the long lead times needed forthe administration's planning of major initiatives, the committee recommendsthat funding necessary for exploitation of recently performed research andthe investigation of new research topics be fully assessed sometime during fy1994 with an eye toward a followon hpcc program.recommendation 2. the federal government should initiate an effort tosupport interdisciplinary and applicationsoriented cs&e research inacademia that is related to the missions of the missionexecutive summary6computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.oriented federal agencies and departments that are not now major participants in the hpcc program. collectively, this effort would cost anadditional $100 million per fiscal year in steady state above amountscurrently planned.many federal agencies are not currently participating in the hpcc program,despite the utility of computing to their missions, and they should be brought intothe program. those agencies that support substantial research efforts, though notin cs&e, should support interdisciplinary cs&e research, i.e., cs&e researchundertaken jointly with research in other fields. problems in these other fieldsoften include an important computational component whose effectiveness couldbe enhanced substantially by the active involvement of researchers working at thecutting edge of cs&e.those agencies that do not now support substantial research efforts of anykind, i.e., operationally oriented agencies, should consider supportingapplicationsoriented cs&e research because of the potential that the efficiencyof their operations would be substantially improved by some research advancethat could deliver a better technology for their purposes. such research could alsohave considerable ''spinoff" benefit to the private sector as well.to universities regarding researchrecommendation 3. academic cs&e should broaden its researchhorizons, embracing as legitimate and cogent not just research in core areas(where it has been and continues to be strong) but also research in problemdomains that derive from nonroutine computer applications in other fields and areas or from technologytransfer activities. the academic cs&ecommunity should regard as scholarship any activity that results in significantnew knowledge and demonstrable intellectual achievement, without regard forwhether that activity is related to a particular application or whether it falls intothe traditional categories of basic research, applied research, or development.chapter 5 describes appropriate actions to implement this recommendation.recommendation 4. universities should support cs&e as a laboratorydiscipline (i.e., one with both theoretical and experimental components).cs&e departments need adequate research and teaching laboratory space; staffsupport (e.g., technicians, programmers, staff scientists); funding for hardwareand software acquisition, maintenance, and upgrade (especially important onsystems that retainexecutive summary7computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.their cutting edge for just a few years); and network connections. new facultyshould be capitalized at levels comparable to those in other science or engineeringdisciplines.to federal policy makers regarding educationrecommendation 5. the basic research and human resourcescomponent of the highperformance computing and communicationsprogram should be expanded to address educational needs of certainfaculty. the program described in chapter 5 to address these needs is estimatedto cost $40 million over a fouryear period.of particular concern are two groups: cs&e faculty who are not themselvesinvolved in cs&e research and researchers from other scientific and engineeringdisciplines that depend on computation. many of these individuals received theireducation in computing many years ago and are unfamiliar with new paradigmsin cs&e developed over the last decade or so. they would benefit from exposureto these paradigms, and such exposure could well have a major impact on thequality of undergraduate cs&e education in the united states, as well as on thenation's ability to use computing in support of other science and engineering.the committee believes that senior academic cs&e researchers have anobligation to participate actively in providing such continuing education efforts.mechanisms to encourage their attention to these matters need to be developed;one example is that research funding could be used to some extent to encourageparticipation in these efforts.to universities regarding educationrecommendation 6. so that their educational programs will reflect abroader concept of the field, cs&e departments should take the followingactions: (a) require ph.d. students either to take a graduate minor in a noncs&e field or to enter the ph.d. program with an undergraduate degree in anoncs&e field, (b) encourage ph.d. students in cs&e to perform dissertationresearch in nontraditional areas, (c) offer undergraduate students not majoring incs&e a wide range of cs&e courses and programs, and (d) provide mechanismsto recognize and reward faculty for developing innovative and challenging newcurricula that keep up with technological change and make substantive contactwith applications in other domains.executive summary8computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.recommendation 7. the academic cs&e community must reach out towomen and to minorities that are underrepresented in the field (particularlyas incoming undergraduates) to broaden and enrich the talent pool. suchoutreach is necessary if cs&e is to fulfill the potential for inclusion of suchgroups that might be expected given the youth of the field.conclusionssince the invention of the electronic storedprogram digital computer lessthan 50 years ago, cs&e has blossomed into a new intellectual discipline withbroad principles and substantial technical depth. by embracing the computingchallenges that arise in many specific problem domains, computer scientists andengineers can build on this legacy, guiding and shaping the course of theinformation revolution. this expansive view of cs&e will require acommensurately broader educational agenda for academic cs&e, as well asundergraduate education of higher quality. adequate funding from the federalgovernment and greater interactions between academia and industry andcommerce will help immeasurably to promote the broadening and strengtheningof the discipline. if the major thrusts of this reportšsustaining the cs&e core atcurrently planned levels, broadening the cs&e discipline, and upgradingundergraduate cs&e education to reflect the best of current knowledgešarewidely accepted in the cs&e community, the communityšas well asgovernment, industry, and commercešwill be well positioned to meet thecoming intellectual challenges as well as to make substantial and identifiablecontributions to the national wellbeing and interest.note1. office of science and technology policy, the federal highperformance computing program,executive office of the president, washington, d.c., september 8, 1989, p. 8.executive summary9computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.executive summary10computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.part iexecutive summary11computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.executive summary12computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.1computingšsignificance, status,challengescomputing in societycomputing is inextricably and ubiquitously woven into the fabric of modernlife. in nearly all sectors of the economy, computing makes it possible to deliverservices and products of higher quality to more people in less time than wouldotherwise be possible (box 1.1).as seen from the perspective of other technical fields (box 1.2) and in termsof its potential to enhance u.s. industrial strength and the national defense(box 1.3), computing is a truly enabling and central technology. consider: in large businesses, electronic mail enabled by computing is increasinglycommon. in communications, computing makes it possible to switch and routeover 100 million longdistance telephone calls per day. in aeronautics, computeraided design techniques are expected to saveboeing as much as a billion dollars in the development of the 777airliner.1 in pharmaceuticals, computing enables chemists to conduct systematicsearches for compounds that will fight specific diseases. in automobile engineering, computing makes it possible to simulateautomobile crash tests that would otherwise cost hundreds of thousandsof dollars apiece.2 in the oil industry, computing has saved hundreds of millionscomputingšsignificance, status, challenges13computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.box 1.1 applications of computing in national lifešselected examplesbanking and finance. automatic teller machines are used by millionsof bank customers, who can now obtain cash on their own schedules ratherthan those of the banks that hold their money. a highly automated checkclearing system used by most banks nationwide processes 55 billion paperitems (e.g., checks) per year, a task that would be virtually impossiblewithout computers. in many cases, electronic funds transfer (eft)eliminates the need for paper checks entirely. computerized pointofsalesystems link registers electronically to debit card networks, enablingmerchants to obtain their money instantly without the need for cash.transportation. computing improves the fuel efficiency of manyautomobiles, as a microprocessor automatically optimizes the fuelairmixture for a variety of different driving conditions. antilock brakes dependon modern computer technology; modern automobiles are made lighter andsafer through knowledge gained by computer simulations. airlines all overthe world depend on computers to manage reservations. computing isincreasingly important to the design and manufacture of today's passengerplanes. pilots rehearse improbable emergencies in computer simulators,making air travel incomparably safer.health care. computermediated medical imaging and automatedlaboratory analysis have dramatically improved diagnostic power, earlydetection, and the planning and assessment of treatment for a burgeoninglist of ailments. modern radiation therapy would not be possible withoutcomputer control. in some areas, computer technology helps to spreadspecialist care, or at least informed specialist referral, to populationswithout diversified medical expertise. of dollars in the past five years by helping drillers to avoid "dry" wells.3 in offices, computerbased spreadsheets enable thousands of analysts andmanagers to model and predict financial and economic trends. in science, computing is becoming a third paradigm of scientific inquiry,on a par with theory and observation or experiment and often yieldingunexpected or unanticipated insights not possible through purelytheoretical or experimental means.computingšsignificance, status, challenges14computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.box 1.2 views on computing in other fields". . . versatile computers [provide] opportunities for significantimprovements in materials processing technologies. online computationalcontrol of process parameters can lead to major improvements in productquality and performance as well as to increased efficiency and reducedcosts." national research council, materials science and engineering forthe 1990s: maintaining competitiveness in the age of materials, nationalacademy press, washington, d.c., 1989, p. 234."new chemical products that today are discovered predominantlythrough laboratory work may be discovered in the future by computercalculations based on models that predict the detailed behavior ofmolecules. . . . advanced engineering development will be based more thanever on mathematical modeling and scientific computation." nationalresearch council, frontiers in chemical engineering: research needs andopportunities, national academy press, washington, d.c., 1988, p. 136.''as biology moves toward an ever more detailed analysis of thechemistry of life, computers will play an everincreasing role in datamanagement, data analysis, pattern recognition, and imaging. the trainingof computerliterate biologists will be essential. conversely, the training ofcomputer scientists with greater understanding of chemistry and biologypresents an immediate and compelling need." national research council,opportunities in biology, national academy press, washington, d.c.,1989, p. 37."nearly all of today's experiments in physics depend on computers, andmany experiments would be impossible without them . . . . computers arealso widely employed by theorists to carry out calculations far exceedinghuman capability, thus achieving new orders of precision . . . . largecomputers are being increasingly used as numerical laboratories in whichcomplex physical systems can be simulated and studied in ways notpossible by experiment. timedependent processes . . . can be visualized,providing a powerful guide to theory. the transition from order to chaosšone of the most profound problems in contemporary physicsšcan beobserved and studied in systems ranging from a few particles to theturbulence around an aircraft. in such applications, computers are providinga new approach to understanding nature called simulation physics. neitherprecisely theoretical nor experimental, this style of physics possessesenormous potential, and it is growing rapidly." national research council,physics through the 1990s: an overview, national academy press,washington, d.c., 1986, p. 72.computingšsignificance, status, challenges15computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.box 1.3 other reports on the importance ofcomputing to society"the strong [computing and] information emphasis in the defensecritical technologies list [i.e., 21 critical technologies that are essential formaintaining the qualitative superiority of u.s. weapon systems] correspondsto the growing importance of information in both deterrence and moderncombat. the ability of the u.s. to acquire and effectively use information . . .can help compensate for planned reductions in u.s. force structure andforward deployed assets." department of defense, critical technologiesplan, ada234 900, 1 may 1991, p. ii4."twentytwo technologies deemed critical to the satisfaction of nationalneeds have been identified . . . . information and communicationstechnologies (including software, microelectronics and optoelectronics,highperformance computing and networking, highdefinition imaging anddisplays, data storage and peripherals, and computer simulation andmodeling) . . . will play a substantial role in determining the rate of progressin other critical technology areas . . . . sensors, software, simulation andmodeling, and computing are increasingly becoming the criticalunderpinnings of advanced manufacturing processes . . . . informationtechnologies also enable or limit advances in the performance of nextgeneration weapon systems, military training, and the planning andexecution of military operations . . . . maintaining stateoftheart capabilitiesin information technologies will, without question, determine the economicperformance of increasing numbers of segments of the u.s. manufacturingand service sectors." department of commerce, report of the nationalcritical technologies panel, march 1991, pp. 1, 52, 53."information technologies [including software, computers, humaninterface and visualization technologies, database systems, networks andcommunications] are increasingly critical to the competitiveness of all nineindustries that the council examined [aerospace, chemicals, computers andsoftware, construction, drugs and pharmaceuticals, electronic componentsand equipment, machine tools, motor vehicles, and telecommunications],with applications in the design of products, management of productionprocesses and improvement in the performance of products." council oncompetitiveness, gaining new ground: technology priorities for america'sfuture, washington, d.c., 1991, pp. 27œ28.computingšsignificance, status, challenges16computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.why should computing be so importantševen essentialšin these and somany other areas of human endeavor? fundamentally, the answer is thatcomputing can be usefully applied to any endeavor that uses or can be made touse information in large quantities (informationplenty) or information that hasbeen highly processed and manipulated (informationrich).informationrich and informationplenty endeavors primarily involveproducts of the human mindšnumbers, pictures, ideas. as a device that excels inthe storage and manipulation of information, the computer serves primarily as anamplifier of human intellectual capabilities. by operating very rapidly, it enablesinformationplenty activities. by undertaking efforts that are beyond theintellectual reach of human abilities, it enables informationrich activities.it is this enabling amplifier effect that is at the heart of today's informationrevolution, a revolution that may be as significant to human destiny as theagricultural and industrial revolutions. to paraphrase john seely brown,corporate vice president of the xerox corporation, mass and energy are beingreplaced by information and computing. the examples above include vignettes onhow computing makes automobiles more energyefficient and manufacturing lessmaterially wasteful. but what is obvious only at a macrolevel is the change in thenational economy itself. once buttressed primarily by the sales of materialartifacts such as inventory parts, airplanes, and automobiles that derive their valuefrom structuring the atoms that give them substance, the economy is nowincreasingly one of information artifacts that may, for example, derive value fromstructuring musical notes into a symphony, words into a book, binary digits into acomputer program, or figures from a business projection modeled on aspreadsheet.nowhere is the shift from tangible artifact to information artifact betterillustrated than in the computer industry itself. in its first few decades ofexistence, the computer industry made its money in the manufacture ofcomputers. today, the software sector is the most rapidly growing and profitablesector of the industry, as illustrated by its 19 percent growth rate in 1990 over1989 levels versus a 9 percent growth rate in the industry overall.4 yet softwareitself consumes no material, weighs nothing, and requires essentially no power.5software is information crystallized in a particular form, and in this form it isvalued at over $20 billion per year by the united statesšand this estimateexcludes the substantial amounts of custom software developed "in house" bycomputer users. other examples of the increasing importance of informationinclude the entertainment industry (over $12 billion in sales in 1990 by fivemajor entertainmentcomputingšsignificance, status, challenges17computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.companies6 and videogame manufacturers7) and telecommunications ($107billion in sales of services by the telephone companies listed in the businessweek 10008), both industries that trade mostly in ideas, information, andimagination. information technology (including computer and communicationshardware, plus computer software and services) directly accounts for around 5percent of the gnp,9 even disregarding its enabling role in other sectors of theeconomy.scope and purpose of this reportas a key force driving the development of ever more sophisticatedcomputing and as the supplier of a large proportion of the trained computingpersonnel in industry, academic computer science and engineering (cs&e) hashad a substantial impact on the nation.10 but today, both the intellectual focus ofacademic cs&e and the environment in which academic cs&e is embedded arein the midst of significant change. the intellectual boundaries of academic cs&eare blurring with the rise of indepth programs and activities in computationalsciencešthe application of computational techniques to advance such disciplinesas physics, chemistry, biology, and materials science. universities themselves areretrenching; the computer industry is undergoing substantial and rapidrestructuring; and the increasingly apparent utility of computing in all aspects ofsociety is creating demands for computing technology that is more powerful andeasier to use.in light of these changes, the committee to assess the scope and directionof computer science and technology was convened to determine how best toorganize the conduct of research and teaching in cs&e for the future. the resultof its twoyear study is an action plan that calls both for sustaining traditionalcore activities within cs&e and broadening the scope of cs&e's intellectualagenda as the field evolves into the 21st century.this report is divided into two parts. part i addresses in broad strokes thefundamental challenges facing the field and discusses what the committeebelieves is an appropriate response to these challenges. chapter 1 brieflydiscusses the intellectual nature of cs&e and then elaborates on the nature of theimpending challenges. chapter 2 provides the philosophical underpinning for anappropriate response by the academic cs&e community. chapter 3 outlines acore research agenda to carry cs&e into the future. chapter 4 discusses the stateof cs&e education at all levels. chapter 5 articulates a set of judgments andpriorities for the field and presents recommendations informed by thosejudgments and priorities. part ii explains in greatercomputingšsignificance, status, challenges18computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.detail three aspects of the field: cs&e as an intellectual discipline, in chapter 6;the institutional infrastructure of academic cs&e, in chapter 7; and thedemographics of the field, in chapter 8.computer science and engineeringcomputational poweršhowever measuredšhas increased dramatically inthe last several decades. what is the source of this increase?the contributions of solidstate physicists and materials scientists to theincrease of computer power are undeniable; their efforts have made successivegenerations of electronic components ever smaller, faster, lighter, and cheaper.but the ability to organize these components into useful computer hardware (e.g.,processors, storage devices, displays) and to write the software required (e.g.,spreadsheets, electronic mail packages, databases) to exploit this hardware areprimarily the fruits of cs&e. further advances in computer power and usabilitywill also depend in large part on pushing back the frontiers of cs&e.intellectually, the "science" in "computer science and engineering" connotesunderstanding of computing activities, through mathematical and engineeringmodels and based on theory and abstraction. the term "engineering" in"computer science and engineering" refers to the practical application, based onabstraction and design, of the scientific principles and methodologies to thedevelopment and maintenance of computer systemsšbe they composed ofhardware, software, or both.11 thus both science and engineering characterize theapproach of cs&e professionals to their object of study.what is the object of study? for the physicist, the object of study may be anatom or a star. for the biologist, it may be a cell or a plant. but computerscientists and engineers focus on information, on the ways of representing andprocessing information, and on the machines and systems that perform thesetasks.the key intellectual themes in cs&e are algorithmic thinking, therepresentation of information, and computer programs. an algorithm is anunambiguous sequence of steps for processing information, and computerscientists and engineers tend to believe in an algorithmic approach to solvingproblems. in the words of donald knuth, one of the leaders of cs&e:cs&e is a field that attracts a different kind of thinker. i believe that one whois a natural computer scientist thinks algorithmically. such people are especiallygood at dealing with situations where different rules apply in different cases;they are individuals who cancomputingšsignificance, status, challenges19computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.rapidly change levels of abstraction, simultaneously seeing things "in the large"and "in the small."12the second key theme is the selection of appropriate representations ofinformation; indeed, designing data structures is often the first step in designingan algorithm. much as with physics, where picking the right frame of referenceand right coordinate system is critical to a simple solution, picking one datastructure or another can make a problem easy or hard, its solution slow or fast.the issues are twofold: (1) how should the abstraction be represented, and(2) how should the representation be properly structured to allow efficient accessfor common operations? a classic example is the problem of representing parts,suppliers, and customers. each of these entities is represented by its attributes(e.g., a customer has a name, an address, a billing number, and so on). eachsupplier has a price list, and each customer has a set of outstanding orders to eachsupplier. thus there are five record types: parts, suppliers, customers, price, andorders. the problem is to organize the data so that it is easy to answer questionslike: which supplier has the lowest price on part p?, or, who is the largestcustomer of supplier s? by clustering related data together, and by constructingauxiliary indices on the data, it becomes possible to answer such questionsquickly without having to search the entire database.the two examples below also illustrate the importance of properrepresentation of information: a "white pages" telephone directory is arranged by name: knowing thename, it is possible to look up a telephone number. but a "crisscross"directory that is arranged by number is necessary when one needs toidentify the caller associated with a given number. each directorycontains the same information, but the different structuring of theinformation makes each directory useful in its own way. a circle can be represented by an equation or by a set of points. a circleto be drawn on a display screen may be more conveniently representedas a set of points, whereas an equation may be a better representation if aproblem calls for determining if a given point lies inside or outside thecircle.a computer program expresses algorithms and structures information using aprogramming language. such languages provide a way to represent an algorithmprecisely enough that a "highlevel" description (i.e., one that is easily understoodby humans) can be mechanically translated ("compiled") into a "lowlevel"version that the computer can carry out ("execute"); the execution of a programcomputingšsignificance, status, challenges20computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.by a computer is what allows the algorithm to come alive, instructing thecomputer to perform the tasks the person has requested. computer programs arethus the essential link between intellectual constructs such as algorithms andinformation representations and the computers that enable the informationrevolution.computer programs enable the computer scientist and engineer to feel theexcitement of seeing something spring to life from the "mind's eye" and ofcreating information artifacts that have considerable practical utility for people inall walks of life. fred brooks has captured the excitement of programming:the programmer, like the poet, works only slightly removed from purethoughtstuff. he builds castles in the air, creating by the exertion of theimagination . . . . yet the program construct, unlike the poet's words, is real inthe sense that it moves and works, producing visible outputs separate from theconstruct itself . . . . the magic of myth and legend has come true in our time.one types the correct incantation on a keyboard, and a display screen comes tolife, showing things that never were nor could be.13programmers are in equal portions playwright and puppeteer, working as anovelist would if he could make his characters come to life simply by touchingthe keys of his typewriter. as ivan sutherland, the father of computer graphics,has said,through computer displays i have landed an airplane on the deck of a movingcarrier, observed a nuclear particle hit a potential well, flown in a rocket atnearly the speed of light, and watched a computer reveal its innermostworkings.14programming is an enormously challenging intellectual activity. apart fromdeciding on appropriate algorithms and representations of information, perhapsthe most fundamental issue in developing computer programs arises from the factthat the computer (unlike other similar devices such as nonprogrammablecalculators) has the ability to take different courses of action based on theoutcome of various decisions. here are three examples of decisions thatprogrammers convey to a computer: find a particular name in a list and dial the telephone number associatedwith it. if this point lies within this circle then color it black; otherwise color itwhite. while the input data are greater than zero, display them on the screen.when a program does not involve such decisions, the exact secomputingšsignificance, status, challenges21computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.quence of steps (i.e., the "execution path") is known in advance. but in a programthat involves many such decisions, the sequence of steps cannot be known inadvance. thus the programmer must anticipate all possible execution paths. theproblem is that the number of possible paths grows very rapidly with the numberof decisions: a program with only 10 "yes" or "no" decisions can have over 1000possible paths, and one with 20 such decisions can have over 1 million.algorithmic thinking, information representation, and computer programsare themes central to all subfields of cs&e research. box 1.4 illustrates a typicaltaxonomy of these subfields. consider the subarea of computer architecture.computer engineers must have a basic understanding of the algorithms that willbe executed on the computers they design, as illustrated by today's designers ofparallel and concurrent computers. indeed, computer engineers are faced withmany decisions that involve the selection of appropriate algorithms, since anyprogrammable algorithm can be implemented inbox 1.4 a taxonomy of subfields in cs&e algorithms and data structures programming languages computer architecture numeric and symbolic computation operating systems software engineering databases and information retrieval artificial intelligence and robotics humancomputer interactioneach of these areas involves elements of theory, abstraction, anddesign. theory is based on mathematics and follows the mathematician'smethodology (defining objects, proving theorems); abstraction is based onthe investigative approach of the scientist (hypothesizing, makingpredictions, collecting data); design is based on the methodology of theengineer (defining requirements and specifications, implementing a system,testing a system).source: peter denning, douglas e. comer, david gries, michael c.mulder, allen tucker, joe turner, and paul r. young, "computing as adiscipline," communications of the acm, volume 32(1), january 1989, pp.9œ23.computingšsignificance, status, challenges22computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.hardware. through a better understanding of algorithms, computer engineers canbetter optimize the match between their hardware and the programs that will runon them.those who design computer languages (item two in box 1.4) with whichpeople write programs also concern themselves with algorithms and informationrepresentation. computer languages often differ in the ease with which varioustypes of algorithms can be expressed and in their ability to represent differenttypes of information. for example, a computer language such as fortran isparticularly convenient for implementing iterative algorithms for numericalcalculation, whereas cobol may be much more convenient for problems that callfor the manipulation and the input and output of large amounts of textual data.the language lisp is useful for manipulating symbolic relations, while ada isspecifically designed for "embedded" computing problems (e.g., realtime flightcontrol).the themes of algorithms, programs, and information representation alsoprovide material for intellectual study in and of themselves, often with importantpractical results. the study of algorithms within cs&e is as challenging as anyarea of mathematics; it has practical importance as well, since improperly chosenalgorithms may solve problems in a highly inefficient manner, and problems canhave intrinsic limits on how many steps are needed to solve them (box 1.5). thestudy of programs is a broad area, ranging from the highly formal study ofmathematically proving programs correct to very pracbox 1.5 about the study of algorithmshow many steps are necessary to solve a given problem? thisquestion led to the development of the area known as computationalcomplexity. consider alphabetizing a list of 1000 names. a straightforwardalgorithm ("insertion sort") takes on the order of a million (i.e., 1000 × 1000)onetoone comparisons of names in the worst case, but a clever algorithm("heap sort") would take just 10,000 comparisons in the worst case (1000 ×log2 1000 or about 10:00 × 10). further, this is the best possible result, forit has been shown that sorting a list of n items requires n log2 n pairwisecomparisons in the worst case, no matter what algorithm is used.theoreticians have found arguments that apply to whole classes ofalgorithms and problems, opening questions about computing that have notyet been solved.computingšsignificance, status, challenges23computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.tical considerations regarding tools with which to specify, write, debug, maintain,and modify very large software systems (otherwise called software engineering).information representation is the central theme underlying the study of datastructures (how information can best be represented for computer processing) andmuch of humancomputer interaction (how information can best be represented tomaximize its utility for human beings).contributions of cs&e to computing practicecs&e research has made enormous contributions to computing practice.insights from cs&e research inform the approach of programmers and machinedesigners at all levels, from those designing a stillfaster supercomputer to thoseprogramming a small personal computer. techniques and architectural themesdeveloped or codified under the banner of cs&e are familiar to every developerof software and hardware.consider modern wordprocessing systems, familiar to millions of officeworkers with no technical training. many features that make these systems sopopular (e.g., fullscreen "what you see is what you get" (wysiwyg) editing,automatic linewrapping at the end of a line, automatic pagination, mousepointing) first appeared in text editors developed by computer scientists andengineers. as importantly, the internals of modern wordprocessing systemsdepend on a host of algorithms and data structures investigated in the course ofcs&e research: automata theory, dynamic programming, constraint satisfaction,incremental updating, partialmatch retrieval, data compression. spreadsheets,though not first conceptualized by computer scientists, also depend on many ofthese algorithms, data structures, and concepts for efficient implementation onpersonal computers. these ideasšthe result of cs&e research and disseminatedby cs&e educationšare second nature in programming, just as kirchhoff'slaws, amplifiers, and flipflops are elemental ideas in electrical engineering. fromonly the most rudimentary idea of a word processor or spreadsheet, goodprogrammers can quickly determine how to make one and can explain the planconcisely.modern database management systems, for mainframes and personalcomputers alike, rely on computer science and engineering research from top tobottom. for example, computer science researchers in the late 1960s and early1970s created the relational data model to represent data in a simple way.computer engineers worked through the 1970s on techniques to implement thismodel. by the mid1980s these ideas were understood well enough to bestandardized by thecomputingšsignificance, status, challenges24computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.international organization for standardization (iso) in the language sql.sql has become the lingua franca of the database business. the committeeestimates that today about 100,000 computer programmers in the united statesuse a database system as their main tool; hiring these programmers costs about$10 billion per year. improving their productivity by even a small amount has ahuge payoff, and most studies indicate that the relational database model and itsassociated tools more than double programmer productivity.cs&e has been profoundly helpful to much of modern science andengineering. for example, the speed with which certain types of partialdifferential equations may be solved has improved by a factor of around 1011since 1945 (figure 1.1), due in about equal measure to faster machines developedby computer engineers and better algorithms developed by mathematicians andtheoretical computer scientists. just as importantly, computer scientists havedeveloped programming languages that enable scientists to use computers moreeffectively and computerbased techniques for interactive scientificfigure 1.1 speedup in the solution of poisson's equation, on a grid 64 pointson a side. note that the increased speed results from both better computerhardware and better algorithms to solve the equation. source: jon bentley,more programming pearls, addisonwesley, new york, 1988, p. 158.copyright © 1988 by bell telephone laboratories, inc. reprinted withpermission of addisonwesley publishing company.computingšsignificance, status, challenges25computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.visualization, in which huge amounts of datašperhaps generated by solvingthese partial differential equationsšare transformed into easily understandableimages. indeed, visualization is now a new paradigm for the presentation ofinformation.many operating systems (i.e., the system software that provides basicmachine functions on which applications software can build) such as msdosand unix make use of many years of experimental cs&e. operating systemsprovide abstractions and comprise components developed through the study ofcs&e: processes, files, address spaces, concurrency, synchronization. variousparts of operating systems are engineered according to analyses of strategies formemory allocation, scheduling, paging, queuing, and communication. in eachcase, the best modern practices to which implementors instinctively turn havebeen explored and codified by cs&e.theoretical computer scientists studying computational complexity withmathematical tools have had a major impact on computer security. moderncryptographic methods (e.g., publickey encryption systems) devised to protectand guarantee the integrity of electronically transmitted messages are based onwork in complexity theory performed since the 1970s (box 1.6).as the complexity of computing has grown since the invention of the digitalcomputer in the 1940s, so also has the need for wellunderstood concepts andtheories with which to manage this complexity. whereas intuitively groundedinsight was often sufficient to lead to substantial progress in the earliest days ofthe field, a systematic approach has become increasingly important. thus theimportance of cs&e to computing can only be expected to grow.computings as a twoedged swordas with many technologies, the initial applications of computing to variousproblems were widely regarded as positive. this history has raised socialexpectations with respect to computer technology, expectations that simply resultin a larger ''fall from grace" when the inevitable problems associated with anytechnology become manifest.new technologies widen the number of options with which a given problemmay be addressed. but whether any particular option is desirable is an issue thatdepends very much on the perspective of the person considering it. a nationalpointofsale network15 might be highly desirable for merchants, since it couldgreatly reduce the need for cash and paper work, but undesirable for individualswishing to keep private their buying patterns. a higher degree of autocomputingšsignificance, status, challenges26computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.mation in an office may be good from the standpoint of office managers wishingto increase efficiency, but bad from the standpoint of the union that fears a loss ofjobs.16 networked computers, introduced to facilitate computertocomputercommunication, now provide pathways for computer worms and viruses, as theinternet "worm" incident demonstrated in 1989.box 1.6 cryptography and digital signaturessome cryptosystems require the users to agree on a secret key inadvance (or rely on a trusted intermediary) in order to send a privatemessage between them. however, publickey cryptosystems eliminate thisdifficulty. if alice and bob wish to communicate, each makes up a secretkey and a public key. the public key is sent to the other party withoutprotection. the key can even be published in a "telephone book." thesystem is set up so that alice can send bob a message based on hersecret key and bob's public key. bob can decipher this based on his secretkey and alice's public key. however, an eavesdropper, without access toeither alice's or bob's secret key, cannot decipher the message.publickey cryptosystems can also be used to implement socalleddigital signatures. a digital signature works as follows. suppose alice wantsto send bob a signed public message: "alice owes bob $1000. signed,alice." clearly this will not do, since bob could add an extra 0 to theamount. instead, a signed message would look more like: ''alice owes bob$1000. signed, alice 285408382175."the number at the end of the second message is a digital signature. itdepends on the whole message and alice's secret key in a complicatedway. the signature has the property that its authenticity can be verified justfrom alice's public key, but neither bob nor anyone else can forge adifferent message, claiming it to be from alice.the security of publickey cryptosystems typically depends onnumerical operations that are difficult to invert. for example, the rsapublickey system depends on the difficulty of factoring large numbers. itseems hard to find the factors of 277,009. however, given the numbers 439and 631, it is easy to determine that 439 × 631 = 277,009. theoreticalcomputer scientists have played a major role in making precise the notionof computational complexity of various problems, i.e., the minimum numberof steps required to solve a problem such as factoring for inputs up to agiven size (measured, for example, by the number of digits).moreover, the introduction of new technologies increases the numbercomputingšsignificance, status, challenges27computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.of things that can go wrong. an airplane simulator can have incomparable valuefor training a pilot, but a design error in the simulator that results in a mismatchbetween simulated and actual airplane behavior could have disastrous results. themuchreported failures in 1991 of various new telephone switching systems,introduced to provide new telephone services, are another example. early buterroneous computerbased forecasting of election results could subvert thedemocratic process. false computergenerated reports of incoming missiles, suchas those reported in 1979 and 1980,17 could have catastrophic results for theentire planet.finally, despite their apparent sophistication, new technologies may beinadequate for many tasks demanded of them. computerbased automatic targetrecognition systems may be able to identify tanks on a battlefield, but inadequateto distinguish between friendly and hostile tanks. businesses that now rely oncomputers for the performance of critical tasks may still be frustrated by theirinability to adapt their computers readily to a changing business environment.new computing technologies also raise the issue of their cost. newtechnology is generally expensive, and thus it can benefit only those who canafford to acquire it. for example, highresolution monitors installed in schoolsand connected to a national network may enable students to view images stored innational archives, but schools that can barely afford basic school supplies will notbe the first to acquire such monitors. useful electronic information in the form ofsoftware, data files, and database access is often sold as a highpriced specialtyitem rather than a highvolume commodity for consumption by all. withoutpublic policy and moral commitment to notions of universal access, theinformation revolution may increase the gap between the haves and havenots toour collective detriment.it is unlikely that society will be willing to give up the benefits thatcomputing confers upon it, but society is rightly concerned with the problemsthat computing can cause or exacerbate. this concern generates opportunities forcomputer scientists and engineers to investigate the development of even newertechnologies that deliver more of the benefits but with fewer of the attendantcosts.the relationship between the federalgovernment and cs&e researchthe history of cs&e in both academia and industry reflects the stronginfluence of strategic investments by federal agencies. these investments havefunded work of direct and immediate relevance to government responsibilities(e.g., the use of the first electronic comcomputingšsignificance, status, challenges28computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.puters for military purposes) as well as work dispersed in large part through theacademic research community, the latter especially so in more recent years.without these investments, the computer industry and indeed the informationrevolution would have taken off much more slowly. while the nature andallocation of federal investment in cs&e have changed over the past fourdecades, the substantial rise in constant dollars of federal obligations in the last 15years suggests that the development of advanced computing capabilities throughthe support of cs&e research is increasingly understood by the federalgovernment to be essential to the missions of many government agencies as wellas to the welfare of the nation.a variety of federal agencies support research in cs&e, including thenational science foundation (nsf) and a few missionoriented agencies, e.g., thedepartment of defense, the national aeronautics and space administration, andthe department of energy. missionoriented agencies support basic and appliedresearch with the potential to contribute to their missions, while the nsf supportsless directed research. these four agencies accounted for 92 percent of cs&eresearch in fy 1991, both basic and applied, as indicated in table 1.1. thehighperformance computing and communications program, discussed below,promises to have a substantial impact on cs&e research in the next severalyears, since it calls for substantial interagency cooperation and substantialfunding increases to support highperformance computing and communications.figure 1.2 illustrates that support for cs&e research (basic and appliedtaken together) to all performers has increased substantially in the last decade,18as would be expected for a new and intellectually growing field; funding foracademic cs&e research exhibits a similar trend. (some readers may object tothe grouping together of basic and applied research. this has been done forreasons that will become apparent in chapter 2, but the general trend also holdsfor basic research alone.)the federal government is also a prodigious consumer of informationtechnology and related services, budgeting some $24 billion for informationtechnology in fy 1992.19 such expenditures reflect a much broader interest incomputer technology than might be implied by the government's researchinvestments alone. government computer use cuts across agencies andsometimes stimulates development by the private sector of new technologies tomeet government needs.the two agencies that account for the largest fraction of federal obligationsfor academic cs&e research are the department of defense and the nsf. (amore extended discussion of federal agenciescomputingšsignificance, status, challenges29computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.supporting cs&e research is contained in chapter 7.) among federal agencies,the department of defense is the largest single funder of cs&e research; indollar terms, it also accounts for the largest single share of academic research(figure 1.3). defense department support for cs&e research has contributeddirectly to many areas that have had a profound impact on computing practicetoday: timetable 1.1 federal funding (in fy 1991 dollars) for cs&e research and all scienceand engineering (s/e) research, fy 1991agencycomputerscienceresearch ($millions)cumulativepercentage oftotal forcomputer scienceall s/eresearch ($millions)defense418.7623,805national sciencefoundation122.7801,847national aeronauticsand spaceadministration52.2873,463energy33.3922,963commerce18.495444interior11.496549environmentalprotection agency8.398343transportation6.199146agency forinternationaldevelopment3.699290treasury1.79922health and humanservices1.51008,201agriculture1.51001,177education0.9100157housing and urbandevelopment0.210011federalcommunicationscommission0.11002other agenciesašš631total680.624,051note: table reflects the final disposition of federal obligations for fy 1991, includingcongressional action and administration budget reprogrammings in response to congressional action.figures for "computer science" are assumed to include computer engineering.a other agencies that supported some type of basic or applied research, but not in computer science,include the arms control and disarmament agency; the tennessee valley authority; thedepartments of labor, justice, veterans affairs, and state; the smithsonian institution; the nuclearregulatory commission; and the international trade commission.source: data from division of science resource surveys, national science foundation.computingšsignificance, status, challenges30computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.sharing, networks, artificial intelligence, advanced computer architectures, andgraphics. of course, it is not surprising that a missionoriented agency would tendto favor research focused on developing operational prototypes; what is strikingis that these research projects, initially justified on the grounds of military utility,have yielded such a rich harvest of civilian application.figure 1.2 total federal obligations for research for computer science (basicand applied), fy 1976 to fy 1991, in constant fy 1992 dollars. source:basic data (in thenyear dollars) for all recipients taken from federal funds forresearch and development (federal obligations for research by agency anddetailed field of science/engineering: fiscal years 1969œ1990), division ofscience resource studies, national science foundation. data for fy 1990 takenfrom federal funds for research and development: fy 1989, 1990, and 1991,national science foundation, nsf 90327. data for fy 1991 are preliminaryand were supplied to the committee by the division of science resourcestudies, national science foundation. basic data (in thenyear dollars) foracademia taken from federal funds for research and development (federalobligations for research to universities and colleges by agency and detailedfield of science/ engineering: fiscal years 1969œ1990), division of scienceresource studies, national science foundation. figures include both "computerscience" and "mathematics and computer science, not elsewhere classified."constant dollars calculated from gnp deflators used in national sciencefoundation, science and engineering indicators, 1991, nsf, washington, d.c.,1991, table 41.computingšsignificance, status, challenges31computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.figure 1.3 department of defense obligations for research for computerscience (basic and applied), fy 1976 to fy 1991, in constant fy 1992 dollars.source: basic data (in thenyear dollars) for all recipients and academia weretaken from the corresponding sources cited in the caption for figure 1.2.the nsf is the primary supporter of academic research in cs&e, asmeasured by the number of individual investigators supported. it also contributesthe second largest share of federal obligations to cs&e research, and almost allof that support goes to academia. figure 1.4 illustrates the nsf's history offunding cs&e research for the last 15 years. the budget for cs&e is the fastestgrowing budget category at nsf, although the budgets for other disciplines startat much higher levels.the nsf is the primary federal supporter of investigatorinitiated cs&eresearch. research agendas within the research directorates of nsf tend toreflect the needs and interests of the field as a whole, although program officialsdo exercise judgment in determining the appropriate mix of research topics beinginvestigated. moreover, since the mission of the nsf is largely to support basicresearchšwhich the u.s. government defines as research without application inmind (more on this point in chapter 2)šresearch supported by nsf is likely tobe farther removed from commercial or applicationscomputingšsignificance, status, challenges32computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.oriented impact than research, such as that supported by much of the defensedepartment, that is aimed specifically at developing operational prototypes ordemonstrating concept feasibility.that said, the nsf has supported research in cs&e that has had asubstantial impact on computing practice. for example, in the 1960s the nsfsupported the development of basic, a computer language designed for ease oflearning that is used in some applications even today. programming environments(i.e., systems used to support groups of programmers working together inconstructing, testing, and maintaining programs) and many important softwarepackages for numerical analysis (e.g., linpack for linear algebra) havebenefited from more recent nsf support. nsfsponsored work on imageprocessing in the 1970s and 1980s has led to better imaging scanners inmedicine.finally, both nsf and the defense advanced research projects agency(darpa) of the department of defense supported substanfigure 1.4 national science foundation obligations for research for computerscience (basic and applied), fy 1976 to fy 1991, in constant fy 1992 dollars.source: basic data (in thenyear dollars) for all recipients and academia weretaken from the corresponding sources cited in the caption for figure 1.2.computingšsignificance, status, challenges33computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.tial efforts in the 1970s and 1980s to build equipment infrastructure inuniversities for the support of experimental research projects in cs&e.although federal support for cs&e research has more than doubled in thelast 15 years, and support for academic cs&e research has about tripled, thenumber of active researchers in the field has also grown considerably. this hasbeen particularly true in the academic cs&e community, for which the availablefunding per active researcher has dropped slightly in the last 15 years and moresubstantially in the last year for which data are available (figure 1.5). this dropin available funding per active researcher is consistent with the fact that since1987, the number of awards made by nsf for cs&e research has lagged behindthe number of proposals submitted, resulting in a declining success rate for mostof this period (see figure 7.3 in chapter 7). thus the level and adequacy offederal funding for cs&e continue to be a source of major concern to academiccomputer scientists and engineers.in the last year, a program that cuts across the entire federal government wasbegun that is expected to have a major impact on federal funding for cs&e. thehighperformance computing and communications (hpcc) program began infy 1992 and is based on a 1989 report by the white house office of science andtechnology policy (ostp)20 that called for a program coordinated across allagencies with responsibilities for or an interest in highperformance computing.the program grew out of efforts by several federal agencies operating under thefederal coordinating council for science, engineering, and technology(fccset) umbrella and in conjunction with the ostp;21 at present, the initiativeinvolves darpa, the national aeronautics and space administration, thedepartment of energy, and the national science foundation, with theparticipation of the national institute of standards and technology, the nationaloceanic and atmospheric administration (noaa), the environmental protectionagency (epa), and the national institutes of health.the hpcc program addresses four areas of interest: highperformance computing systems that will improve the speed ofcomputing by two to three orders of magnitude; advanced software technology and algorithms that focus on softwaresupport for addressing certain grand challenges in science andengineering to best exploit highperformance computer systems andtools for more effective development of software systems; networking that will support research, development and deployment for agigabit national research and education networkcomputingšsignificance, status, challenges34computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.(nren) and eventual transition of this network to commercial service;andfigure 1.5 federal funding for academic computer science research (inconstant fy 1992 dollars) per academic researcher for fy 1977 through fy1989. source: data for federal funding were taken from the sources cited inthe caption for figure 1.2. data on the number of academic researchers weretaken from table 8.13 in this report. while funding for fy 1990 and fy 1991has risen (as depicted in figure 1.2), no definitive data are available on thenumber of academic researchers working in cs&e for these years, although thetaulbee survey of 1990œ1991 (david gries and dorothy marsh, "the 1990œ1991 taulbee survey," computing research news, volume 4(1), january 1992,pp. 8 ff.) reports that the number of cs&e faculty at ph.d.granting institutions(i.e., major research institutions) may be leveling off. these considerationssuggest that the funding per researcher may have risen in these years. human resources and basic researchšefforts will focus on expandingbasic research in all areas of computer science and technology relevantto highperformance computing and increasing the base of skilledpersonnel.22at this writing, the hpcc program has strong presidential support, and thecongress has authorized most parts of the hpcc procomputingšsignificance, status, challenges35computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.gram for five years.23 however, money for the program must be appropriatedyearly, and those portions whose authorization has expired must bereauthorized.24 if the program is fully funded, it will amount to some $1.9 billionover five years in "new money," i.e., money above and beyond amounts spent inthe baseline budget of fy 1991 (table 1.2).25the amount requested for appropriations in fy 1993 for highperformancecomputing and communications is an increase of $148 million over thecomparable amount in the fy 1992 budget. table 1.3 describes the fundinghistory of the hpcc program to date. the magnitude of the requested increasefor fy 1993, as well as the fact that the hpcc program for fy 1992 was actuallyfunded at an overall level higher than that proposed by the administration, is aclear recognition of the importance of highperformance computing andcommunications to national goals.26 however, given the central role that nsfplays in supporting the academic cs&e community, considerable concern withinthis community has been raised regarding the fact that the nsf portion of thehpcc program was funded below the requested level.overall, future federal funding trends are uncertain. while most federalpolicy makers appear to understand that cs&e is a field with major impact on thenation's economic health and social wellbeing, the federal budget will comeunder increasing stress in years ahead as the result of expected growth in federalbudget deficits in the future and the elimination of the socalled peace dividend.27also, major budget initiatives require long gestation periods; a significantinitiative proposed after august in year n, even if approved at alltable 1.2 proposed fiveyear funding profile for the hpcc programfiscal year and amount ($ millions)program area19921993199419951996highperformance computing systems5591141179216software and algorithms5190137172212networking305095105110basic research and human resources1525384659total151256411502597source: office of science and technology policy, the federal highperformance computingprogram, executive office of the president, washington, d.c., 1989, p.46.computingšsignificance, status, challenges36computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.steps in the process, is not likely to appear in the budget until year n + 3.28a key feature of the hpcc program is that it is framed within the context ofspecific applications of computingšthe socalled grand challenges. these grandchallenges are ''fundamental problem[s] in science and engineering, withpotentially broad economic, political, and/or scientific impact, that could beadvanced by applying highperformance computing resources."29 the hpccprogram recognizes that major improvements in computing performance relevantto these grand challenges will be possible only through highlevel collaborationamong computer scientists and engineers and scientists and engineers in therelevant areas. one result has been that missioncomputingšsignificance, status, challenges37computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.agencies not typically associated with cs&e research (e.g., noaa, epa) areassigned significant roles in the hpcc program.the relationship between cs&e and thecomputer industryone primary reason underlying the remarkable successes of cs&e is theextraordinarily fruitful interaction between academia and industry. academia hassupplied the computer industry with many cs&e graduates at all levels wellgrounded in the best that academic cs&e has to offer. furthermore, theinterchange of research ideas and problems in both hardware and software hasbeen strong and plentiful. for example, universities have contributed greatly tonew computer architectures such as the hypercube, the connection machine, andreducedinstructionset computing. workstations are another example of fruitfulcollaboration between the majority of computer manufacturers and the leadingcs&e research universities (box 1.7). in addition, many ideas on specificapplications developed in the academic environment, including documentpreparation systems, computer graphics, and database systems, resulted in a largenumber of startup companies that have had a significant impact on society atlarge and also on wellestablished computer and software manufacturers.industry has been a fountain of creative intellectual ferment for academia aswell. industry investment in cs&e research is considerable, estimated by thecommittee to be in order of magnitude comparable to the federal expenditures oncomputer science research.30 these investments have resulted in many computinginnovations (table 1.4) and have spun off considerable academic research. forexample, the first compilers for translating highlevel programming languagesinto machine language were invented in industry; this groundbreaking worklaunched hundreds of subsequent university research projects. the unix operatingsystem and the c programming language originated at bell laboratories;however, berkeley computer scientists have extended and modified the originalconcepts into a new version of unix that now enjoys wide acceptance as the basisfor highly interoperable and portable open software systems. in short, ideas firstnascent in the industrial sector have often given impetus to academic research incs&e.academic research and industrial cs&e research have both advanced thecomputing state of the art, but the differences in perspective between academicand industrial researchers are substantial. these differences are important foracademic researchers to understand.companies in the computer industry employ researchers to givecomputingšsignificance, status, challenges38computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.them a competitive edge in bringing new products to the marketplace or inimproving existing products. industrial researchers influence products through thecreation of innovative ideas, but to be useful to product developers, these ideasmust be taken to the point of product viability. at the same time, the only viablemechanism for the continual replenishment of the intellectual capital of theseresearchers is for them to be active contributors to the research enterprise.academic researchers have different goals. they may seek knowledge for its ownsake, circumscribed in part by the availability of funding, but are not necessarilybound by the need to translatebox 1.7 sun microsystemssun microsystems, a leading supplier of workstations, is an example ofsuccessful technology transfer from academia to industry. the first sunworkstation was built at stanford universityšin fact the company nameoriginally meant stanford university networkšand the system software forthat original machine came from the university of california at berkeley.graduate students and faculty from each university became the technicalfounders of that company. sun became the leader in the fastestgrowingsegment of the computer industry by continuing to take advantage ofinnovations developed at universities. (the current sun hardware andsoftware bear little resemblance to the sun 1.) in less than a decade sunhas grown to a $3 billion per year company that employs about 12,000people.table 1.4 computing innovations to which industry has contributedinnovationcompanies contributing to relatedcs&e researchfortranibmunix operating system,c programming languageat&t bell laboratoriesworkstationsxerox palo alto research centermicroprocessorsintelsupercomputerscray researchminicomputersdigital equipment corporationlocal area networksxerox; ibm; bolt, beranek, andnewmanreducedinstructionset computing(riscs)ibmrelational databasesibmcomputingšsignificance, status, challenges39computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.new ideas into viable products. table 1.5 depicts some of the characteristics ofwork in cs&e that tend to make a given problem a better fit to the environmentsoffered by either academia or industry.table 1.5 problem and project characteristics tending to favor academia or industryacademiaindustrywork directed primarily at the creationof new knowledgework directed primarily at improvedproducts and greater competitivenesssmall projectslarge projectswork with potential benefit in thelongtermwork with potential benefit in the shortand medium termsystematic investigation, theorybuildinginterdisciplinary innovationresearch can thus be viewed as providing a "service function" to those whodevelop the nation's computing capability, i.e., the product developers. in somecases, research provides technologies that are directly applicable to products,even though the need for those technologies may not be known by the productdevelopers. but in many other cases, research is needed to systematize thesometimes ad hoc discoveries and inventions that arise from the practicalimperative. entire new research areas in cs&e have developed in this way:operating systems (from the need to use advanced computer systems), databasetechnology (from the need to manage large volumes of data), computer securityand encryption (from the need to ensure privacy), image processing (from theneed to handle pictorial information), and data compression (from the need toreduce transmission times of data objects). in all these cases, a rudimentarypractice preceded the formulation of the subfield. the role of systematic researchwas then to systematize, generalize, and clarify the concepts, so that systems ofmuch greater capability could be built more easily. this is another way ofexpressing the important concept that theory often lags practice.if researchers are to perform research that is relevant to products, they mustunderstand the objectives and capabilities of the product developers of industry.to the extent that they can anticipate the future needs of product developers,researchers can address the concerns of product development managers bycarrying their innovative ideas far enough to demonstrate both their value forproducts and the improbability of nasty implementation surprises. box 1.8 decomputingšsignificance, status, challenges40computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.scribes some of the things that researchers need to understand about thedevelopment environment.how is research best coupled to practical ends? the literature on technologytransfer is vast, and a comprehensive examination of technology transfer issues isfar beyond the scope of this report; box 1.9 describes some of the issues posed bytechnology transfer for the computer industry.still, in its examination of technology transfer in the context of academiccs&e, the committee identified one point as particularlybox 1.8 what researchers need to know aboutdevelopmenta developer's job is not to use the best technology, but rather to buildthe needed product quickly and inexpensively and in a way that meetsrequirements. development managers usually regard new technology asdangerous and to be used only when there is no alternative; onedevelopment manager at a major computer company with a 30year recordof success has said, "whenever anything fails, look at the latestimprovement." in most cases, given a choice between proven and unprovenapproaches, applications developers (who must commercialize someproduct) are understandably biased toward the proven, even when anunproven approach might ultimately prove more appropriate. butsometimes developers have to use new technology, because oldertechnology won't do the job.the research community has a different perspective. the business ofthe research community is new technology, and so researchers view thedevelopment manager the way a social worker views a drug addictšassomeone to be reformed, regardless of what is needed for productdevelopment. because industrial researchers get so few chances to movetheir new technology into practice, they want developers to take hugechunks of it all at once; what development managers want is to take thesmallest possible chunks, to improve the chances of coping with thesurprises sure to spring from it. and, because of the pioneering spirit thatled them into research in the first place, researchers think "compatible" is adirty word, whereas developers live in a world where products fail if they arenot compatible with what the customer already has. moreover, researchersoften do not appreciate the fact that new ideas may have substantial impacton the internal technical environment and on assumptions about theintended customer environment. managing this impact may well be moredifficult than introducing the basic core of the new ideas.computingšsignificance, status, challenges41computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.critical: the routes by which research knowledge can be transferred to those whocan benefit from it.box 1.9 issues posed by technology transfer forthe computer industrythe strong coupling between academic cs&e and the industrial sector,especially compared to other fields. technology transfer is both moreimmediate and more bidirectional for cs&e than for other fields.rapid changes in the computing technology base, which increase theimportance of effective technology transfer. indeed, new computingtechnologies often stress the limits of human understanding at the time oftheir introduction; that is why they lead to new research questions. in orderto exploit fully the potential of these new technologies, deeperunderstanding and insights made possible by academic research must bemade available to the industrial sector promptly. the speed of thesechanges may also discourage interactions between researchers who areconcerned primarily with the longterm effects of their work and productdevelopers who are more concerned with shorterterm payoffs.the different mix of r&d vs. manufacturing costs for computerproducts (specifically software) visàvis products related to other fields. (inparticular, r&d costs dominate the manufacturing cost of software, but thereverse is true for other products.) this mix increases the feasibility oftechnology transfer based on smallscale startup companies formed on thebasis of freely available (academic) research.specifically, there is a broad consensus that knowledge is best transferred bymoving or using the people who understand the new technology, believe in it, andare motivated to fix and solve any problems that may arise during the transfer.31the presence of such individuals is also reassuring to decision makers becausethe former are thoroughly familiar with the new ideas. technology transfer viapublications (newsletters, papers, technical reports), documentation, managementdecree, or onetime workshops is far less effective. one reason that people aremore effective conduits than paper in this context is that technical concepts haveto be adapted for product usage, often substantially, in the giveandtake essentialbetween conception and use. individuals familiar with the new technical conceptcomputingšsignificance, status, challenges42computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.are generally in the best position to perform dynamic adaptation, whilepublications and paper are static.against this background, the committee notes that many current industrialaffiliates programsšuniversity programs designed to facilitate greater contactsbetween industry and university researchšemphasize prepublication access toresearch results, but not frequent interaction. the principle that people are moreimportant than paper seems to govern the most successful mechanisms fortechnology transfer: institutional collaborations between academia and industry that providefor sustained, sidebyside contact between the participants. in theabsence of such intimate contact, academic research can go off indirections that are essentially irrelevant to commercialization. cs&e faculty and graduate students who form startup companies.small startup firms have been responsible for a disproportionately largeshare of new commercial applications, often exploiting research andgood ideas from elsewhere,32 although work funded today by venturecapital tends to have a shortrange focus, may be less ambitious or riskythan research without clear commercial applications, and is increasinglyscarce.33 graduating students who bring knowledge of the most recentdevelopments in cs&e to the companies that hire them. of course, ifsuch students are inadequately exposed to these developments, theycannot perform this function; thus undergraduate and graduate educationthat reflects the best the field has to offer is a sine qua non if this route isto be effective.if such cooperative efforts are to succeed, it is important that industryunderstand important values of academia, and vice versa. thus successfulcooperative efforts will tend to involve minimal restrictions on academicpublication, e.g., delays in publication of at most a few months. in practice, suchlimitations may have little practical significance either to academics34 (sincepreparing a paper for publication often lags the obtaining of research results byseveral months to a year or more in any event, and the appearance of a paper inprint often takes another six months or more) or to industry (since the duration ofthe advantage that industry reaps from product innovations is often measured inmonths rather than years). furthermore, papers often emphasize concepts andtechniques, which are usually not as sensitive as proprietary details that tend to berelatively uninteresting from a scientific perspective in any event.in return, academia must understand industrial concerns. one such concerninvolves intellectual property rights, a new and evolving area of both legal andethical concern. in addition to the issuescomputingšsignificance, status, challenges43computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.raised by the intangibility of software (box 1.10), other concerns arise withrespect to the knowledge gained through or with that software, the conflictbetween possible patent or copyright rights of the people who wrote that softwareversus the people who financed it versus the people who use it, and the grantingof recognition to those who have done the work while protecting theentrepreneurial rights of the sponsors. as one example, many companies in thecomputer industry crosslicense their patents with one another to ease the processof bringing individual products to market. but in an environment of pressures forexclusive licenses to maximize revenues (pressures often exerted by universities)and legal uncertainties regarding patent and copyright protection for software, theinterests of academia and industry may diverge. however these issues areresolved in any given case, resolution takes time. universities and companies thatbox 1.10 intellectual property issues in cs&elegal concepts of "property" are based on the notion of property as aphysical object. but when artifacts are composed fundamentally ofinformation and are thus much more easily reproduced than created, theseconcepts break down. although notions of patents, copyrights, and tradesecrets have been applied variously to computer software (an artifact forwhich intellectual property issues often arise), the entire area with respect tosoftware is today fraught with legal uncertainty.some of the issues that arise are the following: the patentability of algorithms; the extent to which copyright can protect modifiable source code; the protectability of user interfaces, especially their "look and feel"; and the balance of compatibility and interoperability vs. innovation andcreative advances.a challenge for the cs&e discipline is therefore to develop ways torepresent the structure and content of its unique contributions that can passlegal muster; in the absence of such representations, industry and thediscipline will continue to suffer from conflicting jurisprudence.for more discussion of these issues, see computer science andtelecommunications board, national research council, intellectualproperty issues in software, national academy press, washington d.c.,1991.computingšsignificance, status, challenges44computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.have made use of umbrella agreements covering all joint work have found thatthe time between initial contact and final settlement of terms has been sharplyreduced.the changing environment for academic cs&ein its infancy and adolescence, academic cs&e has experienced rapidgrowth and progress. support for the field increased at a fast clip, and thefounders of the new field sustained a high degree of productivity. computersthemselves, once housed in a few large buildings, are now everywhere on officedesks.but many important changes are pending in the intellectual, economic, andsocial milieu in which academic cs&e is embedded. perhaps the most importantis that a solid record of success increases expectations of those inside the field forcontinued support and outside the field for continued practical benefits. fiscalconstraints faced by the major funders of research in this countryšthe federalgovernment and industryšare likely to result in greater pressure to trim researchbudgets and at the same time generate increased pressure for research to producetangible benefits.against this backdrop, several additional changes must also be considered.changes in the computer industrythe computer industry is itself undergoing massive change. the influence offormerly strong players such as data general, unisys (and its presecessors), andcontrol data has waned considerably in the past 20 years, and an environment inwhich ibm and apple computer are motivated to collaborate is a different oneindeed. international competition is on the rise. and, although today's computerindustry was built primarily on the sales of large mainframe computers to arelatively few institutions, the computing environment of the future willemphasize to a much greater extent computers as consumeroriented itemsštoolsfor the masses.in this environment, computing technologyšboth hardware and softwarešwill be specialized for intellectual work in much the same way that electricmotors are specialized for physical workšit will be invisible but ubiquitous. justas electric motors are an important but invisible part of heaters, washingmachines, refrigerators, and alarm clocks, so also computers and software aretoday or will be embedded in telephones, televisions, automobiles, andlawnmowers. (actually, it will not be surprising to find them in washingmachines andcomputingšsignificance, status, challenges45computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.refrigerators as well.) it is the increasing ubiquity of computing that has led manyanalysts to predict the eventual convergence of computer, communications, andentertainment technology and the emergence of information appliances that arededicated to specific tasks (such as pocket calendars or remote library accessdevices). new computer systems will be increasingly portable and are likely to beinterconnected to each other or to information service providers, and they maywell embody new computing styles such as penbased computing.accompanying these changes in the computer industry per se are othermajor changes that are affecting all industries. in particular, changes in thebusiness environment portend vastly greater globalization and time compression.to survive, let alone prosper, industry in the future will have to respond to amuch larger range of competitors than in the past, and to respond much morerapidly than it has in the past.for the computer industry, these changes mean that products will have to befitted to customer needs much more precisely. since customers are interested incomputing technology primarily for its value in solving particular problems,knowledge of the customer's application will become more and more important;such knowledge will most likely become embedded in software written to servethese applications. since customers will be understandably reluctant to abandonsubstantial investments in hardware, software, and human expertise, it will benecessary to design new products with a high degree of compatibility with earliergenerations. indeed, even today many customers are unable to keep up with, letalone exploit to best advantage, the capabilities of new computing technologies.since the particular computer products needed by customers cannot be anticipatedyears in advance, industry will have to place greater emphasis on reducing thetime to market for new products; thus tools, technology, and approaches to design(e.g., rapid prototyping) to facilitate shorter response times will be necessary,especially for the now laborintensive software sector.finally, greater concerns about competitiveness will increase financialpressures on the computer industry, just as they affect all other industry. in anenvironment of cost cutting, activities that cannot demonstrate an impact on thebottom line will be highly suspect and subject to reduction or elimination. thus itwould not be surprising to see industrial research laboratories shift their focus toefforts with a more "applied" flavor in the quest of their parent companies forcompetitive advantage.35 such a shift may already be starting to occur: the strongconnection between cs&e and computing praccomputingšsignificance, status, challenges46computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.tice has led to strong demand from the computer industry for individuals who are''system builders," making it more difficult for academia to compete effectivelyfor such individuals. costcutting pressures may also be reflected in thewillingness of the industry to continue unchanged its practices of donatingequipment to academic cs&e departments. these donations (or reducedpricesales) account for a substantial amount of the equipment that these departmentsuse for research and educational purposes.36structural changes in academic cs&efrom a personnel standpoint, the cs&e field has undergone tremendousgrowth in the last decade. for example, according to the office of scientific andengineering personnel of the national research council, u.s. ph.d. productionin cs&e grew from its 1979 level of 235 graduates per year to 531 in 1989.37the number of undergraduate degrees awarded per year grew by more than afactor of three and may be rising again. the number of academic doctoral levelresearchers working in cs&e grew from 1052 to 3860 over the same timeperiod, and the number of individuals who have doctorates and are teachingcs&e increased from 1613 to 5239.38 the median age of doctoral faculty whoteach cs&e grew from 38.4 in 1977 to 43.4 years in 1989, which was about themedian age of all doctoral scientists and engineers regardless of field in 1981.(chapter 8 discusses these and other human resources trends in academic cs&e.)tremendous growth characterizes the intellectual side of cs&e as well.while it is of course difficult to document in quantitative terms the intellectualmaturity of a field, it is nevertheless the judgment of the committee that cs&e asan intellectual endeavor has indeed come of age. although as an organized andindependent intellectual discipline it is less than 30 years old, cs&e hasestablished a unique paradigm of scientific inquiryša computational paradigmšthat is applicable to a wide variety of problems and has become the base onwhich a critical enabling technology of the next century will be built. theopening pages of chapter 3 and the section "selected accomplishments" inchapter 5 discuss the accomplishments and the research paradigm in greaterdetail.changes in the university environmentacademic cs&e will be affected by the university environment, anenvironment that is itself in the midst of remarkable changes.computingšsignificance, status, challenges47computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.one major issue is the fact that the compact between the federal governmentand university research developed in the 1940s and 1950s is under increasingpressure. implicit in this compact was the understanding that placing decisionsregarding the course of basic research in the hands of the investigating scientistwould lead to substantial social and economic benefits as the result ofgovernment support for such research.39 however, recent events such ascongressional interest in alleged abuses in government funding of universityresearch40 suggest that pressures for accountability will increase in the future, andit is entirely plausible that accountability for research will require concretedemonstrations of positive benefit to the nation.financial considerations also loom large. universities everywhere aresuffering from ever tighter budgets, and it does not appear that these exigencieswill abate in the foreseeable future. apart from the difficulties that all academicdisciplines will face in matters such as faculty hiring, academic cs&edepartments will face particular problems in maintaining infrastructure to meetthe field's research needs.as noted earlier, many research problems in cs&e are driven and motivatedby the upper bounds of performance at the cutting edge of computing technology(whether these edges result from sophisticated new components or novelarrangements of older components). the availability of stateoftheart systems toaddress these problems is therefore critical if cs&e departments are to stay at thecutting edge of research, whether in software or hardware. however, stateoftheart systems are always expensive, and acquisition of such equipment does notbenefit from the downward cost trend that characterizes computing equipment of agiven sophistication or performance. compounding the problem is the fact that asystem that is state of the art today may not remain so for very long.41 large andoften recurring replacement costs will be necessary for departments to remain atthe hardware state of the art.capitalization for educational purposes is also an important aspect ofacquisition budgets. cs&e students (especially undergraduate students) may notneed access to computing equipment that is absolutely at the cutting edge, but alltoo often undergraduate cs&e students must make do with personal computersthat were acquired in the mid1980s and that often cannot run modern software.when they must use hardware whose capabilities are so limited, students areforced to struggle with machine limitations rather than focusing on centralconcepts that could be more clearly illustrated with more powerful machines. forthe teaching of some topics, hardware that is so limited in performance is noteffective as a pedagogical tool.computingšsignificance, status, challenges48computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.summary and conclusionscomputing has become indispensable to modern life, and every computer inuse today is based on concepts and techniques developed by research in cs&e.future advances in cs&e research will have a similar impact: they will increasethe use of computing and the effectiveness of computing. but after severaldecades of vigor and growth, the cs&e field is facing a very differentenvironment. academic computer scientists and engineersšthe primary groupaddressed in this reportšwill have to cope with a host of new challenges, somearising from the remarkable successes of the discipline (e.g., the spread ofcomputing to virtually all walks of life) and others from factors entirely outsidethe discipline (e.g., pressures on federal research support).how should the community respond? as chapter 2 describes at length, thecommittee believes that academic cs&e must begin to look outward, embracingrather than eschewing other problem domains as presenting rich and challengingtopics for cs&e research.note1. business week, october 28, 1991, p. 120.2. written testimony of jack l. brock, information management and technology division of thegeneral accounting office, to the subcommittee on science, technology and space of the senatecommerce committee, march 5, 1991, p. 6.3. written testimony of jack l. brock to the subcommittee on science, technology and space of thesenate commerce committee, march 5, 1991, p. 4.4. the business week 1000, 1991 special issue, pp. 174œ175.5. for example, a floppy disk with a wordprocessing program on it and one without the program onit have identical weights, but the first disk is much more useful and valuable.6. the business week 1000, 1991 special issue, p. 167. the "entertainment" category lists five majorcorporations.7. u.s. department of commerce, u.s. industrial outlook 1991, u.s. government printing office,washington, d.c., 1991, p. 39œ6.8. the business week 1000, 1991 special issue, p. 178.9. the gnp of the united states was $5465.1 billion in 1990 (u.s. department of commerce, surveyof current business, volume 71(7), july 1991, p. 5). for 1990, the computer and businessequipment manufacturers association (cbema) estimated revenues derived from computerequipment at $153.7 billion (p. 26), from computer software at $92.4 billion (p. 24), and fromtelecommunications equipment at $61.7 billion (p. 26); in total, these categories accounted for about5.6 percent of the gnp. (page references are for cbema industry marketing statistics committee,the information technology industry data book: 1960œ2000, computer and business equipmentmanufacturers association, washington, d.c., 1990.) a different set of estimates is provided by theu.s. department of commerce (u.s. industrial outlook 1991, u.s. government printing office,washington, d.c., 1991): computers and peripherals, $71 bilcomputingšsignificance, status, challenges49computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.lion (p. 28œ1); software, $29 billion (p. 28œ15); telephone and telegraph equipment, $18.5 billion (p.30œ1); radio and tv communication equipment, more than $55.8 billion (p. 31œ1); electronicinformation services, $9 billion (p. 27œ2); data processing and network services, $31 billion (p. 27œ3); and computer professional services, $44 billion (p. 27œ4). taken together, these categories totaled4.7 percent of the gnp.10. in this report, the term "computing" denotes both the electronic activity taking place whencomputers are being used and the problemsolving activities to which computers are directed."computing practice" or "the practice of computing" denotes computers used as tools for solvingproblems in domains not intrinsically related to computers themselves. "computer science andengineering" (cs&e) is used more narrowly to denote a field whose research and developmentactivities are related to computers per se.11. the notion of cs&e as a discipline based on theory, abstraction, and design is described in peterdenning, douglas e. comer, david gries, michael c. mulder, allen tucker, joe turner, and paul r.young, "computing as a discipline," communications of the acm, volume 32(1), january 1989, pp.9œ23.12. personal communication, donald knuth, march 10, 1992 letter.13. frederick brooks, the mythical manmonth, addisonwesley, reading, mass., 1975, pp. 7œ8.14. ivan sutherland, "computer displays," scientific american, june 1970, p. 57.15. a pointofsale network is a network of electronically linked cashregister/terminals that cancapture purchasing information at the moment and place a sale is made (i.e., at the "point of sale") forsuch purposes as tracking inventory, debiting and crediting funds between customer and store bankaccounts through electronic funds transfer, or automatically generating purchase orders for newmerchandise. or, it can perform some combination of these tasks.16. in 1990, 53 percent of the american public disagreed with the statement that "computers andfactory automation will create more jobs than they will eliminate." see national science foundation,science and engineering indicators, 1991, nsf, washington, d.c., 1991, p. 455.17. gary hart and barry goldwater, recent false alerts from the nation's missile attack warningsystem, report to the senate armed services committee, u.s. government printing office,washington, d.c., october 10, 1980. in 1979, a test tape was mistakenly entered into the missile earlywarning system of the strategic air command. in 1980, the failure of a computer chip generated twoerroneous warnings of incoming missiles.18. funding figures have been drawn from various sources of the nsf division of science resourcesstudies (srs) series. the nsf srs division compiles these figures on the basis of questionnairescompleted by the various federal agencies. thus it identifies what the various agencies believe shouldbe counted under the label "computer science." such selfidentification of funds, in the absence of astandard and consistent definition, may easily lead to errors and omissions, especially in the case ofprojects that contain important cs&e elements but that are not themselves obviously cs&e.for example, the national institutes of health does not fund much research thatit reports as "computer science" research ($300,000 in fy 1990). nevertheless,according to an nih briefing received by the committee, nih funds some $150million per year in medical imaging research, research that has a strong cs&easpect and may even be performed in cs&e departments. similarly, researchfunded under electrical engineering may be computer design. however, agenciesmay also label as "computer science" work that may more properly be classifiedunder "applied mathematics."computingšsignificance, status, challenges50computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.srs figures have been used because they come from a single source thatattempts to ensure that trend comparisons can be made. the alternative wouldhave been to dig into the data in detail (i.e., at the individual grant and contractlevel) for the years in question, an undertaking well beyond the scope of thisproject. in addition, since individual agencies tend to use the same identificationprocess year in and year out, the srs figures are likely to reflect trends overtime for individual agencies. however, note that when funding figures for fy1990 and fy 1991 are presented, they are preliminary and subject to laterrevision.figures presented in the funding charts of this chapter and in chapter 7 are thesum of line items labeled "computer science" and "mathematics and computerscience, not otherwise classified," are in fy 1992 (constant) dollars, and areassumed to include computer engineering.19. bob brewin, "it dollars to inch up next year," federal computer week, april 25, 1992, p. 1.20. office of science and technology policy, the federal highperformance computing program,executive office of the president, washington, d.c., september 1989.21. the federal coordinating council on science, engineering, and technology consists of the headsof all agencies that have responsibilities for issues with significant scientific or technical aspects.chartered in the early 1970s and revitalized in 1989 under science advisor d. allan bromley, itspurpose is to provide interagency coordination for activities related to such issues.22. the 1989 ostp report articulated specific goals: increasing ph.d. production in computer scienceto 1000 per year by 1995, upgrading 25 additional university computer science departments tonationally competitive quality, and improving connections between computer science and otherdisciplines, including the creation of at least ten computational science and engineering departments(p. 40). but neither the legislation nor its legislative history mention these specific goals, except tospecify that the congress expects the hpcc program to be similar to that presented in the 1989report (senate commerce committee, highperformance computing act of 1991: report of thesenate committee on commerce, science, and transportation, report 102œ57, u.s. governmentprinting office, washington, d.c., 1991, p. 16).23. since the hpcc program is a multiagency program, authorizations are controlled by differentcommittees of the congress. fiveyear authorizations for the nsf, departments of energy andcommerce, nasa, and the environmental protection agency were specified by the highperformance computing act of 1991. a oneyear authorization for the darpa portion was passed bythe national defense authorization act for fiscal years 1992 and 1993, and will be revisited in fy1993. the national institutes of health has been operating under the "rolledover" authorizinglegislation of fy 1990 since that year, although a multiyear authorization bill for fy 1993 and beyondis pending in congress as this report goes to press.24. the budget process typically involves four major steps. the first is that the administrationproposes a budget, called "the administration's request." the second step is usually that the congresspasses "authorizing" legislation that provides what amounts to an upper bound on the amounts that thecongress may appropriate in later years. authorizing legislation also generally determines the broadpolicy outlines that the administration must follow in implementing the program. authorizinglegislation is often (though not always) based on the broad outlines of the administration's request; formajor programs, authorizing legislation nearly always makes some budget or policy changes in therequest. in the event that authorizing legislation is not specifically passed for any given fiscal year,congress often resorts to stopgap legislation that simply rolls over authorizations from previousyears. the third step is thatcomputingšsignificance, status, challenges51computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.the congress passes "appropriating" legislation that provides the administration with the authority toobligate money (i.e., write checks for specific purposes); appropriating legislation is passed yearly.there is no legal requirement that the amounts appropriated match the amounts authorized, though inpractice amounts appropriated above the authorized figures are rare and amounts appropriated underthe authorized figure are somewhat more common. the fourth step is that the administration respondsto the congressional appropriation. for example, if the appropriation for the national sciencefoundation is lower than that proposed in the president's budget, the administration must decide howto parcel out that cut among the various directorates of the nsf; it has complete freedom to makethese decisions, as long as they are consistent with congressional intent on the matter.25. the difference between the 1991 amount in table 1.3 ($489 million) and the amount in table 1.1for fy 1991 ($680 million) reflects the fact that not all federally funded cs&e research is part of thehpcc program. similarly, not all hpcc program funding is intended for the cs&e community;researchers in other "grand challenge" disciplines will also benefit from the hpcc program.26. during congressional debate on the hpcc program, the administration cited a study thatestimated a payback of $10.4 billion in supercomputer revenues from the pursuit (at full fundinglevels of $1.9 billion over the next five years) of the hpcc program (p. 119). this study also forecast acumulative increase in gnp of $172 billion to $502 billion over the next decade (p. 143). see thegartner group, highperformance computing and communications: investment in american competitiveness, stamford, connecticut, march 15, 1991.27. the ending of the cold war was thought by many to herald an era in which military spendingwould be sharply curtailed and the savings made available for other purposes. but the budgetagreement for fy 1991 between the president and the congress stipulated that military spending andnondefense, discretionary spending would constitute two entirely separate categories and that cuts inone category could not be used to increase spending in another category. this agreement wasoriginally scheduled to expire in fy 1993, so that the fy 1994 budget will not be subject to this rule.whether this agreement will continue to remain in effect is not clear as this report goes to press.a very good survey of the pressures on federal funding of the research enterpriseis contained in office of technology assessment, federally funded research:decisions for a decade, u.s. government printing office, washington, d.c.,may 1991.28. david sanchez, "the growing, caring and feeding of a budget," nsf directions newsletter,stis dir916, office of legislative and public affairs, national science foundation, washington,d.c., volume 4(2), marchapril 1991.29. office of science and technology policy, the federal highperformance computing program,executive office of the president, washington, d.c., september 8, 1989, p. 8. some of the grandchallenges listed on pp. 4950 of this document are the prediction of weather, climate, and globalchange; semiconductor design; drug design; the human genome project; and quantumchromodynamics.30. the committee's estimate is based on an assumption that the halfdozen or so major firms in thecomputer and communications industry (e.g., at&t, ibm) employ a few thousand fulltime cs&eph.d. researchers and hire hundreds of new cs&e ph.d.s every year (as indicated by the varioustaulbee surveys). assuming that each researcher costs an average of $200,000 per year in salary,benefits, and equipment, industrial researchers represent an annual investment of several hundredmillion dollars per year. (this estimate does not take into account the fact that a substantialcomputingšsignificance, status, challenges52computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.portion of industrial research is conducted by holders of master's degrees or ph.d.s from other fields.)this figure can only be estimated due to the fact that reports of corporate r&dspending generally do not disaggregate research and development, let aloneresearch in different fields. however, according to common rules of thumb,research costs tend to be perhaps a tenth of development costs, which arethemselves perhaps several percent of gross revenues. thus the figure of"several hundred million" per year spent on cs&e research is not grosslyinconsistent with the $153 billion per year in sales of the computer industryreported by cbema in note 9 above. (one data point on the relative size ofresearch vs. development is that ibm's r&d budget in 1991 was about $6.5billion, of which 90 percent went to development. see john markoff, "abepeled's secret startup at ibm," new york times, december 8, 1991, section 3,p. 6.)31. this understanding is echoed in governmentuniversityindustry research roundtable/academyindustry program, new alliances and partnerships in american science and engineering, nationalacademy press, washington, d.c., 1986, p. 36.32. computer science and technology board, national research council, keeping the u.s.computer industry competitive: defining the agenda, national academy press, washington, d.c.,1989, p. 59. the report notes that many successful ideas in software have had their origin in largeresearch investments by big companies and that these ideas have been commercialized by smallstartup firms. though the report refers to research originating in industry, the same is likely true foracademic research as well, since the difficulties of commercializing research tend to arise regardlessof the research's origin.33. the flow of venture capital to small business had dropped by nearly a factor of two in 1990compared to its peak in 1987. see "agenda for business," u.s. news and world report, june 3,1991, p. 62.34. see also governmentuniversityindustry research roundtable/ academy industry program, newalliances and partnerships in american science and engineering, national academy press,washington, d.c., 1986, p. 29.35. for example, kumar patel, a research director at at&t bell laboratories, says that "we have anarrow view of what's important to us in the long run. what we call basic research is what fits thegeneral needs of the company." see "physics losing the corporate struggle," nature, volume 356,march 19, 1992, p. 184. while this article emphasizes shifts at bell labs, bellcore, and ibm awayfrom basic research in physics, the reasons for such shifts are closely related to the business interestsof the respective companies.36. according to an nsf survey, private and industrial sources accounted for about 29 percent ofresearch equipment acquisition budgets for academic cs&e in 1988. see national sciencefoundation, academic research equipment in computer science, central computer facilities, andengineering: 1989, nsf 91304, nsf, washington, d.c., january 1991, table 4, p. 5.37. throughout this report, figures related to ph.d. production are taken from the office of scientificand engineering personnel (osep) of the national research council. as chapter 8 indicates, thesenumbers at times differ considerably from figures commonly available to the field, such as those ofthe taulbee surveys; these figures also lag the taulbee survey by a couple of years. however, thesefigures have been used because the osep is also responsible for collecting such data for other fields,making the data usable for comparative purposes. reasons for the discrepancies in data from thevarious sources are discussed in chapter 8.38. the number of academic cs&e researchers over time is presented in table 8.13.computingšsignificance, status, challenges53computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.the number of those teaching cs&e in these years is taken from data providedby the office of scientific and engineering personnel of the national researchcouncil and includes those teaching computer science, computer engineering,and information sciences.39. this compact is best described in vannevar bush, sciencešthe endless frontier, nsf908,national science foundation, washington, d.c., 1945/1990: "scientific progress on a broad frontresults from the free play of free intellects, working on subjects of their own choice, in the mannerdictated by their curiosity for exploration of the unknown" (p. 12) and "support of basic research inthe public and private colleges, universities, and research institutes must leave the internal control ofpolicy, personnel, and the method and scope of the research to the institutions themselves" (p. 33), aswell as the text of note 1 in chapter 2.40. colleen cordes, "audits indicate 14 universities improperly charged government for $1.9 to$2.4 million in overhead," chronicle of higher education, volume 38(10), october 30, 1991, pp.a26a29; daniel e. koshland, jr., "the overhead question," science, volume 249, july 6, 1990, pp.10œ13.41. in one nsf survey conducted in 1985œ1986, administrators from computer science departmentsregarded research instrumentation and equipment that was more than one year old (on average) as not"stateoftheart." see national science foundation, academic research equipment in selected science/engineering fields: 1982œ1983 to 1985œ1986, srs 88131, nsf, washington, d.c., june1988, table b5, p. b14.computingšsignificance, status, challenges54computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.2looking to the future of cs&ebroadening the fieldthe time has come for the cs&e community to adopt a broader agenda thatbuilds on the traditional strengths and interests of computer scientists andengineers. in particular, a broader agenda asks the community to: look outward as well as inward. a broader agenda would legitimizecloser couplings to science, engineering, commerce, and industry. thecommittee believes that outwardlooking interactions will enrich cs&eas a discipline by identifying new and challenging research problems,and will provide valuable assistance to those in science, engineering,commerce, and industry whose problems require the best talent andexpertise that cs&e has to offer. encourage greater interaction between research (especially theoretical research) and computing practice. cs&e has a tradition of derivinginspiration and richness from practice, and, in turn, contributing cleanconcepts and fundamental theory that have been effective in furtheringcomputing practice. this tradition is well represented by the extensiveinterplay between theory and practice in programming languages andcompiler design, databases, machine architecture, operating systems,distributed computing, and computer graphics. however, as cs&e hasmatured, the theoretical side of many of these areas has become moreinwardly focused. this is not altogether unlooking to the future of cs&e55computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.desirable, but it is crucial that researchers working in these areasmaintain an active effort to draw inspiration from practice and tocontinue to rise to the challenge of making a difference to the outsideworld. box 2.1 illustrates possible connections between theoreticalresearch and computing practice that arise in the context of the highperformance computing and communications program.box 2.1 some areas of theoretical work incomputer science relevant to the hpcc program numerical and parallel algorithms queuing theory and network flow algorithms efficient pattern matching (including dynamic programming) graph theory and graph embeddings source: national research council, mathematical foundations ofhighperformance computing and communications, national academypress, washington, d.c., 1991, p. 27.the committee's belief in the wisdom of a broader agenda for cs&e is basedon several considerations. the first is that computing most often servesdisciplines and areas other than cs&e; even the practice of such a characteristiccs&e topic as designing computer languages cannot be fully abstracted awayfrom application domains, a point all too often overlooked in cs&e's search forthe generally applicable. it would, for example, be folly to try to build even theframework of a computer language for music composition without a backgroundin music. beyond the inescapable engineering substrate of digital electronics andcommunications, computer scientists and engineers need to have someappreciation for the economics, finance, and administration intrinsic to business,the mathematics and physics behind engineering, and the mathematics and othersciences that underlie computing applications in industry.moreover, the number of problem domains to which cs&e is directlyrelevant will grow dramatically over time as a direct result of the increasingproliferation of computing into all sectors of society. thus broadening presentsmajor intellectual opportunities for researchers in cs&e. a precedent to keep inmind in this regard is that of mathematics (box 2.2).finally, nonroutine applications of computing technology to other problemdomains can be regarded as explorations undertaken tolooking to the future of cs&e56computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.understand empirically the actual utility of a given generation of computingtechnology. if computer scientists and engineers are involved in the design,implementation, and analysis of these experiments, inadequacies in any givengeneration of computing technology will be better understood, laying thegroundwork for the invention of the next generation.box 2.2 the precedent of the mathematicsdisciplineprogress in mathematics has often been stimulated by thedevelopment of new techniques invented to solve hard problems suggestedby outside applications as well as by the inner logic of the subject. thesenew techniques have then been explored for their intrinsic mathematicalinterest, abstracted, and incorporated with the rest of mathematics.mathematics has prospered by balancing these two influences on itsdevelopmentšits own inner logic and the demands of applications. indeed,it is worth recalling the words of one of the foremost mathematicians of alltime (and computer pioneer), john von neumann:as a mathematical discipline travels far from its empirical source, orstill more, if it is a second and third generation only indirectly inspired byideas coming from ''reality," it is beset with very grave dangers. it becomesmore and more purely aestheticizing, more and more purely l'art pour l'art.this need not be bad, if the field is surrounded by correlated subjects, whichstill have closer empirical connections, or if the discipline is under theinfluence of men with an exceptionally welldeveloped taste. but there is agrave danger that the subject will develop along the line of least resistance,that the stream, so far from its source, will separate into a multitude ofinsignificant branches, and that the discipline will become a disorganizedmass of details and complexities . . . .[w]henever this stage is reached, the only remedy seems to me to bethe rejuvenating return to the source: the reinjection of more or less directlyempirical ideas. i am convinced that this was a necessary condition toconserve the freshness and the vitality of the subject and that this willremain so in the future. (john von neumann, "the mathematician" in theworks of the mind, edited by r.b. heywood, university of chicago press,1947, pp. 180œ196.)similarly, cs&e has its own inner logic. but cs&e cuts off potentiallyinteresting areas of inquiry if it chooses to avoid the computational problemsof other disciplines. cs&e, too, must learn to balance these diverseinfluences, on the one hand developing the science base for computing andsystems design and on the other hand responding to outside challengesand technological developments.looking to the future of cs&e57computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.a second consideration is that regardless of whether computer scientists andengineers participate, computing will continue its march into the various sectorsof science, engineering, commerce, and industry. but as argued in chapter 1, thefuture will belong to those who understand best how to apply new computingtechnologies to an ever wider range of problem domains; computer scientists andengineers are ideally situated both to create these technologies and to understandand articulate the appropriate application of these technologies to other domains.indeed, specialists in other areas are often unable to articulate the computingaspects of the problem they want solved. if cs&e professionals remainuninvolved with other areas, the application of computing to those areas willmost likely not reflect the most current or most relevant work that cs&e has tooffer.the pace as well as direction of the information revolution will also beaffected by the participation of computer scientists and engineers. developmentsthat may occur decades in the future without their participation may be only yearsaway with it. the committee believes that dramatic improvements in computingefficiency and performance will be possible only with the full participation ofcomputer scientists and engineers.the third consideration is one of recognizing social responsibility. asrobert m. white, president of the national academy of engineering, has argued,investments in research and development have to have an economic, social, ordefense payback. science and engineering research, like any other [federallyfunded] activity in this country, has a social purpose, and it must justifyexpenditures in ways that can be understood and lead to the social and economicbetterment of the country.1given the growing ubiquity of computing in all sectors of society and theintimate connection between computing and cs&e, research in cs&e among allthe science and engineering disciplines has a particularly powerful justificationwith respect to social payback.the fourth consideration is that cs&e itself may contribute importantintellectual abstractions to other fields. such contributions may be serendipitous,but when these applications do occur, their intellectual reach is often quitecompelling. consider the following: the study of chaos, fractals, and dynamical systems. while work in thisarea goes back to the late 1800s (the days of poincaré), moderncomputation has rejuvenated this work and underscored its importance.many of today's insights into chaotic phenomena are the dilooking to the future of cs&e58computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.rect result of extensive computational experimentation with dynamicalsystems and are often displayed in graphical form. a computer can beused essentially as a laboratory for experimental mathematics; as aresult, computergenerated visualizations of chaotic phenomena at everhigher resolutions have led to conjectures about their properties, whichcan then be addressed in a mathematically rigorous fashion.2 cognitive psychology. the conceptualization of the human brain as acomputational information processor, perhaps operating in parallel, hasemerged as an important paradigm for the investigation of humancognitive processes. a computational model allowsšindeed requiresšresearchers in cognitive psychology to formulate explicit and testablemodels of cognition. the study of algorithms in mathematics. the study of algorithms andcomputational complexity (i.e., the complexity of mathematicalprocesses) has added completely new chapters to mathematical research.the classification by computer scientists of computational problems intolarge classes of problems of equivalent complexity (e.g., p, np,pspace, exptime) has led to new insights in game theory, logic, andrecursive function theory. for example, the study of complexity hasresulted in the systematic study of resourcebounded strategy selectionas a part of game theory. driven by the computer, the study of logic hasalso evolved from an emphasis on the foundations of mathematics to thedesign and study of effective, easytouse proof systems for use in theverification of programs and communication protocols. city and building planning. cities become more congested as theybecome larger, and they are most severely congested near the center.theoretical analysis of the wiring of chips and circuit boards (analysisthat computer scientists and engineers pioneered) helps to explain whycongestion within cities occurs in this fashion and has influenced theplanning of cities, factories, and office buildings.in each of these cases, intellectual insights have been gained not just byusing a computer to perform some calculation more rapidly, but by understandinghow the abstractions of cs&e might be relevant to some conceptual frameworkin another area of inquiry.lastly, a broadening of cs&e speaks to economic realities faced by thefield. as discussed in chapter 1, the computer industry is undergoing a majorshift, from selling thousands of milliondollar computer systems to millions ofthousanddollar systems. the massmarket nature of today's business calls forrelatively fewer people who build computer technology (hardware or systemssoftware) andlooking to the future of cs&e59computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.relatively more people who know what to do with computers (e.g., writeapplications software or integrate complex systems for specific tasks).3 theimportance of domainspecific knowledge relative to programming skills hasincreased, partly because new tools make programming much easier to learn anddo (although this may change if new computing systems such as parallelprocessors require new programming paradigms), and partly because knowing afield (e.g., accounting) is often harder and more relevant than knowing aprogramming language.cs&e researchers also face economic concerns. research budgets for allscience and engineering will come under increasing pressure in the future, anddespite the hpcc program, cs&e is no exception. a broader research agendafor cs&e will enable cs&e researchers to make a better case for receivingsupport from nontraditional sources.4 a relevant point of information is that over42 percent of the entire federal science and engineering research budget (i.e.,over $10 billion out of the total $24 billion) for fy 1991 was obligated by 12federal agencies whose individual science and engineering research budgets eachallocated less than 1 percent to computer science research.5an action plan to develop a broader agenda for cs&e that recognizes theconfidence, strength, maturity, and social obligation of the field calls for thecs&e community to broaden its research scope by expanding intellectualinteraction with science, engineering, industry, and commerce, and to broadenundergraduate and graduate education in cs&e accordingly. (box 2.3 gives theview of the association for computing machinery (acm) on the need to broadenthe cs&e agenda.) concomitantly, other fields will need to develop somefamiliarity with modern cs&e if they are to maximize the benefits thatcomputing can bring to them; this need for other fields to broaden toward cs&eis discussed further in chapter 4.a broader agenda for cs&e in research and education is elaborated in thesections "research opportunities in broadening" and "broadening educationalhorizons in cs&e." the section immediately below provides some historicalperspective and context for understanding the relationship between cs&e andother fields.a historical perspectivechapter 1 described the impact of computing in all aspects of society andexplained the important role cs&e plays in computing practice. increasingly,fields such as computational medicine and computational physics are emerging assubdisciplines of their parentlooking to the future of cs&e60computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.fieldsšindeed, for every field x, it sometimes seems that someone creates asubfield, computational x. cooperation and interconnection of cs&e with thesecomputational subdisciplines should be a major aspect of computing, as suggestedin figure 2.1.box 2.3 the acm's view on broadening"we say that computer scientists, who are at the heart of the computingprofession, must therefore embrace all applications, including commercialapplications and computational science. if computer scientists do not dothis, business people and physical scientists will turn elsewhere for the helpthey need. we hardly need point out that, in this case, computer scientistswould effectively isolate themselves from the computing profession . . ."a close interaction between computer researchers and others isessential so that the questions under investigation remain connected toreal concerns. otherwise computing research can drift into irrelevance andcease to earn public support. for this reason it is in the best interests of thecomputing profession for computer researchers to engage withapplications."source: association for computing machinery, "the scope anddirections of computer science: computing, applications, andcomputational science," communications of the acm, volume 34(10),october 1991, p. 131.in the past, however, cs&e has been slow to participate directly in theresearch and development of these computational fields. this is understandable.even though cs&e was initially populated mainly by people from otherdisciplines,6 a natural tendency was to concentrate on the development of thescientific base in core areas of cs&e. there were more than enough excitingproblems in this core to keep the relatively small number of researchers busywithout worrying about applications in other disciplines, and a lack of incentivesto pursue interdisciplinary work kept most researchers working in the core areas.there have been a few instances of interdisciplinary work. for example,computer science at the university of michigan was closely allied with medicineand psychology, at the georgia institute of technology with library science. theuniversity of north carolina has had medical imaging and molecular graphicsprojects for many years. stanford university was a pioneer in the application ofartificial inlooking to the future of cs&e61computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.telligence to medicine. and from the beginning, numerical analysis wasconsidered part of computer science in many departmentsšmany of thesenumerical analysts are now beginning to call themselves computational scientistsand are playing a major role in computational science. but by and large, the verynature of cs&e and its growing pains forced the field to look inward.figure 2.1 computer science and engineering, computing, and other problemdomains. cs&e is central to computing, which in turn affects many problemdomains.a striking example of this inwardlooking tendency today is the attitude ofthe academic cs&e community toward the general business community. boththe number of commercial users of computers and the dollar value of computersused for commercial purposes far exceed the analogous quantities for academicscience, and yet, apart from a few in the database community, academic cs&eresearchers have been extraordinarily reluctant to engage the problems faced bybusiness and commerce (although they do contribute to and benefit from theactivities of businesses that produce computerrelated products).a simple illustration can be found in the divergent attitudes toward theprogramming language cobol. among those involved in advancing the field,cobol is derided as 30yearold technology, anlooking to the future of cs&e62computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.anachronism. but cobol is the language in which the vast majority of businessand commercial programs have been written and are supported. a second point isthat for the last 25 years, the need to solve computationintensive scientific andengineering problems rather than business problems has motivated the design ofever faster processors. finally, during its deliberations the committee foundrelatively few academic computer scientists or engineers with research intereststhat arise directly from the needs of the commercial domain. this importantaspect of the field has generally been left to business schools, library schools, anddepartments of operations research and manufacturing. as a result, themainstream academic cs&e community has not participated much in thedevelopment of the many computing innovations that have transformed themodern corporation and the practice of business today.the inwardlooking attitude of cs&e manifests itself to a lesser (thoughstill substantial) degree with respect to other applications as well. althoughincreasing numbers of computer scientists and engineers have research interestsrelevant to other scientific and engineering problems, the cs&e community stillviews with some apprehension efforts to promote collaborations with otherdisciplines. for example, a recent cstb workshop intended to bring togetheryoung computer scientists and engineers with molecular biologists in need ofsophisticated computational systems elicited some concerns that pursuing suchchallenges would be inimical to progress in the academic cs&e environment.the relevance and value of such work from a cs&e perspective are not widelyrecognized, and promotion opportunities for computer scientists and engineerswho choose to work in this interdisciplinary area could thus be damaged.7conversely, various disciplines have likewise been mistrustful of cs&e andhave not known whether to embrace cs&e as a real discipline. wasn't computerscience just programming? was it really a science? consider, for example, thefollowing quotation, taken from a recent national research council report onphysics:8. . . computer programming introduces problems . . . . [f]or the computationaltheorist the programming problems have led to special difficulties, including agreat deal of misunderstanding and underestimation of the role and intellectualquality of computational physics. computer programming and debugging is, inlarge part, a minddulling, menial task, in which hours and days and weeks arespent making trivial changes in response to trivial errors or figuring out how toformat the output. yet one must be able at any moment to apply the deepestanalytical skills in order to understand an unexpected result or to track down asubtle bug. [emphasis added.]looking to the future of cs&e63computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.although the statement does acknowledge the intellectual challenges ofdebugging programs, it fails to do justice to the wealth of knowledge and talentneeded to construct correct programs in the first place. indeed, it suggests thatknowledge of a programming language's syntax and the ability to perform lowlevel coding are all that a scientific programmer needs, whereas in factknowledge of data structures and algorithms is the key to effective programming,and the structured decomposition of a problem and the stepwise refinement ofproposed solutions account for the largest portion of serious programmingefforts. even more problematically, it implies that the only function a programmust serve is to solve a given problem. such a view is overly narrow, because itdoes not recognize that problems evolve, that therefore programs must evolve,and that cs&e is responsible for most of the tools and concepts needed to writeevolvable programs. put another way, it is understandable if physicists do notfully comprehend the intellectual challenges required to create the tools they useso freely. but rejection of those challenges as irrelevant to the business at handmay well discourage the intellectual work necessary to develop better tools.beginning around 1986, cs&e as a field began to recognize the importanceof interdisciplinary research and broadening. for example, interdisciplinaryresearch became an issue at the biannual meetings of the chairs of ph.d.grantingcomputer science departments as early as 1986. the hpcc program, with itsinterdisciplinary orientation, had its roots in various planning meetings held in1986. senior officials in nsf's computer and information sciences andengineering directorate in the late 1980s were important advocates forinterdisciplinary work. concerns about the insularity of the field were raised atthe acmcra conference on strategic directions in 19899 and at the 1988snowbird meeting.10 in response to an inquiry from the committee, the acmargued for a cs&e agenda that was broader and more closely linked to socialneeds.11 today, one can find many morešthough still not substantialšinstancesof cs&e faculty members taking part in interdisciplinary work.at present, cs&e is in transition: many computer scientists and engineersare aware of its previous isolation and the need for a broader agenda, but the fieldas a whole has not yet taken sufficient action to remedy the problem or to changeits culture.research opportunities in broadeningone simple principle should guide the formulation of a broader researchagenda:looking to the future of cs&e64computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.address substantive research problems in cs&e in the context oftheir application in and relevance to other problem domains, and derive inspiration for identifying and solving these research problems from theseother domains.by so doing, cs&e can be framed simultaneously as a discipline with itsown deep intellectual traditions, as well as one that is applicable to other problemdomains. cs&e can thus be an engine of progress and conceptual change in theseother domains, even as they contribute to the identification of new areas ofinquiry within cs&e.12in developing this notion further, it is useful to consider the traditionaldistinctions between basic research (conducted to obtain a fundamentalunderstanding of some phenomenon), applied research (done to investigate thenuances of this phenomenon with an application area in mind and perhaps toconstruct proofofprinciple prototypes), and development (which builds onresearchbased understanding to construct engineering prototypes thatdemonstrate economic and manufacturing feasibility and results in items that arevery close to marketable products).13 this neat and orderly progression describesthe evolution of some products, but it often happens that in the course of bringing aproduct to market, it is not clear when a given activity fits into one of thesecategories. indeed, some products have bypassed the traditional developmentphase, going directly from research to use as the core of a new application.although such products generally have not met the usual standards of qualityexpected of more traditionally developed software products, they have establishedmarkets for the services provided by those products. in turn, these markets havethen driven further improvement of those products. examples include the machkernel for operating systems, the scribe text formatter, the emacs text editor, theingres relational database system, the magic cad system, the querybyexample database system, and the unix operating system, all of which were firstdeveloped in a research environment and widely distributed initially at little or nocost. such phenomena persuade the committee that the separation of basicresearch, applied research, and development is dubious, especially within cs&e.given the way research in cs&e is actually done, distinctions between basic andapplied research are especially artificial, since both call for the exercise of thesame scientific and engineering judgment, creativity, skill, and talent.14 althoughthe traditional areas of cs&e research (e.g., those discussed in chapter 3) remainat the core of cs&e research and still present major and substantive intellectualchallenges worthy of sustained effort, they should not alone define the boundariesof the cs&e research agenda.looking to the future of cs&e65computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.rather than a onedimensional characterization of research leading todevelopment, a twodimensional model may be more appropriate (figure 2.2).the committee believes that research is any investigative activity that results inthe creation of new knowledge (i.e., represented in the upper half of figure 2.2),whether or not that activity is associated with a specific product item (i.e.,irrespective of its horizontal coordinate). thus research might well be an aspectof trying to improve the manufacturing or maintenance or upgrading of a specificproduct. academics, who are generally free to choose their areas of researchwithout constraint, should be encouraged to select problems that involvecommercial products as long as significant new knowledge is created anddemonstrable intellectual achievement is the result.as computer scientists and engineers engage research problems that arise inother problem domains, the center of gravity of traditional cs&e research mayshift. for example, a great deal of research in cs&e is now devoted to increasingthe speed of computafigure 2.2 a twodimensional characterization for research and development.the vertical axis refers to the extent to which a given activity results in thecreation of new knowledge, while the horizontal axis refers to the extent towhich that activity is oriented toward a specific deliverable product intended forcommercial sale (usually with associated deadlines).looking to the future of cs&e66computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.tion (i.e., developing faster and faster processors). although researchers currentlyworking on problems in this domain may well continue to proceed as they alwayshave, it may not be surprising to see in addition a larger effort in other areas ofcs&e that, though always considered ''legitimate" cs&e, have not always beenwell represented in the discipline (e.g., design of faster and more capable input/output and storage technologies and of better user interfaces).academic researchers often equate "basic" research with investigatorinitiated research, and "applied" research with funderinitiated research. but eventhis distinction is not as clear today as it once might have been. sponsors in thepast may have been able to support all proposals for good science regardless ofspecific area or topic, although this can be debated. but it is clear today thatfunding sponsors are more selective about the directions in which they wish tofocus their efforts, and they find willing allies in the many researchers whosubmit grant proposals for "basic" research in sponsorpreferred areas of interest.framed as it is in the context of grand challenges in science andengineering, the hpcc program is a good start toward a broader cs&e researchagenda. but other sets of grand challenges can be imagined for differentendeavors of social significance. for example, grand challenges relevant tobusiness could include translating telephones that allow a russian and anamerican to converse without difficulty15 or copiers that reproduce a documentand automatically generate a summary of key points in the document. grandchallenges relevant to medicine might include a "physician's assistant" (box 2.4)with online access to patient data, physician's orders, and laboratory results thatcould monitor patients to provide status reports and alert the physician toimportant events, or an integrated medical information system that would giveclinical practitioners convenient and flexible access to comprehensive, accurate,current medical information.there is no shortage of problem domains outside cs&e in whichchallenging and intellectually substantive cs&e research problems arise. indeed,some research areas have developed directly in response to challengingproblems, such as speech input or physical modeling and simulation. these areasrequire substantial interactions between cs&e researchers and those in otherfields and often have a strong experimental component. almost inevitably, it isresearch that addresses specific and concrete problems that captures the public'sattention, since it is most easily understood by the lay public and also influencesthe public's perception of the benefits of cs&e research.looking to the future of cs&e67computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.box 2.4 a physician's assistant for intensive caremonitoring"an 'intelligent agent' for intensivecare (ic) monitoring would possesscapabilities for continuous sensing, interpretation, summarization, andprediction of a range of patient data; construction, refinement, revision, andimplementation of shortterm and longterm therapy plans; detection,diagnosis, and correction of immediate disease conditions and otherproblems; control of designated patientmanagement parameters, such asdevice settings, drips, etc.; recommendation of a broader range ofdiagnostic and therapeutic actions to human members of the ic team;explanation of observations, diagnoses, predictions, and therapies based onthe underlying anatomy and physiology; and patient presentation andquestion answering."to perform these tasks, the agent would integrate diverse knowledge(e.g., clinical knowledge of common problems, symptoms, and treatments;biological knowledge of anatomy, physiology, and pathophysiology; andknowledge of fundamental physical models and fault conditions) anddiverse reasoning skills (e.g., diagnosis, prediction, planning), . . . improveand extend its knowledge and reasoning skills based on clinicalexperience, . . . function continuously, allocating its own limited resources(e.g., data, knowledge, computation, time) among competing tasks to meetrealtime constraints on the utility of its conclusions and behavior. itsactivities would be coordinated under a unified perspective on the patient'soverall condition and its own goals and position within the ic team."the intelligent icumonitoring agent would be continually present inthe ic environment, possess extreme vigilance, have a broader scope ofrelevant knowledge and expertise (compared to the division of responsibilityand specialization of knowledge among medical professionals), and bethorough in its reasoning. thus, the agent could summarize the patient'sprogress and condition for physicians on rounds; alert clinicians toimminent problems before they might otherwise be noticed; suggest andcritique alternative therapies; consult with nurses to determine whether tocall physicians in 'borderline' situations; 'stand in' for human team memberswho happen not to be present when their expertise is needed; and explainthe physiology underlying a patient's condition to medical students."source: quoted, with a few adaptations, from barbara hayesroth,richard washington, david ash, rattikorn hewett, anne collinot, angelvina, and adam seiver, guardian: a prototype intelligent agent for intensivecare monitoring, knowledge systems laboratory report ksl9142, department of computer science, stanford university, stanford,california, june 1991, p. 2.looking to the future of cs&e68computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.a broader research agendasome illustrationsto suggest what a broader research agenda might entail for cs&e, fourtopics are discussed below to illustrate cs&e research problems that arise inembracing interdisciplinary and applicationsoriented work.16 note that thesedescriptions are intended to be illustrative rather than explicative of priorities forsuch work.earth sciences and the environmentamong the great challenges of computing is modeling the earth system,including the climate, hydrologic cycle, ocean circulation, growth of thebiosphere, and gas exchange between the atmosphere and the terrestrial andoceanic biota. in this complex physical system are a multitude of phenomena thatchange on local, regional, and global scales. detailed scientific models describeprocesses whose temporal and spatial scales differ widely. the data that drive andverify these models come from satellite and groundbased sensors. by the year2000, these sensors will have the spectral and spatial coverage and resolutionneeded to provide data to support accurate modeling and analysis by scientistsand informed decisions by policy makers and legislators.the sensors and their associated scientific data products will generate nearlya petabyte (1015 bytes) of data each year, and these data will be integrated withlocal measurements of fluxes of water, energy, and chemical species. (for scale,note that today, terabyte (1012 bytes) databases are regarded as very large; anordinary book is a few megabytes of textual information.) improvements areneeded in information management systems for these data, along with techniquesfor their analysis, distribution, fourdimensional assimilation, and incorporationinto models.construction and operation of valid scientific models that describe andpredict the dynamics and processes of the earth system will requireinterdisciplinary teams of experts from geophysical, biological, and computersciences and engineering. needed are improvements in our understanding of howprocesses at different spatial and temporal scales interact, substantially morecalculational power, better methods of accessing and storing large volumes ofheterogeneous data of varying structures in distributed archives, and new ways oftranslating scientific ideas more rapidly and more reliably into working computercode. the result of this collaborative theoretical, experimental, and computingeffort will be a much deeper understanding of the earth system as modified byhuman activities.looking to the future of cs&e69computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.the contributions needed from cs&e are fast, reliable networks that allowexamination of large data sets on remote computers; algorithms that can beemployed by fast computers, probably with parallel architectures; and improvedtools for understanding and managing staggering volumes of information.moreover, the crucial dependence of research in the earth sciences on largescalecomputer models of phenomena and processes suggests that cs&e expertise has ameaningful role to play in such work.consider, for example, the historically important concept of repeatability. inthe past, a scientist could often read a paper describing an experiment and redothe experiment, therefore verifying its correctness or identifying an earlier resultas erroneous. but earth scientists whose work depends on largescale computermodels do not have this ability. it is generally not feasible for someone to read afew papers about predictions of a climate model, study the equations, write his orher own model, and subject the model and the conclusions to the necessaryscrutiny; as a result, careful study is rarely given to modeling software. evenwhen every line of source code is made available, a comprehensive understandingof someone else's model may not be achievable, due to possible interactionsbetween different parts of the model. the "same" model run on a differentcomputer may give different results. as a result, the concept of repeatability is indanger of being lost.thus collaboration between cs&e and the earth sciences will be necessary;box 2.5 describes one example of such an interaction.an example of a largescale problem of scientific and social interest thatdepends on advances in computing is nasa's earth observing system (eos).global environmental change has become a toppriority issue in the publicdebate. investigation of the causes and magnitudes of environmental change,especially at large regional and global scales, depends crucially on large data setscontaining geophysical and biological information that are reliable enough toenable the detection of subtle changes. the computing challenge is the creationand integration of these data sets into a system for analysis.the u.s. global change research program was launched in 1989 inresponse to mounting national and international concerns about the globalenvironment. the objectives of the program are to monitor, understand, andpredict environmental change on a global scale. the program calls for earthprobesšsatellite sensors dedicated to nearterm observations of specificphenomenašto be launched in the next few years. beginning in 1998, eos willbe put into place and will collect data for 15 years. eos will provide a muchmore capable spacebased observing system, the eos data and informationsystem (eosdis), and a scientific research program.looking to the future of cs&e70computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.box 2.5 collaboration between the earth sciencesand cs&eglobal change researchers at the university of california havediscovered that serious problems in the data systems available to themimpede their ability to access needed data and thereby do research. theyrequire a massive amount of information to be effectively organized in anelectronic repository. they also require ad hoc collections of information tobe quickly accessed and transported to their workstations for visualization.the hardware, file system, database management system, networking, andvisualization solutions currently available are totally inadequate to supportthe needs of this community.the sequoia 2000 project, named after the longlived trees of thesierra nevada, is a partnership centered at the university of california toaddress issues of global change.one element of the partnership is a technical team, primarily computerscientists from the berkeley campus. they will attack a specific set ofresearch issues based on the above problems as well as build prototypeinformation systems. another element is a collection of global changeresearchers, primarily from the santa barbara, los angeles, and san diegocampuses, whose investigations have substantial requirements for datastorage and access. these researchers will serve as users of the prototypesystems and will provide feedback and guidance to the technical team. athird element of the partnership is a collection of public agencies that mustimplement policies affected by new understanding of global change.a fourth important element of the partnership includes various firmsfrom the computer industry. the digital equipment corporation is theprincipal funder of the project, and will provide extensive hardware supportand key research participants. metrum, hewlettpackard, trw, the hughesaircraft company, and saic are among the industrial participants and willserve as a sounding board for ideas and participate in technology transfer.it is notable that the sequoia partnership is a direct result of an earthscientist and a computer scientist discussing the broadening of cs&eduring meetings of the committee that produced this report.investigating the causes and magnitudes of environmental change onregional and global scales requires large volumes of biological and geophysicaldata, several terabytes per day over a lifetime of 10 to 20 years. eosdis isintended to manage this data, facilitating inexpensive, quick, and convenientaccess, integrating these data in a reliable manner, and promoting the interactionof scientists from alooking to the future of cs&e71computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.broad variety of disciplines and living all over the world. several aspects of theu.s. global change research program present major challenges to computerscience and engineering: large volumes of data require methods for transmitting and storing largedata sets without loss of information, browsing these data sets quickly toidentify interesting features and characteristics, displaying andorganizing these large data sets in meaningful ways, and selectingrepresentative data. to understand the volume of data involved, considerthat a common technique for presenting data organized by spatialposition is to display an image. one hundred megabytes of spatial datacan be reduced to a color image that can be taken in by the human eye in asingle look; for comparison, a single typewritten page holds 2,000 bytes.when the amount of data produced is 30,000 such images every day(many more imageequivalents will result from recombinations of thedata derived by scientific analysis), even visual representations are likelyto be inadequate. automated vision and pattern recognition algorithmsmay well be necessary to allow comprehension of these large amountsof data. information must be distributed reliably among an internationalcommunity of users from different disciplinary traditions. eosdis willhave to validate data and prevent data corruption. since researchers willhave to rely on networks to provide access, eosdis will have tosurmount barriers between different networks and machines. the character of the scientific problem is such that small differences inthe data or results can have large policy implications. therefore,accuracy has a high premium. this requirement drives the need for largeamounts of data and for sophisticated algorithms and models that canmake meaningful predictions and identify longterm trends in a noisyenvironment. the development of such algorithms and modelsnecessarily involves interdisciplinary expertise.the preceding discussion suggests how a tough scientific problem requiressolving generic and genuinely challenging cs&e problems. progress will dependon more scientists knowing something about cs&e as a research discipline, andcomputer scientists and engineers knowing something about the scientific andtechnical problems in other disciplines.17computational biologycomputational methods have a long history of application to problems in thephysical sciences and engineering. however, during the pastlooking to the future of cs&e72computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.decade biomedical research and technology have seen a comparable influx ofcomputational methods. three examples are widespread use of computer imagingtechniques in medical diagnosis, computational methods for drug design andstructure refinement in molecular biology and medicine, and computationalneural science, which attempts to understand the principles of development andfunctional cooperation of the neurons in the brain, using computer simulations as amain tool.the primary driving force behind the proliferation of computationaltechniques in biology and medicine has been the rapid development of computingtechnology, which has become increasingly less expensive and better adapted tothe complex dataprocessing tasks required in these fields.box 2.6 describes one example of a biological problem solved by applying agood algorithm developed by a computer scientist. however, the availablecomputational power and the algorithms for computational biology are in manycases still inadequate.but cs&e advances expected in the next decade will benefit molecular andstructural biology. research in these areas currently relies on computersimulations of the structure and dynamics of biopolymers, e.g., proteins anddna. today, simulations are possible only for small biopolymers of a fewthousand atoms and over very short time periods (109 seconds); moreover, thesesimulations treat proteins mostly as systems of classical particles. a simulation of106 seconds, much more useful but still relatively short on the time scale of manyinteresting biochemical reactions, would require about 100 years on a cray2processor.in actual biological systems, proteins never function in isolation. rather, aprotein is typically surrounded by a membrane (itself a simpler biopolymer)around which is an aqueous environment. such a configuration typically involves105 atoms, or about 30 times the number of atoms in a single protein molecule.the time required to perform a numerical simulation of behavior at the molecularlevel increases with the square of the number of atoms involved (since all pairwise atomic interactions must be computed), and so a simulation of a protein inits natural environment takes on the order of 1000 times as long as that for aprotein in isolation.the computational power available to biologists is increasing by orders ofmagnitude because of faster hardware such as massively parallel processors andbetter algorithms (e.g., multiple time scale and cellular multipole methods). theresult is that larger, longer, and more detailed simulations are becoming possible.for example, more computational power may permit the correct quantummechanicallooking to the future of cs&e73computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.simulation of biopolymer behavior. long time simulations (i.e., simulations of asecond or so of behavior) together with advanced algorithms for predictingstructure may finally enable the prediction of protein structures from their aminoacid sequence. simulations of large molecular assemblies will advance therational design of newbox 2.6 collaboration between biology and cs&eedward lewis and k. mani chandy, respectively professors of biologyand computer science at caltech, were discussing a computing problemthat came up in lewis's research in genetics. lewis was looking formatches of certain "motifs," short sequences of bases (i.e, 12 bases orfewer), within sequences of up to a million dna bases. the matches thatwere to be searched for in the usual agct alphabet of dna were notnecessarily exact. for example, a g might be replaced by an a, but not by ac or a t. another possibility was that a base might be missing from thesequenced dna altogether, due either to a failure in sequencing the dnaor to the gene being transcribed incorrectly. lewis needed a program thatwould look for multiple occurrences of motifs that were not too far apart.the ideal algorithm for this problem would run in a time proportional tothe length of the dna sequence and would be capable of running withlinear speedup on parallel machines. the run times of the most obvioussearch algorithms grow at a rate proportional to the square of the length ofthe dna sequence, and the biologists were indeed using these algorithms.however, chandy was very familiar with the lineartime algorithmsdeveloped by knuth, morris, and pratt (the kmp algorithm) and by boyerand moore. it was initially an afternoon's work for chandy to adapt thesealgorithms to inexact matches and to write a program that solved lewis'sproblem. later, interactions between the biologists and computer scientistsled to changes in the original program that made it even more useful.eventually, parallel versions of the motifmatching program were writtenthat operated by distributing the dna sequence so that the matching on thesubsequences could take place concurrently.to the biologist, it was the wallclock time that mattered, rather than theelegance of the algorithm and program. however, employing lineartimerather than quadratictime algorithms for this problem was not merely anaesthetic advantage; searches that would have taken months were possiblenow in minutes.source: chuck seitz, california institute of technology.looking to the future of cs&e74computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.drugs and allow the pharmaceutical industry to speed up development processesthat today cost many millions of dollars.another field that will gain from cs&e advances is neurobiology.neurobiologists are concerned with describing and understanding brain activityat the neural level. an example is the problem of how neural activity encodesvisual images in a brain area called the visual cortex. this area actually providesmultiple encodings of any image that is seen. one such encoding of an image, asobserved through socalled voltagesensitive dyes in a monkey brain, involvesabout 106 neurons with 109 indirect synaptic connections to the retinas of theeyes that develop during the first few months of the monkey's life. massivelyparallel computers are very well suited to simulating this development processand may make possible simulations of large networks in which neurons aremodeled as nonlinear, dynamical, spiking units. these simulations may shedlight on the hotly debated question of how temporal relationships between spikescontribute to information processing in the brain.further opportunities for advances in structural biology throughcomputational methods arise in connection with the use of two and threedimensional nuclear magnetic resonance (nmr) spectra for protein structureanalysis. such spectra yield information on the interatomic distances between thelarge number of atoms in a biopolymer. together with information regarding thenative forces acting between these atoms (today not well known), knowledgeabout the interatomic distance constraints permits the determination of proteinstructure. it is expected that in the next decade the structure of many biopolymerswill be obtained through this technique, which relies both on nmr measurementsand on advanced computation.diagnostic techniques such as magnetic resonance imaging are based onphysical processes that need to be better understood if the diagnostic method is toachieve resolution on the scale of a single biological cell. a highlevel ofunderstanding is reached if the measuring process for a sample can be simulatedin its entirety, a task that requires monitoring the nuclear spin precession ofmillions of diffusing water molecules over many precession periods of theirnuclear spins. again, such simulation currently requires many days on today'sfastest computers. even faster hardware and algorithms may allow brieferimaging periods and images with more detail.a final example of a rapidly developing role for cs&e in the life sciencesinvolves biological databases. reliable network access needs to be provided anduse of computational resources promoted through tutorial documentation andworkshops. cs&e researchers, through collaborations with biologists, shouldprovide better opportunitieslooking to the future of cs&e75computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.for information "mining" (i.e., examining the data in search of unexpected orunanticipated relationships). new opportunities would arise if the structural andsequence databases were maintained at one location, allowing the furtherdevelopment of existing tools (e.g., gelreader (a creation of the national centerfor supercomputing applications), genworks, gcg, and intelligenetics) toinclude crossreferencing and crosslinking of features between the databases.it is expected that geneticists and structural biologists working with thehuman genome project will consult computerbased databases much morefrequently than today for data analysisšfor example to identify geneticdisorders. this data analysis will require distributed computing in which thecomputingintensive tasks are performed on a supercomputer and the interface ishandled on a graphics workstation, where highspeed rendering and digital videowill be indispensable. supporting the necessary distributed computingenvironment will be software, such as the data transfer mechanism (dtm)software developed at the national center for supercomputing applications, thatallows data exchange between a wide range of computers in a machineindependent manner.the preceding possibilities for computational biology will depend on theavailability of advanced computer technology, including very large massivelyparallel computers deployed at the national supercomputer centers and thenational laboratories, smaller models of scalable parallel computers operating atmany sites for program development and testing, concurrent computationexploited across networks of workstations, and new visualization techniques(perhaps making use of digital video) for data postprocessing and interactivecomputation).commercial computingas mentioned above, academic cs&e has often kept the commercial andbusiness world at arm's lengthšin part because academic computer scientists andengineers tend to focus on the creation of the science, whereas business peopletend to be interested in lowrisk approaches that emphasize the best practice withcurrently available technology. but it is important to realize that in their quest toexploit business opportunities, industry and commerce (and often governmentagencies as well) are another rich source of intellectually challenging problems.manufacturing and service firms have driven the demand for computeraideddesign, online transaction processing systems, and specialized portableinformation appliances; today, american business is exploring the use of verysophisticated computing technology.18 by directly addressing the informationdemands of thelooking to the future of cs&e76computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.business and commercial environment, academic cs&e research can beinvigorated by new and demanding challenges, at the same time makingcontributions that improve the wellbeing of our society.as in the case of scientific computing, it is important to distinguish betweenthe relatively routine uses of computers in organizations (e.g., spreadsheets andword processors on personal computers, large accounting programs or inventorycontrol systems on mainframes) and the uses of computers that extend the state ofthe art. however, where the challenges of largescale scientific computing centeron the need to perform huge numbers of floatingpoint calculations and to displayhuge amounts of data in comprehensible form, the challenges of largescalecommercial computing arise from the need to: process and store huge amounts of data, often with relatively littleprocessing for each piece of data. in many ways, commercial computingis limited by the speed of input and output rather than by the speed ofcomputation itself. use computer systems with very high reliability and availability. faulttolerant systems, first used in lifecritical applications (e.g., realtimeflight control), have been spreading to applications such as banking,where the cost of down time is measured in megadollars rather thanlives. eliminate the deleterious effects of work dispersed across organizationalboundaries. in the course of ordinary business, workers must ofteninteract with people who work in other locations. in addition, workstyles and procedures may be different. computing technology (e.g., email) to promote and facilitate interaction is becoming more common.the commercial environment today is characterized by globalization andworldwide competition, severe time and productivity pressures, and a rapidlychanging and thus unpredictable business environment. computers often play animportant and even enabling role in managing this fastpaced environment;nevertheless, the need to respond rapidly will make even greater demands oncomputing technology to be easily and quickly adaptable to new circumstances.some important problem areas and promising research directions for cs&ewith respect to business and commercial computing are highlighted below.another major and relevant area, ''better" software engineering, is discussed inchapter 3.model management and decision supportmodeling is an essential tool of modern business. companies makedecisions on the basis of likely or expected outcomes of possilooking to the future of cs&e77computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.ble courses of action, and computer models are more and more frequentlyessential to forecasting these outcomes. at present, models generate numberswithout explicit articulation of the underlying premises. through great manualeffort, the premises can be coupled to model output, but the farther one gets awayfrom the original model (e.g., as one model feeds another one that then feeds athird one, and so on), the more likely it is that the premises will be lost. usersneed the ability to inquire easily about the assumptions that underpin the analysisat any level, and to change these assumptions to test various scenarios. thus toolsthat facilitate convenient model creation and management (e.g., by makingassumptions obvious and explicit and easy to change at any level in a chain ofmodels) would contribute a great deal to effective decisionsupport systems.easily usable softwareincreasingly cheaper computer systems have led to the proliferation ofinformation technology into many offices. with such proliferation has comegreater access, by people (from chief executive officers to beginning typists) whocannot be assumed to have the willingness or patience to develop seriouscomputer expertise as a prerequisite for using the computer. numerous computerusers feel daily pain, anxiety, and frustration as they struggle with clumsyinterfaces, incomprehensible error messages, and technical details that areirrelevant to what they want to do (figure 2.3). in the words of mitchell kapor(founder of the lotus development corporation and principalfigure 2.3 the potential down side of "greater" functionality. although thetechnology involved in this example is voice mail, the lesson of the exampleapplies at least as well to computing technology. copyright © 1991 by northamerica syndicate, inc. reprinted with special permission of north americansyndicate, inc.looking to the future of cs&e78computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.architect of the lotus 123 spreadsheet), the lack of usability of software and thepoor design of programs are the secret shame of the computer industry.indeed, even as information comes to play a more and more important rolein society, the pervasiveness of difficulttouse computers in offices and homes isincreasingly the factor limiting the widespread use of information technology.attention to design resulting in computers that are almost as easy to use astelephones or fax machines would have an enormous impact on the number ofpeople using computers and on the variety and effectiveness of different businessapplications to which computers can be applied.software development metrics and modelingsoftware development metrics and modeling present vast researchopportunities. to a large extent these topics have "fallen through the cracks"because computer scientists often view these areas as the domain ofmanagement, while management scientists often view them as the domain ofcomputer science. rigorous approaches that address a broad array of softwaredevelopment issues holistically would be a valuable theme for researchinvestigation.technology for interoperationmost companies must interact with other companies, and divisions withinthe same company must interact with each other. although it is in principle easierto impose a single computing structure on all divisions within a company, inpractice it often turns out that intracompany computermediated communicationis nearly as difficult as intercompany communication, which is a major problemin business today. indeed, while the computers of one company can usuallyexchange strings of letters and numbers with the computers of another company,fax rather than computer networking is the standard means of interchange fordocuments involving images, notes, and formatted text. (the writing of thisreport provides a good example of difficulties encountered in this environment;see box 2.7). in fact, document interchange architectures exist that faithfullyrepresent multimedia documents and that will eventually supplant fax, but thesimple example shows the need for highlevel machineindependentrepresentation of information. with databases, processors, and networksseparately installed and maintained in different offices, automated dataconversion, interoperable network protocols, and transportable software systemsare necessary to provide dynamic reconfiglooking to the future of cs&e79computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.uration. research aimed at creating powerful but flexible technology andstandards that facilitate interoperation among heterogeneous computer systemsand convenient electronic data interchange will be a boon to all computer users,but will be especially valuable for business applications.19box 2.7 standards and report writingthe committee that wrote this report consisted of individuals from 14different institutions, each with its own wordprocessing software running ondifferent computer systems. committee members were connected to eachother by electronic mail. committee staff had to assemble material fromcommittee members into a single document, and the wordprocessingsoftware available to staff produced documents that were largelyincompatible with the software used by committee members. plain textcould be transmitted and received, but only without formatting codes suchas those for indentations, underlining, or boldface; thus, with everytransmission, such codes had to be reinserted manually or done without.committee staff used an email package that could not be integrated withtheir wordprocessing software, except by using plain text (and even then,importation of text from email to the wordprocessing software oftenresulted in the loss of information).although in some ways it is amazing that the system worked at all, theknowledge that these difficulties resulted from various design and marketingdecisions rather than technical inadequacies made them all the morefrustrating.although in many cases the lack of interoperability is a problem of choicefor manufacturers that opt for proprietary architectures and data formats, thedevelopment of good standards nevertheless requires technical expertise. forexample, interconnected devices or software conforming to a set of poorlydesigned or inconsistent standards may exhibit unanticipated interactions orbehaviors, possibly as the result of timing problems. as a result, even if standardsare consistent, devices or software conforming to these standards may requireexcessive execution time. design flaws in a given standard may become apparentonly when it fails in some particular implementation. for criticalimplementations, dynamic recovery techniqueslooking to the future of cs&e80computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.may be necessary to restore proper operation in such an event; these techniquesthemselves may become part of the standard.collaborative workcomputers can help to facilitate cooperative work efforts among manyindividuals in geographically dispersed offices, reducing or eliminating thedisruptive effects of distance on the work process. commonly known as"groupware," such computing technology might, for example, provide ways forcollaborative annotation of a single document, facilitate electronic interaction bykeeping track of different threads of discussion in email messages, or supportdecisionmaking processes in large groups. the development of groupwarecustomized to the requirements of individual offices and different work styleswill require careful attention to the social context in which such groupware willbe used (box 2.8).the electronic librarythe dream of the electronic library dates from the very beginning of thecomputer age. imagine accessing at modest cost, from home or work, thecontents of a local library and, in case of need, escalating to grander sources rightup to the library of congress. (perhaps the library of congress would itself bevirtualšthe networked aggregate of all the libraries in the land.) enticingfragments of the dream have been realized, in the areas of cataloguing, storage,and search. many general libraries have placed their catalogues online. regionallibrary consortia with shared remote catalogues have come into existence.documents from selected corporations are accessible electronically fromcommercial services that provide fulltext search capability. bibliographicdatabases exist in profusion. chemical and biochemical information may beretrieved by performing pattern matches on formulae.electronic libraries promise to accommodate information formats that fituncomfortably in traditional libraries, e.g., images, sound, and small documentssuch as letters. all are more manageable in electronic media than in print. andthe electronic library is the natural home of multimedia "documents" andelectronically published journals, which are just beginning to appear in themarketplace.20to achieve the fully electronic library, many imposing technical problemswill have to be solved: acquisition, indexing, storage, retrieval, transport,presentation, and performance. there are challenges enough for almost everybranch of cs&e (box 2.9).looking to the future of cs&e81computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.box 2.8 collaborative computing, or computingthat is userresponsivebuilding applications which directly impact people is very different frombuilding computational products of the sort taught and studied in thestandard courses on computer science. in traditional computer science, oneworries about data structures and coding practices, about ways to ensurethat the program does what it is promised to do, ways of proving that thecode works, ways of synchronizing procedures within complex data andcomputational streams. all this is important and essential, but it has little todo with the issues that concern the program users.computer systems intended to aid people, especially groups of people,must be built to fit the needs of the people. and there is no way that asystem can work well with people, especially collaborative groups, without adeep, fundamental understanding of people and groups. this is not usuallythe sort of skill taught in computer science departments.too often, technology is constructed for its own merits, independent ofthe uses, independent of the user. this works fine when the technology isused to optimize some complex computation, or to discover algorithms ordatastructures. it is not appropriate when the whole point of the technologyis to act as a direct aid to people's work activities. when this is the case,then the technology will succeed only if the people and the activities arevery well understood.more and more, computer science must confront these issues. moreand more, computers are making large, dramatic differences in the life andworkstyles of people. when this happens, it is time to start examiningpeople . . . .what has this to do with computer science? nothing, directly, butindirectly it means a lot. the same computer that makes work from adistance possible also destroys the cultural structure that makes a workunit succeed. and if this is not understood, the systems will fail, and thefailure will be blamed on "the computer" or even on "those computerprogrammers and scientists." . . . .technology alone cannot provide the answers when we deal withhuman activities. the tasks, the culture, the social structure, and theindividual human are all essential components of the job, and unless thecomputational tools fit "seamlessly'' within this structure, the result will befailure.looking to the future of cs&e82computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.computer scientists cannot become social scientists overnight, norshould they . . . . [but] the design of systems for cooperative work requirescooperative design teams, consisting of computer scientists, cognitive andsocial scientists, and representatives from the user community.source: excerpted from d.a. norman, "collaborative computing:collaboration first, computing second," communications of the acm,volume 34(12), december 1991, pp. 88œ90. copyright © 1991, associationfor computing machinery, inc. reprinted by permission.inputeven attending only to printed matter, the electronic librarian is faced withsource materials of two radically different kinds: printed documents and anelectronic source for digital typesetting. the only feasible way to enter printedmaterial is optical scanning, which creates digital page images. but for retrievalother than simple regurgitation of pages by number, one needs digital text.making optical character recognition practical on a library scale requiresadvances in natural language processing, to say nothing of new specialpurposearchitectures for image and pattern recognition.documents originating in electronic form are not an unmixed blessing,either. there are hundreds of distinct encodingsšinput on diverse media fordiverse typesetting systems. extraction of "meanbox 2.9 cs&e problems that arise in developingelectronic libraries databases information retrieval pattern recognition linguistics statistical classification human factors data structures algorithm engineering data compression distributed systems parallel computation reliability file systems networks storage architectureslooking to the future of cs&e83computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.ing," even at the level of a simple stream of text characters, is a daunting task.furthermore, typesetting is not typically fully automatic. title pages, pagenumbers, figures, and proofreading corrections are likely to come from separateplaces, and so there may be no complete electronic version of a document. theprinted version may have to serve as a guide for the reconstruction of a fullelectronic document from partial electronic sources. we thus have the problem ofcorrelation of multiple texts.retrievala document in a library is useful only insofar as information can beextracted from it, either by direct retrieval or by processing. information retrievalsystems usually depend on indexing (manual or automatic) to home in ondocuments, and then perhaps on fulltext scanning to find exact information. thesuitability of various indexing and scanning techniques depends strongly onscale; there is much room for innovation and experiment. at a higher level, thequality of retrieval should be enhanced by "text understanding." still notcommonly used today, statistical methods for analyzing documents are likely tobe the first scalable techniques. (for statistical analysis, the details of languageare unimportant and sample size is a boon, not a bane.) understanding at thelevel of identifying certain formal parts of a text, such as titles and table ofcontents, will be important for indexing purposes.searching, even among indexed documents, on a library scale is a challengefor both architecture and algorithms. and searching for nontextual matteršvisual or audiošis almost virgin ground. the possibility of novel and massivesearch techniques, however, is a prime motivation for developing the electroniclibrary. in a print library, images can be found only by leafing through theholdings.presentationelectronic libraries promise simultaneous availability to all readers, accessat a distance, and easy capture of relevant passages. offsetting these advantagesis the fact that electronic presentation of substantial amounts of static informationis rarely as satisfying as print, either for browsing or serious reading. thatjudgment may be altered by the advent of new modalities, such as hypertext,21for navigating documents. one thing is certain: the availability of large bodies oftext for experimentation will stimulate creative new ways to present and interactwith the documents and with search procelooking to the future of cs&e84computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.dures, and bring new models to the attention of cs&e. how, for example, can theenormous numbers of "hits" that automated searches often return be summarizedfor effective further selection? or again, how with reasonable speed can a reader"see" a whole book as effectively as one does today by leafing through it?performanceit is easy to conceive of automatically "reading" whole books of text over ahighspeed fiberoptic network; a book is one or a few megabytes of textual data,and a megabyte takes ten milliseconds to transmit at gigabit rates. it is less easy toimagine, say, an art book coming as page images at a megabyte apiece. issues ofdata compression akin to those present in highdefinition television come to thefore, in storage as well as in transmission. memory hierarchies, probablydistributed, will be needed for economical storage of information, the demand forwhich differs by many orders of magnitude. simultaneous searches on behalf ofmultiple readers pose a challenge to information retrieval technology, likelyinvolving massive parallelism, distributed computing, and scheduling.the matter of survival poses problems, too: how can a library that archivesmaterial for the ages exploit technology that goes utterly obsolete in a decade?and how can indexing and retrieval strategies, which will surely evolve rapidly inthe light of experience, be introduced gracefully?broadening educational horizons in cs&ea broader research agenda for the field requires people willing to engage in awider scope of activity than they have been accustomed to pursuing. thuschanges in the educational milieu of both graduate and undergraduate cs&eeducation will be necessary if a broader agenda is to win wide acceptance.computer scientists and engineers may not need to fully master other disciplines,but they will need to know enough about other domains to understand theproblems in those domains and thus how to apply their own unique analyticaltools to their solution. employment opportunities may well be wider for broadlyeducated computer scientists and engineers than for those who know only aboutcomputing per se.in addition, cs&e education will need to reexamine some of the values withwhich it socializes its graduates. at present, cs&e students are led to believethat doing "pure" cs&e research is the highest pinnacle to which all goodstudents should aspire. values consislooking to the future of cs&e85computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.tent with a broader agenda would teach budding computer scientists andengineers that in the information age, they should learn to make contributions to awide range of fields and problem domains. and finally, cs&e has aresponsibility to help those in other areas to understand the implications of thenew information age. thus it must take a broader view of its responsibilities forservice education to practitioners in other disciplines and problem domains.chapter 4, "education in cs&e," discusses these issues in greater detail.a special role for universityindustrycommerce interactionties between universities and the industrial and commercial world have aspecial role to play in promoting a broader agenda for both research andeducation. one overarching reason is that industry and commerce, concerned withdeveloping products and services for customers who want their problems solved,assemble multidisciplinary project teams and research efforts with much greaterease than do universities with their disciplinecentered departments.22computer hardware and software vendors have a vested interest in beingresponsive to the needs of the user community. over the long run, softwarepackages and hardware systems improve, or their vendors go bankrupt. becauseof its need to gauge accurately what its customers are willing to buy, thecomputer industry can play a special role in specifying for computer scientistsand engineers research areas that have relevance to the user community as awholešgeneralpurpose advances that make computers easier to use or morepractically powerful from the perspective of individual users. a good example ofsuch a role is found in the industrydriven spread of graphical user interfaces. (ofcourse, such contributions will be possible only with the involvement of peoplewhose vision can transcend narrow company perspectives.)commercial users of computers can also help to define a broader researchagenda that is relevant to particular segments of the user community. problemsthat arise in specific applications are often an instance of a more general andincompletely understood issue with substantive intellectual challenge. researchundertaken to solve the specific problem may well shed light on the more generalissue. further, by working with the ultimate end users, academic computerscientists and engineers can help those users to better understand their futureneeds in their particular settings and to develop technology that better meets thoseneeds.looking to the future of cs&e86computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.on the educational front, both the computer industry and commercialcomputer users have an important role to play in broadening. as the needemerges for businesses of every possible description to manage information of alltypes, individuals who understand the possibilities of computermediatedmanagement of such information will be in demand by both industry and users.this imperative has fueled the development of a host of computerrelatedprograms in information sciences, information systems, management sciences,and so on, in addition to programs in cs&e. however, a broadly educated cs&egraduate is most likely the person who will understand how or whether existingtechnology can be adapted to meet existing needs and how to specify and designnew technology that may be required. thus a move by industry and commercialusers to widen the employment opportunities they offer to cs&e graduatesbeyond the narrow computerrelated jobs that cs&e graduates now fill may wellbenefit these firms as they move into the 21st century. these issues are discussedat greater length in chapter 4.prerequisites for broadeningalthough the committee found a reasonable consensus that academic cs&ewould benefit from a broader agenda, the inwardlooking and applicationsavoiding traditions of the field are likely to make implementation of a broaderagenda difficult. the present structure of cs&e as an academic discipline oftenimpedes the participation of faculty members in applicationsoriented orinterdisciplinary work. reorienting academic cs&e to embrace interdisciplinaryor applicationsoriented work will require serious attention to several factors,including the following:23 adequate departmental or university support. the research horizons ofmany faculty (especially junior faculty) could be expanded if theybelieved that good applicationsoriented or interdisciplinary researchwould lead to tenure or promotions. senior faculty, even thoughprotected by tenure, are not immune to the pressures of their colleagues,and if other departmental faculty believe that such work is notintellectually worthy of attention, they too may be inhibited frompursuing such activity.many cs&e departments believe that the evaluation ofinterdisciplinary research is daunting when assessment of work related toother fields is required. even the definition of a peer in interdisciplinaryresearch is unclear. in the words of h.e. morgan, "is a peer a personknowledgeable primarily in the technical aspects of the aplooking to the future of cs&e87computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.proach that is to be applied, or is both technical expertise and a broadknowledge of the field encompassed by the hypothesis and questions tobe addressed also a requirement for designation as a peer?"24 when eventhe general characteristics of those who should be making assessmentsare unclear, departments may well shy away from encouraging workthat requires such assessments. provision of appropriate funding. funding to pursue interdisciplinary orapplicationsoriented research is certain to encourage such work,especially in times of tight research budgets. partly because of itsnovelty, interdisciplinary or applicationsoriented research is often seenby the typical funders of research as highrisk or irrelevant. in theabsence of funding specifically targeted to such work, more traditional,disciplineoriented work often appears the safe route to follow forseekers of research funding. strong communication between cs&e and other problem domains. thesine qua non of most academic work is the published paper or book. butinterdisciplinary or applicationsoriented work often lacks suitableforums that will provide appropriate attention. the solution of a givenproblem may require collaboration between researchers in cs&e andanother field, but journals in the other field may be interested only in theresults relevant to that field, while cs&e journals may be unwilling togive space to describing details of the other field relevant to the solutionof the problem. thus special outlets for such work may be necessary. common educational experiences and mutual respect. collaborationsbetween researchers in cs&e and other disciplines and applicationsareas are most successful when computer scientists and engineers have amodicum of knowledge about those other areas and disciplines, andwhen people from those other areas have some familiarity with currentconcepts in cs&e. moreover, each side of the collaboration mustrespect the basic intellectual interests of the otheršthe interest ofcomputer scientists and engineers in the challenging cs&e aspects, andthe interest of other party or parties in the problem at hand. without suchrespect, it is all too easy for the computer scientist or engineer to beregarded merely as a hired hand responsible for the intellectualequivalent of washing test tubes. a broader definition of research. even when interdisciplinary research isconsidered, prevailing notions in the academic cs&e community limitthe definition of research to fundamental intellectual work thatunderpins a product or may have no connection to any product now or inthe future. thus academic cs&e research maylooking to the future of cs&e88computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.involve theoretical work and proofofprinciple and laboratoryprototypes, but nothing closer to product application. in fact, a greatdeal of intellectually substantive work and inquiry can be associated with"productizing" a concept. as an example, chemical engineering and, to alesser extent, chemistry both include within their definitions of ph.d.research work that improves chemical manufacturing processes. certainchallenging computing problems (e.g., the construction of largescalesoftware systems) have solutions that in practice often do not require asingle key insight but rather many small ideas solving subproblemsacross many areas. such problems are best solved by people withbreadth, but breadth often comes at the expense of the depth thatcharacterizes most traditional research.in addition, traditional notions of academic research call for work inwhich students and faculty are expected to make their mark asindividual scholars and researchers, rather than as members of teams orgroups (as would better characterize an industrial environment). sincemany interesting and substantive problems in cs&e involve as aprimary or secondary activity the construction of large systems thatrequire extended efforts by large groups, those with interests in suchareas may be left at a disadvantage. leadership. by definition, the leaders in any given field play a majorrole in setting the tone and character of that field. the judgments andopinions of these leaders determine the standards to which otherparticipants in the field are held. thus, expanding the boundaries ofcs&e research will require the intellectual leaders in the field toproselytize vigorously in favor of such expansion. they must lobby fordepartmental or university support of a broader agenda. and, mostimportantly, they must engage the public policy process on behalf ofchange with an intensity and persistence that they have not oftendemonstrated in the past.25as a general rule, individuals can participate in or contribute to the publicpolicy process through either the executive branch or the legislative branch.interaction with the executive branch is especially meaningful when it involvessustained effort (e.g., serving as a program officer), simply because such servicegenerally involves decisionmaking authority. interaction with the legislativebranch is potentially more profitable for the field, since the legislative branchdetermines actual funding levels. however, it is often much more frustrating,because the congress is often unable to consider the full implications of variousproposals from the scientific community. box 2.10 describes some of theopportunities available to computer scientists and engineers to engage the publicpolicy process.looking to the future of cs&e89computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.box 2.10 examples of public service positions andopportunities federal agency program officers and directors congressional fellowships from professional societies visiting positions at the office of technology assessment service on the computer science and telecommunications board andits committeessummary and conclusionsbroadening academic cs&e offers benefits from several perspectives. fromthe perspective of the field itself, extending its boundaries will identify newchallenges and offer new opportunities for students and research support. thosein other areas and fields will also benefit from the application of stateofthearthardware and software technologies customized to their specific problems. andfinally, the interaction of cs&e with other disciplines is likely to lead tointellectual insights and developments in both cs&e and those other disciplinesthat would not otherwise be possible. the broadening of cs&e will lead to aflowering of new ideas, advancing the knowledge of humankind as well aspromoting the growth of industry and the economy. intellectually substantivecs&e issues and themes can be found in many problem domains, from biologyand the earth sciences to commercial computing and electronic libraries. butbroadening the cs&e field will require concerted university and funding agencysupport, educational programs to support a broader conception of the field, and arethinking of what constitutes research for an academic computer scientist orengineer.notes1. see robert m. white, "the crisis in science funding," technology review, volume 94(4), may/june 1991, p. 47. lest the reader believe that the need to justify science on the basis of its social andeconomic return is a new sentiment brought about today by increasingly tight budgets and shortsighted political leaders, it is interesting to recall that vannevar bush, in the july 1945 documentwidely regarded as the seminal statement of philosophy underlying creation of the national sciencefoundation, argued for the support of science on the basis of its ability to contribute to society.advances in science when put to practical use mean more jobs, higher wages, shorter hours, moreabundant crops, more leisure for recreation, for study, for learning how to live without the deadeningdrudgery which has been the burden of the common man for ages past. advances in science will alsobring higher standards of living, will lead to the prevention or cure of diseases, will promoteconservation of our limited national resources, and will assure means of defense against aggression . . . .[s]ince health, wellbeing, and security are proper concerns of government, scientific progress is ofvital interest to government. without scientific progress the national health would deteriorate;without scientific progress we could not hope for improvement in our standard of living or for anincrease in the number of jobs for our citizens; and without scientific progress we could not havemaintained our liberties against tyranny. (vannevar bush, sciencešthe endless frontier, nsf 90œ8,national science foundation, washington, d.c., 1945/1990, pp. 10œ11.)looking to the future of cs&e90computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.2. for example, mathematically rigorous investigations of the mandelbrot set were begun only afterbenoit mandelbrot had examined many computergenerated visualizations of the set. mandelbrotobserved that the islands present in lowresolution pictures were apparently not present at higherresolutions. as a result of these examinations, mandelbrot conjectured that the set was connected. arigorous proof of this conjecture has subsequently been developed.3. this point was reinforced at the recent cstb workshop on human resources in cs&e, a reporton which is forthcoming.4. an association for computing machinery (acm) position paper notes that "analyzing howcomputer science and engineering r&d can assist with solving national and international needs canresult in new opportunities and directions, such as increasing funding and more diverse fundingsources." see association for computing machinery, "the scope and directions of computerscience: building a research agenda," communications of the association for computing machinery,volume 34(10), october 1991, p. 123.5. the basic data for this claim are given in table 1.1. the agencies in question include thedepartments of education, justice, agriculture, health and human services (including the nationalinstitutes of health), labor, state, and veterans affairs; the smithsonian institution; the nuclearregulatory commission; the tennessee valley authority; the arms control and disarmamentagency; and the international trade commission. even if the national institutes of health is omittedfrom this list, the research budgets for the remaining agencies still account for $3.4 billion.6. at 38 key institutions, academic computer science was seeded by a number of different disciplines,including mathematics, electrical engineering, business, physics, psychology, physiology, linguistics,philosophy, cognitive science, and management information systems. (see lois peters (rensselaerpolytechnic institute) and henry etzkowitz (state university of new york at purchase), "theinstitutionalization of academic computer science," p. 5. paper presented at the study of science andtechnology in the 1990s, a joint conference of the society for social studies of science and theeuropean association for the study of science and technology, amsterdam, november 16œ19,1988.) even today, the majority of cs&e faculty who have ph.d.s received them in other fields (asnoted in table 8.11 in chapter 8), although projecting forward from the approximately 300 newph.d.s in cs&e who took faculty positions in the 19901991 academic year, this may change soon.7. some of the intellectual issues in this area are reported in eric s. lander, robert langridge, anddamian m. saccocio, "computing in molecular biology: mapping and interpreting biologicalinformation," communications of the acm, volume 34(11), november 1991, pp. 33œ39. this articledescribes some of the key computational challenges in molecular biology as discussed by participantsin a cstb workshop.8. national research council, physics through the 1990s: scientific interfaces and technologicalapplications, national academy press, washington, d.c., 1986, p. 121.9. association for computing machinery and the computing research association, strategicdirections in computing research, acm press, 1990, pp. 1œ2.looking to the future of cs&e91computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.10. david gries, terry walker, and paul young, ''the 1988 snowbird report: a discipline matures,"communications of the acm, volume 32(3), march 1989, pp. 294œ297.11. association for computing machinery, "the scope and directions of computer science,"communications of the acm, volume 34(10), october 1991, pp. 121œ131.12. this approach to building a research agenda has much in common with one discussed in an acmposition paper that argues for a strategy that "propose[s] a set of goals and needs, and recommend[s]computing research that can help attain those goals." see association for computing machinery, "thescope and directions of computer science: building a research agenda," communications of theacm, volume 34(10), october 1991, p. 122. the use of "computing research" in this reference isequivalent to the use in this report of "cs&e research." see also john rice, "is computing researchisolated from science?", computing research news, volume 2(2), april 1990, p. 1.13. the definitions used by the national science foundation are the following (national sciencefoundation, federal funds for research and development: fy 1988, 1989, 1990, nsf 90œ306,nsf, washington, d.c., 1990, pp. 2œ3):"research is systematic study directed toward fuller scientific knowledge or understanding of thesubject studied. research is classified as either basic or applied according to the objectives of thesponsoring agency.in basic research the objective of the sponsoring agency is to gain fuller knowledge or understandingof the fundamental aspects of phenomena and of observable facts without specific applications towardprocess or products in mind.in applied research the objective of the sponsoring agency is to gain knowledge or understandingnecessary for determining the means by which a recognized and specific need may be met.development is systematic use of the knowledge or understanding gained from research, directedtoward the production of useful material, devices, systems, or methods."the u.s. definition of "basic research" as research without application in mind stands in markedcontrast to the japanese notion of "basic research" as research that is basic to the future of industry.see david cheney and william grimes, japanese technology: what's the secret?, council oncompetitiveness, washington d.c., february 1991, p. 4.14. indeed, a powerful argument can be made that the linear model of basic research leading toapplied research, applied research leading to development, development leading to productmanufacture, and manufacture leading to sales is highly oversimplified and in many ways downrightmisleading. product innovation rarely resembles the popular view of one revolution followed bytedious development (e.g., invent the transistor, and the rest is reduction to practice). rather, theprocess more resembles something like this:invent the transistor,then invent technology to place 10 transistors on a chip,then invent technology to place 100 transistors on a chip, . . .then invent technology to place 100,000,000 transistors on a chip, and so on.this model, often called the cyclic development model, is discussed in r.e. gomory and r.w.schmitt, "science as product," science, volume 240, may 27, 1988, pp. 1131œ1132, 1203œ1204.15. see, for example, computer science and technology board, national research council, thenational challenge in computer science and technology, national academy press, washington,d.c., 1988, pp. 34œ35.looking to the future of cs&e92computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.16. as used in this report, the terms "interdisciplinary research" and "applicationsoriented research"are not synonymous. interdisciplinary research is research that requires and draws on intellectualcontributions from cs&e and some other discipline together. applicationsoriented research is cs&eresearch pursued in the context of some specific problem that may well be fully understood from anintellectual standpoint but whose scale or nature may overmatch the capabilities of current computingtechnology.17. the national research council's interim report on eosdis noted the synergy possible in acollaboration between the earth sciences and cs&e, arguing that "eosdis, as it evolves, mustmaintain the flexibility to build rapidly on relevant advances in computer science and technology,including those in databases, scalable mass storage, software engineering, and networks. doing someans that eosdis should not only take advantage of new developments, but also should become aforce for change in the underlying science and technology where its own needs will promote stateoftheart developments." see national research council, panel to review eosdis plans: interimreport, washington, d.c., april 9, 1992, p. 3.18. for example, the american express company and schlumberger, both stalwarts of the americanbusiness community, will be among the first organizations to purchase a massively parallel computerrecently offered for sale by the thinking machines corporation. such purchases indicate thatproblems faced by these firms cannot be solved economically with routine computing technology. seejohn markoff, "american express to buy two top supercomputers," new york times, october 30,1991, p. c7.19. however, it should also be noted that technology changes rapidly enough and the lag time inmaking purchases is long enough that it is often difficult for any standard to be widely used andaccepted. still, electronic data interchange of various types is growing rapidly.20. for example, as this report goes to press, the american association for the advancement ofscience and the online computer library center are about to launch an online, peerreviewedjournal titled "the online journal of current clinical trials." manuscripts will be submitted,reviewed, and published in electronic form to as great a degree as possible. see joseph palca, "newjournal will publish without paper," science, volume 253, september 27, 1991, p. 1480.21. hypertext is a way of presenting text that is not structured linearly. a hypertext document hascrossreferences and other links that allow the reader to peruse the document in an order that makessense for his or her needs at the time.22. as the value of interdisciplinary work is recognized, it may become easier to performinterdisciplinary research in universities. the nsfsponsored engineering research centers and thescience and technology centers appear to represent a positive step this direction.23. the first four factors listed are inspired by a presentation in national research council andinstitute of medicine, interdisciplinary research: promoting collaboration between the lifesciences and medicine and the physical sciences and engineering, national academy press,washington, d.c., 1990, pp. 12œ15.24. h.e. morgan, "open letter to nihšreview of crossdisciplinary research," in thephysiologist, volume 31(april), 1988, pp. 17œ20 cited in national research council and institute ofmedicine, interdisciplinary research: promoting collaboration between the life sciences andmedicine and the physical sciences and engineering, national academy press, washington, d.c.,1990, p. 12. although the letter concerns interdisciplinary research in the life and health sciences, themoral is the same.25. an example of past indifference to participation in the public policy process is evident in theexperience of nsf's computer and information sciences and engineerlooking to the future of cs&e93computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.ing directorate, which provides a considerable percentage of research funding for academic cs&eand thus exerts a substantial influence over the field. naturally, nsf looks to the field to provideknowledgeable individuals who can help to shape a research program and make reasonable decisionsabout funding directions. but, according to nsf officials, finding appropriate individuals willing tofill staff and highlevel management positions within the cise directorate has been extraordinarilydifficult.why is it difficult? some people argue that a period of inactivity in research of even a few years canplace an individual at considerable disadvantage. without special provisions such as "exit grants,"faculty may be hesitant to enter public service even temporarily. (an "exit grant" is a grant providedto program officials returning to academia that enables them to restart their own personal researchprograms and thus facilitates their reentry into academic life. such grants may be provided formallythrough a designated program, or informally through a mutual understanding of the participantsinvolved.) others argue that the salaries paid for government service tend to be lower than those thatcould be earned by qualified computer scientists and engineers working outside of government. stillothers contend that most cs&e departments are so "thin" that the departure of an individual for a fewyears could cripple such a department's ability to cover an important subarea of cs&e. finally, therelative youth of academic cs&e tends to increase the number of individuals who, in earlier stages oftheir career, quite naturally and reasonably focus on their own personal research agendas.looking to the future of cs&e94computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.3a core cs&e research agenda for thefuturecore cs&e research is characterized by great diversity.1 some core researchareas are fostered by technological opportunities, such as advances inmicroelectronic circuits or opticalfiber communication. such research generallyinvolves systembuilding experiments. the successful incorporation of theremarkable advances in technology over the past several decades has been largelyresponsible for making computer systems and networks enormously morecapable, while reducing their cost to the point that they have become ubiquitous.for other research areas, computing itself provides the inspiration. complexitytheory, for example, examines the limits of what computers can do. computinginspired cs&e research has often provided the key to effective use of computers,making the difference between the impossible and the routine.the diversity of technical interests within the cs&e research community, ofproducts from industry, of demands from commerce, and of missions between thefederal researchfunding agencies has created an intellectual environment inwhich a broad range of challenging problems and opportunities can be addressed.indeed, the subdisciplines of cs&e exhibit a remarkable synergy, one that arisesbecause the themes of algorithmic thinking, computer programs, and informationrepresentation are common to them all; box 3.1 provides illustrative examples.narrowing the focus to a few research topics to the exclusion of others would be amistake. thus the description ofa core cs&e research agenda for the future95computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.promising research areas that follows below should not be regarded as definitiveor exclusive.box 3.1 synergy between subdisciplines of cs&ea prime example of synergy between subdisciplines is the relationshipbetween theoretical computer science, the development of compilers, andcomputer architectures. a compiler translates a computer program written ina "highlevel" language into instruction sequences ("object code") thatmachines can execute. the first compilers were designed on a largely adhoc basis without the benefit of sophisticated theoretical understanding. thedifficulties encountered in creating these ad hoc solutions motivated thecreation of a new underlying theory of formal languages for computers. thisnew theory in turn enables the creation of new programs that couldautomatically generate other programs to perform various compilingchores. the design of new computer architectures has also benefittedsubstantially from the development of new compiler technology; the designprinciple of today's reducedinstructionset computers (riscs)namely theextensive use of simple instructions that execute very rapidly rather thancomplex instructions that do more work but execute much more slowlyisbased on the ability of current compiler technology to generate highlyoptimized object code.the story of parallel processing also illustrates the impact of synergybetween subdisciplines. for decades computer designers have toyed withthe idea of constructing large computers from several smaller ones. butonly in the last several years has the study of parallel processing taken off.the reason? the shrinking cost and size of processors have made itpossible to connect many of them together in the space of a traditionalcomputer, while portable operating systems mean that these innovatorsdon't have to invent new system software in addition to constructing a newcomputer. this has made smallscale multiprocessors widely available,spurring the research in parallel processing. at present, we can constructhardware with 64,000 processors working in concert, but organizing andprogramming those processors to operate 64,000 times more rapidly is stillvery much an open problem. progress on this challenging problem willrequire extensive cooperation between subdisciplines of cs&e such ascomputer architecture, algorithms, and applications.as the saying goes, precise predictions are difficult, particularly about thefuture. nevertheless, the committee is confident that technologydriven advanceswill be sustained for many more years and that computing and cs&e willcontinue to thrive on the philosophy,a core cs&e research agenda for the future96computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.well stated by alan kay, that the best way to predict the future is to invent it.major qualitative and quantitative advances will continue in severaltechnological dimensions: processor capabilities and multipleprocessor systems; available bandwidth and connectivity for data communications andnetworking; program size and complexity; management of multiple types, sources, and amounts of data; and number of people who use computers and networks.for all of these dimensions, change will be in the same direction: systemswill become larger and more complex. coping with such change will demandsubstantial intellectual effort and attention from the cs&e research community,and indeed in many ways the overall theme of "scaling up" for large systemsdefines a core research agenda. (see also box 3.2.)parts of the following discussion incorporate, augment, and extend keyrecommendations from recent reports that have addressed various fields withincs&e: the 1988 cstb report the national challenge in computer science andtechnology; the 1989 hopcroftkennedy report, computer science achievementsand opportunities; the 1990 lagunita report, database systems: achievementsand opportunities; and the 1989 cstb report scaling up: a research agendafor software engineering. 2processor capabilities and multipleprocessorsystemsas noted in chapter 6, future advances in computational speed are likely torequire the connection of many processor units in parallel; box 3.3 provides moredetail. this trend was recognized in the hopcroftkennedy report, whichadvocated research in parallel computing as described in box 3.4.computing performance will increase partly because of faster processors.advances in technology, the good fit of reducedinstructionset computing(risc) architectures with microelectronics, and optimizing compiler technologyhave permitted processor performance to rise steeply over the past decade.continued improvements are pushing singleprocessor performance towardspeeds of 108 to 109 instructions per second and beyond.even larger gains in performance will be achieved by the use of multipleprocessors that operate in parallel on different parts of aa core cs&e research agenda for the future97computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.box 3.2 cs&e areas of special concern to thecomputer industryconsistent with the idea articulated in chapter 2 that cs&e mightfruitfully look to problems faced by industry, the discussion below outlinesseveral areas in which representatives from the computer industryexpressed special interest in briefings to the committee. testing. although it has been understood for a long time that postdelivery maintenance costs are large, the cost and effort of testing asystem prior to initial release are also considerable. committee briefersnoted that anywhere from 30 to 80 percent of the prerelease effort for amajor commercial software release may be devoted to testing. large, distributed, and interconnected systems. needed in particular area fuller scientific understanding of such systems and the engineeringskills to design, construct, debug, test, and operate them. typicalconcerns that arise with "giant" systems are the ability to program themcorrectly in the first place, to be able to maintain operation underdynamic fault conditions, to facilitate interoperability among diverseprotocols and heterogeneous products, and to optimize performance. examples of such "giant" systems are the national telephone network andlarge geographically distributed networks of workstations, pcs,mainframes, supercomputers, and databases to solve integrated and/orsingle very large (not partitionable) problems. knowledge collection and utilization. computer systems should be ableto converse with a user interactively, allowing the user to ask complexquestions for which the system then finds and displays the answers aftera resourceful analysis of its knowledge database. fundamentally, theintellectual problem is how to represent human knowledge in machineform and how to collect, analyze, and use such knowledge insidecomputers. interoperability. interoperability is necessary for easy operation amonghardware and software from diverse vendors. examples ofinteroperability issues include: exchange of data (including graphical information) among different computers. needed are ideas and standards for exchange that promoteefficient interchange and use of many differing representations. twoexamples are postscript, a formatting and graphics language now usedby many laser printers, and unicode, a 16bit representation ofcharacters from all known languages, including english, chinese,russian, and hebrew. user interfaces. user interfaces are often very different across differentsystems. how can vendors compete using their individually derived andoptimized systems while still presenting a common interface to all users?a core cs&e research agenda for the future98computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.box 3.3 what is parallelism?parallelism can be used to speed up the execution of a given job (e.g.,running it twice as fast by using twice the hardware) or to scale up the sizeof a given job (e.g., handling a job with twice as many variables by usingtwice the hardware). an important goal of parallel computing is to developdesigns and architectures that exhibit linear speedup and scaleup for a wideclass of problems and that are also scalable. (a scalable architecture is onein which it is possible to add another processor without changing the basicdesign, and in which the scaleup or speedup factor changes proportionally.)given this terminology, the parallelism challenge is easily stated: find acomputer architecture, programming style, and algorithms that give largelinear speedups and scaleups for a wide class of problems. that is, a1000module system should be able to solve a wide class of problems 1000times faster, or solve in the same time problems that are 1000 times bigger.box 3.4 hopcroftkennedy agenda on parallelismcomponent design: processors, memory systems, interconnectionnetworks optimized for parallel operation.architecture: how to organize components in ways that maximize theirprogrammability.languages and language implementation: automatic extraction ofparallelism inherent in serial code; developing "natural" languages forparallel programming.algorithms and applications: limits on parallel computation; effectiveparallel algorithms.distributed computing: how to manage loosely coupled andgeographically separated processors.source: j.e. hopcroft and k.w. kennedy, eds., computer science:achievements and opportunities, society for industrial and appliedmathematics, philadelphia, 1989, pp. 72œ74.a core cs&e research agenda for the future99computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.demanding application. the hopcroftkennedy report, written in 1986œ1987, described a goal of 10fold to 100fold speedups.3 but since that time,technological advances have made this goal far too modest. today, it is plausibleto aim for increases in speed by factors of 1000 or more for a wide class ofproblems. apart from this point, the hopcroftkennedy outline remains generallyvalid.vector supercomputers have gained speed, more from multiple (10 to 100)arithmetic units than from increases in singleprocessor performance. massivelyparallel supercomputers (1,000 to 100,000 very simple processors) have nowpassed vector supercomputers in peak performance. workstations with multiple(2 to 10) processors are becoming more common, and similar personal computerswill not be far behind.the ability of parallel systems to handle many demanding computingproblems has been demonstrated clearly during the period since the hopcroftkennedy report was written. it had not been clear that linear speedups arepractically achievable by using processors in parallel; indeed on some problemsthey are not.4 the practical difficulty in exploiting parallel systems is that theirefficient use generally requires an explicitly parallel program, and often a programthat is tailored for a specific architecture. although such programs are generallymore difficult to write than are sequential programs, the investment is oftenjustified. on wellstructured problems in scientific computing, visualization, anddatabases, results have been obtained that would not otherwise be affordable.5insights into possibilities for programming and architectures are provided by thestudy and instrumentation of problems that are less regular or algorithmicallymore difficult and that "push the envelope" of parallel systems. parallelcomputing will be a primary focus of the highperformance computing systemscomponent of the hpcc program.distributed computing, another focus of the hopcroftkennedy report, isperhaps a more pressing concern now than when that report's recommendationswere formulated.6 computing environments have been evolving from individualcomputers to networks of computers. seamless integration of heterogeneouscomponents into a coherent environment has become crucial to manyapplications. customers are increasingly insisting on the freedom to buy theircomputer componentsšsoftware as well as hardwarešfrom any vendor on thebasis of price, performance, and service and still expect these various componentsto operate well together. such pressure from customers has hastened themovement toward "open system" architectures. businesses are becoming moredispersed geographically, yet more integrated logically and functionally.distributed computer systems are indispensable to this trend.a core cs&e research agenda for the future100computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.as computing penetrates more and more sectors of society, reliability ofoperation becomes ever more important. many applications (e.g., space systems,aircraft, airtraffic control, factory automation, inventory control, medicaldelivery systems, telephone networks, stock exchanges) require highavailabilitycomputing. distributed computing can foster highavailability by eliminatingvulnerability to singlepoint failures in software, hardware, electric service, thelabor pool, and so on.what intellectual problems arise in parallel and distributed computing? asdiscussed at length in the chapter 6 section "systems and architectures," paralleland distributed computing systems are capable of nondeterministic behavior,producing different results depending on exactly when and where different partsof a computation happen. unwanted conditions may occur, notably deadlock, inwhich each of two processes waits for something from the other. thesecomplications are exacerbated when the system must continue to operatecorrectly in the presence of hardware, communication, and software faults.sequential programming is already difficult; the additional behavioralpossibilities introduced by concurrent and distributed systems make it evenharder to assure that a correct or acceptable result is produced under allconditions. new disciplines of parallel, concurrent, and distributed programming,together with the development and experimental use of the programming systemsto support these disciplines, will be a high priority of and a fundamentalintellectual challenge for cs&e research for at least the next decade.data communications and networkingcompared with copper wires, fiberoptic channels provide enormousbandwidths at extremely attractive costs. a 1000fold increase in bandwidthcompletely changes technology tradeoffs and requires a radically differentnetwork design for at least three reasons.one reason is that the speed of transmission, bounded by the speed of light,is about the same whether the medium of transmission is copper wire or opticalfiber. current computer networks are based on the premise that transit time (i.e.,the time it takes for a given bit to travel from sender to receiver) is smallcompared to the times needed for processing and queuing.7 however, data can beentered into a gigabit network so fast that transit time may be comparable to oreven longer than processing and queuing time, thereby invalidating this premise.for example, a megabytesize file can be queued in a gigabit network in tenmilliseconds. but if the file is transmitted coast to coast, the transit time is abouttwice as long.a core cs&e research agenda for the future101computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.under these conditions, millions of bits will be pumped into the crosscountry link before the first bit appears at the output.a second reason is that current networks operate slowly enough thatincoming messages can be stored temporarily or examined ''on the fly." suchexaminations underlie features such as dynamic route computation, in which theprecise path that a given message takes through a network is determined atintermediate nodes through which it passes. in a gigabit network the volume ofdata is much larger and the time available to perform "onthefly" calculations ismuch smaller, perhaps so much so that storeandforward operation and dynamicrouting may not be economically viable design options.a third reason is that the underlying economics are very different. in currentnetworks, channel capacity (i.e., bandwidth) is expensive compared with theequipment that allows many users to share the channel. sharing the channel (i.e.,"multiplexing," or switching among many users) minimizes the idle time of thechannel. but fiberoptics is based on the transmission of light pulses (photons)rather than electrical signals. the technology for switching light pulses isimmature compared with that for switching electrical signals, with the result thatswitching devices for fiberoptics are relatively more expensive than channelcapacity.as noted in chapter 6, a complete understanding of networks based on firstprinciples is not available at this time. today's knowledge of networking is basedlargely on experience with and observation of megabit networks. gigabitnetworking thus presents a challenging research agenda, one that is an importantfocus of the hpcc program. consider the following kinds of the researchproblems that arise in the study of gigabit networking: network stability (i.e., the behavior of the flow of message traffic) isparticularly critical for highspeed networks. a network is aninterconnected system, with many possible paths for feedback to anygiven node. a packet sent by one node into the network may triggeršatsome indeterminate point in the futurešfurther actions in other nodesthat will have effects on the originating node. the inability to predictjust when these feedback effects will occur presents many problems forsystem designers concerned about avoiding catastrophic positivefeedback loops that can rapidly consume all available bandwidth. thissocalled delayedfeedback problem is unsolved for slower networks aswell, but our understanding for slower networks is at least informed byyears of experience. network response is another issue that depends on empiricalunderstanding. in particular, the fiberbased networks of the futurea core cs&e research agenda for the future102computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.will transmit data much more rapidly, the computer systemsinterconnected on these networks will operate much more quickly, thenumber of users will be much larger, and computing tasks may well bedispersed over the network to a much greater degree than today. all ofthese factors will affect the behavior of the network. network management itself requires communication between networknodes. gigabit networks will involve significant quantities of this"overhead" information (e.g., routing information), primarily becausethere will be so many messages in transit. thus, fast networks requireprotocols and algorithms that will reduce to an absolute minimum theoverhead involved in the transmission of any given message. network connections will have to be much cheaper. scaling a networkfrom 100,000 connections (the internet today) to 100 millionconnections (the number of households in the united states, and theultimate goal of many networking proponents for which the nationalresearch and education network may be a first step) will requireradical reductions in the cost of installing and maintaining individualconnections. these costs will have to drop by orders of magnitude, aresult possible only with the largescale automation of operations,similar to that used in the telephone network today.software engineeringthe problems of largescale software engineering have been the focus ofmany previous studies and reports, in particular the hopcroftkennedy report(box 3.5) and the cstb report scaling up (table 3.1). nevertheless, largescalesoftware engineering remains a central challenge, as discussed in box 3.6.the committee recommends continuing efforts across a broad front tounderstand largescale software engineering, concurs with the research agendasof the hopcroftkennedy and cstb reports, and wishes to underscore theimportance of two key areas, reengineering of existing software and testing.reengineering of existing softwarelargescale users of computers place great emphasis on reliability andconsistency of operation, and they have enormous investments tied up in softwaredeveloped many years ago by people who have long since retired or moved on toother jobs. these users often recognize that their old software systems areantiquated and difficult to maintain, but they are still reluctant to abandon them.the reason isa core cs&e research agenda for the future103computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.that system upgrades (e.g., converting an air traffic control system that might bewritten in pl/1 to a more modern one written in ada) present enormous risks tothe users who rely daily on that system. the new system must do exactly what theold system did; indeed, a new system may need to include bugs from the oldsystem that previously necessitated " workarounds," because the people andother computer systems that used the old system have become accustomed tousing those workarounds. in many cases the current operating procedures of theorganization are only encoded in (often undocumented) programs and are notwritten down or known completely by any identifiable set of people.box 3.5 hopcroftkennedy research agenda onsoftware engineeringdesign languages that permit programmers to operate at moreproductive levels of abstraction.software engineering environments and databases that provideautomated support from start to end and facilitate configuration andconsistency management.graphics and human interfaces that facilitate human understanding oflarge software systems at various levels of abstraction.design for reusability that would allow software created for oneapplication to be adapted with minimal effort for use in another application.automated systems for program specification, verification, and testingto ensure correctness.techniques for systems maintenance that facilitate error correction andsystem evolution.source: j.e. hopcroft and k.w. kennedy, eds., computer science:achievements and opportunities, society for industrial and appliedmathematics, philadelphia, 1989, pp. 69œ71.thus effective reengineering requires the ability to extract from code theessentials of existing designs. new technologies that support effective and rapidupgrade within operational constraints woulda core cs&e research agenda for the future104computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.have enormous value to software engineering, especially in the commercialworld. such technologies could include graphical problemdescriptionmethodologies that provide visual representations of program or data flow orautomated software tools that make it easier to extract specifications fromexisting code or to compare different sets of specifications for contradictions orinconsistencies.table 3.1 the "scaling up" agenda for software engineering researchshortterm (1œ5 years)longterm (5œ10 years)perspectiveportray systems realistically:view systems as systems andrecognize change as intrinsicresearch a unifying modelfor software developmentšfor matching programminglanguages to applicationsdomains and design phasesstudy and preserve softwareartifactsstrengthen mathematicaland scientific foundationsengineering practicecodify software engineeringknowledge for disseminationand reuseautomate handbookknowledge, access, andreusešand makedevelopment of routinesoftware more routinedevelop softwareengineering handbooksnurture collaboration amongsystem developers andbetween developers andusersresearch modesfoster practitioner andresearcher interactionslegitimize academicexploration of large softwaresystems in situglean insights frombehavioral and managerialsciencesdevelop additional researchdirections and paradigms:encourage recognition ofreview studies, contributionsto handbookssource: reprinted from computer science and technology board, national research council,scaling up: a research agenda for software engineering, national academy press, washington,d.c., 1989, p. 4.a core cs&e research agenda for the future105computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.box 3.6 why is largescale software engineeringdifficult?software systems with millions of lines of code and hundreds ofprogrammers stretch the limits of human comprehensionšindeed, no oneperson can understand every aspect of a system with even 100,000 lines ofcode. small software systems are fundamentally different from largesystems, as the table below makes clear, and so the intellectual andmanagerial tasks of largescale system design and implementation are verydifferent from the corresponding tasks for smallprogram design andimplementation.just as understanding onecelled protozoa does not help much withunderstanding human beings as complete organisms (even though humansare composed of trillions of cells), understanding simple programs does notnecessarily give much insight into much larger programs and systems.issues such as project management, system structure specification, sourcecode control, and code integration arise in largescale softwareengineering, whereas in smallscale software engineering such issues canoften be avoided or ignored. the table below illustrates some of therelevant comparisons between smallscale and largescale softwareengineering.comparisons between smallscale and largescale softwareengineeringproperty or characteristicsmallscalesoftwareengineeringlargescalesoftwareengineeringsize of programthousands of lines(or fewer)hundreds ofthousands of lines(or more)number of peopleinvolvedone or a fewhundreds orthousandsduration of projectweeks/monthsyears/decadesinterconnections withother systemsfewmanyspecifying requirementseasiermuch harderdemonstratingcorrectnessproofs ofcorrectnessplausibleempirical testing isonly optionwhy are software systems becoming so large and complex?fundamentally, the reason is that computer hardware is becoming lessexpensive. for example, the size of computer memories associated withprocessors has grown considerably as the result of lower prices forcomputer memories. larger memories can accommodate larger programs,and since there is no natural limit on the demand for increased systemfunctionality to solve problems of greater complexity, larger and largersoftware systems have been the result.a core cs&e research agenda for the future106computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.the practical ramifications of largescale software engineering areenormous. the nasa earth observing system and data informationsystem, space station freedom, the new air traffic control system, andthe advanced tactical fighter are all projects that will require new softwarein very large amounts (millions or tens of millions of lines of code). without abetter understanding of largescale software engineering, these projectsand others will continue to suffer all too frequently from schedule slippages,cost overruns, incorrect function, inadequate function, and unacceptableperformance.testingas noted in box 3.2, testing is a severe bottleneck in the delivery of softwareproducts to market.8 moreover, while program verification, proofs of programcorrectness, and mathematical modeling of program behavior are feasible atprogram sizes on the scale of hundreds of lines, these techniques are inadequatefor significantly larger programs. one reason is sheer magnitude. a second, moreimportant reason is the inherent incompleteness, if not incorrectness, of large setsof specifications. program verification can show that a program conforms to itsspecifications or that the specifications contain inadvertent loose ends, but notthat the specifications describe what really needs to be done.thus theories and practical methods of software testing that are applicable torealworld development environments are essential. some relevant questions arethe following: how can competent test cases be generated automatically? how can conformity between documentation and program function beachieved? how can requirements be tested and verified?information storage and managementthe lagunita report described an important and farreaching agenda fordatabase research (box 3.7). the committee believes that the lagunita researchagenda remains timely and appropriate, and also commends for attention: data mining and browsing techniques that can uncover previousa core cs&e research agenda for the future107computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.ly unsuspected relationships in data aggregated from many sources.box 3.8 describes some database research questions that are motivatedby commercial computing.box 3.7 the lagunita agendathe lagunita report (avi silberschatz, michael stonebraker, and jeffullman, eds., "database systems: achievements and opportunities,"communications of the acm, volume 34(10), october 1991, pp.110120) emphasized two key areas in database research: nextgeneration database applications and heterogeneous, distributeddatabases.nextgeneration database applications will involve the storage andmanagement of large and internally complex objects (e.g., images), newconcepts in data models (e.g., spatial data such as polygons in space andtimeordered sequences (states) of a given database), longdurationtransactions, versions and configurations, and the scaling up of databasemanagement algorithms to operate effectively on databases severalorders of magnitude bigger than the largest databases found today.heterogeneous, distributed databases will require support to managebrowsing, data incompleteness and inconsistency, security, transactionmanagement, and automated mediators that resolve datainconsistencies. systems architectures, data representations, and algorithms to exploitheterogeneous, distributed, or multimedia databases on scales ofterabytes and up. multimedia databases will be especially useful tomodern businesses, most of which make substantial use of text andimages; document and image scanning, recognition, storage, and displayare at the core of most office systems. current networks, databases,tools, and programming languages do not handle images or structuredtext very well. image searches, in particular, usually depend on keywordtags assigned to images manually and in advance.distributed databases maintained at different nodes are increasinglycommon. integrating data residing in different parts of the database (e.g., indifferent companies, or different parts of the same company) will become moreimportant and necessary in the future. thus research on multimedia anddistributed databases would have a particularly high payoff for commercialcomputing.a core cs&e research agenda for the future108computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.box 3.8 commercial computing concerns thatmotivate some database researchmany database challenges are motivated by the demands ofcommercial computing. for example, today's business environment is "datarich" but often "information poor." companies can drown in a flood of data ifthey do not know how to sort it, categorize it, identify it, summarize it, andorganize it. one major difficulty arises in the use of data that may becollected at a variety of different sites. unless these different sites useexactly the same processes to collect data, data incompatibilities andinconsistencies are likely to occur when the data are aggregated. forexample, a certain insurance company has 17 different definitions of theterm "net written premium," which is its key measure of sales. whichdefinition is used depends on which office and functional group within thecompany is using the term and for what purpose the definition is beingused. a useful integration of this company's databases would be able toreconcile conflicting definitions of the term when possible (e.g., as theydiffer across office boundaries or as they change with time), and flaginconsistencies when necessary.a second problem is retaining information related to data quality as apiece of data works its way through the system. to a large extent, dataquality considerations in the past were handled largely through personalfamiliarity; the user knew the characteristics of the data being used andinformally took this into account when using the data. this approach is notfeasible when the data are drawn from sources that are not well known tothe user, or when an automated process converting, merging, andprocessing the data renders inaccessible annotations to the original datathat previously would have conveyed information about its quality. forexample, the source of a given piece of data is often a key element injudgments about its credibility and quality. yet maintaining the identity of thesource as data move through the system turns out to pose problems ofconsiderable technical difficulty. more generally, the integrity constraintsand normalization theories used to maintain the integrity and consistency ofdata stored in the database at the schema level are not sufficient to assuredata quality at the level demanded by nonsystem constituents.still a third problem arises from the realization that autonomousdatabases are independently evolving in semantics as well as contents. forexample, consider the situation of the stock exchanges around the world.not only are the stock prices changing continuously, but the definition of thestock price reported also can change. thus, at some time the reported priceof shares on the paris stocka core cs&e research agenda for the future109computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.exchange will change from being measured in francs to beingmeasured in european currency units. the normal electronic "ticker tape"data feeds do not explicitly report the currency; rather, it is implicit in thecontext of the source. more subtle examples include changes from reporting"last nominal price" to "last closing price" or from a percentagebasedpricing to actual prices, as is currently happening at the madrid stockexchange. what is needed is not only a way to capture the current meaningof each of these sources but also a way to represent the current desired (orassumed) meaning of the receiver, which may be a human, an application,or another database. furthermore, it is then necessary to have algorithmsthat compare the current semantics of the source and the receiver todetermine if they are compatible, partially compatible, convertible, orincompatible.a good discussion of problems that arise from heterogeneousdatabases is contained in acm computing surveys, special issue onheterogeneous federated databases, september 1990.source: stuart madnick, mit, assisted the committee in preparingthis discussion.reliabilityreliabilityšinformally defined here as the property of a computer systemthat the system can be counted on to do what it is supposed to došis an exampleof a research area that potentially builds on, or is a part of, many other areas ofcs&e. distributed systems provide one promising method for constructingreliable systems. assuring that a program behaves according to its specification isone of the first requirements of software engineering. large (terabytescale)databases will need to be maintained online and to be accessible for periodslonger than the time between power failures, the time between media failures(disk crashes), and the lifetime of data formats and operating software.as computing becomes a crucial part of more and more aspects of our livesand the economy, the reliability of computing correctly comes into question(box 3.9). the following technical problems are often relevant to decisionsregarding whether computers should be used in critical applications: as failures in telephone and air traffic control systems havedemonstrated, errors can propagate catastrophically, causing servicea core cs&e research agenda for the future110computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.outages out of proportion to the local failures that caused the problem.the problems of ensuring reliability in distributed computing aremultiplying at least as rapidly as solutions.box 3.9 problems in reliabilityeach of these typical reliability problems is related to some other areadescribed in the main text of this report. can unwanted race conditions or deadlocks happen in a distributedsystem? (distributed computing) is an electronic mail message authentic? (networking) does a system as built agree with either its specification or its design?(software engineering) how much of a millionline program must be bulletproof to assure that theprogram can recover from its own errors? (software engineering) can a simple human error cause a crash? (ease of use and userinterfaces)for more discussion, see computer science andtelecommunications board, national research council, computers atrisk, national academy press, washington, d.c., 1991. software systems, particularly successful ones, usually changeenormously with time. yet almost everything in the system builder'stool kit is aimed at building static products. existing techniques do notcontemplate making a system so that it can change and evolve withoutbeing taken off line. upgrading a system while it runs is an importantchallenge. a related challenge is doing large computations whoserunning time exceeds the expected "up time" of the computer orcomputers on which the computation is executed. ad hoc methods ofcheckpointing and program monitoring are known, but their use mayintroduce debilitating complications into the programs. nonexpert users of computers need graceful recovery from errors (ratherthan cryptic messages, such as "abort, retry, or fail?") and automaticbackup or other mechanisms that insulate them from the penalties oferror.user interfacesuser interfaces, one dimension of a subfield of cs&e known as humancomputer interaction, offer diverse research challenges.9 the keyboard and themouse remain the dominant input devices today.a core cs&e research agenda for the future111computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.talking to a computer is in certain situations more convenient than typing,but the use of speech as an input medium poses many problems, some of whichare listed in box 3.10. if it is to cope with situations of any complexity, acomputer must be able to interpret imprecisely or incompletely formulatedutterances, recognize ambiguities, and exploit feedback from the task at hand.analogous problems exist in recognizing cursive handwriting, and even printedmatter, in which sequences of letters are merged or indistinct.the experimental darpafunded sphinx system, described more fully inthe chapter 6 section ''artificial intelligence," is a promising start to solving someof the problems of speech recognition, and apple computer expects to bring tomarket in the next few years (and has already demonstrated) a commercialproduct for speech recognition called "plaintalk" based on sphinx.recognition of gestures would also increase the comfort and ease ofhumancomputer interaction. people often indicate what they want with gesturesšthey point to an object. touchsensitive screens can provide a simplekinesthetic input in two dimensions, but the recognition of motions in threedimensions is much more difficult. penbased computing, i.e., the use of a pen toreplace both the keyboard (for the input of characters) and the mouse (forpointing), is another form of gesture recognition that is enormously challengingand yet has the potential for expanding the number of computer usersconsiderably. indeed, the ability to recognize handwritten characters, both printedand cursive, will enable computers to dispense entirely with keyboards, makingthem much more portable and much easier to use.the primary output devices of today adhere to the paper metaphor; even thecrt screen is similar to a sheet or sheets of paper onbox 3.10 difficulties in speech processing recognition of words in continuous speech. in everyday speech, "icecream" can be heard as "i scream," or vice versa. parsing any but the most stereotyped utterances. recognition that is independent of the speaker. tonalities and accents(even colds!) complicate the recognition of speech. recognition of speech in a noisy environment. speechrecognitionsystems have difficulty isolating a specific vocal stream from othernearby conversations.a core cs&e research agenda for the future112computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.which twodimensional visual objects (e.g., characters or images) are presented,albeit more dynamically than on paper. people can, however, absorb informationthrough, and have extraordinary faculties for integrating stimuli from, differentsenses.audio output systems can provide easily available sensory cues when certainactions are performed. many computers today beep when the user has made amistake, alerting him or her to that fact. but sounds of different intensity, pitch,or texture could be used to provide much more sophisticated feedback. forexample, an audio output system could inform a user about the size of a file beingdeleted, without forcing the user to check the file size explicitly, by making a"clunk" sound when a large file is deleted and a "tinkle" sound when a small oneis deleted.touch may also provide feedback. chapter 6 (in the section titled"computer graphics and scientific visualization") describes the use of forcefeedback in the determination of molecular "fitting"šhow a complex organicmolecule fits into a receptor site in another molecule. but the use of a joystick isrelatively unsophisticated compared to the use of force output devices that couldprovide resistance to the motion of all body parts.threedimensional visual output provides other interesting research issues.one is the development of devices to present threedimensional visual output thatare less cumbersome than the electronic helmets often used today. a second issuecuts across all problem domains and yet depends on the specifics of each domain:many appealing examples of "virtual reality" displays have been proposed andeven demonstrated, but conceiving of sensible mappings from raw data to imagesdepends very much on the application. in some cases, the sensible mappings areobvious. a visual flight simulator simulates the aircraft dynamics in real time andpresents its output as images the pilot would see while flying that airplane.(along the lines of the discussions above, the sounds, motions, control pressures,and instruments of the simulated aircraft may also be presented to the pilot. thesesimulations are so realistic that an airtransport pilot's first flight in a real aircraftof a given type may be with passengers.10) but in other cases, such as dealingwith abstract data, useful mappings are not at all obvious. what, for example,might be done with the reams of financial data associated with the stock market?summary and conclusionsthe core research agenda for cs&e has been well served in the past by thesynergistic interaction between the computer industry,a core cs&e research agenda for the future113computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.the companies that are the eventual consumers of computer hardware, software,and services, and the federal researchfunding agencies. as a result, cs&eresearch exhibits great diversity, a diversity that is highly positive and beneficial.in turn, this diversity allows a broad range of challenging problems andopportunities to be addressed by cs&e research. thus, although the committeecannot escape its obligation to address priorities and to provide examples ofresearch areas that it believes hold promise, it must be guarded in its judgment ofwhat constitutes today's most important research.that said, the committee believes that major qualitative and quantitativeadvances in several dimensions will continue to drive the evolution of computingtechnology. these dimensions include processor capabilities and multipleprocessor systems, available bandwidth and connectivity for datacommunications and networking, program size and complexity, the managementof increased volumes of data of diverse types and from diverse sources, and thenumber of people using computers and networks. understanding and managingthese changes of scale will pose many fundamental problems in computer scienceand engineering, and using these changes of scale properly will result in morepowerful computer systems that will have profound effects on all areas of humanendeavor.notes1. the definition of which subareas of cs&e research constitute the "core" is subject to some debatewithin the field. for example, the computer science and technology board report the nationalchallenge in computer science and technology (national academy press, washington, d.c., 1988)identified processor design, distributed systems, software and programming, artificial intelligence,and theoretical computer science as the subfields most likely to influence the evolution of cs&e inthe future, noting that "the absence of discussion of areas such as databases does not mean that theyare less important, but rather that they are likely to evolve further primarily through exploitation ofthe principal thrusts that [are discussed]" (p. 39). in its own deliberations, the committee includedsuch areas in the "core" of cs&e, motivated in large part by its belief that their importance is likely togrow as cs&e expands its horizons to embrace interdisciplinary and applicationsoriented work.2. computer science and technology board, national research council, the national challenge incomputer science and technology, national academy press, washington, d.c., 1988; john e.hopcroft and kenneth w. kennedy, eds., computer science: achievements and opportunities,society for industrial and applied mathematics, philadelphia, 1989; avi silberschatz, michaelstonebraker, and jeff ullman, eds., "database systems: achievements and opportunities,"communications of the acm, volume 34(10), october 1991, pp. 110œ120; computer science andtechnology board, national research council, scaling up: a research agenda for softwareengineering, national academy press, washington, d.c., 1989.a core cs&e research agenda for the future114computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.3. hopcroft and kennedy, computer science: achievements and opportunities, 1989, p. 72.4. a simple simulation argument shows in general that superlinear speedup on homogeneous parallelsystems (i.e., systems that connect the same basic processor many times in parallel) is not possible.superlinear speedup would involve, for example, applying two processors to a problem (or to aselected class of problems) and obtaining a speedup larger than a factor of two. in addition, for manyinteresting applications it turns out that even linear speedup is impossible even when the machinedesign should "in principle" allow linear scaleup. both the limitations of real machines and the issuesof what scaling implies for problems from the physical world make even linear scaleup impossible formany real problems.5. for example, parallel processors are emerging as effective search engines for terabytesizedatabases. automatic declustering of data across many storage devices and automatic extraction ofparallelism from nonprocedural database languages such as sql are demonstrating linear speedupand scaleup. teradata, inc., has demonstrated scaleups and speedups of 100:1 on certain databasesearch problems.6. distributed computing refers to multipleprocessor computing in which the overall cost orperformance of a computation is dominated by the requirements of communicating data betweenindividual processors, rather than the requirements of performing computations on individualprocessors. parallel computing refers to the case in which the requirements of computations onindividual processors are more important than the requirements of communications.7. transit time is important to gigabit networks because the arrival of messages at a given node is astatistical phenomenon. if these messages arrive randomly (i.e., if the arrival times of messages arestatistically independent), the node can be designed to accommodate a maximum capacity determinedby wellunderstood statistics. however, if the arrival time of messages is correlated, the design of thenode is much more complicated, because "worst cases" (e.g., too many messages arrivingsimultaneously) will not be smoothed out for statistical reasons.in gigabit networks, the networkswitching and messagequeuing time for small files will be muchsmaller than the transit time. the result is that the endtoend transmission time for all messages willcluster around the transit time, rather than spread out over a wide range of times as in the case oflowerspeed networks.8. the impact of software testing on product schedules has been known for a long time. in 1975, fredbrooks noted that testing generally consumed half of a project's schedule. see frederick brooks,the mythical manmonth, addisonwesley, reading, mass., 1975, p. 29.9. humancomputer interaction is a very broad field of inquiry, some other areas of which arediscussed in box 2.8 in chapter 2. humancomputer interaction is highly interdisciplinary, drawingon insights provided by fields such as anthropology, cognitive science, and even neuroscience todevelop ways for computer scientists and engineers to maximize the effectiveness of theseinteractions.10. for example, the flight simulator for the a320 airbus is sufficiently sophisticated that pilots canreceive flight certification based solely on simulator training. see gary stix, "along for the ride,"scientific american, july 1991, p. 97.a core cs&e research agenda for the future115computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.4education in cs&eas mentioned in chapter 1, the strong connection of cs&e to computingpractice, both in terms of new computing technologies (the province of thecomputer industry) and the application of computing to problems outside thecomputer industry, results in a certain tension between researchers and productdevelopers. this tension has its counterpart in different conceptions of what acs&e education should involve.academics tend to believe that cs&e education should provide a basis forlater careers by emphasizing fundamental principles, effective knowledge, andskills. a cs&e education cannot provide comprehensive exposure to allcomputing problems that might be encountered later on, but rather should providea good foundation on which to build. with a good understanding of the basicintellectual paradigms of cs&e, graduates can more fully exploit computingwherever they are employed. from this perspective, the role of cs&e educationis to open doors for later exploration by students, but not to lead them throughthose doors.while cognizant of and even sympathetic to this view of education,individuals from outside academia have a different perspective. individuals fromthe computer industry want employees who can apply the fundamentals of cs&eto the creation of marketable products. thus the computer industry recruitsheavily in the cs&e departments of major universities, is generally satisfied withgraduates'education in cs&e116computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.technical knowledge, but notes their lack (especially among undergraduates) ofgood communications and teamwork skills and the reluctance of ph.d.s to workon product development.commercial users of computing are further removed from academic cs&e.they are interested in the most effective use of currently available technology,and they have to maintain compatibility with substantial previous investments;consequently, research is usually of less immediate interest to them. their needsfor employees tend to emphasize the operational and practical. thus they maybelieve that cs&e education is somewhat marginal to their needs.these sentiments are consistent with testimony received by the committeefrom representatives of large commercial users of computers.1 engineering firmsoutside the computer industry stated that they often preferred other engineeringmajors over cs&e majors for computing jobs, because of their grounding inengineering as a way of thought. several servicesector firms told the committeethat, after onthejob training, majors in other disciplines (including music) oftenperformed as well as or better than computer science majors and had broaderperspectives as well. these firms appeared to treat cs&e bachelor's degrees asterminal at best, expecting onthejob learning to be more relevant to their needsthan further formal education.although much can and should be done to stimulate discussion betweenindustry and academia regarding the appropriate content of cs&e education, thedifferences in perspective will never be fully reconciled. the committee doesbelieve that fundamental knowledge, basic concepts of broad applicability, andgeneral techniques of analysis and synthesis are best taught in the universityenvironment, while knowledge of more specific relevance to particular industrialor commercial settings is best taught "in house," although the lines between thesecategories are often unclear.an appreciation for the perspective of both academia and industry sets thecontext for the remainder of this chapter. on balance, the committee believes thatfor a field as young as cs&e and that is advancing so rapidly, education in cs&ehas many strengths. for example, in many universities, cs&e attracts more thanits share of the best students (as measured by sat scores, gre scores, andgraduate fellowships awarded by the university).2 graduate education in cs&e,which is older and more established than undergraduate education in cs&e, isheld in high regard by the universities and industrial research laboratoriesresponsible for hiring many of cs&e's graduates. and the significant advances incomputing over the years can be attributed in part to the educational system thatproduced the people making those advances.education in cs&e117computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.at the same time, cs&e education exhibits much greater variability in tone,emphasis, and quality than does education in other more mature scientificdisciplines, such as physics or chemistry, in which core curricula (especially atthe undergraduate level) have had much more time to evolve. this variability andother aspects of cs&e education at all postsecondary levels are discussed in thischapter.undergraduate education in cs&eundergraduate education for cs&e majorsthe variations in cs&e education are greatest at the undergraduate level.some undergraduate programs concentrate on the intellectual framework neededto cope with rapid change and pay less attention to practical skills. someprograms concentrate on practical skills but include enough fundamentals that thestudent is well prepared for the future. still others have not changed theircurricula for over ten years and consequently produce students who are alreadybehind the times when they graduate.these differences are particularly apparent for introductory courses. whileintroductory courses for most scientific and engineering disciplines exhibit arelatively high degree of uniformity in content and approach, universitylevelintroductory courses in cs&e exhibit striking variation. some emphasize newerconcepts in functional programming, logic programming, or objectorientedprogramming. others teach no theory and are focused more on teachingprogramming languages than on programming itself, and still others emphasizetheories of program correctness and programming methodology.some diversity at the introductory level is appropriate and desirable, as longas the diversity results from informed choice on the part of faculty. but to theextent that this diversity reflects a lack of current knowledge about the field, it isundesirable. cs&e is a rapidly advancing field, and it is important for those whoteach the subject to maintain currency in it. thus it is worth looking at the facultywho teach cs&e throughout the nation. notably, of the 1000 or so u.s.institutions that have a cs&e undergraduate program, only 15 percent areph.d.granting institutions for cs&e. by and large, ph.d.granting institutionsare where the bulk of academic research is performed, and thus it is reasonable toexpect that faculty at these institutions generally have a more currentunderstanding of new research than do faculty at institutions that do not grantph.d.s. of course, graduates from many liberal arts colleges can receive quitegood educations in computer science. but on the whole, the disparieducation in cs&e118computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.ty in undergraduate cs&e education available at ph.d.granting vs. nonph. d.granting institutions in cs&e is accentuated by two factors.the first factor is that, contrary to the norm in other disciplines, well overhalf of the cs&e faculty in nonph. d.granting departments do not havedoctorates in cs&e (see table 8.12 in chapter 8).3 since a major function of theph.d. degree program is to socialize graduate students into the culture of thediscipline, faculty who teach cs&e without an advanced degree in cs&e may behandicapped in presenting the discipline's mindset and key concepts.moreover, newly graduated ph.d.s who go to nonph. d.grantinginstitutions bring along their own recent work, which at least for a while booststhe currency of the receiving institution. a nonph.d.granting institution thathires no ph.d.s in cs&e will find it considerably more difficult to maintain anuptodate curriculum, since to keep current it will be forced to rely on journalsand publications, which, as noted in chapter 1, are less effective than people intransferring technology.aggravating problems of faculty currency is the relatively slow rate at whichit is possible to change the content of undergraduate courses. while new ideasgenerated by research institutions may influence undergraduate education at thoseinstitutions within a year or two, those ideas may take much longer to propagatebeyond local boundaries.4 thus the problem of faculty currency in undergraduateeducation is more acute in cs&e than in older disciplines like mathematics orphysics, which have had a sound and stable foundation for decades.the second factor, noted also in the section "synergy enables innovations"in chapter 5, is that parts of cs&e are strongly driven by the pace of technology.some cs&e research (e.g., in parallel programming and in graphics) depends onthe availability of stateoftheart equipment, which is expensive and thus muchless accessible to nonph.d.granting institutions. although this situation maychange when highspeed networks become available to link universities andcolleges, the widespread availability of such links is many years in the future. as aresult, even those cs&e faculty at nonph.d.granting institutions who wish tokeep current may find it difficult to do so, since the equipmentacquisitionbudgets of these institutions are likely to be more limited than those of theph.d.granting institutions.5a similar consideration holds for cs&e education; resourcepoordepartments find it difficult to maintain a current educational computerinfrastructure. for entirely financial reasons, a cs&e departeducation in cs&e119computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.ment may slight important concepts that are missing from the software of itscomputing environment. for example, the basic notion of recursion appeared inalgol and lisp in the 1960s but did not find its way into fortran or basic for over15 years, and departments basing their curricula on these older languages oftendid not cover this important concept. as a more modern example, fewundergraduate programs teach parallel programming, because parallelprogramming is difficult to teach without access to parallel computers. in somecases, even when suitable software is available, the cost (in terms of money andtime) to acquire it and adapt courses to it may be prohibitive. functionalprogramming and objectoriented programming are examples of major newconcepts that may not be taught for this reason.6variations in quality and outlook in undergraduate cs&e education haveenormous impact on the strength of academic cs&e and also on computingpractice. holders of cs&e bachelor's degrees move on to graduate school, tojobs that center on computing practice per se (programming, softwareengineering, and so on), and to fields such as business, law, and economics. theprinciples, viewpoint, skills, and techniques that are taught to undergraduatemajors (especially in the nonph.d.granting institutions) have an enormousimpact on how they later practice computing,7 affecting programming, softwareengineering, and the transfer of academic research in cs&e to industry.with this perspective in mind, the committee identified several areas ofconcern in undergraduate cs&e programs as they are generally constituted.rigor and clarityaccording to testimony received from industrial representatives by thecommittee, new hires in programming and software engineering seldom approachtheir tasks with sufficient rigor, whatever their baccalaureate degree. they fail tobe precise, do not consider thoroughly all aspects of a problem (e.g., what shouldbe done when unexpected data are received), are unable to see a problem fromseveral viewpoints, and do not know when and how to abstract. why is this?partly, the committee believes, because of a lack of rigor in early parts of thecs&e curriculum.it is important to distinguish between rigor and formalism. the intent ofrigor is precision and thoroughness. using formalisms such as mathematics andformal logic may help achieve rigor, but one can be rigorous without usingformalisms. mathematics is a rigorouseducation in cs&e120computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.discipline, but it is not always formal. for example, many theorems and proofsare given in english instead of in a formal notation, and intermediate steps areomitted if it is felt that the reader can fill them in. cs&e has perhaps evenstronger requirements for rigor than does mathematics, stemming from the wealthof detail and complexities that arise in algorithm design and software or hardwareengineering. students must especially understand that rigor and clarity areessential for the specification, design, and implementation of softwarecomponents.the committee was unanimous in its view that many undergraduates do notlearn to approach software problems in a disciplined or systematic manner (e.g.,they jump into coding before they understand a problem adequately, or they patchtheir programs locally without understanding how those patches affect the globalstructure or function of the program). this view is reinforced by evidence ofenormous variation in the productivity of programmers.8 one of the reasons forthis variation may be an immature educational process.rigor and clarity in specification and design are especially important, since aspecification is an implicit contract between the customer (who takes thespecification as the contract to be filled) and the software engineer (who uses it todesign and implement the software). having a rigorous and clear specification atany given moment allows both sides to evaluate much more easily how suitablethe specification is, how much it will cost to implement, and what any requestedchange in the specification would cost. this last point is particularly importantbecause requests for changes in specifications are made quite often during thedesign, development, and implementation processes. rigor and clarity are alsonecessary during the implementation phases of software engineering, and theirimportance increases for larger and more complex projects.the qualities of rigor and clarity must be learned early and reinforcedthroughout the curriculum; they cannot easily be taught as mere addons in latercourses. even an introductory cs&e course can communicate the need for rigorand clarity, as described in box 4.1.taught in isolation, more theory will not lead to rigor or clarity and may beviewed by students as nothing more than academic exercises to fill thecurriculum. however, programming and the understanding of programs taught asan ad hoc exercise, without underlying foundations and principles, is just as bad.instead, throughout the undergraduate cs&e curriculum, practice and theoryshould be integrated, without an artificial distinction made between the two and ina way that achieves the necessary mindset of rigor and clarity.education in cs&e121computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.box 4.1 rigor in the first programming courseeven a first programming course can communicate the need for rigorand clarity. for example, students can be taught the importance of craftingclear and precise specifications for all program segments in a way thatdeals suitably with error conditions and boundary conditions. the learningprocess should include the task of making imprecise specifications precise.also instructive is the requirement that students provide in variabledeclarations englishlanguage descriptions of variables that show theirrelation to other variablesšessentially assertions that hold in strategicplaces of the program. few introductory texts on programming teach ordemonstrate such a rigorous approach.mathematics and formalismlearning mathematics serves two purposes. first, it imparts mathematicalmaturity, which is one way of developing an appreciation for rigor. second,mathematics is central to many subfields of cs&e. discrete mathematicsunderlies correctness of programs and compiler construction as well as chipdesign, and certain branches of mathematics such as logic and algebra arefoundational to some of the more theoretical aspects of cs&e. thus lack ofsufficient mathematics will limit the horizons of cs&e students.moreover, as discrete mathematics (e.g., logic, set theory, graph theory) hasfound its way into the cs&e curriculum, continuous mathematics (e.g., calculus,differential equations, statistics) has been slighted. this is unfortunate, becausecontinuous mathematics is essential in important subfields in cs&e such asperformance analysis, computational geometry, numerical analysis, and robotics.further, continuous mathematics is the language of many scientific andengineering fields, and an adequate understanding of continuous mathematics isneeded to approach computing applications in such areas with confidence.mathematics also underpins the use of formalism, some degree of whichshould be an essential aspect of an undergraduate cs&e curriculum. however,formalism should not be taught for its own sake, but rather as a tool thatstimulates understanding and provides help in solving problems. for example,very basic formal logic can be used as a tool in writing clear and rigorousspecifications, in reasoneducation in cs&e122computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.ing about problems, and in switching circuit theory. formal languages can beused in designing not only programming languages but also applicationspecificlanguages. automata theory can help to explain aspects of languageimplementation as well as concepts necessary for the analysis of the executiontime of programs. the study of formal methods in programming can change howone views the programming process and makes one better equipped to berigorous, thorough, and clear in later work, whether or not formal methods areactually used in the development of software systems.it has been said that the best mathematics has just the right balance betweenformalism and common sense. the same holds in computer science. however,without suitable education and training in formalism, this balance cannot beachieved, and a formal component of undergraduate cs&e education is thereforenecessary.current attitudes are something of an impediment to a better integration ofmathematics into cs&e. cs&e students often dislike mathematicsšsome seemto have chosen cs&e as a less formal alternative to mathematics or hard science,simply because they find mathematics intimidating. the challenge for cs&eeducators is to integrate mathematics into the cs&e curriculum in a way thatbuilds and reinforces respect for mathematics as it contributes to the discipline.breadthas argued in chapter 2, a broader definition of cs&e is necessary if thefield is to continue to prosper intellectually in the years ahead. computerscientists and engineers need to understand areas outside cs&e to enlarge theirown perspective and so that they can work with others more effectively.furthermore, a student with some substantive competence in an applications areawill be much more capable of designing software and hardware suitable for use inthat area.students should also have broadly integrated views of theory and practice(so that students do not become narrowly entrenched in either) and of hardwareand software (since today's computer systems are designed as a mixture ofsoftware and hardware, and the successful computer or software engineer musthave an understanding of both).finally, the various existing views of programming should be integrated.the venerable programming style characterized by the procedural approach oflanguages such as fortran and algol has since been joined by others: (1) logicprogramming and declarative proeducation in cs&e123computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.gramming, in which the intent is to let the specification be the program; (2)functional programming, which simplifies reasoning about programs; (3) objectoriented programming, which generalizes data modeling by encapsulating dataand the functions that operate on the data; (4) concurrency, which copes with therealworld of multiple simultaneous activities; and (5) parallelism, which gainsspeed and capacity by bringing multiple computing elements to bear on a singleproblem.all of these styles are important, not so much as alternative techniques butas aspects of the general problem of programming. some of them, so oftenregarded as distinct, have been successfully integrated into common frameworks,for example in the programming languages scheme and ml. these differentprogramming paradigms should be integrated into the undergraduate curriculum.successful integration of these paradigms into the undergraduate curriculum willallow the student to view them not as competing methods with little in common,but as a continuum of complementary tools that build on each other.broadening in all these ways puts pressure on the undergraduate curriculum;there is simply no room for all the broadening as long as the rest of the curriculumstays the same. part of the problem is that the curriculum does not build on itselfenough; too many courses have few prerequisites and are devoted to studyingartifacts rather than establishing foundations and teaching enduring principles.9 inparticular, undergraduate cs&e curricula typically include a large number of''systems" coursesšcompilers, operating systems, database systems, datacommunication, graphics, and so on. the challenge will be to teach both thescience and the engineering in more comprehensive courses unfettered by thistaxonomy. students should come to understand these artifacts as applications ofunifying principles. for example, central concerns in compilers may beconsidered as laboratory examples under the headings of state machines and ofvarious programming constructs. graphics ties in to advanced calculus. operatingsystems and data communication illustrate concurrency and queuing theory.however, despite these comments, it should be recognized that achievingbreadth is at least as much an issue of culture, mindset, and expectations as oneof specific courses to fulfill some additional breadth requirements. if indeed theidea of a broader agenda for cs&e is fully embraced by cs&e faculty, formerly"pure" cs&e courses will make use of examples drawn from other fields,stressing connections between cs&e and the outside world.education in cs&e124computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.the limits of a fouryear programacknowledging the pressures on the undergraduate cs&e curriculum ledthe committee to conclude that the undergraduate curriculum cannot meet alldemands. for example, it can provide only partial preparation for softwareengineering positions in industry, because there is simply not enough time toteach all that is necessary. the committee believes that paying more attention torigor and to foundations in the early part of the curriculum and paying lessattention to artifacts can make the educational program more efficient, leaving,perhaps, some room for broadening (see chapter 2). nevertheless, there is littletime to devote to issues that arise in the development of large software systems,such as teamwork, system testing, software maintenance, and version control.also, more advanced topics relating to other disciplines (e.g., mathematics,humanities, arts and sciences, engineering) cannot be incorporated because oflack of room in the curriculum and insufficient background.there is a place in industry for the person with a bachelor's degree incs&e, especially in organizations that provide their own training, but it isimportant that industry not expect too much from someone with such a degree.recognizing the basic limitations of a fouryear curriculum, it is time to put moreemphasis on a master's or masterofengineering degree as a professional degree,as discussed below.undergraduate service educationcomputing has become so pervasive throughout society that a basicunderstanding of computing is becoming essential for all educated citizens. allpeople should understand the ramifications of computing in society and shouldhave a basic (if simple) technical understanding of computers and computing, inmuch the same way that citizens should both be literate and have basic numeracyskills. computer scientists and engineers, as the vanguard of this informationage, have a responsibility to explain its implications to the general public.in addition, computing has become as important as mathematics to scienceand engineering. specialists in other science and engineering fields are beginningto understand that cs&e is more than fortran programming and to recognizethat an understanding of the computer scientist's or engineer's approach toalgorithms and information may be useful in their own areas (see boxes 4.2 and4.3; comparable though not identical boxes could also be constructed forchemists, biologists, earth scientists, and others). as a result, severalcomputational science and engineering programs have been developed.education in cs&e125computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.box 4.2 a 1982 view of why physicists should learnsome computer sciencecomputing has become an integral part of the practice of physics, fromexperimental control to plotting graphs, from simulations to typesettingpapers. physicists well versed in the methods of computer science can usecomputers to advance science more effectively. computing knowhow canhelp the physicist in: experimental control: running an experiment with various detectors andinstruments operating simultaneously is similar to keeping a timesharingoperating system running. data reduction: interactive statistical packages and computer graphicsenable a scientist to understand the implications of an experiment muchsooner than line printer graphics on a batch computer. discrete simulations: many physical problems involve more thansolutions to differential equations. group theory, nonrectangularlattices, and other components of modern problems can be approachedmore easily using ideas from graph theory and other computing fields farremoved from the typical fortran program. numerical simulations: many numerical problems are now consideredsolved, in the sense that commercial software packages are availablethat give accurate solutions efficiently. symbolic manipulation: using computers to solve algebraic problems ismore a computing task than a mathematical one, and using an algebraicmanipulation system requires a thorough understanding of theprogramming art, as well as one of mathematics.a physicist need not be an expert computer scientist, but wellinformeduse of computers can be more effective than the ad hoc use that pervadesthe physics community. knowing what computers can do and how they doit enables physicists to constructively criticize researchbased on computermethods. physicists need both healthy skepticism about the possiblyerroneous output of computer programs and an appreciation that computerscan solve problems that are unmanageable by other means.in addition, with computers controlling and interpreting so much inmodern experiments, there are important issues in the design of largeexperiments that depend on understanding of a computer's capabilities forproper solution. without some experience in realtime programming, anexperimentalist might be surprised that the online processing power isinsufficient to keep up with the data flowing from the apparatus; forexample, the processor may be too busy servicing i/o interrupts keepingthe experiment running to perform firstorder event rejection.education in cs&e126computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.good algorithms can mean the difference between an unsolvableproblem and an tractable one. in percolation problems, the obviousalgorithms execute in a time proportional ton2, where n is the number ofoccupied sites in a cluster. algorithms based on sets and graphs solve thesame problem in time proportional to n log n. on problems involving amillion sites, a 50year calculation can be reduced to hours.finally, though many physicists believe that fortran is the languageof choice for many problems, it has many limitations when compared tomodern programming languages that make programs easier to write,understand, and debug. for example, fortran lacks facilities forstructuring data. an ideal gas particle is characterized by a position 3vector, a velocity 3vector, and a mass. in fortran, these variableswould be stored in separate arrays, and a particular particle identified by anindex into the arrays. in a modern language, the variables of each particlecan be grouped together in a single structure and treated as a unit. forexample:structure particle{ float pos[3];float vel[3];float mass;character particledescription[10] };the particle structure contains all the information about a particle in asingle place, and the different components of the data have names withmnemonic significance.source: adapted from robert pike, physics vs computer science,at&t technical memorandum, mh11271rcpunix, october 11, 1982.presented at workshop on software in high energy physics cern 8212.cs&e education has a major service role to play. but for a variety ofreasons, cs&e has been unable to fully meet its obligations in this role. themost significant reason has been the enormous load placed on cs&e departmentsto teach their own majors. for example, in the mid1980s, at some institutions theaverage cs&e faculty member was teaching twice as many credit hours as his orher counterpart in engineering disciplines, and the ratio of degrees awarded perfaculty member in cs&e is still more than twice that of all scieducation in cs&e127computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.box 4.3 a 1982 view of some computer science forphysicists to knowmachine models. what a computer can do; the notion of aninstruction; recursion and iteration; basic complexity theory. writing goodprograms requires some understanding of what computers are, how theyperform computation, and how long the computation will take.algorithms and data structures. basic algorithms; sorting; graphtheory; abstract data types. the simplest solution to a problem may be verydifferent from a do loop, and the data to describe parameters in the problemmay be represented poorly by integers or arrays. some understanding ofthe basic algorithms and data structures of computer science can makesimple solutions easier to find.programming languages and parsing. fortran viewed as onelanguage, not the only language; pascal or some other language with datastructures; parsing techniques. learning two or more languages gives amuch better understanding of what a computer can do, and how to make itdo it. as with the wave and matrix formulations of quantum mechanics,some problems are easier to handle in one language than in another. thefortran part of a course should be handled as necessary background,teaching not only how to use fortran but how to avoid it when possible,or at least overcome its inadequacies. physicists sometimes write specialprograms that read some input language, and knowledge of parsing theorymay help in designing a language that can be easily handled by both thephysicist and the computer.numerical analysis. basic error analysis; what numerical problemscomputers can solve; where to get subroutines to do the job. it is muchmore important to give guidelines for finding a solution than to develop theskills to write libraries of numerical analysis subroutines. physicists must beaware of what problems are solvable by current numerical software, whatcommercial subroutines are available, and how to express their problems ina form suitable for solution.operating systems and realtime programming. i/o architecture;interrupts; realtime processing; multiprogramming. modern experimentsrequire computer control, and a physicist needs at least the basic notions ofhow a computer can control a machine. interrupts allow a processor toservice multiple i/o ports conveniently, but a program that spends muchtime servicing interrupts may be unable to keep upeducation in cs&e128computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.ence and engineering disciplines taken together (see figure 8.5 inchapter 8).10with the incoming data. operating systems face the same sorts ofproblems, and the techniques developed to solve themšprocesses andinterprocess communication, buffering, and so onšare directly applicableto realtime control of experiments.graphics. bitmap and vector displays; interactive graphics; datadisplay; 3d graphics. physics describes the interrelationships between thevariables in a system, and graphs are used constantly to present andexplore relationships. computers are good at drawing graphs on paper, butthey can also be used interactively to explore dynamically the properties of afunction or the parameters of a system in pictorial form. as with numericalmethods, familiarity with commercial software is more important than beingable to create new graphics packages.source: adapted with minimal change from robert pike, physicsvs computer science, at&t technical memorandum, mh11271rcpunix, october 11, 1982.the bulk of service offerings should stress the cs&e necessary for effectivecomputing practice. service offerings should teach proven techniques, models,and principles, with the intent of promoting and enhancing the utility ofcomputing in general. they should not take on the appearance of either"computer appreciation" or vocational training; the needs of discipline x forinstruction in specific languages, software packages, or "computational x" willtypically best be met from within that discipline.the reach of service activities may be multiplied by computer scientistshelping, through consultation and teamwork, computeroriented courses in otherdisciplines to adopt unifying terminology and viewpoints from computing. theconcomitant interaction with faculty in other disciplines may incidentally revealinteresting computing problems. it cannot help but further the goal of broadeningcs&e.finally, cs&e should actively promote minors in cs&e (and doublemajors). society needs people in all fields who have a deep enough understandingof computing to apply computing effectively and effieducation in cs&e129computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.ciently, but such understanding does not come from one or two courses. a minorin computer science will more suitably satisfy this need.unfortunately, double majors and minors are usually only for the most giftedand hardworking students. another approach would be to develop a joint programbetween cs&e and a discipline x that uses computation heavily. both cs&e andx would relax some of their degree requirements so that a suitable degree in"computational x" could be developedšone strict enough to satisfy conventionalstandards for a degree but relaxed enough so that a normal student could handlethe requirements.the master's degree in cs&eas discussed above, a fouryear undergraduate curriculum is inadequate tosatisfy many of the needs of industry. a fouryear program is simply unable toprovide the kind of professional training that industry often desires in softwareengineering, management, and the like. accordingly, the committee believesmore emphasis should be placed on a master's degree or masterofengineeringdegree in which this professional training could be achieved, even though itrecognizes the difficulty of implementing such a change in many institutions.a master's degree can also be used to develop deeper understanding aboutthe nature of cs&e as an intellectual discipline, as a prelude to possible ph.d.work, or to broaden the perspective of a student who has previously focused in anarrow way on cs&e. both deepening and broadening could be synergisticallyachieved by requiring a design project that involves some substantial topic withincs&e (for deepening) or some topic outside cs&e (for broadening). such adegree might be especially useful to students graduating from fouryear liberalarts programs that have a relatively large number of electives.the undergraduate curriculum should be a foundation for professionalachievement. a master'slevel program and continuing education programs couldmore closely approximate programs that do provide professional certification. aprogram of mutual exploration between academia and industry could result inmaster's programs that would better suit industrial needs.certain individual companies and organizations do recognize the value ofmaster's degree programs. before its divestiture, at&t was famous for its oneyearoncampus (oyoc) program, which supported hundreds of new hires eachyear in master's degree programs in cs&e, electrical engineering, physics, andneighboring fields. bellcoreeducation in cs&e130computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.has recently teamed with carnegie mellon university to establish a master'sdegree program that covers computer networks, business, and management. thesoftware engineering institute in pittsburgh, sponsored by the department ofdefense, is engaged in developing curricula for master's degree programs insoftware engineering. still, the committee believes that industry would be welladvised to seek out cs&e master's degree holders for software development andengineering positions to a much greater extent than it does now.the ph.d. degree in cs&ecs&e graduate education at the ph.d. level is today characterized bytraditions that have many elements in common with those of mathematics andmany of the natural sciences. the graduate course work in most cs&e programsis concentrated in the core areas of cs&e with little exposure to otherdisciplines. a doctoral dissertation is intended to demonstrate a student'sindividual ability to conduct original research in the field's "core" areas. it tendsto emphasize work on manageable problems that are selfcontained, with cleanformulations. since original contributions are highly valued, both teamwork andincremental improvements (however substantial or important) are less valued.within this intellectual environment, it is difficult to do interdisciplinary work,and applicationsoriented work is not fully appreciated. as a result, the typicalcs&e ph.d. is usually not well matched to many industrial jobs (e.g., those inproduct development) that call for work in a team environment in whichrelevance rather than originality or intellectual achievement is the most importantcriterion.11 (box 4.4 describes the sentiments of the council on competitivenesson this subject.)however valuable the traditional approach to ph.d. education has been inthe past, holding it as the only good model for ph.d. education unnecessarilylimits the scope of the field. for example, the committee believes that ph.d.students in cs&e should be permitted and even encouraged to do dissertationresearch in some interdisciplinary or applicationsoriented problem area.substantial research that builds on previous accomplishments should likewise beconsidered appropriate for dissertation work. ph.d. students exposed to problemareas of interest to industry or other disciplines would be better prepared forgrappling with the realities of implementation, problemsolving, and perhapscommercialization in a practical context, and their supervising faculties wouldhave a better appreciation of these needs through the work of their students.moreover, if ph.d. students are encouraged in this manner, those who becomefacultyeducation in cs&e131computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.will be more likely to encourage their own students in such a direction, thuspromoting the broader research agenda advocated in chapter 2.box 4.4 on educationša view of the council oncompetitiveness"america's research universities constitute a great national asset, buttheir focus on technology and competitiveness is limited. u.s. universitiesproduce firstrate scientists and engineers and conduct pioneering researchthat lays the foundation for many advances in technology. however, theirfocus on . . . preparing future scientists and engineers for the needs ofindustry . . . has been inadequate."source: council on competitiveness, gaining new ground:technology priorities for america's future, washington, d.c., 1991, p. 3.there are, of course, difficulties with establishing interdisciplinary ph.d.programs. does the student have to demonstrate substantial contributions to bothfields? is a novel application in another discipline of known cs&e techniquesworth a ph.d.? this issue is being faced not by cs&e alone but by all scientificand engineering fields, as more interdisciplinary work is being promoted. onlythrough serious and concerted effort can such questions be answered, althoughthe committee believes that he criterion that a piece of work exhibit demonstrableintellectual achievement should not be abandoned.in some universities, breadth outside cs&e is promoted by requiring for theph.d. a minor outside cs&ešusually, the equivalent of two to four courses atthe graduate level in some other field. typically, the field is mathematics orelectrical engineering. the committee believes that ph.d.granting departmentsshould require an outside minor. further, the range of possible minors should beexpanded to include not only science and engineering fields but also fields suchas economics and finance.finally, "systems" ph.d.s appear at present to be in much shorter supply than"theory" ph.d.s. universities and industry compete much harder for systemspeople than for all but the most outstanding theoreticians, and there is stronganecdotal evidence that theoreticians have a much harder time finding suitablepositions. it is worth asking whether the dichotomy has not become too wide.efforts should be made to reduce the distance between theory and practice, todevelop researchers who can do both. most computer scientists andeducation in cs&e132computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.engineers and most departments would be stronger if the important interplaybetween analysis and construction were emphasized.employment expectations for holders of cs&edegreesformal education imparts values as well as knowledge to students; thesevalues structure what students at all levels regard as important and determinewhat graduates see as the boundaries of cs&e. a broader perspective on thediscipline for cs&e students will require a correspondingly different culturalsocialization process.a narrow perspective of the discipline affects student perceptions aftergraduation. in particular, it is commonly believed that the best jobs for cs&eph.d.s are in academic or industrial positions that allow the graduate to doresearch as a major portion of those jobs, and that the best cs&e undergraduatestudents inevitably go to graduate school in cs&e. these beliefs, thoughunderstandable, are fundamentally part of a disciplinary culture that looksinward.as an alternative, cs&e departments could present the information age asan opportunity for many of their graduates to make their mark on the world atlarge. ph.d.s (even some of the best ones) can and should be encouraged to takeindustrial positions in which they invent new technology or facilitate technologytransfer, working on intellectually challenging problems that are also directlyrelevant to our economic wellbeing;12 teaching positions at nonresearchinstitutions are another possibility. bachelor's degree holders (even some of thebest ones) should be encouraged to take jobs that make broad rather thanspecialized use of their cs&e backgrounds. beyond specific facts andtechniques, cs&e does teach important ways to analyze and solve problems, i.e.,to think. taught with sufficient fundamental foundations and breadth, a cs&edegree program should prepare the student for a nontechnical position in businessat least as well as a program in any other scientific or engineering discipline.13but even in 1989, only 363 of the 648 cs&e ph.d. recipients (56 percent) wereanticipated to take positions working in cs&e.14continuing educationmany of the people now doing software engineering in industry have hadlittle exposure to recent developments in cs&e. because of the rapid changesand advances in the field, the committee believes that continuing education forthis population is extremely importanteducation in cs&e133computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.for the health of the industry. software engineers using only techniques and skillslearned ten or more years ago cannot in general be reaching their full potential.objectoriented programming, functional programming, and many other ideasšas well as languages that embody themšhave come to fruition and in suitablecontexts can be used far more profitably than older concepts and notations.for ideas such as these, the committee believes that academia is the bestvehicle for continuing education. however, it recognizes a meaningful role forcontinuing education based outside academia to expose practitioners toincremental improvements in new technologies, i.e., improvements that will allowpractitioners to do their jobs better but that will not require substantial changes intheir approach to their jobs.given the speed with which cs&e changes, reeducating the work force is animportant task. it is also a large and difficult task, for the potential need anddemand for continuing education in cs&e is enormous. an estimated 800,000individuals were employed as computer specialists in 1991.15 if the number ofthese individuals remained constant, and each individual required one semesterlength course every five years and if 25 students constituted one course, therewould be a demand for 3200 nonintroductory courses per semester.16in spite of the fact that some companies make huge investments in theeducation of their employees (for example, the investment in continuingeducation at ibm is estimated to be $1 billion per year), the committee believesthat the united states underinvests in keeping its employees technically current.many professionals take no courses while on the job. further, many companiesare apparently unwilling to invest in continuing education. moreover, much of thecontinuing education that does go on in industry is management oriented and nottechnical.part of the negative feeling toward education for employees comes from ashortterm approach to profits and to the mobile work force. if employees aregoing to resign for another position elsewhere in two or three years, goes theargument, why give them education? if activities do not contribute to the bottomline three months from now, how can they be justified? such attitudes willultimately work to the detriment of the field, and they are also the opposite ofattitudes in europe and japan.even worse, many universities and cs&e departments ignore continuingeducationšpartly because of a lack of resources to deal properly with continuingeducation and partly because of a value system that places such education at thebottom of the list of valued activities. the committee believes that the academiccs&e community shouldeducation in cs&e134computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.work more intensively with industry to develop adequate continuingeducationprograms. such programs need reliable and longterm support from industry andcommerce. two kinds of programs could be developed:1. oncampus programs would allow continuingeducation students totake advantage of departmental facilities and to better understand theacademic culture. more parttime programs and more fulltimeprogramsšthe latter to be funded partly by industry and partly bythe employeešare needed.2. university teleeducationšteaching by video, with eithersimultaneous or delayed broadcastšshould be developed to itsfullest extent. more use should be made of existing and emergingnetworks to bring education into the homes and offices of students.17teleeducation would provide needed funds to universities and at thesame time would satisfy a deep need of industry. emerging highbandwidth networks may also provide a good mode of educationaldelivery.in 1985, an nrc panel on continuing education concluded that ''continuingeducation of engineers is essential to increasing national productivity" and that"continuing education is an entity in itself and can no longer be viewed as an'addon' role of industry and academia."18 the increasing thrust into theinformation age makes it even more important that software engineers obtaincontinuing education in cs&e.precollege cs&e educationcommittee discussions of undergraduate cs&e education touched on theimpact of the previous exposure of undergraduates to programming and computerscience in high school. although the committee did not address this topic indetail, it believes that all of the difficulties experienced by cs&e faculty at nonph.d.granting institutions in keeping up with current knowledge in the field arealso characteristic of high school teachers of computer science. indeed, highschool computer science education may well promote the attitude that "computerscience is programming."precollege teachers of computer science share problems of currency withcs&e faculty from nonph.d.granting institutions and other scientists andengineers who received their educations in computing many years ago. thusoutreach efforts toward precollege teachers could capitalize on efforts madetoward these other groups. however, the difference in educational backgroundsand professioneducation in cs&e135computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.al cultures may well present different outreach problems, and so a study toaddress high school computer science education is discussed in chapter 5,"recommendations."summary and conclusionscs&e education at the undergraduate level is highly variable in outlook andquality. concerted efforts to improve undergraduate cs&e education across the1000odd undergraduate programs throughout the nation could have a significantpositive effect on the graduate programs and on the practice of softwareengineering in particular and computing in general. such improvement wouldalso support the broader agenda described in chapter 2. at the same time, it mustbe realized that the undergraduate program is too short to accomplish all thatindustry would like it to accomplish, and industry should put more emphasis onhiring students with master's degrees.given the ubiquity of computing in society, cs&e departments have animportant service role as well; in particular, they must also take an active role inconveying to other areas and fields a better understanding of computing'spotential.ph.d. education is in reasonably good shape, but several areas of concernneed to be addressed. first, because it deals so heavily with research, the ph.d.program is an excellent place to begin the process of broadening. for many ph.d.students, more education in continuous mathematics would prepare them forinterdisciplinary work. all ph.d. students should be required to minor in a fieldoutside of cs&e, and more faculty and ph.d. students should begin doinginterdisciplinary research. beyond that, it is necessary to address the presentundersupply of new ph.d.s and faculty in systems and in areas that span theoryand practice.continuing education in cs&e remains weak. academia places relativelylow value on providing continuing education, and industry does not understandits importance well enough. an active partnership is needed to strengthen andrevitalize continuing education programs in cs&e.notes1. for additional discussion of these issues, see association for computing machinery, "the scopeand directions of computer science: adequacy and health of academicindustry coupling,"communications of the acm, october 1991, volume 34(10), p. 127, as well as the forthcoming cstbreport on human resources in cs&e.2. one illustration is that for the 1988œ1989 academic year at cornell university, the incominggraduate student in computer science on average outperformed the incomeducation in cs&e136computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.ing graduate student in english on the verbal part of the gre and outperformed the incominggraduate student in mathematics on the quantitative part. see alison p. casarett, the annualstatistical report of the graduate school, cornell university, ithaca, new york, 1989œ1990, pp. 79,88, and 113.3. although this observation applies to senior cs&e faculty in the ph.d.granting institutions as well,at those institutions the faculty are by and large active researchers.4. it may take a decade or longer for substantial and radically new ideas to make their way into thenational undergraduate curriculum. first, it may take three to five yearsšor morešto investigate andexperiment with the ideas before they have jelled enough for someone to consider writing a textbook.writing that first textbookšand perhaps also developing software to accompany itšmay takeanother two to three years. but fully understanding and accepting the implications of that text maytake another five years.5. for example, in 1988 none of the 20 largest institutions in cs&e (as measured by researchexpenditures on cs&e) reported insufficiencies in the research equipment at their disposal. fortyfour percent of the remaining 127 institutions surveyed reported that the amount of researchequipment at their disposal was insufficient. see national science foundation, academic researchequipment in computer science, central computer facilities, and engineering: 1989, nsf 91œ304,nsf, washington, d.c., 1989, table 6, p. 7.6. an example makes the point more strongly. borland's turbo c package (version 2.0) for ibmpersonal computers was selling in mid1990 for around $100 (pc magazine, february 13, 1990, p.198). today, borland's c++ package, version 2.0 (an objectoriented version of c), sells for around$340 from the same vendor (pc magazine, february 11, 1992, p. 242). when such price differentialsare multiplied by several dozen, it is easy to see that upgrades can involve substantial expenses thatare especially difficult to manage for institutions with very limited budgets.7. in the 1989œ1990 academic year, ph.d.granting institutions in cs&e awarded 9037 undergraduatedegrees (david gries and dorothy marsh, "the 1989œ1990 taulbee survey," computing researchnews, volume 3(1), january 1991, p. 6 ff.), or less than a third of all undergraduate degrees in thecs&e field. (table 8.2 in chapter 8 provides the total undergraduate degree production in computerscience over time.) however, table 7.2 in chapter 7 indicates that undergraduates from ph.d.granting institutions constitute the bulk of ph.d. graduates in cs&e. taken together, these pointssuggest that it is graduates of the nonph.d.granting institutions that take the majority of computerrelated jobs in industry and commerce.8. citing a study of experienced programmers in which the best and worst programmers (as measuredby their productivity) varied by a factor of ten, fred brooks has noted that "managers have longrecognized wide productivity variations between good programmers and poor ones." see frederickbrooks, the mythical manmonth, addisonwesley, reading, mass., 1975, p. 30.9. the fact that more and more junior and seniorlevel courses (e.g., graphics, operating systems) dorequire more prerequisites is a positive development. of course, this makes it difficult for students inother disciplines to take these courses; essentially, they have to minor in cs&e. however, in nogenuinely mature scientific or engineering field is it possible to take junior and seniorlevel courseswithout taking prerequisites; this fact simply reflects the use of cumulative knowledge.10. a "credit hour" signifies a single student enrolled in a course for a single hour. thus if adepartment teaches a course worth 3 credit hours to 20 students, that department teaches 3 × 20 = 60credit hours.11. although the same situation might well be true in most other fields, there are exceptions:"academic chemistry, however, has from the beginning been closely tiededucation in cs&e137computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.to industrial chemistry. chemistry, as a field, took hold in universities in the united states at aboutthe same time that the u.s. chemical industry was beginning to grow. from the late 19th century on,professors of chemistry have served as consultants to chemical firms, often moving back and forthbetween industry and academy. chemistry undergraduates then and now have found their careerslargely in industry . . . . [in addition, the field has seen] the training of industryoriented ph.d.s in thelandgrant colleges and technical schools." see governmentuniversityindustry researchroundtable/ academy industry program, new alliances and partnerships in american science andengineering, national academy press, washington, d.c., 1986, pp. 4œ5.interestingly, this source also asserts that "computer science, by the very nature of the subject, isclosely tied to applications" (p. 5). as a statement about cs&e relative to other fields, this may wellbe true; nevertheless, it is inconsistent with the sentiment encountered by most cs&e academicsseeking to perform applicationsoriented work.12. the demand from industry is certainly present. for example, despite anticipated future downsizingof its work force, ibm is reported to have a hiring need for ph.d.s in computer science that is greaterthan the entire supply produced by american universities each year. see peter h. lewis, "computerscience is going down," new york times, april 5, 1992, education section, pp. 42œ43.13. such roles for computer scientists and engineers have been found even in academia. for example,project athena at the massachusetts institute of technology (mit), a large and ultimately successfuleffort to tie together thousands of workstations across the entire campus, was inspired largely by thevision of faculty members in the electrical engineering and computer science department at mit,even though the project was not intended to be a research project in cs&e and no faculty used it as aplatform on which to conduct original cs&e research. information provided by jerome saltzer, mitprofessor of computer science, and former technical director of project athena. see also george a.champine, mit project athena: a model for distributed campus computing, digital press, bedford,mass., 1991, pp. 23œ24, xxœxxi.14. data provided by survey of earned doctorates, office of scientific and engineering personnel,national research council, washington, d.c.15. the nsf estimated that in 1988, 710,200 people were employed as computer specialists in bothscience and engineering and nonscience and engineering fields (national science board, science andengineering indicators, 1989, nsf, washington, d.c., 1989, pp. 240œ241). a modest 4 percentgrowth rate compounded annually over the subsequent three years would yield 800,000 people in1991.16. at a recent cstb workshop on human resources in the computer field, some participants arguedthat continuing education in the field is needed every two years. but five years is far more plausiblefor the time scale on which new cs&e paradigms and approaches are created and found to be broadlyapplicable. a report on this workshop is forthcoming.17. in assessing student learning, a study performed at stanford university found no difference indemonstrated performance between students viewing a videotape of a lecture with a teachingassistant (ta) providing assistance and students enrolled in more traditional instructional formats.such a result suggests that under certain circumstances, taassisted video instruction may becompetitive with lectures for delivering information. see j.f. gibbons, w.r. kincheloe, and k.s.down, "tutored videotape instruction: a new use of electronics media in education," science,volume 195, march 18, 1977, pp. 1139œ1146.18. national research council, continuing education for engineers, national academy press,washington, d.c., 1985, p. 3.education in cs&e138computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.˛priority 1: sustain the cs&e core,.computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.and has been indispensable for its impact on practice in the last couple ofdecades. but this track record of success has a down side, in the sense that anyfield with a long history of successes risks being taken for granted. only bycontinuing support for the core effort (support whose importance to the nationmay well grow if industrial cs&e research is cut back substantially in the future)will the field continue to make progress that is broadly applicable over manyfields of inquiry and areas of human endeavor. while tantalizing successes havebeen achieved with promising technologies such as distributed and parallelcomputing, objectoriented programming, and graphical user interfaces, the fullpractical exploitation of these and other computing technologies will requireconsiderable research in the future. the committee notes with approval thatfederal funding agencies appear to recognize the importance of continued supportfor core cs&e activities, and it wishes to encourage this trend in every waypossible.the committee calls attention to its use of the word "sustain." many in thecs&e community (and some on the committee itself) have been concerned aboutthe increasing tightness in the availability of research funding for core topics incs&e and have argued with some cogency that the first priority should be tostrengthen rather than merely sustain the core. advocates of this position wouldsay that the track record of cs&e in research and education has been so positiveand successful that it speaks for itself, that there is not enough support forcomputer scientists and engineers to perform the "core" research in cs&e thatwill be necessary in the future, that computing technology will improve as theresult of advances in cs&e, and that the information revolution promises todevelop as it has in the past. why, these individuals would argue, should awinning research agenda be changed?the committee is sympathetic to this perspective and would have liked torecommend a substantial increase in such funding, especially in light of thegrowing numbers of academic cs&e researchers relative to available researchfunding. however, it concluded that such a recommendationšamounting inessence to "we should continue to be supported in the style to which we havebeen accustomed"šwould have been seen unfavorably by policy makers as anentitlement argument, particularly in view of the substantial increases in researchfunding that will be made available to the cs&e community by the hpccprogram. in the committee's overall judgment, more benefit is likely to accrue tothe field and the nation if the broadening course is taken rather than if efforts atthe core are redoubled. the reasoning is clear: relatively few cs&e researchersare devoted to the pursuit of interdisciplinary and applicationsoriented work,while relarecommendations140computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.tively many are devoted to investigating problems at the core, and human andfiscal resources devoted to the former are likely to have a more significantimpact.accordingly, the committee was led to its second priority.priority 2: broaden the fieldthe second priority is to broaden the field. given the many intellectualopportunities available at the intersection of cs&e and other problem domainsand a solid and vigorous core effort in cs&e, the committee believes thatacademic cs&e is well positioned to broaden its selfconcept. given the pressingeconomic and social needs of the nation and the changing environment forindustry and academia, the committee believes that academic cs&e mustbroaden its selfconcept or risk becoming increasingly irrelevant to computingpractice.more specifically, academic cs&e must: increase its contact and intellectual interchange with other disciplines(e.g., other science and engineering fields). increase the number of applications of computing and the quality ofexisting applications in areas of economic, commercial, and socialsignificance, and understand that from such applications substantivecs&e problems often emerge. embrace the creation of significant new knowledge and demonstrableintellectual achievement as the relevant standards of meaningfulscholarship in cs&e, rather than focusing on artificial distinctionsamong basic research, applied research, and development (as discussedin chapter 2). increase traffic in cs&erelated knowledge and problems amongacademia, industry, and society at large, and enhance the crossfertilization of ideas in cs&e between theoretical underpinnings andexperimental experience.such broadening would serve the interests of society at large by coupling theformidable intellectual resources of academic cs&e more directly to the practiceof computing, thereby increasing the likelihood that the full potential ofcomputing can be realized. it would also serve the field by increasing intellectualopportunities and diversifying the sources of funding.priority 3: improve undergraduate educationthe third priority is to improve undergraduate education in cs&e. asdiscussed in chapter 4, undergraduate cs&e education is highlyrecommendations141computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.variable in quality and outlook from institution to institution. given theimportance of cs&e to computing practice and the large flow of those withundergraduate cs&e degrees to business and industry, the quality ofundergraduate cs&e education is inextricably tied to the state of computingpractice in all sectors of society. moreover, better undergraduate education isnecessary for better research, since it is necessary for transmitting recentlydeveloped core knowledge to the next generation and for providing theintellectual basis in cs&e for individuals pursuing a broader research agenda.thus, improving undergraduate education is a necessary component of bothpriorities.the natural evolution of undergraduate cs&e education will ultimatelyresult in the synthesis and dissemination of modern approaches to cs&e,enhancing the present skills and future adaptability of cs&e graduates as well asproviding a good foundation on which to build knowledge in other fields. butnatural evolution occurs on the time scale of several decades. a major programaimed at accelerating the process could reduce the time to a decade or less. moreimportantly, the nation is likely to reap considerable benefits from such aprogram, since undergraduate cs&e programs from nonph.d.grantinginstitutions supply a considerable fraction of the computer specialists responsiblefor implementing and maintaining the software systems in all areas of applicationthat underlie the information age.to suggest more specifically how these priorities translate into an actionplan, the committee grouped its recommendations into two categories: research1and education. each category contains action items for universities and federalfunding agencies. taken together, these action items constitute a coherent planthat will improve the state of the cs&e discipline on a much shorter time scalethan would otherwise be possible, to the benefit of the discipline and the nation as awhole in a rapidly changing world.all of the action items described below will demand considerable leadershipfrom the academic cs&e community. if the community is to adapt to changingcircumstances in a proactive and constructive manner, senior researchers in theacademic cs&e communityšthe ones whose words and actions shape thevalues of the communityšmust take the lead in promoting the cultural changesnecessary for success in the new environment. moreover, senior academicresearchers in the cs&e community are widely regarded as spokespersons for thediscipline, and their continuing presence and participation in policy debates inboth the executive and legislative branches of the federalrecommendations142computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.government will be necessary for years to come if federal policy and funding areto evolve in the best interests of the field.recommendations regarding researchto federal policy makersas noted in chapter 1, federal policy toward computing and cs&e has hadan enormous impact on the field's shape and development. as the scale ofcomputing activities increases, the importance of a strong federal role can onlygrow.recommendation 1. the highperformance computing andcommunications (hpcc) program should be fully supported throughoutthe planned fiveyear program. full support for the hpcc program will entailabout $3.7 billion dollars over the next four years, or about 1.2 percent of theentire federal research budget.2the hpcc program is of utmost importance for three reasons. the first isthat highperformance computing and communications are essential to thenation's future economic strength and competitiveness, especially in light of thegrowing need and demand for ever more advanced computing tools in all sectorsof society. the second reason is that the program is framed in the context ofscientific and engineering grand challenges. thus, although the program willsupport research and development in a variety of fields, the program is a strongsignal to the cs&e community that good cs&e research can flourish in anapplications context and that the demand for interdisciplinary and applicationsoriented cs&e research is on the rise. and finally, a fully funded hpcc programwill have a major impact on relieving the funding stress affecting the academiccs&e community. consistent with priority 1, the committee believes that thebasic research and human resources component of the hpcc program is critical,because it is the component most likely to support the research that will allow usto exploit anticipated technologies as well as those yet to be discovered throughsuch research.the committee is concerned about the future of the hpcc program after fy1996 (the outer limit on current plans). if the effort is not sustained after fy 1996at a level much closer to its planned fy 1996 level than to its fy 1991 level of$489 million, efforts to exploit fully the advances made in the preceding fiveyears will almost certainly be crippled. in view of the long lead times needed forthe administration's planning of major initiatives, the committee recomrecommendations143computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.mends that funding necessary for exploitation of recently performed research and the investigation of new research topics be fully assessed sometime during fy 1994 with an eye toward a followon hpcc program.recommendation 2. the federal government should initiate an effort tosupport interdisciplinary and applicationsoriented cs&e research inacademia that is related to the missions of the missionoriented federalagencies and departments that are now not major participants in the hpccprogram. collectively, this effort would cost an additional $100 million perfiscal year in steady state above amounts currently planned.3for the participating agencies, the hpcc program is a good model for howto encourage interdisciplinary and applicationsoriented research in cs&e. butmany federal agencies are not currently participating in the hpcc program,despite the utility of computing to their missions, and they should be brought intoit. (as noted in chapter 2, 12 federal agencies controlling over $10 billion in fy1991 obligations for scientific research each allocated less than 1 percent tocomputer science research.)such agencies can be divided into two groups: those that support substantialresearch efforts, though not in cs&e, and those that do not support substantialresearch efforts of any kind. the committee believes that both groups wouldbenefit from supporting interdisciplinary or applicationsoriented cs&eresearch, but for different reasons.support of interdisciplinary cs&e research, i.e., cs&e research undertakenjointly with research in other fields, should be taken on by mission agencies withresponsibilities for those other fields. that research will often involve animportant computational component whose effectiveness could be enhancedsubstantially by the active involvement of researchers working at the cutting edgeof cs&e research. examples of interdisciplinary cs&e research were discussedin the chapter 2 section "a broader research agenda."the case for support for applicationsoriented cs&e research from agenciesthat do not now support research is less obvious, but to the committeenevertheless cogent. while these agencies are generally focused on operationalmatters (e.g., processing tax forms or income support payments) and thus areexpected to make the best use of available technology, it may be that in manycases the efficiency of their operations would be substantially improved by someresearch advance that could deliver a better technology for their purposes. arecommendations144computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.case in point is the research in the application of imagerecognition technology tothe processing of government forms that is being performed by the nationalinstitute of standards and technology in support of the bureau of the census andthe internal revenue service.4moreover, the federal government's computer operations are often conductedat scales of size and complexity whose ramifications are poorly understood.without an adequate understanding of these ramifications, it will be difficult toprevent computerrelated disaster or reduce the likelihood of computerrelatedinefficiency or fraud. given its operational responsibilities, the federalgovernment must do the best it can with what it has, but cs&e researchundertaken to better understand these problems could have substantial payofflater with respect to reliability, security, efficiency of operation, lower cost, andso on. an additional benefit is that applications explored and developed in such acontext may have considerable ''spinoff" benefit to the private sector, since manygovernment information processing needs (e.g., for security) are similar to thosefound in the private sector.5how can the talents of the academic cs&e research community be tapped toprovide maximum benefit to the nation in these interdisciplinary andapplicationsoriented areas? a first step would be to establish a research programwithin mission agencies that would tap the talents of cs&e researchers in theservice of each agency's own needs. this may be easier said than done, sincecs&e researchers interact primarily with only the four federal agencies thatcontribute the bulk of cs&e research support, a group of agencies that places ahigh value on research and provides many opportunities for interaction betweenagency staff and researchers. the very existence of such a program wouldprompt strong interest on the part of academic computer scientists and engineersin pursuing interdisciplinary and applicationsoriented research, but some caremust be taken to ensure that the "bridging of cultures" between cs&e and othersis successful.agencies might jointly sponsor research on problems of collectiveimportance. for example, several agencies process vast amounts of paper andmight benefit from advanced imaging and database technologies; another groupof agencies might have a special interest in using computers and communicationsto facilitate service for the disabled. when the work is specified and undertaken,it is essential that such work be done by investigators from cs&e and otherdisciplines and areas who regard each other as intellectual equals; only inrecommendations145computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.this way will it be possible to maintain both an understanding of the future stateof the art in computing and an appreciation of the real problems in the applicationdomain. one way of ensuring true collaborative work is to consider onlyproposals whose principal investigators are drawn both from both cs&e andsome other discipline or area.the location of such a program within the federal government is a sensitiveissue. on balance, the committee believes that the existing hpcc programprovides the most reasonable home for this program, subject to one crucialprovision to be discussed below. (other organizations have developed similarpositions; for an example, see box 5.1.) the hpcc program has strong supportfrom congress, the white house, and the office of management and budget;thus individual agencies have strong incentives to participate. most importantly,the federal coordinating council for science, engineering, and technology(fccset) is already in place and can facilitate interagency collaborations.in performing the coordinating role for this new program, fccset wouldapproach agencies not already participating in the hpcc program, such as thedepartment of the treasury and the department of transportation. it would alsobe appropriate for fccset to ask large commercial users of computers toindicate what cs&e research might be relevant to their needs. such users (andthe computer industry) might be willing to support applicationsoriented researchto a certain extent, especially if such support could be leveraged (or matched) byfederal dollars.the committee recognizes a certain danger in recommending that the hpccprogram be augmented to provide for this new program. in particular, it isconcerned that planned hpcc budgets would simply be reprogrammed toaccommodate this program. such reprogramming would be inconsistent with theframework of priorities laid out above. it is the intent of this recommendationthat agencies that have not traditionally supported cs&e should also participatein the hpcc program; along with such participation should come additionalresources from those agencies. these resources would support research thatwould contribute directly to the goals of those agencies by improving theefficiency of computing practice in support of those goals.box 5.2 describes some additional implementation issues that this effortcould entail.recommendations146computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.box 5.1 position of the computer systems policyproject on the hpcc programto make the most of federal and private research investments in thehpcc [program], . . . the software, hardware, and networking technologiesbeing developed must be based on the broadest possible vision of whathighperformance computing and communications can make possible in thefuture. this requires expanding the current vision of the hpcc [program] toinclude grand challenges motivated by social and economic needs in areasof interest to the government and general public, such as advances in thedelivery of health care and services for senior citizens; improvements ineducation and opportunities for lifelong learning; enhanced industrial designand intelligent manufacturing technologies; and broad access to public andprivate databases, electronic mail, and other unique resources . . . . byleveraging [its] investments [in computer research], the government candevelop more broadly applicable generic, enabling technologies andstimulate the additional research by the private sector needed to solve theexpanded grand challenges . . . . [thus,] the cspp recommends . . . [that]the vision of the hpcc program [be expanded to] include research ongeneric, enabling technologies to support a wider range of applications . . . .working together, the government, industry, and the broader science andtechnology community can construct an hpcc program that will contributeto our nation's ability to meet many of the science, engineering, economicand social challenges we face. [emphasis added by committee.]note: the computer systems policy project (cspp) is a coalition of the 12 major u.s.computer system manufacturers (apple, at&t, compaq, cray research, the controldata corporation, data general, the digital equipment corporation, hewlettpackard,ibm, sun microsystems, tandem, and unisys) whose goal is to provide u.s. policymakers with data and the perspective necessary to the development of effective, longrange policies both in the development of technology and in the improvement of the u.s.trade position globally.source: adapted, with a few minor wording changes, from computersystems policy project, expanding the vision of highperformance computing and communications, executive summary, cspp, washington,d.c., 1991.recommendations147computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.box 5.2 implementation issues for a governmentwide effort to promote interdisciplinary andapplicationsoriented cs&e research awards should be provided for a period of multiple years (subject toannual review), long enough that some funding stability can be assuredbut not so long as to reduce accountability or to impede the pursuit ofpromising new developments. individual awards should be substantial enough to provide a critical massof effort. requests for proposals should be directed to universities with researchstrength in both cs&e and other problem domains (e.g., biology,business, library science) in order to draw on a talent pool that haspreviously gone (for the most part) untapped for interdisciplinary andapplicationsoriented cs&e research. a phased implementation of this effort is in order. for example, amountsof $10 million, $20 million, and $30 million allocated to the effort in thefirst three years would enable the continuation of threeyear awardsmade in the first year while still accommodating new proposals. afterexperience has been developed with this rampup, full funding could beimplemented.to universitiesuniversity policy will play a key role in broadening academic cs&e. anyone of the recommendations below may suggest a specific action that has beentaken in the past, but their collective strength is that they are part of a coherentstrategy to broaden the scope of academic cs&e. their implementation willdefine a leadership role for many senior cs&e faculty, who together have (orshould have) very influential roles in defining the tone and character ofuniversities and academic cs&e departments with respect to promotion policiesand the boundaries of "acceptable" research and education.recommendation 3. academic cs&e should broaden its researchhorizons, embracing as legitimate and cogent not just research in core areas(where it has been and continues to be strong) but also research in problemdomains that derive from nonroutine computer applications in other fields and areas or from technologytransfer activities. "nonroutine" applications arethose that pose substantive and intellectually challenging problems and may bebest solvedrecommendations148computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.by research collaborations with experts in the application area; some examplesare provided in the section "a broader research agenda" in chapter 2. currentand future cs&e faculty should be encouraged to undertake collaborativeresearch both with faculty in other disciplines and with appropriate parties inindustry and commerce,6 and in government laboratories. these collaborationswill benefit both computer scientists and engineers (as a result of new intellectualchallenges posed) and those from other problem domains (as a result of the moreeffective use of their computational resources).7 as argued in chapter 2, thecentral focus of scholarship in cs&e should be activity that results in significantnew knowledge and demonstrable intellectual achievement, without regard forwhether that activity is related to a particular application or whether it falls intothe traditional categories of basic research, applied research, or development.to promote broadening, action should be taken to make the universityenvironment more accommodating to interdisciplinary and applicationsorientedresearch and to stimulate the interpersonal interactions needed for the successfulconduct of such research. university administrations and cs&e departmentsshould:3a. develop and promulgate explicit policies that assure andinform all faculty members that research in interdisciplinary orapplicationsoriented areas or work oriented toward technologytransfer8 will be competitive in the tenure and promotionevaluation process with work that is more traditionally oriented,assuming that necessary standards of quality and achievement aremet. such policies will require mechanisms by whichinterdisciplinary and applicationsoriented work can be evaluated,possibly including:(i) evaluation committees with members familiar with the intellectualrequirements of the other (noncs&e) problem domains representedin the work being evaluated. such committees will have to addressthe very problematic issue of how to interpret the traditional criterionof "demonstrable intellectual achievement" in an interdisciplinary orapplicationsoriented context.(ii) ways to take into account the fact that meaningful evidence ofintellectually substantive work in cs&e often takes the form ofsystem demonstrations as well as the publication of journal articles,and thus that many cs&e experimentalists up for promotion ortenure may submit portfolios with fewer published papers than theirpeers.9recommendations149computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.3b. support cs&e faculty who wish to gain expertise in otherfields so that they may more effectively pursue interdisciplinaryor applicationsoriented research. possible mechanisms forsupport could include:(i) establishment of shortterm academic appointments (one to threeyears) that academic computer scientists and engineers could use todevelop familiarity with and expertise in other areas. suchappointments would typically involve reduced teachingresponsibilities and could be held by new ph.d.s and senior facultyalike.(ii) sponsorship of seminar series that describe challenging cs&eproblems that arise in other disciplines.3c. invite qualified individuals from industry and commerce toserve on university and academic departmental advisory andreview committees for cs&e programs. this is a commonpractice among some leading research universities, but the practiceshould be more widespread.3d. eliminate or reduce practices that impede intellectual contacts with industry. in particular, universities should consider greater useof more open arrangements with respect to the protection ofintellectual property, such as crosslicensing for universitydeveloped technology, rather than insisting on exclusive rights forthemselves. such practices conflict with norms in the computerindustry and set up roadblocks to collaboration.3e. encourage cs&e research faculty to seek out nontraditionalsources of funding to pursue interdisciplinary or applicationsoriented research. nontraditional sources would include theprogram described under recommendation 2; federal agencies otherthan darpa, nsf, nasa, and the department of energy; largecommercial users of computers; and state governments. as noted inchapter 2, federal agencies without a tradition of supporting cs&eresearch may still control substantial research budgets.recommendation 4. universities should support cs&e as a laboratorydiscipline (i.e., one with both theoretical and experimental components). withrespect to its need for equipment, many parts of cs&e are more like physics orengineering than like mathematics. cs&e departments need adequate researchand teaching laboratory space; staff support (e.g., technicians, programmers, staffscientists); funding for hardware and software acquisition, maintenance, andrecommendations150computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.upgrade (especially important on systems that retain their cutting edge for just afew years); and network connections. new faculty should be capitalized at levelscomparable to those in other scientific or engineering disciplines.recommendations regarding educationto federal policy makersthe federal government has a history, dating to the days of sputnik, oftaking strong and decisive action to improve science and mathematics educationin times of great national need. the committee believes that undergraduate cs&eeducation would benefit tremendously from such action today and that thebenefits of such action will echo throughout all sectors of society.recommendation 5. the basic research and human resourcescomponent of the highperformance computing and communicationsprogram should be expanded to address educational needs of certainfaculty. in particular, college and university cs&e faculty who are notthemselves involved in cs&e research and researchers from other scientific andengineering disciplines that depend on computation need to become familiar withrecent developments in cs&e. the program described below to address theseneeds is estimated to cost $40 million over a fouryear period.as argued in chapter 4, the lack of current knowledge about the approachesand intellectual themes of modern cs&e is an impediment to the full exploitationof computing. this is true for those who teach undergraduate cs&e without thebenefit of sustained contact with cuttingedge research as well as for manyscientists and engineers whose education in computing was received many yearsago. for these individuals, programs of continuing education to bring them up todate on recent developments in cs&e would have significant value. suchprograms would enable them to develop their own approaches to the subjectmaterial, informed on the one hand by exposure to the current state of the art andon the other by knowledge of local institutional needs, and they could have amajor impact on the quality of undergraduate cs&e education in the unitedstates, as well as on its ability to use computing in support of other science andengineering.as major players in pushing back the frontiers of cs&e through theirresearch and in educating students through their teaching, academic researchersare best equipped to take responsibility for disrecommendations151computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.seminating their knowledge to parties that could benefit from it. morespecifically, it is their broad knowledge about important developments in the fieldin the last ten years that is most important to disseminate to these parties, ratherthan their detailed knowledge about their own particular research specialtiesgenerated in the last couple of years.a continuing education program to meet the needs described above couldfirst sponsor intensive monthlong workshops to promote discussion among thetop researchers from academia. these workshops would focus on the problems ofundergraduate cs&e education (e.g., content, scope, style, broadening,recruitment and retention of women and minorities). neither course developmentnor consensus among the individuals participating would be necessary outcomesof these workshops; instead, the object would be for participants to becomeacquainted with the various approaches to teaching undergraduate cs&e in orderto provide a basic platform of understanding from which would emerge differentways to integrate various paradigms.following these workshops, the participants would give a series of shortcourses for individuals who are not current with recent developments in the field,including cs&e faculty at nonph. d.granting institutions, scientists andengineers from other disciplines, and appropriately qualified high schoolteachers. (the program would provide course leaders with financial support forthe development of materialsštext materials, exercises, software, and so on. itwould also provide some financial assistance for the workshop attendees.)the active participation of senior academic cs&e researchers is critical tothe success of this program; indeed, participation could be seen as an activedemonstration by these individuals of leadership for the field as a whole. sincesenior academic researchers have, by definition, made their careers by performingresearch of extraordinary quality, it will take more than mere exhortation topersuade them to become substantially involved in educational matters. onemechanism to encourage their attention to such matters would be to coupleresearch funding to participation in these workshops. for example, anaugmentation fund for research grants could be set aside, for which onlyresearchers taking part in these workshops would be eligible. research proposalswould be submitted and awarded through the ordinary review process;researchers whose proposals were successful and who had participated in theseeducation workshops would be eligible to receive an additional amount from theaugmentation fund to support their research as they saw fit. alternatively, grantawarding agencies might give some degree of preferential treatment to proposalsreceived from participants in this program.recommendations152computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.the estimate of $40 million for the total cost of this program is based on anassumed 100 researchers leading 400 short courses for 6000 other individuals.10these funds should be an addition to the very important elements already coveredby the basic research and human resources (brhr) component of the hpccprogram. like the original brhr component, it is appropriate that the proposedcontinuing education program be funded by most if not all agencies participatingin the hpcc program, although such a program could be administered within thensf.to universitiesuniversities are the front line of educational delivery. if cs&e is tobroaden, university policy and departmental programs must support andencourage such change. graduate cs&e education should reflect and besupportive of a broader research agenda. only in this way can cs&e graduatesunderstand the applicability of current and rapidly emerging future cs&edevelopments to the increasing number of business, commercial, scientific, andengineering problems that have (or ought to have) a significant computingcomponent.recommendation 6. so that their educational programs will reflect abroader concept of the field, cs&e departments should take the followingactions:6a. require ph.d. students either to take a graduate minor in a noncs&e field or to enter the ph.d. program with anundergraduate degree in some other science or engineering ormathematical field. those in the latter category may lack some ofthe skills and knowledge possessed by incoming graduate studentswith undergraduate cs&e degrees, but they can use the time thatothers would use for a noncs&e minor to strengthen their cs&ebackground. the choice of graduate minor should be broad enough toallow the student a high degree of discretion to select the minor, butconstrained enough that the student cannot evade the spirit of therequirement by selecting a minor in a field that is too closely relatedto his or her major interest. the committee recognizes that arecommendation of this scope may well generate considerableresistance in the affected departments, but it nevertheless believesthat attempts to overcome this resistance will ultimately benefit thefield.6b. encourage ph.d. students in cs&e to perform dissertationresearch in nontraditional areas, as described in chapter 2.recommendations153computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.in addition, expose ph.d. students to a variety of projects andintellectual issues in their predissertation work.6c. offer undergraduate students not majoring in cs&e a widerange of cs&e courses and programs. by teaching other coursesless frequently, cs&e departments might:(i) offer undergraduate minors in cs&e and/or general educationcourses in computing.(ii) collaborate with other departments in teaching courses thatfamiliarize noncs&e undergraduate students with advancedcomputational tools in the context of their own fields of interest.such courses might have appeal to cs&e majors, therebycontributing to their broadening as well.6d. provide mechanisms to recognize and reward faculty fordeveloping innovative and challenging new curricula that keep upwith technological change and make substantive contact withapplications in other domains. in particular, find ways to give creditfor the professional effort involved in developing the following:(i) laboratories. in both software and hardware engineering education,laboratories are essential if students are to obtain firsthandexperience with the nontheoretical side of cs&e. in the fastchanging cs&e environment, laboratories must be completelyrevised frequently, i.e., every several years.(ii) textbooks. textbooks that are both good and current are important tocs&e and are difficult to produce as well. the commitment ofeffort and time needed to write a quality textbook is far greater thanthat needed to produce multiple research papers, and in the case of afastchanging field such as cs&e the amount of professionalcompetence and talent required is often at least as great.(iii) interdisciplinary courses. given the requirements for a minimallevel of applicationsspecific competence in teaching applicationsoriented cs&e, the development of interdisciplinary courses shouldbe expected to take longer and be more difficult than teaching corecs&e courses, even if faculty from other disciplines are involved.recommendation 7. the academic cs&e community must reach out towomen and to minorities that are underrepresented in the field (particularlyas incoming undergraduates) to broaden and enrich the talent pool.as noted in chapter 8, cs&e attracts women and minorities at all levels atabout the same rate as the physical sciences. however,recommendations154computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.cs&e is also significantly younger than the physical sciences, and to the extentthat a younger field should be expected to be more inclusive of women andminorities, the field has an opportunity for outreach that it is not fullyexploiting. moreover, the underrepresentation of women and minorities in cs&eis particularly unfortunate given the impact of cs&e on society; their exclusionfrom cs&e will mean that their voices and values will not be heard as society istransformed by the information revolution.a secondary benefit of outreach is that such outreach might well contributeto achievement of a broader agenda. this report has argued that the field will beenriched by interactions with those from other disciplines and fields.recommendation 6c recognizes the need for these individuals to learn aboutcs&e. to the extent that women and minorities constitute a larger fraction ofthese fields than they do of cs&e, outreach programs for these groups shouldfocus their attention on cs&e, thereby increasing the likelihood of couplingbetween their ''home disciplines" and cs&e. computing practice as well ascs&e can only benefit from the greater inclusion of individuals with a morevaried set of perspectives and experiences.11outreach programs need to take into account the special needs andbackgrounds of individuals from underrepresented groups so that more areretained within and attracted to the field. although these programs are useful atall levels of cs&e education, undergraduate cs&e education is the point ofhighest leverage for the academic computer scientist or engineer. thus outreachis an essential element of improving undergraduate cs&e education.additional studiesin the course of its deliberations, the committee identified several areas ofspecial concern that should be addressed in future reports. these areas include: the computer infrastructure for undergraduate cs&e educationin all cs&e departments. while important parts of the undergraduatecs&e curriculum are technologyindependent, other aspects arestrongly dependent on the technological state of the art. withoutsuitable, uptodate equipment and software, it is impossible to exposestudents to concepts and environments that will affect all aspects offuture practice. for example, very fast computers with large amounts ofstorage are necessary to support threedimensional realtime graphicsand certain new and important programming languages and systems.12keeping the educational computer infrastructure approximately currentwith the cutting edge of technology will be anrecommendations155computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.ongoing enterprise. the committee hoped to be able to makerecommendations about the cost of a program that would keepeducational institutions current technologically, but was unable to locatefirm and relevant data on the subject.a study on this subject would address issues such as the magnitudeof the need for new machines, current university policies regardingreplacement of educational computer equipment and software,community views on how current the educational computerinfrastructure must be to support a good undergraduate cs&eeducation, and ways to fulfill the need in the most inexpensive mannerpossible. continuing education for cs&e. in considering the views of thecomputer industry and large commercial users of computers, thecommittee concluded that the needs for continuing education in cs&e,especially among those responsible for designing, programming, testing,and maintaining the software systems on which the information agedepends, are enormous, especially given the speed with which the fieldchanges. these needs are often recognized by all potential participants inthe continuing education endeavor, but for various reasons not fullyunderstood by the committee, continuing education is often relegated tothe backwaters of universities and neglected by industry and commerce.a study on this subject would document the magnitude of the needfor continuing education and explore mechanisms to encourage industryand academia to pay more attention to continuing education. such astudy would also focus on the needs of industry and commerce for thecontinuing education of those already in their work forces, and wouldspeak to a continuing education issue different from the one underlyingrecommendation 5. precollege cs&e education. in considering the state of undergraduatecs&e education, the committee was struck by the large extent to whichincoming students have some computer experience. acquired in highschool classes or in avocational pursuits, such familiarity has both apositive and a negative impact. the positive impact is that theseindividuals arrive with some of the basic vocabulary for and a certainintimacy with computer hardware. the negative impact is that theseindividuals often have misconceptions about the nature of theintellectual discipline, imagining, for example, that programming (all toooften, even bad programming) is identical to computer science.moreover, these individuals tend to be overwhelmingly white and male,a fact that works against the recruitment of women and minorities intothe field.a study on the subject of precollege cs&e education would adrecommendations156computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.dress both pedagogical issues (e.g., what are the essentials of cs&e that shouldbe presented at the precollege level?), teacher training issues (e.g., how are thosewho teach cs&e at the precollege level to be prepared to make appropriatepresentations?), and recruitment issues (e.g., how can more interest in computingbe generated among women and minorities at the precollege level?). meshingprecollege education with undergraduate cs&e education would be an importanttask of such a study.conclusionsover the past 50 years, cs&e has blossomed into a new intellectualdiscipline with broad principles and substantial technical depth. by embracing thecomputing challenges that arise in many specific problem domains, computerscientists and engineers can build on this legacy, guiding and shaping the courseof the information revolution. this expansive view of cs&e will require acommensurately broader educational agenda for academic cs&e, as well asundergraduate education of higher quality. adequate funding from the federalgovernment and greater interactions between academia and industry andcommerce will help immeasurably to promote the broadening and strengtheningof the discipline. (table 5.1 recapitulatesrecommendations157computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.the relationship of these recommendations to the overall priorities discussed at thebeginning of this section.) if the major thrusts of this reportšsustaining thecs&e core at currently planned levels, broadening the cs&e discipline, andupgrading undergraduate cs&e education to reflect the best of currentknowledgešare widely accepted in the academic cs&e community, thecommunityšas well as government, industry, and commercešwill be wellpositioned to meet the intellectual challenges of the future and to makesubstantial and identifiable contributions to the national wellbeing and interest.notes1. the reader will notice that the committee has not laid out a set of topical research priorities. asnoted in chapters 1 and 6, cs&e changes with extraordinary speed; moreover, its subdisciplines arehighly synergistic, as progress in one subdiscipline may have profound effects on another. thusrecommendations that favor one subdiscipline over another could divert the field and fundingagencies from opportunities that could well emerge in the future. the recommendations of thecommittee are structured to emphasize flexibility and to hedge against developments that are not nowforeseen.a historical precedent is worth some mention. in 1966, the automatic language processing advisorycommittee of the national research council issued a report, languages and machines: computers intranslations and linguistics, that was widely viewed as a highly influential study in the machinetranslation of foreign languages. by concluding that the basic technology for machine translation hadnot been developed at that time (and by implication that work on machine translation was not likely tobe immediately fruitful), the report contributed to a subsequent and substantial decline in funding forsuch research. supporters of machine translation argue that such a decline was inappropriate and thatthe current state of the art would otherwise be much more advanced, as it is in japan, where support inthe last two decades has been greater. for more discussion, see office of japan affairs and computerscience and technology board, national research council, japanese to english machinetranslation: report of a symposium, national academy press, washington, d.c., 1990, pp. 3œ4.2. this percentage is calculated on the basis of $14.3 billion in basic research and $59.3 billion inapplied research proposed in the president's budget request for fy 1993. these levels in the budgetrequest represent a growth in basic research of 8 percent and in applied research of 3 percent over thefy 1992 levels. if these growth rates are maintained, a total of some $312 billion will be allocated toresearch. (for this calculation, thenyear dollars were used. the $3.7 billion is the total projected "newmoney" for fy 1993 to fy 1996 from table 1.2 in chapter 1 plus the baseline funding from fy 1991of $489 million in each of these four years.)3. the size of the proposed effort ($100 million) was estimated on the following basis. according todata provided by the nrc's office of scientific and engineering personnel, there were 3860academic cs&e researchers in 1989. this figure suggests that in 1991, there would have been about4500 researchers (assuming that onethird of the ph.d. production since 1989 went into academicresearch). the cise directorate at the national science foundation received about 1200 proposals infy 1990, of which about 300 received funding. nsf program officials state informally that about halfof all proposals deemed scientifically meritorious do not receive funding due to budgetrecommendations158computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.limitations. thus it appears reasonable to suggest that perhaps 500 cs&e researchers might beavailable as coprincipal investigators for interdisciplinary or applicationsoriented work. of these,the committee assumed that about half would be both willing and able to pursue such work. thus 250awards per year could be made to teams consisting of two principal investigators, one from cs&e andthe other from outside cs&e. assuming $200,000 per award per year in direct costs, and a total cost(including overhead) of perhaps $400,000, an additional $100 million would be needed. this summight pay for a modest equipment purchase, summer research salaries for the principal investigators, acouple of graduate students, and a computer scientist or engineer investigating some applicationsoriented problem or a researcher from the applications domain investigating potentially relevantcs&e.4. u.s. department of commerce, national computer systems laboratory: annual report 1990,nistr 4492, national institute of standards and technology, washington, d.c., 1990, p. 21.5. in this context, note that the federal government explicitly acknowledges a responsibility to"participate with the private sector in precompetitive research on generic, enabling technologies thathave the potential to contribute to a broad range of government and commercial applications. in manycases, . . . technical uncertainties are not sufficiently reduced to permit assessment of full commercialpotential." (precompetitive research is defined as research that "occurs prior to the development ofapplicationspecific commercial prototypes.") see office of science and technology policy, u.s.technology policy, executive office of the president, washington, d.c., september 26, 1990, p. 5.6. for purposes of this discussion, the term "industry" refers to the computer industry. the term"commerce" refers to commercial (or nonspecialized governmental) users of computers, especiallythose with informationprocessing needs in large volume.7. this recommendation is consistent with an acm recommendation that "institutions shouldencourage more faculty in the discipline of computing to engage with business people in the design ofcommercial applications, especially those that will give contact with industry thinking on longtermissues. institutions should encourage more computing researchers to embrace computational scienceby joining in projects with physical scientists, bringing their expertise in algorithms andarchitectures." see association for computing machinery, "the scope and directions of computerscience: computing, applications, and computational science," communications of the acm,volume 34(10), october 1991, p. 131. (this paper uses the term "computing" as this report uses"computer science and engineering.")8. work on technology transfer is not envisioned as consulting activities that consist primarily ofgiving advice. rather, this work should usually involve sustained and intimate interaction betweenacademic computer scientists and engineers and those in working in nonresearch activities in industryand commerce. while it would be most desirable if the computing aspects of the problem were novel,such activity would in any event enhance the social and economic impact of cs&e research.9. a forthcoming cstb report will address in detail the issues faced by experimental computerscientists and engineers in academia. among these issues are the long time that system buildingrequires relative to publishing papers (and thus the lower volume of papers), the tendency for manysystem builders to present their work in conferences rather than in archival journals, and theevaluation of systems in an academic context.10. course leaders (i.e., participating researchers) are assumed to spend one fulltime month inintensive discussion workshops on undergraduate education, one monthrecommendations159computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.preparing a short course, and four months teaching four short courses over a fouryear period.assuming that about $100,000 would be needed per person per fulltime year in summer salary,travel, lodging, and so on, the efforts of course leaders would cost about $5 million in direct costsover four years, or about $8 million including a 66 percent overhead rate. grant augmentation iscalculated on the basis of 25 researchers every year receiving an additional $70,000 in researchfunding (not including overhead), or $12 million over four years. over a fouryear period, a programfor 6000 individuals (including a large fraction of the 5000 or so cs&e faculty in the nation)attending a fourweek short course might cost $2000 per attendee (likely not to cover the entire costof the course), for direct costs of $12 million and an additional $8 million in overhead.11. this sentiment was expressed at the recent cstb workshop on human resources in cs&e, onwhich a report will be forthcoming in the summer of 1992.12. inadequate laboratory infrastructure for cs&e was noted as a problem in 1989 by the nationalscience foundation. see national science foundation, report on the nsf disciplinary workshops inundergraduate education, nsf, washington, d.c., april 1989, p. 39.recommendations160computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.part iirecommendations161computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.recommendations162computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.6 what is computer science andengineering?chapter 1 provided a brief sketch of computer science and engineering as anintellectual discipline. this chapter elaborates on that discussion, discusses somekey structural features of the field, and provides some history on some of themajor intellectual accomplishments of the field in a few selected areas. for thereader's convenience, the chapter 1 section "computer science and engineering"is reproduced in its entirety here.computer science and engineeringcomputational poweršhowever measuredšhas increased dramatically inthe last several decades. what is the source of this increase?the contributions of solidstate physicists and materials scientists to theincrease of computer power are undeniable; their efforts have made successivegenerations of electronic components ever smaller, faster, lighter, and cheaper.but the ability to organize these components into useful computer hardware (e.g.,processors, storage devices, displays) and to write the software required (e.g.,spreadsheets, electronic mail packages, databases) to exploit this hardware areprimarily the fruits of cs&e. further advances in computer power and usabilitywill also depend in large part on pushing back the frontiers of cs&e.6 what is computer science and engineering?163computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.intellectually, the "science" in "computer science and engineering" connotesunderstanding of computing activities, through mathematical and engineeringmodels and based on theory and abstraction. the term "engineering" in"computer science and engineering" refers to the practical application, based onabstraction and design, of the scientific principles and methodologies to thedevelopment and maintenance of computer systemsšbe they composed ofhardware, software, or both.1 thus both science and engineering characterize theapproach of cs&e professionals to their object of study.what is the object of study? for the physicist, the object of study may be anatom or a star. for the biologist, it may be a cell or a plant. but computerscientists and engineers focus on information, on the ways of representing andprocessing information, and on the machines and systems that perform thesetasks.the key intellectual themes in cs&e are algorithmic thinking, therepresentation of information, and computer programs. an algorithm is anunambiguous sequence of steps for processing information, and computerscientists and engineers tend to believe in an algorithmic approach to solvingproblems. in the words of donald knuth, one of the leaders of cs&e:cs&e is a field that attracts a different kind of thinker. i believe that one whois a natural computer scientist thinks algorithmically. such people are especiallygood at dealing with situations where different rules apply in different cases;they are individuals who can rapidly change levels of abstraction, simultaneouslyseeing things "in the large" and "in the small."2the second key theme is the selection of appropriate representations ofinformation; indeed, designing data structures is often the first step in designingan algorithm. much as with physics, where picking the right frame of referenceand right coordinate system is critical to a simple solution, picking one datastructure or another can make a problem easy or hard, its solution slow or fast.the issues are twofold: (1) how should the abstraction be represented, and(2) how should the representation be properly structured to allow efficient accessfor common operations? a classic example is the problem of representing parts,suppliers, and customers. each of these entities is represented by its attributes(e.g., a customer has a name, an address, a billing number, and so on). eachsupplier has a price list, and each customer has a set of outstanding orders to eachsupplier. thus there are five record types: parts, suppliers, customers, price, andorders. the problem is to organize the data so that it is easy to answer questionslike: which supplier has the lowest price6 what is computer science and engineering?164computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.on part p?, or, who is the largest customer of supplier s? by clustering relateddata together, and by constructing auxiliary indices on the data, it becomespossible to answer such questions quickly without having to search the entiredatabase.the two examples below also illustrate the importance of properrepresentation of information: a "white pages" telephone directory is arranged by name: knowing thename, it is possible to look up a telephone number. but a "crisscross"directory that is arranged by number is necessary when one needs toidentify the caller associated with a given number. each directorycontains the same information, but the different structuring of theinformation makes each directory useful in its own way. a circle can be represented by an equation or by a set of points. a circleto be drawn on a display screen may be more conveniently representedas a set of points, whereas an equation may be a better representation if aproblem calls for determining if a given point lies inside or outside thecircle.a computer program expresses algorithms and structures information using aprogramming language. such languages provide a way to represent an algorithmprecisely enough that a "highlevel" description (i.e., one that is easily understoodby humans) can be mechanically translated ("compiled") into a "lowlevel"version that the computer can carry out ("execute"); the execution of a programby a computer is what allows the algorithm to come alive, instructing thecomputer to perform the tasks the person has requested. computer programs arethus the essential link between intellectual constructs such as algorithms andinformation representations and the computers that enable the informationrevolution.computer programs enable the computer scientist and engineer to feel theexcitement of seeing something spring to life from the "mind's eye" and ofcreating information artifacts that have considerable practical utility for people inall walks of life. fred brooks has captured the excitement of programming:the programmer, like the poet, works only slightly removed from purethoughtstuff. he builds castles in the air, creating by the exertion of theimagination . . . . yet the program construct, unlike the poet's words, is real inthe sense that it moves and works, producing visible outputs separate from theconstruct itself . . . . the magic of myth and legend has come true in our time.one types the correct incantation on a keyboard, and a display screen comes tolife, showing things that never were nor could be.36 what is computer science and engineering?165computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.programmers are in equal portions playwright and puppeteer, working as anovelist would if he could make his characters come to life simply by touchingthe keys of his typewriter. as ivan sutherland, the father of computer graphics,has said,through computer displays i have landed an airplane on the deck of a movingcarrier, observed a nuclear particle hit a potential well, flown in a rocket atnearly the speed of light, and watched a computer reveal its innermost workings.4programming is an enormously challenging intellectual activity. apart fromdeciding on appropriate algorithms and representations of information, perhapsthe most fundamental issue in developing computer programs arises from the factthat the computer (unlike other similar devices such as nonprogrammablecalculators) has the ability to take different courses of action based on theoutcome of various decisions. here are three examples of decisions thatprogrammers convey to a computer: find a particular name in a list and dial the telephone number associatedwith it. if this point lies within this circle then color it black; otherwise color itwhite. while the input data are greater than zero, display them on the screen.when a program does not involve such decisions, the exact sequence ofsteps (i.e., the ''execution path") is known in advance. but in a program thatinvolves many such decisions, the sequence of steps cannot be known inadvance. thus the programmer must anticipate all possible execution paths. theproblem is that the number of possible paths grows very rapidly with the numberof decisions: a program with only 10 "yes" or "no" decisions can have over 1000possible paths, and one with 20 such decisions can have over 1 million.algorithmic thinking, information representation, and computer programsare themes central to all subfields of cs&e research. box 6.1 illustrates a typicaltaxonomy of these subfields. consider the subarea of computer architecture.computer engineers must have a basic understanding of the algorithms that willbe executed on the computers they design, as illustrated by today's designers ofparallel and concurrent computers. indeed, computer engineers are faced withmany decisions that involve the selection of appropriate algorithms, since anyprogrammable algorithm can be implemented in hardware. through a betterunderstanding of algorithms, computer6 what is computer science and engineering?166computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.engineers can better optimize the match between their hardware and the programsthat will run on them.box 6.1 a taxonomy of subfields in cs&e algorithms and data structures programming languages computer architecture numeric and symbolic computation operating systems software engineering databases and information retrieval artificial intelligence and robotics humancomputer interactioneach of these areas involves elements of theory, abstraction, anddesign. theory is based on mathematics and follows the mathematician'smethodology (defining objects, proving theorems); abstraction is based onthe investigative approach of the scientist (hypothesizing, makingpredictions, collecting data); design is based on the methodology of theengineer (defining requirements and specifications, implementing a system,testing a system).source: peter denning, douglas e. comer, david gries, michael c.mulder, allen tucker, joe turner, and paul r. young, "computing as adiscipline," communications of the acm, volume 32(1), january 1989, pp.9œ23.those who design computer languages (item two in box 6.1) with whichpeople write programs also concern themselves with algorithms and informationrepresentation. computer languages often differ in the ease with which varioustypes of algorithms can be expressed and in their ability to represent differenttypes of information. for example, a computer language such as fortran isparticularly convenient for implementing iterative algorithms for numericalcalculation, whereas cobol may be much more convenient for problems that callfor the manipulation and the input and output of large amounts of textual data.the language lisp is useful for manipulating symbolic relations, while ada isspecifically designed for "embedded" computing problems (e.g., realtime flightcontrol).the themes of algorithms, programs, and information representation alsoprovide material for intellectual study in and of themselves,6 what is computer science and engineering?167computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.often with important practical results. the study of algorithms within cs&e is aschallenging as any area of mathematics; it has practical importance as well, sinceimproperly chosen algorithms may solve problems in a highly inefficientmanner, and problems can have intrinsic limits on how many steps are needed tosolve them (box 6.2). the study of programs is a broad area, ranging from thehighly formal study of mathematically proving programs correct to very practicalconsiderations regarding tools with which to specify, write, debug, maintain, andmodify very large software systems (otherwise called software engineering).information representation is the central theme underlying the study of datastructures (how information can best be represented for computer processing) andmuch of humancomputer interaction (how information can best be represented tomaximize its utility for human beings).box 6.2 about the study of algorithmshow many steps are necessary to solve a given problem? thisquestion led to the development of the area known as computationalcomplexity. consider alphabetizing a list of 1000 names. a straightforwardalgorithm ("insertion sort") takes on the order of a million (i.e., 1000 × 1000)onetoone comparisons of names in the worst case, but a clever algorithm("heap sort") would take just 10,000 comparisons in the worst case (1000 ×log2 1000 or about 1000 × 10). further, this is the best possible result, for ithas been shown that sorting a list of n items requires n log2 n pairwisecomparisons in the worst case, no matter what algorithm is used.theoreticians have found arguments that apply to whole classes ofalgorithms and problems, opening questions about computing that have notyet been solved.abstractions in computer systemswhile algorithmic thinking, computer programs, and informationrepresentation are the key intellectual themes in the study of cs&e, the design,construction, and operation of computer systems require talents from a widerange of fields, such as electrical engineering for hardware, logic andmathematical analysis for writing programs, and psychology for the design ofbetter user interfaces, to name just a few. this breadth reflects the fact thatcomputer systems are among6 what is computer science and engineering?168computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.the most complicated objects ever created by human beings. for example, thefastest computers today have around 100 billion transistors, while the personalcomputer on one's desk may have "only" a few million. people routinely usecomputer programs that are hundreds of thousands of lines long, and the largestsoftware systems involve tens of millions of lines; printed at 50 lines per paperpage, such a system might weigh several tons.one of the most effective ways to cope with such complexity is to useabstractions. abstraction is a generic technique that allows the human scientist orengineer to focus only on certain features of an object or artifact while hiding theothers. however, while scientists in other disciplines typically use abstractions asa way to simplify calculations for purposes of analysis, those in cs&e areconcerned with abstractions for purposes of synthesis: to build working computersystems. other engineering disciplines also use abstractions as the basis ofsynthesis, but the "stuff" of these disciplinesšengineered and created artifactsšis ultimately governed by the tangible reality of nature, which itself imposesstructure on these abstractions. computer programs are not similarly limited;instead, they are built out of ideas and information whose structuring isconstrained only by human imagination. this extraordinary flexibility instructuring information has no analog in the material world.the focus of the computer scientist or engineer in creating an abstraction isto hide the complexity of operation "underneath the abstraction" while offering asimple and useful set of services "on top of it." using such abstractions is cs&e'sprincipal technique for organizing and constructing very sophisticated computersystems. one particularly useful abstraction uses hardware, system software, andapplication software as successive layers on which useful computer systems canbe built.at the center of all computer systems is hardware, i.e., the portion of acomputer system that one can see and touch. hardware is divided into threeprincipal components: processors. processors perform the arithmetic and logical operations,much like the arithmetic operations available in a handheld calculator.processors also handle conditional behavior, executing one or anotherset of operations depending on the outcome of some decision. memory (shortterm and longterm). memory hardware is like thememory function of a calculator, since it can be used to save and laterretrieve information. such information may be lost in a calculator whenthe power is turned off, as it is in the shortterm memory of6 what is computer science and engineering?169computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.most computer systems. computer systems also need a longtermmemory that doesn't forget when the power is lost. communication (usermachine and machinemachine). for computers tobe useful, they must be able to communicate with people, and so displayscreens and keyboards are critical components of computer systems. formaximum usefulness, computers must be able to communicate withother computers, and so computers are often equipped with modems orother network connections that can transmit and receive data to and fromother computers.readers familiar with personal computers are likely to have encountered thetechnical names or brand names for these three types of hardware. personalcomputers might use an intel 80486 processor as the processor hardware,dynamic random access memory (dram) as the shortterm memory hardware,and disks as the longterm memory hardware. the usercomputer communicationhardware for personal computers is the keyboard, mouse, and video screen, whileexamples of machinemachine communication hardware are telephone modemsand networks such as appletalk and ethernet.while the characteristics of the underlying hardware are what ultimatelydetermine the computational power of a computer system, direct manipulation ofhardware would be cumbersome, difficult, and errorprone, a lesson learned inthe earliest days of computing when programmers literally connected wires toprogram a computer. thus computer scientists and engineers construct a layer of"system software" around the hardware.system software hides the details of machine operation from the user, whileproviding services that most users will require, services such as displayinginformation on a screen, reading and writing information to and from a diskdrive, and so on. system software is commonly divided into three components: operating systemšsoftware that controls the hardware and orchestrateshow other programs work together. the operating system may alsoinclude network software that allows computers to communicate withone another. toolsšthe software (e.g., compilers, debuggers, linkers, databasemanagement systems) that allows programmers to write programs thatwill perform a specific task. compilers and linkers translate "highlevel"languages into machine language, i.e., the ones and zeros that governmachine operation at the lowest level. debuggers help programmers tofind errors in their work. database management systems store, organize,and retrieve data conveniently. user interfacešthe software that enables the user to interact with themachine.6 what is computer science and engineering?170computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.once again, personal computer users are likely already familiar with thebrand names of these pieces of system software. msdos in ibm personalcomputers and the system in macintoshes are examples of operating systemsoftware; novell is the brand name of one type of widely used networkingsoftware; dbase iv is a popular database management system; microsoft basicand borland c are examples of compiler system software; and microsoftwindows and the macintosh desktop are examples of user interface systemsoftware.while the services provided by system software are usually required by allusers, they are not in themselves sufficient to provide computing that is useful insolving the problems of the end user, such as the secretary or the accountant orthe pilot. software designed to solve specific problems is called applicationssoftware; examples include word processors, spreadsheets, climate models,automatic teller machines, electronic mail, airline reservation systems,engineering structural analysis programs, realtime aircraft control systems, andso on. such software makes use of the services provided by system software asfundamental building blocks linked together in such a way that useful computingcan be done. examples of applications software include wordperfect and word(wordprocessing applications) and lotus 123 and excel (spreadsheetapplications), and there are as many varieties of applications software as there aredifferent computing problems to solve.the frequent use of system software services by all varieties of applicationssoftware underscores an important economic point: providing these services insystem software means that developers of applications software need not spendtheir time and effort in developing these services, but rather can concentrate onprogramming that is specifically related to solving the problem of interest to theend user. the result is that it becomes much easier to develop applications,leading to more and higherquality computing applications than might otherwisebe expected.this description of layering a computer system into hardware, systemsoftware, and applications software is a simple example of abstraction.5 (box 6.3explains why abstraction is so powerful a tool for the computer scientist andengineer.) but it suffices to illustrate one very important use of abstraction incomputer systems: each layer provides the capability to specify that certain tasksbe carried out without specifying how they should be carried out. in general,computing artifacts embody many different abstractions that capture manydifferent levels of detail.a good abstraction is one that captures the important features of an artifactand allows the user to ignore the irrelevant ones. (the features decided to beimportant collectively constitute the interface6 what is computer science and engineering?171computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.box 6.3 why is abstraction possible?although abstractions are pervasive in the construction of moderncomputer systems, it is not obvious that abstractions should work. how is itthat the abstractions between the user and the machine do not reduce theavailable computational capability?results from the theory of computation provide the answer. in 1936,the british mathematician alan turing explored the computationalcapabilities of an abstract computing device, now known as the turingmachine. this work laid the scientific foundations for the age of computingand led directly to modern computers with internally stored programs.turing showed that there exist universal computing machines that cansimulate the performance of any computing machine, provided theuniversal machine has a description of the instruction set of the machinebeing simulated and has sufficient memory.this crucial insight underlies much of computing, and is fundamentalto: the interchangeability of hardware and software, since a universalmachine with a very small instruction set (i.e., very little hardware) cansimulate, with adequate programming, a computer with a much largerinstruction set. the existence of highlevel computer languages. such languages aremuch more easily understood by human beings than are the "machinelanguage" of ones and zeroes that machines can execute. programswritten in these languages can be translated into the equivalent machinelanguage programs, yielding the desired result without sacrificingexpressive power. the ubiquity of computer "bugs," i.e., mistakes in programs. bugs arepossible because a computer that is universal can perform any possiblecomputation, the wrong ones as well as the right ones. but the differencebetween a program that does the right thing and one that does the wrongthing may be as small as one bit; a small mistake in a program can leadthe computer to execute a computation very different from the oneintended.in general, the different layers of the computing system tame the rawuniversal power of the hardware by hiding (but not eliminating) thesensitivity of the underlying machine to the exact formulation of its program.successive layers make the computer much easier to use and permit theuse of higherlevel languages in which the desired computation can still bedescribed very precisely but with less chance of error.6 what is computer science and engineering?172computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.of the artifact to the outside world.) by hiding details, an abstraction canmake working with an artifact easier and less subject to error. but hiding detailsis not cost freešin a particular programming problem, access to a hidden detailmight in fact be quite useful to the person who will use that abstraction. thusdeciding how to construct an abstraction (i.e., deciding what is important andirrelevant) is one of the most challenging intellectual issues in cs&e.a simple example of this issue is the writing of a system program thatdisplays data on a screen. the program may allow the user to specify the size ofthe screen, or it may assume that "one screen size fits all." in the first case, theuser is given control over the screen size, at the cost of having to remember tospecify it every time this program is used. in the second case, the user does notneed to choose the screen size, but also loses the flexibility to use the program onscreens of different size.a second challenging issue is how to manage all of the details that arehidden. the fact that they are hidden beneath the interface does not mean thatthey are irrelevant, but only that the computer scientist or engineer must designand implement approaches to handle these details "automatically," i.e., withoutexternal specification. decisions about how best to handle these details aresubject to numerous tradeoffs. for example, a designer of computer systems mayface the question of whether to implement a certain function in hardware or insoftware. by implementing the function in hardware, the designer gains speed ofexecution, but at the cost of making the function very difficult to change. afunction implemented in software executes more slowly, but is much easier tochange.abstractions enable computer scientists and engineers to deal with largedifferences of scale. at the highest level of abstraction, a person editing adocument in a wordprocessing program needs only to mark the start and the endof the block of text and then press the del key to delete the block of text. butthese few keystrokes can initiate the execution of thousands or tens of thousandsof basic instructions in the machine's hardware. only by inserting abstractionsintermediate between the user's keystrokes and the basic machine instructions canthose keystrokes be predictably translated into the correct set of instructions.thus the programmer who writes the wordprocessing program will provide oneabstraction (in this case called a subroutine) that will delete the block from thescreen, a second to reformat and redisplay the remaining text, and a third to savethe changed document to disk. within each of these abstractions will be lowerlevel abstractions that perform smaller tasks; each succes6 what is computer science and engineering?173computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.sive lowerlevel abstraction will control the execution of ever smaller numbers ofbasic machine instructions.abstractions are also central to dealing with problems of different size (e.g.,searching a database with a thousand or a billion records) or hardware ofdifferent capability (e.g., a computer that performs a million or 10 billioncalculations per second). ideally, the user ought to be presented with the samehighlevel abstraction in each of these cases, while the differences in problem sizeor hardware capability ought to be handled at lower levels of abstraction. in otherwords, the user ought not be obligated to change his or her approach simplybecause the problem changes in size or the hardware becomes more capable.however, in practice today, users must often pay logical and conceptualattention to differences in hardware capability and problem size. querying adatabase of a billion records requires a strategy different from one for querying adatabase of a thousand records, since narrowing the search is much more difficultin the larger case; writing a program to run on a computer that performs 10 billioncalculations per second most likely requires an approach different from one for aprogram written to run on a computer that performs one million calculations persecond, since the former is likely to be a parallel computer and the latter a serialcomputer.bridging the gap between today's practice and the idealšmaking the"oughttobe's" come truešis the goal of much cs&e research today.selected accomplishmentsthis section is intended to be partly tutorial (describing some of theintellectual issues in the field) and partly historical (describing accomplishmentsthat have had some impact on computing practice). the committee also wishes tobring to the reader's attention a 1989 report of the nsf advisory committee forcomputer research, computer science: achievements and opportunities, thatprovides a good discussion of the accomplishments of cs&e from theperspective of the field's own internal logic and intellectual discipline.6 further,the committee stresses that this sampling of intellectual accomplishments doesnot differentiate between those made by academia and those made by industry; asthe discussion in chapters 1 and 2 suggests, the determination of "credit" for anygiven accomplishment would be a difficult task indeed.6 what is computer science and engineering?174computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.systems and architecturescomputingsystem architects and building architects play similar roles: bothdesign structures that satisfy human needs and that can be constructedeconomically from available materials. buildings have many sizes, shapes,styles, and purposes; similarly, computing systems range from the singlemicroelectronic chips that animate calculators, fuelinjection controls, cardiacpacemakers, and telephoneanswering machines, to the geographically distributednetworks of thousands of computers. just as buildings are constructed from avariety of materials, so also do computing systems incorporate many differentmanufacturing processes and technologies, from microelectronic chips, circuitboards, and magnetic disks to the software that tailors the machine to anapplication or for general programming use. standardization yields economiesboth in buildings and in computers; for example, precut 8foot studs and premade windows are commonly used in residential houses, whereas 8bit bytes,commodity processor and memory chips, and standardized programmingnotations and system conventions may be used in many different models ofcomputing systems.for both computing systems and buildings, the conceptual distance from theavailable materials and technologies to the requirements established by societyand the marketplace is so great that a diversity of designs might satisfy a givenrequirement, and the task of producing any complete design is extremelycomplex. how, starting with a "blank slate" of silicon and access to computeraideddesign and programming tools, does someone create a system to playchess, to process radiotelescope signals into images, or to handle financialtransactions in a bank? the complexity is managed by applying the abstractionprinciples described above in "abstractions in computer systems."the foundation of nearly all computing systems today is microelectronics.even within the design of a microelectronic chip, the task is organized aroundsuch specialties as circuit design, logic design, system organization, testing, andverification, and each of these specialties must deal with physical design andlayout together with logical behavior. design techniques similar to thoseemployed for chips are applied at higher levels to assemble aggregates of chips oncircuit boards, and to provide interfaces to disks, displays, communicationnetworks, and other electronic or electromechanical systems.microelectronics technology has blurred the boundary between hardwareand software. programs written in specialized notations are commonly built intochips. systems designed for a single pur6 what is computer science and engineering?175computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.pose may, for example, incorporate application software that is compiled intosuch circuit structures as programmed logic arrays or into microprograms thatreside in readonly memories. computing systems also employ builtin programsto provide for maintenance, initialization, and startup.generalpurpose computing systems require additional layers of software toprovide the standardized system services needed to execute a variety ofapplications. the operating system allocates resources to run multiple programs,handles input and output devices, maintains the file system, and supportsinterprocess and network communications. application programs may alsorequire packages that provide standard interfaces for windows, graphics,databases, or computation, whether local or remote.the remarkable advances in the performance, performancecost ratio, andprogrammability of computing systems have been the combined result ofachievements of cs&e within each of these layers. although these advanceshave built on improvements in the base technologies, computing scientists andengineers have exploited these improvements extremely rapidly. the followingsubsections describe achievements within these layers: microelectronics; theorganization of processors, memories, and communication; operating systems;computer networks; and database systems.microelectronicsthe history of microelectronics is one of increasing the density of circuitryon a chip; the smaller the transistors and wires within a chip, the faster they canwork, and the less energy they require. the process by which a microelectronicchip is fabricated is independent of the particular circuitry that will ultimatelyreside on the chip. the chip designer creates informationprocessing, memory,and communication structures within the chip by defining geometric patterns on aset of 10 to 15 photomasks; these patterns define the transistors and wires thatwill be embedded in the chip. some of these wires lead to connections external tothe chip that allow it to be hooked up to other chips and components.in the 1960s, early integrated circuits (i.e., smallscale and mediumscaleintegration (ssi and msi) chips) implemented electronic gates, adders, selectors,decoders, register memories, and other modules that had previously requiredcircuit boards of discrete transistors and other electronic components.photomasks for ssi and msi chips were simple enough that they could be cut byhand from large sheets of plastic film before being photographically reduced.6 what is computer science and engineering?176computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.the largescaleintegration (lsi) chips of the 1970s contained thousands totens of thousands of components and included the dynamic randomaccessmemory (dram) chips that eventually displaced the magneticcore memory, andthe first singlechip processors. these lsi chips were too complex for theirphotomasks to be created without computeraideddesign (cad) tools. however,the cad tools of that era served principally as drawing aids, a computerizedextension of the drafting board. they did not otherwise assist designers inmanaging the complexity of their creations.by the late 1970s, it was widely recognized that continued advances inmicroelectronics depended at least as much on streamlining the design processesand on new system ideas as on improving the fabrication processes. the timerequired to design thensophisticated chips had already grown from weeks toyears. few people were prepared to address the functions (other than memory)that more complex chips would serve. one of the great achievements of cs&ewas to show how to design the nowcurrent generation of verylargescaleintegration (vlsi) chips.many advances in chip design resulted from the application of lessonslearned from managing the complexity of large programs. structured designdisciplines based on a layoutcell hierarchy encouraged designers to composelayouts flexibly and systematically, to focus on improvements at the highlevelsof algorithms and overall organization, and to abstain from certain lowleveloptimizations (analogous to ''goto" statements in programming) that are known tobe more troublesome than useful.these structured design approaches achieved significant successes evenwhen applied using only computeraided layout tools; these achievements mightbe compared with writing structured programs in assembly language. the nextstep was to incorporate these disciplines into highlevel tools called siliconcompilers. a silicon compiler includes a "front end" that translates a behavioraldescription for a chip into an intermediate structural representation, and a "backend" that translates the intermediate representation into layout that is tailored tothe geometrical and electrical design rules of a particular fabrication process.designs produced in this way can accordingly be recreated easily for differentfabrication processes. today, many chips are designed entirely by siliconcompilation; nearly all complex chips employ these procedural approaches forcreating layouts of some of their major cells.of course, chip designs may contain errors. to reduce the cost and effortrequired to debug chip designs, tools for simulation and verification are essentialto the chip designer. the development of6 what is computer science and engineering?177computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.algorithms and programs for multilevel simulation, including where necessarysimulation down to the transistorswitch level, is the principal reason that today'smilliontransistor chips more often than not function correctly the first time theyare fabricated. symbolic verification that a chip's behavior will conform to itsspecification is not yet universal, but these techniques have been employed, forexample, to demonstrate that a floatingpoint element within a chip will behaveidentically to software routines previously used for the same functions, and toverify the logical design of processors of moderate complexity.the net result of these advances in design disciplines and tools is that today'sstateoftheart chips, although they approach being 100 times as complex asthose of a decade ago, require a comparable design effort. whereas a decade agovlsidesign courses were offered in only a few universities, today they areoffered in approximately 200 colleges and universities and have supplied theincreasing demand for chip designers. students in these courses use moderndesign and simulation tools to produce projects in a single term that aresubstantially more complex than stateoftheart chips of a decade ago.processor and memory designin recent years, it has become clear on theoretical grounds that forcomputations performed in vlsi chips and other highperformance digitaltechnologies that press against the physical limits of intrachip communication, itis less costly to perform many operations at once (in parallel or concurrently) thanit is to perform an operation correspondingly faster; see box 6.4. for example,performing a given operation 10 times faster on a chip of a given family ofdesigns would require a chip 100 times larger. however, by replicating theoriginal chip only 10 times and connecting the chips in parallel, the same speedupfactor of 10 could be obtained.7 much of highspeed computer architecture (bothon the market now and to be available in the future) can be understood today asan endeavor to increase performance with parallelism and concurrency, whichresults in only a proportional increase in area, and to avoid bruteforce attacks onserial speed, which leads into a realm of diminishing returns in performance forchip area.based on this complexity model and theory, algorithms that approachmaking optimal use of limited communication resources have been devised forsuch common operations as arithmetic, sorting, fourier transforms, matrixcomputations, and digital signal processing. analyses6 what is computer science and engineering?178computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.of such algorithms account for the patterns of data movement, often in regulartopologies such as hypercubes, meshes, and trees. these algorithms have beenapplied both to the internal design of vlsi chips and to the design of specialpurpose systems such as digital signal processors, and to the programming ofparallel and concurrent computers. the analyses of communication topologieshave, in addition, been important to the design of the communication networksused in programmable parallel and concurrent computers.box 6.4 limits on chip designearly complexity theory originally emphasized the number of operationsand the storage required to solve a given problem, since these parameterswere the primary cost drivers for solving a problem. but most of amicroelectronic chip is devoted to wirebased intrachip communicationsbetween its various elements. a specific complexity theory was thereforedeveloped for verylargescaleintegration (vlsi) circuitry that accountedfor the cost and time of communication in parallel, concurrent, anddistributed algorithms running on a vlsi chip.vlsi complexity theory relates the chip's area a and the time t itrequires to perform a given operation. optimal chip designs were shown tohave the property that for any chip, at2 is equal to a constant that dependsonly on the base technology and the nature of the operation. for example,for multiplication of nbit binary integers, optimal chip designs have theproperty that at2 is greater than or equal to kn2, where the coefficient kincorporates characteristics of the target technology and problem. thus,within the class of optimal designs, it is possible to trade off area (e.g., alarger chip) and time (e.g., faster operation).vlsi technologies led to a renaissance in many computer architecturesduring the 1980s. an example is the emergence of reducedinstructionsetcomputers (riscs), which provide a good illustration of why singlechipprocessor performance has advanced more rapidly than might have been expectedsimply from technology gains. as we have seen, vlsi technology favorsconcurrency. riscs exploit concurrency by employing a pipeline structure thatcan overlap the execution of several instructions. dependencies betweensequential instructions can defeat this approach, but riscs employ compilers toanalyze dependencies and to generate object code (i.e., machine instructions) toschedule the pipeline in advance.6 what is computer science and engineering?179computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.highspeed processors also place severe demands on the memory system.again, the solutions include parallelism and concurrency: wider data paths toconvey more bits in parallel, separate memories for instructions and data, andmultiple banks of memories. these innovations were first applied to thesupercomputers of the 1960s, and they are applicable to singlechip design issuesas well. fast memories require more area per bit than do slow memories; it isaccordingly not economical to implement the primary memory of a computingsystem entirely from fast memories. a more sensible organization employs asmall, fast memory called a cache to store frequently accessed items. if theprocessor makes reference mostly to items that are stored in the cache, memoryoperations can be speeded up considerably. of course, the cache memory musthave a way of determining what items will be most frequently accessed. analysisof cache sizes, organizations, and replacement strategies has been an active andproductive area of research and development. just as advances in processordesign have depended on compiler technology, so also is cache design starting tobenefit from compiler analysis and optimization of the patterns of memoryaccesses.operating systemsone of the important, practical ideas that led to modern operating systemswas timesharing, invented in the early 1960s. operating systems are part of thesystem software that provides frequently needed functions to the user. theearliest operating systems limited the computer to running one user program at atime: these systems could not start the next program until the preceding one hadterminated. a timesharing operating system interleaves execution over shortintervals between several executable programs available in the machine. thisinterleaving is equivalent to executing a number of programs concurrently on anumber of somewhat slower machines. before the invention of timesharing,interactive tasks required a computer dedicated to that task, whereas withtimesharing, a large number of users can tap into a single machine that providesbursts of computation on demand.the introduction of hardware addresstranslation, memoryprotection, andsystemcall mechanisms under the control of a multiprogramming operatingsystem made timesharing a good way to increase the efficiency of a machine's useby allowing multiple processes to reside in memory at the same time, andprovided processes with virtual memory. a "process" is an instance of a programduring its execution, including both the program's instructions and data.processes are the basic schedulable units of activity or execution for6 what is computer science and engineering?180computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.nearly everything an operating system does. execution of a user program,updating a graphics window, transferring data from or to an open file, andcontrolling the operation of an inputoutput device can be regarded as processesthat execute concurrently. "virtual memory" refers to the ability of a process torefer to a larger range of memory addresses than may be present in primarymemory at any one time.in addition to their demand for execution cycles, processes require memoryresources. the physical memory of a computer is limited and contains layers ofincreasing size and access time: machine registers are few but very fast, primarymemory is intermediate, and disks store large volumes of data but with largeaccess times. the greatest disparity in access time, hence, the most critical choicefor memory allocation, occurs between the disks and the primary memory. shouldprograms explicitly manage the memory actually available by bringing data andsubroutines from disk on demand, or can the operating system perform thesefunctions? theory and practice have shown that programs do not need to becomplicated with the extra task of memory management; indeed, operatingsystems that manage memory for many processes at once can do a better joboverall than is possible when applications programs handle memory managementindividually.paging is one technique operating systems use to provide virtual memory. incommon with other caching strategies, paging is based on the likelihood that theinstruction and data words accessed are the same as or located close to previouslyaccessed words. a paging strategy partitions memory into frames of, forexample, four kilobytes each. a frame can hold a page of information that can bemoved between disk and primary memory. put simply, the operating system'sgoal is to make it likely that the pages involved in active computations areavailable in the primary memory, and that inactive pages remain on disk.accessing a page that is not in the primary memory is called a page fault. theoperating system must then intervene to move the needed page from disk toprimary memory.what happens when the primary memory is full with active pages, andanother page must be loaded? one algorithm is to replace the least recently used(lru) page, or any page not used recently. the operating system decides on thereplacement based only on its record of the pageaccess behavior of the processeswhose pages it manages, and not on information about their internal structure.the lru algorithm generally performs well in an environment of unrelatedconcurrent processes for the same reason that paging works: programs tend toaccess memory locations that are local to recently accessed locations, acharacteristic referred to as locality of reference.6 what is computer science and engineering?181computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.processes may need to communicate with each other. that need existed 30years ago when timesharing was applied to single machines but has becomeincreasingly necessary now that processes that are parts of the same computationare commonly distributed across networks of computers.applications that employ communicating concurrent processes may requirecertain actions to be performed by no more than one process at a time, or to bedone in a particular order. suppose, for example, that processes x and y share theuse of a file that contains a record of your bank balance, and that both processesattempt to record deposits. processes x and y can each have read the initialbalance, added their own deposit, and written the new balance back into the file.if the processes are interleaved so that the two read operations precede thetwo write operations, one of the deposits will be lost when one process overwritesthe result of the other. what is necessary to prevent this error is to lock the fileagainst other accesses when the first process requests the balance, and to unlockthe file after its deposit transaction is completed. problems with the order ofexecution may arise in situations where one process cannot proceed becauseanother process has not acted. for instance, if process x reserves airline seats andprocess y records cancellations, x must wait for y whenever all seats for a flightare booked.computer scientists and engineers invented synchronization mechanisms todeal with these problems long before they became common in practice. the firstsuch mechanism employed operations that are analogous to the way semaphoresprotect railroad sections against conflicting train traffic. a critical section ofprogram that must not be executed by more than one process can be entered onlywhen the semaphore is green, and entering has the sideeffect of turning thesemaphore red. the semaphore is set back to green when the process that enteredhas completed the execution of the critical code.when the semaphore has been set back to green, which of several waitingprocesses will be allowed to enter the critical section? a priority queue thatrecords the processes waiting for the critical section can ensure fairness, so thatall waiting processes will eventually be served. once these properties of logicalcorrectness and fairness are assured, what is the performance of these solutions?queuing models have been devised that lead to solutions that are optimaldepending on the particular characteristics of the concurrent processes.a nasty problem not solved by synchronization mechanisms is deadlock. adeadlock arises due to mutual dependencies between processes, such as process xusing resource a and requesting the use6 what is computer science and engineering?182computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.of additional resource b without giving up a, and process y using resource b andrequesting the use of a without giving up b. when these processes fail to makeprogress, all services dependent on them cease to function, eventually halting theentire system. (this is one reason that the distributed systems used by banks andairlines sometimes cease to function.) proving that a system of concurrentprocesses will operate correctly and will not deadlock is generally harder thanproving the correctness of a single sequential program. reasoning aboutconcurrent processes has been aided by temporal logic, a firstorder predicatelogic with the addition of temporal notions such as "forever" and "eventually."in common with other systembuilding disciplines, the design of operatingsystems illustrates how progress has benefitted from theoretical work, has beenstimulated by practical ideas, and has been responsive to changes in technologiesand needs.data communications and networkingcomputer technology is becoming increasingly decentralized; the personalcomputers on office desks are an example of this trend. similarly, centralizedcorporate control is passing down the hierarchy, and in many cases the hierarchyitself is flattening. to provide connectivity among these distributed parts,computer networks have proliferated at an amazing speed in business,government, and academia. in fact, perhaps the only remaining aspect ofinformation technology that remains in the hands of central management is thenetwork itself.the first successful largescale computer network was the experimentalarpanet, first deployed in 1969 to provide terminaltocomputer andcomputertocomputer connectivity to major cs&e departments in the nation.arpanet was based on packetswitching technology (box 6.5). the networkitself consisted of many switches, each connected to at least one and usuallyseveral others so that there were multiple paths between any two. thecommunication lines connecting the switches were not used exclusively for anyparticular endtoend communication. rather, each line was used by a particularmessage only when one of its packets happened to be transmitted over that line.packet switching provided highspeed, costeffective connectivity betweenattached devices across long distances by dynamically sharing the expensivebandwidth of highspeed lines among many users. a major benefit of packetswitching is fault tolerancešwhen a node or line fails, other nodes can simplybypass it.6 what is computer science and engineering?183computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.box 6.5 about packet switchingpacket switching, which has been made possible by highspeedcomputers, involves two separate concepts. the first concept is that amessage can be broken into discrete parts, or packets, at the originatingstation, sent in pieces through a network of switches, and reassembled atits destination. "packetizing" a message breaks a big message into manysmall ones, which in turn makes it easier to process.the second concept is demand multiplexing, a way to keep a single(expensive) circuit in relatively continuous use. this notion is not new inprinciple: transoceanic voice communications have for many years beenbased on the routing of different calls through the same circuit on a timeshared basis. in the arpanet packetswitching context, each switchroutes incoming packets to adjacent switches based on the packet'sultimate destination and based online congestion. the goal is to minimizecongestion and the number of hops a message must travel.thus the path of an individual packet through a packetswitchednetwork is determined dynamically, and the path of one packet may or maynot duplicate the path of another packet, even though each is part of thesame message. when a switch receives a packet, it decides where next tosend the packet depending on its ultimate destination and on the availablecapacity of communications lines between itself and the switches to whichit is connected.arpanet had substantial impact on architectures for commercial networkssuch as sna and decnet. it also spurred a great deal of research on datacommunications and resulted in the tcp/ip network protocol, now widely used toconnect heterogeneous computer networks.while nationwide packet networks such as the arpanet met the needs ofthe 1970s, a new requirement arose from the proliferation of personal computersin the 1980s. personal computers presented a rapidly rising demand forconnectivity among themselves, peripherals, and servers in a local officeenvironment. the cs&e research community anticipated this need in the 1970sand pioneered the hardware and software technologies for local area networks(lans). current lan technologies grew out of small research investments atxerox (ethernet) and cambridge (token ring). the growth of lans paralleledthat of personal computers in the 1980s. lans provided6 what is computer science and engineering?184computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.relatively inexpensive connectivity in an office environment, and did so at multimegabitpersecond speeds.today, lans are being interconnected to form larger networks. meanwhile,the bandwidths of wide area networks (wans) have evolved from 64 kilobits persecond to the widespread availability of economically tariffed t1 service at 1.5megabits per second (mbps). thus wan speeds are approaching the speeds ofmany lans deployed today, though lans are getting much faster, too. recentneeds for faster metropolitan area network (man) speeds have been met with theacceptance of the fiber distributed data interface (fddi) standard for mans.this is a 100mbps offering based on fiberoptic media.lans, mans, and wans were initially designed to meet datacommunications problems based on copper technology. the emergence of fiberoptic technology and its consequent 1000fold increase in bandwidth completelychanges the technology tradeoffs and requires reconceptualization of networkdesigns. this subject is the focus of effort currently under way in the nationalresearch and education network component of the highperformancecomputing and communications program, discussed in chapter 1.database systemsmodern database technology has been shaped from top to bottom by cs&eresearch. computer science researchers in the 1960s created the relational datamodel to represent data in a simple way. computer engineers worked through the1970s on techniques to implement this model. by the mid1980s these ideas wereunderstood well enough to be standardized by the international organization forstandardization (iso) as the sql language. today, the sql language hasbecome the lingua franca of the database business, for reasons described below.computerized databases began in the late 1950s, each built as a specialapplication. by the late 1960s, the network data model had emerged as ageneralization of the way these systems stored and accessed data. the networkmodel is a lowlevel approach that represents and manipulates data one record at atime.however, despite its undeniable utility at the time, the network data modelwas troubling, because it lacked a firm mathematical foundation. indeed, databasesystems at the time were ad hoc and seemed to behave in random ways, especiallyin unusual cases. database researchers needed a good theory with which topredict the behavior of databases and a data manipulation language with clearproperties, reasoning that these tools would make it easier to program database6 what is computer science and engineering?185computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.applications that could be insulated from changes to the database as the databaseevolved over the decades.the relational data model grew out of this research effort. it is a highlevel,setoriented, automatic approach to representing data. the data are structured intabular form, making them easier to visualize and display. database queries andmanipulations are based on pure mathematics (sets and relations) and theoperators on them. as a result, the model is simpler to use, and gives applicationsgreater independence from changes in technology and changes in the databaseschema. early studies showed it to improve programmer productivity by largefactors.the relational model was initially rejected by database implementors andusers because it was so inefficient. all agreed that it boosted productivity, butthere was skepticism that the nonprocedural language could ever be efficientlyimplemented.research computer engineers in academia and industry took on thechallenge of efficiently implementing the relational model. the goal was toprovide the simplicity and nonprocedural access of the relational model andperformance competitive with the best network database systems. this effortrequired the invention of many new concepts and algorithms.this research took about ten years. in the end, the relational system'sperformance began to meet and even exceed the performance of systems usingthe network data model. with this feasibility demonstration, the slow transitionfrom network to relational systems began. first, computer vendors began offeringdatabase systems based on the research prototypes. then the sql internationalstandard was approved, and customers began to use the relational approach.today, 25 years after the first research papers appeared, the transition to sqldatabases is in full swing.without the early seed work by computer scientists, the relational modelwould not have been invented. without the early seed work by computerengineers, the implementation feasibility would not have been demonstrated. andwithout these two advances, we would likely still be programming databases inlowlevel programming languages.based on the relational model, the database community spent much of the1980s investigating distributed and heterogeneous database systems. those ideasare now well understood, and the iso is about to approve a standard way fordatabases to interoperatešthe remote database access standard. this standardpromises to allow diverse computers to interchange information and to allowdatabase queries and transactions to span multiple database systems; see box 6.6for more discussion.6 what is computer science and engineering?186computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.box 6.6 reliable transaction systemssome of the cs&e work on transaction systems addresses issues thatarise in databases, distributed computing, and operating systems.consider the problem of a distributed computation running on manydifferent computers and databases. if one computer or process fails, whatare the others to do? for example, one bank may attempt an electronictransfer of money to another bank. if the receiving bank's computer fallswhile the transfer from the sending bank is in progress, then the money maydisappear, because the sending bank will have debited the right accountbut the receiving bank will not have been able to credit the right account(since it never received the message).transaction concepts and techniques are designed to simplify thedesign and implementation of operating systems for such distributedapplications. the transaction concept requires that a change in the state of asystem involved in a transaction should have the acid properties: thechange should be all or nothing (atomic), result in a correct statetransformation (consistent), be free of concurrency anomalies (isolated),and generate transaction outputs that are not lost in case of system failure(durable). if anything goes wrong before the entire transaction is complete,the use of transaction techniques will ensure either that all changes areundone (so that the databases of sender and receiver are restored to thestate that existed before the transaction began) or that changes todatabases will endure and also that all output messages will be delivered.most current database research activity focuses on adding more semantics tothe relational model and to the transaction model. this work, operating under thebanner of objectoriented databases, is still in its early stages. in addition,consistent with the applicationoriented trend of computer science in general,there is considerable interest in databases designed specifically for certainproblems: geographic databases, text databases, image databases, and scientificdatabases.programming languages, compilers, and softwareengineeringprogramming languagesadvances in science often lead to the invention of notations to express newconcepts or to simplify and unify old ones. the advent of computers spurred theinvention of programming languages, a6 what is computer science and engineering?187computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.new kind of notation for expressing algorithms in terms more appropriate tohuman discourse, while nevertheless meeting the exacting mechanical needs ofcomputers. the development of programming languages introduced newchallenges. first, more formality and greater detail were required; because ofprocessing by computer, nothing could be left for interpretation by humans.second, new conceptsbox 6.7 programming languages of significancec allows the programmer to control details of machine operation in amanner that depends only on general features common to most machinesand not on the idiosyncracies of individual machines.lisp concentrates on the manipulation of symbolicšas distinct fromarithmeticšinformation. lisp is used widely for symbolic computation, therepresentation of cognitive processes, and in systems that can reasonabout and manipulate computer programs.smalltalk was the first ''objectoriented" programming languageintegrated with a graphical environment, and was based on "active" dataobjects (i.e., objects with some executable code associated with them)coupled together by simple interfaces.prolog allows a program to be written by specifying logical relationsthat have to be satisfied by the solution of the problem. the "logicprogramming" paradigm is now spreading into database practice.visicalc was the first spreadsheet for personal computers. it simulatedbusiness ledgers and reports, automatically performing the necessaryrecalculating when any change was made to a table entry.ml provides the ability to separate the definitions of data types from themanipulations to be performed on them.tex is a highlevel typesetting and formatting language, often used fortechnical manuscripts, and is the basis for electronic submission of papersto journals of the american mathematical society.postscript is a language developed for use by laser printers ratherthan people; its purpose is to express the detailed layout of pages of textand pictures, providing the link between typesetting programs and printersof different manufacture.6 what is computer science and engineering?188computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.had to be developed to deal with execution of an algorithm; previously,mathematics dealt primarily with a more static world. third, a conflict betweentransparency of notation and efficiency of execution was introduced; programshave to be reasonably efficient as well as perspicuous.since the dawn of the computer age, many developments in the area ofprogramming language design have emerged, developments that are reflected in asuccession of programming languages that make it easier to express algorithmsprecisely in various problem domains. many are familiar with mainstreamlanguages descended from fortran via algol and cobol through countlessvariants such as basic and pascal. however, though the bulk of programmingtoday is done in these languages, there are countless alternate styles, as noted inbox 6.7. without the development of the concepts that have been embedded inthese languages as well as the languages themselves, the information age wouldnot be so advanced.compilerscompilers provide the essential link between programming languages (i.e.,the readable, higherlevel, problemoriented text written by human beings) to thelowlevel machine encodings that actually govern the operation of a computer.advancements in compiler technology have been remarkable in the past fewdecades. for example, the first fortran compiler took dozens of personyears tobuild with what were essentially ad hoc techniques. today, the same effort isrequired to develop compilers that handle languages of much greater complexityand sophistication. this vast increase in productivity is due partly to betterprogramming environments and more powerful computers, but most significantlyto a sound theoretical understanding of compiler techniques that allow a highdegree of automation for much of the compiler writing process. several waves oftheory have contributed to compiler technology: formal languages, semanticanalysis, and code optimization and generation.formal language theory provided the understanding of grammar upon whichthe mechanical parsing in every modern compiler is built, and that shaped thevery form of programming languages. the theory provides the mathematicalbasis for engineering the front ends of compilers, where the structure of a programis extracted from a program text. the subject of parsers is now so well understoodthat their construction has been highly automated. from a precise definition of thesyntax of a programming language a parser can be generated in a very short time.6 what is computer science and engineering?189computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.formal theories of semantic analysis gave accurate descriptions of themeaning of programming languages, which has led to more predictable languagesand in some commercial instances to automatic generation of compilers fromlanguage specifications. in concert with type theory, formal semantics haveenabled compiler writers to implement languages that support these advancedmodes of expression in a clean and rigorous manner. one result has been thatpolymorphic programming systemsšprogramming systems that can support thedesign of algorithms that work independently of the type of input datašcan nowbe transported from one computing environment to another with a minimum ofdifficulty.techniques for analyzing programs have been developed that enableoptimizing compilers to speed up the execution of programs substantially. animportant approach is called dataflow analysis, a method for determining wherevalues once computed in the program can possibly be used later in the program.this information is now routinely applied in optimizations such as registerallocation, common subexpression elimination, and strength reduction in loops.register allocation attempts to keep the most frequently used quantities in themost accessible storage. common subexpression elimination attempts to identifyand obviate the need to recalculate equal quantities at different points of aprogram. strength reduction in loops amounts to identifying instances of repeatedevaluation of polynomial expressions that can instead be updated by finitedifferences, thus simplifying the computation.more elaborate optimization is required to compile sequentially writtenprograms to make effective use of parallel hardware architectures. although goodparallelizing fortran compilers exist for particular computers, this technology isstill far from routine.software engineeringsoftware engineering refers to the construction of software systems. whilethere is some controversy in cs&e regarding the extent to which the"engineering" in software engineering is truly sciencebased engineering (asopposed to management), it is clear that large software systems (in excess ofseveral million lines) can and have been built. without many of the softwareengineering tools that have been developed in the last 30 years, the constructionof large software systems would not be possible. these tools have been developedin response to pragmatic needs, but they may well incorporate some of theproducts of research. two generic types of software engineering tools aredescribed below. in addition, software reuse,6 what is computer science and engineering?190computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.an issue of generic concern to software engineers, and realtime computing, acomputing application of great engineering importance to society, are discussed.project control systems the development of largescale software systems isparticularly problematic. a largescale software system may have millions oflines of code. writing such a system is obviously a manyperson enterprise, andthe efforts of all of these programmers need to be coordinated efficiently. projectcontrol systems enable the construction of large systems by automatingcoordination processes that would otherwise be subject to human error.for example, large systems are written in modular form, to facilitatedebugging. modules must be relatively small (hundreds of lines of code), so thatthe programmer can understand the details of the module. any given module mayexist in several different versions, perhaps an early version that is known to workproperly but implements only basic functions and a later one in progress that isbeing enhanced. to assemble the large system (e.g., for systemlevel testing), it isnecessary to gather together the working versions of each module, taking intoaccount the fact that a change in module a may mean that an earlier version ofmodule b must be included. assembly can be performed manually, but with tensof thousands of modules to be gathered, it is inevitable that mistakes will bemade. automated configuration management systems keep track of a myriad ofbookkeeping details, enabling such assembly to proceed with far fewer errors.source code control systems the source code of a software system iswritten by human programmers. when source code becomes voluminous andinvolves a team effort of many programmers, management is often difficult.source code control systems automate many managementrelated tasks. forexample, such systems can ensure that only one person is making changes to anygiven module at any time, and that all changes are recorded (usually through anautomated comparison of the new file to the old version) so that there is a trail ofimplementation responsibility comparable to the change history found onengineering drawings."discovery tools" are an important adjunct to source code control. thesetools provide for navigation within a large library of software so that developersand maintainers can efficiently answer questions such as, what modules touchthis variable? what does that subroutine do? what is the data layout of that table?do two selected actions always happen in order? by what routes through theprogram can control get to this point? such tools help programmers understandbetter the code on which they work.6 what is computer science and engineering?191computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.as an aside, it is interesting to note that the algorithm used to compare filesin the source code control system of unix came from research; the same is truefor the flow analysis tools used in many discovery tools.software reuse an elusive grail of cs&e has been the reuse of softwareparts. just as one constructs electrical appliances or houses largely from offtheshelf parts, so would one also like to build software in a similar fashion. becausesoftware is the dominant cost of most systems, developing ways to reuse softwareis a growing concern and is being tackled in a variety of ways.in some domains, useful libraries of software components have long existed.particularly well known are the libraries that provide efficient implementations ofcommon mathematical constructs, such as trigonometric functions or matrixoperations. major packages of routines for statistics and other fields ofmathematics and engineering are also available. programming tools, asexemplified by unix, foster the construction of systems out of many small andindependent generic components coupled by a "shell" language.by generalizing from specific instances, the domain of applicability of codemay be widened. for example, the notions of polymorphism and modularityfound in ada and ml (and other languages) make it possible to write portions ofprograms that are applicable to more kinds of data, thus reducing the need towrite highly similar portions of code that differ primarily with respect to the kindof data being processed.objectoriented programming provides a different way to structure programsto increase abstraction and reduce the need for rewriting. in objectorientedprogramming, the basic procedures that operate on an object are encapsulatedwith that object. for example, an object called polygon can have associatedwith it not only a description of its sides but also a procedure for calculating itsarea. a second important feature of objectoriented programming is that anobject can "inherit" procedures of a higherlevel object but can also replace someof them with new procedures. for example, having already defined apolygon, one can then define a square as a special instance of polygonbut with a more efficient procedure for calculating its area. a program thatmanipulates polygons or squares would not include code to calculate area, sincethe object itself would provide that code. objectoriented programming provides anew flexibility in structuring programs, leading to easier development andmaintenance of large programs and programming systems.despite such developments, reuse of software is not common, and muchmore remains to be done in this area.6 what is computer science and engineering?192computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.realtime computing realtime computing refers to a mode of computingin which computing tasks must be accomplished within certain time limits. forexample, computers controlling the operation of an airplane's engines and flapsare computing in real time; failure to meet the relevant computing deadlines couldeasily lead to disaster.when a processor for a realtime computer system must handle several taskssimultaneously, how much time to give to each task and in what order they shouldbe performed are crucial considerations for the realtime programmer: sometimesa lowerpriority task must be preempted by a higherpriority one, even thoughboth must be completed on time (because the higherpriority task may depend on aresult computed by the lowerpriority one). such decisions are simple to makewhen the number of tasks is small and the amount of available computationalpower is large relative to the demands of the tasks taken in the aggregate. but as amatter of engineering practicality, the designer of an airplane or a missile doesnot have unlimited freedom to choose processors that are greatly overmatched tothe computing tasks involved.the traditional method of scheduling concurrent tasks is to lay out anexecution time line manually.8 although for simple cases it is relatively easy todetermine task execution sequences, the resulting program structure is hard tochange. for example, the specification of a task may be altered so that it demandsmore computational resources; accommodating such a change might well requireredoing the entire time line. the scheduling must accommodate the differingdeadlines, demands on resources, and priorities of the various tasks to ensure thatall tasks are completed on time.however, by taking advantage of certain features of a relatively newprogramming language, ada, it is possible, under certain circumstances, toimplement conveniently an algorithm ensuring that all tasks will be completed ontime without knowing the precise details of the execution sequence. thisalgorithmšthe rate monotonic scheduling algorithmšalso enables theconvenient accommodation of changes in task specification without redoing theentire time line, and is starting to come into use in some realtime aerospaceapplications.algorithms and computational complexityas noted above in the section "computer science and engineering,"algorithms as an intellectual theme pervade all of cs&e. however, the formalstudy of algorithms is an important subarea of cs&e research in its own right(but see box 6.8). the design and analysis6 what is computer science and engineering?193computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.of algorithms combine intellectual and theoretical challenges with the satisfactionof computational experimentation and practical results.9box 6.8 a note on terminologythe term "theory" as used within the cs&e community is oftenconstrued quite narrowly, as exemplified by work presented at conferencessuch as the acm symposium on theory of computing (stoc) and theieee computer society's symposium on foundations of computer science(focs).however, the committee believes that a narrow view of theory isexcessively limiting. indeed, it believes that "theory in cs&e," "theoreticalcomputer science," and "theoretical work in cs&e" should in fact refer to allnonexperimental work in cs&e intended to build mathematical foundationsand models for describing, explaining and understanding various aspects ofcomputing. such work includes research performed in the focsstoccommunity, such as the theory of computation, study of formal models ofcomputation, and computational complexity, but is not limited to theseareas. it also includes the analysis and synthesis of algorithms, the study offormal languages, the syntax and semantics of programming languages,compiling techniques and code optimization, principles of operatingsystems, various logics for reasoning about programs and computations,fundamentals of databases, expert systems, knowledge representation,mechanical theorem proving, heuristic search, principles of computerarchitecture, vlsi design, parallel and distributed computing, and the richtheory of numerical analysis and scientific computation.in the interests of clarity and in deference to traditional usage, thecommittee has generally used the term "theoretical work'' in this report torefer to this broader notion of theory. further, it believes that the theoreticalcs&e community will be enriched by an expansive vision of theory incs&e.algorithms everywhereone domain in which better algorithms are enormously important is scienceand engineering research. chapter 1 noted that the solution of certain partialdifferential equations has been speeded up by a factor of about 1011 since 1945. alarge part of this speedup is due to the development of faster algorithms, asoutlined in table 6.1.6 what is computer science and engineering?194computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.table 6.1 algorithmic improvements in solving elliptical partial differentialequations in three dimensionsmethodyearrun timeatimebsuccessive overrelaxation (suboptimal)19548 n5250 yearssuccessive overrelaxation (optimal)19608 n4 log2 n2.5 yearscyclic reduction19708 n3 log2 n22 hoursmultigrid197860 n317 hoursa n is the number of grid points in one linear dimension.b time required for solving an equation with n = 1000, assuming a time of 106 seconds forprocessing each of the 10003 points.source: adapted from john rice, numerical methods, software, and analysis, mcgrawhill,1983, p. 343.box 6.9 describes an algorithmic advance that made feasible acomputationally complex calculation for a member of the committee.in many problems faced by industry and commerce, other types ofalgorithms are often relevant, including linear programming (lp) algorithms andalgorithms to solve the traveling salesman problem.an lp problem is one that requires the maximization or minimization ofsome output subject to a set of constraints on the inputs. for example, amanufacturing firm makes two different products, widgets and gizmos. the saleof each product results in a different profit; the manufacture of each productrequires a different combination of engineering, inspection, and packagingattention as it moves through the shop. the total amount of attention that can begiven every day is fixed by union regulation. what is the optimal combination ofwidgets and gizmos for the firm to manufacture so that it can maximize itsprofits?problems of this general form arise in all walks of life. airlines use highlysophisticated lp algorithms to schedule equipment and flight crews.10 equipmentscheduling seeks to optimize the use of available aircraft and maintenancefacilities by assigning aircraft to each route to meet the requirements of inspectionand maintenance at the right ground facilities, with the needed personnel, andwith the fewest delays. crew scheduling seeks to optimize the use of personneland layover facilities by matching the available crew qualifications and homebases with equipment demands, while respecting government and other workrules, such as those governing time between flights and travel time betweenassignments. airline scheduling problems may involve millions of variables.until recently, the solutions could only be guessed at.6 what is computer science and engineering?195computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.box 6.9 better algorithms help the earth sciencesin the late 1970s, a member of this committee (jeff dozier) began toexamine the problem of calculation of the surface energy balance overmountainous terrain. different patches of terrain receive different amountsof sunlight, depending on the position of the sun in the sky and the shadingon nearby slopes caused by mountain peaks. the latter calculation iscomputationally complex, because the calculation at a given point must takeinto account shading that might arise from all other points in the region ofinterest.the initial solution to this problem in 1979 calculated the tangent of theangle from a particular point to all other points in a specified arc. themaximum tangent is the horizon angle. this solution requires a computingtime proportional to n2, since each point in the terrain grid requirescomparison to the other n1 points.in 1980, john bruno (computer science department, university ofcalifornia at santa barbara) and dozier began to discuss this problem.bruno and peter downey (computer science department, university ofarizona) formulated a much more elegant solution that eliminatedcomparison to most of the other points in the grid and therefore ran in a timeproportional to n rather than n2. computations not previously feasiblebecame feasible.source: jeff dozier, university of california at santa barbara.the chemical industry also uses sophisticated lp algorithms to compute, forexample, the least expensive mix for blending various distillates of crude oil tomake the desired products.another scheduling problem that arises in many practical applications isbest understood in terms of the traveling salesman problem (tsp): given a set ofcities and the distances between them, find the shortest tour for a salesman tovisit all cites.the tsp arises in planning deliveries and service calls. less obviously, itappears in many different contexts in science, engineering, and manufacturing.for example, in crystallography with highenergy and highdensity x rays, theirradiation time is small compared to the time needed for the motors to change theorientation of the sample and/or the detector's position. for an experiment withsome 14,000 readings, minimizing the repositioning time can cut the duration of aweeklong experiment by 30 percent to 50 percent. in manufacturing electroniccomponents, the drilling of holes in circuit boards6 what is computer science and engineering?196computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.with an expensive laser drill puts a premium on minimizing the total "travel time"of the drill head (or board) between successive holes. for a complex circuitboard, the shortest total route among as many as 17,000 holes may be required.the fabrication of a vlsi circuit may call for solving an analogous problem withover a million sites.faster algorithms can reduce significantly the time required to perform taskson the computer, thus making industry more efficient, and can also increasedramatically the sizes of the problems that can be feasibly solved. an appropriateselection of algorithms can also have a profound effect on the conduct of science,again by reducing the time needed to solve certain problems from utterlyunfeasible times to quite feasible times.the study of algorithmsin the 1930s, before the advent of electronic computers, church, kleene, andturing laid the foundations for the theory of computation and algorithms. theirwork showed that there was essentially only one basic concept of effectivecomputabilityšaside from matters of speed and capacity, a problem solvable onone computer is solvable on all.particularly relevant to the study of algorithms is the turing machine, anabstract computational device that turing used to investigate computability. theturing machine is also a vehicle through which it can be shown that allcomputational problems have an intrinsic difficulty that theoretical computerscientists call complexityšan inescapable minimum degree of growth of solutiontime with problem size.11 the solution time for a problem with high complexitygrows very rapidly with problem size, whereas the solution time for a problemwith low complexity grows slowly with problem size. switching to a fastercomputer may improve the computation time by a constant factor, but it cannotreduce the rate of increase of computation time with the growth of the problemsize.suppose we could prove that finding the optimal tour for tsps with n citiesrequires on the order of 2n steps in the worst case.12 then no computer couldovercome the prohibitive exponential growth rate for the solution of the tsp. afactorof100 speedup over a machine that could barely solve 100city problemswould not increase the capacity enough to solve 110city problems. (this wouldbe true whether the faster computer were a parallel computer with 100 processorsor a serial computer that operated 100 times as fast.)a particular algorithm that solves a given problem may result in solutiontimes that grow with problem size at a faster rate than one6 what is computer science and engineering?197computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.would expect from the intrinsic complexity of the problem. designing algorithmsthat run in times that are close to those implied by the intrinsic complexity of aproblem becomes an important task, because the need to solve ever largerpractical problems grows much faster than our ability to buy larger or fastercomputers.computational complexitythe study of the complexity of computations, referred to as computationalcomplexity theory, has revealed remarkably rich connections between problems.good examples of complexity theory arise in the context of linear programmingand traveling salesman scheduling problems.if the solution time for a problem grows exponentially with problem size,then there is no hope of solving large instances of the problem, and it becomesnecessary to look for approximate solutions or study more tractable subclasses ofthis problem. the question of minimal growth, or "lower bounds," requires a deepunderstanding of the problem, since all possible ways of computing a problemmust be considered to determine the fastest one.the study of lp algorithms provides an interesting illustration of how theoryand practice interact. for many years, the simplex algorithm was the main way tosolve lp problems. in 1972, it was shown that its worstcase running time grewexponentially with the size of the problem, although the algorithm worked wellon practical problems.in 1979 came the startling announcement of a new polynomialtimealgorithm. although this algorithm did not compete well with the highlydeveloped simplex algorithms, the proof that polynomial time was possiblegalvanized the research community to activity. the new research yielded"interiorpoint" algorithms that today compete successfully with the simplexalgorithm. a major airline now uses an interiorpoint algorithm for schedulingequipment and crews.although lp problems can be solved in polynomial time, it is widelybelieved that there is no polynomialtime algorithm for the tsp. however, inspite of at least 20 years of hard effort, there is no proof of the conjecture that theproblems in the class denoted by np are not computable in polynomial time. theclass of np problems, which contains many important combinatorial problems, iswidely believed to be not computable in polynomial time. the tsp and hundredsof other problems are known to be "npcomplete," which means that apolynomialtime algorithm for any one of them will guarantee a polynomialtimealgorithm for all. for example, from a polynomialtime algorithm for determiningwhether a set of jigsaw6 what is computer science and engineering?198computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.pieces will tile a rectangle, it is possible to derive a polynomialtime algorithmfor the tsp, and vice versa. the question of determining whether npcompleteproblems have polynomialtime algorithms, the "p = np" question, is one of themost notorious and important problems in computer science.at present, there is no hope of solving exactly 10,000 to 1 millioncityproblems, which do arise in practical applications. therefore researchers seekefficient algorithms that find good approximations to the optimal tour. the studyof approximate methods is an interesting blend of theory and experiment toassess the performance of various approaches and to gain insight toward betteralgorithms. for some approximate algorithms, there exist theoretical guaranteesof running time and accuracy. tradeoffs between time and accuracy have alsobeen demonstrated. good solutions have been obtained for practical problemswith 10,000 to i million cities.besides its practical relevance, computational complexity provides beautifulexamples of how the computing paradigm has permitted computer scientists togive precise formulations to relevant problems that previously could be discussedonly in vague, nonscientific terms.consider, for example, a problem about the basic nature of mathematics. allour experience suggests that the creative act of finding a proof of a theorem ismuch harder than just checking a proof for correctness. however, until recentlythis problem could not be formulated in precise quantitative terms. complexitytheory allowed this problem to be made precise: how much harder is itcomputationally (for an algorithm) to find a proof of a theorem than to check itsvalidity? the correctness of a properly formalized proof can be checked inpolynomial time in the length of the proof. however, finding a proof of a givenlength can be done by "nondeterministically" guessing a proof and then checkingin polynomial time whether it is a correct proof. thus the finding of proofs of agiven length is an np problem. furthermore, it is not hard to show that it is npcomplete.hence a startling conclusion emerges: finding proofs of length n ispolynomially equivalent to solving ncity tsps! either both can be done inpolynomial time (which is strongly doubted) or neither can.complexity theory seeks to understand the scope and limitations ofcomputing. in doing so, it addresses questions about the basic nature ofmathematics and the power of deductive reasoning.artificial intelligenceartificial intelligence (ai) is founded on the premise that most mentalactivity can be explained in terms of computation. ai's scien6 what is computer science and engineering?199computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.tific goal is to understand the principles and mechanisms that account forintelligent action (whether or not performed by a human being), and itsengineering goal is to design artifacts based on these principles and mechanisms.the premise of ai has a long tradition in western philosophy. aristotle andplato believed that thought, like any other physical phenomenon, could beunraveled using scientific observation and logical inference. leibniz equatedthought with calculation, setting the stage for boole's treatise on propositionallogic, titled "the laws of thought." much later, the advent of computers ledalan turing to envision a new field, computing machinery and intelligence. theformal discipline of ai was inaugurated in 1956 at a dartmouth conference byjohn mccarthy, marvin minsky, allen newell, and herbert simon.as an empirical science, ai follows the classical hypothesisandtestresearch paradigm. the computer is the laboratory where ai experiments areconducted. design, construction, testing, and measurement of computer programsare the major elements of the process by which ai researchers validate modelsand mechanisms of intelligent action. the original research paradigm in aiinvolved the following steps: identify a significant problem that everyone would agree requires"intelligence." identify the information needed to produce a solution(conceptualization). determine an appropriate computer representation for this information(knowledge representation). develop an algorithm that manipulates the representation to solve theproblem (e.g., heuristic search, automated reasoning). write a program that implements the algorithm and experiment with it.a few subareas of ai are examined below from two points of view: theirimpact on society (including their economic impact) and their influence onscientific thought.impact on societythe impact of ai on society can be measured using two criteria. do theproducts that result from ai help society at large? how much of an industry hasbeen created as a consequence of ai? the use of ai technologies in industry hasled to significant economic gains in tasks involving analysis (e.g., machinediagnosis), synthesis (e.g., de6 what is computer science and engineering?200computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.sign and configuration), planning, simulation, and scheduling. ai technologiesare also helping chemists and biologists study complex phenomena, for example,in searching enormous databases, creating new molecular structures, anddecoding dna sequences. stockbrokers use software assistants in analysis,diagnosis, and planning at the expert level. medical doctors can examine morehypothetical cases, thus including notsoobvious symptoms in their diagnoses.the bestknown economic impact comes from the widespread use of expertsystem technologies. (table 6.2 lists some expert systems that have been or arebeing used in industry.)but ai offers more to business than just applications of expert systems.speechgeneration products have been in use for 15 years, and speechanalysisproducts are beginning to reach the market. imageprocessing programs are indaily use in the government, health care organizations, manufacturing, banking,and insurance. it is hard to measure the economic impact of image processing,because such systems often provide better performance rather than savings, butimage processing is a billiondollarperyear industry. vision and robotics areaffecting manufacturing. planning and scheduling systems are used routinely inindustrial and military settings. the impact of ai in manufacturing manifestsitself at least in two ways: in the automation of assembly, quality control, andscheduling operations and in the use of ai techniques in engineering design.table 6.2 examples of expert systems in usecompanyexpert systempurposeschlumbergerdipmeter advisorinterpret data from oil wells for oilprospectingkodakinjection molding advisordiagnose faults and suggest repairsfor plastic molding mechanismshewlettpackardtrouble shooting advisordiagnose faults in semiconductormanufacturingxeroxpridedesign paperhandling systemshazeltineopgenplan assembly instructions for pcboardssource: data from raj reddy, "foundations and grand challenges of artificial intelligence," aimagazine, volume 9(4), winter 1988, pp. 9œ21.6 what is computer science and engineering?201computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.impact on scientific thoughtperhaps the most important impact of ai on scientific thought is therealization that computable information can be symbolic as well as numeric. thebeginning of ai was marked by the introduction of the programming languagesipl and lisp, whose purpose was the manipulation of symbolic information.symbolic manipulation became a fertile ground for problems of searching,organizing, and updating databases efficiently and for reasoning frominformation. theoretical and practical results in searching, reasoning, andlearning algorithms are now part of the core of cs&e. furthermore ai has hadsuch a deep impact on psychology, linguistics, and philosophy that a newdisciplinešcognitive sciencešhas emerged. ai has contributed to otherbranches of cs&e as well. many concepts in programming languages andenvironments arose directly from attempts by ai researchers to build aisoftware. ai languages like lisp, prolog, and scheme are used in many parts ofcs&e. the nature of ai forces experimentation more so than in some otherbranches of cs&e, and the challenges brought forth by the need to experimenthave led to the development of important concepts. ai has also dealt with areasof perception and interaction of artificial agents with the physical world. thechallenge of how to deal with multimodal information in a consistent, predictablefashion has resulted in new efforts in applied mathematics and control theory,called discreteevent dynamic systems, which combine dynamic systems andtemporal logic. such hybrid systems offer a suitable modeling tool for manydifficult physical and economic models. the following subsections look moreclosely at three subareas of ai: heuristic search, reasoning and knowledgebasedsystems, and speech.heuristic search in the 1960s and early 1970s, the tasks that were identifiedas requiring intelligence were mostly of the puzzlesolving variety. theypossessed simple specifications but lacked feasible algorithmic solutions. taskssuch as theorem proving and cryptarithmetic puzzles were studied at carnegiemellon university. at the massachusetts institute of technology, the problemsof understanding simple sentences in english in the world of children's blocks andsolving freshman calculus problems were studied. work at stanford universityfocused on automatic speech recognition and the advice takerša system thatwould take advice specified in a logical language to solve simple problems inplanning courses of action. even though these problems sound deceptively simplein that humans accomplish them routinely, calculating exact solutions for themcan be infeasible. instead, goodenough answers, which are close to but not6 what is computer science and engineering?202computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.optimal, are accepted if they can be calculated within given time constraints. aitries to find these goodenough answers using heuristic search. the problem isformulated as a search through the space of systematically generatedpossibilities, and methods are determined to reduce the search to likelycandidates.for example, a checkerplaying program would generate possible moves upto a certain play and would select the move using an evaluation function like"choose a move that results in the largest piece gain." the evaluation functionsare heuristic: they are rules of thumb that work most but not all of the time. thiswork in heuristic search borrows from operations research, statistics, andtheoretical computer science. the phenomenal progress in computer chessaffords a practical demonstration of the power of heuristic search. in 1967,richard greenblatt's program defeated a classc player at a tournament; today,the program deep thought has a grandmaster's rating. advances in searchstrategies, combined with specialpurpose hardware matched to these strategies,have been an important part of this success.reasoning and expert systems fascination with search strategies has led toinvestigations of reasoning as a process. since reasoning requires knowledge, theissue of representing knowledge has become central to ai. in the early researchon reasoning, it soon became apparent that generalpurpose methods would notbe capable of delivering expertlevel performance on problems that requireddomainspecific knowledge, and, because of this, early research dealt more withdomainspecific problems. such research has led to expert systems (also knownas knowledgebased systems) that are based on a simple idea: symbolic reasoningguided by heuristics over declaratively specified knowledge of a domain canresult in impressive problemsolving ability. the main research issues in thedevelopment of expert systems are the extraction of knowledge about the domainand the criteria used for decisionmaking.speech perceptual tasks, such as speech, are characterized by high datarates, the need for knowledge in interpretation, and the need for realtimeprocessing. error tolerance is an important issue. the usual methods for symbolicprocessing do not directly apply here. an important speechunderstanding systemdeveloped in the 1970s at carnegie mellon university was the harpy system,which is capable of understanding speakerdependent continuous speech with a1000word vocabulary. the early 1980s witnessed a trend toward practicalsystems with larger vocabularies but with computational and accuracy limitationsthat made it necessary to pause between words.6 what is computer science and engineering?203computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.the recent speakerindependent speechrecognition system, sphinx, bestillustrates the current state of the art. sphinx is capable of recognizing continuousspeech without training the system for each speaker. operating in near real timeusing a 1000word resource management vocabulary, sphinx achieves 97 percentword accuracy in speakerindependent mode on certain tasks. the system derivesits highperformance by careful modeling of speech knowledge, by using anautomatic unsupervised learning algorithm, and by fully utilizing a large amountof training data. difficulties in building practical speech interfaces to computersinclude cost; realtime response; speaker independence; robustness to variationssuch as noise, microphone, and speech rate and loudness; and the ability to handlespontaneous speech phenomena such as repetitions and restarts.while satisfactory solutions to all these problems have not been realized,progress has been substantial in the past ten years.the future of aithe theoretical and experimental bases of ai are still under development.initial forays on the experimental end have led to the development of a host ofgeneralpurpose problemsolving methods (e.g., search techniques, expertsystems). experimental ai can be expected to broaden the scope (scalability andextensibility) of applications in the areas of speech, vision, language, robotics,expert systems, game playing, theorem proving, planning, scheduling,simulation, and learning. integrated intelligence systems that couple different aicomponents (e.g., a speech comprehension program to a reasoning program) willbecome more common. collectively, these results should lead to significanteconomic and intellectual benefits to society.theoretical ai should lead to the development of a new breed ofapproximate but nearly optimal algorithms that obtain inputs as an ongoingprocess during a computation and that cannot precommit to a sequence of stepsbefore embarking on a course of action.13 and finally, ai will continue its questfor theories about how the human mind processes information made available toit directly via the senses and indirectly via education and training.computer graphics and user interfacesgraphicscomputer graphics has its roots in data plotting for science and engineering.in many problems, the significance of a calculation does6 what is computer science and engineering?204computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.not rest in the exact numerical results of computation, but rather in qualitativetrends and patterns that are best illustrated by a graph. indeed, pattern recognitionis a wellhoned skill in which humans still have substantial advantages overcomputers.from the earliest days of printed output, computers have thus been printinggraphs as well as tables and lists of numbers. however, paper is a static outputmedium; electronic display screens offer the ability to create dynamicrepresentations of data. since the human eyebrain system is better adapted todetecting change over time than to inferring it from a sequence of images or evensimultaneously viewed images, the ability to display dynamically changing imageforms on a screen is as much a step forward over a graphical depiction (or a set ofthem in sequence) as a static graphic depiction is over a table of numbers.dynamic displays are particularly powerful for the interactive user when heor she can influence the parameters of both the model and its visualization(s).indeed, the shift from the batch computing14 typical 25 years ago to interactivecomputing today enables the user to make realtime changes in input datasupplied to a program or in the operating parameters governing the operation of aprogram and to receive much more rapid feedback. computer graphics hasbecome a standard tool for visualizing large amounts of scientific data, as well asfor the design of objects from scratch or from standard buildingblockcomponents.wimp interfacesthe use of computer graphics is not restricted to design and scientificvisualization; it is rapidly becoming the standard method by which human beingscommunicate with computers. in particular, the use of typed commands tocontrol the operation of a computer is giving way to the use of socalled wimpinterfaces that make use of windows in which different programs may be run,graphical icons on a screen that represent actions that can be taken or files thatcan be opened, and mice with which the user can point to icons or select optionsfrom a menu.wimp graphical user interfaces have three major advantages over the use ofcommand languages. they simplify computer operation for novices, because theyprovide a constant reminder of what the allowable actions are at any givenmoment. for most users, they are faster than typing, since it is usually easier topoint to an icon or a menu selection than to type it in. lastly, they are lesssusceptible to error, because the use of icons and menus limits the range ofchoices6 what is computer science and engineering?205computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.available to the user (unlike typing, in which the user may type anything).a bit of historythe history of interactive computer graphics goes back to the 1950s:pioneers using mit's whirlwind computer in the 1950s were the first to make useof an experimental display screen, and these graphics were later elaborated withthe tx2 computer and its extensive collection of input mechanisms: knobs,dials, keyboard, and light pen.in 1963, ivan sutherland developed a system called sketchpad for the tx2,which introduced the notion of interactively constructing hierarchies of objects ona virtual piece of paper, any portion of which could be shown at arbitrarymagnification on the screen. users could specify simple constraints on pictureelements to have the computer automatically enforce mathematical relationshipsamong lines drawn with a light pen. this ''constraint satisfaction" allowed a kindof freehand sketching, with the computer "straightening out" the approximatedrawings. sketchpad was the opening round in an era of computerdrivendisplays as tools for specifying geometry for cad/cam in the automotive andaerospace industry and flight simulators for pilot training. mit's steven coonsand other researchers in academia and industry developed various kinds of spline"patches" to define freeform surfaces for vehicle design.in the 1960s, douglas engelbart at sri pursued the notion of usingcomputer displays to "augment human intellect." his group built the firsthypermedia systems and office automation tools, including word processors,outline processors, systems for constructing and browsing hypermedia, softwarefor telecollaboration, as well as various input devices, including the mouse. inthe early 1970s, the xerox palo alto research center (parc) pioneered bitmapraster graphics workstations with graphics in the user interface, both for officeautomation tools such as word processors and illustration programs and forsoftware development (e.g., smalltalk). these developments, coupled with thereduction in price of display subsystems from $100,000 in the mid1960s to$10,000 in the early 1970s to about $1,000 today, are the underpinning fortoday's graphical user interfaces.the macintosh personal computer and windows (for ibmcompatiblepersonal computers) have brought the experience of sri and xerox to thepopular marketplace, dramatically lowering knowledge barriers to computerusage, and the commercial success of these products6 what is computer science and engineering?206computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.testifies to their significance and impact. as users have required more and moreinteractive capability with their computers, the popularity of graphical userinterfaces and graphical displays has grown by leaps and bounds. now, millionsof users have graphical interfaces on their desks in the office or at home, and it isoften possible for users to operate new applications with little or no reference to amanual. even young children are comfortable using paint programs andillustration programs, not to mention video games and multimedia electronicbooks and encyclopedias. with these developments, graphics at long last hasbegun to fulfill in earnest the promise of the sketchpad days as a standard meansof humancomputer communication.scientific and engineering visualizationthe science and engineering community has also made good use of graphicstechnology. an nsfsponsored report15 in 1987 emphasized the need ofcomputational science and engineering for graphics to help make sense of"firehoses of data" generated by supercomputers and highpowered workstations.in such datarich situations, graphics are not merely desirablešthey are essentialif users are to design objects or perceive subtle patterns and trends. coupled tointeractive computing that enables the user to use the visualization ofintermediate stages in the computation to direct further exploration, intuition andinsight about phenomena and objects can be developed that would not otherwisebe possible.scientific visualizations make increasing use of threedimensional graphics.depth perception is greatly enhanced by real threedimensional stereo imagesthat are possible only when the visual inputs to each eye are slightly different, andso some special input mechanism (e.g., glasses) is necessary to present each eyewith the information necessary for the brain to construct a threedimensionalimage. technology such as stereo headmounted displays with miniature displayscreens for each eye can control all of the visual input received by the wearer andis the basis for "virtualreality" presentations that completely immerse the user in asynthetic world.16 coupled with the ability to "walk through" such a presentation,suitable immersive representations can give the user an even better and morevisceral understanding of the spatial relationships involved than is possible evenwith a nonimmersive stereo presentation, provided that the user can control thevantage point from which the scene is viewed.graphics are also beginning to assist scientists in developing pro6 what is computer science and engineering?207computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.grams to analyze their data. traditionally, scientists had to choose between twostyles of software use. they could write their own programs from scratch, oneline at a time, until the entire program was written, or they could use a prewritten"canned" application. writing from scratch gives the scientist tight control overwhat the program does but is time consuming and prone to error. using a cannedapplication is faster but lacks flexibility.however, in recent years, a style of "visual programming" has begun toemerge that is intermediate between these two choices. the key construct withinvisual programming is the explicit specification of data and control flow betweenpreexisting processing modules; such flows are implicitly specified when aprogram is written in the conventional manner. using a screen and a mouse, thescientist specifies data and control flows between different modules, and thesystem automatically assembles the corresponding program from these modules.the scientist has the freedom of specifying the flows as appropriate to the problemat hand and also has some freedom to customize each module to a limited extentby adjusting a few parameters (typically using a wimp interface to do so). theresult is that scientists can rapidly assemble systems that will perform a giventask, much as a music buff can assemble stereo components into a working soundsystem without being much of an electrical engineer. further, while this style ofprogramming is currently limited to highend scientific users, it is likely thatvisual programming will become increasingly important in the commercial worldas well. microsoft's visual basic is a good example of such an application.engineering visualizations are at the heart of computeraided design today.computeraided design now includes design of mechanical parts, architectural orengineering structures, and even molecules. large structures such as airplanes orbuildings can be assembled "virtually," i.e., as information artifacts stored in acomputer. parts, substructures, and entire structures can be designed and fittedtogether entirely on a display and are usually fed to online simulations that cananalyze and virtually "test" these artifacts. the result is a dramatic shortening ofthe time required from concept to product.touch, sound, gesturessince vision is the communications channel with the highest bandwidth forinformation transmission, it is not surprising that a great deal of work has beendone in graphics. but human beings also make use of sound, touch, and gesturesto communicate information. computer scientists and engineers have thusdeveloped a variety of6 what is computer science and engineering?208computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.devices that allow users to communicate with computers using these channels aswell: head or eye trackers, force and tactile feedback devices, gesture recognitionvia wands and data gloves provided with manydegreeoffreedom sensors, and soon. these devices are especially useful in virtualreality environments.tactile feedback is illustrated well in the problem from biochemistry ofunderstanding molecule "docking." how a complex organic molecule physically"fits" into another is often a key to understanding its biological function. it ispossible, although demanding, to calculate the molecular forces that determinedocking positions and orientations. but as an aid to developing the biochemist'sintuition and feel for docking behavior, computer scientists and engineers havedeveloped ways to depict molecules visually. by controlling their orientation andposition through the use of a specially designed joystick capable of respondingwith graduated forces, the user can orient and move the molecule along the pathof least resistance as he or she tries to fit the molecule into the receptor site.sound will become a more important medium for interaction with computersin the future. audio feedback is widely used in keyboards today to inform theuser that a key has actually been pressed. some computer systems have voicesynthesis systems that make information accessible to users without demandingtheir visual attention. such output would be particularly useful when the demandsof a task (e.g., driving) would prohibit the diversion of visual attention; thus anintelligent onboard navigator for automobiles will almost surely providedirections (e.g., "turn left on willow street") by voice. speech input is a morechallenging problem discussed in chapter 3.finally, human beings use gestures to communicate meaning and manipulateobjects. the dataglove is a glove with sensors that can indicate the extent towhich the fingers of a hand are bent. appropriate processing of this informationallows the computer to construct a representation of the hand's configuration atany given moment. coordinated with a virtual reality presented via stereogoggles, the dataglove allows the user to perform certain manipulations of asynthetic object with his or her hand as though it were real; however, the glovedoes not provide the tactile feedback that real objects provide.intellectual challengescomputer graphics and interactive computing pose many intellectualchallenges for cs&e. in the earliest days of computing, most of thecomputational power in a computer system could be devoted6 what is computer science and engineering?209computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.to executing the algorithm to solve a problem of interest, and developers ofcomputer systems paid a great deal of attention to making the best use ofavailable computer resources. but with interactive computing that places the userat the center of the process and ever cheaper computational power, a larger andlarger fraction of the computational power will be devoted to making the specificapplication as easy and comfortable to use as possible for the user. developers ofcomputer systems will pay much more attention to making the best use of humancognitive resources such as attention span and comprehension time.this trend is reflected in the development of computer hardware. realtimeuser interaction requires that the time between user input (e.g., pointing to ascreen icon) and computer response (updating of the displayed scene) be veryshort, on the order of a few tenths of a second. this limit places a premium onvery fast processors and also drives the development of specialpurpose hardware(e.g., graphics accelerators) that can relieve the central processing unit of some ofthe responsibility for servicing the user interface.the importance of specialpurpose hardware is complemented by anincreasing emphasis on investigating better representations of data andinformation. while a "better" algorithm to solve a problem is generally one thatruns faster, a "better" representation of information in the context of userinteraction is one that provides insights not previously accessible.for example, consider the difference between looking at an orthographicengineering diagram of a metal bracket and holding a metal bracket in one'shand. an orthographic diagram provides front, top, and side views of an object. insome sense, the orthographic diagram is "equivalent" to the metal bracket inhand, since the engineering diagram can be used to manufacture the bracket. butvery few individuals can look at an arbitrary object in an engineering diagram andcomprehend just what the object is. a much better sense for the object is obtainedby manipulating it, looking at it from different angles, turning it in hand. today'scomputers provide the next best thing: a visual image that can be viewed fromdifferent perspectives under user control. using such a display provides a betterintuition for timevarying behavior than does viewing a sequence of staticimages, because the viewer need not cope with the cognitively demandingprocess of identifying correspondences between the images but rather can followthe correspondences from one moment to the next. the best of today's cadsystems provide images on screen that can be rotated, exploded, collapsed, andotherwise manipulated on command.6 what is computer science and engineering?210computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.the techniques of computer graphics offer a host of different choices torepresent information, as demonstrated by modern molecular graphics. animportant aspect of modern biochemistry is to understand how complexmolecules fit together. among other things, such understanding involvesknowledge of the outside contours of several molecules in their "fittedtogether"configuration, as well as knowledge of how surfaces from different molecules fittogether "on the inside." to take a simple example, imagine two cubical dice thatare brought together so that they are face to face. it would be important to knowwhich surfaces were exposed to the outside (e.g., the 1, 2, 3, 4, 5 of die 1 and the1, 2, 4, 5, 6 of die 2) and which surfaces were touching (the 6 of die 1 and the 3of die 2).visualizations of molecule fittings could use socalled spacefilling models,the equivalent of models made of clay that were fitted together. but the surfacesin contact would then be invisible from the outside, and since the precisegeometry of the proper fit is determined in a very complex manner, it is not assimple, as in the case of the two dice, to determine what surfaces from eachmolecule are in contact.but an alternative representationšthe dotsurface representationšprovidesa notable alternative. the dotsurface representation replaces the spacefillingmodel with a shell representation. however, the shell's surface is not solid; ratherit is composed of many dots. these dots are spaced far enough apart that it ispossible to see past them, but close enough that the eye can perceive the surfaceof which they are a part. using these dot shells instead of spacefilling models torepresent molecules enables the user to see both the outside contours (as before)but also "through" them to see formerly invisible surfaces in contact with eachother.in many situations, the availability of different representations provides anunparalleled degree of flexibility in depicting information. computer graphicsbased representations can be created, modified, and manipulated with such speedthat the utility of different representations can be rapidly determined, and the usercan choose which to use under any particular set of circumstances. theserepresentations are limited only by the imagination of their creator, since anyrepresentation that can be imagined can be implemented. thus computer graphicscan be said to have spawned a new set of concepts for depicting information thatwere not practical prior to the computer.finally, finding ways to provide perceptual coupling between computergenerated image and user is important. evolution has provided human beingswith rich and coordinated senses of sight, sound,6 what is computer science and engineering?211computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.and touch; immersive virtualreality presentations, data gloves, audio output, andforce feedback are the first steps toward the full exploitation of human senses.synergy leading to innovations and rapidprogressas anyone who has recently purchased a personal computer knows all toowell, computer technology advances at an extraordinarily rapid rate. to a largedegree, the rapidity of technological change arises from a high degree ofinterconnectedness among the various subdisciplines of cs&e. the resultingsynergy is intellectually powerful and results in many practical benefits as well.synergy arises in many contexts. box 3.1 offered two examples of theimpact of synergy between subdisciplines of cs&e. a third example, with moreobvious and tangible economic impact, is that of workstations. it is commontoday to see a multimilliondollar mainframe computer replaced by workstationsthat offer much better costperformance ratios. the growth in the use ofworkstations is fueled by advances in many areas: networking (that enablesworkstations to be interconnected and thus enables users to share resources),reducedinstructionset computers (that speed up computations by large factors),portable operating systems (that can run on computers made by differentvendors), and a better theoretical understanding of how compilers can best usehardware resources. no single one of these areas is solely responsible for growthin the workstation industry, but taken together they have produced amultibilliondollar industry.the benefits of synergy are not limited to innovation. synergy also speedsup developments in the field. consider, for example, the development of the80x86 processor chip family by the intel corporation. (the 80x86 processor isthe heart of ibmcompatible personal computers.) in designing the 80286processor chip, intel engineers had to use seven different brands of computersdepending on the step of the design process. this was before portable operatingsystems were available, and so a designer had to learn seven different commandlanguages and seven different text editors to complete a microprocessor.however, shortly after the 80286 was released, portable operating systemscame into use at intel. this meant that designers of the later processor chips hadto learn only one command language and only one text editor, and thiscontributed to the shortening of the time between the 80486 and the 80586compared to the time between6 what is computer science and engineering?212computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.the 8086 and the 80286.17 other reasons for the reduced development timeinclude a graphical user interface to the design software (making the designermore productive), better compilers and algorithmic improvements to the designsoftware (making the design software faster), and simply faster computers (sothat the designer can perform even unchanged tasks in less time). and in a fastmoving technology like electronics, shorter design time corresponds directly tobetter performance and lower cost. hence the designers of the intel 80486 and80586 (as well as their customers) have and will benefit from advances inhardware technology, operating systems, compilers, user interfaces, designsoftware, and theory of algorithms.intellectual and structural characteristicsof cs&e as a disciplinecs&e is an entirely new entity, neither simply science nor simplyengineering. its historical roots derive from mathematics and electricalengineering, and this parentage gives cs&e its distinctive flavor.perhaps most important in creating this flavor is the close relationshipbetween the intellectual content of cs&e and technology. technologicaldevelopments have a major influence in defining what the interesting problems ofthe field are; problems in cs&e often become interesting because the technologynecessary to implement solutions becomes available. put another way, newgenerations of technology open up and make interesting a variety of new andchallenging questions. this intimate link to technology creates a deep connectionbetween the research and educational activities of academic cs&e and amultibilliondollar computer industry, with intellectual progress stronglyinfluenced by and at times strongly influencing commercial development.intellectually, cs&e includes programming, which allows a programmer'sthoughts to spring to life on the screen. as a result, computing technology cancreate virtual realitiesšuniverses unconstrained by the laws of physicsšand thisability to take part in creating new worlds is part of the excitement ofprogramming. in other words, the objects of study in cs&e (ways to process andrepresent information) are created by those who study those objectsšwhereasother scientists study what nature gives them (atoms, cells, stars, planets). thisnew form of expression also offers opportunities for important intellectualcontributions to the formal study of information.theory in cs&e often develops after years of practice, with experimentslargely establishing the feasibility of new systems. by contrast, experimentsconducted by physicists and biologists play an im6 what is computer science and engineering?213computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.portant role in proving or disproving their theories. the experimental constructionof algorithms as programs is more likely to uncover practical issues that wereignored in the mathematical proofs rather than flaws in the proofs themselves.feedback from experimentation can then lead to the creation of more realisticmodels that in turn give rise to more relevant theories. in other words, cs&eexperiments often demonstrate the irrelevance and inadequacy of previoustheoretical work, rather than proving that theoretical work wrong.the commercial and industrial side of computing is also unusual. whereasmost industries focus on producing tangible goods, an increasingly large part ofcomputing involves software and information. software and information areunlike other products in that they are primarily an intangible intellectual output,more akin to a useful report than a useful widget. by far the largest cost ofsoftware and information involves research, development, documentation,testing, and maintenance (primarily intellectual activities) rather thanmanufacturing (typically, copying disks and manuals). as a result, capitalrequirements for software production are relatively much lower than those forwidget production, while personnel requirements are relatively much higher.when the recognition of cs&e's intellectual heritage is combined withunderstanding of the rapid pace of change, the youth of the field, the impact of alarge industry, the magical implications of programming, and the mathematicalimplications of algorithms, the exciting and enticing nature of cs&e is revealed.notes1. the notion of cs&e as a discipline based on theory, abstraction, and design is described in peterdenning, douglas e. comer, david gries, michael c. mulder, allen tucker, joe turner, and paul r.young, "computing as a discipline," communications of the acm, volume 32(1), january 1989, pp.9œ23.2. personal communication, donald knuth, march 10, 1992 letter.3. frederick brooks, the mythical manmonth, addisonwesley, reading, mass., 1975, pp. 7œ8.4. "computer displays," ivan sutherland, scientific american, june 1970, p. 57.5. this division isn't even necessarily unique, since abstractions in cs&e can be created with a greatdeal of flexibility. a grammar checker builds on a wordprocessing program, and thus the wordprocessor plays a "system software" role for the grammar checker. but the word processor buildsupon the operating system, so that the word processor is applications software from the perspective ofthe operating system.6. john e. hopcroft and kenneth w. kennedy, eds., computer science achievements andopportunities, society for industrial and applied mathematics, philadelphia, 1989.7. in practice, the tradeoff is even more favorable than this discussion implies. in6 what is computer science and engineering?214computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.the former case, the area of the chip would be more than 100 times larger, due to secondorder effectsthat are not accounted for in the vlsi complexity model. in the latter case, it could be possible toreduce the increase in total chip area to less than a factor of ten by designing its functional elements toperform multiple operations in a "pipeline" arrangement. (a pipeline architecture is analogous to abucket brigade; when it is finished with an elementary operation, an element passes its result to thenext element in sequence, thereby allowing the entire array of elements to be kept busy nearly all ofthe time.)8. the discussion that follows is taken largely from lui sha and john b. goodenough, realtimescheduling theory and ada, technical report cmu/sei89tr14, software engineering institute,carnegie mellon university, pittsburgh, pennsylvania, april 1989.9. the reader may find it amusing to multiply two arbitrary 2 × 2 matrices with a minimal number ofmultiplications. the standard method of multiplying these matrices requires four additions and eightordinary multiplications to compute the matrix product. it turns out that it is possible to compute thematrix product with only seven scalar multiplications though at the expense of additional additions(and subtractions). the algorithm for multiplying 2 × 2 matrices is the basis for a more generalalgorithm for multiplying matrices of any size that requires considerably fewer arithmetic operationsthan would be expected from the standard definition of matrix multiplication.the details of the sevenmultiplication algorithm are described on p. 216.10. actually, lp is applicable to a much broader class of problems.11. a problem's complexity depends on the model of computation used to derive it. an area of activeinvestigation today within theoretical computer science concerns the use of different models thatdiffer in the fidelity with which they match actual computers. for example, some theoreticalcomputer scientists consider socalled random access machine models, on the grounds that thesemodels can access any memory location in a constant number of steps, while a turing machine canaccess a given memory location only by stepping through all preceding locations.12. the function 2n is called "exponential" in n. the function n2 is polynomial in n, as would be nraised to any exponent. an algorithm that executes in a time proportional to a function that ispolynomial in n, where n is the size of the problem, is said to take "polynomial time" and may befeasible for large n, whereas an exponential algorithm is not. the reason is that for sufficiently largevalues of n, an exponential function will increase much faster than any polynomial function.13. these algorithms would be nearly optimal in the sense that a given algorithm working onproblems of a particular nature would consume no more than a certain amount of computationalresources while generating solutions that are guaranteed to be very close to optimal with very highprobability.14. batch computing refers to a mode of computing in which the user submits a program to acomputer, waits for a while, and then obtains the results. if the user wishes to correct an error orchange a parameter in the program, he or she resubmits the program and repeats the cycle.15. bruce h. mccormick, thomas a. defanti, and maxine d. brown, eds., visualization in scientificcomputing, acm press, new york, july 1987.16. virtual reality, the object of both serious intellectual work and farfetched hyperbole, will be thesubject of a forthcoming nrc project to be conducted jointly by the nrc's computer science andtelecommunications board and the committee on human factors. as a mode of informationrepresentation, virtual reality is still in its infancy compared to modes such as ordinary stereo or twodimensional graphics.17. the time between the 8086 and the 80286 was five years, while the time between the 80486 andthe 80586 is expected to be three years (the 80586 will be released in6 what is computer science and engineering?215computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.1992). see john l. hennesy and david a. patterson, computer architectures: a quantitativeapproach, morgan kaufmann publishers, san mateo, california, 1990, inside front cover.solution to the problem posed in note 9 above.let the elements of matrix a be denoted byand similarly for b. if we definex1 = (a11 + a22) · (b11 + b22)x2 = (a21 + a22) · b11x3 = a11 · (b12  b22)x4 = a22 · (b21  b11)x5 = (a11 + a12) · b22x6 (a21  a11) · (b11 + b12)x7 = (a12  a22) · (b21 + b22)then the entries of c are given byc11 = x1 + x4  x5 + x7c21 = x2 + x4c12 = x3 + x5c22 = x1 + x3  x2 + x6the significance of this algorithm is not its utility for 2 × 2 matrices per se, but rather that it is thebasic building block for an algorithm to multiply n × n matrices. such a matrix can be treated simplyas a 2 × 2 matrix where the elements are themselves matrices of size n/2 × n/2. the result is thatinstead of requiring on the order of n3 multiplications and additions (as would be expected from thedefinition of matrix multiplication), the matrix multiplication requires on the order of n2.81 arithmeticoperations. while such an algorithm involves some additional overhead, it turns out that forsufficiently large values of n, it will run faster than one based on the definition for matrixmultiplication.source: this method was first published by v. strassen, ''gaussian elimination is notoptimal," numerische mathematik, volume 13, 1969, pp. 354œ356.6 what is computer science and engineering?216computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.7 institutional infrastructure of academiccs&ethe term "institutional infrastructure" is used here to refer to the institutionsthat have some important bearing on academic cs&e. thus institutionalinfrastructure includes major funding agencies that support research, theuniversities that house academic cs&e, and the various professionalorganizations that provide vehicles for dissemination of research and othersupport to the discipline.federal agencies funding computer science andengineeringan overview of federal support for cs&e was provided in chapter 1. amore detailed description of each major researchsupporting agency is providedbelow. (figures cited are presented in constant 1992 dollars and are subject to thecaveats specified in note 18, chapter 1.)department of defensethe modern military is highly dependent on computers in almost everyaspect of its responsibilities, including weapons acquisition, command andcontrol, communications, intelligence, weapons control, and administration.among federal agencies, the department of defense is the largest singlefunder of cs&e research; historically a little over onethird of7 institutional infrastructure of academic cs&e217computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.this money has gone to universities and colleges, making the department ofdefense the largest supporter of academic cs&e research as measured by dollaramounts. figure 7.1 illustrates the defense department's history of funding cs&eresearch for the last 15 years.within the department of defense, the defense advanced research projectsagency (darpa) is responsible for the majority offigure 7.1 department of defense obligations for research for computerscience (basic and applied), fy 1976 to fy 1991, in constant fy 1992 dollars.source: basic data (in thenyear dollars) for all recipients taken from federalfunds for research and development (federal obligations for research byagency and detailed field of science/ engineering: fiscal years 1969œ1990),division of science resource studies, national science foundation. data for fy1990 taken from federal funds for research and development: fy 1989, 1990,and 1991, national science foundation, nsf 90327. data for fy 1991 arepreliminary and were supplied to the committee by the division of scienceresource studies, national science foundation. basic data (in thenyear dollars)for academia taken from federal funds for research and development(federal obligations for research to universities and colleges by agency and detailed field of science/engineering: fiscal years 1969œ1990), division ofscience resource studies, national science foundation. figures include both"computer science" and "mathematics and computer science, not elsewhereclassified." constant dollars calculated from gnp deflators used in nationalscience foundation, science and engineering indicators, 1991, nsf,washington, d.c., 1991, table 41.7 institutional infrastructure of academic cs&e218computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.cs&e research. other important roles are played by the science offices of thevarious services, the office of the secretary of defense, and the national securityagency.the influence of darpa on cs&e has been pervasive. founded in 1958 topromote research in fields of military interest, darpa has been directly involvedin supporting timesharing (1960s), networks (late 1960s to mid1980s), artificialintelligence (1970 to present), advanced computer architectures and verylargescaleintegration circuitry (1970 to present), and graphics (mid1960s).in recent years, the major areas of cs&e concern to darpa have includedhighperformance computing, networks, software, artificial intelligence (ai), andapplications of these areas. darpa divides its overall computing program intoscience (including machine translation, scalable software libraries for highperformance computing, software understanding for the future), technology(including speech understanding, knowledge representation, embeddedmicrosystems), and applications (including image understanding, naturallanguage processing, transportation planning).darpa has long had a reputation for supporting highrisk, highgainresearch in pursuit of military applications. its style of research support is highlyproactive in that darpa identifies areas of potential interest for military needsand orients its research support mostly toward experimental and prototype systemdevelopment. individual program managers have been highly influential, both inarticulating areas of need and in stimulating the cs&e community to beinterested in these areas. thus darpa has often played a key role in definingresearch agendas for the cs&e field.in the past, darpa tended to concentrate its support in a few selectedinstitutions, thereby creating an infrastructure of centers of excellence withcritical masses of interested and active researchers. however, since themid1980s darpa has been required to engage in competitive procurementpractices, even for the award of contracts for basic research. this requirement hasbroadened somewhat the number of institutions receiving darpa funding incs&e but has also increased the administrative burdens (e.g., by insisting onmore precise definition of deliverables than before) on established centers eventhough they may have demonstrated records of excellence and success.other agencies within the department of defense fill somewhat morespecialized niches. for example, the office of naval research (onr), the airforce office of scientific research (afosr), and the army research office(aro) fund small but important research programs in cs&e. in contrast todarpa's emphasis on experimental7 institutional infrastructure of academic cs&e219computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.and prototype work, these offices tend to emphasize relatively smallscaleconcept and algorithm development oriented toward the fundamental science thatwill underlie future military applications. rather than covering cs&ecomprehensively, their research portfolios thus depend strongly on judgmentsabout what these future applications will entail. the early onr and the afosrhad a tremendous impact on the development of computers in the 1940s and1950s (box 7.1).budgets for cs&e research within these offices are about 5 to 10 percentthat of darpa. the onr research program includes activities in software designand construction, distributed and parallel systems, database systems, ai androbotics, realtime computing, fault tolerance, highperformance computing, andsecure computing. in the near future, onr expects to focus on dependablemulticomputer systems, mathematical logics for programming languages, casebased reasoning, massively parallel computing for the physical sciences,algorithmic structural complexity, and visual processing. afosr's scientificprogram includes a variety of mathematical areas of interbox 7.1 early catalysts for computing: a historicaside about onr and the air forcethe office of naval research (onr) played a key role in thedevelopment of the storedprogram electronic computer in the late 1940sand early 1950s. indeed, the whirlwind computer was one of the firstcomputers to operate in real time, and it became a forerunner of modernprocess control and embedded computing systems. another onrsponsored project, the ias program, directed by john von neumann,developed the foundations for serial computer architectures that remain inwide use today.the air force also played an important role in the early days ofcomputing. for example, its semiautomatic ground environment (sage)system for air defense was the first largescale distributed computer systemand one of the first to make use of computer graphics, datacommunications, and timesharing. in addition, the air force has beeninstrumental in supporting computeraided manufacturing technologies.sources: mina rees, "the computing program of the office ofnaval research, 1946œ1953," communications of the acm, volume 30(10), october 1987, pp. 830œ848; and kenneth flamm, targeting thecomputer, the brookings institution, washington, d.c., 1987, p. 49.7 institutional infrastructure of academic cs&e220computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.est (e.g., dynamics, control theory, statistics, and signal processing) andfundamental computer science as well. the aro supports work on highperformance computing, intelligent systems, artificial intelligence, and software.the office of the secretary of defense (osd) is the umbrella supervisorybody for projects that do not fall within the jurisdiction of any existing bodywithin the department of defense (dod). the osd (or its historicalpredecessor) has supported a variety of computerrelated r&d efforts over thelast several decades.1 in the late 1950s, a dod task force designed thespecifications for cobol, which ultimately became the standard language forbusiness and commercial applications. more recently, the dod initiated andsupported the development of ada, a programming language prompted by adefenseestablishmentwide concern about the proliferation of different computerlanguages and the increasing dependence of the u.s. military on computers. in1984, the dod established the software technology for adaptable reliablesystems (stars) program to promote better software practice in both themilitary and the private sectors.currently, the osd (through the office of the director of defense researchand engineering) has begun to develop a software action plan to "develop andimplement integrated technology and management plans to ensure more costeffective software support."2 in conjunction with the management initiatives ofthis plan, the software technology strategy is intended to reduce equivalentsoftware lifecycle costs by a factor of two and to reduce software problem ratesby a factor of ten by the year 2000, as well as to achieve new levels of missioncapability.3 this strategy is based on five themes: software reuse, softwarereengineering to support already deployed systems, process support for softwaredevelopment, leverage of commercial technology for defense department needs,and the integration of artificial intelligence and software engineering technology.finally, over the last 40 years the national security agency (nsa) hasplayed important roles in the development of supercomputers, primarily insupport of its intelligencegathering mission. nsarelated research in cs&e hasfocused on highperformance computing, language processing, cryptography, andsecure computing and communications.national science foundationnow the primary supporter of academic research in cs&e as measured bythe number of individual investigators supported, the7 institutional infrastructure of academic cs&e221computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.national science foundation (nsf) became a major supporter of cs&e researchin the mid1970s, when it shifted support for scientific applications of computersto their parent sciences but left funding for the computer area unchanged, so thatessentially the entire allocation became available for research in cs&e.4 bydollar volume, the nsf is now the second largest funder of cs&e research withinthe federal government. figure 7.2 illustrates the nsf's history of funding cs&eresearch for the last 15 years. the budget for cs&e is the fastest growing budgetcategory at nsf, although the budgets for other disciplines start at much higherlevels.another major turning point in the relationship of the nsf to cs&e was theformation of the cise directorate in april 1986. prior to 1986, cs&e receivedfunding through several directorates (engineering, mathematics and physicalsciences, and biological and behavioral sciences). a memo to nsf staff fromthendirector erichfigure 7.2 national science foundation obligations for research for computerscience (basic and applied), fy 1976 to fy 1991, in constant fy 1992 dollars.source: basic data (in thenyear dollars) for all recipients and academia weretaken from the corresponding sources cited in the caption for figure 7.1.7 institutional infrastructure of academic cs&e222computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.bloch stated the rationale for restructuring: "nsf has considerable activities incomputer science, information science, computer engineering, supercomputersand networking. our investment in these new and important areas is growingrapidly. many of the existing projects, programs, and initiatives are interrelatedand support a common community of scientists and engineers. in order to assure abroad and thorough understanding of our opportunities and responsibilities, acloser linkage between these organizationally separate groups is important."5nsf's cise directorate is the primary federal supporter of investigatorinitiated cs&e research, although programs in other directorates do supportrelated research. for example, elements of the fy 1993 highperformancecomputing and communications program, discussed in chapter 1, can be foundin the biological sciences directorate, for protein folding; the engineeringdirectorate, for optical computing; and the mathematical and physical sciencesdirectorate, for parallel algorithms for computational physics.prior to the formation of the cise directorate, the case for funding cs&eresearch was argued not by computer scientists or engineers but by otherswithout substantial background in cs&e. current and former nsf officials arguethat the combination of several programs under the cise directorate strengthensthe institutional influence of the cs&e community.6 in addition, the creation ofthe cise directorate is an acknowledgment that cs&e as a discipline issufficiently different from others to warrant consideration on its own; this pointechoes those made in the chapter 6 section "intellectual and structuralcharacteristics of cs&e as a discipline" about differences between cs&e andother disciplines.figure 7.3 illustrates various programmatic statistics of significance to thecs&e community:7 the number of proposals submitted and awards made has grown steadilyand substantially since fy 1986. however, proposal growth hasoutstripped award growth for most of the period from fy 1985 to fy1990, leading to a declining success rate (i.e., the ratio of proposalsfunded to proposals submitted). in fy 1990, the success rate rose for thefirst time in several years, from 26 percent in fy 1989 to 30 percent infy 1990; it is now comparable to the average across all nsfdirectorates. (nevertheless, cise officials report that they receive morescientifically meritorious proposals than they can fund. several currentand former cise officials have said that their best guess is that onaverage, about 50 percent of proposals submitted would probablyproduce good science.)7 institutional infrastructure of academic cs&e223computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.figure 7.3 changes in program statistics for the nsf computer andinformation sciences and engineering directorate, fy 1985 to fy 1990,including relative number and size of awards, and relative number of proposalssubmitted. 1985 = 1.0. source: national science foundation, backupdocumentation for "background material for longrange planning: 1993œ1997," national science board, nsf, washington, d.c., june 20œ21, 1991. the constantdollar value of the median award dropped by about 20percent between fy 1985 and fy 1990, a trend that has raised concernin the community, given the increasing costs of doing research.the cise directorate allocates a little under 10 percent of its budget to thedevelopment of institutional infrastructure to support experimental computerscience and engineering ($19 million under the fy 1992 spending plan, out of atotal cise budget of $210.9 million); the impact of this program on universitiesis discussed below (see the section "private nongovernmental organizations"). afar larger portion of its budget (about 47 percent for fy 1992) supports asubstantial computing infrastructure for use by the general science andengineering community as well as the cs&e field. the most important7 institutional infrastructure of academic cs&e224computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.aspects of this infrastructure are the nsf supercomputer centers ($64.3 million),nsfnet ($25.8 million), and several science and technology centers ($8.8million).nsf supercomputer centersthe four nsf supercomputer centers provide academic and industrial userswith powerful stateoftheart computational capabilities. these centers wereestablished in 1985œ1986; they are not now and have never been intended to becenters of cs&e research. but in the halfdozen years since their establishment,it has become increasingly clear that drawing sharp lines between providingcomputational facilities for other disciplines as opposed to cs&e is oftenunfeasible. for example, as new parallel computers become available at thecenters, nearby departments of cs&e may use them for educating their ownstudents about new parallel programming paradigms. given the increasinglyvaried choice of parallel architectures on the market, supercomputer centers andcs&e departments may find it beneficial to cooperate in choosing machinesappropriate to the local environment.in addition, it is true that most novice users are unable to exploit the fullpotential of supercomputers without extensive consultation with computerscientists and engineers who have a much keener understanding of the hardwareand software available. as these consultations have proceeded, inadequacies inexisting tools (especially software) have been identified, and work has beenundertaken to eliminate these inadequacies. some nontrivial portion of such workhas been nonroutine work that by any reasonable standard qualifies as research.for example, the supercomputer centers have played a major role in thedevelopment of scientific visualization, i.e., displaying for human consumptionmany megabytes of data in a form that is quickly and easily understood.performance evaluation of new supercomputer architectures is technicallydemanding. to the extent that novel architectures for parallel processing will firstcome into scientific and engineering use at the supercomputer centers, their rolein providing software to exploit these architectures will increase, requiring evengreater cs&e effort to develop such software.finally, the supercomputer centers are likely to serve an ever larger clientelein the future, most of whom will not have local access. thus the centers maybecome hubs for highspeed networking activities that will require substantivecs&e input.the role of the supercomputer centers in technology transfer to thecommunity at large has also increased as many of the software7 institutional infrastructure of academic cs&e225computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.tools developed within these centers have been put to use in other highperformance computing environments. for example, these centers have beenmajor distributors of socalled coordination languages (e.g., linda, char, pvm,and xpress); such languages are integral to machineindependent programmingenvironments that facilitate the transfer of programs between computers, rangingfrom networked workstations acting as a single machine to largescale parallelmachines. transfer of software by network (the ''file transfer protocol," or ftp)accounts for a great deal of technology transfer.nsfnetthe nsf also supports the nsfnet, the backbone of a network thatconnects hundreds of colleges and universities in the united states with highspeed links and is used by departments of all varieties, including cs&e. theextent to which nsfnet serves cs&e versus other disciplines is unclear. giventhe role that cs&e departments have played in the development of networkservices and data communications, it is likely that cs&e department membersuse nsfnet more than members of other departments. yet workers in otherdisciplines often need to transfer data in much larger quantities (e.g., for scientificvisualization) than do computer scientists or engineers, and so cs&e may be aless dataintensive user than other disciplines.science and technology centersfinally, in recent years, the nsf has begun to support interdisciplinaryscience and technology centers (stcs). four involve cs&e departments in amajor wayšthe stc for discrete mathematics and theoretical computerscience (involving rutgers university, princeton university, bell laboratories,and bellcore), the stc for computer graphics and scientific visualization(involving brown university, the university of utah, cornell university, theuniversity of north carolina, and the california institute of technology andpartially supported by darpa, ibm, digital equipment corporation, andhewlettpackard as well as nsf), the stc for parallel computing (involvingrice university, the california institute of technology, argonne nationallaboratory, oak ridge national laboratory, and los alamos nationallaboratory), and the stc for research in cognitive science at the university ofpennsylvania. the stcs are intended to support work on "complex researchproblems that are largescale, of long duration, and that may require specializedfacilities or collaborative relationships across scientific and engineeringdisciplines."87 institutional infrastructure of academic cs&e226computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.national aeronautics and space administrationthe cs&e research of the national aeronautics and space administration(nasa) involves concurrent processing, highly reliable costeffectivecomputing, scientific and engineering information management, and artificialintelligence (ai). the first three areas support work in networked access,management of large scientific data sets, scientific visualization, massivelyparallel processing, development of very reliable, very complex software, andsoftware producibility. the ai effort is relevant to a variety of nasaresponsibilities and focuses on expert systems for diagnostic, consulting, andultimately online control of shuttle and planetary probe operations, dynamicschedulers for shuttle operations, and largescale capture of knowledge for use inknowledge engineering databases.nasa's support for cs&e has fluctuated considerably over the years, as hasthe fraction that has gone to universities and colleges. figure 7.4 illustratesnasa's history of funding cs&e research for the last 15 years.figure 7.4 nasa obligations for research for computer science (basic andapplied), fy 1976 to fy 1991, in constant fy 1992 dollars. source: basicdata (in thenyear dollars) for all recipients and academia were taken from thecorresponding sources cited in the caption for figure 7.1.7 institutional infrastructure of academic cs&e227computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.in the 1960s, the apollo program made substantial use of advancedcomputer systems.9 nasa focused on reliable and faulttolerant computing. inthe early and mid1970s, nasa supported computer work related to the spaceshuttle, which declined as the shuttle reached operational capability in the early1980s. nasa initiated work on the use of supercomputing for image processingand modeling of aerodynamic structures and created several centers (the researchinstitute for advanced computer science, the institute for computerapplications in science and engineering, and the center of excellence in spaceand data information studies) in which a substantial amount of internal andexternal cs&e research is supported. in recent years, nasa has started to focusmore on issues of scientific data management, as the forthcoming mission toplanet earth begins. (more information on the computing aspects of nasa'searth observing system is contained in chapter 2.)in addition to its support for cs&e research, nasa spends about $40million per year on computational science and modeling and an additional $250million per year on computational facilities (including networking, equipmentleases, and software support); indeed, nasa spends more on supercomputersthan does nsf, although nasa supercomputing is missionoriented, whereasnsf supercomputing serves many research users.department of energythe office of energy research (oer) is the primary source of funding forcs&e research supported by the department of energy (doe), which includeswork on programming languages, automated reasoning systems, distributedsystems, machine architectures for scientific computation, algorithms for parallelcomputing, and management of scientific data. future programs are likely toemphasize distributed and massively parallel computing, portable and scalablelibraries, environments for computational science, security, visualization andimaging, and very large scientific databases.since 1945, the doe and its predecessors have supported the developmentof highperformance supercomputers for their application in the design anddevelopment of nuclear weapons. indeed, the first american electronic digitalcomputer ever developedšeniacšwas used to support problems incomputational physics and engineering associated with the development ofatomic bombs in the postwar era.10 along the way, a variety of supercomputerapplications relevant to other doe missions have emerged, and a great deal ofsophisticated mathematical software has been distributed for general7 institutional infrastructure of academic cs&e228computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.use outside the nuclear weapons community. given its interests in simulation,doe has been an important stimulator of developments in highperformancecomputing and computational science, and it has provided an important marketfor the domestic supercomputer industry.figure 7.5 department of energy obligations for research for computerscience (basic and applied), fy 1976 to fy 1991, in constant fy 1992 dollars.basic data (in thenyear dollars) for all recipients and academia were taken fromthe corresponding sources cited in the caption for figure 7.1.the doe is the fourth largest funder of cs&e research within the federalgovernment. figure 7.5 illustrates the doe's history of funding cs&e researchfrom 1976 to 1991. a substantial fraction of the doe budget for cs&e researchis consumed by national laboratories, whose future with respect to budgets andshifts to civilian work after the collapse of the soviet union remains to be seen.other federal agenciesother federal agencies account for only a small fraction of the total cs&eresearch budget. among these, two are notable.7 institutional infrastructure of academic cs&e229computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.national institutes of healththe national institutes of health (nih) has supported several important butspecialized advances in computer science, particularly in expert systems formedical purposes. in the late 1960s, it took over funding for a former nasaproject, dendralšan expert system developed to interpret mass spectrogramsand thus to elucidate chemical structures. dendral laid many of thefoundations for current expert systems. in 1973 nih began to support a center atstanford university for applications of ai to medicine and biology. work at thiscenter has led to a variety of expert systems: mycin for matching patients withserious infections to appropriate antibiotics, puff for diagnosing lung diseases,the casnet glaucoma specialist, and internist, a diagnostic system forinternal medicine.11the nih does not today support a great deal of research that it identifies ascs&e research per se. however, it does sponsor externally and conduct internallya large amount of biomedical research that has important cs&e components. asmall fraction of the total nih budget for biomedical research of about $6.5billion per year supports computational tools for medical research, mostly forsoftware development. computer sciencerelated activities supported by the nihinclude imaging and virtualreality projects, molecular modeling, highspeedcomputing, largedatabase technology, statistics, instrumentation, ai and expertsystems for medicine, medical language systems, and simulation.national institute of standards and technologythe national institute of standards and technology (nist) within thedepartment of commerce houses the national computer systems laboratory, anin house research effort in computer science with resources of about $25 millionper year and 250 people, but does not support extramural research. nistconducts some cs&e research (e.g., on optical character recognition) that isfocused primarily on the needs of other government organizations and agencies.nevertheless, private industry makes considerable use of nist work, since nistplays a key role in setting standards and does other important work in security.the nist also supports the advanced technology program (atp), a program tosupport the development of generic, precompetitive technologies. the atpprogram was funded at $50 million for fy 1992 and is directed primarily atindividual businesses or consortia of businesses and universities.7 institutional infrastructure of academic cs&e230computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.private nongovernmental organizationsuniversitiesuniversities and departments are a key aspect of the institutionalinfrastructure that supports academic cs&e. but the youth of cs&e as adiscipline has led to certain anomalies in its role within the university.for example, in contrast to disciplines such as chemistry and physics that areoverwhelmingly housed in departments dedicated to these disciplines and aregenerally located in colleges of arts and sciences, academic cs&e programs arehoused in a variety of departments (table 7.1). highly rated programs in cs&eare housed variously in autonomous departments (e.g., the department ofcomputer science at stanford university), in mixed departments (e.g., thedepartment of electrical engineering and computer science at mit and at theuniversity of california, berkeley), and in separate schools (e.g., the school ofcomputer science at carnegie mellon university). the computer sciencedepartment at brown university is treated as any other department in a universityof arts and sciences, whereas the computer science departments at ucla andthe university of pennsylvania are located within the school of engineering; thecomputer science department at cornell university is part of the college of artsand sciences and the college of engineering.table 7.1 departmental titles for cs&edepartment titlenumber of departmentscomputer science(s)92electrical and computer engineering19computer and information science(s)10computer science and engineering13electrical engineering and computer science10electrical engineering2computer engineering4computing science2computer science and operations research2mathematical and computer sciences3other titles9 (1 each)note: a total of 166 departments are represented, out of a total of 168 ph.d.granting departments inthe united states and canada.source: david gries and dorothy marsh, "the 1990œ1991 taulbee survey," computing researchnews, volume 4(1), january 1992, pp. 8 ff.7 institutional infrastructure of academic cs&e231computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.cs&e ph.d. production is concentrated in a relatively few departments. the12 topranked departments of 137 ph.d.granting computer science (note:computer science only) departments awarded 233 doctorates in computer sciencein the 19901991 academic year, or 27 percent of all computer science ph.d.s inthat year; the 36 topranked departments accounted for 57 percent of the ph.d.sawarded. the ph.d.perdepartment average of these 36 departments (13.7 perdepartment) was well over three times that of the remaining 101 departments (3.6per department).12 major research institutions are also the most importantundergraduate source for academic cs&e ph.d. graduate students (table 7.2).the number and size of ph.d.granting departments in computer sciencehave grown considerably in the past several years. according to the annualtaulbee surveys, in 1984œ1985 there were 103 such departments with a total of1741 faculty members (or 16.9 faculty members per department); by the 1990œ1991 academic year, these figures had increased to 137 departments with 2725faculty members (ortable 7.2 baccalaureate origins of doctorate recipients in cs&e, by carnegieclassification, 1989carnegie classificationcomputer sciencecomputer engineeringresearch ia43%50%research iib+ doctorate grantingc24%26%comprehensived19%12%liberal artse11%5%other3%7%total with known classification29142total ph.d.s531117a university receives at least $33.5 million per year in federal money for r&d and awards at least 50ph.d.s per year (e.g., university of california at berkeley).b university receives between $12.5 million and $33.5 million per year in federal money for r&d andawards at least 50 ph.d.s per year (e.g., university of california at santa barbara).c university awards at least 20 ph.d.s per year in one discipline or 10 or more in three disciplines(e.g., university of california at santa cruz).d institution awards undergraduate and master's degrees only; more than 1500 students enrolled; morethan half of undergraduate degrees awarded in occupational or professional disciplines (e.g., anyuniversity in the california state university system).e institution awards more than half of its degrees in liberal arts fields.source: data from survey of earned doctorates, office of scientific and engineering personnel,national research council, washington, d.c.7 institutional infrastructure of academic cs&e232computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.19.9 faculty members per department).13 (figures for computer engineering for1984œ1985 are not available.)a second relevant aspect of university infrastructure is the capitalization ofcs&e departments. as noted in chapters 1 and 6, research problems in cs&eare often driven and motivated by the upper bounds of performance at the cuttingedge of computing technology (whether the cutting edge results fromsophisticated new components or novel arrangements of older components); goodcurrent examples include graphics and parallel computing. research in computergraphics is very difficult today without the very fast graphics processors neededfor threedimensional displays, and experimental research parallel computing isimpossible without access to parallel computers. however, stateoftheartsystems are always expensive, and acquisition of such equipment does notbenefit from the downward cost trend that characterizes computing equipment of agiven sophistication or performance. researchers in these areas are thereforeoften hardpressed to assemble sufficient funds to pursue their research agendas.compounding the problem is the fact that a system that is state of the art todaymay not remain so for very long.since hardware evolves rapidly, recently purchased hardware contributesmore to the generation and solution of research problems than does olderhardware. since a considerable fraction of new cs&e ph.d.s enter academiaeach year, and relatively few researchers retire, the pool of cs&e researcherscompeting for access to stateoftheart equipment grows ever larger.nevertheless, annual equipmentacquisition budgets remain level at best, and thetrend indicated in figure 7.6 suggests that annual spending on equipment haseven begun to drop. the inescapable conclusion is that the availability of stateoftheart computational resources is not keeping up with the demand for theiruse, and that this has been true for a long period of time.in addition, academic computer scientists and engineers have oftenexpressed concern that the costs of software are not adequately included in mostassessments of capitalization. software is of course a key element of research incs&e, but the available data do not permit a determination of the extent to whichsoftware is included in assessments of capitalization.capitalization for educational purposes is also an important aspect ofacquisition budgets. as noted in chapter 1, students who must use computersystems with limited capability must often struggle with machine limitationsrather than focusing on central concepts that could be more clearly illustratedwith more powerful machines. for example, truly interactive visualization orcomputeraided de7 institutional infrastructure of academic cs&e233computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.sign (cad) requires a response time of less than a few tenths of a second betweenuser input and screen response. a visualization or cad that responds in 2seconds rather than 0.2 seconds gives the user an entirely misleading sense of itsfull value and potential.figure 7.6 academic spending on equipment for use in computer scienceresearch, fy 1981 to fy 1990, in constant fy 1992 dollars. source: basicdata (in thenyear dollars) provided by science resources survey, nationalscience foundation, washington, d.c.equipment capitalization is concentrated in a relatively few departments. in1988, 20 institutions had about 58 percent of the dollar value of computer scienceresearch equipment held by a total of 147 institutions (including those 20);14 thesefigures do not include computer centers operated for the benefit of the entireinstitution.the concentration of resources for cs&e research in a few selectedinstitutions has been noted from time to time by the cs&e community. forexample, the feldman report15 issued in the late 1970s argued that experimentalcomputer science was threatened by inadequate equipment capitalization at toomany schools. one response to these concerns was the coordinated experimentalresearch (cer) program initiated in 1979 by the national science foundation.this program was designed to support the development of research equipmentinfrastructure at universities for the support of experimental research7 institutional infrastructure of academic cs&e234computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.projects in computer science. universities were selected on the basis of havingstrong cs&e programs.16 the ultimate purpose of the cer program and itsfollowon (the institutional infrastructure program) is to increase the number ofuniversities that are capable of performing sophisticated experimental cs&eresearch (by faculty and graduate students engaged in dissertation work). underthe present institutional infrastructure program, firsttime awards range from $2million to $4 million for five years, or about $400,000 to $800,000 per year; thefy 1991 budget allocated about $16.5 million to the institutional infrastructureprogram.efforts (including but not limited to the nsf cer program) to support theresearch equipment infrastructure in cs&e have been largely successful. forexample, 62 percent of the research equipment owned by the 127 other cs&edepartments in 1988 was purchased in the two years preceding, compared to 52percent in the top 20 departments.17 however, unless a cer grant is renewed,grants terminate in five years, leaving recipients to pay afterwards for bothreplacement and maintenance.18university infrastructure for cs&e may gain a further boost from the highperformance computing and communications program. of course, as previouslynoted, actual funding levels for the hpcc program have yet to be determined.professional organizationsseveral professional organizations have had an impact on the practice ofresearch and education in cs&e. these organizations include the association forcomputing machinery (acm) and the ieee computer society, the computerscience and telecommunications board of the national research council, andthe computing research association.the acm and the ieee computer society are the leading professionalsocieties for cs&e. for example, the dozen or so publications each of the acmand the ieee computer society are major channels for the archival storage of newresults and at times provide the first public look at innovations in commercialcomputing technology. some of the journals published by these organizations arethe most prestigious in cs&e; others are sent to the entire membership of theorganization and thus serve to promote intellectual awareness of othersubspecialties among more narrowly focused researchers.both organizations also sponsor a wide variety of conferences andworkshops every year. conferences and workshops serve to disseminate newresults more rapidly than is possible through printed media, a feature that isparticularly important to a field as fast7 institutional infrastructure of academic cs&e235computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.moving as cs&e. for the acm, conferences and workshops often revolvearound its 30 or so special interest groups (sigs). the acm sigs are proposed,organized, and operated by a group of researchers in a particular area of the fieldwho want more interaction with their colleagues. some of these sigs are quitelarge and involve most of the important researchers in a given subspecialty.several conference proceedings (e.g., those of siggraph (sig on computergraphics), sigops (sig on operating systems), sigcomm (sig on networkingand communications), focs (foundations of computer science), sigact (sigon automata and computer theory), sigplan (sig on programming languages),and sigarch (sig on computer architectures)) are prestigious and tightlyrefereed; thus they often serve as the premier vehicles of dissemination fordevelopments in the fields they cover, and are often preferred over archivaljournals.conferences sponsored or organized by the ieee computer society centeron its 30 or so technical committees (analogous to the special interest groups ofthe acm). some technical committees are also quite large and have had a majorimpact on the field. the ieee computer society has also played a role in thepromulgation of standards for various computing technologies.undergraduate education in cs&e in its early days owes much to the acm,which has been responsible for a number of initiatives over the years indeveloping curricula for undergraduate degrees in computer science. forexample, the acm sponsored the first major work on curricula in computerscience, curriculum 68, which had a major influence on the undergraduatecurriculum in the many cs&e departments formed in the 1970s. more recently,the acm and the ieee computer society have worked together on curricularefforts, and they jointly created the computer science accreditation board, anorganization that accredits undergraduate departments of computer science.for many years, academic cs&e lacked a major voice in the public policydebate. by contrast, most other disciplines have an organization that representsthat discipline to society. in many instances, policy makers know about theseorganizations and respect their judgments on issues of public importance. theorganization monitors events, provides information when requested, organizestask forces on topics that need attention, keeps in touch with similar organizationsin neighboring fields, and works to inculcate in its members the idea that serviceto the community and society is not only useful but necessary.an example is the american physical society (aps), which represents theresearch physicists of the nation. in the midst of the debate7 institutional infrastructure of academic cs&e236computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.over the strategic defense initiative in the late 1980s, the aps issued what waswidely regarded within the public policy community as an authoritative report onthe feasibility of directedenergy weapons for defense against strategic ballisticmissiles.in recent years, two rather different organizations have begun to serve such arole for the cs&e research community. they are the computer science andtelecommunications board and the computing research association.the computer science and telecommunications board (cstb) of thenational research council (the operating arm of the national academy ofsciences, the national academy of engineering, and the institute of medicine)provides representation for the computing and communications field in aprestigious organization that provides independent analysis and advice to thefederal government. the charter of the cstb is to examine technical,competitiveness, and public policy issues related to computer andcommunications science and technology. in this role, the cstb composes studycommittees of leading computer scientists and engineers in academia andindustry; convenes highlevel meetings among senior researchers, executives, andgovernment officials to discuss specific issues; and produces and disseminatesreports. through its activities, the cstb promotes active intellectual crossfertilization among the technical, business, and public policy communities.the computing research association (cra) is supported primarily byacademic departments of cs&e that engage in research activity, whetherdoctorategranting or not, and engages the public policy process on their behalf.in addition to sponsoring the biannual snowbird meetings of departmental chairs,the cra is responsible for the annual taulbee surveys of ph.d.grantingdepartments. it also issues a wellreceived newsletter, organizes other surveys andreports where appropriate, and promotes service work in the cs&e community.other professional organizations that serve the cs&e community are thesociety for industrial and applied mathematics (emphasizing the theory andcomputational aspects of cs&e), the computer professionals for socialresponsibility (an organization representing those interested in the social impactof computing technology), and the ieee communications society (serving thenetworking community).notes1. kenneth flamm, targeting the computer: government support and international competition,the brookings institution, washington, d.c., 1987, pp. 75œ76.2. u.s. department of defense, department of defense software technology strategy, december1991, prepared for the director of defense research and engineering, p. es1.7 institutional infrastructure of academic cs&e237computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.3. u.s. department of defense, department of defense software technology strategy, 1991, p. es2.4. kenneth flamm, targeting the computer, 1987, p. 88.5. john walsh, ''nsf to establish computer directorate," science, volume 232, april 4, 1986, page18œ19.6. this strength is illustrated by the fact that the cise directorate's budget has grown significantlyrelative to those of the other research directorates within nsf (from 8.5 percent of the total nsfbudget in fy 1986 to about 11 percent in fy 1991). moreover, although the cise budget providesfor service functions to the entire science and engineering community as well as research support forthe cs&e community (e.g., the nsf supercomputer centers and nsfnet), the research componentof the cise budget exhibits a similar trend. put another way, growth in the service functions of thecise directorate is not disproportionately responsible for growth in the overall cise budget. aneasily available source for the funding history of nsf and cise can be found in terry walker, "areview of federal funding for research in computer science and engineering," computingresearch news, april 1990, pp. 6œ14.7. data presented for fy 1985 and fy 1986 are for those proposals submitted to the various nsfprograms that were consolidated into the cise directorate in 1986.8. national science foundation, nsf science and technology research centers, omb 31450058,undated.9. kenneth flamm, targeting the computer, 1987, pp. 84œ85.10. kenneth flamm, targeting the computer, 1987, p. 78.11. kenneth flamm, targeting the computer, 1987, pp. 90œ91.12. david gries and dorothy marsh, "the 1990œ1991 taulbee survey," computing research news,volume 4(1), january 1992, pp. 8 ff.13. data for 1984œ1985 are taken from david gries, "the 1984œ1985 taulbee survey,"communications of the acm, volume 26(10), october 1986, pp. 972œ977. data for 1990œ1991 aretaken from david gries and dorothy marsh, "the 1990œ1991 taulbee survey," computingresearch news, volume 4(1), january 1992, p. 10.14. these 20 institutions have about $97.9 million of total in use research equipment held by allinstitutions in the sample. this estimate is derived by multiplying the mean dollar amount ofcomputer science research equipment for these 20 institutions (listed in table 7 in the nsf reportcited below as $4.895 million) by 20. table 2 in the same report lists the aggregate purchase price ofresearch equipment in these institutions as $168 million. see national science foundation, academicresearch equipment in computer science, central computer facilities, and engineering: 1989, nsf91304, nsf, washington, d.c., 1989, table 2 (p. 4) and table 7 (p. 7).15. jerome a. feldman and william r. sutherland, "rejuvenating experimental computer science,"communications of the acm, september 1979, pp. 497œ502.16. the three institutions with the largest federal grants for computer science (stanford university,carnegie mellon university, and mit), each with about $5 million to $8 million annually in federalfunding for computer science in 1979, agreed not to apply for these grants.17. see national science foundation, academic research equipment in computer science, centralcomputer facilities, and engineering: 1989, nsf 91304, nsf, washington, d.c., 1989, figure 4,p. 8.18. maintenance and repair costs are considerable. for example, annual expenditures for equipmentmaintenance and repair are about $0.37 per dollar of cs&e research equipment, compared to anaverage of $0.21 per dollar of scientific and engineering equipment taken across all fields. indeed,cs&e maintenance and repair costs are the highest among those for all science and engineeringfields. see national science foundation, academic research equipment and equipment needs inselected s/e fields: 1989œ1990, nsf 91311, nsf, washington, d.c., may 1991, table 3, p. 4.7 institutional infrastructure of academic cs&e238computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.8 human resourcesbaccalaureate and postbaccalaureate degreeproductioncompared to other academic disciplines, academic cs&e is new and isgrowing rapidly. the electronic storedprogram computer is some 50 years old,and around this invention has grown a thriving and productive intellectualdiscipline. in this time, over 150 ph.d.granting cs&e departments have beenestablished, along with perhaps 850 other cs&e programs nationally. theseinstitutions have produced thousands of ph.d.s and hundreds of thousands ofgraduates with bachelor's degrees. in addition, many other institutions havedeveloped programs in information sciences, library sciences, managementinformation systems, and so on; in many cases, degrees awarded by these latterinstitutions include at least some of the cs&e material that other institutionsmight include as part of a cs&e undergraduate degree, although they tend not tocover such material as broadly or as deeply.this diversity in computerrelated degree programs makes it difficult toobtain detailed insight into degree production. in gathering data sources for thisreport, the committee considered whether or not to include in its definition ofcs&e degree recipients those who had received degrees in "informationsciences" or "information systems," since many sources group these categoriestogether. because it was8 human resources239computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.most concerned with what might be considered "core" activities in cs&e, thecommittee chose to exclude these categories, recognizing that in doing so itmight also exclude, for example, those for whom cs&e database work was somepart of their educational or research portfolios.partly for definitional reasons such as these, data sources for ph.d.production in cs&e conflict, as illustrated in table 8.1.1 however, despite thesediscrepancies, it is clear that growth in cs&e ph.d. production has been large inpercentage terms when measured over the last decade or so.in the shortterm, the future supply of ph.d.s depends in part on the pipelineof people obtaining bachelor's and master's degrees. the major source of cs&eph.d. students is students graduating with bachelor's degrees in cs&e. as notedin table 8.2, the number of bachelor's degrees awarded in cs&e climbed sharplyin the early 1980s but began to drop after 1986. if this indicates an enduring8 human resources240computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.table 8.2 total degree production in computer science, 1980 to 1989total number of degrees awardeddegree awarded1980198119821983198419851986198719881989b.s./b.a.11,21315,23320,43124,68232,43539,12142,19539,92734,89630,963m.s./m.a.3647421849355321619071018070849191669399ph.d.218232220286295310399450515612total15,07819,68325,58630,28938,92046,53250,66448,86844,57740,974(pct.)a(4.1)(5.4)(6.8)(7.9)(9.9)(11.6)(12.5)(12.2)(11.3)(10.4)a degrees awarded in computer fields as a percentage of all science and engineering degrees awarded.source: national science foundation, science and engineering indicators: 1991, nsf, washington, d.c., 1991, tables 2œ7, 2œ14, and 2œ16 (category listed as "computerscience" includes information sciences).8 human resources241computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.trend, it could portend difficulties for the supply of quality graduate studentsin cs&e2 unless the attrition in supply is limited strictly to undergraduatestudents of lower quality. (representatives from the computer industry whobriefed the committee noted their concern about dropping degree production aswell, since they are major employers of persons with bachelor's degrees incs&e.)the downturn in bachelor's degrees awarded has been a matter of somespeculation in the academic community. some believe the downturn istemporary, and indeed some institutions (such as berkeley and mit) havereported an upturn in 1991 in undergraduate enrollments. others have reported nosuch turnaround.there is also no consensus concerning possible reasons for the downturn.some note that the peak occurred roughly five years (i.e., about the average timeit takes to obtain a bachelor's degree) after the introduction of the personalcomputer; perhaps personal computers have demystified the field, reduced theneed for students to major in cs&e to obtain access to computers, or otherwisechanged its image and allure. others have argued that an increase in the numberof students taking programming in high school has led to the downturn.although ph.d. production in cs&e has risen rapidly in the last decade, it isstill small compared to that of other fields, as table 8.3 indicates. note inparticular that the number of cs&e ph.d.s produced in 1989 is less than twothirds that of its parent disciplines, electrical engineering and mathematics, andabout onehalf that of physics. production of ph.d.s in cs&e is also timeconsuming: the total time to degree (i.e., the interval between receipt of abachelor's degree and receipt of the ph.d. degree) is somewhat longer for cs&ethan for other major science fields, and the change in total time to degree hasbeen largest for cs&e and biological sciences (table 8.4). given theemployability of individuals with strong computer skills, it is likely that thereason for the greater total time to degree of cs&e ph.d.s is that many withbachelor's and master's degrees in cs&e enter the work force prior to resumingph.d. study in cs&e. this possibility is consistent with the approximatecomparability of "registered" time to degree for cs&e ph.d. recipients and thosein other fields.the primary source of support for ph.d. recipients in cs&e is researchassistantships, although the percentage of recipients with this source of supporthas dropped slightly over the last decade (table 8.5). of interest is the substantialfraction of recipients who are supported by "other" sources (which includeindustry, family, nonu.s. government support for foreign students, savings, andself).8 human resources242computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.table 8.3 relative ph.d. production, cs&e vs. other fields, 1979 to 1989number ofdoctoratesawardeddoctoralfield19791980198119821983198419851986198719881989growthatotal ph.d.productioncomputerscienceb2352312482312762692643653984545312.263,502computer engr.7862717283565577621001171.50833electrical engr.5334784785445175936317066918869951.877,052mathematics7447317127096896856737197267378471.147,972physics andastronomy11089831015101410431080108011871237130212741.1512,323biologicalsciences364638033804389337413880379338073840411241151.1342,434a "growth" refers to the ratio of 1989 production to 1979 production.b "computer science" includes ''computing theory and practice," often classified as a subfield of mathematics.source: data from survey of earned doctorates, office of scientific and engineering personnel, national research council, washington, d.c.8 human resources243computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.as table 8.6 indicates, a substantial fraction of new cs&e ph.d. holdersplan to go directly into faculty positions rather than the postdoctoral positionsthat characterize other fields. industry absorbs a substantial portion of cs&eph.d.s as well.increasing ph.d. production in cs&e to 1000 per year is one statedobjective of the hpcc program. given the lack of a systematic study of recentacademic opportunities,3 the appropriate level of ph.d. production for the cs&efield is a matter of some controversy in the community.on the one hand, many new cs&e ph.d.s (and their faculty mentors) report arecent tightness in the academic market, suggesting that even current levels ofph.d. production are high given the demand for new faculty. on the other hand,other observers believe8 human resources244computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.8 human resources245computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.that the reported tightness refers to faculty positions in the top tier of majorresearch institutions, and that demand for new cs&e ph.d.s is higher in othersectors, such as mathematics and computer science departments in fouryearcolleges. (the filling of such positions by cs&e ph.d.s might well have asubstantial and positive impact on the level and quality of cs&e instruction atsuch institutions, as suggested in chapter 4, "education in cs&e.") since cs&eph.d.s have major roles to play in the computer industry and throughout societyas well, some even suggest that 1000 ph.d.s per year will ultimately proveinadequate (especially if their skill sets are broadened to accommodateresponsibilities other than traditional cs&e research). achieving this dispersionmay entail a shift in job expectations among new cs&e ph.d.s, as discussed inthe chapter 4 section "employment expectations for holders of cs&e degrees."even with an expansion in the number and size of ph.d.granting departments,positions in these departments will be only a portion of the total employment basefor cs&e ph.d.s.information on the demand for holders of bachelor's and master's degrees iseven less certain than that for holders of ph.d.s. it is known that a very largefraction of bachelor's and master's degree holders go to industry and commerceupon graduation, and it makes sense to assume that a significant fraction of themtake computerrelated jobs (e.g., programming).4most current or proposed definitions of "computing professional" or"computer specialist" inevitably reflect a narrow characterization of the positionas one in which a substantial portion of the job responsibilities require nonroutineinteraction with a computer. federal statistics experts recognize that a finerdegree of differentiation of computing professional is needed, and a proposedrevision to the master list of occupations, the dictionary of occupational titles,may add perhaps 30 computerrelated occupations. a finer differentiation is madepossible by both growth in the number of people in computer specialist jobs(supporting accurate statistics on subgroups) and recognition of the diversity ofcomputerrelated jobs. moreover, narrow characterizations of the employmentopportunities for cs&e graduates may become increasingly less appropriate(chapter 4).composition of academic cs&erepresentation of women and minoritiestotal numbers and trends tell only part of the story. prospects for the cs&etalent pool depend also on its makeup. women and8 human resources246computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.nonasian minorities continue to be underrepresented in cs&e relative to theirnumbers in the population at all levels in the cs&e educational pipeline. asshown in figures 8.1 and 8.2, cs&e has shown no demonstrable improvementover time in the rates at which ph.d.s have been awarded to women and nonasian minorities. at present, cs&e attracts women and nonasian minorities atapproximately the same rates as for the physical sciences at all levels, as noted intable 8.7; however, for both fields, women and minorities are increasinglyunderrepresented at higher levels of educational attainment.the representation of women and nonasian minorities in faculty ranks issomewhat lower than their representation as recipients of doctoral degrees incs&e. according to the 1990œ1991 taulbee survey,5 women and nonasianminorities account for about 7.5 percent and 2.2 percent, respectively, of alltenuretrack and tenured faculty in ph.d.granting cs&e departments. about 4.4percent of all fullfigure 8.1 percentage of doctorates awarded to women in cs&e and inphysical sciences (physics, astronomy, and chemistry), 1979 to 1989. source:data from survey of earned doctorates, office of scientific and engineeringpersonnel, national research council, washington, d.c.8 human resources247computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.professors in these departments are women, and about 1.7 percent of all fullprofessors in these departments are nonasian minorities.figure 8.2 percentage of doctorates awarded to nonasian minorities in cs&eand in physical sciences (physics, astronomy, chemistry), 1979 to 1989.percentage is calculated on basis of all ph.d. recipients who are u.s. citizens orpermanent residents, since data on race are not collected for those withtemporary visas. source: data from survey of earned doctorates, office ofscientific and engineering personnel, national research council, washington,d.c.anecdotal evidence that enrollments of women and minority students areshrinking disproportionately has prompted individuals, departments, andprofessional organizations to examine opportunities for women and minorities.of special concern is evidence suggesting barriers to full participation by womenand minorities. although many of these barriers are typical in all science andengineering fields (box 8.1), they are disturbing in light of the fact that cs&e isgenerally younger than other scientific and engineering disciplines. one mighthave presumed that the relative youth of cs&e relative to, say, physics, wouldhave led to a more inviting and welcoming8 human resources248computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.environment for women and minorities. that this is not so suggests a missedopportunity to support a more inclusive view of the field.table 8.7 percentage of degrees awarded to women and nonasian minorities incomputer and information sciences (cis) and in physical sciences (ps), 1989percentage of degrees awardedwomennonasian minoritiesadegreecispscispsbachelor's30.830.913.58.0master's27.926.86.34.4doctorate17.619.01.84.7note: the slight discrepancy in percentage of doctorates awarded between this table and those offigures 8.1 and 8.2 is due to the inclusion of information sciences and the exclusion of computerengineering in this table.a figures for nonasian minorities include only u.s. citizens and permanent residents.source: national science foundation, science and engineering indicators, 1991, washington,d.c., 1992, tables 27, 28 for bachelor's data, tables 214, 215 for master's data, and tables 216,217 for doctorate data.these trends have been recognized to a certain degree. for example, thepearl et al. article cited in box 8.1 summarizes a report prepared by the acmcommittee on the status of women in computer science. the leveson reportcited in box 8.1 was prepared for the nsf advisory committee on women incs&e and described a variety of activities that the nsf could undertake toimprove the status of women in cs&e. in addition, a variety of outreachprograms have been suggested; such programs include those aimed atencouraging women and minorities to take highschool courses in mathematicsand science, computer camps specifically for girls, production of game softwaredesigned to appeal to the interests of girls, support groups for women andminorities to reduce their sense of isolation in graduate school, and introductorycomputer science courses that emphasize the use of computers as tools.alternative cs&e degree programs can also be designed for adult students whowish to reenter the work force. for example, the electrical engineering andcomputer science department at the university of california, berkeley has anoutreach program, the computer science reentry pro8 human resources249computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.box 8.1 factors affecting participation of womenand minorities in all fields of science andengineeringmath avoidance in secondary school. girls tend to take far fewermath courses in high school than boys, in part because they arediscouraged by parents, teachers, and peers. a mathpoor high schooleducation makes the pursuit of a college degree in science or engineeringmuch more difficult.lack of role models. the number of women or minority facultymembers tends to be small (and it is not uncommon to find women orminority faculty members preferring to teach at nonresearch institutions).insensitive or hostile work environments. the evolution of socialattitudes towards women and minorities has not kept pace with laws barringsexual or racial discrimination. while there are fewer displays of overtdiscrimination today than in the past, attitudes of the "old boy" network oftenplay decisive roles in determining the advancement of women or minorityscientists or engineers.balancing personal and professional concerns. the time requiredto do quality cuttingedge work in the sciences or engineering is enormous,and s&e professionals must often spend considerable time in laboratoriesaway from family. while men and women both face this challenge, womenare much less likely than men to find companions willing to assume familyresponsibilities, and thus the challenge is greater for them.effects of cumulative disadvantage. the educational opportunitiesfor minorities are much poorer across the board than for the rest of thepopulation: education for minorities is associated with fewer role models,fewer educational resources, lower teacher expectations, and poorerteacher quality. taken together, such disadvantages pose tremendousbarriers to advancement.sources: includes information from nancy leveson, women incomputer science, a report for the nsf cise crossdirectorate activitiesadvisory committee, national science foundation, washington, d.c.,december 1989; and amy pearl, martha pollack, eve riskin, beckythomas, elizabeth wolf, and alice wu, "becoming a computer scientist: areport by the acm committee on the status of women in computingscience," communications of the acm, volume 33(11), november 1990,pp. 48œ57.8 human resources250computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.gram, designed for women and minorities who already have degrees. theacademic core of the oneyear program consists of a threesemester introductoryseries and three other courses in digital design, efficient algorithms, and discretemathematics. the program also provides tutoring. results appear promising.6these activities suggest that the cs&e field is awakening to the fact thatinvolving all types of students more fully can broaden and enrich the pool oftalent.involvement of foreign studentsas in other scientific and technical fields, a significant fraction of cs&egraduate students consists of individuals who are not citizens or permanentresidents. these foreign students account for a somewhat higher fraction ofph.d.s in cs&e than in the physical sciences, and the trend is uniformly upwards(figure 8.3).figure 8.3 percentage of doctorates awarded to foreign students in cs&e andin physical sciences (physics, astronomy, and chemistry), 1979 to 1989. foreignstudents are defined as those with temporary visas. percentage is calculated onbasis of all ph.d. recipients whose citizenship or visa status is known (alwaysover 92 percent). source: survey of earned doctorates, office of scientificand engineering personnel, national research council.8 human resources251computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.the implications of this trend are at present unclear. one issue is whetherforeign recipients of u.s.awarded ph.d.s return to their native lands (creating a"brain drain" from the united states to potential foreign competitors) or whetherthey stay in the united states. one data point is that in 1989, the percentage ofnew ph.d.s in cs&e that planned to work abroad in 1989 (7 percent) is muchlower than the number who have temporary visas (about 35 percent).these data also suggest that new cs&e ph.d.s tend to stay in the unitedstates in proportions about equal to those in other fields of science (table 8.8).these data do not account for visaexpiration lag times, but a 1989 nationalscience board (nsb) report noted that "overall, the u.s. research system shows adependence on foreign scientists and engineers, and this dependence is expectedto continue."7a second issue is whether foreign students displace u.s. students. the 1989nsb report also noted that "the impact of foreign enrollment on the quality ofprograms was generally viewed as positive" and called special attention to "ashortage in the supply of high quality u.s. applicants [and] a surplus of highquality applicants from abroad" and to a "substantial dependence upon the supplyof foreign applicants . . . to maintain the quality of graduate programs [incomputer science, physics, chemistry, and mathematics]."8the nsb also concluded that "both industry and engineering schools wouldexperience severe problems if engineering schools should severely restrict thetraining of foreign students or if the influx of fortable 8.8 breakdown (by percentage) of planned residency of new ph.d.s invarious disciplines, 1989percentage choosing residence indicateddisciplinetotal newph.d.sunitedstatesother countriesunknowncs&e64860733electricalengineering995521037mathematics847541136physics andastronomy127459833chemistry197072523biologicalsciences411571722note: percentages include ph.d.s awarded to both u.s. and foreign citizens.source: data from survey of earned doctorates, office of scientific and engineering personnel,national research council, washington, d.c.8 human resources252computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.eign students would diminish abruptly and significantly''šindustry because "asignificant proportion of foreign graduate students ultimately obtain employmentin the u.s." and engineering schools because "u.s.born students alone would beinsufficient to keep engineering education and research programs at their presentlevel."9a third concern has been that high percentages of foreign students appear tocorrelate with low percentages of women across several scientific disciplines.again, the reasons for this correlation are unclear. it is a matter of record thatforeign students are predominantly male, and so large numbers of foreignstudents would bias the overall gender balance towards men.10 but in addition,some have speculated that foreign cultures tend to be less accepting of women asscientific workers than is american culture, and that attitudes brought byforeignborn faculty and graduate students to american graduate education tendto discourage the full participation of women.11 others have argued that the fieldsinvolved have simply found it less threatening or difficult to seek qualifiedstudents from abroad than to undertake the largescale changes that would benecessary to attract larger numbers of women to these fields.youth and rapid growth of computer science andengineeringthe median age of faculty in a given field is one indicator of the maturity ofthat field. in 1989 the average doctoral faculty member in computer science was2.8 years younger than counterparts in other scientific and engineering fields(table 8.9). these age distributions also suggest that faculty retirements incomputer science are likely to lag somewhat behind those in other science andengineering disciplines.12 note that the median age for faculty in computerscience in 1989 was the median age for all science and engineering faculty in1981šan 8year lag.the youth and rapid growth of cs&e are also reflected in the distribution ofits faculty ranks. as noted in table 8.10, the fraction of cs&e faculty with therank of full professor actually decreased between 1977 (35 percent) and 1989 (30percent), probably as the result of a rapid influx of new assistant professors; thecomparable fraction for other disciplines grew during the same period.still another indicator of a field's youth is the fraction of faculty withdegrees awarded in that field. although the percentage of academic doctoralfaculty working in all institutions in computer science or computer engineeringwho also had ph.d.s from computer science or computer engineeringdepartments grew from 29 percent8 human resources253computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.table 8.9 median age of faculty (tenured and not) working in various fields,1977 to 1989medianage (inyears)field1977197919811983198519871989computersciencea38.439.540.340.941.343.343.4electrical andelectronicengineering44.347.046.747.047.248.047.8mathematics39.140.641.943.144.445.546.4physics andastronomy40.943.244.746.347.747.448.5chemistry41.742.443.945.346.047.148.0biologicalsciences40.641.542.443.243.943.644.2all science andengineeringfields41.742.743.444.444.845.446.2note: faculty without doctorates are not included in this tabulation.aexcludes information sciences and computer engineering.source: data from survey of doctoral recipients, office of scientific and engineering personnel,national research council, washington, d.c.table 8.10 distribution (by rank) of faculty for various disciplines, 1979 and 1989percentage with rank19771989disciplineasst. prof.full prof.asst. prof.full prof.computer science andengineering26352830physics17431352mathematics25361951electrical engineering18481956biology26341839all science and engineeringdisciplines24391944note: includes faculty with doctorates working at all academic institutions (except twoyearcolleges), both those that grant ph.d.s and those that do not.source: data from survey of doctorate recipients, office of scientific and engineeringpersonnel, national research council, washington, d.c.8 human resources254computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.to 41 percent between 1977 and 1989 (table 8.11), individuals withdoctorates in other fields still make up more than half of all such cs&e faculty.table 8.11 degree distribution of doctoral faculty working in cs&e, 1977 and1989percentage with doctorate in indicated disciplinediscipline19771989computer science or computerengineering2941electrical engineering2525other engineering2620natural sciences or mathematics127other86total number of faculty withdoctoral degrees from anydiscipline working in the fieldsof computer science or computerengineering18735131source: date from survey of doctorate recipients, office of scientific and engineeringpersonnel, national research council, washington, d.c.with respect to the fraction of their cs&e faculty who have ph.d. degreesin computer science or computer engineering, the ph.d.granting departmentsdiffer sharply from the nonph. d.granting departments (table 8.12). between1977 and 1989, the percentage of cs&e facultytable 8.12 percentage of doctoral cs&e faculty whose doctorate is in cs&e,1977 and 1989percentage with doctorate in cs&etype of school worked at19771989forsythe list schoolsa33(n=1091)b55(n=2660)nonforsythe list schools23(n=782)26(n=2471)a the forsythe list consists of institutions granting doctorates in computer science or computerengineering. the list from 1989 was used for this table.b n, number of faculty working at the indicated type of school in the year given.source: data from survey of doctorate recipients, office of scientific and engineeringpersonnel, national research council, washington, d.c.8 human resources255computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.with ph.d.s in cs&e grew substantially for ph.d.granting institutions, whereasthat percentage barely changed in the nonph. d.granting institutions. recentph.d.s in cs&e who have entered academia in this time frame (and remained inthe field) have gone predominantly to the ph.d.granting institutions.both the number of researchers in cs&e and their output have increasedsubstantially in the last decade. as table 8.13 indicates, the number of academicresearchers in cs&e increased by over a factor of four in the years from 1977 to1989.13 the output of cs&e researchers shows comparable growth. between1970 and 1990, the number of computerrelated articles in the inspec database(a major researchoriented science and technology database) grew by 242 percent(from 19,278 to 65,863); the number of physicsrelated entries in the samedatabase grew by 99 percent (from 70,785 to 141,215); the number of biology andlife sciences articles in a different researchoriented database (biologicalabstracts) grew by 153 percent (from 211,759 to 534,911).finally, the number of ph.d.s teaching in computerrelated fields (includingcomputer science, information science, and computer engineering) alsoincreased, as did the number of degrees awarded in these fields at all levels(undergraduate and graduate) in each year of the period from 1977 to 1989(figure 8.4).14 (in 1977, the number of8 human resources256computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.degrees awarded at all levels in computer and information science was 9255,15and the number of faculty with doctorates teaching in computer science,information science, and computer engineering was 1495.16) however, the growthin the number of degrees awarded far exceeded the growth in the number ofteaching faculty until 1986;figure 8.4 relative growth in the number of degrees at all levels awarded andin the number of individuals with doctorates teaching in these areas, 1977 to1989. 1977=1.0. source: raw data on degrees awarded are presented intable 8.2, and otherwise taken from the same source as that for table 8.2; in1977, the number of degrees awarded at all levels was 9255. degree data reflect"computer and information sciences." comprehensive data on computerengineering degrees at the bachelor's and master's level are not available.however, it is known from the taulbee surveys that at the ph.d.grantinginstitutions, degree production in computer engineering is low compared todegree production in computer science. on the assumption that this trend holdsfor the nonph.d.granting institutions as well, the neglect of computerengineering degrees in this figure does not appear unreasonable. number ofdoctoral teachers includes those teaching in computer science, informationscience, and computer engineering and was obtained from the survey ofdoctorate recipients, office of scientific and engineering personnel, nationalresearch council, washington, d.c.; in 1977, the number was 1495.8 human resources257computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.such data do not account for the large amount of service teaching for nonmajorsthat cs&e departments have provided.figure 8.5 suggests that growth in the number of faculty positions in cs&edid not keep pace with the growth in bachelor's degrees awarded for severalyears, although if current enrollment trends continue a better balance of degreesawarded to number of teaching faculty may be achieved. note, however, that ifthe ratio of bachelor's degrees awarded to number of teaching faculty in 1989 hadmatched the ratio for 1977 (i.e., 6.31 degrees awarded per teaching facultymember), a total of nearly 1200 additional filled teaching positions would havebeen necessary in 1989.clearly, teaching loads in cs&e are much heavier than those in otheracademic fields. more quantitatively, it would take over 11,000 additional facultyteaching in cs&e to achieve the degreestofaculty ratio (2.45 in 1989) thatcharacterizes science and engineering fields across the board.figure 8.5 number of degrees awarded divided by number of individualsteaching for computer science and for all science and engineering fields, 1977 to1989. see source notes for figure 8.4.8 human resources258computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.notes1. the reasons for these discrepancies are unclear, but may include differences in who is asked tosupply data (degree recipients or degree granters), different definitions of categories (e.g., "computerscience" including or not including "information sciences"), different "binning" of the data, differentinstitutions sampled (e.g., u.s. vs north american), and statistical sampling errors. sources onpatterns of employment reflect similar inconsistencies.for purposes of this report, all data issues that involve ph.d. production levels or employmentpatterns draw on data provided by the survey of earned doctorates (sed) and the survey ofdoctorate recipients (sdr) of the office of scientific and engineering personnel (osep) at thenational research council. the reasons for this choice are that osep collects data on ph.d.production in a variety of fields (and thus crossfield comparisons can be presumed to have a measureof consistency in terms of category definition and the like) and that osep also collects a variety ofstatistics related to employment and graduation plans that are not collected by other surveys. the sedtargets all those who received doctorates from u.s. universities in a given year but conducts a fullcensus of this population. the survey of doctorate recipients is conducted to obtain longitudinal dataon employment of individuals with doctorates from u.s. universities over a 42year time span. (thus afigure reported in an sdr survey in 1989 samples from a universe of individuals who received theirdoctorates between 1947 and 1989.) both surveys use selfreported classifications (so that, forexample, the degree recipient is asked to categorize the field in which his or her doctorate isreceived). by contrast, the taulbee surveyšbest known within the cs&e fieldšmakes inquiries ofph.d.granting departments to determine the number of ph.d.s awarded, and it encompasses bothu.s. and canadian institutions.in recognition of a largely inadequate understanding of human resources in the computer field, thecomputer science and telecommunications board and the office of scientific and engineeringpersonnel of the national research council held a workshop in october 1991 to explore issues in theareas of data and taxonomy for computer specialists, demand for and mobility of people trained incs&e, the cs&e pipeline and equality of opportunity, and implications for training. a report on thisworkshop will be released in the summer of 1992.2. a major difficulty in tracking degree production at all levels in cs&e is the long lag time in theavailability of data. even as this report goes to press, 1989 is the most recent year for whichcomprehensive statistics on undergraduate degree production are available. evaluating ph.d.production is somewhat less problematic due to the relatively rapid publication of the annual taulbeesurvey.3. the taulbee surveys report on departmental growth projected five years into the future. but thematch between these projections and the actual number of opportunities available is often poor. moreto the point, the taulbee surveys cover only the 150odd ph.d.granting institutions and not the morethan 850 other cs&e departments in the rest of higher education.4. no office or agency either tracks the employment of nondoctoral cs&e degree holders assystematically as the nrc's office of scientific and engineering personnel tracks employment plansof new ph.d. recipients or has the data to correlate fields of employment with fields of degree.the bureau of labor statistics does develop data in a couple of programs, both of which count"computer programmers" and "systems analysts and computer scientists," and forecasts demand inthese categories, but such categories reflect the job responsibilities of those employed in thosecategories rather than their educational8 human resources259computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.pedigree. at present, these forecasts predict growth in both occupational categories, andrepresentatives from the computer industry who briefed the committee believe that industry will needlarge numbers of computer specialists for years to come.5. david gries and dorothy marsh, "the 1990œ1991 taulbee survey," computing research news,volume 4(1), january 1992, pp. 8 ff.6. mary grigolia, "computer science reentry program," computing research news, volume 2(2), april 1990, p. 19.7. national science board, report of the nsb committee on foreign involvement in u.s. universities,nsb89œ80, national science foundation, washington, d.c., 1989, p. 19.8. national science board, report of the nsb committee on foreign involvement in u.s. universities,1989, p. 8.9. national science board, report of the nsb committee on foreign involvement in u.s. universities,1989, p. 7.10. this point was made at the recent cstb workshop on human resources in cs&e.11. "most of these foreign teachers are men who come from cultures that do not view women ascolleagues. the result can be what american women see as sexual harassment and as refusal to takethem seriously as students." see betty vetter, "demographics of the engineering pipeline,"engineering education, may 1988, pp. 735œ740. cited in national science board, report of the nsbcommittee on foreign involvement in u.s. universities, 1989, p. 8.12. the taulbee survey of 1990œ1991 reports that in the 1990œ1991 academic year, the 137 ph.d.granting computer science departments (with an average of 19.9 faculty members) had 35 deaths andretirements. in the steady state, a department with a faculty of 20 and an average faculty work life of40 years (from age 30 to age 70) could expect to see, on average, about one retirement every twoyears, so that in a field of 137 departments, one could expect about 67 retirements each year. seedavid gries and dorothy marsh, "the 1990œ1991 taulbee survey," computing research news,volume 4(1), january 1992, p. 12.13. "academic researchers" are defined as doctorate holders working in cs&e as employees of aninstitution of higher education (but not twoyear colleges) who indicate that their primary orsecondary work is basic or applied research. thus academic researchers include both faculty withresearch and teaching responsibilities and other academic scientists with only researchresponsibilities.14. 1977 was chosen because it is the first year for which reasonably authoritative data on number ofteaching faculty (grouping together all professorial ranks, instructors, lecturers, adjuncts, and so on)are available.15. national science foundation, science and engineering indicators, 1989, nsf, washington, d.c.,1989, table 212, p. 224.16. survey of doctorate recipients, office of scientific and engineering personnel, nationalresearch council, unpublished data.8 human resources260computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.appendixcontributors to computing the futuremark abbott, oregon state universityhal abelson, massachusetts institute of technologyrichard adrion, university of massachusetts at amherstsudhir aggarwal, state university of new york at binghamtonashok agrawala, university of marylanddonald allison, virginia polytechnic institutegregory andrews, university of arizonadean arden, state university of new york at albanyjohn armstrong, ibm corporationlinda ashcraft, sra corporationc. gordon bell, me ltd.susan berkowitz, westat, inc.joel birnbaum, hewlettpackardbarry boehm, defense advanced research projects agencyelaine bond, chase manhattan bankrobert borchers, lawrence livermore national laboratoryfiona branton, computer systems policy projectjoseph bredekamp, national aeronautics and space administrationfrederick brooks, jr., university of north carolina at chapel hillj.c. browne, university of texas at austincharles brownstein, national science foundationwilliam buzbee, national center for atmospheric researchjohn cavallini, department of energyappendix261computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.eric clemons, university of pennsylvaniajacques cohen, brandeis universityrichard conway, cornell universitydonald coughlin, don coughlin and companyjerome cox, washington universitydarl davidson, electronic data systemsjack demember, digital equipment corporationallan ditchfield, mcirichard dubois, national institutes of health, national center for researchresourceskaren duncan, association for computing machinerysara durand, bank of americacharles dyer, university of wisconsin at madisonpatrick dymond, university of california at san diegorobert elmore, arthur andersen & companydomenico ferrari, university of california at berkeleygideon frieder, syracuse universityjohn gage, sun microsystems, inc.thomas gannon, digital equipment corporationgeorge hedrick, oklahoma state universityrussell hobbie, university of minnesotalee holcomb, national aeronautics and space administrationcharles holland, air force office of scientific researchjohn hopcroft, cornell universityrichard ivanetich, institute of defense analysisanita jones, university of virginiarobert kahn, corporation for national research initiativesmalvin kalos, cornell universitysidney karin, san diego supercomputer centerkenneth kay, computer systems policy projectjeffrey kennington, southern methodist universityellen knapp, coopers & lybrandjoseph lambert, pennsylvania state universitylawrence landweber, university of wisconsin at madisonduncan lawrie, university of illinois at urbanachampaignpeter lax, new york universitymichael levine, carnegie mellon universitydaniel lewis, santa clara universitystuart madnick, massachusetts institute of technologykurt maly, old dominion universitywilliam marcy, texas technical universitymanton matthews, university of south carolinaappendix262computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.raymond miller, university of marylandharlan mills, software engineering technology, inc.david nagel, apple computer, inc.michael nelson, committee on commerce, science and transportation,subcommittee on science, technology, and space, u.s. senatenorine noonan, office of management and budgetrodney oldehoeft, colorado state universityjoseph pasquale, university of california at san diegojesse poore, university of tennessee at knoxvilleedward prell, at&tfranklin prosser, indiana universitytodd qunito, tufts universitygeorge radin, ibm t.j. watson research centerdavid reed, lotus development corporationjuris reinfelds, new mexico state universityrobert reynolds, wayne state universityrobert roe, boeing aerospace & electronicssteven rosenberg, hewlettpackardjohn savage, brown universityfred schneider, cornell universitystephen seidman, auburn universitybruce shriver, ieee computer societyjanos simon, university of chicagobarbara simons, ibm almaden research centerirwin sitkin, aetna insurance company (retired)larry smarr, university of illinois at urbanachampaignbrian smith, university of new mexicoolaf stackelberg, kent state universitydevika subramanian, cornell universityrobert sugar, university of california at santa barbararichard swan, digital equipment corporationsatish tripathi, university of maryland at college parka. joseph turner, clemson universityjeffrey ullman, stanford universityandre van tilborg, office of naval researchfrederick weingarten, computing research associationmark weiser, xerox palo alto research centerstephen weiss, university of north carolina at chapel hillbarry whalen, conductus, inc.john white, association for computing machinerypeter will, hewlettpackardappendix263computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.james wilson, committee on science, space, and technology,subcommittee on science, research, and technology, u.s. house ofrepresentativesshmuel winograd, ibm t.j. watson research centereugene wong, office of science and technology policyhelen wood, national oceanic and atmospheric administrationwilliam wulf, university of virginiapaul young, university of washingtonjoel yudken, rutgers universityappendix264computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.indexaabstractions in computer science andengineeringexplanation of, 169functioning of, 172use of, 171, 172173accountability for research, 48acmcra strategic directions conference (1989), x, 64advanced technology program (atp),230air force office of scientific research(afosr), 219220algorithmsfor computational biology, 73, 74computational complexity and, 198199meaning and use of, 1920, 22, 64, 127,164patentability of, 44study of, 23, 59, 168, 197198use of, 193œ197applicationsoriented research. see interdisciplinary and applicationsoriented researchapplied research. see researcharmy research office (aro), 219220artificial intelligence (ai)explanation of, 199200future of, 204impact on scientific thought, 202204impact on society, 200201medical and biological applications for,230nasa support for, 227association for computing machinery(acm)activities of, 235, 236view of broadening of cs&e, 60, 61, 64bbasic research. see researchbloch, erich, 222223broadening of computer science and technologyacm views regarding, 60, 61, 64committee views on, 56, 65, 141conclusions regarding, 90educational changes and, 8586index265computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.need for, 5561, 123124prerequisites for, 8789research opportunities in, 6467role of educational institutions in, 38,8687, 150brooks, fred, 21, 165brown, john seely, 17bruno, john, 196business sector.see also commercial computingattitude of academic cs&e communitytoward, 6263interaction between universities and,8687investment in continuing education by,134135cchandy, k. mani, 74chip designadvances in, 176178limits on, 178, 179commercial computingcollaborative work in, 8183concerns that motivate databaseresearch, 109110cs&e and, 7677interoperation technology for, 7981model management and decision support for, 7778software development metrics and modeling for, 79usable software for, 7879committee to assess the scope and direction of computer science and technology, xareas of concern for future reports identified by, 155157conclusions drawn by, 9, 157158interoperation difficulties experiencedby, 79, 80judgments made by, 45priorities formulated by, 5, 114, 139142recommendations to federal policy makers regarding education, 8,151153to federal policy makers regardingresearch, 67, 143146to universities regarding education, 89,153155to universities regarding research, 78,146, 148151study made by, 1819views on broadening of field, 56, 65, 141views on education, 117, 121, 125, 130,133134, 141142compilersadvances in, 97, 189190explanation of, 96, 170complexity theory, 26, 95, 178, 179,198199computational biology, 7276computational complexity. see complexity theorycomputational science, 62computer architecture and developmentof compiler technology, 96, 97elements involved in, 2223highspeed, 178computer graphicsadvances in, 204205, 211history of, 206207intellectual challenges in, 209212for scientific and engineering community, 207208computer hardware. see hardwarecomputer industrychanges in, 3, 4547, 5960cs&e areas of interest to, 98relationship between cs&e and, 3845role in broadening research agenda, 86view of educational needs, 116117computer languagesadvances in, 187189development of more effective, 2526elements involved in design of, 23highlevel, 172list of significant, 188index266computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.computer networksgigabit, 101103research agenda for, 101103computer professionals for socialresponsibility, 237computer programming.see also software engineeringdiscussion of, 2021, 6364, 165167rigor and clarity in, 121123study of, 2324computer science: achievements andopportunities (hopcroft andkennedy) , 97, 174research agenda on parallelism, 99, 100research agenda on software engineering, 103, 104computer science accreditation board,236computer science and engineering (cs&e)as academic discipline, 87background information regarding, 13,1924, 6064, 163168broader agenda for. see broadening ofcomputer science and technologychanging environment for academic,4548contributions to computing practice by,2426core of, 4, 139140discussion of field of, viiviii, 163168foreign students in, 251253groups to benefit from assessment of,viiiixintellectual and structural characteristicsof, 213214as laboratory discipline, 78, 150151minorities and women in, 154155previous studies in field of, ixxpriorities for, 5, 114, 139142professional organizations for, 235237subdisciplines of, 4, 22, 95, 96, 167sustaining core effort in, 5, 139141synergy leading to innovations in,212213universityindustrycommerce interaction in, 8687, 150youth and growth of, 253258computer science and engineering(cs&e) accomplishments, 174.see also individual listingin algorithms, 193198in artificial intelligence, 199204in compilers, 189190in computational complexity, 198199in computer graphics and user interface,204212in data communications and networking,183185in database systems, 185187in microelectronics, 175178in operating systems, 180183in processor and memory design, 178180in programming languages, 187189in software engineering, 190193in systems and architecture, 175187computer science and telecommunications board (cstb), ix, vii, x, 63,235, 237computer software. see software;software engineering;software industrycomputer systems policy project (cspp),147computingbenefits and drawbacks of, 2628conclusions regarding, 49cs&e and commercial, 7681distributed, 100101realtime, 193speedup in, 25uses in society for, 1318computing research association (cra),235, 237coons, steven, 206council on competitiveness, 131, 132index267computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.ddata communications.see also computer networksadvances in, 183185research agenda for, 101103database model, relational, 2425, 185, 186database research, 108110database systems, 170, 185187defense advanced research projectsagency (darpa), 112role in support of cs&e research,3334, 218220defense department. see department ofdefensedegree productiondata regarding undergraduate and graduate, 239246, 251involvement of foreign students in,251253women and nonasian minorities and,246247department of defense (dod)involvement of in support of cs&eresearch, 217221office of the secretary of defense, 219,221software engineering institute sponsored by, 131support for cs&e research by, 2931, 33department of energy (doe), 29, 34,228229developmentdistinctions between basic and appliedresearch and, 65researchers' view of, 4041discovery tools, 191distributed computing, 100101downey, peter, 196dozier, jeff, 196eearth observing system (eos) (nasa),70earth sciences and environmental studies,6972, 196educationbroadening agenda for cs&e, 8586committee recommendations regarding,89, 151155committee views regarding, 117, 121,125, 130134, 141142continuing, 133135, 151153, 156different views of, 116118involvement of senior researchers in, 145master's degree cs&e, 130131ph.d. cs&e, 119, 131133, 153154,232233, 239, 240precollege (high school) cs&e,135136, 156157summary and conclusions regarding, 136teaching loads for cs&e faculty, 127,257258undergraduate cs&e, 118125, 154, 236.see also undergraduate educationundergraduate service, 125130educational degrees. see degree productioneducational institutions.see also facultychanges in environment in, 4748and continuing education, 134cost of computing technology for, 48,119120, 155156, 233234education recommendations for, 89,153155and interaction with industry and commerce to produce broader agenda ,38, 8687, 150need for reorientation of, 8788research recommendations for, 78, 146,148151role of cs&e within, 231235educators. see facultyelectronic library, 81, 8385employmentexpectations for, 133opportunities for cs&e graduates, 87index268computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.energy department. see department ofenergyengelbart, douglas, 206environmental protection agency (epa),34eos data and information system (eosdis), 7072expert systems, 201, 202.see also artificial intelligenceffacultyage, degrees, and rank of, 253258expansion of hpcc to address needs of,151overview of cs&e, 118, 119recognition and rewards for, 154support for interdisciplinary or applicationsoriented research among , 150teaching loads for, 127, 257258women and nonasianminorities represented in, 247248federal coordinating council for science,engineering, and technology (fccset), 34, 146federal governmentcompact between university researchand, 48relationship between cs&e researchand, 2838federal policy makerseducation recommendations to, 8,151153research recommendations to, 67,143146foreign students, 251253fundingamounts per academic researcher, 3435broadening of agenda to gain nontraditional, 60critical nature of, 23encouragement for faculty to seek out,150by federal agencies, 2931, 60, 217230.see also individual agenciesfor hcpp program, 36, 37, 60impact of fiscal restraints on, 45ggesture recognition, 208209global change, 7072graphics. see computer graphicsgreenblatt, richard, 203hhardwareadvances in, 210components of, 169170health carecomputer applications for, 14physician's assistant for intensive caremonitoring, 67, 68highperformance computing and communications (hpcc) program, 67,223areas of theoretical research relevant to,56committee recommendations regarding,3, 143144, 146, 151153cspp position on, 147description of, 3436features and focus of, 3738, 100, 102funding of, 3637impact on cs&e research of, 29origin of, 64high school computer science education,135136, 156157hopcroftkennedy study. see computerscience: achievements and opportunities (hopcroft and kennedy)humancomputer interaction, 111, 112iinformation representationselection of appropriate, 19, 20, 164165index269computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.study of, 24information storage, 107110intellectual propertyissues regarding, 4344protection of, 150interdisciplinary and applicationsorientedresearchin commercial computing, 7683committee recommendations regarding,144146, 148, 149in computational biology, 7276in earth sciences and environment, 6972in electronic library, 81, 8385faculty support for, 150funding for, 88, 144146importance of, 63, 64at science and technology centers, 226interdisciplinary courses, 154kkapor, mitchell, 7879kay, alan, 97knuth, donald, 1920, 164llagunita report, 107108languages. see computer languageslewis, edward, 74linear programming (lp) algorithms,195, 196, 198mmathematicscomparison of cs&e to, 57contributions of cs&e to, 58, 59influences on development of, 56, 57mccarthy, john, 200microelectronics, 175178minoritiesin cs&e, 154156, 246247in faculty positions, 247248minsky, marvin, 200mission to planet earth, 228morgan, h.e., 8788multiple processors, 97, 100101nnational aeronautics and space administration (nasa), 29, 34, 70, 227228the national challenge in computer science and technology (cstb), ix, 97,103national institute of standards and technology (nist), 34, 145, 230national institutes of health (nih), 34, 230national oceanic and atmosphericadministration (noaa), 34national research and education network (nren), 3435, 103, 185national science foundation (nsf) cisedirectorate, 222224coordinated experimental research(cer) program, 234235funding for hpcc program by, 36institutional infrastructure program, 235science and technology centers of, 226supercomputer centers of, 225226support for cs&e research by, 29,3234, 221225, 234support for nsfnet, 225, 226national security agency (nsa), 219, 221networks. see computer networksnewell, allen, 200oobjectoriented programming, 124office of naval research (onr), 219220office of science and technology policy(ostp), 34operating systemsadvances in, 180183index270computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.development of, 26software, 170171pparallel computingas component of hpcc program, 100parallel processing, 96parallelism, 99, 124progress in, 96research agenda for, 97, 99science and technology center for, 226in undergraduate programs, 120, 124prioritiesto broaden field, 5, 141to improve undergraduate education, 5,141142setting of, for the future, 113114, 139to sustain core effort, 5, 139141processorsadvances in, 178180capabilities of, 97, 100101as component of computer hardware, 169design of, 178professional organizations, 235237programming. see computer programmingprogramming languages. see computerlanguagesproject control systems, 191rrealtime computing, 193reducedinstructionset computing(risc), 38, 96, 97, 179relational database model, 2425, 185, 186reliability, 110111research.see also interdisciplinary and applicationsoriented researchacademic vs. industrial, 3840basic vs. applied, 29, 65, 67broader definition of, 8889computer industry involvement in, 3845interaction between computing practiceand, 5, 5556in nonph.d.granting institutions, 119overview of federal support for, 2838recommendations regarding, 68,143151relationship to development, 65role of, 2research agendafor data communications and networking, 101103formulation of broader, 6468, 8689for future, 9598for information storage and management, 107110regarding processor capabilities andmultipleprocessor systems,97 ,99101for reliability, 110111for software engineering, 103107summary and conclusions regarding,113114for user interface, 111113research funding. see fundingresearchersacademic vs. industrial, 3840growth in number of active, 34, 256need to engage with applications, 61production of doctorallevel, 47understanding of development, 4041sscaling up: a research agenda for software engineering (cstb), 97, 103scientific visualization, 207208, 211simon, herbert, 200softwarecost of, in assessment of capitalization,233intellectual property issues regarding, 44index271computing the future: a broader agenda for computer science and engineeringcopyright national academy of sciences. all rights reserved.operating system, 170171osd action plan for, 221reengineering of existing, 103105reuse of, 190, 192testing of, 98, 107transfer by network, 226undergraduate approach to problems in,121usability of, 7879software engineeringadvances in, 190193continuing education in, 134difficulty of largescale, 106107research agenda for, 103105rigor and clarity in, 121, 122software industry, 17, 18source code control systems, 191192speech recognition, 112, 203204sutherland, ivan, 21, 206ttaulbee surveys, ix, 232, 237, 247technology transferissues posed by, 4143theoretical researchinteraction between computing practiceand, 5, 5556meaning of, 194theory, 194traveling salesman problem (tsp),196197turing, alan, 172, 197, 200uundergraduate education.see also educationachievement of breadth in, 123124limits of fouryear, 125mathematics and formalism in, 122123for noncs&e majors, 125130, 154priorities to improve, 5, 141142rigor and clarity in, 120122role of acm in, 236variability in, 118120, 136universities. see educational institutionsu.s. global change research program,70, 72user interfacesadvances in, 205212explanation of, 170gestures in, 208209research agenda for, 111113vvisualization. see scientific visualizationvon neumann, john, 57, 220wwhite, robert m., 58wimp interfaces, 205206, 208womenamong foreign students, 253in cs&e, 154156, 246249degrees awarded to, 247, 249in faculty positions, 247248participation barriers in science andengineering for, 248250, 253index272
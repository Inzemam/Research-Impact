detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/10784the future of supercomputing: an interim report58 pages | 8.5 x 11 | paperbackisbn 9780309089951 | doi 10.17226/10784committee on the future of supercomputing; computer science andtelecommunications board; division on engineering and physical sciences;national research councilthe future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved.         committee on the future of supercomputing  computer science and telecommunications board  division on engineering and physical sciences    the national academies press washington, d.c. www.nap.edu  the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved. the national academies press 500 fifth street, n.w. washington, dc 20001  notice: the project that is the subject of this report was approved by the governing board of the national research council, whose members are drawn from the councils of the national academy of sciences, the national academy of engineering, and the institute of medicine. the members of the committee responsible for the report were chosen for their special competences and with regard for appropriate balance.  support for this project was provided by the department of energy. any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsor.  international standard book number 0309089956 (book) international standard book number 0309526752 (pdf)  cover designed by jennifer m. bishop.  copies of this report are available from the national academies press, 500 fifth street, n.w., lockbox 285, washington, dc 20055, (800) 6246242 or (202) 3343313 in the washington metropolitan area. internet, http://www.nap.edu  copyright 2003 by the national academy of sciences. all rights reserved. printed in the united states of america  the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved. the national academy of sciences is a private, nonprofit, selfperpetuating society of distinguished scholars engaged in scientific and engineering research, dedicated to the furtherance of science and technology and to their use for the general welfare. upon the authority of the charter granted to it by the congress in 1863, the academy has a mandate that requires it to advise the federal government on scientific and technical matters. dr. bruce m. alberts is president of the national academy of sciences.  the national academy of engineering was established in 1964, under the charter of the national academy of sciences, as a parallel organization of outstanding engineers. it is autonomous in its administration and in the selection of its members, sharing with the national academy of sciences the responsibility for advising the federal government. the national academy of engineering also sponsors engineering programs aimed at meeting national needs, encourages education and research, and recognizes the superior achievements of engineers. dr. wm. a. wulf is president of the national academy of engineering.  the institute of medicine was established in 1970 by the national academy of sciences to secure the services of eminent members of appropriate professions in the examination of policy matters pertaining to the health of the public. the institute acts under the responsibility given to the national academy of sciences by its congressional charter to be an adviser to the federal government and, upon its own initiative, to identify issues of medical care, research, and education. dr. harvey v. fineberg is president of the institute of medicine.  the national research council was organized by the national academy of sciences in 1916 to associate the broad community of science and technology with the academy™s purposes of furthering knowledge and advising the federal government. functioning in accordance with general policies determined by the academy, the council has become the principal operating agency of both the national academy of sciences and the national academy of engineering in providing services to the government, the public, and the scientific and engineering communities. the council is administered jointly by both academies and the institute of medicine. dr. bruce m. alberts and dr. wm. a. wulf are chair and vice chair, respectively, of the national research council.  www.nationalacademies.org the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved. iv committee on the future of supercomputing susan l. graham, university of california, berkeley, cochair marc snir, university of illinois at urbanachampaign, cochair william j. dally, stanford university james demmel, university of california, berkeley jack j. dongarra, university of tennessee, knoxville kenneth s. flamm, university of texas at austin mary jane irwin, pennsylvania state university charles koelbel, rice university butler w. lampson, microsoft corporation robert lucas, university of southern california paul c. messina, argonne national laboratory (parttime) jeffrey perloff, university of california, berkeley william h. press, los alamos national laboratory albert j. semtner, naval postgraduate school scott stern, kellogg school of management, northwestern university shankar subramaniam, university of california, san diego lawrence c. tarbell, jr., eagle alliance steven j. wallach, chiaro networks  staff cynthia a. patterson, study director and program officer phil hilliard, research associate margaret marsh huynh, senior project assistant  the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved. v computer science and telecommunications board  david d. clark, massachusetts institute of technology, chair eric benhamou, 3com corporation elaine cohen, university of utah thomas e. darcie, university of victoria mark e. dean, ibm joseph farrell, university of california, berkeley joan feigenbaum, yale university hector garciamolina, stanford university randy h. katz, university of california, berkeley wendy a. kellogg, ibm t.j. watson research center sara kiesler, carnegie mellon university butler w. lampson, microsoft corporation, cstb member emeritus david liddle, u.s. venture partners teresa h. meng, stanford university tom m. mitchell, carnegie mellon university daniel pike, gci cable and entertainment eric schmidt, google inc. fred b. schneider, cornell university burton smith, cray inc. william stead, vanderbilt university andrew j. viterbi, viterbi group, llc jeannette m. wing, carnegie mellon university  alan s. inouye, interim director jon eisenberg, interim deputy director kristen batch, research associate jennifer m. bishop, senior project assistant janet briscoe, administrative officer david drake, senior project assistant renee hawkins, financial associate phil hilliard, research associate margaret marsh huynh, senior project assistant herbert s. lin, senior scientist lynette i. millett, program officer david padgham, research associate cynthia a. patterson, program officer janice sabuda, senior project assistant brandye williams, staff assistant steven woo, dissemination officer  for more information on cstb, see its web site at <http://www.cstb.org>, write to cstb, national research council, 500 fifth street, n.w., washington, dc 20001, call at (202) 3342605, or email the cstb at cstb@nas.edu.the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved. the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved. vii preface       highperformance computing is important in solving many kinds of complex problems in domains from weather science and biology to national security.  u.s. government spending on supercomputing has been relatively flat over the last 10 years and has declined compared to industrial or commercial purchases of highperformance systems.1 some observers associate the trends with the end of the cold war, during which both national security and scientific research needs were believed to justify spending on a range of highperformance computing programs. others point to the influence of the changing marketplace for computing systems overall, which has seen an increase in demand for less expensive systems of less than maximal performance. several factors have led to the recent interest in reexamining the rationale for federal investment in research and development in support of highperformance computing, including (1) continuing changes in various component technologies and their markets, (2) the evolution of the computing market (and particularly the highend supercomputing segment), (3) experience with several systems using the clustered processor architecture, and (4) the evolution of the problems, many of them missiondriven, for which supercomputers are used. the department of energy™s (doe™s) office of science expressed an interest in sponsoring a study by the computer science and telecommunications board (cstb) of the national research council that would assess the state of u.s. supercomputing capabilities and relevant research and development. spurred by the development of the japanese vectorbased earth simulator supercomputer, the senate™s energy and water development appropriations committee directed the advanced simulation and computing (asc) program of the national nuclear security administration (nnsa) at doe to commission a study (in collaboration with doe™s office of science) by the national research council. congress also commissioned a study by the jasons2 to identify the distinct requirements of the stockpile stewardship program and its relation to the asc acquisition strategy. cstb convened the committee on the future of supercomputing to assess prospects for supercomputing technology research and development in support of u.s. needs, to examine key elements of contextšthe history of supercomputing, the erosion of research investment, the changing nature of problems demanding supercomputing, and the needs of government agencies for supercomputing capabilitiesšand to assess options for progress. the committee has been tasked with preparing two  1debra goldfarb, idc™s hpc industry analyst, presentation to the committee on may 23, 2003. 2the jasons, formed in 1959, are a select group of scientific advisors who consult with the federal government chiefly on classified research issues. for more on the jasons, see ron southwick, 2002, elite panel of academics wins fight to continue advising military, the chronicle of higher education, june 7. available at <www.globalsecurity.org/org/news/2002/020607jason.htm>. the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved. viii reportsša brief interim report and a final indepth report. this interim report is intended only to establish contextšthe history and current state of supercomputing, application requirements, technology evolution, the socioeconomic context, and so onšand to identify some of the issues that may be explored in more depth in the second phase of the study. in order to provide feedback as soon as possible, this interim report has been developed on a very tight time line. it is based on committee deliberations and briefings received from numerous experts at two committee meetings.3 it does not contain formal conclusions or recommendations. the committee expects that its understanding of some background issues will change as it collects more data and deepens its analysis in the final report, anticipated in late 2004. the committee thanks the many individuals who contributed to its work. the people who briefed the committee at one of the plenary meetings are listed in appendix c. their willingness to answer our questions was most helpful. stanford university, with the able local support of pamela elliott and charles m. orgish, hosted the second plenary meeting of the committee. the sponsors of the report at the department of energyšdaniel hitchcock, fred johnson, josé l. muñoz, dimitri kusnezov, and hans ruppelšhave been most supportive and responsive in helping the committee to do its work. the reviewers of the draft report provided insightful and constructive comments that contributed significantly to the clarity of the report. the work of the committee was made considerably easier because of the participation of excellent nrc staff members. marjory blumenthal, the outgoing director of cstb, shared her wisdom and knowledge with the committee until she left the nrc in june. she will be greatly missed by all of us who have worked with her on this study and in earlier cstb activities. jon eisenberg, herb lin, and richard rowberg have given valuable counsel to the chairs. margaret marsh huynh is providing firstrate administrative and logistical support to the committee.  liz fikre, the report editor, has taught us much about clear exposition. phil hilliard has willingly and competently given research support to the committee. finally, cynthia patterson, the study director, has been an outstanding partner and mentor to the chairs. her contributions have strengthened both the study and the interim report.  susan l. graham and marc snir, cochairs committee on the future of supercomputing   3the speakers are listed in appendix c. the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved. ix acknowledgment of reviewers       this report has been reviewed in draft form by individuals chosen for their diverse perspectives and technical expertise, in accordance with procedures approved by the national research council™s (nrc™s) report review committee. the purpose of this independent review is to provide candid and critical comments that will assist the institution in making the published report as sound as possible and to ensure that the report meets institutional standards for objectivity, evidence, and responsiveness to the study charge. the review comments and draft manuscript remain confidential to protect the integrity of the deliberative process. we wish to thank the following individuals for their review of this report: roy radner, new york university, ahmed h. sameh, purdue university, charles l. seitz, myricom, inc., allan snavely, san diego supercomputing center, francis sullivan, ida center for computing sciences, and paul r. woodward, university of minnesota.  although the reviewers listed above have provided many constructive comments and suggestions, they were not asked to endorse the conclusions or recommendations, nor did they see the final draft of the report before its release. the review of this report was overseen by samuel h. fuller, analog devices, inc. appointed by the national research council, he was responsible for making certain that an independent examination of this report was carried out in accordance with institutional procedures and that all review comments were carefully considered.  responsibility for the final content of this report rests entirely with the authoring committee and the institution.the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved. the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved. xi contents       executive summary 1  1 introduction 5  study context, 6  about this interim report, 8  2 supercomputing past and present 9  previous reports and recent federal initiatives, 9  supercomputing technology, 13  vendors, 13  architecture, 13  products, 15  the nec earth simulator, 16  software, 17  algorithms, 17  3 continuity and predictability 18  important work is getting done, 18  no nearterm alternatives, 19  older architectures coexist with new ones, 19  the importance and continuing value of software research  and algorithm development, 20  legacy codes cannot be abandoned until they are replaced, 21  uncertainty and inconsistent policies can be expensive, 21 4 future supercomputing and research 22  innovation in highend computing, 22  architecture research, 23  software research, 24  research on applications and algorithms, 25  the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved.xii contents xii 5 the role of government in supercomputing 28  government as a leading customer, 28  national security implications, 29  market forces, 29  6 conclusion 31    appendixes a committee member and staff biographies 35 b acronyms 43 c briefers to the committee 45  what is cstb? 46 the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved. 1 executive summary       by definition, supercomputers are those hardware and software computing systems that provide close to the best currently achievable sustained performance. the performance of supercomputers, which is normally achieved by introducing parallelism, is typically much better than that of the vast majority of installed systems. supercomputers are used to solve complex problems, including the simulation and modeling of physical phenomena such as climate change, explosions, or the behavior of molecules; the analysis of data from sources such as national security intelligence, genome sequencing, or astronomical observations; or the intricate design of engineered products. their use is important for national security and defense, as well as for research and development in science and engineering. as the uses of computing have increased and broadened, supercomputing has become less dominant than it once was. many interesting applications require only modest amounts of computing, by today™s standards. because of the increase in computer processing power, many problems whose solution once required supercomputers can now be solved on relatively inexpensive desktop systems. that change has caused the computer industry, the research and development community, and some government agencies to reduce their attention to supercomputing. yet problems remain whose computational demands for scaling and timeliness stress even our current supercomputers. many of those problems are fundamental to the government™s ability to address important national issues. one notable example is the department of energy™s computational requirements for nuclear stockpile stewardship. the government has sponsored studies of a variety of supercomputing topics over the years. some of those studies are summarized in the body of this report. recently, questions have been raised about the best ways for the government to ensure that its supercomputing needs will continue to be satisfied in terms of both capability and costeffectiveness. to answer those questions, the national research council™s computer science and telecommunications board convened the committee on the future of supercomputing to conduct a 2year study to assess the state of supercomputing in the united states and to give recommendations for government policy to meet future needs. this study is sponsored jointly by the department of energy™s office of science and by its advanced simulation and computing (asc) program. this interim report, presented approximately 6 months after the start of the study, reflects the committee™s current understanding of the state of u.s. supercomputing today, the needs of the future, and the factors that contribute to meeting those needs. after such a short time, the committee is not yet ready to comment in detail on the specifics of existing supercomputing programs or to present specific findings and wellsupported recommendations. although the committee has made considerable progress in understanding the current state of supercomputing and how it got there, it still has much more work to do before it develops recommendations. the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved.2 the future of supercomputing: an interim report  many technical, economic, and policy issues need to be addressed in this study. they include (1) the computational needs of present and future applications and approaches to satisfying them; (2) the balancing of commodity components and custom design in supercomputer architectures and the effects of software design improvements and industry directions on that balance; (3) the interplay of research, development, prototyping, and production in creating innovative advances; (4) the extent and nature of direct government involvement to ensure that its needs are met; and (5) the important requirement that the present not be neglected while the future is being determined. although this report touches on each of those topics, the committee has not completed its consideration of them. the particular technical approaches of any program that develops or uses supercomputing represent a complex compromise between conflicting requirements and an assessment of risks and opportunities entailed in various approaches. an evaluation of these approaches requires a detailed understanding of (1) the relevant applications, (2) the algorithms used to solve those application problems, (3) the performance likely to be achieved by codes that implement these algorithms on different platforms, (4) the coding efforts required by various approaches, (5) the likely evolution of supercomputing technology over multiple years under various scenarios, and (6) the costs, probabilities, and risks associated with different approaches. in its final report, the committee will seek to characterize broadly the requirements of different application classes and to examine the architecture, software, algorithm, and cost challenges and tradeoffs associated with these application classesškeeping in mind the needs of the nuclear stockpile stewardship program, the broad science community, and the national security community. (note that a separate, classified report by the jasons is expected to identify the distinct requirements of the stockpile stewardship program and its relation to the asc acquisition strategy.) the committee believes that it would be unwise to significantly redirect or reorient current supercomputing programs before careful scientific consideration has been given to the issues described above. such changes might be hard to reverse, might reduce flexibility, and might increase costs in the future. in the period ahead, the committee will continue to learn and to analyze. a workshop focused on applications, to be held in the fall of 2003, will include a number of applications experts from outside the committee. its purpose will be to identify both the computational requirements of important applications and the opportunities to adapt and evolve current solutions so as to benefit from advances in algorithms, architectures, and software. in addition, the committee will meet with experts who are developing solutions for applications of particular importance for national defense and security within the department of energy (doe) and the national security agency (nsa). the committee will also meet with managers of supercomputing facilities, procurement experts, industrial supercomputer suppliers, experts on computing markets and economics, and others whose expertise will help to inform it.   supercomputing today according to the june 2003 list of the 500 most powerful computer systems in the world, the united states leads the world in the manufacture and use of supercomputers, followed by japan.1 a number of other countries make or use supercomputers, but to a much lesser degree. virtually all supercomputers are constructed by connecting large numbers of compute nodes, each having one or more processors and a common memory, by an interconnect network (a switch). supercomputer architectures differ in the design of their nodes, their switches, and the nodeswitch interfaces. higher node performance is achieved by using commercial scalar microprocessors with 64bit data paths intended primarily for commercial servers or nodes designed specially for supercomputing, rather than the highvolume, 32bit scalar microprocessors used in workstations and lower capability cluster systems. the custom nodes tend to use special mechanisms such as vectors or multithreading to reduce memory latency rather than relying solely on the more limited latency avoidance afforded by caches. highbandwidth, scalable interconnects are typically custombuilt and more expensive than the   1the top500 list is available at <www.top500.org>. the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved.executive summary 3 more widespread ethernet interconnects. custom switch use is often augmented by custom node/switch interfaces. the highestranked system in the top500 list is the japanesebuilt earth simulator, released in the spring of 2002 and designed specifically to support geoscience applications. that system has custom multiprocessor vectorbased nodes and a custom interconnect. the emergence of that system has been fueling recent concerns about continued u.s. leadership in supercomputing. the system software that is used on most contemporary supercomputers is some variant of unix, either open source or proprietary. programs are written in fortran, c, and c++ and use a few standard application libraries. all of these supercomputers use implementations of the message passing interface (mpi) standard to support messagepassingstyle internode communication. relatively little standardization exists for other aspects of software environments and tools.  evolution in supercomputing  a major policy issue for supercomputing is the proper balance between investments that exploit and evolve current supercomputing architectures and software (the evolutionary aspect) and investments in alternative approaches that may lead to a paradigm shift (the innovative aspect). both aspects are important. at this stage in the study, the committee sees the following advantages for an evolutionary approach to investment and acquisition. first, much useful work is getting done using the existing systems, and their natural successors can be expected to continue that work. in addition, there are no obvious nearterm architectural alternatives: the promising technology breakthroughs, such as processorinmemory, streaming architectures, and the like, that might revolutionize supercomputing are far off in the future and less than certain. of course, higher capability would enable better solutions to be obtained faster. but history suggests that even when revolutionary advances come along, they do not immediately supplant existing architectures. different problems benefit from different architectures and no one design is universally best. the committee sees a need for evolutionary investments in all major approaches to supercomputing that are currently pursued: clusters built entirely of commodity components; scalable systems that reuse commodity microprocessors together with custom technology in the interconnect or the interconnect interface; and systems in which microprocessors, the interconnect, and their interface are all customized. although some advantages also accrue from evolutions in software, the committee sees a need for investment that would accelerate that evolution. the advantages of commodity architectural components might be more easily realized if more applications were redesigned, better custom software was provided, and the cost of bringing software to maturity was better appreciated. the benefit of software investment is that it tends to have continuing value as architectures evolve. at the same time, both the maintenance and the evolution of legacy applications must be anticipated and supported. finally, the committee observes that uncertainties in policy and inconsistencies over time can be both disruptive and expensive. unexpected pauses in an acquisition plan or failure to maintain a diversity of suppliers and products can cause both the suppliers and the skilled workforce to divert their attention from supercomputing. it can be difficult and expensive to recover the supply of expertise.   innovation for supercomputing  innovation in supercomputing stems from applicationmotivated research that leads to experimentation and prototyping, to advanced development and testbeds, and to deployment and products. all the stages along that path need continuous, sustained investment in order that the needs of the future will be met. if basic research activities are not supported, revolutionary advances are less the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved.4 the future of supercomputing: an interim report  likely; if experimentation and advanced development are not done, the promising approaches never ripen into products. in supercomputing, innovation is important in architecture, in software, in algorithms, and in application strategies and solution methods. the coupling of these aspects is equally important. major architecture challenges stem from the uneven performance scaling of different components. in particular, as the gap between processor speeds, memory bandwidth, and memory and network latency increases, new ideas are needed to increase bandwidth and hide (tolerate) latency. additionally, as new mechanisms are introduced to address those issues, there is a need for ways to supply a stable software interface that facilitates exploiting hardware performance improvements while hiding the changes in mechanism. the need for software innovation is motivated by its role as an intermediary between the application (the problem being addressed) and the architectural platform. innovation is needed in the ways that system software manages the use of hardware resources, such as network communication. new approaches are needed for ways in which the applications programmer can express parallelism at a level high enough to reflect the application solution and without platformspecific details. novel tools are needed to help applicationlevel software designers reason about their solutions at a more abstract and problemspecific level. software technology is also needed to lessen future dependence on legacy codes. enough must be invested in the creation of advanced tool and environment support for new language approaches so that users can more readily adopt new software technology. importantly, advances in algorithms can sometimes improve performance much more than architectural and software advances do. more realistic simulations and modeling require not only increased supercomputer performance but also new methods to handle finer spatial resolution, larger time scales, and very large amounts of observational or experimental data.  additional applications challenges for which innovation is needed are the coupling of multiple physical systems, such as the ocean and the atmosphere, and the synthesizing of a physical system™s design by analytic estimation of its properties. emerging applications in areas such as bioinformatics, biological modeling, and nanoscience and technology are providing both new opportunities and new challenges.   the role of the government  there are several important arguments for government involvement in the advancement of supercomputers and their applications. the first is that unique supercomputing technologies are needed to perform essential government missions and to ensure that critical national security requirements are met. furthermore, without the government™s involvement, market forces are unlikely to drive sufficient innovation in supercomputing, because the innovatorsšlike innovators in many other hightechnology areasšdo not capture the full value of their innovations. historically, it seems that innovations in supercomputing have played an important role in the evolution of today™s mainstream computers and have provided important benefits by virtue of their use in science and engineering. these benefits seem to significantly exceed the value captured by the initial inventors. it appears that the ability of government to affect the supercomputing industry has diminished because supercomputing is a smaller fraction of the total computer market and computer technology is increasingly a commodity. this situation requires a careful assessment of the most effective ways for government to influence the future of supercomputing. the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved. 5 1 introduction       supercomputers are systems that provide significantly greater sustained performance than is available from contemporary mainstream computer systems. in applications such as analysis of intelligence data, weather prediction, or climate projection, supercomputers enable the generation of information that would not otherwise be available or that could not be generated in time to be actionable. supercomputing can also accelerate scientific research in important areas, such as physics, biology, or medicine. it can augment experimentation or replace it with simulation, thus reducing the cost and increasing the accuracy and repeatability of experimentation in science and engineering. further, supercomputing has the potential to suggest entirely novel experiments that can revolutionize our perspective of the world. it enables faster evaluation of design alternatives, thus improving the quality of engineered products. the value of supercomputers derives from the problems they solve, not from the innovative technology they showcase. the technology must be motivated by the application requirements. historically, better performance was achieved using faster logic and more parallelismšthat is, by performing many computations and data accesses concurrently. today, although there is some promising device research, parallelism is still the primary approach. the performance of supercomputers should be measured in terms of the time required to solve problems of interest. some problems, such as searches for patterns in data, can be broken down into subproblems that can be solved independently and the results easily combined later. thus, a collection of pcs that are intermittently available and are connected by a lowspeed network such as the internet can exhibit supercomputing performance, as shown by the example of seti@home.1 for such problems, a computational grid can replace a conventional supercomputer.2 however, many important problems requiring highperformance computing, such as the modeling of fluid flows, do not admit that kind of decomposition. while these problems can be solved using parallelism, dependencies among the subproblems necessitate frequent exchange of data and partial results, requiring significantly better communication (higher bandwidth and lower latency) between the computation and data storage loci than that achieved by networkconnected pcs.3  1ﬁseti@home: the search for extraterrestrial intelligence.ﬂ available at <setiathome.ssl.berkeley.edu>. 2a computational grid is a hardware and software infrastructure that provides dependable, consistent, pervasive, and inexpensive access to highend computational capabilities (i. foster and c. kesselman, 1999, computational grids, the grid: blueprint for a new computing infrastructure, morgankaufman.). 3although the grid does not replace supercomputers for many highend applications, grid computing enhances our ability to solve the kinds of problems for which large amounts of computation and oftenunique data are essential. in addition, the grid provides the infrastructure within which tightly coupled supercomputers reside. it enables efficient access to remote or specialized computation resources and efficient exchange of results between collaborating scientists. the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved.6 the future of supercomputing: an interim report this report focuses mostly on the latter kind of problem and, hence, on ﬁone machine roomﬂ systems. to achieve high performance on problems of interest, such supercomputers need not only the ability to perform operations at a high rate but also support for highbandwidth, lowlatency internal communication, large memories, and highperformance i/o subsystems. they also need suitable software, both highquality systems softwarešsuch as compilers and operating systemsšand welltuned applications software. to maintain focus, this report does not address networking (i.e., the external communication requirements of supercomputers) except to note its importance, nor does the report address specialpurpose systems such as signal processors.   study context  much has changed since the 1980s, when a variety of agencies invested in developing and using supercomputers and when the high performance computing and communications initiative (hpcci), which bridged and built on these agency efforts, was conceivedšand the 1990s, when the hpcci evolved into a broader and more diffuse program of computer science research support.4 more recently, federally supported highperformance computing research has deemphasized computer architecture research and begun to emphasize the networked grid for highperformance computing, an emerging industry interest,5 as a unifying concept. whereas early investments in highperformance computing research were shown to have had trickledown benefits for mainstream computing,6 recent trends cloud the picture for such benefits. meanwhile, there is increasing evidence of and concern about technical leadership in japan, where the industry has benefited from sustained government investment.7 concern about the diminishing u.s. ability to meet national security needs led to a recommendation in 2000 that dod subsidize a cray computer development program as well as invest in relevant longterm research.8 cstb convened the committee on the future of supercomputing, sponsored jointly by the doe office of science and doe™s advanced simulation and computing (asc) program to assess the state of supercomputing in the united states, including the characteristics of relevant systems and architecture research in government, industry, and academia and the characteristics of the relevant market. specific questions of interest to both the sponsors and congress are listed in box 1.1.   4the proliferation of pcs and the rise of the internet commanded attention and resources, diverting attention and effort from research in highend computing. there were, however, efforts into the 1990s to support traditional highperformance computing. see, for example, nsf, 1993, from desktop to teraflop: exploiting the u.s. lead in high performance computing. nsf blue ribbon panel on high performance computing. arlington, va.: national science foundation, august. 5barnaby j. feder. 2000. supercomputing takes yet another turn. the new york times, november 20, p. c4. 6computer science and telecommunications board (cstb), national research council. 1995. evolving the high performance computing and communications initiative to support the nation™s infrastructure. washington, d.c.: national academy press. this report noted the timemachine quality of highperformance systems. 7japanese support for indigenous capabilities has led to u.s. allegations of dumping, the most recent of which were resolved by an arrangement featuring u.s. resale of japanese machines by a u.s. vendor (see william m. bulkeley, 2001, outlook improves for u.s. supercomputer access, the wall street journal, march 2, p. b6.) meanwhile, u.s. vendors have long complained about controls on highperformance computer exports (see ted bridis, 2001, study suggests easing limits to export highperformance computers overseas, the wall street journal, june 8, p. b5.) 8defense science board. 2000. report of the defense science board task force on dod supercomputing needs. washington, d.c.: office of the under secretary of defense for acquisition and technology, october 11. the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved.introduction 7 box 1.1 concerns of the sponsor and of congress  issues of concern to the department of energy:  • how should the nation approach research and development of the highest end computers to ensure its future leadership in science and technology? • what is the economic model (e.g., commercial investment, particle accelerator, or submarine) for highperformance computers? • how do we allocate the investment between scientific applications, mathematical algorithms, system software, hardware architectures, hardware engineering, and so on? • what is the current state of the art of supercomputing in the united states and the rest of the world? • what/who are the requirements drivers of supercomputing? • what are supercomputing™s ﬁgold nuggetsﬂ and how might they be exploited? • does (or should) open source software have a role to play? • what are the shortfalls and how do we address them over the next 3, 5, 10, and 20 years? what are the costs of the solutions? • what should be the u.s. supercomputing vision? what role should (or can) the government play? questions of particular interest to the senate™s energy and water development appropriations committee, which funds the department of energy:  • what are the mission requirements driving the asc program? • how much capacity is needed, and when is it needed over the next 10 years? • what is the maximum capability required in the top asc platform, and when over the next 10 years? • was the nnsa wise to abandon customdesigned chips and vector architecture for much cheaper commoditychipbased, massively parallel systems? • what level of customization is needed for the various government interests in supercomputingšfor example, weapons design, molecule modeling and simulation, cryptanalysis, bioinformatics, climate modeling? • how effective are the current or planned asc platforms in addressing the requirements of the program? • are there alternative architectures, interconnect technologies, systems software and tools, or other approaches that will improve the performance of future asc platforms? • if so, can industry supply the required alternative architectures and software? that is, is industry properly motivated to do this? or must government fundamentally lead the development of these alternatives? • is the current asc approach the most costeffective and efficient manner of achieving the desired capability and capacity? • finally, as they relate to the asc mission requirements, what are the costs and benefits of investing more heavily in capacity now and deferring acquisition of capability machines, so as to take advantage of the falling price per teraflop?   source: daniel hitchcock, doe, josé l. muñoz, doe, and clay sell, senate energy and water development appropriations committee, presentations to the committee, march 6, 2003. many of the questions in the second list are to be addressed by the jasons™ study, mentioned elsewhere in this report.  the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved.8 the future of supercomputing: an interim report about this interim report  this interim report focuses on stage setting and contextšfor example, the history and current state of supercomputing and the socioeconomic contextštogether with some identification of issues being addressed by the committee. the committee expects that its understanding of these issues will change as it collects more data and deepens its analysis for the final report. the short time it had to prepare the interim report did not allow the committee to develop findings or recommendations. the presentations to the committee and the collective knowledge and experience of committee members have, however, enabled it to come to a preliminary understanding of some important issues, which are outlined in this interim report. however, the report does not document the detailed evidence that supports these views. the final report will provide the needed depth of information and analysis. since the committee appreciates the desire to use this interim report to inform budgetary discussions for fy 2005, it shares some of its initial views in this preliminary form. chapter 2 outlines the history and current state of supercomputing. the importance of continuity is summarized in chapter 3. chapter 4 discusses the need for research and innovation. chapter 5 addresses the role of the government in ensuring the future health of supercomputing. the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved. 9 2 supercomputing past and present       this chapter provides background material on supercomputing to establish key elements of context. a summary of reports and government activities illuminates the recent history of supercomputing. a brief overview of the current state of supercomputing technology follows.  previous reports and recent federal initiatives  during the past few decades, a number of reports have dealt with supercomputing and its role in science and engineering research. the first of the modern reports is the report of the panel on large scale computing in science and engineering (the lax report).1 the lax report made four basic recommendations: (1) increase access for the science and engineering research community to regularly upgraded supercomputing facilities via high bandwidth networks, (2) increase research in computational mathematics, software, and algorithms necessary to the effective and efficient use of supercomputing systems, (3) train people in scientific computing, and (4) invest in research and development basic to the design and implementation of new supercomputing systems of substantially increased capability and capacity, beyond that likely to arise from commercial requirements alone. in 1985, following the guidelines of the lax report, the national science foundation (nsf) established five supercomputer centers. following the renewal of four of the five nsf supercomputer centers in 1990 and the possible implications for them contained in the 1991 high performance computing act (p.l. 102194), the national science board (nsb) commissioned the nsf blue ribbon panel on high performance computing to investigate the future changes in the overall scientific environment due to rapid advances in computers and scientific computing.2 the panel™s report, from desktop to teraflop: exploiting the u.s. lead in high performance computing (the branscomb report), recommended a significant expansion in nsf investments, including accelerating progress in highperformance computing through computer and computational science research. in 1995, nsf formed a task force to advise it on the review and management of the supercomputer centers program. the chief finding of the report of the task force on the future of the nsf   1panel on large scale computing in science and engineering. 1982. report. sponsored by the department of defense and the national science foundation in cooperation with the department of energy and the national aeronautics and space administration. washington, d.c., december 26. 2national science foundation. 1993. from desktop to teraflop: exploiting the u.s. lead in high performance computing. nsf blue ribbon panel on high performance computing, august. the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved.10 the future of supercomputing: an interim report supercomputer centers program (the hayes report)3 was that the advanced scientific computing centers funded by nsf had enabled important research in computational science and engineering and had also changed the way that computational science and engineering contribute to advances in fundamental research across many areas. the recommendation of the task force was to continue to maintain a strong advanced scientific computing centers program. congress asked the national research council™s computer science and telecommunications board to examine the high performance computing and communications initiative (hpcci).4 cstb™s 1995 report evolving the high performance computing and communications initiative to support the nation™s infrastructure (the brooks/sutherland report)5 recommended the continuation of government support of research in information technology; the continuation of the hpcci; funding of a strong experimental research program in software and algorithms for parallel computing machines; hpcci support for precompetitive research in computer architecture (but end direct hpcci funding for development of commercial hardware by computer vendors and for ﬁindustrial stimulusﬂ purchases of hardware); and the development of a teraflop computer as a research direction rather than a destination. in 1997, following the guidelines of the hayes report, nsf established two partnerships for advanced computational infrastructure (paci), one with the san diego supercomputer center as a leadingedge site and the other with the national center for supercomputing applications as a leadingedge site. each partnership includes participants from other academic, industry, and government sites. a third participant, the pittsburgh supercomputer center, was added in 2000. the paci program is scheduled to end in the fall of 2004. in 1999, the president™s information technology advisory committee™s (pitac™s) report to the president: information technology research: investing in our future (the pitac report) made recommendations similar to those of the lax, hayes, and branscomb reports.6 pitac found that federal information technology r&d is too heavily focused on nearterm problems and that investment was inadequate. the committee™s main recommendation was to create a strategic initiative to support longterm research in fundamental issues in computing, information, and communications. supercomputing applications have also been studied. in 1999, the biomedical information science and technology initiative found that because the number of biomedical researchers who could profit from using supercomputing facilities was increasing, the national institutes of health (nih) should take a strong leadership position and help support the national supercomputer centers.7 the 2003 report revolutionizing science and engineering through cyberinfrastructure: report of the national science foundation blueribbon advisory panel on cyberinfrastructure (the atkins report)8  3national science foundation. 1995. report of the task force on the future of the nsf supercomputer centers program. september 15. 4hpcci was formally created when congress passed the highperformance computing act of 1991 (p.l. 102194), which authorized a 5year program in highperformance computing and communications. the goal of the hpcci was to ﬁaccelerate the development of future generations of highperformance computers and networks and the use of these resources in the federal government and throughout the american economyﬂ (federal coordinating council for science, engineering, and technology (fccset), 1992, grand challenges: highperformance computing and communications. fy 1992 u.s. research and development program, office of science and technology policy, washington d.c.). the initiative broadened from four primary agencies addressing grand challenges such as forecasting severe weather events and aerospace design research to more than 10 agencies addressing national challenges such as electronic commerce and health care. 5computer science and telecommunications board (cstb), national research council. 1995. evolving the high performance computing and communications initiative to support the nation™s infrastructure. washington, d.c.: national academy press. 6president™s information technology advisory committee (pitac). 1999. report to the president. information technology research: investing in our future. february. 7national institutes of health. 1999. the biomedical information science and technology initiative. working group on biomedical computing, advisory committee to the director, national institutes of health, june 3. 8national science foundation. 2003. revolutionizing science and engineering through cyberinfrastructure: report of the national science foundation blueribbon advisory panel on cyberinfrastructure. january. the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved.supercomputing past and present 11  found that scientific and engineering research is pushed by continuing progress in computing, information, and communication technology (among other things) and pulled by the expanding complexity, scope, and scale of today™s research challenges. the panel™s overall recommendation was that nsf should establish and lead a largescale interagency and internationally coordinated advanced cyberinfrastructure program (acp) to create, deploy, and apply cyberinfrastructure in ways that radically empower all scientific and engineering research and allied education. the panel strongly recommended that the u.s. academic research community have access to the most powerful computers that can be built and operated in production mode and that nsf should support five centers that will provide highend computing resources. there have also been studies of the use of supercomputing for missions important to the united states such as national security. the doe accelerated strategic computing initiative (asci) 9 was established in 1995 to transition from a testbased to a simulationbased certification program to analyze and predict the performance, reliability, and safety of nuclear weapons. the first supercomputer, asci red, which had 1 tflop performance, was delivered in 1996. other asci supercomputers include asci blue, asci white, and asci q. the goal of asci purple, scheduled for 2005, is 100 tflop. in 1996, a study by the office of the director of defense research and engineering (ddr&e) stated that in order for the united states to maintain supremacy in the highend computing field, a major national security program would be necessary.10 two reports by the general accounting office (gao) examined doe™s use of its computing capabilities. the titles of these reports summarize the gao findings: information technology: department of energy does not effectively manage its supercomputers11 and nuclear weapons: doe needs to improve oversight of the $5 billion strategic computing initiative.12 the first report citied utilization rates that showed, in the gao™s view, that the national laboratories were underutilizing their supercomputing capacity and missing opportunities to share it. (doe disputed those findings.) the lack of an investment strategy and a defined process was cited as a reason why doe was not fully justifying its supercomputer acquisitions. the second report found that a lack of comprehensive planning and progress tracking systems in the asci program made assessment of the initiative™s progress difficult and subjective. in 1998 nsa and ddr&e joined forces and funding to support the development of the sv2 (now the x1) by cray research in order to meet government needs that could not be met elsewhere in the marketplace. in the third quarter of 2002, cray delivered five early production versions of the x1. a 1024processor commercial x1 was delivered in early 2003. the department of defense sponsored the report of the defense science board task force on dod supercomputing needs.13 the task force found that there is a significant need for highperformance computers that provide extremely fast access to very large global memories and that such computers support a crucial national cryptanalysis capability.  task force recommendations included providing additional financial support for the development of the cray sv2 (now the x1), developing an integrated system based on commercial offtheshelf (cots) microprocessors and a new highbandwidth memory system, and investing in longterm research on critical technologies.   9this initiative subsequently became the advanced simulation and computing program but is still often referenced as asci. 10director of defense research and engineering. 1996. ddre integrated process team studyš a national security high end computing program. 11general accounting office (gao). 1998. information technology: department of energy does not effectively manage its supercomputers. report to the chairman, committee on the budget, house of representatives (gao/rced98208). washington, d.c.: gao, july. 12gao. 1999. nuclear weapons: doe needs to improve oversight of the $5 billion strategic computing initiative. report to the chairman, subcommittee on military procurement, house committee on armed services (gao/rced99195). washington, d.c.: gao, july. 13defense science board. 2000. report of the defense science board task force on dod supercomputing needs. october 11. the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved.12 the future of supercomputing: an interim report in 2001, charles holland, principal assistant deputy under secretary of defense for science and technology, and a team of experts authored a report14 that focused on dod™s research and redevelopment agenda for highperformance computing. the report found that current research does not adequately address medium to longterm needs and proposed an agenda with three thrustsštechnology development, concept demonstration, and industry adoptionšto address the challenges of producing innovative ideas and reinvigorating the academic and industry research communities. supercomputing architecture was the focus of survey and analysis of the national security high performance computing architectural requirements (the games report).15 the survey found that a major investment had been made by the national security community to migrate legacy applications from vector supercomputers to commodity highperformance computers (hpcs). it found that although vector supercomputers process more efficiently than commodity hpcs, most but not all large applications scale well on commodity hpcs. finally, it reported that some researchers found it increasingly difficult to program distributedmemory commodity hpcs, which had a negative impact on their research productivity. recommendations were to assess the usefulness of japanese vector supercomputers, reach out to researchers through the use of openmp on sharedmemory systems, promote flexibility through software that combines openmp and message passing interface and that switches between vector and cachebased optimizations, and establish a multifaceted r&d program to improve the productivity of highperformance computing for national security applications. the goal of the darpa high productivity computing systems (hpcs) program, initiated in 2002, is to provide a new generation of economically viable, highproductivity computing systems for the national security and industrial user community in 20072010. it is focused on addressing the gap between the capability needed to meet mission requirements and the current offerings of the commercial marketplace. hpcs has three phases: an industrial concept study currently under way with cray, sgi, ibm, hp, and sun; an r&d phase that was awarded to sun, cray, and ibm in july 2003 and lasting until 2006; and fullscale development, to be completed by 2010, ideally by the two best vendors from the second phase. the defense appropriations bill for fy 2002 directed the secretary of defense to submit a development and acquisition plan for a comprehensive, longrange, integrated, highend computing (ihec) program to congress by july 1, 2002. the resulting report, high performance computing for the national security community, was released in the spring of 2003. the report recommends an ihec program that integrates applied research, advanced development, and engineering and prototype development. the applied research element will focus on developing the fundamental concepts in highend computing and creating a pipeline of new ideas and graduatelevel expertise for employment in industry and the national security community. the advanced development element will select and refine innovative technologies and architectures for potential integration into highend systems. the engineering and prototype development element will build operational prototypes and system level testbeds. the report also emphasizes the importance of highend computing laboratories that will test system software on dedicated largescale platforms; support the development of software tools and algorithms; develop and advance benchmarking, modeling, and simulations for system architectures; and conduct detailed technical requirements analysis. the report suggests $390 million per year as the steadystate budget for this program. the program is planned to consolidate existing darpa, doe/nnsa, and nsa r&d programs and will feature a joint program office with ddr&e oversight. in addition to the study by the nrc™s committee on the future of supercomputing that resulted in this interim report, two other studies of the future of u.s. supercomputing are under way: one by the national coordination office for information technology research and development (itrd) and another by the jasons. the itrd highend computing revitalization task force has been charged with developing a plan and a 5year roadmap to guide federal investments in highend computing starting   14charles j. holland. 2001. dod research and development agenda for high productivity computing systems. white paper, june 11. 15richard a. games. 2001. survey and analysis of the national security high performance computing architectural requirements. june 4. the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved.supercomputing past and present 13  with fiscal year 2005. the final report is due in august 2003, in time to influence the fy 2005 budget. the jasons™ study, commissioned by doe at the request of congress, will identify the distinct requirements of the stockpile stewardship program and its relation to the asci acquisition strategy. the jasons are expected to complete their (classified) report in august 2003.  supercomputing technology  vendors supercomputers have been manufactured in the united states and abroad since early in the history of the computer industry. since 1993, a list of the sites operating the 500 most powerful computer systems has been available to the public. this list, called the top500, is updated twice a year.16 performance is measured by the number of floating point operations performed per second (flops) while executing the linpack benchmark to solve a dense system of linear equations.17 according to the june 2003 top500 list, the united states and japan dominate the use of and manufacture of highperformance systems (although supercomputers are used in europe, european computer companies have been limited to the integration of relatively small cluster systems). the top500 data show that the united states has a 50 percent share of installed supercomputers, germany has 11 percent, and japan has 8 percent, accounting for 69 percent of the total. another interesting comparison is to look at the aggregate performance by country. from the distribution by performance, the u.s. has 54 percent of the aggregate performance of the top500 computers and japan has 17 percent, together accounting for 71 percent of the total. breaking the numbers down by manufacturers, the top three, all u.s. companies, are hewlettpackard (32 percent of the top500 machines), ibm (31 percent), and sgi (11 percent); together they account for 74 percent of the systems. performance by manufacturer shows that 35 percent of the performance is attributable to ibm™s aggregate share of 31 percent, hewlettpackard™s 24 percent, and nec™s 12 percent. ninetyone percent of the top 500 systems are u.s. made.18 in summary, in both use and manufacture, the united states is the dominant participant, followed by japan. a small number of companies dominate the market. germany is a large user of supercomputing but not a large producer.   architecture  contemporary supercomputers are all built by clustering large numbers of compute nodes. they span a spectrum of architectural choices, from clusters that are assembled from lowcost, highvolume components, to systems that are custom built for highend scientific computing. the main differentiators are the node technology, the switch (sometimes called the interconnect) technology, and the nodeswitch interface.  16see <http://www.top500.org/>. 17no single number captures system performance across a wide range of applications and architectures. flops in a dense linear algebra benchmark is but one figure of merit; however, it is the one used for this widely referenced list. 18although these percentages would probably change if different metrics were used, the dominance of the united states over other countries would most likely remain. the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved.14 the future of supercomputing: an interim report node technology most lowcost clusters use 32bit intel or advanced micro devices (amd) microprocessors. these microprocessors are targeted for lowend servers and are produced in very large volumes (on the order of hundreds of millions). they are not optimized for scientific computing. sixtyfourbit microprocessors (alpha, power, sparc, mips, itanium, opteron) offer the advantages of support for larger memories, a better performing memory subsystem, and support for larger sharedmemory multiprocessor (smp) configurations. the production volumes of these microprocessors are two orders of magnitude smaller than the volumes for 32bit microprocessors. these microprocessors are mostly targeted for highend commercial servers, although on occasion vendors will develop smp configurations that are optimized for scientific computing. both 32bit and 64bit scalar microprocessors are optimized for singlethread performance on codes that exhibit good temporal and spatial locality.19 these codes make most of their memory references to an onchip cache, with good cache reuse. such processors have limited offchip bandwidth, can support only a small number (at most 8 or 16) of simultaneously outstanding memory references, and have cache line mechanisms that are not ideal for scientific applications. in highend application codes that do not make good use of caches, this approach to memory system design leads to a dramatic drop in actual performance when compared with the theoretical peak. this problem is mitigated in processors that employ either multithreading or vectors to generate a large number of outstanding memory references and that therefore tolerate long memory latencies while sustaining high bandwidth (thereby reducing the need for data locality). over 20 percent of the systems are based on intel and amd 32bit processors. about 8 percent of the systems use vector processors. approximately 60 percent of the systems use 64bit processors. switch technology lowend clusters, including some on the top500 list, use highvolume switched ethernet technology for the interconnect. higher bandwidth and lower latency are achieved by using custom interconnects from thirdparty vendors (e.g., quadrics and myricom) or from the system vendors (e.g., cray, ibm, nec, and sgi). a key differentiator between systems is the fraction of total system cost allocated to the interconnect: lowbandwidth networks will represent less than 10 percent of total system cost; a highbandwidth network may approach half of total system cost. another important differentiator is the scalability of the interconnect to large numbers of nodes.  nodeswitch interface nodes of lowend clusters connect to the switch via a standard i/o bus, such as peripheral component interconnect (pci). this choice reuses highvolume, lowcost technology but limits the function and performance of the interconnect, since i/o interfaces are not optimized for fast processortoprocessor communication. in such systems, global bandwidth is an order of magnitude lower than local memory bandwidth. the software for communication typically uses message passing, further increasing communication latency and limiting bandwidth for short messages. a custom memoryconnected interface, typically proprietary, can be used to increase bandwidth, reduce latency, or provide added functionality. such interfaces are usually paired with higherperformance custom switches.20 in particular, a custom interface can directly support shared memory communication, allowing a processor to access the memory of a remote node via load and store   19temporal locality is the property that data accessed recently in the past are likely to be accessed soon in the future. spatial locality is the property that data that are stored very near one another tend to be accessed closely in time. 20recently, intel and other companies have been directly attaching standard interconnects such as ethernet and infiniband directly to the memory system rather than via the pci bus; thus, some of the performance advantages of custom interfaces are becoming available with standard interconnects. the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved.supercomputing past and present 15  instructions. since shared memory communication has little software overhead, it has lower communication latency; however, the small number of pending memory references supported by massmarket microprocessors limits global bandwidth. shared memory support is generally believed to facilitate parallel programming, because of the single name space it provides; it also facilitates the use of a single operating system image to control the entire machine. approximately half the systems in the top500 list use proprietary switch interfaces.  products close to 20 percent of the top500 systems are selfmade or are assembled by system integrators from commodity components. almost all of these systems use intel or amd 32bit microprocessor nodes and run linux. the use of this type of cluster architecture was popularized by the beowulf project,21 following on previous network of workstations projects. such beowulf clusters are increasingly used as commercial capacity machines (e.g., web servers and search engines) and as departmental or project scientific computing machines in research and industry. such clusters are attractive because of their low purchase cost, the large number of component suppliers, and the ease of adding components. clusters of this type, which use lowcost ethernet interconnects, are often used to run ﬁembarrassingly parallelﬂ jobs consisting of many almost independent sequential subtasks. the top u.s. vendors all offer clusters with 64bit smp nodes and custom switches. with the exception of hp, all provide custom switch interfaces. the top ranked hewlettpackard (hp) systems, including the second top500ranked asci q system, use alphaserver smp nodes connected (via a standard pci interface) by a quadrics switch; global communication uses message passing. previous hewlettpackard clusters used the custom hyperfabric interconnect. the topranked ibm systems, including the fourthranked (by top500) asci white system, use power smp nodes connected with an ibm proprietary switch using a proprietary interface (power 4 systems currently use a standard i/o interface); global communication uses message passing. the cray t3e uses alpha uniprocessor nodes connected by a cray proprietary switch with a proprietary interface that supports fast (put/get) remote memory access; the largest such system on the top500 list has 1,900 processors. (cray is no longer pursuing the t3e architecture.) the sgi origin uses mips quadprocessor nodes connected with an sgi proprietary switch and an interface that supports cachecoherent global shared memory; the largest such system, with 1024 processors, is deployed at nasa ames (sgi is now shipping systems that use itanium processors). the sun fire, with up to 106 sparc processors, also supports global cachecoherent shared memory. nec in japan and cray in the united states are at present the only vendors that manufacture vector processors for largescale computing; their production volumes are significantly smaller than the volumes for nonvector 64bit microprocessors. such processors tend to be used in smallvolume systems for the high end of the scientific and technical computing markets. in the past, other top japanese vendors (fujitsu and hitachi) offered systems with vector processors. in the united states vector processors are being developed by cray, with its new x1 product line. ten systems on the current top500 list use cray vector processor nodes.22  21thomas sterling, donald j. becker, daniel savarese, john e. dorband, udaya a. ranawak, and charles v. packer. 1995. beowulf: a parallel workstation for scientific computation. proceedings of the 24th international conference on parallel processing. 22some massmarketed microprocessors have limited support for vector instructions in a form that typically cannot be used to hide memory latency. the committee reserves the term ﬁvector processorﬂ for systems that have large vector register files and that support vector load/store instructions that can address noncontiguous memory locations. the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved.16 the future of supercomputing: an interim report the nec earth simulator  the most significant japanese supercomputer manufacturer is nec. in the spring of 2002, nec released the earth simulator (es), a system with a peak performance of 40 tflops/sec that is ranked first on the june 2003 top500 list. based on the top500 linpack benchmark, the es is the world™s fastest computer by a factor of 2.58. an even greater ratio seems to hold for geosciences applications that it was specifically designed to support. the es is a cluster of 640 sharedmemory multiprocessor (smp) nodes. each smp node has eight processors, based on the sx6 nec processor design; each processor is a vector processor with a clock frequency of 500 mhz. eight vector units within each processor provide a peak performance of 8 gflop/sec per processor. the peak memory bandwidth is 32 gbps. each processor has 72 vector registers, each with 256 elements. a robust crossbar network connects the nodes and provides a peak bandwidth of 16 gbps per node. the sustained bandwidth is approximately 12 gbps, full duplex. the design of the nodes of the es (including vector processor and memory system) is evolutionary within the sx vector family. semiconductor technology and advanced packaging are used to achieve performance. the software is also evolutionary and fairly stable. it is instructive to compare the es to the asci q system at los alamos national laboratory (lanl), which uses hp alphaserver es45 nodes and a quadrics switch. compared with the asci q, the significant characteristics of the earth simulator are these:  • higher ratio of memory bandwidth to floatingpoint rate (4 b/flop versus 0.8 b/flop). this ratio improves performance significantly for many codes that are memory intensive but do not exhibit the spatial and temporal locality exploited by caches. although some such codes can be rewritten to be more cachefriendly, certain algorithms seem intrinsically difficult to localize.23 • use of vector parallelism in addition to smp and messagepassing parallelism. the availability of a large number of vector registers and of vector load instructions makes it possible to prefetch data and to hide memory latency for codes where data accesses are predictable but not spatially localized. codes that vectorize well can achieve a high fraction of the peak floating performance of the sx6. on the other hand, the scalar performance of the sx6 processor is not as good as the scalar performance of the alpha processor, so the alpha processor may achieve better performance on codes that do not vectorize well and are cache friendly. • use of a global switch with a higher ratio of global bandwidth to floatingpoint rate (0.2 b/flop versus 0.03 b/flop). this property contributes to performance on codes that require large amounts of global communication. in summary, es achieves a higher fraction of peak floating performance on many codes because of better memory bandwidth, better global bandwidth, and the availability of a memory prefetch mechanism (vector registers and vector load/store operations). there are no new microarchitectural concepts or unique technologies that are noteworthy in the es. rather, the performance is achieved through the use of a purposebuilt microprocessor with high memory bandwidth and latencyhiding hardware and through the acceptance of a different budget balance between node hardware and interconnect hardware.  23see, for example, the gups benchmark described in brian r. gaeke, parry husbands, xiaoye s li, leonid oliker, katherine a yelick, and rupak biswas, 2002, memoryintensive benchmarks: iram vs. cachebased machines, international parallel and distributed processing symposium (ipdps). the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved.supercomputing past and present 17  software message passing is the main programming model used to scale applications to large systems; the mpi standard messagepassing library is available on all top500 systems, including shared memory systems. lower overhead communication can be achieved using put/get libraries on systems with suitable switch interfaces, such as the cray t3e.  shared memory parallelism on smp nodes is often exploited using openmp (i.e., c or fortran with extensions for loop and task parallelism). however, openmp does not seem to be used for systemwide parallelism on large systems (even those supporting shared memory), perhaps because programmers lack the skill to use it well. almost all top500 systems use variants of unix for their operating system. shared memory systems are controlled by one global os image, while distributed memory systems typically have one os image per node. lowerend beowulf clusters typically use linux, while higherend systems use proprietary unix systems. libraries, programming tools, parallel file systems, and various system management tools complete the parallel programming environment available on these platforms. most vendor platforms use proprietary parallel programming environments. the proprietary software is often derived from opensource software; for example, all proprietary mpi implementations are derived from open source mpi implementations.  beowulf clusters mostly use opensource parallel software that is contributed by developers worldwide. support for standard programming environments and interfaces, across all platforms, is an important goal that is only partially achieved. mpi and openmp are two successful standardization efforts in which industry adopted a de facto standard developed by the hpc community and/or the hpc vendors. another successful model is provided by the totalview parallel debugger, where a thirdparty software vendor supports the same software product across all main hpc platforms. however, totalview is a singular example; attempts to standardize various tool interfaces and parallel system services, in particular parallel i/o, have had limited success. although programming for largescale parallel machines is more complex than programming for sequential machines, the typical programming environment available for scalable parallel computing is less sophisticated and less standardized than the environment available on small systems.  algorithms  the algorithms used to run supercomputing applications are needed not just within the applications themselves but also to analyze the output data, store and transmit the data over unreliable media, load balance efficiently, and so on. the primary challenge introduced by supercomputing is that many conventional algorithms for these problems must be modified so as to scale effectively to much larger data sets or numbers of processors and to run efficiently on machines with deep memory hierarchies. for example, a numerical simulation on a very large mesh may involve converting an algorithm from one using dense matrices or even direct solvers on sparse matrices to one using a specialized iterative method that may still use a parallelized direct method on subproblems. initially it may be possible to use a serialized mesh partitioner to load balance the matrix across processors, but as the matrix grows a parallel mesh partitioner may be needed. as another example, the problem may be scaled in order to introduce new physical models (e.g., one that respects polycrystalline structure in plasticity models), requiring wholly new discretizations and subgrid models. as this example illustrates, some of these algorithms are very specialized to particular application domains, whereas others, like mesh partitioners, are of quite general use. the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved. 18 3 continuity and predictability       the history of supercomputing in the u.s. in the last few decades is not a history of stability. technical directions have changed (for instance, single instruction multiple data (simd) architectures have come and gone); tens of supercomputer manufacturing companies have gone out of business; models for government support of supercomputing r&d have changed; and levels of government support have fluctuated. although technical fluctuations are caused, in part, by unpredictable changes due to innovation and may therefore be inevitable, more stability in supercomputingrelated government policy would be advantageous. a major policy issue for supercomputing, as well as for other technological fields, is the proper balance between investments that exploit and evolve current architectures and software and investments in alternative approaches that may lead to a paradigm shift. the interim report does not make recommendations on the crucial issue of how best to find that balance in the present context. this chapter and the next outline some of the main arguments for each type of investment. this chapter outlines the arguments for a continued, steady investment in the evolution of current architectures and software. the next chapter outlines the arguments for a sustained and vigorous research program in supercomputing. the two aspects are complementary.  important work is getting done  in support of both basic scientific research and large missions of national importance, today™s mix of supercomputing architectures and infrastructure are producing tangible results. for example, significant finding issues (sfis) in the nuclear weapon stockpile have been closed with the help of massively parallel simulations run on asci platforms.1 current missions require two kinds of supercomputer: (1) machines of high capability, on which a single very demanding problem uses the entire machine (for problems in which higher resolution in space or time is critical and for the most important and timeurgent problems, where faster times to solution are critical) and (2) workhorsecapacity machines that are used for multiple simultaneously executing jobs or embarrassingly parallel computations such as parameter studies. capability machines stretch the scalability of current supercomputing technology to its limitsšthey are designed for the most demanding computational problems. typically, the solutions to some of the computational problems within a mission organization will not exploit the full capability of such systems, either because the problem does   1as an example, an asci code has contributed to the yield reanalysis of a particular weapon that resulted in a revised certified yield. the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved.continuity and predictability 19  not demand that level of capability or because current methods fail to achieve it. for those problems, systems using less aggressive technology and lower levels of parallelism will typically reach capacity at better overall cost and performance. however, the lesser systems will not provide the capability needed to solve critical problems in a timely manner. using asci codes and computers, the design of an arming, fusing, and firing device for the w76 warhead was optimized over a weekend. if run on serial machines, the same analysis would have taken significantly longer to complete.  no nearterm alternatives  one reason for fluctuations in investments in supercomputing is the perpetual hope for a silver bullet that will revolutionize supercomputing technology. while there are a number of promising opportunities for technology advances (e.g., processor in memory or streaming architectures), they are far in the future and will require concentrated investments to bring to fruition. they are also less than certain. nearterm research breakthroughs in highperformance computer architecture are unlikely, perhaps because of the lack of research investment in recent years. evolution from current architectures is the only viable approach to meet needs in the immediate future (i.e., 3 to 5 years).   older architectures coexist with new ones  changes do occur over time. for example, the ﬁattack of the killer microsﬂ2 was largely successful, and supercomputers built of conventional microprocessors have largely displaced vector supercomputers. however, the current success of the japanese earth simulator and the reentry of cray in the vector market with the new x1 system also show the limitations of a shortterm view: supercomputers built of ﬁkiller microsﬂ have not fully replaced vector architectures, even after many years, and the alternative technology still has its niche. similarly, current supercomputing architectures are likely to be around for many years to come. furthermore, current architectures will survive and coexist with new innovations well after the latter are introduced. there has never been a single architecture that is best for all applications. historically, the diverse needs of the u.s. scientific and defense missions have often led to the coexistence of major supercomputer architectures. the evolution of supercomputer architecture probably will include supercomputers built from commercial microprocessor servers (such as the asci q machine), hybrid systems that use commercial microprocessors with custom interfaces and switches (such as the t3e and the planned red storm system being built for sandia national laboratory), and purposebuilt supercomputers (such as the cray x1). building supercomputers from commodity components (as in the asci machines) will continue to be an attractive approach. the highvolume commodity market produces components, from processor chips to complete machines, with priceperformance ratios difficult to achieve in any lowvolume product. in the future, as in the past, there will continue to be opportunities to build larger machines with these components, whether as clusters of multiprocessors with standard i/o interfaces or as more integrated and higher bandwidth machines like red storm (using commodity amd processors). scalable machines built from commodity parts have other attractive features in the context of a national supercomputing program. because they are built on a scalable technology, the supercomputers are merely the extreme end of a continuum of products. thus, some aspects of software and application   2eugene brooks. 1989. ﬁattack of the killer micros.ﬂ white paper presented at supercomputing 1989, reno, nev. a ﬁkiller microﬂ is a microprocessorbased machine that infringes on mini, mainframe, or supercomputer performance (see http://jargon.watsonnet.com/jargon.asp?w=killer%20micro). the allusion is to the science fiction spoof attack of the killer tomatoes (1978).  the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved.20 the future of supercomputing: an interim report development can be done on smaller machines, affordable to an academic research group or department, for example, and then adapted to larger machines at national laboratories and centers. supercomputers built from custom processors also embody an evolutionary path with an implied human and technology investment over time. for example, the cray x1 shares an architectural heritage with the cray t3e, and its vector processors are a natural evolution from previous vector processor designs. similarly, the nec earth simulator should be seen not as a radically new design but as the natural evolution of earlier sx processors. more customized supercomputers enlarge the set of applications that can be supported in a supercomputing universe. there is not a problemindependent, timeindependent, or costindependent dominance of customcomponent supercomputers over commoditycomponent supercomputers, or vice versa. each type contributes unique attributes to nationalscale programs; the problem mix appropriate to each type can change as technology changes, but it is unlikely in the near future that either type will come to replace the other. the importance and continuing value of software research and algorithm development  this committee notes, as many previous committees have noted, that software research and development continue to receive inadequate attention in national supercomputing programs. hardware evolution needs to be supported by a robust investment in software development. not only is such support necessary to ensure our ability to migrate key applications and algorithms to new architecturesšindeed, if timely, it may inspire improvements in those architecturesšbut also it is necessary in support of the unique scaling requirements of supercomputing now and in the future. government support for the development of the unique portable parallel debugger totalview, which is marketed by etnus and essential in supercomputing but of problematic economic viability in the broader computing marketplace, is a success story that needs to be repeated many times. neither the current limited investments of platform vendors nor open source code developed by the national laboratories, industry, and academia are likely, by themselves, to fulfill the need for the standardized, highquality programming environments that are needed to enhance programmer productivity in highperformance computing. an interesting aspect of the earth simulator system is the successful use of high performance fortran (hpf) on realistic applications to achieve significant performance levels (e.g., 14.9 tflop on a threedimensional fluid simulation for fusion science). hpf is an extension to fortran that was developed in the early 1990s and was viewed at the time as a promising approach for achieving high performance on scalable computers while programming at a higher level of abstraction. however, hpf did not deliver (fast enough) on its early promises and was largely abandoned in the united states the successful use of hpf on the earth simulator suggests that there may have been a lack of perseverance in pursuing this and other software technologies. this is not to argue for the merits of hpf per se. rather, it is to point out that lack of appreciation for the promise of supercomputing software technology may rob promising approaches of the time they need to mature. a similar observation applies to application code development. changing computing platforms require continual rethinking and redesigning of codes. additionally, the continually increasing computing power produces new scientific targets of opportunity for which code enhancements must be made. support for this activity has been consistently undervalued. investments in improvements to parallelism and in testing and validating algorithms and methods will continue to have value, even if supercomputing architectures change. for example, many software strategies for handling parallelism transcend the details of a particular parallel architecture. additionally, over time, successors to some highend machines tend to resemble widely available machines. put most simply, tomorrow™s beowulf clusters may well look like today™s asci machines. hence, software research and development done on today™s supercomputers will benefit tomorrow™s smaller research the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved.continuity and predictability 21  machines. opportunities for leveraging people, talent, and experience across institutional boundaries can be exploited.  legacy codes cannot be abandoned until they are replaced  an important aspect of software evolution is the handling of existing scientific application codes (often referred to as legacy codes). legacy codes represent a major investment; they evolve over many years, even decades, of use. changes in programming languages, tools, and libraries, in programming models, or in hardware platforms entail a significant cost as codes are ported or rewritten. several software capabilities are needed to support both the use and the evolution of legacy applications. supercomputer system software must provide continuing support for the basic operations used in the applications by keeping the legacy software running until it can be replaced, by providing tools for performance tuning and debugging on new platforms, and by providing effective methods for porting and evolution. ideally, software should present a stable programming model. this allows programmers to be more productive in creating new software, because they can better leverage their experience. moreover, developing new algorithms to exploit different costs of computation takes time. to the extent that a stable programming model reduces the need to reprogram fundamental computations, it can smooth this transition. it also mitigates the training costs. many changes in architecture have been accompanied by disruptions in programming models and software tools, impeding progress. it is imperative that new systems be developed in conjunction with development software (preferably using familiar, portable programming models) if they are to increase user productivity. of particular importance is ensuring that changes in lowlevel hardware do not create unnecessary changes in higherlevel software; for example, programmers working in objectoriented languages should not have to rewrite their code for new processor instruction sets. this will happen only by structuring funding to consider both hardware and software at the research stage and not allowing one to go forward without the other.   uncertainty and inconsistent policies can be expensive  failure to maintain steady, substantial investment in supercomputing could raise the cost of developing new generations of supercomputers in a timely manner.  rational firms will substantially reduce their level of effort when there is uncertainty about the demand for certain products. because the cost of building up and tearing down the small, highly skilled teams that develop supercomputers can be significant, a temporary reduction in supercomputer acquisitions might in the long run raise the overall cost of supercomputing procurements. it may be that the ﬁkeep the shipyard aliveﬂ argumentšused to ensure that navy ships are built at a continuous, predictable rate as a matter of national interestšapplies to supercomputer acquisitions from multiple vendors. while maintaining a predictable, continuous stream of investments in supercomputing is important, it appears that diversity of investment also is crucial (see chapter 4). a rational investment program requires a diversified portfolio of investments in a variety of platforms, both hardware and software, and in basic research. in summary, it is important that the longterm benefits of maintaining multiple suppliers, and helping them to do the longrange planning that sustains their supercomputing expertise, be carefully considered by policy makers. the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved. 22 4 future supercomputing and research       as the uses of computing to address important societal problems continue to grow and the place of supercomputing within the overall computing industry continues to change, the value of innovation in supercomputer architecture, modeling, systems software, applications software, and algorithms will endure. drawing on the recent supercomputing reports summarized in chapter 2 and on its own insights, the committee outlines the main arguments for a vigorous research program in supercomputing. some characteristics of successful innovation in areas such as highperformance computing are described and some key research problems are identified.   innovation in highend computing  a mature field is one characterized by small incremental improvements rather than large changes. by that measure, computing, and in particular highend computing, is not a mature field. the underlying technology continues to evolve at a rapid pace,1 and there are ample opportunities for innovations in architecture, systems software, and applications software to dramatically improve performance and productivity. new architecture and software technologies are needed to maintain historical growth in performance. to ensure that new technologies are available to form the basis for supercomputers in the 515 year time frame (a typical interval between research innovation and commercial deployment in the computer industry), a significant and continuous investment in basic research is required. historically, such an investment in basic research has returned large dividends in terms of new technology. the need for basic research in supercomputing is particularly acute. although there has been basic research in generalpurpose computing technologies with broad markets, and there has been significant expenditure in advanced development efforts such as the asc program and the teragrid, there has been relatively little investment in basic research in supercomputing architecture and software over the past decade, resulting in few innovations to be incorporated into today™s supercomputer systems. because supercomputing is affected by dislocations due to nonuniform technology scaling (described subsequently) before mainstream computers are affected, the lack of that research will eventually weigh on the broader computer industry as well. successful innovation programs in areas such as highperformance computing have a number of characteristics:   1international technology roadmap for semiconductors. 2002. update. available online at <http://public.itrs.net/files/2002update/home.pdf>. the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved.future supercomputing and research 23  • continuous investment is needed at all stages of the technology pipeline, from initial investigation of new concepts to technology demonstrations and products. with no initial, speculative research, the pipeline dries out. with no technology demonstrations, new research ideas are much less likely to migrate to products. with no investment in products, supercomputers are not built. • continuous investment is needed in all contributing sectors, including universities, national laboratories, and vendors. all of these sectors, which often depend on government funding for their existence, have small communities of researchers and developers that are necessary for the continued evolution of supercomputing. • a mix of small science projects and large efforts that create significant experimental prototypes is necessary. large numbers of small individual projects are often the best way of studying new concepts. a smaller number of technology demonstration systems can draw on the successes of basic research in architecture, software, and applications concepts, demonstrate their interplay, and validate concepts ahead of their use in production systems. it is important that such pilot systems be built because without real hardware platforms, systems software and applications programs will not be written nor will the experience gained from such system building be acquired. for instance, pilot systems serve to identify research issues associated with the integration of hardware and software and to address systemlevel problems such as i/o performance in highperformance computing. • research is not a linear pipeline or a funnel, where losers are successively winnowed out until one winning product emerges. successful research projects often incorporate the best ideas from related efforts. in addition, the experience gained from later stages often triggers reconsideration of earlier decisions. good research should be organized to maximize the flow of ideas and people across projects and concepts. architecture research  the memory wall and the programming wall are two particularly challenging problems in supercomputing that could benefit from architecture research. the memory wall, the growing mismatch between memory bandwidth and latency and processor cycle time, is a major factor limiting performance. on many applications, modern processors are limited by memory system performance to a small fraction of peak performance. this problem appears to be growing worse over time. the memory wall is a special case of nonuniform scaling. as technology improves, different aspects of technology scale at different rates, leading to disparities in system parameters. when these disparities become large enough, or when a new technology is introduced, there is a discontinuity in system design that calls for innovative architecture and software. to address the memory wall problem, innovative architectures are needed that increase memory bandwidthšor perhaps memory bandwidth per unit costšboth to local memory and across an interconnection network to remote memory. in addition, architectures must tolerate memory latency. memory latency (local and global) is expected to increase relative to processor cycle time. processors, however, can be designed to tolerate this latency without loss of performance by exploiting parallelism. while waiting for one result to return from memory, the processor works on a different, parallel part of the problem, perhaps using some combination of vectors and finegrained multithreading. innovation is needed to better identify and exploit locality. the memory bandwidth required by an application can often be reduced by a combination of architecture and software. the architecture provides local storage locations (registers and caches), and the software transforms the program to reduce the volume of intermediate data produced so that it fits in these local stores. a programming wall also existsšnamely, it is becoming increasingly difficult to write complex codes for highend computers. moreover, considerable effort is required to port these codes to new systems with different performance parameters. innovation in architecture must take that issue into account. architecture research is needed to devise a stable and efficient interface to systems software and applications, thereby masking, at least to some extent, the variety of architectural strategies that enhance the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved.24 the future of supercomputing: an interim report performance. for example, one could attempt to define an appropriate simple abstract virtual machine that reflected the architectural characteristics. all applications would then be programmed and compiled for that virtual machine. this approach could make it much easier for the software system to provide performance portabilityšthat is, the ability to move a code from one machine to another without extensive performance tuning.2 in addition, widespread adoption of better abstractions than those we have now would enable higherlevel programming languages and simpler algorithm and software development. sequential orientationšthe oneoperationatatime nature of contemporary architecture and softwarešis another major impediment to highperformance computing. while most highend computers are necessarily parallel, they are built using processors and programming languages that are fundamentally serial. by incorporating notions of parallelism in the virtual machine described above, some issues of sequential orientation also might be mitigated.3 software research the development of scalable scientific codes today is a laborious process. mathematical algorithms are translated by a programmer into detailed programs and tuned to a specific architecture using programming notations that reflect the underlying architectureša manual, errorintensive process. the resulting code is hard to maintain, evolve, and port to new machines. the programmer must provide a wealth of detail that can obscure the highlevel structure of the application solutionšfor example, the strategy to obtain parallelism. also, the programmer may have an imperfect understanding of how lowlevel mechanisms are best used to achieve high performance. highperformance computing offers unique challenges because of the need for largescale parallelism and for latency tolerance and because performance is important for large, expensive hardware platforms. research is needed to find fresh approaches to expressing both data and control parallelism at the application level, so that the strategy for achieving latency tolerance, locality, and parallelism is devised and expressed by the application developer, while separating out the lowlevel details that support particular platforms. both new compilation and operating system capabilities and new tools are needed to realize high performance on modern supercomputing architectures. many of the languages, operating systems, and tools in current use have evolved by modifying languages and operating systems designed for sequential systems to infer opportunities for parallelism and to add explicit mechanisms to invoke parallel layouts and operations. similarly, sequential tools have been modified for use in parallel environments. while that evolution is natural (and leverages existing knowledge and skills), it may no longer be sufficient. for example, the effort spent to maintain compatibility by changing the sequential base may limit the time available for enhancing support for parallelism and weaken the integrity of the parallel versions. new software approaches for highperformance computing could exploit several special advantages. first, many application solutions are derived from a precise mathematical formulation of a physical problem, such as a finite difference discretization of a differential equation on a grid. by taking the problem domain into account, the necessary relationships between states of the computation and states of a mathematical system can facilitate both the mapping of the computation onto a largescale parallel machine and the ensuing code development and testing. second, hpc codes are often developed by small teams of highly capable scientists, who are often willing and able to use expertfriendly tools and environments if those tools will enhance their productivity. finally, the difficulties in using the current tools for challenging and hard problems are a strong incentive for the user community to explore more advanced software technology.   2mpi is sometimes cited as such a target, but it is a lowlevel abstraction. 3threads provide an aspect of concurrency, but possibly not in the form most appropriate for some applications developers and some hardware architectures. the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved.future supercomputing and research 25  it is important to pursue the most promising software research with perseverance and a longterm view. it often takes significant investments and a long time to change widely used programming paradigms. a significant investment also may be needed in compilers, run times, tools, and libraries to induce a user community to shift paradigms, even if the new paradigm holds the promise for significant productivity enhancements. an alternative approach, which is more common, is to fund a diversity of small enhancements to current systems. that approach runs the risk that none of the enhancements will make enough of a difference, and that few of them will be pushed far enough to be transferred to practice. at the operating systems level, current hpc systems (especially cluster systems) have inherited a design that is not well tuned to the needs of largescale parallel processing. as an example, a linux cluster is managed by a large number of autonomous kernels, each making independent decisions on memory or processor allocation even though the entire cluster (or large parts of it) needs to run as one tightly coupled application. this discrepancy has been observed on many systems to have a negative effect on performance. a parallel application is not an entity that is recognized as a whole by the distributed operating system; there are no standardized parallel operating system services (e.g., parallel scheduling, parallel i/o, parallel memory management), although some implementations exist and are used. communication and synchronization within a parallel application are achieved inefficiently using the same operating system mechanisms that are used for communication and synchronization across independent processes, or they bypass the system services. the mere need for bypass indicates that current interfaces are inadequate. research that addresses the performance and semantic inadequacies of operating systems could lead to significant benefits in performance and software productivity and could push highperformance computing into new realmsšfor example, the use of largescale parallelism for interactive computing. software research in hpc is likely to be more successful if closely coupled with research on algorithms and on architecture. innovation often comes from a redesign of the interfaces between the various layers and from a better match between functionality across layers. a major challenge in building revolutionary architectures and software systems is dealing with the large volume of legacy code. on the one hand, innovative research should not be constrained by compatibility needs of existing instruction sets, programming languages, operating systems, and application implementations. advanced hpc research is likely to be more productive if it is free to explore paradigmshifting approaches. on the other hand, a transition plan is needed to encourage adoption of new technologies. the transition plan should leverage the existing code base, because the cost of rewriting all of the legacy code from scratch is prohibitive.  research on applications and algorithms  supercomputing applications exist in a number of wellestablished and important fields, such as national security (cryptanalysis, intelligence, defense systems design, and nuclear stockpile stewardship), weather and climate forecasting, and automotive and aircraft design. new applications are emerging in the life sciences and biochemistry, among others. this interim report was written in advance of an applications workshop to be held by the committee that will help it to formulate the supercomputingrelated research needs in these areas. following are some tentative observations. there is an everincreasing need for increased performance. the limitations of presentday supercomputers prevent many applications from being run using realistic parameter ranges of spatial resolution and time integration. for such applications, a significant increase of simulation and prediction quality can be attained by applying more computer power with primarily the same algorithms. for example, mesh resolution can be increased. in other applications, new algorithms and/or new processes are required to substantially advance the application involved. increased mesh resolution often requires the development of new physics or algorithms for subgridscale processes. in some cases, submodels of detailed processes may be required within a coarser mesh (e.g., cloudresolving submodels embedded within a larger climate model grid). the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved.26 the future of supercomputing: an interim report as applications evolve, the workload characteristics change. many codes evolve toward more complex, timedependent and datadependent control logic and more irregular data structures. this evolution taxes current architectures. huge amounts of model output and real data play an integral part of almost all supercomputing applications. the ways in which large datasets are prepared, stored, visualized, and analyzed highlight the need for new software, input/output, storage, and communication capabilities to go along with enhanced supercomputers and advanced methods. improvements in algorithms can sometimes improve performance much more than improvements in hardware and software do. for example, algorithms for solving the special linear system arising from the poisson equation4 on a regular grid have improved over time from needing o(n2) arithmetic operations to o(n log n) or even o(n). such algorithmic improvements can contribute to increased supercomputer performance as much as decades of hardware evolution.  while such breakthroughs are hard to predict, the rewards can be significant. further research can lead to such breakthroughs in the many complicated domains to which supercomputers are applied. new algorithmic demands are driven by the following needs:  • disciplinary needs. the need for higherresolution analyses leads to larger problems to solve that lead to the need for faster algorithms (e.g., o(n log n) instead of o(n2) ). as the resolution increases, completely different physical models may be required (e.g., particle models instead of continuum models), which in turn require different solution methods. in some problems (such as turbulence), physically unresolved processes at small length or time scales may have large effects on macroscopic phenomena, requiring approximations that differ from those for the resolved processes. • interdisciplinary needs. many realworld phenomena involve two or more coupled physical processes for which individual models and algorithms may be known (clouds, winds, ocean currents, heat flow inside and between the atmosphere and the ocean, atmospheric chemistry, and so on) but where the coupled system must be solved. vastly differing time and length scales of the different disciplinary models frequently makes this coupled model much harder to solve. • synthesis and optimization replacing analysis. after one has a model that can be used to analyze (predict) the behavior of a physical system (such as an aircraft or weapons system), it is often desirable to use that model to try to synthesize or optimize a system so that it has certain desired properties. such a problem can be much more challenging than analysis alone. as an example, a typical analysis computes, from the shape of an airplane wing, the lift resulting from air flow over the wing, by solving a differential equation. the related optimization problem is to choose the wing shape that maximizes lift, incorporating the constraints that ensure that the wing can be manufactured. solving that problem requires determining the direction of change in wing shape that causes the lift to increase, either by repeating the analysis as changes to shape are tried or by analytically computing the appropriate change in shape. • huge data sets. many fields (one is biology) that previously had relatively few quantitative data to analyze now have very large quantities, often of varying type, meaning, and uncertainty. these data may be represented by a diversity of data structures, including tables of numbers, irregular graphs, adaptive meshes, relational databases, two or threedimensional images, text, or various combined representations. extracting scientific meaning from these data requires coupling numerical, statistical, and logical modeling techniques in ways that are unique to each discipline. • changing machine models. a machine model is the set of operations and their costs presented to the programmer by the underlying hardware and software. as the machine model changes between technology generations, an algorithm will probably have to be changed to maintain performance and scalability. this could involve adjusting a few parameters in the algorithm describing data layouts, running a combinatorial optimization scheme to rebalance the load, or using a completely different algorithm that trades off computation and communication in different ways. some success has been   4a poisson equation is an equation that describes many physical systems, including heat flow, fluid flow, diffusion, electrostatics, and gravity, with n unknowns. the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved.future supercomputing and research 27  achieved in automating this process, but only for a few important algorithmic kernels. for example, the atlas5 and fftw6 systems automatically choose implementations of matrixmatrixmultiplication and the fast fourier transform, respectively, to maximize performance on a particular architecture, depending on properties such as memory speed and number of registers.  emerging application areas also drive the need for new algorithms and applications. bioinformatics, for example, is driving the need to couple equationdriven numerical computing with probabilistic and constraintdriven computing. large volumes of data from the human genome project, clinical trials, statistics, population genetics, and imaging and visualization research stress the i/o capabilities of contemporary systems. many simulation and optimization codes that are now evolved and maintained by independent software vendors (isvs) originated in research labs and universities. the arguments for government funding of supercomputing that are outlined in the next section apply as well to supercomputing application software: markets are likely to underinvest in such software, the government is an early and main customer for many such packages, and isv codes are heavily used in weapon design. thus, there is a strong case for government investment, not only in algorithm and application research, but also in the development of robust and scalable application software.   5see <http://mathatlas.sourceforge.net>. 6see <http://www.fftw.org>. the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved. 28 5 the role of government in supercomputing       the federal government has been involved in the development and advancement of supercomputing since the advent of computers themselves. while the precise mechanism and level of support have varied over time, there has been a longstanding federal commitment to encourage the technical progress and diffusion of highperformance computing systems (some of this history is summarized in chapter 2). economic and policy analysis emphasizes several broad justifications for government involvement in technology development; the key justifications that apply to supercomputing are outlined below.  government as a leading customer  much technological innovation is (at least initially) directed toward applications dominated by government involvement and purchasing. most notably, defense needs have often been the specific setting in which new technologiesšincluding supercomputingšare first developed and applied. even when commercial firms are purchasing or exploiting supercomputer technology, as in the pharmaceutical and biotechnology industries, governments are often the largest single customer for the resulting innovations (e.g., through the medicare and medicaid programs in the united states or national health insurance programs in other countries). the federal government remains the single largest purchaser of supercomputers in the world, for missionoriented tasks ranging from national security to health to climate modeling.1 some government missions may require specifications that are peculiar to the individual applications. in such cases, it may be important for the relevant federal agencies and research labs to be closely involved in the associated r&d (including prototyping), even when the research and development are carried out in the private sector. in the united states, japan, and elsewhere, the majority of supercomputers have been purchased directly or indirectly using government funds, and the committee has no evidence that this pattern is likely to change in the foreseeable future. as the social custodian of welldefined government missions and the largest and most aggressive customer for new technology related to these missions, the government has an incentive to ensure appropriate and effective funding for innovative investments in   1idc estimates that the highend hpc market in the united states has been around $1 billion (±$200 million) per year since 1994. the u.s. government (including dod, the national laboratories, and classified programs) spends roughly $700 or $800 million per year (and this spending has been relatively flat for the last 10 years). source: debra goldfarb, idc hpc industry analyst, presentation to the committee on may 23, 2003. the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved.the role of government in supercomputing 29  supercomputing technology so as to guarantee that the technology progresses at a rate and in a direction that serve government missions.  national security implications  u.s. government users require assured, secure, and reliable access to supercomputing capabilities to ensure that critical national security requirements are met. supercomputers are used to develop intelligence gathering equipment (e.g., better antennas), to find important information through massive data mining, and for cryptography and cryptanalysis.  they are used to design better weapons, airplanes, and tanks as well as for battlefieldrelated calculations that must be carried out very quickly. for example, the timely calculation of areas of enemy territory where enemy radars are not able to spot our airplanes (as was done during the first gulf war) can be crucial. design and refurbishment of nuclear weapons depends critically on supercomputing calculations, as does the design of nextgeneration armament for the army™s future combat system. it is likely that supercomputing will be increasingly important to homeland security. examples include micrometeorology analysis to combat biological terrorism and computer forensic analysis of terrorist bombings. the federal government must be able to guarantee that such systems do what they are intended to do with no harmful side effects from, for instance, malicious insertion of incorrect calculations. it must guarantee that supercomputers are available to u.s. security agencies with no hindrance and must be able to restrict access to some supercomputing technologies abroad. all in all, the government has an incentive to ensure a strong supercomputing technology base in the united states.  market forces  economists are generally reluctant to see government intervene in highly competitive markets, where the costs of disruption to wellfunctioning and efficient resource allocation mechanisms are likely to be high. however, in many circumstances, marketbased incentives for scientific discovery and innovation are likely to be insufficient. because innovators often are unable to capture the full value of their inventions, market forces alone typically will bring less innovation than is worthwhile from society™s perspective. typically, underinvestment is greatest for basic research, fundamental scientific discoveries, or technologies that serve as steppingstones for followon research by others. a number of computing innovations first implemented in supercomputers (for example, instruction lookahead, multiple arithmetic units, multiple instruction buffers and data operators, pipelining, programmable i/o processors)2 played an important role in shaping the architecture and performance of mainstream computers today (from workstations to personal computers). initiatives funded in the context of supercomputers have influenced the ability to commercialize innovations, from workstation architecture to the latest intel cpu. machines equivalent in power to the supercomputer of 15 years ago that cost millions of dollars can now be purchased online for less than $1,000. the dramatic decrease in price for the same performance is due to continuing advances in semiconductor technology. however,   2for historical compilations of these and other major innovations in computer architecture, see harold s. stone et al., 1980, ﬁhardware systems,ﬂ in what can be automated? the computer science and engineering research study, bruce w. arden, ed., cambridge, mass.: mit press, pp. 319 and 320; c. gordon bell and allen newell, 1971, computer structures: readings and examples, new york, n.y.: mcgrawhill, fig. 2c, p. 45, p. 71; c. gordon bell and allen newell, 1982, computer structures: principles and examples, new york, n.y.: mcgrawhill, p. 393. these and other sources have been drawn together and annotated with additional information in k. flamm, 1988, creating the computer: government, industry, and high technology, washington, d.c.: brookings institution press. pp. 260269. the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved.30 the future of supercomputing: an interim report the architecture of the chip and the software that drives it are based on the supercomputers of 15 years ago. supercomputing technology created with government funds has found its way into commercial computers and, in turn, has been used by private firms in basic research and to improve the design of products, including airplanes, automobiles, and pharmaceuticals. supercomputing is essential for fundamental and applied research in materials science, earth science, life sciences, and so on. it is interesting to note that the earth simulator system that is described earlier is dedicated exclusively to the geosciences. the availability of such a powerful tool is likely to accelerate research in those disciplines by leading, for example, to a better understanding (and, hence, better prediction) of climate change and earthquakes. the value of such accelerated progress appears to be immense. it would seem, however, that only a small share of all the benefits is being captured by those involved in the initial stages of their development. in summary, keeping the u.s. supercomputing industry at the technological cutting edge is vital for the security interests of the united states. in light of this imperative, there are several economic arguments that might justify continued government funding for supercomputing r&d rather than reliance on the marketplace alone. the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved. 31 6 conclusion       the united states has always played a leadership role in bringing computing technology to bear on science and engineering research and development, advancing entirely new frontiers. the design of the earliest computers, for examplešbritain™s colossus and the u.s.™s eniac during world war ii1šwas driven by defense and national security needs. in the united states, eniac was followed by a steady stream of governmentdriven highperformance systems.2 supercomputing continues to be important for satisfying those needs. the particular technical approaches of any program that develops or uses supercomputing represent a complex compromise between conflicting requirements and the risks and opportunities entailed in various approaches. as described earlier in this report, an assessment of the approaches requires a detailed understanding of (1) the applications, (2) the algorithms used to solve those applications, (3) codes and programming environments, (4) the performance of codes on various platforms, (5) the likely evolution of various hardware and software technologies under various funding scenarios, and (6) the costs, probabilities, and risks involved in various approaches. in its final report, the committee will seek to characterize broadly the requirements of different application classes and to examine architecture, software, algorithm, and cost challenges and tradeoffs associated with these application classes, keeping in mind the needs of the nuclear stockpile stewardship program, the broad science community, and the national security community. (note that the identification of the distinct requirements of the stockpile stewardship program and its relation to the asc acquisition strategy is expected to be the focus of a separate classified report by the jasons). the committee believes it would be unwise to significantly redirect or reorient current supercomputing programs before careful scientific consideration has been given to the issues described above. such changes might be hard to reverse, might reduce flexibility, and might increase costs in the future. exciting opportunities to advance knowledge and to serve society using supercomputing continue to emerge. the life and health sciences are becoming extraordinarily data rich, and researchers in those sciences are struggling to make sense of the data. the fruits of the genome projects for physiology and medicine cannot be realized without significant investments in computational hardware, algorithms, the   1colossus was designed to decrypt german codes. see <http://www.codesandciphers.org.uk/lorenz/colossus.htm>. eniac (electronic numerical integrator analyzer and computer) was built to calculate ballistic firing tables. see <http://ftp.arl.mil/~mike/comphist/eniacstory.html>. 2for a discussion of early governmentfunded projects in the late 1940s and 1950s that essentially created the early u.s. computer industry, see chapter 3 (ﬁmilitary rootsﬂ) in kenneth flamm, 1988, creating the computer: government, industry, and high technology, washington, d.c.: brookings institution press. the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved.32 the future of supercomputing: an interim report  software infrastructure, and the human infrastructure. the understanding of the physical world enabled by simulation and modeling is reaching everhigher levels of fidelity and timeliness. as in many other areas of technology r&d, there seem to be sound economic and social arguments for continued government investment in supercomputing. to sustain our leadership in supercomputing, to meet the security and defense needs of our nation, and to realize the opportunities to use supercomputing to advance knowledge, progress in supercomputing must go on. continuity and stability in the government funding of supercomputing appear to be essential to the wellbeing of supercomputing in the united states. an appropriate balance must be struck between evolutionary and innovative advances. evolution is important because it allows present achievements to be exploited and because a diversity of approaches to supercomputingšincluding refinements of existing approachesšappears to be necessary to address the diversity of the computational challenges we face.  innovation in supercomputing stems from applicationmotivated research, which leads to experimentation and prototyping and then, in turn, to advanced development and testbeds and, finally, deployment and products. all the stages along that path need sustained investment. coupled innovations in architecture, in software, in algorithms, and in application strategies and solution methods are equally important.  balance is also needed between exploiting costeffective advances in widely used hardware and software and developing custom solutions that meet the most demanding needs. as we reach the limitations of current approaches and encounter the disruptions that are unavoidable when different technologies grow at different rates, the fruits of that research and its maturation into practice will prepare us for major paradigm shifts in the future. the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved. appendixes the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved. the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved. 35 a committee member and staff biographies   committee biographies  susan l. graham (nae), cochair, is the pehong chen distinguished professor of electrical engineering and computer science at the university of california, berkeley, and the chief computer scientist of the national partnership of advanced computations infrastructure (npaci). her research spans many aspects of programming language implementation, software tools, software development environments, and highperformance computing. as a participant in the berkeley unix project, she and her students built the berkeley pascal system and the widely used program profiling tool gprof. their paper on that tool was selected for the list of best papers from 20 years of the conference on programming language design and implementation (19791999). she has done seminal research in compiler code generation and optimization. she and her students have built several interactive programming environments, yielding a variety of incremental analysis algorithms. her current projects include the titanium system for language and compiler support of explicitly parallel programs and the harmonia framework for highlevel interactive software development. dr. graham received an a.b. in mathematics from harvard university and m.s. and ph.d. degrees in computer science from stanford university. she is a member of the national academy of engineering and a fellow of the association for computing machinery (acm), the american association for the advancement of science (aaas), and the american academy of arts and sciences. in 2000 she received the acm sigplan career programming language achievement award. in addition to teaching and research, she has been an active participant in the development of the computer science community, both nationally and internationally, over the past 25 years. she was the founding editor in chief of acm transactions on programming languages and systems, which continued under her direction for 15 years. she has also served on the executive committee of the acm special interest group on programming languages and as a member and chair of the acm turing award committee. dr. graham has served on numerous national advisory committees, boards, and panels, including the national research council™s (nrc™s) computer science and telecommunications board, the nrc™s commission on physical sciences, mathematics, and applications, the advisory committee for the nsf science and technology centers, and the advisory committee of the nsf center for molecular biotechnology. dr. graham is a former member of the presidential information technology advisory committee (pitac).  marc snir, cochair, is michael faiman and saburo muroga professor and head of the department of computer science at the university of illinois at urbanachampaign. dr. snir™s research interests include largescale parallel and distributed systems, parallel computer architecture, and parallel programming. he received a ph.d. in mathematics from the hebrew university of jerusalem in 1979 and worked at new york university (nyu) on the nyu ultracomputer project from 1980 to 1982; at the hebrew university of jerusalem from 1982 to 1986; and at the ibm t.j. watson research center from 1986 to 2001. at the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved.36 the future of supercomputing: an interim report  ibm he headed research that led to the ibm scalable parallel system; contributed to power 4 and intel server architecture; and initiated the blue gene project. dr. snir has published more than a hundred papers on computational complexity, parallel algorithms, parallel architectures, interconnection networks, compilers, and parallel programming environments; he was a major contributor to the design of mpi. dr. snir is an acm fellow and a fellow of the institute of electrical and electronics engineers (ieee). he serves on the editorial boards of parallel processing letters and acm computing surveys. william dally received the b.s. degree in electrical engineering from virginia polytechnic institute, the m.s. degree in electrical engineering from stanford university, and the ph.d. degree in computer science from caltech. he is currently a professor of electrical engineering and computer science at stanford university, where his group developed the imagine processor, which introduced the concepts of stream processing and partitioned register organizations. dr. dally and his group have developed system architecture, network architecture, signaling, routing, and synchronization technology that can be found in most large parallel computers today. while at bell telephone laboratories he contributed to the design of the bellmac32 microprocessor and designed the mars hardware accelerator. at caltech, he designed the mossim simulation engine and the torus routing chip, which pioneered wormhole routing and virtualchannel flow control. while a professor of electrical engineering and computer science at the massachusetts institute of technology, his group built the jmachine and the mmachine, experimental parallel computer systems that pioneered the separation of mechanisms from programming models and demonstrated very low overhead synchronization and communication mechanisms. dr. dally has worked with cray research and intel to incorporate many of these innovations in commercial parallel computers and with avici systems to incorporate this technology into internet routers, and he cofounded velio communications to commercialize highspeed signaling technology. he is a fellow of the ieee, a fellow of the acm and has received numerous honors, including the acm maurice wilkes award. he currently leads projects on highspeed signaling, computer architecture, and network architecture. he has published over 150 papers in these areas and is an author of the textbook digital systems engineering. james w. demmel (nae) joined the computer science division and mathematics department at the university of california, berkeley, in 1990, where he holds a joint appointment as the dr. richard carl dehmel distinguished professor. he is also the chief scientist of the center for information technology research in the interest of society (citris) and has worked to create the atmosphere of collaboration and communication that fosters an interdisciplinary approach to information technology research. dr. demmel is an expert on software and algorithms to facilitate computational science, having contributed to the software packages lapack, scalapack, blas, and superlu. he is an acm fellow and an ieee fellow and has been an invited speaker at the international congress of mathematicians. he received a b.s. in mathematics from caltech in 1975 and a ph.d. in computer science from the university of california, berkeley, in 1983.  jack j. dongarra (nae) is a university distinguished professor of computer science in the computer science department at the university of tennessee, an adjunct r&d participant in the computer science and mathematics division at oak ridge national laboratory (ornl), and an adjunct professor in the computer science department at rice university. he specializes in numerical algorithms in linear algebra, parallel computing, use of advanced computer architectures, programming methodology, and tools for parallel computers. his research includes the development, testing, and documentation of highquality mathematical software. he has contributed to the design and implementation of the following open source software packages and systems: eispack, linpack, the blas, lapack, scalapack, netlib, pvm, mpi, netsolve, top500, atlas, and papi. he has published approximately 200 articles, papers, reports, and technical memoranda and is coauthor of several books. he is a fellow of the aaas, the acm, and the ieee. he earned a b.s. in mathematics from chicago state university in 1972. a year later he finished an m.s. in computer science from the illinois institute of technology. he received the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved.committee member and staff biographies 37  his ph.d. in applied mathematics from the university of new mexico in 1980. he worked at the argonne national laboratory until 1989, becoming a senior scientist.  kenneth flamm holds the dean rusk chair in international affairs at the university of texas lyndon b. johnson (lbj) school of international affairs. he joined the lbj school in 1998, is a 1973 honors graduate of stanford university, and received a ph.d. in economics from mit in 1979. from 1993 to 1995, dr. flamm served as principal deputy assistant secretary of defense for economic security and as special assistant to the deputy secretary of defense for dualuse technology policy. defense secretary william j. perry awarded him the department™s distinguished public service medal in 1995. prior to his service at the defense department, he spent 11 years as a senior fellow in the foreign policy studies program at the brookings institution. dr. flamm has been a professor of economics at the instituto tecnológico a. de méxico in mexico city, the university of massachusetts, and george washington university. he has also been an adviser to the director general of income policy in the mexican ministry of finance and a consultant to the organization for economic cooperation and development, the world bank, the national academy of sciences, the latin american economic system, the u.s. department of defense, the u.s. department of justice, the u.s. agency for international development, and the office of technology assessment of the u.s. congress. dr. flamm, an expert on international trade and high technology industry, teaches classes in microeconomic theory, international trade, and defense economics.  mary jane irwin (nae) is distinguished professor of computer science and engineering at the pennsylvania state university. her research and teaching interests include computer architecture, embedded and mobile computing systems design, low power design, and electronic design automation. her research is supported by the national science foundation, the marco gigascale silicon research center, intel corporation, and microsoft. she received an honorary doctorate from chalmers university, sweden, in 1997 and the penn state engineering society™s premier research award in 2001. dr. irwin was named a fellow of the ieee in 1995 and a fellow of the acm in 1996. she is currently serving as chair of the national science foundation™s computer information sciences and engineering directorate™s advisory committee, as a member of the technical advisory board of the army research laboratory, as the editor in chief of acm™s transactions on design automation of electronic systems, and as an elected member of the computing research association™s board of directors. in the past she has served as an elected member of the ieee computer society™s board of governors, of acm™s council, and as vice president of the acm. she served as general chair of the 1996 federated computing research conference, the 36th design automation conference, and the 2002 international symposium on low power electronics and design. dr. irwin received her m.s. (1975) and ph.d. (1977) degrees in computer science from the university of illinois, urbanachampaign.  charles koelbel is a research scientist in the computer science department at rice university. dr. koelbel™s area of expertise is in languages, compilers, and programming paradigms for parallel and distributed systemsšin layman™s terms, developing computer languages and algorithms that let several computers ﬁtalkﬂ to each other and work together efficiently. he has contributed to many research projects while at rice, mostly through the center for research on parallel computation, an nsffunded science and technology center with the mission to make parallel computation usable by scientists and engineers. these projects include the national computational science alliance technology deployment partners program, the department of defense™s highperformance computing modernization program, and the fortran d programming language project. he was executive director of the high performance fortran forum, an effort to standardize a language for parallel computing. more recently, he served for 3 years as a program director at the national science foundation, where he was responsible for the advanced computational research program and helped coordinate the information technology research program. he is coauthor of the high performance fortran handbook, mit press, 1993, and many papers and technical reports. he received his ph.d. in computer science from purdue university in 1990. the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved.38 the future of supercomputing: an interim report  butler w. lampson (nae) is an architect and a distinguished engineer at microsoft corporation and an adjunct professor of computer science and electrical engineering at the massachusetts institute of technology. he was on the faculty at the university of california, berkeley, at the computer science laboratory at xerox parc, and at digital™s systems research center. dr. lampson has worked on computer architecture, local area networks, raster printers, page description languages, operating systems, remote procedure call, programming languages and their semantics, programming in large, faulttolerant computing, transaction processing, computer security, and wysiwyg editors. he was one of the designers of the sds 940 timesharing system, the alto personal distributed computing system, the xerox 9700 laser printer, twophase commit protocols, the autonet lan, microsoft tablet pc software, and several programming languages. he received an a.b. from harvard university, a ph.d. in electrical engineering and computer science from the university of california at berkeley, and honorary science doctorates from the eidgenoessische technische hochschule, zurich, and the university of bologna. dr. lampson holds a number of patents on networks, security, raster printing, and transaction processing. he is a former member of the nrc™s computer science and telecommunications board. he has served on numerous nrc committees, including the committee on high performance computing and communications: status of a major initiative. he is a fellow of the association for computing machinery and the american academy of arts and sciences. he received acm™s software systems award in 1984 for his work on the alto, ieee™s computer pioneer award in 1996, the turing award in 1992, and the von neumann medal in 2001.  robert f. lucas is the director of the computational sciences division of the university of southern california™s information sciences institute (isi). he manages research in computer architecture, vlsi, compilers, and other software tools. prior to joining isi, he was the head of the high performance computing research department in the national energy research scientific computing center (nersc) at lawrence berkeley national laboratory. he oversaw work in scientific data management, visualization, numerical algorithms, and scientific applications. prior to joining nersc, dr. lucas was the deputy director of darpa™s information technology office. he also served as darpa™s program manager for scalable computing systems and dataintensive computing. from 1988 to 1998, he was a member of the research staff of the institute for defense analysis™s center for computing sciences. from 1979 to 1984, he was a member of the technical staff of the hughes aircraft company. dr. lucas received b.s., m.s., and ph.d. degrees in electrical engineering from stanford university in 1980, 1983, and 1988 respectively.  paul messina retired in march 2002 from the california institute of technology (caltech), where he was assistant vice president for scientific computing, director of caltech™s center for advanced computing research, and faculty associate in scientific computing.  he also served as principal investigator for the distributed terascale facility and extensible terascale facility projects at caltech and was coprincipal investigator of the national virtual observatory project. currently, dr. messina is a distinguished senior computer scientist (part time) at argonne national laboratory and a senior advisor on computing to the director general of cern, in geneva. during a leave from caltech from january 1999 to december 2000, he was director of the office of advanced simulation and computing for defense programs in the nnsa at doe. in that capacity he had responsibility for managing the accelerated strategic computing initiative, the world™s largest scientific computing program, which is defining the state of the art in that field. he holds the position of chief architect for the national partnership for advanced computational infrastructure (npaci), a partnership established by the national science foundation and led by the university of california, san diego. his recent interests focus on advanced computer architectures, especially their application to largescale computations in science and engineering. he has also been active in highspeed networks, computer performance evaluation, and petaflop computing issues. prior to his assignment at doe, he led the computational and computer science component of caltech™s research project, funded by the academic strategic alliances program (asap) of the asci (now called the advanced simulation and computing program). in the mid1990s he established and led the scalable i/o the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved.committee member and staff biographies 39  initiative (sio). in the early 1990s, he was the principal investigator and project manager of the casa gigabit network testbed. during that period he also conceived, formed, and led the consortium for concurrent supercomputing, whose 13 members included several federal agencies, the national laboratories, universities, and industry. that consortium created and operated the intel touchstone delta system, which was the world™s most powerful scientific computer for 2 years. he also held a joint appointment at the jet propulsion laboratory as manager of highperformance computing and communications from 1988 to 1998. dr. messina received a ph.d. in mathematics in 1972 and an m.s. in applied mathematics in 1967, both from the university of cincinnati, and a b.a. in mathematics in 1965 from the college of wooster. he is a member of the ieee computer society, the aaas, the acm, the society for industrial and applied mathematics, and sigma xi. he is coauthor of four books on scientific computing and editor of more than a dozen others.  jeffrey m. perloff is a professor of agricultural and resource economics at the university of california at berkeley. his economics research covers industrial organization and antitrust, labor, trade, and econometrics. his textbooks are modern industrial organization (coauthored with dennis carlton) and microeconomics. he has been an editor of industrial relations and associate editor of the american journal of agricultural economics and is an associate editor of the journal of productivity analysis. he has consulted with nonprofit organizations and government agencies (including the federal trade commission and the departments of commerce, justice, and agriculture) on topics ranging from a case of alleged japanese television dumping to the evaluation of social programs. he has also conducted research in psychology. dr. perloff is a fellow of the american agricultural economics association. he received his b.a. in economics from the university of chicago in 1972 and his ph.d. in economics from the massachusetts institute of technology in 1976. he was previously an assistant professor in the department of economics at the university of pennsylvania.  william h. press (nas) is deputy laboratory director for science and technology at the los alamos national laboratory (lanl). before joining lanl in 1998, he was professor of astronomy and physics at harvard university and a member of the theoretical astrophysics group of the harvardsmithsonian center for astrophysics. he is also the coauthor and comaintainer of the numerical recipes series of books on scientific computer programming. dr. press was assistant professor of physics at princeton university and richard chace tolman research fellow in theoretical physics at caltech, where he received a ph.d. in physics in 1972. he is a member of the national academy of sciences and was a founding member of its computer and information sciences section. he has published more than 140 papers in the areas of theoretical astrophysics, cosmology, and computational algorithms. he is also a fellow in the american academy of arts and sciences, a member of the council on foreign relations, and a past recipient of an alfred p. sloan foundation fellowship and the helen b. warner prize of the american astronomical society. dr. press is a past cochair of the commission on physical sciences, mathematics, and applications (cpsma) of the national research council (nrc); a past member of the chief of naval operations™ executive panel, the u.s. defense science board, the nrc™s computer science and telecommunications board, the astronomy and astrophysics survey committee, and a variety of other boards and committees. he has led national studies in subjects including highbandwidth telecommunications (the global grid), national science and technology centers (especially for computational science), and a wide variety of national security issues. dr. press serves as a scientific advisor to the david and lucille packard foundation and other foundations. he is a member of the board of trustees of the institute for defense analyses (ida) and serves on its executive committee and on the external advisory committees of its ccs and ccr divisions. he serves on the defense threat reduction agency™s science and technology panel.  albert semtner is a professor of oceanography at the naval postgraduate school in monterey, california. he received a b.s. in mathematics from caltech and a ph.d. in geophysical fluid dynamics from princeton. his prior professional positions were in ucla™s meteorology department and in the the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved.40 the future of supercomputing: an interim report  climate change research section of the national center for atmospheric research (ncar) in boulder, colorado. his interests are in global ocean and climate modeling and in supercomputing. dr. semtner has written extensive oceanographic codes in assembly language for shipboard use. he produced the first vectorized (fortran) version of a standard ocean model in 1974 and the first parallelvector version (in collaboration with robert chervin of ncar) in 1987. he interacted with los alamos scientists on transitioning the parallelvector code to massively parallel architectures in the early 1990s. under the leadership of warren washington of ncar, he participated in the development of the doe parallel climate model using the los alamos parallel ocean program and a parallel sea ice model from the naval postgraduate school. that climate model has been ported to numerous parallel architectures and used as a workhorse climate model in numerous scientific applications. dr. semtner has been an affiliate scientist with ncar for the last 12 years and simultaneously a member (and usually chair) of the advisory panel to the ncar scientific computing division. he is a winner (with r. chervin) of a 1990 gigaflop achievement award (for the vectorparallel code) and the 1993 computerworldsmithsonian leadership award in breakthrough computational science (for global ocean modeling studies that included ocean eddies for the first time). dr. semtner is an associate editor of ocean modeling and of the journal of climate. he is also a fellow of the american meteorological society.  scott stern graduated with a b.a. degree in economics from new york university. after working for a consulting company in new york, he attended stanford university and received his ph.d. in economics in 1996. from 1995 to 2001, dr. stern was assistant professor of management at the sloan school at mit. since september 2001, he has been an associate professor in the kellogg school of management at northwestern university, a nonresident senior fellow of the brookings institution, and a faculty research fellow of the national bureau of economic research. he is also a coorganizer of the innovation policy and the economy program at the national bureau of economic research and an associate editor of management science. dr. stern explores how innovationšthe production and distribution of ideasšdiffers from the production and distribution of more traditional economic goods and the implications of these differences for both business and public policy. often focusing on the pharmaceutical and biotechnology industries, this research is at the intersection of industrial organization and economics of technological innovation. specifically, recent studies examine the determinants of r&d productivity, the impact of incentives on r&d organization, the mechanisms by which firms earn economic returns from innovation, and the consequences of technological innovation on product market competition. a key conclusion from this research is that translating ideas into competitive advantage requires a distinct and nuanced set of resources and strategies. effective management of innovation therefore requires careful attention to the firm™s internal ability to develop truly distinct technologies and to subtle elements of the firm™s external development and commercialization environment.  shankar subramaniam is a professor of bioengineering, chemistry, and biochemistry and biology and director of the bioinformatics graduate program at the university of california at san diego. he also holds adjunct professorships at the salk institute for biological studies and the san diego supercomputer center. prior to moving to the university of california, san diego, dr. subramaniam was a professor of biophysics, biochemistry, molecular and integrative physiology, chemical engineering, and electrical and computer engineering at the university of illinois at urbanachampaign (uiuc). he was also the director of the bioinformatics and computational biology program at the national center for supercomputing applications and codirector of the w.m. keck center for comparative and functional genomics at uiuc. he is a fellow of the american institute for medical and biological engineering (aimbe) and is a recipient of smithsonian foundation and association of laboratory automation awards. dr. subramaniam has played a key role in raising national awareness of training and research in bioinformatics. he served as a member of the national institutes of health (nih) director™s advisory committee on bioinformatics, which produced the report biomedical information science and technology initiative (bisti). the report recognized the dire need for trained professionals in bioinformatics and recommended the launching of a strong nih funding initiative. dr. subramaniam the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved.committee member and staff biographies 41  serves as the chair of a nih bisti study section. dr. subramaniam has also served on bioinformatics and biotechnology advisory councils for virginia tech, the university of illinois at chicago, and on the scientific advisory board of several biotech and bioinformatics companies. dr. subramaniam served as review panel member of nih cit and his focus was on how cit should respond to the bisti initiative. dr. subramaniam has served as a member of the state of illinois governor™s initiative in biotechnology and as advisor and reviewer of the state of north carolina initiative in biotechnology. dr. subramaniam has published more than a hundred papers in the interdisciplinary areas of chemistry/biophysics/biochemistry/bioinformatics and computer science.  lawrence c. tarbell, jr., is the deputy director of the technology futures office for eagle alliance, a company formed in 2001 by the computer sciences corporation and northrop grumman to outsource part of the it infrastructure (workstations, local area networks, servers, and telephony) for the national security agency (nsa). his particular area of responsibility is it enterprise management, with backup responsibility in distributed computing and storage. mr. tarbell spent the previous 35 years at nsa with responsibilities for research and development of highperformance workstations, networks, computer security, mass storage systems, and systems software. for over 13 years, he managed and led supercomputing research and applications development for nsa, sponsoring highperformance computing and mass storage research (both independently and jointly with darpa and nasa) at many u.s. companies and universities. in 1990, he cochaired frontiers of supercomputing ii, sponsored jointly by nsa and los alamos national laboratory. mr. tarbell received his m.s. in electrical engineering from the university of maryland and his b.s. in electrical engineering (magna cum laude) from louisiana state university.  steven j. wallach (nae) is vice president of technology for chiaro networks, an advisor to centerpoint venture partners, and a consultant to the u.s. department of energy asc program. chiaro networks provides major disruptive technologies in a highend routing platform for reliability, scalability, and flexibility. previously, he was cofounder, chief technology officer, and senior vice president of development of convex computers. after hewlettpackard bought convex, mr. wallach became the chief technology officer of hp™s large systems group. he was a visiting professor at rice university from 1998 to 1999 and manager of advanced development at data general from 1975 to 1981. he was the principal architect of the 32bit eclipse mv supercomputer and, as part of this effort, participated in the design of the mv/6000, mv/8000, and mv/10000 (chronicled in the pulitzer prizewinning book the soul of a new machine, by tracy kidder). mr. wallach was an engineer at raytheon from 1971 to 1975, where he participated in various hardware design efforts, including the computer used to control the launching of the patriot missile system and various signal processors. he had primary responsibility for the design of the all applications digital computer (aadc), which was intended for military specification airborne applications and was made up of gate arrays (one of the first such systems) and a vector instruction set based on apl. mr. wallach holds 33 patents. he was a member of the president™s information technology advisory committee (pitac). he holds a b.s. in engineering from the polytechnic institute of brooklyn, an m.s.e.e. from the university of pennsylvania, and an m.b.a. from boston university.  staff biographies  cynthia a. patterson is a study director and program officer with the computer science and telecommunications board of the national academies.  she is currently involved in a diverse set of cstb projects, including a project on critical information infrastructure protection and the law, a study on the future of supercomputing, and a study on telecommunications research and development. she just completed a project that outlined a research agenda at the intersection of geospatial information and computer science and a joint study with the board on earth sciences and resources (besr) and the the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved.42 the future of supercomputing: an interim report  board on atmospheric sciences and climate (basc) on publicprivate partnerships in the provision of weather and climate services. she has also been involved with the congressionally mandated study on internet searching and the domain name system. prior to joining cstb, ms. patterson completed an m.sc. from the sam nunn school of international affairs at the georgia institute of technology. her graduate work was supported by the department of defense and saic. in a previous life, ms. patterson was employed by ibm as an it consultant for both federal government and private industry clients. her work included application development, database administration, network administration, and project management. she received a b.sc. in computer science from the university of missourirolla.  phil hilliard is a research associate with the computer science and telecommunications board. he provides research support as part of the professional staff and is working on projects focusing on telecommunications research, supercomputing, and dependable systems. before joining the national academies, mr. hilliard worked at bellsouth in atlanta, georgia, as a competitive intelligence analyst and at ncr as a technical writer and trainer. he earned an m.b.a. from georgia state university (2000) in atlanta, georgia, and a b.s. in computer and information technology from georgia institute of technology (1986) in atlanta, georgia. he is currently working on his master™s of library and information science in florida state university™s online program.  margaret huynh, senior project assistant, has been with cstb since january 1999, working on several projects. she is currently working on the future of supercomputing, wireless technology prospects and policy, and internet navigation and the domain name system and worked on the report beyond productivity: information technology, innovation, and creativity. she previously worked on the projects that produced the reports it roadmap to a geospatial future, building a workforce for the information economy, and the digital dilemma: intellectual property in the information age. ms. huynh assisted on the project exploring information technology issues for the behavioral and social sciences (digital divide and democracy). ms. huynh assists on other projects as needed. prior to coming to the national academies, ms. huynh worked as a meeting assistant at management for meetings for 4 months and as a meeting assistant at the american society for civil engineers from september 1996 to april 1998. ms. huynh has a b.a. (1990) in liberal studies with minors in sociology and psychology from salisbury university, salisbury, maryland. the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved. 43 b acronyms       acp advanced cyberinfrastructure program amd advanced micro devices asc advanced simulation and computing [formerly accelerated strategic computing initiative (asci)] asci accelerated strategic computing initiative  cots commercial offtheshelf cstb computer science and telecommunications board  darpa defense advanced research projects agency ddr&e director of defense research and engineering dod department of defense doe department of energy  es earth simulator  gao government accounting office  hpc highperformance computer hpcci high performance computing and communications initiative hpcs high productivity computing systems  ihec integrated highend computing isv independent software vendor itrd information technology research and development  jasons a group of scientific advisors that provides the federal government with largely classified analyses  lanl los alamos national laboratory linpack library of fortran 77 routines for solving problems in numerical linear algebra. it is used for benchmarking the speed of supercomputers.  mpi message passing interface  nih national institutes of health nnsa national nuclear security administration the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved.44 the future of supercomputing: an interim report  nsa national security agency nsb national science board nsf national science foundation  paci partnerships for advanced computational infrastructure pci peripheral component interconnect or interface pitac president™s information technology advisory committee  sfi significant finding issue simd single instruction multiple data smp sharedmemory multiprocessor  the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved. 45 c briefers to the committee       march 67, 2003 washington, d.c.  george cotter, national security agency john crawford, intel robert graybill, darpa john grosh, office of the secretary of defense daniel hitchcock, doe, office of advanced scientific computing research (os) gary hughes, national security agency david kahaner, asian technology information program jacob v. maizel, jr., national cancer institute josé muñoz, doe, office of advanced simulation and computing (asci) clay sell, clerk of the senate subcommittee on energy and water development david turek, ibm   may 2123, 2003 stanford, california  greg astfalk, hewlettpackard gordon bell, microsoft research debra goldfarb, idc james gray, microsoft research john levesque, cray inc. john lewis, boeing scott mcclellan, hewlettpackard william reed, doe, office of advanced simulation and computing (asci) mark seager, lawrence livermore national laboratory burton smith, cray inc. the future of supercomputing: an interim reportcopyright national academy of sciences. all rights reserved. 46 what is cstb? as a part of the national research council, the computer science and telecommunications board (cstb) was established in 1986 to provide independent advice to the federal government on technical and public policy issues relating to computing and communications. composed of leaders from industry and academia, cstb conducts studies of critical national issues and makes recommendations to government, industry, and academia. cstb also provides a neutral meeting ground for consideration of complex issues where resolution and action may be premature. it convenes discussions that bring together principals from the public and private sectors, assuring consideration of key perspectives. the majority of cstb™s work is requested by federal agencies and congress, consistent with its national academies context. a pioneer in framing and analyzing internet policy issues, cstb is unique in its comprehensive scope and its effective, interdisciplinary appraisal of technical, economic, social, and policy issues. beginning with early work in computer and communications security, cyberassurance and information systems trustworthiness have been a crosscutting theme in cstb™s work. cstb has produced several reports known as classics in the field, and it continues to address these topics as they grow in importance. to do its work, cstb draws on some of the best minds in the country and from around the world, inviting experts to participate in its projects as a public service. studies are conducted by balanced committees without direct financial interests in the topics they are addressing. those committees meet, confer electronically, and build analyses through their deliberations. additional expertise is tapped in a rigorous process of review and critique, further enhancing the quality of cstb reports. by engaging groups of principals, cstb gets the facts and insights critical to assessing key issues. the mission of cstb is to  • respond to requests from the government, nonprofit organizations, and private industry for advice on computer and telecommunications issues and from the government for advice on computer and telecommunications systems planning, utilization, and modernization; • monitor and promote the health of the fields of computer science and telecommunications, with attention to issues of human resources, information infrastructure, and societal impacts; • initiate and conduct studies involving computer science, technology, and telecommunications as critical resources; and • foster interaction among the disciplines underlying computing and telecommunications technologies and other fields, at large and within the national academies.  cstb projects address a diverse range of topics affected by the evolution of information technology. recently completed reports include cybersecurity today and tomorrow: pay now or pay later; youth, pornography, and the internet; broadband: bringing home the bits; the digital dilemma: intellectual property in the information age; idsšnot that easy: questions about nationwide identity systems; the internet under crisis conditions: learning from september 11; and it roadmap to a geospatial future. for further information about cstb reports and active projects, see <http://cstb.org>. 
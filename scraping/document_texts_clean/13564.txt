detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/13564for attribution: developing data attribution and citationpractices and standards: summary of an internationalworkshop238 pages | 8.5 x 11 | paperbackisbn 9780309267281 | doi 10.17226/13564paul f. uhlir, rapporteur; board on research data and information; policy andglobal affairs; national research councilfor attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.for attributionš developing data attribution and  citation practices and standardssummary of an international workshopfor attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved. for attributionš developing data attribution and citation practices and standards  summary of an international workshop  paul f. uhlir, rapporteur board on research data and information policy and global affairs for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.the national academies press 500 fifth street, nw washington, dc 20001 notice: the project that is the subject of this report was approved by the governing board of the national research council, whose members are drawn from the councils of the national academy of sciences, the national academy of engineering, and the institute of medicine. the members of the committee responsible for the report were chosen for their special competences and with regard for appropriate balance. this project was supported by the alfred p. sloan foundation under grant no. 2011319, and by the institute of museum and library services under grant no. 1042078. this report was prepared as an account of work sponsored by an agency of the united states government. neither the united states government nor any agency thereof, nor any of their employees, makes any warranty, express or implied, or assumes any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed, or represents that its use would not infringe privately owned rights. reference herein to any specific commercial product, process, or service by trade name, trademark, manufacturer, or otherwise does not necessarily constitute or imply its endorsement, recommendation, or favoring by the united states government or any agency thereof. any opinions, findings, conclusions, or recommendations expressed in this publication are those of the authors and do not necessarily reflect the views of the national academies or the organizations or agencies that provided support for the project. international standard book number13: 9780309267281 international standard book number10: 0309267285  additional copies of this report are available for sale from the national academies press, 500 fifth street, nw, keck 360, washington, dc 20001; (800) 6246242 or (202) 3343313; internet, http://www.nap.edu/. copyright 2012 by the national academy of sciences. all rights reserved.  printed in the united states of america   for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved. the national academy of sciences is a private, nonprofit, selfperpetuating society of distinguished scholars engaged in scientific and engineering research, dedicated to the furtherance of science and technology and to their use for the general welfare. upon the authority of the charter granted to it by the congress in 1863, the academy has a mandate that requires it to advise the federal government on scientific and technical matters. dr. ralph j. cicerone is president of the national academy of sciences. the national academy of engineering was established in 1964, under the charter of the national academy of sciences, as a parallel organization of outstanding engineers. it is autonomous in its administration and in the selection of its members, sharing with the national academy of sciences the responsibility for advising the federal government. the national academy of engineering also sponsors engineering programs aimed at meeting national needs, encourages education and research, and recognizes the superior achievements of engineers. dr. charles m. vest is president of the national academy of engineering. the institute of medicine was established in 1970 by the national academy of sciences to secure the services of eminent members of appropriate professions in the examination of policy matters pertaining to the health of the public. the institute acts under the responsibility given to the national academy of sciences by its congressional charter to be an adviser to the federal government and, upon its own initiative, to identify issues of medical care, research, and education. dr. harvey v. fineberg is president of the institute of medicine. the national research council was organized by the national academy of sciences in 1916 to associate the broad community of science and technology with the academy™s purposes of furthering knowledge and advising the federal government. functioning in accordance with general policies determined by the academy, the council has become the principal operating agency of both the national academy of sciences and the national academy of engineering in providing services to the government, the public, and the scientific and engineering communities. the council is administered jointly by both academies and the institute of medicine. dr. ralph j. cicerone and dr. charles m. vest are chair and vice chair, respectively, of the national research council.  www.nationalacademies.org . for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.v steering committee, developing data attribution and citation practices and standards: an international workshop christine borgman (chair) professor and presidential chair graduate school of education and information studies university of california, los angeles steven jackson assistant professor, school of information, and director, technology policy culture research lab university of michigan gary king albert j. weatherhead, iii. professor, department of government, and director, institute for quantitative social science harvard university david kochalko vice president, business strategy and development, ip & science thomson reuters allen renear associate dean for research university of illinois at urbanachampaign graduate school of library and information science herbert van de sompel research scientist los alamos national lab john wilbanks vice president, creative commons, director, science commons creative commons  project staff at the national academies paul f. uhlir, director, board on research data and information  daniel cohen program officer (on detail from library of congress)  cheryl williams levey senior program associate for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.vi board on research data and information membership (as of the date of this workshop) michael lesk, chair, rutgers university  roberta balstad, vice chair, columbia university  maureen baginski, serco francine berman, rensselaer polytechnic institute r. steven berry, university of chicago  christine borgman, university of california, los angeles norman bradburn, university of chicago bonnie carroll, information international associates michael carroll, american university, washington college of law  paul a. david, stanford institute for economic policy department of economics  barbara entwisle, university of north carolina michael goodchild, university of california, santa barbara alyssa goodman, harvard university margaret hedstrom, university of michigan  michael keller, stanford university  michael r. nelson, georgetown university daniel reed, microsoft research cathy h. wu, university of delaware and georgetown university medical center for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.vii board on research data and information membership (as of the date of this report) francine berman, cochair, rensselaer polytechnic institute clifford lynch, cochair, coalition for networked information laura bartolo, kent state university philip bourne, university of california, san diego henry brady, university of california, berkeley mark brender, geoeye foundation bonnie carroll, information international associates michael carroll, washington college of law, american university sayeed choudhury, johns hopkins university keith clarke, university of california, santa barbara paul david, stanford institute for economic policy research kelvin droegemeier, university of oklahoma clifford duke, ecological society of america barbara entwisle, university of north carolina stephen friend, sage bionetworks margaret hedstrom, university of michigan alexa mccray, harvard medical school alan title, lockheed martin advanced technology center ann wolpert, massachusetts institute of technology ex officio robert chen, columbia university michael clegg, university of california, irvine for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.viii sara graves, university of alabama in huntsville john faundeen, earth resources observation and science center eric kihn, national geophysical data center chris lenhardt, oak ridge national laboratory kathleen robinette, air force research laboratory alex de sherbinin, columbia university  board on research data and information staff paul f. uhlir, board director  subhash kuvelker, senior program officer  daniel cohen, program officer (on detail from library of congress)  cheryl williams levey, senior program associate for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.ix preface and acknowledgments the growth of electronic publishing of literature has created new challenges, such as the need for mechanisms for citing online references in ways that can assure discoverability and retrieval for many years into the future. the growth in online datasets presents related, yet more complex challenges. it depends upon the ability to reliably identify, locate, access, interpret and verify the version, integrity, and provenance of digital datasets. data citation standards and good practices can form the basis for increased incentives, recognition, and rewards for scientific data activities that in many cases are currently lacking in many fields of research. the rapidlyexpanding universe of online digital data holds the promise of allowing peerexamination and review of conclusions or analysis based on experimental or observational data, the integration of data into new forms of scholarly publishing, and the ability for subsequent users to make new and unforeseen uses and analyses of the same data œ either in isolation, or in combination with other datasets.  the problem of citing online data is complicated by the lack of established practices for referring to portions or subsets of data. as funding sources for scientific research have begun to require data management plans as part of their selection and approval processes, it is important that the necessary standards, incentives, and conventions to support data citation, preservation, and accessibility be put into place.  there are, in fact, a number of initiatives in different organizations, countries, and disciplines already underway. an important set of technical and policy approaches have already been launched by the u.s. national information standards organization (niso) and other standards bodies regarding persistent identifiers and online linking. another important group is datacite. the world data system is also focusing on these issues, but other initiatives remain ad hoc and uncoordinated. the workshop summarized here was organized by a steering committee under the national research council™s (nrc™s) board on research data and information, in collaboration with an international codataicsti task group on data citation standards and practices. the purpose of the symposium was to examine a number of key issues related to data identification, attribution, citation and linking, to help coordinate activities in this area internationally, and to promote common practices and standards in the scientific community. more specifically, the statement of task for this project asked the following questions: 1. what is the status of data attribution and citation practices in the natural and social (economic and political) sciences in united states and internationally? 2. why is the attribution and citation of scientific data important and for what types of data? is there substantial variation among disciplines? 3. what are the major scientific, technical, institutional, economic, legal, and sociocultural issues that need to be considered in developing and implementing scientific data citation for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.x standards and practices? which ones are universal for all types of research and which ones are field or context specific? 4. what are some of the options for the successful development and implementation of scientific data citation practices and standards, both across the natural and social sciences and in major contexts of research? the workshop that was organized pursuant to these questions was held in berkeley, ca on august 2223, 2011. the presentations and discussions that are summarized from this meeting in the volume that follows are part of this effort. this report has been prepared by the workshop rapporteur as a factual summary of what occurred at the workshop. the committee™s role was limited to planning and convening the workshop. the views contained in the report are those of the individual workshop participants and do not necessarily represent the views of all workshop participants, the planning committee, or the national academies.  acknowledgments we are grateful to the following for support of this project: institute of museum and library services, grant number imls lg0011012311; sloan foundation, grant number 2011319; the committee on data for science and technology (codata); and microsoft research. any views, findings, conclusions or recommendations expressed in this publication do not necessarily represent those of the institute of museum and library services, or the other sponsors. this report has been reviewed in draft form by individuals chosen for their diverse perspectives and technical expertise, in accordance with procedures approved by the national academies™ report review committee. the purpose of this independent review is to provide candid and critical comments that will assist the institution in making its published report as sound as possible and to ensure that the report meets institutional standards for quality and objectivity. the review comments and draft manuscript remain confidential to protect the integrity of the process. we wish to thank the following individuals for their review of this report: suzanne allard, university of tennessee; anne fitzgerald, queensland university, australia; charles humphrey, university of alberta; brian mcmahon, international union of crystallography, united kingdom; and john rumble, information international associates (retired). although the reviewers listed above have provided many constructive comments and suggestions, they were not asked to endorse the content of the report, nor did they see the final draft before its release. responsibility for the final content of this report rests entirely with the rapporteur and the institution. many people devoted many months of effort to organizing this event. dan cohen and cheryl levey of the staff of the board on research data and infrastructure spent much of their 2011 for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.xi summer working on the workshop project. christine borgman, paul uhlir, and dan cohen had conference calls with each session panel to ensure synthesis and continuity. the workshop was coordinated with the activities of the codataicsti task group on data citation standards and practices, whose cochairs are bonnie carroll, jan brase, and sarah callaghan. members of that task group are (in alphabetical order) micah altman, elisabeth arnaud, christine borgman, dora ann lange canhos, todd carpenter, vishwas chavan, michael diepenbroek, john helly, jianhui li, brian mcmahon, karen morgenroth, yasuhiro murayama, helge sagen, eefke smit, martie van deventer, john wilbanks, and koji zettsu. paul uhlir, dan cohen, and franciel linares are staff consultants to the task group. special thanks also are due to the workshop steering committee, consisting of christine borgman (chair), allen renear, herbert van de sompel, gary king, steven jackson, david kochalko, and john wilbanks, as well as to the young scientists who served as rapporteurs in the final afternoon sessions: franciel linares, matthew mayernick, jillian wallis, and laura wynholds.  christine borgman steering committee chair paul f. uhlir project director for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.xiii contents 1 why are the attribution and citation of scientific data important? ............................................................... 1 christine borgman part one  technical considerations .................................................................................................. 9 2 formal publication of data: an idea whose time has come? ....................................................................... 11 jeanbernard minster 3 attribution and credit: beyond print and citations ......................................................................................... 15 johan bollen 4 data citationštechnical issues šidentification .............................................................................................. 23 herbert van de sompel 5 maintaining the scholarly value chain: authenticity, provenance, and trust .............................................. 31 paul groth discussion by workshop participants .............................................................................................. 35 moderated by john wilbanks part two  disciplinespecific issues .................................................................................................. 41 6 towards data attribution and citation in the life sciences ............................................................................ 43 philip bourne 7 data citation in the earth and physical sciences .............................................................................................. 49 sarah callaghan 8 data citation for the social sciences ................................................................................................................... 55 mary vardigan 9 data citation in the humanities: what's the problem? .................................................................................... 59 michael sperbergmcqueen discussion by workshop participants .............................................................................................. 65 moderated by herbert van de sompel part three  legal, institutional, and sociocultural aspects ..................................... 69 10 three legal mechanisms for sharing data ...................................................................................................... 71 sarah hinchliff pearson 11 institutional perspective on credit systems for research data ..................................................................... 77 mackenzie smith 12 issues of time, credit, and peer review ........................................................................................................... 81 diane harley  for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.xiv discussion by workshop participants .............................................................................................. 89 moderated by paul f. uhlir part four  examples of data citation intitiatives ................................................................ 93 13 the datacite consortium .................................................................................................................................. 95 jan brase 14 data citation in the dataverse network ® ....................................................................................................... 99 micah altman 15 microsoft academic search: an overview and future directions .............................................................. 107 lee dirks 16 data centerlibrary cooperation in data publication in ocean science ................................................... 109 roy lowry 17 data citation mechanism and service for scientific data: defining a framework for biodiversity data publishers ...................................................................................................................................................... 113 vishwas chavan 18 how to cite an earth science dataset? ........................................................................................................... 117 mark parsons 19 citable publications of scientific data ............................................................................................................ 125 john helly 20 the sagecite project ........................................................................................................................................ 131 monica duke discussion by workshop participants ............................................................................................ 137 moderated by david kochalko part five  institutional perspectives ........................................................................................... 141 21 developing data attribution and citation practices and standards: an academic institution perspective ............................................................................................................................................................. 143 deborah l. crawford 22 data citation and data attribution: a view from the data center perspective ........................................ 147 bruce e. wilson 23 roles for libraries in data citation ................................................................................................................ 151 michael witt 24 linking data to publications: towards the execution of papers ................................................................. 157 anita de waard 25 linking, finding, and citing data in astronomy .......................................................................................... 161 michael j. kurtz  for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.xv discussion by workshop participants ............................................................................................ 167 moderated by bonnie carroll 26 standards and data citations .......................................................................................................................... 173 todd carpenter 27 data citation and attribution: a funder™s perspective ............................................................................... 177 sylvia spengler discusssion by workshop participants .......................................................................................... 179 moderated by christine borgman part six summary of breakout sessions ........................................................................................ 187 breakout session on technical issues .................................................................................................................... 189 moderator: martie van deventer rapporteur: franciel linares breakout session on scientific issues .................................................................................................................... 193 moderator: sarah callaghan rapporteur: matthew mayernik breakout session on institutional, financial, legal, and sociocultural issues ................................................. 199 moderator: vishwas chavan rapporteur: laura wynholds breakout session on institutional roles and perspectives ................................................................................... 209 moderator: bonnie carroll rapporteur: jillian wallis appendix a: agenda ............................................................................................................................................... 211 appendix b: speaker and moderator biographical information ....................................................................... 217 for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.1 why are the attribution and citation of scientific data important? christine borgman1 university of california at los angeles introduction my roles as the project chair and as keynoter are to frame the problems to be addressed in two days of discussion. this is a very sophisticated set of speakers and participants. each of you has been concerned with research data, in some way, for some years. by now, all of us are familiar with the data deluge metaphor. we are being drowned in data, much of which is runoff. valuable research data often are not captured, cited, or reused. our challenge is to identify what part of these resources should be kept, the right way to keep them, and the right tools and services to make them useful. data have become a critical focus for scholarly communication, information management, and research policy. we cannot address the full array of these issues, fascinating though they may be. our two days will focus closely on questions of attribution and citation of scientific research data, although we frame scientific broadly enough to include most areas of scholarship. we will devote little time to definitional issues (e.g., what are data?), except to acknowledge that data often exist in the eyes of the beholder. our principal concerns are how to assign credit for data (attribution) and how to reference data (citation) in ways that others can identify, discover, and retrieve them. among the questions to be explored are what a community considers to be data, what data might be shared, what data should be shared, when data can be shared, and in what forms can data be shared? we will consider what approaches may be generic across disciplines and what practices may be fieldspecific. data citation and attribution are not new topics.2 we have had standards for cataloging data files since the 1970s. objects that can be cataloged also can be cited. similarly, data archives have been promoting data citation practices for several decades. however, over this same period, very few journal editors required data citations, disciplines did not instill data citation as a fundamental practice of good research, granting agencies did not reward the data citations of applicants, tenure and reward committees did not recognize data citations in annual performance reviews, and researchers did not take responsibility for citing data sources. what have we learned from the past? what seems to be new today? several developments contribute to the renewed interest in data citation and attribution, all of which are topics of this workshop. one is the growth in data volume relative to storage and analytic capacities. fields such as astronomy, physics, and genomics are producing more data than investigators can investigate themselves. by sharing and combining data from multiple sources, other researchers can ask new questions. another factor is advances in the technical   1 presentation slides are available at: http://sites.nationalacademies.org/pga/brdi/pga064019. 2 thanks to an anonymous reviewer for suggesting a fuller discussion of drivers for data citation and attribution than was included in the oral presentation at the meeting. some of the reviewer™s comments are included in this text. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.2 developing data attribution and citation ractices and standards  infrastructure for generating, managing, analyzing, and distributing data. tools are more sophisticated, bandwidth capacity is greater, and transfer speeds continue to improve. third, and by no means least, are associated shifts in research policy. data are now viewed as significant research products in themselves, more than just adjuncts to publications.3 funding agencies now expect investigators to capture, manage, and share their data. when viewed as research products, data deserve attribution similar to that of publications. attribution, in turn, requires mechanisms for references to be made and citations to be received. yet data are very different entities than publications. they take many more forms, both physical and digital, are far more malleable than publications, and practices vary immensely by individual, by research team, and by research area. institutional practices to assure stewardship of data are far less mature than are practices to sustain access to publications. all of these factors contribute to the complexity of data citation and attribution. it is the many interacting dimensions of data attribution and citation that make it a problem worthy of this workshop and of the multiyear effort with which the meeting is associated. scholarly infrastructure questions of data citation and attribution are best framed in terms of the infrastructure for digital objects. for our purposes, scholarly infrastructure is captured by the eight dimensions of infrastructure identified by susan leigh star and karen ruhleder (1996)4, as mapped in figure 11, taken from bowker, et al (2010)5:  3 borgman, c. l. (2012). the conundrum of sharing research data. journal of the american society for information science and technology, 63(6): 10591078. http://dx.doi.org/10.1002/asi.22634. 4 star, s. l. & ruhleder, k. (1996). steps toward an ecology of infrastructure: design and access for large information spaces. information systems research, 7(1): 111134. 5 bowker, g. c., baker, k., millerand, f., ribes, d., hunsinger, j., klastrup, l. & allen, m. (2010). toward information infrastructure studies: ways of knowing in a networked environment. in hunsinger, j., klastrup, l. & allen, m. (eds.). international handbook of internet research. dordrecht, springer netherlands: 97117.  for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.why are the attribution and citation of scientific data important? 3 figure 11 dimensions of infrastructure. source: bowker, g. c., baker, k., millerand, f., ribes, d., hunsinger, j., klastrup, l. & allen, m. (2010). toward information infrastructure studies: ways of knowing in a networked environment. in hunsinger, j., klastrup, l. & allen, m. (eds.). international handbook of internet research. dordrecht, springer netherlands: 97117.  our presentations will touch upon each part of this model. at the technical edge of the model, infrastructure is the embodiment of standards, which in turn are built on an installed base. among the installed bases that influence data citation are internet protocols, publishing practices, and library cataloging methods. at the social end, infrastructure is linked to conventions of practiceœwhether cataloging or data managementœand learned as part of membership in a community (e.g., librarians or astronomers). a social topic of particular interest is the relationship of reward systems to data citation. at the local edge of the model are individual practices for managing data and library practices for data stewardship. the global edge represents the inherently international character of scientific scholarship. data practices, data exchange, and citation and attribution all must work effectively across political, institutional, and disciplinary boundaries. data in the globaltechnical quadrant of figure 11 are most amenable to automated capture, management, and discovery. these are data, for example, from shared instruments such as spacebased telescopes, and are associated with established data structures, analytical tools, and repositories. it is these types of data that are most readily cited. conversely, data in the localsocial quadrant tend to be more heterogeneous in form and content, more artisanal in data for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.4 developing data attribution and citation ractices and standards collection methods, and more varied in practices for management, use, and reuse. these data are much less amenable to established methods of data ingest, stewardship, citation, and attribution. the infrastructure for digital objects has many features. we are concerned at this workshop with how they apply to data attribution and citation, but we must remember that they are part of a larger internet architecture of digital objects. the list of features below, around which the rest of my presentation is organized, is neither exhaustive nor mutually exclusive. rather, it is a useful starting point to assess how these infrastructure features are applied to data citation and attribution: social practice  usability identity  persistence discoverability  provenance relationships intellectual property  policy social practice among the drivers for this workshop are: renewed interest in data citation due to increases in data volume, to advances in technical infrastructure, and to shifts in research policy associated with data. these developments still beg the questions of why data should be attributed and cited. those questions have at least as many answers as there are persons attending this event. at the highest level, most of these answers can be grouped into categories of reproducing research, replicating findings, or more generally, to reuse data. to reuse data, it is necessary to determine what useful data exist, where, in what form, and how to get them. in turn, data must be described in some way if they are to be discoverable. for people to invest effort in making data discoverable, they should receive credit for creating, cleaning, analyzing, sharing, and otherwise making data available and useful. to get credit, some means must exist to associate names of individuals and organizations with specific units of data. this project is titled with the awkward phrase ﬁdeveloping data attribution and citation practices and standardsﬂ to make the point that these are not equivalent concepts. the distinction is both subtle and important. attribution is made to the responsible party. attribution might thus be given to an individual investigator, to a research team, to a university, to a funding agency, to a data repository, to a library, or to another party responsible for gathering, assembling, curating, or otherwise contributing to the availability of data for others to use. attribution is more closely associated with the notion of contribution, or contributor, than with author, which is among the differences between handling data and handling publications. like publications, however, attribution implies social responsibility to give credit where credit is due. when we write journal articles and books, we reference other publications and the evidence on which they are based to attribute our sources. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.why are the attribution and citation of scientific data important? 5 citation, in contrast, is the mechanism by which one makes references to other entities. in bibliometric parlance, references are made and citations are received. even in the bibliographic world, reference/citation formats are many and varied: the american psychological association standard is popular in the social sciences, the modern language association standard in the humanities, the association for computing machinery in computer science and engineering, and the blue book in law, for example. these standards vary by the units they reference (e.g., full publications or individual pages), presentation (e.g., numerical references to the bibliography or author names and dates in text), choice of data elements (e.g., author, title, date, volume, issue, page numbers, doi, url, legal jurisdiction), and other factors. the multiplicity of bibliographic standards reflects the diversity of practices within and between research areas. none of them map easily to data or datasets, which have yet more diversity in form and practice. usability data citation and attribution must be considered in the context of the usability of data as digital objects. while data in the form of physical objects (e.g., samples, artifacts, lab notebooks on paper) also must be referenced, digital descriptions of those objects typically serve as surrogates. among the actions people œ or machines œ may wish to perform on digital objects are to interpret, evaluate, open, read, compute upon, reuse, combine, describe, and annotate. this incomplete list suggests the range of capabilities that must be accommodated by a successful system for citing and attributing data. identity to be citable and attributable, data must be identified uniquely. identity and identification are well known problems in computer science and in epistemology. our speakers on these topics bring those fields to bear on the question of identity for units of data. identity is complex when we think in terms of people reading books and reading data. humans can disambiguate similar objects, such as different editions of a book. identity questions are even more complex when computers are discovering, reading, and interpreting data. identity also is closely intertwined with usability and with trust. among the questions to ask are: what are the dimensions of data identity? what identity levels are necessary to open, to interpret, to read, to compute upon, to combine, and to trust data as digital objects? an effective set of identity mechanisms for data citation and attribution must incorporate a trust fabric. persistence the next session in this workshop is on identity and persistence of digital objects. identity and persistence tend to be more concerned with containers of the data than about the data per se œ how we package, name, and label data will influence the ability to identify them, to ensure they persist, or to dispose of them accurately. the data may exist, but unless we have labeled them and stored them in a place to which others can return, their usability will be negatively affected. a variety of persistent identifier systems already exist, including uniform resource identifiers (uri), digital objects identifiers (dois) and other types of handles, and other namespaces. while all are useful, none addresses all of the needs for data identity and persistence. much remains to be learned about which systems are best, for which types of data, and for what purposes. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.6 developing data attribution and citation ractices and standards  discoverability discoverability is a broad topic, most researched in information retrieval. for the purposes of data citation and attribution, discoverability is the ability to determine the existence of a set of data objects with specified attributes or characteristics. the attributes of interest include the producer of the data, the date of production, the method of production, a description of an object™s contents, and its representation. discoverability may also include aspects such as levels of quality, certification, and validation by third parties. discoverability depends both on the description and representation of data and on tools and services to search for data objects. description and representation usually take the form of metadata, some of which may be automated if data are generated by instruments such as sensor networks or telescopes. even for these types of data, metadata creation may require considerable human effort, making it an expensive process that is often avoided by researchers. human intervention is necessary to add metadata and description to most other kinds of data. as data move from one place to the next, those metadata may be augmented incrementally. unlabeled bits are equivalent to books shorn of their covers and title pages. data generally are discoverable via the metadata that describe them. a variety of approaches to discovery are possible. web search engines are one possibility, assuming that data descriptions are reachable via standard web protocols. with the introduction of semantic web technologies and associated search engines, location of datasets of interest based on semantic content becomes possible. alternatively, more disciplinespecific and structured catalogs can be created. data are discoverable only as long as someone keeps them, somewhere. library and archival practice tends toward saving forever anything that is worth saving, although both professions also have long histories of weeding collections and of scheduling record disposal. individual investigators are less likely, and less able, to maintain data permanently for discovery at some unknown later date. discoverability is thus associated with economics, a topic largely beyond the scope of this meeting. many research libraries and archives view data as important special collections, but also are concerned that data stewardship is an unfunded mandate. data retention schedules will influence data discovery. some data may be discoverable only in the short term, such as a scratch space for other people to use. other data will be kept at least until the associated reports are published, and for some time thereafter. yet other data will be ﬁlonglived,ﬂ usually defined long enough to be concerned about migration from one format to the next6,7. data citation and attribution practices and standards may vary considerably depending on the period of time data are expected to remain available. considerations also will vary between raw data, observations, models, physical samples, the predicted life span of utility, and many other factors.   6 reference model for an open archival information system (2002). recommendation for space data system standards: consultative committee for space data systems secretariat, program integration division (code m3), national aeronautics and space administration. http://public.ccsds.org/publications/archive/650x0b1.pdf. 7 longlived digital data collections. (2005). national science board. http://www.nsf.gov/pubs/2005/nsb0540/. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.why are the attribution and citation of scientific data important? 7 provenance provenance is particularly important for data citation. in citing data, it is important to reference the correct version, and where possible, to cite prior states of data and the transformations made between states. provenance was once the exclusive concern of museum curators, archivists, and forensic specialists, all of whom view provenance as the chain of custody of an object. for example, the getty museum trusts the authenticity of an artwork only if the custody of that object can be documented at all steps since its origin. this linear model of provenance is less applicable to digital objects. in computing, provenance is the ability to track all transformations from the original state. data provenance is becoming an active research area, and one to which we devote a substantial time at this workshop. relationships while data can be discrete digital objects, they usually are related to other objects such as publications. often multiple types of data have relationships to each other, providing context, calibration, and comparisons. data citation and attribution mechanisms thus must facilitate linking of related objects and be able to refer to groups of objects, as well as to individual items. the choice of units for reference is a particularly contentious topic in data citation and attribution. when does citing a dataset associated with a journal article provide sufficient granularity? when is it necessary to cite each observation, each cell in a table, or each point on a graph? identifying units, relationships among units, and types of relationships are all aspects of data citation and attribution. intellectual property intellectual property is a broad topic even if confined to scientific data. the discussion at this meeting on intellectual property will focus on rights associated with data, such as the rights to use, reuse, combine, publish, and republish. discovering data is but a first step. once discovered and retrieved, users need to be able to identify what rights are associated with those data. for example, may we use the data for commercial purposes? may we share them with others? may we use them for teaching? research teams, especially small teams, may not have documented ownership or rights associated with their data. until data came to be viewed as valuable research products, ownership was an issue rarely discussed. data often are not shared for the simple reason that it is not possible to determine who in a collaborative project has the rights to release them. open access, albeit an overused term with many meanings, has sensitized researchers to the value of making their research products available. from an intellectual property perspective, making a reference to data should be no different than a reference to a book or published paper. including bibliographies in published works does not violate the copyright of the works cited. rather, the bibliography is the form of attribution most central to scholarly practice.  policy both data citation and attribution have policy components. many stakeholders are concerned with scholarly information policy, including funding agencies, publishers, data repositories, universities, investigators, and students. each has policy concerns, thus we must ask what policy, for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.8 developing data attribution and citation ractices and standards  what kinds of policy, and whose policy? data management plan requirements and data sharing policies are case examples. many funding agencies have established such policies, the specifics of which vary widely between the national science foundation and the national institutes of health in the united states, the wellcome trust and the economic and social research council in the united kingdom, and others in the u. k., the european union, and asia. these requirements may evolve to become more explicit about who is to receive what kinds of attribution for what kinds of data contributions, and how such contributions are to be cited. workshop themes all of these infrastructure issues, and more, will be explored in our program. the two days of the workshop are organized around these driving questions from the project™s task statement (emphasis added): 1. what are the major technical issues that need to be considered in developing and implementing scientific data citation standards and practices? 2. what are the major scientific issues that need to be considered in developing and implementing scientific data citation standards and practices? which ones are universal for all types of research and which ones are field or context specific? 3. what are the major institutional, financial, legal, and sociocultural issues that need to be considered in developing and implementing scientific data citation standards and practices? which ones are universal for all types of research and which ones are field or contextspecific? 4. what is the status of data attribution and citation practices in individual fields in the natural and social (economic and political) sciences in the united states and internationally? provide case studies. 5. institutional roles and perspectives: what are the respective roles and approaches of the main actors in the research enterprise and what are the similarities and differences in disciplines and countries? the roles of research funders, universities, data centers, libraries, scientific societies, and publishers will be explored. next steps this summary report of the workshop, published by the national academy of sciences™ board on research data and information, is the first formal product of the overall initiative on data attribution and citation. the codataicsti task group on data citation standards and practices is conducting a survey and literature review, and gathering other materials for an international white paper. task group members are giving presentations at international meetings over the next several years. future efforts of this task group are expected to lead to standardization work. these efforts also will continue via the participants™ dissemination of the ideas generated here. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.9 part one technical considerations for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.11 2 formal publication of data: an idea whose time has come? jeanbernard minster1 university of california at san diego every time i participate in a discussion on data citation and attribution or talk to colleagues who deal with a lot of data, the issue of data publication comes up. the point is that the whole idea of citation is difficult to discuss in the absence of the concept of publication. the idea of longterm data preservation, citation, and publication is a concept that is growing in the community. in my scientific union, the american geophysical union (agu), there is a statement on data publication that reads: the cost of collecting, processing, validating, and submitting data to a recognized archive should be an integral part of research and operational programs. such archives should be adequately supported with longterm funding. organizations and individuals charged with coping with the explosive growth of earth and space digital data sets should develop and offer tools to permit fast discovery and efficient extraction of online data, manually and automatically, thereby increasing their user base. the scientific community should recognize the professional value of such activities by endorsing the concept of publication of data, to be credited and cited like the products of any other scientific activity, and encouraging peerreview of such publications.2 if you look at the literature, figure 21 from the paper by hilbert and lopez shows the growth in total information. what is amazing is that between 1986 and 2007, everything having to do with vinyl (analog) has disappeared and everything that is now digital becomes completely dominated by pcs. most of the data we have now are on people's pcs. the growth has been quite constant. so, for any conclusion that you draw from a study from 1986 to 2007, you probably have to scale those estimates upward considerably, in order to assess the situation today accurately.  1 presentation slides are vailable at http://sites.nationalacademies.org/pga/brdi/pga064019. 2 ﬁthe importance of longterm preservation and accessibility of geophysical dataﬂ agu, may 2009. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.12 developing data attribution and citation practices and standards  figure 21 growth in total information. source: hilbert, m. & lopez, p. (2011). the world's technological capacity to store, communicate, and compute information. science, 332, 6065 [doi: 10.1126/science.1200970].  the storage capacity globally is shrinking in relation to amount of information. data compression technologies show some promise in addressing the mismatch between our growing storage needs and the available capacities. lossless3 compression strategies have already been deployed in many data centers, but the compression ratios they can provide are fairly modest for many types of data. existing lossy compression methods (i.e., compression algorithms that achieve greater compression ratios at the cost of some degradation to the quality of the original data), such as those now available for digital images and video, are problematic for some kinds of data because we do not know what information may be important to future researchers.   3 lossless compression algorithms restore 100 percent of the original data upon decompression. they achieve compression by techniques such as representing strings of repeated instances of the same character with a single instance plus additional characters indicating the number of repetitions in the original. the files compressed using lossless compression may require the use of a decompression algorithm in order to be read by the application that created them, but once decompressed, the resulting file is essentially identical to the original. lossy compression algorithms achieve greater compression ratios at the cost of the loss of some portion of the information contained in the original. for example, lossy compression techniques may reduce the color depth or resolution of a graphical image, may use a lower sampling rate of audio content, or may preserve only the delta between frames of a video sequence rather than the entirety of all the frames. the compressed result is an approximation of the original that is ﬁgood enoughﬂ for many purposes, but once so compressed, the content cannot be restored to the quality of the original prior to compression. for some types of content, lossy compression techniques can achieve dramatically higher compression ratios than lossless techniques, but carry the risk that something lost in the compression process may be important for a future use perhaps not contemplated at the time. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.formal publication of data: an idea whose time has come 13 consequently, data center managers are reluctant to use these methods and continue to rely instead upon the continued expansion of physical storage capacity. we have to find a way of saving the materials that are worth saving and this can be achieved through the process of publication. we all have enormous file cabinets in our offices, but the information that is published is really what gets preserved for a long time. the problem of how to deal with the growing deficit in storage capacity is beyond the scope of this workshop, but it is worth noting that citation to data has little value if the data being cited are not preserved and accessible for however long they may be needed. this whole idea of data publication, citation, and attribution is a very current concept. however, some best practices and critical research needs are beginning to emerge. it is also getting increasing attention from the scientific community. for example, there was a whole session on these topics at the codata conference in october 2010 in cape town, south africa. also, another session will be devoted to these issues at the world data systems science symposium in kyoto, japan in september 2011. the international council for science (icsu) envisions a global world data system (wds) that will:  emphasize the critical importance of data in global science activities,  further icsu strategic scientific outcomes by addressing pressing societal needs (e.g., sustainable development, the digital divide). highlight the very positive impact of universal and equitable access to data and information.  support services for longterm stewardship of data and information. promote and support data publication and citation.  the maturity of the development of these practices is not uniform across fields and disciplines, however. in crystallography, for example, you do not get credit for your work unless you publish your data and it has to be published in certain formats. the field has procedures and protocols. this is an example of a discipline that is very well organized. it is not the same in other fields, although the technology is available. the wds faces certain challenges however in order to accommodate at the same time giant data facilities, such as the nasa distributed active archive centers or noaa national data centers, and very small facilities such as the wdc for earth tides, the same model will not work equally well. similarly, the international global navigation satellite system, which involves an enormous projected data flow, will function according to a certain model, but the very small international data services, such as those for the glaciological or the solar physics communities for example, will function in a very different way. not all wds members are capable of providing all the necessary infrastructure components identified here. consequently, the wds scientific committee realized that one type of membership was inadequate. it created four separate types of memberships, described in some detail on the wds website. so far, only ﬁregularﬂ members have the mandate to provide a ﬁsecure repositoryﬂ function. however, the definition of wds member roles is still a work in progress. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.14 developing data attribution and citation practices and standards  so what is the purpose of data citation? it is, as i see it, to give credit and make authors accountable, and to aid in the reproducibility of science. this is a way we could cite data: cline, d., r. armstrong, r. davis, k. elder, and g. liston. 2002, updated july 2004. clpxground: isa snow pit measurements. edited by m. parsons and m. j. brodzik. boulder, co: national snow and ice data center. data set accessed 20080514 at http://nsidc.org/data/nsidc0176.html. in this example, we have a description of a dataset. it shows the proper citation of certain data out of the total number of entries, who is responsible for the dataset, who edited it, what was the location, and when it was last accessed online. the latter element may be important for some continuously changing datasets (e.g., timeseries weather records); it is often much less important than a specific version number or revision date of the dataset. this, of course, assumes that the data ﬁpublisherﬂ both maintains a clear history and can provide access to specific revisions of the dataset. while it is not difficult to specify these elements for a data citation, even this fairly simple citation format received negative feedback from some researchers in my field. some of my colleagues and students said: "we cannot possibility remember all those things. it is just too hard." this suggests that, at least in some fields and disciplines, the cultural challenges may be greater than the technical ones. let me conclude with what i think is needed:  data collection coupled with quality control quality assurance (a function of the data)  peer review ascertaining the authoritative source, assessed data  ease of publication easily understood standards (especially metadata)  simple steps to place data in the public domain (e.g., the polar information commons)  secure repository and longterm data curation  preferred use of this reliable source by data users preservation of longterm data time series  repositories that adapt to evolving technology collaboration with libraries and the publishing communities  ease of citation credit given to data authors and proper recognition and citation by users professional recognition (besides credit) perhaps a change in academic mindset. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.15 3 attribution and credit: beyond print and citations johan bollen1 indiana university the main focus of my work is not on citations, let alone data citations, but on computational methods to study scientific communication by analyzing very largescale usage data. this is quite different from citation data, but how we organize and analyze our data is probably a useful and worthwhile perspective to contribute here. when researchers talk about data citations, the assumption is that a citation has value. but why is it valuable? it is valuable because it defines a notion of credit and attribution in scientific communication. it is the mechanism by which one author explicitly indicates that he or she has been influenced by the thinking or the work of another author. citations are very strongly grounded in the tradition of printed scientific paper, but we are thinking about data now, and data is much more difficult to cite in that context. the main problem here is that technology has fundamentally changed scholarly communication, and in fact even how scholars think, but scholarly review and assessment are still stuck in the paper era (e.g., peer review, print, citations, journals) that we have known since the late 19th century. however, if you look at how scholarly communication has been evolving over just the past 10 to 15 years, most of it has moved online. most of my colleagues are on twitter and facebook now. one of the ways that they communicate their science is by posting tweets that make references to their papers and data. in other words, the way they publish has fundamentally changed. this is also true for my own experience. when i write a paper the first thing i do is to deposit it in my web site or in an archive. the community then finds its way to my paper and if people find errors, they will provide extensive feedback, through, for example, a blog post. so, in addition to publishing my papers online, they are also ﬁpeer reviewedﬂ online. the whole notion here is that the entire spectrum of scholarly communication is moving online. before, it seemed to be occurring mostly within the confines of the traditional publication system. if you look at scholarly assessment, however, it seems like it has skipped that evolution nearly entirely. therefore, i think that we need to talk about changing scholarly assessment beyond the traditional way of doing things, to systems that can actually keep up with the changes in the scholarly communication process. figure 31 shows that publication data and citation data are the endproduct of a long chain of scholarly activities. usage data can be harvested for each of the antecedent activities, such as when authors read the scholarly literature as part of their research, submission, and peer review process.  1 presentation slides are available at http://sites.nationalacademies.org/pga/brdi/pga064019. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.16 developing data attribution and citation practices and standards  figure 31 data for assessing scholarly communication.  for that reason we have looked at applications of usage data for scholarly assessment. the main promise of usage data is that it can be recorded for all digital scholarly content (e.g., papers, journals, preprints, blog postings, data, chemical structures, software), not just for 10,000 journals and not only for peerreviewed, scholarly articles. it provides extensive information on types of user behavior, sequences, timing, context, and clickstreams. it also reflects the behavior of all users of scholarly information (e.g., students, practitioners, and scholars in domains with different citation practices). furthermore, interactions are recorded starting immediately after publication; that is, the data can reflect realtime changes (see figure 31). finally, usage data offers very largescale indicators of relevancešbillions of interactions recorded for millions of users by tens of thousands of scholarly communication services. however, there are significant challenges with usage data. these include: (1) representativeness: usage data is generally recorded by a particular service for its particular user community. to make usage data representative of the general scholarly community, i.e. beyond the user base of a single service, we must find ways to aggregate usage data across many different services and user communities. (2) attribution and credit: a citation is an explicit, intentional expression of influence, i.e., authors are explicitly acknowledging which works influence their own. usage data constitutes a behavioral, implicit measurement of how much ﬁattentionﬂ a particular scholarly communication item has garnered. the challenge is thus to turn this type of behavior, implicit, clickstream data into metrics reflecting actual scholarly influence. (3) community acceptance: whereas an entire infrastructure is now devoted to the collation, aggregation and disposition of citation data and statistics, usage data remains largely unproven in terms of scholarly impact metrics or services, due to a lack of applications and community services. the challenge here is to create a framework to aggregate, collate, normalize, and process usage data that the community can trust and from which we can derive trusted metrics and indicators. enter the metrics from scholarly usage of resources (mesur) project! the mesur project was funded by the andrew w. mellon foundation in 2006 to study science itself from largescale usage data. the project was involved with largescale usage data acquisition, deriving for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.attribution and credit: beyond print and citations 17 structural models of scholarly influence from said usage data, and surveying a range of impact metrics from the usage and citation it collected.  figure 32 modeling the scholarly communication process š the mesur ontology.2 so far, the mesur project has collected more than one billion usage events3 from publishers, aggregators and institutions serving the scientific community. these include: biomedcentral, blackwell, the university of california, ebsco publishing, elsevier, emerald, ingenta, jstor, the los alamos national laboratory, zetoc project of the university of manchester, thomson, the university of pennsylvania, and the university of texas. this data provided to the project has to conform to specific requirements, which were fortunately met by all our providers. in particular, we required that the data had an anonymous but unique user identifier, unique document identifiers, data and time of the user request to the second, an indicator of the type of request, and a session identifier, generated by the provider™s server, which indicates whether the same user accesses other documents within the same session. the latter is an important element of the mesur approach. we are not just interested in total downloads, but their context, the structural features of how people access scholarly communication items over time. we therefore required session identifiers, meaning that if users access a document at a particular time, they are assigned a session identifier before they move on to the next document. they maintain this session identifier throughout their movement from one   2 marko a rodriguez, johan bollen and herbert van de sompel. a practical ontology for the largescale modeling of scholarly artifacts and their usage, in proceedings of the joint conference on digital libraries 2007, vancouver, june 2007. 3 data from more than 110,000 journals, newspapers and magazines, along with publisherprovided usage reports covering more than 2,000 institutions, is being ingested and normalized in mesur's databases, resulting in largescale, longitudinal maps of the scholarly community and a survey of more than 40 different metrics of scholarly impact.  for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.18 developing data attribution and citation practices and standards  document to the next. as a result we can reconstruct socalled clickstreams and model how people move from one document to the next in any particular session. because we have that kind of data, we can track how users collectively move from one article or journal to the next, and map the collective flow of ﬁscientific traffic.ﬂ such a map is shown in figure 33 and was published in plos one in 2009.4 figure 33 visualization of mesur clickstream data showing how users move from one journal to the next in their online access behavior. each circle represents a journal. journals are connected by a line if they frequently cooccur in user clickstreams. looking at the map we can see a rich tapestry of scholarly connections woven by the collective actions of users who express their interests in the sequence by which they move from one article and journal to the next in their online explorations. although from our data we cannot prove that any individual user actually followed a certain path, we can say that it reflects the fact that users   4 bollen j, van de sompel h, hagberg a, bettencourt l, chute r, et al. (2009) clickstream data yields highresolution maps of science. plos one 4(3): e4803. doi:10.1371/journal.pone.0004803. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.attribution and credit: beyond print and citations 19 collectively felt these journals are related somehow, leading to the formation of clusters of interests which do not always coincide with traditional domain classifications, cf., the position of psychology journals in this map. once we have derived a network structure of related journals from usage data, as shown in figure 33 we can use it to perform the same kind of scholarly assessment analysis that is now commonly conducted on the basis of citation data, and the resulting citation networks. we can actually calculate how important a journal is to the structure of the network, and use it as a measure of scholarly influence or impact. this is what the mesur project has done. we surveyed nearly forty different impact metrics, most based on social network analysis. we calculated one half of the metrics from our usage network, and the other half from a citation network that we derived from the journal citation reports. most usagebased network metrics had a citationbased counterpart. we also added several existing citationbased metrics that are not necessarily based on a citation network, such as the journal™s hindex and its impact factor. each of these metrics, depending on whether they were based on usage data or citation data, and their method of calculation, will reflect a different perspective of scholarly impact in the journal rankings it produces. for example, some metrics will indicate how centrally located a journal is in the usage network and serve as an indication of its general impact according to patterns of journal usage. we can also calculate a journal™s ﬁbetweenness centrality,ﬂ i.e., how often users or citations pass through the particular journal on their way to another journal from another one. this may be construed as an indication of the journal™s interdisciplinary nature, its ability to bridge different areas and domains of interest in the usage and citation network vs. how popular or well connected it is in general. each metric by virtue of its definition will have something different to say about a journal™s scholarly impact, and can furthermore be calculated from either usage networks or citation networks, offering even more perspectives on the complex notion of scholarly impact. a comparison of all of these metrics was published in plos one in 2009, and yielded a model of the main dimensions along which scholarly impact can fluctuate.5 we are also working on a number of online services to make our results accessible to the public. as mentioned, the problem with this kind of usage data is that people have a hard time accepting its validity since citation data is so ingrained. usually, i get arguments such as ﬁyou may have nice results, but i don't believe it.ﬂ a public, open, freely available service will allow people to play with the data and results themselves and might make them more community accepted. finally, i want to mention that we secured new funding in 2010 from the andew w. mellon foundation to develop a generalized and sustainable framework for a public, open, scholarly assessment service based on aggregated largescale usage data, which will support the evolution of the mesur project to a communitysupported, sustainable scholarly assessment framework. this new phase of the project will focus on four areas in developing the sustainability model:   5 bollen j, van de sompel h, hagberg a, chute r (2009) a principal component analysis of 39 scientific impact measures. plos one4(6): e6022. doi:10.1371/journal.pone.0006022.  for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.20 developing data attribution and citation practices and standards  financial sustainability, legal frameworks for protecting data privacy, technical infrastructure and data exchange, and scholarly impact. it will integrate these four areas to provide the mesur project with a framework upon which to build a sustainable structure for deriving valid metrics for assessing scholarly impact based on usage data. simultaneously, mesur's ongoing operations will be continued with the grant funding and expanded to ingest additional data and update its present set of scholarly impact indicators. i would like to end my presentation by highlighting the following interesting initiatives and some relevant publications. initiatives:  microsoft/msr: http://academic.research.microsoft.com/  altmetrics:http://altmetrics.org/  mendeleybased analytics: using mendeley™s bookmarking and reading data to rank articles.  publisherdriven initiatives: elsevier™s scival , mapping of science: http://www.elsevier.com/wps/find/authorednewsitem.cwshome/companynews0501743 google scholar : http://scholar.google.com/  science of science cyberinfrastructure: http://sci.slis.indiana.edu/ (katy borner at indiana university) relevant publications by the presenter:  johan bollen, herbert van de sompel, aric hagberg, luis bettencourt, ryan chute, marko a. rodriguez, lyudmila balakireva. clickstream data yields highresolution maps of science. plos one, march 2009. johan bollen, herbert van de sompel, aric hagberg, ryan chute. a principal component analysis of 39 scientiþc impact measures. arxiv.org/abs/0902.2183 johan bollen, marko a. rodriguez, and herbert van de sompel. journal status. scientometrics, 69(3), december 2006 (arxiv.org:cs.dl/0601030)  johan bollen, herbert van de sompel, and marko a. rodriguez. towards usagebased impact metrics: first results from the mesur project. in proceedings of the joint conference on digital libraries, pittsburgh, june 2008.  marko a. rodriguez, johan bollen and herbert van de sompel. a practical ontology for the largescale modeling of scholarly artifacts and their usage, in proceedings of the joint conference on digital libraries, vancouver, june 2007. johan bollen and herbert van de sompel. usage impact factor: the effects of sample characteristics on usagebased impact metrics. (cs.dl/0610154) johan bollen and herbert van de sompel. an architecture for the aggregation and analysis of scholarly usage data. in joint conference on digital libraries (jcdl2006), pp. 298307, june 2006. johan bollen and herbert van de sompel. mapping the structure of science through usage. scientometrics, 69(2), 2006. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.attribution and credit: beyond print and citations 21 johan bollen, herbert van de sompel, joan smith, and rick luce. toward alternative metrics of journal impact: a comparison of download and citation data. information processing and management, 41(6): 14191440, 2005.  for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.23 4 data citation štechnical issuesš identification herbert van de sompel1 los alamos national laboratory i am going to speak today about a slightly narrow topic, which is about the identification of data as part of the citation process. i will also talk about different use cases, such as assigning credit, accessing and reusing the data, and involving both humans and machines. to make clear what i am talking about, i will give some alternate examples of a citation: bollen, j., van de sompel, h., hagberg, a., chute, r. a principal component analysis of 39 scientific impact measures. plos one, 4(6), pp. e6022, 2009. doi:10.1371/journal.pone/0006022 bollen, j., van de sompel, h., hagberg, a., chute, r. a principal component analysis of 39 scientific impact measures. plos one, 4(6), pp. e6022, 2009. doi:10.1371/journal.pone/0006022 http://dx.doi.org/10.1371/journal.pone/0006022 bollen, j., van de sompel, h., hagberg, a., chute, r. a principal component analysis of 39 scientific impact measures. plos one, 4(6), pp. e6022, 2009. http://dx.doi.org/10.1371/ journal.pone/0006022 these are the three considerations that merit more attention:  the nature of identifiers for citation, access, and reuse.  catering to human and machine agents.  granularity, both spatial and temporal. first, let us examine the nature of identifiers for these different use cases:  figure 41 data publicationšdata citation   1 presentation slides are available at http://sites.nationalacademies.org/pga/brdi/pga064019. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.24 developing data attribution and citation practices and standards  at the lefthand side, we have someone who is sharing data (e.g., data generation, data use, and data publication) and, at the righthand side, there is someone who wants to do something with the data (e.g., access the data, reuse the data, and cite the data). the first consideration is about the nature of the identifier. we have these two parties and in order for the consumer to access, use and cite the data, some information needs to be made available by provider. i am going to distinguish between identifiers that enable citation and identifiers that enable access and reuse.  figure 42 identifiers that enable citation, access, and reuse. if you look at the identifiers that enable citation, we can think of two choices. we have identifiers like the dois that are used extensively, but we also have the choice for a http uri. moreover, an http uri could be based on a doi (i.e., the http version of a doi) or it could be any other stable, cool http uri. figure 43 actionable and nonactionable identifiers for citation, access, reuse.  for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.data citationštechnical issuesšidentification 25 when thinking about identifiers for enabling accessing and reuse, there is only one choice because the focus with this regard is on access. access means an identifier that is actionable using widely deployed technologies such as web browsers, web crawlers, etc. this yields the use of http uris for access. so, dois as such can be used for citation, whereas cool http uris (including the http version of a doi) can be used for citation, access, and reuse. this was the first consideration. figure 44 cool http uris can be used for citation, access, and reuse. the second consideration is about catering to human and machine agents when talking about accessing and reusing data. when it comes to papers, the typical enduser is a person. however, when it comes to data, the most important consumer most likely is going to be a machine. it is not clear at this point how machine agents will be enabled to access and reuse the data. this means that we need to think carefully about aspects such as links, metadata, and discovery measures to cater to machines. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.26 developing data attribution and citation practices and standards  figure 45 data access and reuse by both human and machine agents.  as you know it today, we have a uri that sits somewhere in a citation and these are the reference data sheets for humans and machines. catering to human and machine agents open human actionable splash page open machine actionable splash page figure 46 splash pages for humans and machines. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.data citationštechnical issuesšidentification 27 when it comes to machine processing, there is some critical thinking that needs to be done about what has to be involved there. this is not only about discovery of the data or metadata that supports discovery of the data. this is about metadata that supports understanding of the data and interacting with the data in automated ways. for example, for any dataset, there can be several uris involved, including uris for the splash pages (one for human and one for machine agents) and uris for multiple components of the dataset. maybe there are different formats of the same data available, in which case there are even more uris and the need to express a format relationship. we have to distinguish between all those uris and make sure that it is clear to the machine what each uri stands for. we need technical metadata about the data so that the machine can automatically interact with the data, process it. also, not all data is necessarily being downloaded; some is accessed via apis. in this case, a description of those apis is required. overall, a rich description of the data that is able to be processed by machines is required. the third consideration is granularity and i am going to distinguish between two types. one is spatial and by spatial i mean dimensions in the dataset, and the other is temporal. figure 47 identifying, accessing, and reusing parts of a dataset.  regarding spatial granularity: there is a need to say i have used this entire data set, but i actually only worked with the parts of it. it is important to be able to describe that slice or segment of the dataset. i would like to draw some parallels here with ongoing work related to webcentric annotation of research materials as pursued by the open annotation and annotation ontology efforts. what you see in figure 48 is an image that is being annotated. it is not the entire image that is annotated; it is just a segment thereof. and that segment is being described, in this case by means of an svg document. a similar approach could be used when referring to parts of a dataset: for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.28 developing data attribution and citation practices and standards  identify the dataset, and convey the part of the dataset that is of interest by means of an annotation to the dataset. figure 48 an annotation on a part of an image per the open annotation approach.  regarding temporal granularity: we need to be able to refer to a certain version of a dataset as it changes over time, or a certain temporal state of the dataset when a dynamic dataset is concerned. figure 49 dataset changes over time.  for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.data citationštechnical issuesšidentification 29 here, again, i would like to draw a parallel with other work that i am doing. this is the memento project, which is a simple extension of the http protocol that allows accessing prior versions of resources. in order to do this, memento uses the existing content negotiation capability of http and extends it into the time dimension. so, what you have in figure 410 a generic uri for a certain resource (urir). but the resource evolves over time and each version receives its own uri (urim1, urim2). without memento, you need to explicitly know urim1 and urim2 in order to access those respective versions. with memento, you can access the old versions of the resource, only knowing the generic uri (urir) and a datetime. memento can play a role in accessing prior versions of datasets.  figure 410 memento allows to access old versions of resources using a generic uri and a datetime. i would say that these granularity requirements are needed for both the data publisher and the data user. if we want to address those granularities, we need a solution that allows both the publisher and the user of the data to specify what that segment is going to be. finally, i see three options regarding identity in regarding the notion of granularity: option 1. for citation, access and reuse: mint a new identifier for each segment.  option 2. for citation, access and reuse: use the entire dataset identifier with a query component.  option 3. o for citation: use the entire dataset identifier. o for access and reuse: use an additional uri that refers to an annotation to the dataset that describes the segment (both spatial and temporal) of the dataset. the citation thus becomes a tuple [dataset uri; annotation uri]. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.31 5 maintaining the scholarly value chain: authenticity, provenance, and trust paul groth1 vu university of amsterdam, the netherlands i think the following quote is important. it is from jeff jarvis2: in content, as creation becomes overabundant and as value shifts from creator to curator, it becomes all the more vital to properly cite and link to sources [...]. good curation demands good provenance. [...] provenance is no longer merely the nicety of artists, academics, and wine makers. it is an ethic we expectﬂ. i agree that this is an ethic that we expect and that is one of my motivations for doing research on provenance. i am a computer scientist. one of the design principles that computer scientists adopt a separation of concerns. i think that when we talk about data citation, we need to be very careful about separating concerns. this lack of separation of concerns occurs because when we speak about data citation we adopt practices from the way we cite traditionally. my goal is to convince you that we can do better. let me give you an example. the figure below is a standard reference from an older famous paper in the field of artificial intelligence titled "a truth maintenance system".3 figure 51 a reference from the article, ﬁa truth maintenance system.ﬂ  this reference contains a lot of information. it tells you where to find this article. it gives you a search term to help you find it in libraries or in google. it also helps with provenance. for example, it gives information about the person who wrote this paper and that it appeared in a book called the fourth proceedings of ijcai, as identified in figure 52.   1 presentation slides are available at http://sites.nationalacademies.org/pga/brdi/pga064019 2 jeff jarvis, media company consultant and associate professor at the city university of new york's graduate school of journalism, in ﬁthe importance of provenanceﬂ, on his buzzmachine blog, june, 2010. 3 jon doyle, a truth maintenance system, artificial intelligence, volume 12, issue 3, november 1979, pages 231272, issn 00043702, 10.1016/00043702(79)900080.  for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.32 developng data attribution and citation practices and standards  figure 52 provenance information with a reference. furthermore, the reference helps with making trust interpretations. for example, i personally know that pat, the author, is a good person. this means that i should follow this citation because i know this person is trustworthy. or maybe, i should follow the citation because it is at the international joint conference on artificial intelligence and i happen to have background knowledge about artificial intelligence to know that this is a top conference in the field. so, again, i think i trust this piece and think it can be used in my work. the point is that in this one simple citation and its corresponding reference, we were provided with information about four different things: information to lookup the paper, its identity, provenance information, and trust. however, giving this same reference to a computer changes things completely. i think that for data citation we need to try to address all of these different areas separately. we need identity and we need provenance, but provenance is not part of making persistent identifiers. it is something separate. once we have provenance, we can start computing trust metrics. different people have different ideas about what they trust, but based on where the material comes from and describing how the experiments were produced we can compute different kinds of trust metrics. finally, over the top of all this, once we have identifiers, once we have some information about where the material comes from and how it was produced, once we have these trust metrics, we can build good search engines. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.maintaining the scholarly value chain  33 each area is a different concern and there are some technical issues with each one. for identity, we have already heard about computer understandability, about identity persistence and also the notion of lookup versus identity. whether we need to put together or pull apart, lookup and identity, i do not know. for provenance, we need to deal with the issue of scale. one data point can have hundreds of gigabytes of provenance. i remember when i did my ph.d., i was doing provenance work and i had a result that was one number with one gigabyte of provenance information associated with it. so scale is a huge issue here, especially how much of this is computer understandable? right now most of what we say about provenance of data is encoded in text that people can read, but increasingly we encode in the form of computational workflows or in other computer reasonable formats. it is important to note, that trust is different than provenance. provenance can be viewed as a platform on which we can develop different ways of determining trust. i think we need to develop this platform trust metrics separately. we also need to develop scalable algorithms for calculating these trust metrics. finally, i think trust is different for different actors and, right now, we do not have that clear a conception of it. so this also requires more work. for example, for my many data applications, i may trust everything because statistically it does not matter. however, for something that my tenure or promotion package is based on, maybe i want a different, higher level of trust. to conclude, this is my appeal: citation does not have to contain everything. when we talk about data citation, we should not try to include everything possible in the citation or reference. maybe all we need is simple pointers that are understandable by machines. the final thought i want to leave you with is that we can build it. we have the technology and i do not think we are far off. i will end with some references: w3c provenance incubator final report: http://www.w3.org/2005/incubator/prov/xgrprov20101214/. w3c provenance working group standardization activity: http://www.w3.org/2011/prov/wiki/mainpage. surveys of provenance and trust: donovan artz and yolanda gil. a survey of trust in computer science and the semantic web, journal of web semantics, volume 5, issue 2, 2007. rajendra bose and james frew. lineage retrieval for scientific data processing: a survey. acm computing surveys, volume 37, issue 1, 2005). j. cheney, l. chiticariu and w.c. tan. provenance in databases: why, where and how, foundations and trends in databases, 1(4):379474, 2009. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.34 developng data attribution and citation practices and standards  david deroure, replacing the paper: the twelve rs of the eresearch record: http://blogs.nature.com/eresearch/2010/11/27/replacingthepaperthetwelversoftheeresearchrecord juliana freire, david koop, emanuele santos, claudio silva. provenance for computational tasks: a survey, computing science and engineering, vol 10, no 3, pp 1121, 2008. luc moreau, the foundations for provenance on the web, 2010, foundations and trends® in web science: vol. 2: no 23, pp 99241. http://dx.doi.org/10.1561/1800000010 yogesh l. simmhan, beth plale, dennis gannon. a survey of data provenance in escience. acm sigmod vol 34, no 3, 2005. see also a longer version. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.35 discussion by workshop participants moderated by john wilbanks  participant: i did not get the distinction between usage data, citation data, and bibliography data. can you please define these terms? dr. bollen: the least we can say about usage data is that someone paid attention to a particular resource, whereas with citations, it is an explicit public statement of someone indicating that they have been influenced or impacted by someone else's work. in our statistics, it is sometimes obvious that some materials are used a lot but never cited and vice versa. when we raise this data usage issue, we also talk about data downloads and access, and we are focusing on aspects that are somehow measurable. participant: i believe i got your distinction between usage data and citation data, but i think that this is not well developed. we might need to make a distinction between citation and usage of data versus citation and usage of literature. participant: i think that the issue of data citation has two components: technical and sociocultural. we still have questions related to how we write a citation for a dataset, and there are some technical challenges in terms of space or in terms of the style of writing the citation for a given dataset. however, are we focusing too much only on the technical challenges? the real challenge is the culture of citing datasets and providing a proper citation. we therefore need to focus on both the technical and the social and cultural challenges. dr. minster: there is a wellknown trick to put the subtle error in one of your papers and then everybody is going to cite your publication. you can do the same with data. if we stop insisting that every single granule of a database should have its own identifier and be quoted by those who use it, it would be in the interest of the data provider to just give miniscule granules. participant: why can i not point to every piece of data at the smallest granularity possible? dr. bollen: fair enough, but if you rely on this mechanism to provide people with recognition, advancement, tenure, things like this, the small cites are not very useful. participant: this question is for dr. bollen. did you put your usage data in your tenure package? dr. bollen: yes, i did. i had a couple of pages showing some statistics on my papers. participant: did you make the raw data available? dr. bollen: no. i could not do that because of the agreement that we have to sign with the rights holder. this issue is always a big challenge and a big responsibility. participant: one of the great things about print citations is that they are notation dependent. in the past, if you happened to work at a university, you knew how to go to the library and look for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.36 developing data attribution and citation practices and standards  up a journal title and get it. those practices survived hundreds of years, and i worry sometimes that if we revert to just uris and require that it should be dereferenced, we are losing something very important from the old model. do you have any thoughts about this tradeoff between location and dependence on simplicity? dr. bollen: this is a good question. the best answer i heard this morning was that when you want to deal with regularity, you have to retrieve the same dataset that somebody else used. maybe storing separately the data and the dataset of the query is the right way to do it. then you can say, "in this research, i have used the output of this query on that dataset". participant: where would i find the dataset? where do i look? dr. bollen: the dataset should be findable by uri on the web, no matter where it is, although some providers may require enormous granules or miniscule granules. having that as the output of the query would be a reasonable thing to do. dr. groth: i think this is one of the places where we may have to sacrifice because of the scale we are talking about. the reason we can look at a normal citation and find it in the library is because there is lots of background information about what all the pieces of a citation mean. in the data area, it is not that clear. we had a great example of what a possible data citation could look like in a paper, but we already have been told that it is too much work. if we think about the scale we are talking about, then urls may even be too much work. so we have to sacrifice this location independence. dr. van de sompel: i am in favor of the http uris in this case. i have some reasons for that. one, we get an entire infrastructure that is freely available. we can get all the new developments free rather than having to reinvent the wheel. in order to get closer to what we refer to as longevity of identifiers similar to what we have in print, i am actually in favor of having an accession number that is not protocol based, like the doi or an ark identifier as introduced by the california digital library. it is just a unique string that you carry with you into the future. i call this technology independent. you can print it on paper. then you basically instantiate that nonprotocol identifier and use the protocol of the moment. so, even if all those citations become invalid at one point because http goes away, there will be ways to recover that information and transpose it to the next protocol. plus, if indeed http is going to become obsolete at some point, there are going to be services to migrate us from one to the other. we just have to rely on that. so, yes i am now in favor of using http uris, but probably under the condition that the nonprotocol string identifier exists also. dr. bollen: i share that concern. there is definitely going to be some problem in this area. i am not a specialist on identifiers, but a lot of this seems to me to be an artifact of the technology that we have at our disposal right now. if we had machine intelligence that could, in essence, unpack the kind of information that we use, none of this would be an issue. participant: however, humans do create identifiers. dr. bollen: this involves a lot of background knowledge and implicit information. the reason why we are coming up with these machine identifiers specifically, however, is because the machines are not capable of accessing that information. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.discussion by workshp participants  37 dr. sperbergmcqueen: we should not overestimate the power of human contextual information. bear in mind that in difficult cases, in particular in identifying musical compositions, we always use catalog notes. you do not refer to a piece by mozart without a peripheral number precisely because context does not suffice for average use in those situations. participant: dr. minster, you referred to something being ﬁproperly cited.ﬂ what did you mean by that? dr. minster: i used "properly cited" in a loose way, but i believe it has two purposes. the first purpose is to give appropriate credit to the person who actually created the data, analyzed them, calibrated them, or did whatever was necessary before they become public. the second is for someone else to be able reuse these data and do his/her own analysis. participant: i have another question for dr. minster. what did you mean by "data publication?" are you talking about fixing data in time or printing it on paper? or are you talking about just making it available in some other form? or are you actually talking about a publication like a data journal? dr. minster: no, i do not mean a data journal, but i do mean something that may have a versioning capacity. for instance, if we want to refer to a book, we have to say which edition we want. that is the same thing about data. there is another aspect, however, which i am not sure how to solve. in our world today, we trust the gold standard for scientific publication: peer review. i do not know how one would do a dataset peer review. this is something that might have to be invented in the future. participant: does citation have anything to do with peer review? that is the thing that i do not really get. just being able to point to something formally and give it credit should be fine. we need to be able to do that first and then we may need new mechanisms for peer review of data. dr. minster: the question was about publication, however. i have a hard time accepting the concept of citable publication without some kind of quality control, either peer review or an equivalent mechanism. participant: when a database comes out, it is often transformed before it is published. that transformation can then be done later by a third party and, in fact, there could be a whole new data chain. at what level of granularity does it make sense to track this into a machine understandable form so that we can say, ﬁi am looking at this transformation of this data by this person?ﬂ dr. groth: i have worked with people who track the provenance of data at the operating system level. i think it is the decision of scientists. there is no hard and fast rule for that. dr. minster: in my field of geophysics, more and more datasets are the output of a large computer program and, in that case, provenance is a real issue because we have to say which version of the code we used, on what machine, and using what operating system. dr. van de sompel: i agree with paul groth that this is a more curatorial decision. i am not an expert in this area, but it seems to me that we can hold on to all the provenance or workflows for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.38 developing data attribution and citation practices and standards  that have taken place. the question, though, is do we also need to hold on to all intermediate versions of the data? are these workflows still relevant when we do not have the underlying data anymore? maybe they are because we can figure out what had happened even though the source data are gone. dr. groth: in general, if we have undeterministic data, we can throw away the intermediate data. but if we have undeterministic data or information, we cannot throw away steps because we cannot reproduce them. we did some work in astronomy, where we put everything on a virtual machine and anyone could just reexecute everything. it was fine because we had all the input data and everything could be reproduced. dr. callaghan: i want to respond to paul groth™s comment on the relationship between data publication and data citation. i think that we need to have citation before we can have data publication. this is because the way we are looking at publishing data (i.e., actual formal journal publishing with the associated peer review arrangements) is to give it that cited stamp of quality. we cannot do that unless the dataset that is being peer reviewed is findable, static, a host of other things, and is persistent. i work for a data center and we do what some people might consider data publishing. it gets a bit confusing because we have two levels of publishing. there is publishing that involves making labels, handing it out to the web, and the like. we call it publishing with a small ﬁpﬂ. then there is publishing that carries all the connotations of scientific peer review, quality, persistence, and so on. we call that publishing with a big ﬁp.ﬂ i view citation of data as occurring somewhere between those two. so, from my point of view, citation means that as data centers, we are making some judgments about the technical quality of the datasets. we cannot say anything about the scientific quality of the data because that is for the domain experts and the scientific peer review process to decide. dr. groth: when we use a citation in a journal article, i know that in geography, for example, they only cite things that are peer reviewed. in computer science, however, if you examine the articles produced by the logic community of practice, they cite their own technical reports because their proofs are so long that they cannot put them in the paper itself. so, i think we need to allow for citation of the small "p" and that should be the same mechanism for the citation of the big "p." dr. callaghan: i have a quick point related to the granularity issue and citing a small part of the dataset. the way i think about it is that we do not need to have a doi or a uri for every word in a book, for example. we cite the books and some location information at the end, such as page number, paragraph number, or something similar. that is the analogy i would think of when it comes to citing a portion of the dataset. we cite a dataset as a whole, but we can also provide extra information to locate the specific part of the data that we want to reference. dr. van de sompel: that is exactly the formulation that i showed on my presentation slides, which i feel is also the most generic approach to this: use the identifier of the entire thing and in addition use an annotation (with an identifier) to express which part of the thing is of concern. all of this is easier when using http uris, including ones that carry dois or ark identifiers, because a whole range of capabilities comes freely with http. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.discussion by workshp participants  39 ms. carroll: we got separated from the publication and citation issues, and i would like to get back to them. i think we have to have a common understanding of what we are talking about when it comes to publication. i think the division between scholarly articles and technical reports is a good analogy. the vast majority of datasets that people want are like technical reports rather than like scholarly journal articles. so, let us not lose sight of those little "p™s". dr. van de sompel: i agree and i would like to reiterate that peer reviewing in the classic way is not going to be scaleable in the area of small ﬁp.ﬂ mr. wilbanks: i think it is also important to note that even in the big "p," traditional peer review is starting to change. recently, we started to see articles getting published that are only reviewed for scientific validity, without attempting to judge input in advance. this is not just in the library science area; it is in nature and is used by some other publishers. dr. callaghan: i think that what is really important is how people actually use the material, regardless of whether it has gone through a formal review process or not. if we are looking to the future and how people are collaborating and communicating, that whole formal scholarly review and the metrics of it should be reconsidered. dr. bollen: that is exactly the point. i think that within the next 10 or 15 years, we will see the momentum of building new systems and mechanisms and that the traditional big "p" publication process will be slowly but surely replaced by new approaches. mr. parsons: continuing on this peer review theme, one of the things that i think is critical in this area is training in relation to the concept of data citation. i have been pushing data citation for more than a decade and only recently, in the last couple of years, has the issue really taken off. the scholarly literary mechanisms will likely transform, but also let us be honest and admit that academia is an incredibly conservative institution. it does not transform quickly and one of the reactions i have gotten in pushing data citation is that the data producers do not want the data to be cited. they want the paper about the data to be cited, because they get more credit for that. participant: just a quick clarification, dr. bollen. i thought that all of the materials you were studying were big "p" publication. dr. bollen: yes, we were heavily focused on crossvalidation of our results and the only comparable datasets that we had were citation statistics, which do focus on big "p" publications. i am not expert on persistent identifiers, but i feel that these issues were working themselves out into the larger community by mechanisms that are very difficult to anticipate at this particular point. i think they will be very different from the publication and the scholarly communication mechanisms that we have seen in the past. this is my personal experience and that is true for my students. that is also true for all of my colleagues. a lot of the material that they publish these days is distributed primarily online, and is reviewed primarily online by the general community and not by an editor or a committee of three or four reviewers. i see this phenomenon coming into play when it comes to datasets that are increasingly made publicly available, but not in any systematic way anytime soon. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.40 developing data attribution and citation practices and standards  dr. groth: i have three comments. one, i think that we need a simple and straightforward way to point to small "p" and big "p" datasets. two, once we have that, we can use it as the basis for building more complicated systems around provenance and later on, trust. so, let us start with the simple task first. finally, we may need some standard ways to make citations look nice in the back of our papers. dr. minster: it is a sad reality that some of our colleagues work very hard to produce the datasets and they get no credit. once we have done a piece of work and published it with a big "p," it ends up in a journal or in a book. how often is it that you get somebody to hack the book and change the contents of, for example, science magazine online? i just do not see this happening. in datasets, however, this could happen all the time and therefore it is very important to have trusted repositories. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.41 part two disciplinespecific issues for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.43 6 towards data attribution and citation in the life sciences philip bourne1 university of california at san diego my talk today will focus on some observations about data citation and attribution from the life sciences perspective. the protein data bank (pdb) is a data repository that i am involved with and i am going to use it an example of some of the things that are happening with data in the life sciences. let me start with the following observation. in terms of life sciences data repositories, the national library of medicine (nlm) is one of the largest in the field. in many ways, the nlm has done a very good job of providing data resources. it also allows people to deposit data and has some level of integration with the literature. however, it appears to me that what they are doing is not fully consistent with some of the data citation and attribution principles and best practices that have been discussed in this workshop so far. for example, the ability to cite and attribute the data at the nlm is highly variable:  digital object identifiers (dois) are assigned in some cases, but are not used.  attribution is made through the metadata in most cases.  such attribution is typically through the associated literature reference, if it exists, or by a database identifier.  the use of data repositories such as dryad is compelling for the ﬁlong tailﬂ problem, but not recognized by nlm. data journals are on the horizon. there is an interplay between data and publication that is very interesting in the life sciences. to people that maintain data repositories, their metric of success is being published in nucleic acids research, since it brings prominence to these data resources through traditional publication. similarly, i am an author of a paper about the pdb, which is cited yet i can guarantee that few people have ever read it. there is no reason to read it. it is just there to provide a conventional value metric for a database. the pdb is a resource that is distributing worldwide the equivalent to one quarter of the library of congress each month. the pdb is one of the oldest data repository in biology, a 1 tb resource that is used by approximately 280,000 individuals per month, including an increasing number of school kids. there is absolutely no way that when this resource was founded over forty years ago that scientists ever believed that school children would be using that data.  1 presentation slides are available at http://sites.nationalacademies.org/pga/brdi/pga064019. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.44 developing data attribution and citation practices and standards  figure 61 protein data bank repository growth over time.  the main point in this diagram is that pdb data volumes are increasing and that the complexity of the data is increasing dramatically as well. therefore, how we define data, how we cite them, and what granularity we use is changing all the time. another point i would like to make is that people are doing more with the data. the following is our big metric of success that we use when we go to funding agencies. we have grown the user base considerably and people are spending more time working with the data than they ever did before. for example, statistics show that the number of visits and page views is growing faster than the number of unique visitors, implying that users are spending more time on the site. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.towards data attribution and citation in the life sciences 45 figure 62 web site visitors and bandwidth. then there is the question of what are valuable data and what are not? figure 63 provides an example related to h1n1 pandemic data. effectively, associated pdb data were hardly used at all but suddenly, during that pandemic, the data became highly accessed. we cannot really tell when data are going to be valuable, so, how do we decide what to keep and what not to keep? the data may save lives? * http://www.cdc.gov/h1n1flu/estimates/aprilmarch13.htm jan. 2008 jan. 2009 jan. 2010 jul. 2009 jul. 2008 jul. 2010 1ruz: 1918 h1 hemagglutinin structure summary page activity for h1n1 influenza related structures * 3b7e: neuraminidase of a/brevig mission/1/1918 h1n1 strain in complex with zanamivir  figure 63 data related to the h1n1 pandemic. source: centers for disease control.  let me now talk about data attribution and citation. we spend about 25 percent of the pdb's budget on remediating data that we already have. this has introduced issues related to our support of multiple versions, which we also have handled. there is the socalled copy of record, which is the version that actually went with the publication which we also always make for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.46 developing data attribution and citation practices and standards  available. the community of depositors of the data also requested that scientists cannot publish their articles unless the supporting data are deposited. we do cite dois but, as i said earlier, no one uses them. database identifiers are preferred. this exemplifies the kind of problems that we are facing and i think it is a sociological matter. take the example below of a molecule. it is a receptor on the cell surface called cd4. it is important for many reasons, but one of them is that there is a protein on the hiv virus that binds to it and so it has been identified as the causal agent of hiv infection. when this structure came out, we were not using dois. figure 64 image of 1cd4. figure 64 is known in the biology community everywhere as cd4 because this is the document identifier that it got when it was assigned by a group of scientists at yale university. that is the identifier we use for character identifiers in the pdb. so what happened is that a group of scientists from columbia university did a better job on this dataset and then called it in the database 2cd4. the yale group then went back and did some more work. as a result, they created another dataset and called it 3cd4. but the problem was that 3cd4 had already been given to someone else. this actually caused angst in the community and it has absolutely no relevance, whatsoever. it is just an identifier. if there were different sources of those data and we could not be clear on what the copy of record was, it would be a problem but, in this case, it is quite clear. so, rather than thinking about data and journals, it is the idea of trying to bring all this together into a seamless connection. the journal is just one view on the data in some ways and we have been working on this with a number of journals. previously, when people did not cite the data, the authors could not use a standard mechanism to find out who has been using the dataset. now they can because we scan all the open access literature. we can point out to people all of the references that a particular piece of data has in for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.towards data attribution and citation in the life sciences 47 the open access literature. what is even more interesting is that we can then see in each of those articles what else is being cited. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.49 7 data citation in the earth and physical sciences sarah callaghan1 rutherford appleton laboratory, united kingdom when i was asked to speak about the physical and earth sciences, i thought this was a very broad area to cover! so i decided that the best approach was to focus in on a number of issues and examples. i am a member of the british atmospheric data center (badc) and we are one of the united kingdom™s national environmental research centre™s (nerc's) data centers. nerc funds the majority of the earth sciences and ecological research work in the united kingdom. i am part of a federation of data centers, which covers the environmental sciences broadly, including hydrology, atmosphere, ecology, ocean and marine, and so on. we deal with a lot of data from many different fields. it is important in our work to define what a dataset is for ourselves because otherwise, datasets can get very fuzzy. we define a dataset as a collection of files that share some administrative and/or project heritage. in the badc we have about 150 real datasets and thousands of virtual datasets. we have also 200 million files containing thousands of measured or simulated parameters. the badc tries to deploy information systems that describe those data, parameters, projects and files, along with services that allow one to manipulate them. also, in 2010 we had 2800 active users (of 12000 registered), who downloaded 64 tb of data in 16 million files from 165 datasets. to put that into context, less than half of the badc data users or consumers are atmospheric science users. we have people coming to us to download data for all sorts of reasons, even including school children. so, what are data for us? data can be anything from:  a measurement taken at a single place and time (e.g., water sample, crystal structure, particle collision) measurements taken at a point over a period of time (e.g., rain gauge measurements, temperature)  measurements taken across an area at multiple times by a static instrument (e.g., meteorological radar, satellite radiometer measurements)  measurements taken over and area and a time by a moving instrument (e.g., ocean traces, air quality measurements taken during an airplane flight, biodiversity measurements)  results from computer models (e.g., climate models, ocean circulation models)  video and images (e.g., cloud camera images, photos and video from flood events, wildlife camera traps)  physical samples (e.g., rock cores, tree ring samples, ice cores)   1 presentation slides are available at http://sites.nationalacademies.org/pga/brdi/pga064019. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.50 developing data attribution and citation practices and standards  historically speaking, even though it was very laborintensive to create new datasets, it was often (relatively) easy to publish the data in a visual form, like an image, graph or table. this picture is an example of one of the earliest published datasets. it was created by robert hooke and dates back to 1665. figure 71 suber cells and mimosa leaves. source: robert hooke, micrographia, 1665.  one of the big drivers for data citations in the earth and physical sciences is to make it easier to identify products and projects when one is comparing them. a major example of this is a set of experiments being done by climate modelers all over the world under the auspices of the world meteorological organization (wmo) via the world climate research program (wcrp). it is called cmip5: fifth coupled model intercomparison project. these climate model experimental runs will produce the climate model data that will form the basis of the fifth assessment report for the intergovernmental panel on climate change (ipcc). in particular, cmip5 aims to:  address outstanding scientific questions that arose as part of the ar4 (the most recent ipcc assessment report) process,  improve understanding of climate, and  provide estimates of future climate change that will be useful to those considering its possible consequences. the method used in cmip5 is based on a standard set of model simulations which will:  evaluate how realistic the models are in simulating the recent past, for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.data citation in the earth and physical sciences 51 provide projections of future climate change on two time scales, near term (out to about 2035) and long term (out to 2100 and beyond), and  understand some of the factors responsible for differences in model projections, including quantifying some key feedbacks such as those involving clouds and the carbon cycle. climate models are usually run on supercomputers, and produce a lot of data. for example, the numbers for cmip5 are below:  simulations: ~90,000 years ~60 experiments ~20 modeling centers (from around the world) using ~30 major model configurations ~2 million output ﬁatomicﬂ datasets ~10's of petabytes of output of the replicants: ~ 220 tb decadal ~ 540 tb long term ~ 220 tb atmosphereonly ~80 tb of 3hourly data ~215 tb of ocean 3d monthly data ~250 tb for the cloud feedbacks ~10 tb of landbiochemistry (from the long term experiments alone)  these numbers are not particularly important from the point of view of data citation, but they do indicate the sheer volume of data that has to be dealt with. it is not only climate scientists who will have to work with these data, but members of the general public will also try and make sense of them. this is the sort of data that will impact how governments will plan for the next 10 to 50 years. the researchers who are supporting the whole cmip5 data management effort have spent a great deal of time and effort thinking about and preparing for how they can store and manage the data. quality control of the data is also important, not only to ensure that valid crosscomparisons between model runs can be made, but also because this is important to the data provenance and it provides reassurance to the outside world that the data are not being deliberately hidden or obfuscated. cmip5 (and the climate modelling groups involved in it) will continue to produce a lot of data! it is an international effort, with everyone involved wanting to ensure proper citation, attribution and location of the data produced. citation will allow the researchers to have traceability and accountability for their datasets. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.52 developing data attribution and citation practices and standards  cmip5 has issued the following guidelines for the citation of datasets (quote is from the cmip5 website): digital object identifiers will be assigned to various subsets of the cmip5 multimodel dataset and, when available and as appropriate, users should cite these references in their publications. these doi™s will provide a traceable record of the analyzed model data, as tangible evidence of their scientific value. instructions will be forthcoming on how to cite the data using doi™s. at the badc, we have for many years now had a citation approach where in all our dataset catalogue pages you will find a little box which gives the proper way to cite that particular data set. we have attempted to produce some metrics on how many people actually used these citation instructions, unfortunately without great results. i think this is because users of our datasets do not have the culture of citing data in the first place. that is something we need to change. we are currently working with all the other nerc data centers to assign dois to certain datasets that meet our technical criteria. we expect that this will make it more obvious to our users what the correct way to cite a dataset and use a doi is, and will encourage more of our users to use the citations. in terms of earth sciences, the pangaea data center (http://www.pangaea.de) is further ahead than us when it comes to assigning dois to data sets. if you look at their repository catalogue pages they give the citation for the dataset with the doi and then it says, "supplement to", which gives the citation for the paper of reference. finally, i work at the same site as isis, which is pulsed neutron and muon source produces beams of neutrons and muons that allow scientists to study materials at the atomic level using a suite of instruments, often described as ‚supermicroscopes™. it supports a national and international community of more than 2000 scientists who use neutrons and muons for research in physics, chemistry, materials science, geology, engineering, and biology. isis is now issuing dois for experiment data to allow easy citation. principal investigators will be sent dois shortly before their experiment is due to start. dois issued by isis are in the form of: 10.5286/isis.e.1234567. the recommended format for citation is: author, a n. et al; (2010): rb123456, stfc isis facility, doi:10.5286/isis.e.1234567 let me conclude by saying that the flood of data is now so great that scientific journals cannot now communicate everything we need to know about a scientific event, whether that is an observation, simulation, development of a theory, or any combination of these. there is simply too much information, and it is too difficult to publish it in the standard journal paper format. data always have been the foundation of scientific progressšwithout them, we cannot test any of our assertions. we need to provide a way of opening data up to scientific scrutiny, while at the same time providing researchers with full credit for their efforts in creating the data. we need data citation not only to provide credit to the scientists who create data, but also for the general public to provide traceability and accountability and to show that as far as possible, we are doing our jobs the way we should. also, there is serious pressure in the earth and climate for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.data citation in the earth and physical sciences 53 sciences to publish data, but there is also a need to ensure proper accreditation. finally, how we communicate scientific findings is changing and data citation practices are a big part of that. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.55 8 data citation for the social sciences  mary vardigan1 university of michigan, interuniversity consortium for political and social research this presentation focuses on norms and scientific issues in the social sciences, and their implications for data citation and attribution. social science advances like other datadriven disciplines through knowledge claims presented in the literature. secondary analysis, which enables other scientists to extend and to verify those claims using the data, is an important component of social science research. it is expensive to collect data and it is usually the case that the analytic potential of a dataset is not exhausted by the original researchers, so making data available for others to use makes good sense. to be used by others, data need to be shared and discoverable with proper attribution, and thus there is a need for good data citation practice. data sharing a strong tradition of data sharing, both formal and informal, exists in the social sciences. there are many active social science data archives around the world that disseminate research data; an example of a successful data archive in this space is the interuniversity consortium for political and social research (icpsr), which is one of the largest such archives and has been in existence since 1962 with the goal of making data available to others for reuse. some social scientists request funding to distribute their data through web sites designed for data dissemination. despite all of the data sharing activity in the social sciences, pienta, alter, and lyle (2010)2 have found that about 88 percent of data generated since 1985 have not been publicly archived. metadata metadata are critical to effective social science. a typical social science data file in ascii format appears as a matrix of numbers, requiring technical documentationšoften referred to as a codebook or metadataš to understand what the numbers represent so that the data may be interpreted. metadata may also be found in other forms such as questionnaires, user guides, methodology descriptions, record layouts, and so forth. in general, all of this metadata and documentation in the social sciences is quite heterogeneous in format and most of it is unstructured. the data documentation initiative (ddi) is an effort to create a structured machineactionable metadata standard for the social sciences. this effort is gaining traction and is used increasingly by data archives and major data projects around the world. it is important to acknowledge the critical role of metadata because when we are citing data, we are implicitly also citing the documentation that is used to understand the data.   1 presentation slides are available at http://sites.nationalacademies.org/pga/brdi/pga064019. 2 pienta, amy m., george c. alter, and jared a. lyle (2010). ﬁthe enduring value of social science research: the use and reuse of primary research data.ﬂ http://deepblue.lib.umich.edu/handle/2027.42/78307. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.56 developing data attribution and citation practices and standards granularity and versioning granularity and versioning of datasets are both important in the social sciences. social science studies may be single datasets or aggregations. for instance, a longitudinal study may include several discrete datasets, one for each wave of data collection. icpsr provides data citations at the study level but other data providers are citing at the dataset level. there is also a need in the social sciences to cite deeper into the dataset. articles often include and it is important to understand exactly which data are behind those tables. data in the social sciences are sometimes updated, so there is a need for versioning to indicate corrections or the addition of new data. types of data in general, icpsr and its sister archives around the world hold mostly quantitative data, both microdata and macrodata, but qualitative data are increasingly being generated and archived. we are also seeing that the boundaries between social sciences and other disciplines are blurring. social science and environmental data are being used together to yield new findings. survey data are being supplemented by biomarkers and other biomedical information and are being merged with administrative records to provide richer information about respondents. in general, there is a trend towards greater complexity because funders are supporting innovative collections that are multifaceted, rich, and comprehensive. social media data and video and audio data are also being used. disclosure risk in data preserving privacy and confidentiality in research data is a key norm in the social sciences. survey respondents are promised at the time of data collection that their identities will not be disclosed, and the future of science depends on this ethic. providing access to archived confidential data must be done in the context of legal agreements between the user and the distributor. new mechanisms for analyzing restricted data online are coming into existencešfor example, we are seeing virtual enclaves and synthetic datasets. there are online analysis systems that enable the user to analyze restricteduse data with appropriate disclosure risk protections, such as suppressing small cell sizes. it is often the case that a publicuse version of a dataset may coexist with a restricteduse version that has more information on itšmore variables, and possibly more information about geography. these versions need to be distinguished. this has implications for data citation. replication replication is, of course, important for science in general. most claims in the social science literature cannot be replicated given the amount of information that is provided in publications. the community has been working to remedy this situation. icpsr has a publicationrelated archive, a small subset of its holdings that is intended to be a repository for all the data, scripts, code, and other materials needed to reproduce findings in a particular publication. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.data citation in the social sciences  57 it is important to understand the chain of evidence behind the findings and to have some idea of the record of decisions made along the way to the final analysis. sometimes this is called deep data citation and provenance. we need both production transparency (i.e., how the data are transformed to get to the final analytic file) and then transparency about how conclusions were drawn. data citation practice there is some tradition of data citation in the social sciences. a standard for citing machinereadable data files was created by sue dodd in 1979, and icpsr has been using a variant of that standard. the census bureau has also been providing citations since the late 1980s. journals are beginning to cite data in a way that is useful. we have found through google scholar that some of icpsr™s citations have actually been used. in the social sciences, persistent identifiers for data are now being assigned. icpsr uses dois, but handles and uniform resource names (urns) are also used. with respect to journal practices, historically there has not been much effort put into citing data properly or in the right place in articles. there is, however, a growing movement and a lot of momentum behind good data citation practice now. many publishers are requiring that the data associated with publications be publicly available. in fact, the american economics review states that they will publish papers only if ﬁdata used in the analysis are clearly and precisely documented and are readily available to any researcher for purposes of replication.ﬂ at icpsr, we have been working with our partners in the datapass project to influence journal practices in this area. datapass, or data preservation alliance for the social sciences, is an alliance of social science data archives in the united states, including the odum institute, the roper center, harvard™s institute for quantitative social science, the university of california at los angeles, and the national archives and records administration. datapass has mounted a campaign to contact the professional associations that sponsor journals. we have written to them highlighting the inconsistencies in their data citation practices and have had some success. the american sociological review, for example, has changed its submission guidelines to require data citations in the reference section of articles and to require persistent identifiers for data. linking data and publications linking data and publications is also important in the social sciences, just as in the natural sciences. when data citation works as it should, these linkages will happen in an automated way, but up until now, linking data and publications has been a manual process. icpsr has developed a bibliography of over 60,000 citations to publications that use icpsr data, with twoway linking between the data and the publications. vendors like thomson reuters are also interested in these linkages. summary in summary, some of the key issues for the social sciences include versioning, which is important for the social sciences because archived data can change substantively over time with new additions. granularity is an interesting issue as well; it would be useful to define and publish for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.58 developing data attribution and citation practices and standards best practices and guidelines for the granularity of data objects that we intend to cite. there is a need to identify very small units like variables uniquely in social science data; they do not need full citation, but identifying them in a globally unique way is important. metadata seems particularly significant in the social sciences because there needs to be a durable link between the metadata and the data. and, finally, there is replication, a key tenet across the sciences. it is critical that we cite and provide access to all the information necessary to reproduce findings. it is encouraging that many are thinking about these issues across domains and are working on technological solutions to several of the problems identified. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.59 9 data citation in the humanities: what's the problem? michael sperbergmcqueen1 black mesa technologies as a complement to the presentations on data citation in the life, natural, and social sciences, this presentation will discuss data citation in the humanities. the reaction of some people to that topic will be to say: ﬁwhat?! when did humanists start working with data? what is considered to be data in the humanities?ﬂ to respond to these questions, i will start with a little background on what counts as data in the digital humanities, then survey current practice with respect to data citation in this domain, concluding with some remarks on intellectual issues with data citation in the humanities. data in the digital humanities humanities scholars started using machinereadable data in 1948, when father roberto busa began work on the index thomisticus. this index was a fulltext concordance of every word of every work published by st. thomas aquinas, as well as every work that historically had ever been attributed to aquinas (even those that busa felt confident were not actually by aquinas). in his theological research, busa was particularly interested in the concept of the eucharistic presence, which required an exhaustive list of aquinas' use of the preposition in. since prepositions tend to have very high frequency in naturallanguage texts, such a list is very hard to prepare with note cards. during the 1950s and 1960s, a great many individual texts were encoded and work of various kinds was performed with them. often, concordances were made; sometimes stylistic studies were performed. in the early 1960s, henry kucera and nelson francis created the brown corpus of american english, a onemillionword corpus drawn from works published in 1961. the journal computer and the humanities began publication in 1966, and in the 1970s, organizational structures formed to support the field: the association for literary and linguistic computing (1973) and the association for computers and the humanities (1978). also in 1978, the lancasteroslobergen corpus of british english was published, created as a pendant to the brown corpus of american english. typically, because of the amount of work involved, the first texts to be encoded were the sacred texts. busa, as a theologian, found aquinas worth the effort of 30 years of work (the index thomisticus was finally published in 1978), and the bible too was one of the earliest texts put into electronic form. of course, the classics were not far behind; classicists are accustomed to very laborintensive work with their texts. the thesaurus linguae graecae project was started in the 1970s with the  1 presentation slides are available at http://sites.nationalacademies.org/pga/brdi/pga064019. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.60 developing data attribution and citation practices and standards  aim of making a comprehensive digital library of greek writing in all genres from homer to the fall of constantinople in 1453; it issued its first cdrom in 1985. what counts as data in the humanities? the humanistic disciplines seek, in general, better understanding of human culture, which means that pretty much anything created by humans is a plausible object of study. data in these fields may include:  digitized editions of major works;  transcriptions of manuscripts;  thematic collections (e.g., author, period, genre); language corpora (balanced or opportunistic; monolingual or multilingual [parallel structure or paralleltext translation equivalents]); images of artworks (e.g., rossetti, blake, deyoung museum imagebase); and  maps. digital representations of preexisting artifacts now often take multimedia forms, e.g., scans plus transcriptions. human culture did not end when humans built computers, however, so digital artifacts and borndigital objects are also objects of study for humanities disciplines. scholars are studying digital art forms, hypertexts, interactive games, databases, and digital records of any kind. there is no human artifact that is a priori unsuitable as an object of historical or cultural critical study. if humanists have been creating large, laborintensive, expensive digital resources for six decades, the question arises: is anyone taking care of the materials thus created? the answer is yes and no.  publishers of digital editions presumably have a commercial interest in retaining an electronic copy of the edition. (i do not actually have any evidence that they are aware of having that interest or that they are acting on that interest, but i hope some of them are, because to the extent that publishers regard digital editions as commercial products, they have historically not wanted to deposit the editions with a repository for longterm holding.) individual projects also have an interest in the preservation of the materials they create, but individual projects sometimes suffer from the illusion that they are going to live forever, with the consequence that they are often taken by surprise when their funding runs out. so neither publishers nor individual datacreating projects have, as a rule, been reliable longterm custodians of humanities data. from as early as the 1970s, there have been projects to collect electronic texts and to preserve them in an archive, beginning with project libri at dartmouth university, which no longer exists. fortunately, before project libri was terminated, its managers deposited all their texts with the oxford text archive, founded a few years later in 1976, which does still exist. there are currently many electronic text centers in university libraries, which have retained digital objects. as a rule, however, most librarybased electronic text centers are interested in displaying and making accessible things that they have digitized and are less interested in acquisition of digital resources from elsewhere. also, there are (at least in theory) digital repositories of various kinds, including institutional digital repositories. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.data citation in the humanties: what™s the problem? 61 although there has been a long history of this work, there is nothing in the humanities like the network of social science data archives that we see with the interuniversity consortium for political and social research (icpsr) in the united states, the economic and social research council data archive in the united kingdom (uk), or the danish data archive, and their various analogues in other countries. the arts and humanities data service in the uk essentially tried to fill that role and did so very successfully for the eleven years that they were funded before they were terminated. (it should be noted that the esrc data archive has now been renamed the uk data archive and describes itself as holding research data in ﬁthe social sciences and humanities.ﬂ current data citation practice in the humanities the current status of data citation practice in the humanities is mixed. there are some hopeful signs. for example, the tei guidelines explicitly require internal metadata for the electronic object itself, and not just for the exemplar of the electronic object. so in principle digital humanists should be familiar with the idea that an electronic object representing a nondigital artifact is distinct from its source and needs to be documented and cited in its own right. in theory, at least, people should know what to cite. also, most citation styles used by humanists now have been revised to allow the inclusion of iris (internationalized resource identifiers) in the citation, which ought to be helpful. some citation styles, of course, refer not to iris but to uris (uniform resource identifiers) or urls (uniform resource locators) instead, which will irritate those of us who believe iris should be the identifiers of choice. but the principle of providing an identifier for an electronic object is at least recognized. in preparation for this presentation, i examined a random sample of papers in the field, looking to see whether there are fairly complete digital resources behind these papers, and if so, whether i could understand or read what they are? there are, as might be expected, several patterns. ideally, one might want to see published resources used in the papers, with explicit citations in the references so services like isi will find them and so that the data citations will show up in a citation index. as far as i can tell, however, this is purely a theoretical category: i found no instances of it in my small sample. the closest thing found in the sample to this ideal practice were papers which mention published resources, which are explicitly described, sometimes with a url pointing to the item, but with no reference to the resource in the references. sometimes, the references include instead a reference to a related paper, which may indicate both a desire to cite the work and a discomfort with citing resources which do not take traditional scholarly forms, or perhaps uncertainty about how to cite data resources directly. in other cases, papers mentioned resources that are clearly identifiable as objects, that clearly have an identity of their own, and that have not been published. the resources are explicitly mentioned and acknowledged in the text, but naturally enough they are not cited because they have not been published. there is not even the equivalent of a personalcommunication citation. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.62 developing data attribution and citation practices and standards  there also are many resources that clearly must exist unless the paper is a complete forgery, but they are unpublished. these resources are implicit in the argument of the paper, but they remain cited because, clearly, the author thinks of them as analogous to working notes. in summary, the situation for data citation in the humanities is completely confused. some problems in humanities data citation some of the problems arising for data citation in the humanities are problems already discussed in the natural and social science context. others may be particular to the humanities. if i, as an author, want to cite a resource, how should it be cited? what exactly should be cited? am i citing the entire british national corpus? am i citing a particular sample of the british national corpus? am i citing the archive from which i got the british national corpus? in the case of the british national corpus, those distinctions are very clear. in other cases, the distinction is not clear at all. when working with digital resources which combine and recombine with each other in unpredictable ways, scholars will find philosophical questions of identity taking on an unwontedly urgent practical aspect. second, it is sometimes difficult to locate reliable metadata concerning a resource one might want to cite. without a physical title page, it may be challenging to identify the title of a data resource, or the names of those intellectually responsible for its content, or the nature of their contribution. in many cases, it is difficult to identify a publisher or a date of publication. those responsible for distributing a data resource may not regard themselves as the publishers of the work, because they do not regard themselves as engaged in publishing in the conventional sense. (this problem may be familiar to those who have tried to cite microfilms created for individual scholars by individual photographers.) it is not even clear whether familiar roles like ﬁpublisherﬂ have the same relevance for electronic resources as they do for print materials. if the familiar division of labor among publishers, distributors, repositories, libraries and archives is, fundamentally, a way of organizing the management of information, we may expect those roles to be recognizable in the digital world. if, on the contrary, that familiar division of labor reflects only a way of organizing the management of paper and other physical objects, then the digital world may well converge on a different and incommensurable set of roles. in many ways, the challenge of locating reliable metadata among them, digital objects seem to be in an incunabular phase; like the earliest printed books, digital objects lack established conventions for identifying the object or those responsible for it. in pessimistic moments, an observer might fear that the situation is even worse than that, and that the creation and dissemination of digital objects does not resemble the creation and dissemination of early printed books so much as it resembles scribal transmission of manuscripts. a third challenge for citation of humanities data resources is that many of those involved, whether as producers or as consumers of resources, want turnkey systems, not a set of tools and materials which leave them to their own devices. this perfectly understandable desire for ease of use tends in practice to lead to tight coupling (both in the technical sense, and psychologically in producers and consumers) of the data resource, the software used to provide access to the resource, and the user interface of that software. (there are notable exceptions, including the for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.data citation in the humanties: what™s the problem? 63 perseus digital library, which has over the years released its data with different software, as the computational infrastructure available to its users has changed.) when a data resource is tightly coupled with a particular piece of software, it can be difficult to distinguish the one from the other, with consequent difficulties for anyone who would like to cite the data resource itself, and not the dataplussoftware combination. several factors may be identified which tend to inhibit data citation in the humanities. fear of copyright issues: when digital resources are constructed without copyright clearance (perhaps on the theory that they are for personal use only), the creators of the resources will understandably hesitate to publish them, or even to cite them explicitly. as a senior figure in the field wrote to me: i think you will still find plenty of people saying ﬁwe ran a stylometric analysis on a corpus which has these properties, but we cannot let you see the actual corpus because we did not obtain the copyright.ﬂ antiscientism: citing data resources may seem foreign to the culture of humanistic scholarship, an eruption into the humanities of naturalscientific practices and perhaps a symptom of science envy, to be discouraged as naïve and unhelpful. citation chains: print has (reasonably) wellestablished conventions for republication and citation of earlier publications. not so digital resources, which may include refinements, revisions, elaborations, subsets, derivations, annotations, and so on, often made without any explicit reference to the sources from which they were derived. this is not unknown in print culture, of course: some editions of classic authors provide no information about the copy text used in the edition. but it does make it hard to trace the provenance of some digital resources, and when resource creators fail to cite the prior resources they use, it is not surprising if users of the later resources also fail to cite the resource when they use it. versioning: large humanities projects typically make multiple passes over the same material. in the future, it is not unlikely that early results will be published (under pressure from the web culture and from funders). if there are multiple versions of a resource from the same source at least two problems may be expected. will the metadata for the resource label the version and explain the nature of its relation to other versions of the resource? what will be the unit of change? will changes be clumped into groups in the way familiar from print editions? or will the resource change continuously (in which case, will it be possible to pluck out a given state of the resource at a given instant to represent a version of the resource? quiddity: large humanities projects typically make multiple passes over material. reading text; textcritical variorum text; text with literary annotations; linguistic annotations (glosses for cruxes? parse trees? ...); or formalization of propositional content. which of these is the thing i am publishing? and which of these is the thing i am citing? longevity: finally, there is the question of longevity. it is well known that the halflife of citations is much higher in humanities than in the natural sciences. we have been cultivating a culture of citation of referencing for about 2,000 years in the west since the alexandrian era. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.64 developing data attribution and citation practices and standards  our current citation practice may be 400 years old. the http scheme, by comparison, is about 19 years old. it is a long reach to assume, as some do, that http urls are an adequate mechanism for all citations of digital (and nondigital!) objects. it is not unreasonable for scholars to be skeptical of the use of urls to cite data of any longterm significance, even if they are interested in citing the data resources they use. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.65 discussion by workshop participants moderated by herbert van de sompel  participant: this is a question for mary vardigan. when you have data systems that are based on surveys, do you have to include in the metadata how exactly the survey was conducted? the reason i am asking is that a couple of years ago, my city paid a contractor to do a big survey of citizen satisfaction. it was done classically with randomly drawn phone numbers and when i looked into it, there was no list of people with cell phones. as a result, cell phones, essentially used by the hispanic population, were vastly undersampled. how do you deal with this kind issue? dr. vardigan: that is a good question. the sampling information is part of the important documentation and metadata that we distribute with every dataset we make available. it is important in assessing data quality. at icpsr, we do not assess data quality ourselves; it is the community that will determine whether the sample is adequate and scientifically sound. it is important, therefore, to have that descriptive information about how the survey was conducted. dr. borgman: we were taught in this session about what might be generic solutions across the disciplines, as well as what could be specific. so far, i have heard two things in common across them. one is that there are data papers or surrogates, where a journal article that describes the data will be cited in lieu of citing the data per se. the other is that there is a deep complexity and confusion in the field. i think it would be good if each of you could highlight what you heard from the other panelists that might work in your field and might be really useful. dr. sperbergmc queen: another item to add to your list is the issue of granularity and the perceived need to be able to cite parts as well as entire datasets. dr. callaghan: i will emphasize the importance of metadata when it comes to doing data citation. it is not enough to validate our datasets unless we have got the full description of what the numbers are and what they actually mean. dr. vardigan: i would like to add the issue of versioning, which seems to be common across disciplines, because data do change over time. there are some dynamic datasets that are continually being generated and there may be disciplinespecific solutions to this issue that could be deployed across other disciplines if we knew more about them. dr. bourne: the notion of peer review of data is being brought up in different contexts. my sense is that this is something that is dependent on the maturity of the field that is generating those data. unlike new fields, when a field is pretty mature, i think the community must have come to a good understanding of how the data could be peer reviewed as part of the process. dr. chavan: i want to emphasize that some of the data that we deal with are very complex and this affects our ability to cite them properly. what we have done is that we got involved with a publisher who is innovative and willing to experiment to work on this issue. gbif currently publishes more than 18,000 datasets and none of those datasets have more than four metadata for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.66 developing data attribution and citation practices and standards  fields complete out of 64 lines. so, publishing data is easy, but writing metadata is really a difficult task. we came up with the approach of publishing the metadata document as a scholarly publication. we publicized an announcement about three months ago regarding a technical solution and a recognition mechanism that we will put in our papers. in the last three months, we approached about 350 data publishers who publish 18,000 datasets and few of them took it up. they said that writing a good metadata document that can actually be published is difficult because every metadata document will have to go to a review process. this is difficult and i personally did it. i wrote a metadata document that could qualify for review and it took about eight hours. this is something that needs to be addressed. nonetheless, out of the 350 publishers, we were able in three months to convince about ten of them. dr. callaghan: i agree that comprehensive dataset management and metadata work are very challenging tasks. however, we are in the position of having good incentives for data producers until they give us their metadata. metadata are very important to help data producers understand the complexity of the systems related to data management. when it comes to the peer review of metadata, i do not think that we want to get scientists and journals involved in this process because it is time consuming, complicated, and technically biased toward data management professionals. i am of the opinion that it is the job of the data centers to make sure that the metadata are complete. we can do that and it is well within our area of expertise. dr. vardigan: just one more point related to incentivizing data producers to create good metadata. in the united states, we now have the national science foundation asking researchers applying for grants to provide data management plans, and metadata are a big component of those plans. we are hoping that this will be a positive influence on what eventually gets deposited into the data centers. dr. bourne: i think that the provision of good metadata is dependent on the reward systems. in some biosciences communities, it is not only that you cannot publish without depositing the data, you must also deposit that data with a fair degree of rigorous metadata. that is also a reward because you cannot publish without it. participant: i want to ask all the panelists how do you see the chances of some entity that would be a registry of unique and persistent identifiers across the different domains? dr. bourne: can you turn it around? the problem right now is that publishing is a competitive business and no single publisher is going to demand getting something standardized, because there is a risk to their business model. however, if publishers got together and insisted that there should be a standardized metadata description that we can use across the board, there could be a chance for it to happen. dr. callaghan: i would say that dois are very well accepted and that is the route that we have chosen to use as far as our datasets are concerned. i would like to add, though, that a doi should not be the sole basis of the citation. there has to be more information on the doi because a doi is just an alphanumeric string. a person will look at it and will not understand anything. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.discussion by workshop participants  67 whereas, if you have another part of the citation that gives the author name, title, and perhaps other information it might not be any good for computers, but it will help the humans. dr. brase: i think part of the issue about how much metadata is already contained in an archive depends on the discipline. on the one hand, for example, in the life sciences, there is a large amount of data already in their archives and they have their own way of doing things. therein lies the problem. change will be difficult. on the other hand, for longtail data that do not have a home, i think there is real opportunity because people want to deposit those data and get credit for them. this is where you can standardize the process. that is where datacite and other similar initiatives can come in. dr. wilson: it is important in a lot of fields that we have documentation of the method by which the data was generated. i also want invite more comment on the division between what are metadata and what are data, because this is not always a clear linešone person's data may be another's metadata, and vice versa. dr. van de sompel: i agree. i think that we still have a huge gap in our understanding of what people consider to be ﬁdata.ﬂ participant: i would like to see a universal approach or guidelines in relation to data citation and attribution. i think that it is very encouraging that we have people representing different fields and areas looking at the same set of problems here. let us just try to be simple and work on things gradually. these discussions and emerging tools and technologies have tremendous motivation for publishers and researchers, and i think that this meeting is a very good starting point. we might not come up with the best solution right now, but as time goes on, i think it is very encouraging. participant: learning from each other is a useful approach as well. i have been working towards citable references for a long time and as i had this subject as a priority; it has been less of a priority in other disciplines. other disciplines suffer from this syndrome of incrementally refined datasets. for example, sequences from genbank are refined by many people and that makes it a complicated coauthorship situation. have you run into that in icpsr and, if so, how do you handle such a situation in a cited reference? dr. vardigan: we have some instances of data that have multiple contributors. some datasets have over a hundred contributors. we have just used ﬁname, et al.ﬂ to acknowledge the variety of people involved. we do not have a specific approach to deal with such a situation. participant: i would be very interested if dr. vardigan or other colleagues can talk about thirdparty metadata. for example, if there is a record put somewhere with an appropriate link that states, ﬁthis sample did not include cell phones,ﬂ this would tell us that the sample is biased. that would be a really useful approach to have. dr. vardigan: i do not know of anything like that currently in existence, but we all rely on the scientific method. if a paper is published and others decide to make judgments about its merits and publish something themselves about the quality of the data or its content, they can do that. as a data center, icpsr does write what we consider to be comprehensive metadata references, and we track publications based on our data. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.69 part three  legal, institutional, and sociocultural aspects for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.71 10 three legal mechanisms for sharing data sarah hinchliff pearson1 creative commons  sharing data today can be easy; you can simply post them on the web. but doing so means losing some control over the data, including whether you will be accurately and properly credited. this is obviously the case when you share data without a related license, contract, or waiver. as i will explain, to a certain extent this is true even when any one of those legal mechanisms is used. i will begin by defining some terms. for purposes of this presentation, attribution, credit, and citation all have distinct meanings. attribution refers to the legally imposed requirement to attribute the rights holder when the data are copied or reused in a specified manner. the remedy against someone who fails to attribute is a lawsuit, either based on breach of contract or infringement of an intellectual property right, depending on the legal mechanism used to impose the attribution requirements. credit, on the other hand, is what we all wantšexplicit recognition for our contribution to someone else's work. finally, there is citation, which is rooted in norms of scholarly communication. the purpose of citation is to support an argument with evidence. however, citation has also become a proxy for credit, albeit an imperfect one. this is an important starting point. it reminds us that legal attribution requirements do not necessarily match our expectations for receiving credit, nor do they perfectly map to accepted standards of citation. when the remedy for failure to attribute is a lawsuit, we are wellserved to recognize this incongruity. with that in mind, let us turn to the law. there are three main legal mechanisms for sharing data: licenses, contracts, and waivers. whenever data are shared, there is a possibility they will not be properly cited upon reuse. licenses and contracts attempt to eliminate this risk by imposing legal attribution requirements. waivers, however, do not legally impose attribution. instead, they rely on community norms to ensure proper citation. there are consequences to each of the three approaches. i will address each below. licenses we will start with the approach for which creative commons is best known  licenses. licenses operate by granting permission to copy, distribute, and adapt data upon certain conditions. one of those conditions is attribution, as it is in all creative commons licenses. a license sounds a lot like a contract because it grants permission to use data under certain conditions. however, they are actually quite different because a license is built upon an underlying exclusive right. therefore, in order to understand the scope of a license, you have to understand the scope of the underlying right. in the context of sharing scientific data, the rights involved are typically copyright or database rights.  1 presentation slides are available at http://sites.nationalacademies.org/pga/brdi/pga064019. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.72 developing data attribution and citation practices ande standards  we will begin by taking a closer look at copyright law. copyright law grants a bundle of exclusive rights to creators of original works at the moment the work is fixed in a tangible medium. in nonlegalese, that means copyright is granted automatically once you write your work down or enter it into the computer. copyright is limited in scope and duration, and the specific limitations vary by country. for scientific data, the most important limitation of copyright is that copyright never extends to facts. copyright does, however, extend to a collection of facts if they are selected, arranged, and coordinated in an original way. the required threshold is low. there is significant uncertainty about where the line of copyright extends, even among copyright lawyers. to complicate matters further, this line varies somewhat according to the laws of each country. determining what is subject to copyright is only the first hurdle. the next task is identifying the scope of copyright protection. even when a database or a collection of facts is subject to copyright, the facts themselves remain in the public domain. this means that the general rule in the u.s. and elsewhere is that data can be extracted from a copyrighted database without infringing copyright law. that is not true, however, in the european union (eu). in the eu and a few other countries, governments have implemented what are called sui generis (ﬁof their own kindﬂ) database rights. these rights allow a database maker to prevent the extraction and reuse of a substantial part of the contents of a database, even if the contents are otherwise in the public domain. a license can be built atop copyright or database rights or both. by way of example, creative commons (ﬁccﬂ) licenses are copyright licenses. if a cc license is applied to a database, it covers both the data and the database, all to the extent each is subject to copyright. any use of the data or database that implicates copyright, requires attribution. any use of the data that does not implicate copyright œ if for example, the data are in the public domain œ does not require attribution, even if it triggers database rights. because of the difficulty of deciphering the contours of copyright protection in scientific data and databases, it is very hard for both the data provider and data user to know when the license applies and when it does not. in other words, it is difficult to know when attribution is legally required. this creates a number of risks. for one, it creates the risk that data providers will be misled about what they are getting when they apply a license to their data. they may believe that if they apply a license to their data, any use of the data will require attribution. as i explained earlier, that is not the case. if the data are in the public domain, or if the use of copyrighted data falls under fair use, the attribution requirement is not triggered.  it also creates the risk that data users (also referred to as the licensee) will misjudge their attribution requirements because of the difficulty in determining when copyright applies. they may under or overcomply with the license without realizing it. either situation can be problematic. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.three legal mechanisms for sharing data 73 in addition to the legal uncertainty, licenses also create the risk of imposing burdensome attribution requirements. in the science context in particular, projects often rely on data gathered from a variety of different sources. depending on the licenses used, it is possible that would require attributing each individual or institution that contributed any piece of data to the project. this is a problem we call attribution stacking. this raises yet another potential problem with attribution. attribution obligations written into a license are, by their nature, inflexible. no lawyer can anticipate every situation in which the attribution requirements would be triggered and account for all of the circumstances in which they will be applied. this can create some absurd situations where, for example, a user or aggregator of data may technically be required to attribute 1000 different data providers, all in the idiosyncratic manner that the rights holder has dictated. conceivably, the user could do all this and still not satisfy people™s expectations for receiving credit or accepted standards of citation. contracts the next legal mechanism for requiring attribution is contract law. contracts can have different names and take a lot of different forms, but they are often called data use agreements or data access policies. unlike a license, a contract does not necessarily require an underlying intellectual property right. technically, it requires a few legal formalities, including an offer and acceptance. in practice, sometimes that manifests in an online agreement, where the user has to click to accept the terms to access to data. other times the user is presumed to have accepted the terms by continuing to use the site. if you read those terms, they may require attribution. like licenses, contracts suffer from a number of potential downsides. for one, they likely impose confusing obligations on users who get data from a variety of sources, all subject to different user agreements. this problem is even more pronounced with contracts because at least public licenses are somewhat standardized. user agreements are not, which means each data source likely has a different user agreement, filled with legalese imposing attribution and other obligations on users. the consequence is that some data sources may not be used simply because users cannot understand the terms. another limit to contract law is that it only binds the parties to the agreement. that may sound obvious, but this is not the case with licenses. if someone obtains licensed data and shares them, the person who obtains them it from that second user is still bound by the conditions of the license. if the data were shared by contract alone, the person who obtained the data from the second user would not be bound by the terms of the contract because they were not a party to the original agreement. in this respect, contracts have a more limited reach than licenses. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.74 developing data attribution and citation practices ande standards  in a different respect, contracts have a broader reach than licenses. because they are not tied to an underlying right, contracts can impose obligations on actions that are not restricted by copyright or database rights. the effect could be to restrict or take away important rights granted to the public. for example, in 2011, the government of canada launched an open data portal with a related contract controlling access to the data. this agreement initially had a provision that forbid any use of the data that would hurt the reputation of the canada. this requirement created an uproar and was changed within a day. nevertheless, this example shows the potential for overreaching. this sort of thing is particularly troublesome in the context of standardized contracts, where the terms are rarely read and almost never negotiated. waivers the last legal mechanism is the waiver. waivers can take many forms, but the purpose is to dedicate the data to the public domain. waivers are not enforceable in every jurisdiction. to deal with this problem, cc has created a tool called cc0 (read cc zero) that uses a threepronged approach designed to make it operable worldwide. the first layer is a waiver of copyright and all related rights. if the waiver fails, cc0 has a fallback license that grants all permissions to the data without any conditions. as a final backup, cc0 contains a nonassertion pledge, where the rights holder promises not to assert rights in the data. obviously waiving rights to a dataset means the provider no longer has control over it. among other things, that means the data provider cannot require attribution (although they can certainly encourage it). yet, as mentioned above, nearly every approach requires losing some measure of control in the data. waivers also provide legal certainty in a way that contracts and licenses do not. there is no need to try to decipher the scope of copyright protection or consult a lawyer. nor is there a need to try to parse the legalese of a variety of different user agreements. note this certainty does not exist when data are released without any legal mechanism. the silent approach leaves people guessing about whether property rights exist in the dataset and whether they risk liability by using it. to summarize, each approach has consequences. with licenses, we face legal uncertainty about the scope of the license, and we risk imposing attribution requirements that are inconsistent with relevant community norms and expectations. with contracts, we gain some measure of legal certainty, but we risk imposing even more burdensome attribution obligations as each institution or data provider creates its own contractual terms. contracts also pose the risk of overreaching and imposing obligations that may restrict important rights of users. waivers avoid the problems associated with licenses and contracts, but they require giving up control. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.three legal mechanisms for sharing data 75 it is important to remember that there is no mechanism that can impose legally binding obligations in a way that perfectly maps to our expectations for receiving credit or accepted standards of citations. by trying to use the law for control, we risk imposing unnecessary transaction costs on data sharing. we also potentially push people away from using our data sources. choosing the right approach requires an understanding of the consequences. the conversation at this workshop is a good start. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.77 11 institutional perspective on credit systems for research data mackenzie smith1 massachusetts institute of technology   my presentation is about the institutional perspective on credit systems for research data. why does credit matter to the institution? simply put, it is because academic research institutions depend on reliable records of scholarly accomplishments for key decisions about hiring, promotion, and tenure. these mechanisms evolved over decades for books, peerreviewed publications, and sometimes grey literature (e.g., theses, technical reports and working papers, conference proceedings, and similar kinds of information that are not peerreviewed). also, a lot of services emerged to make assessment of the record easier for the administration. this includes impact factors, academic analytics, and other methods. the traditional assessment model as we have it now is falling apart because it does not allow new emerging modes of scholarship and scientific communication to be included. for example, the current traditional evaluative process does not consider the following:  preprint repositories like arxiv or ssrn (the social science research network).  blogs, websites, and other social media.  digital libraries like perseus, alexandria.  software tools, e.g., for processing, analysis, visualization. there are important reasons why institutions care very deeply about these issues. one of them is institutional representation. there are national and world rankings in universities. one of the things they look at is the accomplishments of faculty and researchers in the institution. these practices that we come up with, like impact factor, play a big role in some of the ranking decisions, which are extremely important to the administration of the university. then there is academic business intelligence. many universities now have major industrial liaison programs and technology licensing offices. they are always trying to figure out what the academics are producing that might be commercialized or otherwise exploited, both for the university's benefit and the researcher. furthermore, it is important for recruitment. the institution needs to be highly ranked in order to recruit excellent students and faculty members. finally, there are public relations and fundraising considerations that are extremely important for the university. it is easier to raise money from donors if you have a good reputation and when you have some famous researchers. i know this can be very irritating to those of us who are working in research, but this is real life at the university. in the past few decades, at least, the publishing process did not really involve the institution at all. the researchers did the research, wrote the papers, and published them on an outsourced basis through their societies, or increasingly with commercial publishers. the university did not get involved until the library bought it back. so, the only role that the university had was as a consumer. the researchers were acting almost like independent agents in that model.   1 presentation slides are available at http://sites.nationalacademies.org/pga/brdi/pga064019. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.78 developing data attribution and citation practices and standards  however, that is changing with data because in order to produce data, you often need institutional infrastructure. sometimes it is infrastructure related to a disciple, but a lot of times it is institutional. this is where we get into disciplinerelated variations. in fields like geophysics and genomics, for example, the infrastructure is not usually provided by the institution, but in the social sciences, it is frequently provided. in the neuroscience field, it is often the institution that funds the various imaging machines and pays for all the storage and infrastructure to maintain the resulting data. we thus have gone from a system where the institution was not involved in the publishing process to one where the researchers cannot really do what they need to do without support from their institution. furthermore, institutions have other responsibilities when research is concerned. for example, they have some responsibilities when it comes to funding. the institution is the grantee and is legally responsible for enforcement of the terms of the contract. also there is additional infrastructure that we all rely on now to do our work, such as digital networks and computing, the library, the licensing office, and the like. the university is responsible for making sure that the infrastructure is wellmaintained and functioning. lastly, institutions are responsible for the longterm storage of scholarly records so they are preserved and will be available and accessible to all interested stakeholders. now i need to focus on the intellectual property (ip) part. i would say that to the extent that ip exists in data, or that it has commercial potential, oversight for citation or attribution requirements is unclear (see the presentation by sarah pearson). researchers assume that they control the data and have the intellectual property rights and that they can decide what terms to impose on their data. often, however, researchers do not, in fact, have these rights. although funders do not assert intellectual property rights, they frequently do have policies about what should happen to those rights when they give a grant. for example, this is a quote from the nsf administration guide: ﬁinvestigators are expected to share with other researchers, at no more than incremental cost and within a reasonable time, the primary data, samples, physical collections and other supporting materials created or gathered in the course of work under nsf grants. grantees are expected to encourage and facilitate such sharing.ﬂ2 also, university copyright policies are evolving. this is another quote from an unnamed university™s faculty policy. ﬁin the case of scholarly and academic works produced by academic and research faculty, the university cedes copyright ownership to the author(s), except where significant university resources (including sponsorprovided resources) were used in creation of the workﬂ [italics added]. this quote is typical. you can find a similar formulation in just about every institution's faculty policy document. this is what historically has been applied to things such as software platforms developed with university infrastructure. the same thing is being applied to data now. note that the word ﬁsignificantﬂ in the statement is not defined. patent policy is similar. here is another quote from an unnamed university: ﬁany person who may be engaged in university research shall be required to execute a patent agreement with the   2 nsf award and administration guide, january 2011.  for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.institution al prerspective on credit sytems for research data 79 university in which the rights and obligations of both parties are defined.ﬂ in other words, researchers do not get exclusive rights to their patents. they will have to negotiate with the university. this is somewhat vague, however. when data have commercial potential, and they do sometimes, this starts to get really interesting. the new nsf requirement was not received well by all researchers. some said: ﬁi think i might be able to patent something from these data that will make me money. so please keep your hands off my research. i am not sharing.ﬂ i am exaggerating to make a point here, underscoring the fact that as commercial applications of data become better understood, especially in the life sciences and engineering, this could become a really tricky area for everyone involved in academic research. from an institutional perspective, some of the requirements for data citation include:  persistent or discoverable location works even if the data moves or there are multiple copies  verifiable content  authenticity ( i.e., ﬁi am looking at what was cited, unchangedﬂ) requires discovery and provenance metadata  standardized  data identifiers: datacite, dois  people identifiers: orcid registry institutional identifiers: oclc? niso i2?  financial viability  identifiers cost money to assign, maintain  metadata is expensive to produce let me elaborate on these requirements. first, a citation has to be persistent or provide a discoverable location. we need the citation and the discovery mechanism to work, no matter where the database is located. we need some way of proving the authenticity of the data. in other words, i am looking at a uri that is referenced in a research paper. how do i know that the dataset i get to by resolving that uri is the dataset that the researcher was using at the time? that requires discovery and enough metadata. we also need more standardization in key areas. we have to have identifiers for the data, but we also need identifiers for the people and for the institutions involved. for example, i am involved in the orcid (open researcher and contributor identification) initiative, which is looking at ways of creating identifiers for researchers that would be interdisciplinary, international, and portable across time. lastly, there is the issue of financial liability. we have to keep these efforts affordable so we can talk about identifiers, be it dois or datacite uris. i know that there has been contention for using identifiers for data in the past, since if we are talking about a million researchers, that is one thing, but if we are talking about billions of datasets and data points, all of which need unique uris, that could cost a lot of money. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.80 developing data attribution and citation practices and standards  also, the metadata is currently very expensive to produce. this has to be done in a partnership between researchers and specialists who are paid to do this kind of work, whether it is in data centers or libraries. we have to involve experts whose job is to worry about quality control and metadata production, and that is also very expensive. so, we have to keep in mind these issues and requirements when we think about data citation techniques. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.81 12 issues of time, credit, and peer review diane harley1 center for studies in higher education university of california at berkeley i will be speaking today as an anthropologist who has spent a large part of the last decade thinking deeply about and conducting research on issues of scholarly communication, the future of publishing, and academic values and traditions in a digital age. i am a social science scholar. i am not an advocate for particular approaches nor an administrator or librarian. that being said, i will put my comments today in the context of our research findings regarding the drivers of faculty behavior, the importance of peer review in academic life, and the various incentives and barriers to scholars regarding where and when to share and publish the result of research (including data) over the entire scholarly communication life cycle (not just in final archival publications such as journal articles and books). first, an important note about our research and methods. our work is based upon the rigorous qualitative interview, observational, and textual data collected during the sixyear future of scholarly communication project (20052011),2 funded by the andrew w. mellon foundation. more detailed information on our sample population and research design can be found in harley et al. (2010: 1315) and harley and acord (2011: 1213); i include more specific references throughout this paper. i refer readers to the ﬁthick descriptionsﬂ of 12 disciplinary case studies3 and the more extensive literature reviews in these publications. in brief, our sample spans 45 elite research institutions, more than 12 disciplines, and includes the results of more than 160 formal interviews with scholars, administrators, publishers, and others. 4 my comments today will be almost exclusively focused on an elite class of research institutions. one of our motivations has been to analyze what roles universities and faculties play in the resolution of the perceived "crises" in scholarly communication. (and there are of course a number of crises that are fielddependent.) our premise is that disciplinary traditions and culture matter significantly in both predicting possible futures and the success or failure of policies that attempt to dictate scholarly behavior.  1 presentation slides are available at http://sites.nationalacademies.org/pga/brdi/pga064019. 2 the future of sc project website and associated links: project site: http://cshe.berkeley.edu/research/scholarlycommunication. many of the arguments around sharing, time, and credit made here are given in more detail in acord and harley (in press), harley et al. 2010, harley and acord, 2011). 3 disciplines included anthropology, biostatistics, chemical engineering, law and economics, englishlanguage literature, astrophysics, archaeology, biology, economics, history, music, and political science. all the interviews have been published as part of thickly described disciplinary case studies. the entire research output is online and open access at http://escholarship.org/uc/cshefsc.  4 interview protocols covered a variety of broad questions: tenure and promotion, making a name; criteria for disseminating research at various stages (publication practices, new publication outlets, new genres); sharing (what, with whom, when, why or why not?); collaboration (with whom, when, why or why not?); resources created and consumed: needs, discoverability, priorities, data creation and preservation; public engagement; the future. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.82 developing data attribution and citation practices and standards  as i was exploring the issues related to this particular workshop and the various references the organizers had assembled for the meeting, i found this quote from the australian national data service website of particular interest:  data citation refers to the practice of providing a reference to data in the same way as researchers routinely provide a bibliographic reference to printed resources. the need to cite data is starting to be recognised as one of the key practices underpinning the recognition of data as a primary research output rather than as a byproduct of research. while data has often been shared in the past, it is seldom cited in the same way as a journal article or other publication might be. this culture is, however, gradually changing. if datasets were cited, they would achieve a validity and significance within the cycle of activities associated with scholarly communications and recognition of scholarly effort.5 the last statement (highlighted in bold) is actually quite complex and fraught. i will argue today why it has at least two questionable underlying assumptions. the first, is that by virtue of being citable, data achieve an equal footing with traditional publications in institutional merit review of scholars. the second, is that data standing alone, without an interpretive layer (such as an article or book) and without having been peer reviewed, will be weighted in tenure and promotion decisions the same as traditional publication. the centrality of career advancement in a scholar's life my argument (which i hope is not too circuitous) is that these two assumptions are contrary to what our research would suggest. as we have demonstrated (harley et al., 2010), the primary drivers of faculty scholarly communication behavior in competitive institutions are career selfinterest, advancing the field, and receiving credit and attribution. although the institutional peerreview process allows flexibility for differences of discipline and scholarly product, a stellar record of highimpact peer reviewed publications continues to be the most important criterion for judging a successful scholar in tenure and promotion decisions. the formal process of converting research findings into academic discourse through publishing is the concrete way in which research enters into scholarly canons that record progress in a field. and, as the formal version ﬁof record,ﬂ peerreviewed publication establishes proof of concept, precedence, and credit to scholars for their work and ideas in a way that can be formally tracked and cited by others. accordingly, data sets, exhibitions, tools/instruments, and other ‚subsidiary™ products are awarded far less credit than standard publications unless they are themselves ‚discussed™ in a an interpretive peerreviewed publication. the importance placed by tenure and promotion committees, grant review committees, and scholars themselves, on publication in the top peerreviewed outlets is growing, not decreasing, in competitive research universities (harley et al., 2010: 7; harley and acord 2011). there is a concomitant pressure on all in the academy, including scholars at aspirant institutions globally, to model this singular focus on publish or perish, which we and others would argue translates into a growing glut of lowquality publications and publication outlets. this proliferation of outlets has placed a premium on separating prestige outlets (with their imprimatur as proxies for quality) from those that are viewed as less stringently refereed. consequently, most scholars choose outlets to publish their work based on three factors: (1) prestige (perceptions of rigor in  5 australian national data service: http://www.ands.org.au/guides/datacitationawareness.pdf. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.issues of time, credit, and peer review  83 peer review, selectivity, and ﬁreputationﬂ), (2) relative speed to publication, and (3) highest visibility within a target audience (harley et al., 2010: 10). as we determined, this system is not likely to disappear soon and certainly will not be overturned by the adoption of new practices by young scholars, who hew, often slavishly, to the norms in their discipline in the interests of personal career advancement. securing credit and attribution we note a continuum by field in how scholars receive attribution for their ideas.. in highly competitive fields like molecular biology, which have a race to publish and a fear of being scooped, early sharing of ideas, data, and working papers is almost unheard of. the archival journal article takes precedence and seals a scholar's claim on ideas. in relatively smaller (and often high paradigm and/or emerging) fields, with low internal competition, informal mechanisms for reputation management can enforce attribution because academic communities are centrally organized and maintained through facetoface interaction (often via conferences and workshops). this sharing culture can change, however, with funding and other exigencies of a field. to give just one example, although economics is commonly described as a big sharing group where we are very open about ideas (and has a thriving environment of working papers, harley et al., 2010: 357), the subfield of neuroeconomics is moving towards less sharing and mirrors practices in some of the biological sciences. the importance of filters and managing time as scholars prioritize their core research activities, they struggle to keep up to date and they look for more filters, not fewer, in determining what to pay attention to. in fact, time, and the related need for filters, is cited as one of the most influential variables in a scholar™s decision whether to adopt new scholarly communication practices (harley et al., 2007, 2010: 97). most scholars turn to the familiar filters of peer review, perceived selectivity, reputation, and personal networks to filter what they pay attention to. we would argue that, given this background, and an exceptionally heavy reliance on peer review publications to aid the tenure and promotion committees, it comes as no surprise, that competitive scholars divert much of their energies and activities toward the development and production of archival publications and the circulation of the ideas contained within them rather than focusing, for example, on curating and citing data sets. variation in how different scholarly outputs are weighed lest you think the way in which scholarship is credited in tenure and promotion decisions in research universities is binary, it is important to note that teaching, service, and the creation of nonpeer reviewed scholarship such as data bases or tools, are most certainly credited, but they do not receive as much emphasis as peer reviewed articles or books published in prestigious outlets. some things are weighted more heavily than others (and what is weighted is fielddependent). for example, people can get credit for developing software or an important database, but that would rarely be the sole criteria in most fields and not given equal weight as publications. we heard again and again that new genres of scholarship in a field are acceptable as long as they are peer reviewed. examples abound on the emphasis in tenure and promotion for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.84 developing data attribution and citation practices and standards  based on interpretive work versus cataloging or curating. data curation and preservation alone are simply not considered to be a high level of scholarship. let me give you some examples. in biology, new and emerging forms of scholarship are not valued much in their own right, but supplemental articles describing a novel tool or a database may be considered positively in review. "just" developing software or data resources can be perceived as less valuable tool development, rather than scholarship. in history, a scholar who published only footnoted sources, but no interpretation of the sources in the form of a peer reviewed book published by a prestigious press, would not be promoted. that is, new and emerging forms of scholarship (e.g., curating data sets, creating webbased resources, blogs) are valued only insofar as they are ancillary to the book/monograph. in political science, scholars often create and publish datasets. similar to the biological sciences, these efforts can earn a scholar increased visibility when other researchers use their data. significant institutional credit is only received for this work, however, if a strong peer reviewed publication record, based on the dataset, accompanies it. in archaeology, developing and maintaining databases or resource websites, which is common, is considered a research technique or a service to scholarship but not a substitute for the monograph. in astrophysics (and despite the reliance on arxiv for circulation of early drafts, data, and so on), developing astronomical instrumentation, software, posting announcements, and database creation are considered support roles and are usually ascribed a lower value in advancement decisions than peer reviewed publications. how will datasets be peer reviewed? what, do you ask, does any of this discussion have to with motivating scholars to abandon their traditional data creation, citation, and sharing practices in the face of calls (and sometimes mandates) from some journals and funding bodies to publish data sets, particularly in the sciences and quantitative social sciences? these are powerful calls that are motivated by the desire for more transparency in research practice, greater returns on funders™ investments, as well as claims that the growing availability of digital primary source material is creating novel opportunities for research that is significantly different than traditional forms of scholarship. we predict, however, that despite this power, changes in data management practices, as with inprogress scholarly communication, will be heavily influenced by matters of time, credit, personality, and discipline. i would suggest that the most important question for motivating scholars to conform fully to developing new practices is, how will data creation and curation ever be weighted similarly to traditional publications if not peer reviewed? and who will do the peer review and how? i cannot emphasize enough that we just do not know how or when data will be formally peer reviewed in the same way that journals and books are currently. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.issues of time, credit, and peer review  85 some presume that "open" peer review, a freeforall, crowdsourced system, will solve this problem.6 i would reply that such a system would be loaded with intractable problems, not least of which is that scholars, perhaps especially senior scholars, already spend an enormous amount of their time conducting peer review in its myriad forms: evaluating grants, writing letters of reference, mentoring graduate students, responding to emails for feedback on work, and so on. the result is that even established publishers have an exceptionally difficult time recruiting competent reviewers (harley and acord, 2011: 25), and most scholars find it difficult to spare the time to conduct these formal reviews, let alone engage in ﬁoptionalﬂ volunteer and open reviews that will not likely contribute to career advancement. it is our opinion (which we review at length in harley and acord, 2011: 4548 and acord and harley 2011), the lack of uptake of open peer review in a variety of experiments, where commentary is openly solicited and shared by random readers, colleagues, and sometimes editorinvited reviewers (rather than exclusively organized by editors), is not likely to be embraced by the academic community anytime soon. the results of only a few of these experiments indicate that open peer review might have the potential to add value to the traditional closed peerreview process, but that it also exacts large (probably unsustainable) costs in terms of editor, author, and reviewer time. scholars are likely to avoid en masse such experiments because many do not have the time to sort through existing publishervetted material, let alone additional ﬁunvettedﬂ material or material vetted by unknown individuals. in sum, open peer review simply adds one more time consuming circle of activity to a scholar's limited time budget. telling support is provided by the recent policy shift of the journal of neuroscience (maunsell, 2010) and the journal of experimental medicine (borowski, 2011), which recently announced their decisions to cease the publication of supplementary data because reviewers cannot realistically spend the time necessary to review that material closely, and critical information on data or methods needed by readers can be lost in a giant, timeconsuming ﬁdata dump.ﬂ an editorial in the journal of experimental medicine, titled "enough is enough"7 makes the case: complaints about the overabundance of supplementary information in primary research articles have increased in decibel and frequency in the past several years and are now at cacophonous levels. reviewers and editors warn that they do not have time to scrutinize it. authors contend that the effort and money needed to produce it exceeds that reasonably spent on a single publication. how often readers actually look at supplemental information is unclear, and most journal websites offer the supplement as an optional (item to) download–   6 or, many will argue that alternative "bibliometrics" measuring popularity and use is the solution. a consequence of the ‚inflationary currency™ in scholarly communication is a growing reliance on bibliometrics, such as the impact factor, and an increasing ‚arms race™ among scholars to publish in the highest impact outlets. as detailed by harley and acord (2011: 4853), there is widespread concern that at this time, and taken alone, alternative (quantitative) metrics for judging scholarly work are much more susceptible to gaming and popularity contests than traditional peerreview processes. 7 enough is enough" christine borowski, july 4, 2011 editorial published in the journal of experimental medicine (jem). for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.86 developing data attribution and citation practices and standards  in sum, data sharing is greatly impeded by scholars™ lack of personal time to prepare the data and necessary metadata for deposit and reuse (which includes the sometimes herculean efforts of converting analog data to digital formats, migrating old digital formats to new ones, or standardizing messy data). for scholars focused on personal credit and career advancement, narrowly defined, there is no advantage to spending time (and grant funding) curating or peer reviewing data, when that same time can be applied to garnering support for the next research project and/or publishing and peer reviewing books and articles. while data sharing may be facilitated by development of new tools and instruments that ensure standardization (such as in gene sequencing), the idiosyncratic ways in which scholars work, and the extreme heterogeneity of data types in most noncomputational fields, do not lend themselves to onesizefitsall models of data sharing. the escalation of funder requirements (e.g., nsf, nih) for sharing data management plans points to an important space for social scientists to track. we, and others, predict that faculty will not be doing the actual work, but rather a new professional class and academic track (perhaps akin to museum curators, specialist librarians, or toolbuilders) may emerge to take on these new scholarly roles (cf: borgman, 2007; nature, 2008; science, 2011; waters, 2004). they of course will need to be paid and regularized in some fashion. in sum, until issues of time, credit, and peer review are worked out, we predict an uneven and slow adoption by scholars of sharing, curating, and publishing data openly, and hence the citation and attribution of same. references acord, s.k. and d. harley (in press). "credit, time, and personality", invited by new media and society. available for open peer review at nmstheme.ehumanities.nl/manuscript/credittimeandpersonalityacordandharley borgman , c.l. (2007) scholarship in the digital age: information, infrastructure, and the internet cambridge, ma: the mit press. borgman, c.l .(2011) the conundrum of sharing research data. journal of the american society for information science and technology. (accessed 19 october 2011) borowski, c. (2011) enough is enough. journal of experimental medicine 208(7): 1337. harley, d. and acord, s.k. (2011) peer review in academic promotion and publishing: its meaning, locus, and future. university of california, berkeley: center for studies in higher education. available at: http://escholarship.org/uc/item/1xv148c8. harley, d., acord, s.k., earlnovell s., lawrence, s., and king, c.j. (2010) assessing the future landscape of scholarly communication: an exploration of faculty values and needs in seven disciplines, university of california, berkeley: center for studies in higher education. available at: http://escholarship.org/uc/cshefsc. maunsell , j. (2010) announcement regarding supplemental material. the journal of neuroscience 30(32): 1059910600. nature (2008) special issue: big data. nature 455(7209). for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.issues of time, credit, and peer review  87 science (2011) special online collection: dealing with data. science 331(6018). (accessed 19 october 2011) http://www.sciencemag.org/site/special/data/. waters, d.j. (2004) building on success, forging new ground: the question of sustainability. first monday 9(5). for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.89 discussion by workshop participants moderated by paul f. uhlir participant: i was pleased that sarah callaghan mentioned the european database directive. what is the status of the database protection legislation in the united states? in the house of representatives there were the moorehead bill in 1996, and the coble bill in 1997 and again in 1999, and in the senate, the hatch bill in 1998. then such legislation just disappeared. what happened? mr. uhlir: from 1996, there was a coalition of internet service providers, the universities, the libraries, and the academics opposed to the database protection bills. they were just barely hanging on in terms of keeping the legislation from becoming enacted. in 2001, i believe, the chamber of commerce made an assessment of the costs and benefits of the law and, since they are not primarily database providers but users, they determined that it would raise the cost of doing business. they said they would keep track of every vote in favor and keep that in mind when they dole out the reelection money. so, that was the end of those legislative proposals and i have not heard about any new proposal since. participant: i want to ask a question about credit and why anyone would want to share their data. there are real dangers involved in sharing data, not just the work. i thought i would raise it and let someone expound. dr. harley: not all data are created equal and not all data want to be shared. i think christine borgman has done good research of the reasons why data are not shared. we think personality has a lot to do with it and that is why some people are sharing and some people are not. it has to do with preprints and early writings as well. when you submit a paper and it is accepted, you have to have some kind of data management plan. even when journals require data to be available and to be shared, however, we found that less than 10 percent of requests were honored for sharing data. it all goes back to the attitude of: ﬂwhen i am finished with it, i am happy to share it.ﬂ in archeology, for example, that can go on for twenty years. ms. smith: i agree. i think there are strong disciplinary differences in attitudes related to the sharing of data. some fields have the habit of sharing more than others. there are certainly some fields where sharing is not even discussed. the other issue is that even if it is something researchers are willing to do, they are not going to do it if they have to spend time on it. so, if we made it easy, they might be more inclined to do it. participant: i have asked investigators from biology if they think their data will be cited if they are forced by journal publishing policies to make them available. most of them said yes, they thought that they would be cited. my next question was: do you think that citation will be valued by your institutions in peer review of your work? the answer was no. ms. smith: my comment is not based on any actual research, but just my impression from talking to a lot of people. they are sharing their data just because it is the right thing to do. when you can share and there are no negative consequences, you will do it as long as you do not have for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.90 developing data attribution and citation practices and standards to spend a huge amount of time on it. i see this culture becoming more common and policies like those of the nsf are helping. maybe it will just happen naturally and that we are overemphasizing citation as an incentive. it may be that they just want easier ways to share data because they realize it is the right thing to do. dr. chavan: as i keep going through all these presentations and especially in this particular session, it becomes clearer to me what the challenges for data citations are. technology can certainly help in moving in the right direction, but i think that the mindset of those who publish the data and those who are involved in the data management life cycle itself is very critical in moving forward. it is a social and cultural consideration and there is no one single solution to that. i think it has to be worked on every level of the data life cycle. ms. smith: i do not disagree with that, but i think the technology does have a big effect on the costs of this process, and social practices depend a lot on costs. participant: how much of the data that we care about to drive science is not generated by people who are academic faculty looking for tenure and promotion? coming from a federally funded research and development center (ffrdc), the intellectual property issues are drastically different than those at a university. what are the social factors that affect those generating data in other types of institutions? those people also want credit. ms. smith: there are many other players in this ecosystem, such as libraries, but they do not depend on citation that much, which i think is why we are not talking about it here. there are different reward mechanisms for librarians and for data curators in national archives. participant: do you have any examples of people saying ﬁwe have these citations norms, but we do not want to enforce them?ﬂ are there societies with articulated policies that are not legally binding so that they can get around the notion of enforcing credit via the law and instead, have clearer norms? ms. smith: that is how citation works now. i am not legally obliged to cite your article when i use your ideas. there is no intellectual property law that requires me to do that. dr. groth: so then why do we need to discuss these issues? ms. smith: there nonetheless are a lot of researchers using contracts, creative commons (cc) licenses, and data usage license agreements who want to be able to mandate credit. mr. parsons: just a couple of comments on the issue of open data citation as an incentive. i spent too much of my life working on a big international project called the "international polar year," and it had what i think was a landmark data policy pushing for not only open data but also timely release of data. this caused quite a bit of controversy in a lot of different disciplines. we developed citation norms and there is a document associated with the polar information commons, where we have actually documented some of these norms, but that is not what motivates people to share. i think there are two things that motivate people to share. every time i share data, i learn something and i think that is partly the personality aspect that was mentioned earlier. sharing is also moving the field forward. so if people can share in a way that makes them feel that they are collaborators, they will be happy to share. however, the most effective way in for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.discussion by workshop participants  91 getting people to share was when the funding agencies required it by saying that researchers will not get their next year's funding or the next grant if their data are not available. dr. harley: i think you are right. i think the stick is probably your best tool, given the way people react, and given their personal motivations, concerns, and fears. ms. smith: if there is a requirement from an agency to share the data in order to get the next grant, researchers will need to prove to the agency that they met the requirement. so, citation does come back as maybe a simpler way of proving that the data were actually shared, because the data have an uri or are deposited at a reputable data archive. mr. wilbanks: the main reason why we are talking about these issues is that there is a growing push from the open science movement to put cc waivers or licenses on data, and to draft other licenses for data and databases. dr. minster: this notion that data work is good science is just fine, but does not obviate the need to address other issues that diane talked about. we have to change the mindset of those engaged in the scientific process and give proper recognition and advancement to people who spend their life doing this, if they do it well. dr. harley: i think it starts with the investigators mentoring of their graduate students and post docs. it is the senior scientists setting an example of good practice for those younger individuals in their labs and projects. for example, i know a researcher at ucsf who makes very clear his guidelines with regard to good scholarship in his lab: what he will and will not publish, what are good and bad practices. i also want to emphasize that it is not a black and white issue. it should be acceptable to have different gradations of credit for different cases. dr. helly: the issue about carrots and sticks is kind of a false dichotomy in some sense. the sticks are helpful in encouraging proper data practices and they indicate that the agency values and gives credit for delivering the data products. however, it does not in any way do enough to get the data products to a quality great enough to ensure that other people can actually use those data constructively. people will just take their data and dump them in these repositories. the real motivation that was discussed here is the basic scientific realization that there has to be good practice and that this practice has to be learned and taught. this has not been done. once we get to that stage, the community minded people would adhere to these policies, especially in the sciences where community data resources are a very powerful tool for people doing individual science. i think in the earth sciences, in particular, this is a very strong motivation, where there are global issues and data have to be put together across the globe. dr. borgman: i just want to add two policy issues that i am surprised have not come up yet, and i would like the panel to address them. one is the role of embargoes, which are in place in many fields. employees protect the investigators and the funding agencies. those who impose embargo periods generally pick a time period that is long enough for people to get their publications out, but short enough to encourage getting the data out. the other point is about data registries. we have found in our research that even if people are not willing to deposit their data, they are willing to register their data with metadata and then one can at least find them. the pointer might be a telephone number or an uri, but the data registry turns out to be a lower bar. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.92 developing data attribution and citation practices and standards participant: to the extent embargoes are imposed from outside or internally depends on the discipline itself. ms. smith: yes, embargoes are very important to people. i agree that is key, but the registry is an interesting point because i actually cannot remember a case in which that was an option for people. if registries exist, they are new. i know there are efforts to build them, but it is not on the radar of most of the researchers i have talked to. they think that sharing means putting their data on the web or giving them to an archivist. that might change the culture a little bit, but we have to have better examples of it that people can see. participant: i am just wondering if in any of your studies whether you have found any correlation between how much data sharing there is versus how much money is in the system? because the funding levels fluctuate over the years, my intuition would tell me that people are less likely to share when there is less money. ms. smith: this is a very interesting question, i would have guessed the same thing, but that is not what i see in practice, in the health sciences for example, where there is a lot of money. in practice, we are seeing more data sharing in the health sciences than other fields where there is less funding. dr. harley: one of the things we talked to people about in our research is what do you see in the future? there appear to be different trajectories, depending on the history and economics of the discipline. they can be rather divergent. whether or not the value systems are going to change with the way data are captured and described in these fields, is unknown at this point. participant: there is an important question here about data citation and whether that is what we are asking people to do? i think we should keep in mind that data publication and data citation are both metaphors taken from the journal publication system, and i think metaphors are tricky. the danger with any metaphor is to say that it is exactly the same. we want to define what is it that we want to consider about data and how we want the data to be used differently from journals, but still using that same sort of metaphor. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.93 part four examples of data citation initiatives for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.95 13 the datacite consortium jan brase1 datacite i would like to give you an overview of what datacite is doing. the idea behind our project is that science is global and therefore we need global standards to do it. we need some global workflows and cooperation between global players such as global data centers and publishers. of course, science is also carried out locally. scientists themselves do global science but they are embedded in their local institutions, libraries, and funding agencies. so, that is the paradox that we are dealing with in our project. we want to have global answers but at the same time want to act on a very local basis. that was the main motive behind founding datacite as a global consortium of local institutions. on the one hand, datacite is carried out by members like the technische informations bibliothek (the german library of science and technology), the california digital library, and the british library. these institutions act locally with local data centers. for example, the british data center works with the british library and uses their services as their local coverage partner. on the other hand, datacite itself as a global organization consisting of other global organizations, such as publishers. this is important, because the publishers now only have one central partner to work with on data citations and do not have to deal with the dozens of data centers individually on the local level. this is the list of the current membership of datacite. there are the 15 members from 10 countries. figure 131 list of datacite members.   1 presentation slides are available at http://sites.nationalacademies.org/pga/brdi/pga064019. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.96 developing data attribution and citation practices and standards  the next question is what are we doing? simply, datacite is by definition a registration agency of digital object identifiers (dois). datacite assigns doi names to datasets. this is one of our main pillars. we are also actively involved with our members to work on standard definitions. we try to activate those standards for data citation. we also have plans to establish a central metadata portal, where you will have free access to the metadata of all content that we have registered. one of our main functionalities is to provide doi names for datasets. we all agree that identifiers for datasets are important to make data citation possible. let me explain with an example of citations for a dataset: storz, d. et al. (2009): planktic foraminiferal flux and faunal composition of sediment trap l1k276 in the northeastern atlantic. http://dx.doi.org/10.1594/pangaea.724325. as a supplement to the article: storz, david; schulz, hartmut; waniek, joanna j; schulzbull, detlef; kucera, michal (2009): seasonal and interannual variability of the planktic foraminiferal flux in the vicinity of the azores current.  deepsea research part ioceanographic research papers, 56(1), 107124, http://dx.doi.org/10.1016/j.dsr.2008.08.009. first, we have a dataset citation. you will see that one of the good things about using doi names is that it actually has the same look and feel as a classical journal citation, with the title and the doi name that you can click on to access the article. it also has the data center as the affiliation. the most useful part, however, is that you can click on the doi to directly access the data. this allows you to download the data into your system for your own analysis or visualization. if you decide to reuse the data for your own work, the fact that the data has a doi name allows you to cite the data and give the original author credit for using those data. if the user goes to the webpage of the article, that person then sees that there is data available for this article. in contrast to the article that may only be available to paying customers (subscribers), the access to the data is free of charge. so, if the user is interested in the article, he or she can look at the data first and then decide what to do. that is one of the fundamentals of datacitešwe believe that the data that support the article should be freely available. in cooperation with the publishers, we have designed our system in a way that even if you do not have the right to look at the article (without paying) and you can only access the abstract and the table of contents, the link to the data is displayed and access to the data is free of charge. in a way, this is a winwin situation for the publishers because the availability of the data enhances the value of the article and promotes its use, while the publishers do not any lose revenues. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.the datacite consortium  97 finally, let me briefly tell you where are we at this point. datacite has registered over a million records with doi names. we have also published metadata schema2 that we use for all records. we just released the beta version of datacite metadata store online in july 2011: see http://search.datacite.org. in 2012, we expect to have around 800,000 datasets in the metadata store and hope to have all records available in the middle of the year. as for next steps, we are working with other organizations, such as thomson reuters science, to index our content. the metadata are freely available to harvesters via http://oai.datacite.org. we are also working with elsevier and the pangaea data center and other publishers to find more articledata links.   2 http://schema.datacite.org. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.99 14 data citation in the dataverse network® micah altman1 director of research, mit libraries head/scientist, program for information sciences nonresident senior fellow, the brookings institution  overview of the dataverse network the dataverse network ® (dvn) project is an open source application for discovering, publishing, citing, and preserving research data. (see crosas 2011 for a detailed description) created by gary king (2007), and based on over a decade of digital library research (altman, et al. 2001), the dvn has been described as the ﬁstate of the practiceﬂ in open source data sharing infrastructure (novak, et al. 2011). the dvn functions as an institutional repository system designed specializing in quantitative data, a federated library and catalog system, and as virtual archiving system. there are multiple installations of the dvn software. each ﬁdataverse networkﬂ is hosted on its own host server, and hosts numerous virtual archives, each known as a ﬁdataverse.ﬂ a dataverse can act as a virtual archive for a journal, a department, a research group, an individual, or a library. the dvn server allows an institution to provide unified backup services, citation generation, discovery, preservation and other sorts of core services while enabling the owners of the virtual archives to manage content deposit, dissemination, terms of use, and branding. dataverse networks can be used to host metadata, text, images, and data from many different disciplines. the first version of the dvn allowed deposit of any format, and provided enhanced services for individual microdata tables. support for other data structures, such as social network data and timeseries, have been added as the software evolved and as the evidence base of social science research has shifted (see altman & klass 2005; altman & rogerson 2008). the first installation of the dvn was at the harvard institute for quantitative social sciences (iqss). that installation has been officially incorporated into the harvard library system, but remains open to researchers worldwide. it provides search, data access, and data hosting, at no charge. the harvard dvn functions also as a federated catalog of over 50,000 social science data data sets, including the holdings of the data preservation alliance for the social sciences [altman, et al. 2009] and the council of european social science data archives. the current holdings of the harvard dvn are predominantly used for social science research. additional dataverse networks are hosted at the university of north carolina, in thailand, and elsewhere around the world. most dvn™s are federated using the openarchives metadata protocol, and so almost five hundred virtual archives may be searched and browsed from a single location.  1 this is an updated, corrected, and expanded version of the original presentation, which is available at http://sites.nationalacademies.org/pga/brdi/pga064019. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.100 developing data attribution and citation practices and standards a data citation in depth we will start with a brief example. figure 141 below shows the home page from my own scholarly virtual archive (or ﬁdataverseﬂ). this is virtual archive contains all of the data that i have disseminated as part of my own research. note that although this virtual archive is organized as a single collection, many of the datasets listed actually are distributed by different archives, and may be stored in different locations. figure 141 micah altman™s dataverse. shown below is the catalog page for a particular dataset in this collection. the catalog page for this dataset shows descriptive information about it. the ﬁdata and analysisﬂ tab (not shown), allows users to download the data itself, and to perform online analyses. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.data citation in the dataverse network® 101 figure 142 catalog page for data to replicate ﬁfrom crayons to computers: the evolution of computer use in redistricting.ﬂ while this dataset is in my own scholarly dataverse, it is formally ﬁdistributedﬂ by the murray research archive, as indicated through the distributor information in the citation above. this indicates that the dataset was reviewed by the murray archive and complied with its selection policies. this also indicates that the archive takes ﬁarchivalﬂ responsibility for it, and will continue to ensure its longterm availability. other datasets in my collection are distributed by icpsr, and by the iqss dataverse network. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.102 developing data attribution and citation practices and standards the dvn system provides several varieties of citation related to the dataset. one citation, shown below, is used for articles related to the dataset. in this case, the dataset is explicitly identified as a ﬁreplication datasetﬂ, containing all of the information necessary to replicate a published article.  figure 143 citation for original publication with which data are associated. the printed citation for the dataset is shown below. note that it shows a separate persistent identifier and separate publication information, because, in this case, data and publication are distributed separately. figure 144 printed data citation example. the dataverse network software produces citations that are based on a community standard first proposed by altman and king (2007). the hundreds of virtual archives (ﬁdataversesﬂ) using the system automatically produce these citations for all of their datasets. this standard is also in use by the member archives of the data preservation alliance for social sciences (datapass 2012). the altmanking standard is lightweight, as it requires a minimal number of elements, and does not impose a specific presentation on these. data citations that follow this standard will, at a minimum, include the following information: author(s), date, title, and a persistent identifier that is drawn from any standardized and webresolvable identifier scheme (e.g., doi, handle, purl, lsid). 2 this minimal citation standard has a number of extensions: 1. first, where feasible, the standard recommends the inclusion of a uri corresponding to the persistent identifier; a universal numeric fingerprint, which provides fixity information; and explicit versioning information. 2. second, a default ordering of the elements is suggested for presentation and parsingšbut elements may be reordered if explicitly labeled. 3. third, the standard is extensible, and citations can include elements from any other xmlized citation schema, simply by using xpath syntax and labeling conventions. the dataverse network system currently uses these extensions to indicate the producer and distributor for the dataset.   2 in addition, datapass recommends that (a) cited data be deposited in a archive, and (b) data citations be included in along with other publications in journal articles and indicesšgenerally data citations should not be presented only in an adhoc location, such as the publication text, acknowledgement, figure labels or substantive footnotes. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.data citation in the dataverse network® 103 4. fourth, the standard provides an extension for citing subsets (portions) of the dataset. the dataverse network system uses this to create citations for each of the data extracts it produces. these points are illustrated in the citation below, which is associated and drawn from a separate study in the independent odum dataverse network. this citation includes optional extension fields imported from the data documentation initiative (ddi) metadata standard, which are used to indicate the data distributor, version, and the attributes of the data subset extracted. note that separate unf™s are calculated for both the subset extracted and the dataset in its entirety.  center for survey research, 20111208, "firearms 2004," http://hdl.handle.net/1902.29/10863 unf:5:kkz60uzuqlvi8ckt3i82ga== odum institute for research in social science [distributor] v1 [version]  caseid,communit,people [vargrp/@var(ddi)];  unf:5:wjxtjid/gi9iczcvbeeyiq==  figure 145 data subset citation example. the universal numeric fingerprint (unf) is a novel part of the altmanking citation standard. (the first version of the unf algorithm was developed in altman, et al 2003, and it has been extended in altmanking 2007, and altman 2008.) a unf is a short, fixedlength string that summarizes the entire dataset. thus it provides ﬁfixity,ﬂ enabling a future user of the dataset to verify that the dataset they possess is semantically identical to the one originally cited in a publication. similar to a cryptographic hash function, the unf is tamperproof, and will change if the values represented by the dataset change. unlike a cryptographic hash, the unf is invariant to the specific file format used to store/serialize the dataset. a unf works by first translating the data into a canonical form; computing an approximation of that canonical form using a specified precision; and applying a cryptographic hash function to the approximation of the canonical object. the advantage of canonicalization is that it renders unfs formatindependent: if the data values stay the same, the unf stays the sameševen when the data set is moved between software programs, file storage systems, compression schemes, operating systems, or hardware platforms. extensions to the unf algorithm (altman 2008), describe methods for computing unfs over more complex objects, using recursion. data citation use cases and principles altman and king (2007) motivate their data citation standard on the basis of provenance, replicability, and attribution. data citations should contain information that is sufficient to: locate the dataset that is referred to as evidence for the claims made in the citing publication; verify that the dataset one has located is the semantically identical to the dataset used by the original authors of the citing publication; and correctly attribute the dataset. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.104 developing data attribution and citation practices and standards in the last five years, citation practices have evolved. to explore this issue, the institute for quantitative social science convened a workshop for leaders in publishing, data archiving, and data citation research, with the aims of identifying use cases and principles for data citation. the discussions facilitated through this workshop lead to a consensus on a number of additional core use cases, operational requirements, and principles. time prohibits describing these use cases and operational requirements in detail. to summarize they were grouped into five categories: attribution. data citations are used for enabling appropriate legal and scholarly attribution for the cited work.3 persistence. citations are used to refer to, and to manage, objects that are persistent.  access. citations are used to facilitate short and long term access to the object, by humans and by machine clients.  discovery. citations are used to locate instances of the dataset; and as part of the process of discovering derivative, parent, and related works  provenance. citations are used to associate published claims with evidence supporting them, and to verify that the evidence has not been altered. moreover, the workshop identified a ﬁfirst principleﬂ for citing data: data citations should be treated as firstclass objects for publication. this principle has a variety of implications that depend on the specific context of publication. notwithstanding, workshop participants identified two broad corollaries: citations to data should be presented along with citations to other worksštypically in a ﬁreferencesﬂ section; and data should be made as easy to cite as other worksšpublishers should not impose additional requirements for citing data, nor should they accept citations to data that do not meet the core requirements for citing other works. the workshop articulated three additional principles, based on discussions of the core uses of data citation:  at a minimum, all data necessary to understand assess extend conclusions in scholarly work should be cited. citations should persist and enable access to fixed version of data at least as long as the citing work exists. data citation should support unambiguous attribution of credit to all contributors, possibly through the citation ecosystem.  these principles have implications for the entire ecosystem of publicationšthere are implications for authors, editors, publishers, and software developers. neither standards documents nor software can ensure that data is cited properlyšbut both can help. the dataverse network system, and the citations produced through it, appear consistent with these principles.  3 note that these scholarly and legal attribution are conceptually separable: the first type of attribution is defined in terms intellectual property rights, whereas the second is defined in terms of scholarly norms. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.data citation in the dataverse network® 105 references  altman, m., gill, j., & mcdonald, m. (2003). numerical issues in statistical computing for the social scientist. new york: john wiley & sons.  altman, m. (2008). a fingerprint method for scientific data verification. in t. sobh (ed.), proceedings of the international conference on systems computing sciences and software engineering 2007 (311316). new york: springer netherlands. retrieved from http://www.box.net/shared/0x8ld06hceg0ltpjyfu4 altman, m., & crabtree, j. (2011). using the safearchive system : tracbased auditing of lockss. archiving 2011 (165170). society for imaging science and technology. retrieved from http://www.imaging.org/ist/store/epub.cfm?abstrid=44591 http://www.box.net/shared/8py6vl9kxivo6u21rkn8  altman, m., and king, g. (2007). a proposed standard for the scholarly citation of quantitative data. dlib magazine, 13(3/4), 1082œ9873. ssrn. retrieved from http://www.dlib.org/dlib/march07/altman/03altman.html.  altman, m., and klass, g. m. (2005). current research in voting, elections, and technology. social science computer review, 23(3), 269273. retrieved from http://ssc.sagepub.com/content/23/3/269.full.pdf. http://www.box.net/shared/1xlf6n7erk2bgu34z0nj.  altman, m., andreev, l., diggory, m., king, g., sone, a., verba, s., kiskis, d. l., et al. (2001). a digital library for the dissemination and replication of quantitative social science research: the virtual data center. social science computer review, 19(4), 458470. sage publications. retrieved from http://www.box.net/shared/d3cf8u0gtyml2nqq3u2f.  altman, m., crabtree, j., donakowski, d., maynard, m., pienta, a., & young, c. (2009). digital preservation through archival collaboration: the data preservation alliance for the social sciences. the american archivist, 72(1), 170184. retrieved from http://archivists.metapress.com/content/eu7252lhnrp7h188.  altman, m. and rogerson, k. (2008). open research questions on information and technology in global and domestic policis  beyond ﬁe.ﬂ ps: political science and politics xli.4 (october, 2008): 835837. retrieved from: http://journals.cambridge.org/action/displayfulltext?type=1&fid=2315612&jid=psc&volumeid=41&issueid=04&aid=2315604 http://www.box.net/shared/2tlvqxgnzu5p5sby2mzq. data preservation alliance for social sciences, ﬁdata citationsﬂ, 2012. web page. retrieved from: http://datapass.org/citations.html. crosas, m. 2011. the dataverse network: an opensource application for sharing, discovering and preserving data. dlib magazine. volume 17. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.106 developing data attribution and citation practices and standards king, g. 2007. an introduction to the dataverse network as an infrastructure for data sharing. sociological methods and research. 36:173199. novak, k., altman, m., broch, e., carroll, j. m., clemins, p. j., fournier, d., laevart, c., et al. (2011). communicating science and engineering data in the information age. computer science and telecommunications. national academies press. retrieved from: http://www.nap.edu/catalog.php?recordid=13282. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.107 15 microsoft academic search: an overview and future directions lee dirks1 microsoft research connections i would like to brief you on what we have been doing lately with the microsoft academic search service. it started as a research project that has been conducted at our beijing lab for almost eight years now. over the course of the last eighteen months, our team in redmond has gotten very involved in providing strategic guidance and input. currently, we are in the process of transitioning it from a research project into an operational service that microsoft research will provide to the community. it will be a free academic search engine for tracking academic papers, citation links, and all the various characteristics that can be extracted from papers. what we have been doing over the last six to nine months is working directly with open access repositories and publishers around the world to sign content agreements so we can get access to their papers. this is all about facilitating access to the papers. at present, we have 27 million papers across 14 domains, and we have another 100 million papers across more than 20 domains in the queue, pending indexing. we are going to expand our content about every three months, and are already actively evolving the site. all of the signed content agreements that i was referencing earlieršwith the various open access repositories and publishersšare to make sure that content providers are aware that we are making their data available for free. we are very interested in having the community use this service as widely as possible. i also would like to stress that we are being as transparent as possible in talking about the number of publications and authors that we have. as soon as possible, we are going to post a list of the publishers and all the sources of this material. we are also waiting for orcid to come online, at which point we intend to leverage their work and use their identifiers to help in the name disambiguation process. through the academic search service, people will have the ability to look at citations or publications on a cumulative or on an annual basis. the service also has some powerful visualization abilities. for example, we will have the ability to show a single author in connection with all the people that he/she has worked with in the past (e.g., coauthors). another thing i would like to highlight is the system™s ability to drill down into fields and subfields. for computer science, for example, you can look at the top authors, top publications, top conferences, journals, organizations, and other characteristics. (note that this ranking is solely based on citation counts we have calculated.) also, you can drill down into a subdomain of computer science and visualize, for example, publication activity using what we call the domain trend. we believe that domain trend is a very useful tool for helping researchers find coauthors, principal investigators, and even awards and people to invite to conferences. there is also the ability to do ranking across institutions and across countries.   1 presentation slides are available at http://sites.nationalacademies.org/pga/brdi/pga064019. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.108  developing data attribution and citation practices and standards  again, all of that information is free. we have been getting some good coverage lately, especially about some of the new functionalities of the system. here is a recent quote from nature2: ﬁ–meanwhile, microsoft academic search (mas), which launched in 2009 and has a tool similar to google scholar, has over the past few months added a suite of nifty new tools based on its citation metrics (go.nature.com/u1ouut). these include visualizations of citation networks (see 'mapping the structure of science'); publication trends; and rankings of the leading researchers in a field.ﬂ i would like to stress the fact that the work that we are doing here is for researchers and by researchers. that is something that we will always keep in mind when we grow and make this a more sustainable service. we are also very interested in changing our interface and not just doing citation analysis of papers, but eventually also of data. we are very interested in conducting research projects with the community. from our perspective, microsoft academic search is an open platform and we are going to be as transparent as we can about our work. we want to make sure that this service will accurately represent how science and academia work. we are going to make our domain coverage more extensive. we are also working on more partnerships. for example, we are an associate member of datacite and we are a founding sponsor of orcid. finally, we are tracking these and other activities to see when and how we can integrate them into our service.  2 butler, d. 4 august 2011 computing giants launch free science metrics. nature 476, 18 (2011) (doi:10.1038/476018a).  for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.109 16 data centerlibrary cooperation in data publication in ocean science roy lowry1 british oceanographic data center let me start by providing some background information about the key players in the partnership that has come together to foster data publication in the ocean sciences. the scientific committee on oceanic research (scor) is an international nongovernmental organization formed by the international council of scientific unions (icsu, now the international council for science) in 1957. the committee has scientists from 36 countries participating in different working groups and steering committees. it promotes international cooperation through planning and conducting oceanographic research, and solving methodological and conceptual problems that hinder research. the second partner organization is the international oceanographic data and information exchange (iode). this is a data and information exchange program of unesco™s intergovernmental oceanographic commission (ioc), commenced in 1961. the main goal of this program is to establish national oceanographic data centres or coordinators in ioc member states in order to acquire, enhance, and exchange oceanographic data and information. it also aims at extending the national oceanographic data center network through training and capacity building. the last player in this partnership is the marine biological laboratory woods hole oceanographic institution (mblwhoi) library. the woods hole scientific community library has a strong interest in data publication in digital libraries. the digital library archive (dla) contains:  whoi archives; historical photographs and oceanographic instruments;  scientific data, e.g., echo sounding records from whoi research vessel expeditions; technical report collections; and  maps, nautical charts, geologic and bathymetric maps, and cruise tracks. the group had a series of meetings between june 2008 and april 2010 and there is another meeting scheduled for november 2011. the group's objectives are to:   1 presentation given by sarah callaghan and slides are available at http://sites.nationalacademies.org/pga/brdi/pga064019. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.110 developing data ttribution and citaton practices and standards engage the iode data center and marine library communities in data publication issues.  provide a network of hosts for cited data. motivate scientists through reward for depositing data in data centers.  promote scientific clarity and reuse of data. however, engaging iode data centers effectively in data publication and distribution encounters a problem of different approaches. one model is as follows. data can change significantly as additional value is added by the data center through metadata generation, quality control (e.g., flagging outliers), and the like. the ﬁbest availableﬂ data are served by the data center to other users during data evolution, which means that the dataset is continually changing with no snapshots preserved or formal versioning during workup. this makes it difficult to go back and get the same data that you got a year or six months ago. the second model is the digital library paradigm. a dataset is a ﬁbucket of bytes,ﬂ which is:  fixed (checksum should be a metadata item)  changes generate a new version of the dataset  previous versions must persist  accessible online via a permanent identifier  usable on a decadal timescale (using standards such as the open archive information standard)  citable in the scientific literature to provide links to marine libraries  discoverable to summarize these data distribution paradigm issues, the problem is to find ways for iode data centers to engage in digital library practices while leaving current infrastructure largely intact. change should happen gradually through evolution and not revolution. probably the best way to do that is through pilot projects at the british oceanographic data center (bodc) and whoi. to that end, the bodc has started a pilot project activity with a decision to establish a repository at iode called published ocean data (pod), where data will be accessible to many data centers, with technical quality control and good longterm stewardship credentials in place. the process to achieve this goal has taken longer than anticipated due to extended discussions and resource availability. however, specifications are being produced and accepted now, and the actual building of the systems will start in the fall of 2011. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.data centeršlibrary cooperation data publication in ocean science 111  as for the whoi pilot project, the mblwhoi library has loaded a number of datasets from the national science foundation™s (nsf) biological and chemical oceanography data management office (bcodmo). the datasets have been associated with published journal articles. for example dx.doi.org/ 10.1575/1912/4199, resolves to: https://darchive.mblwhoilibrary.org/handle/1912/4199). the group is also working with a scientist who is submitting a paper to the american geophysical union in september, with a complete publishing process use case including doi assignments to datasets supporting specific figures. these dataset citations will be incorporated in the final version of the paper, subject to publisher approval. furthermore, talks are underway concerning incorporation of the woods hole open access server (whoas) repository in an nsf proposal data management plan. finally, this partnership also has plans for collaboration with bcodmo to develop an automated publication system for all data center accessions. let me conclude with a summary of our future plans. we will:  complete the pilot projects identified earlier.  engage other data centers in data publication through reporting our experiences and disseminating knowledge through appropriate routes, such as workshops, conferences and other publications. engage seadatanet ii when it starts later in 2011.  continue outreach activities to scientific, data management, and marine library communities.  a further meeting is planned to be held in liverpool, uk, on november 34, 2011.  expand bodc activities into an operational service.  develop the mblwhoi library bcodmo ingest system. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.113 17 data citation mechanism and service for scientific data: defining a framework for biodiversity data publishers vishwas chavan1 global biodiversity information facility i am going to focus on how we are working to resolve the issue of data citation for biodiversity data at the global biodiversity information facility (gbif), located in copenhagen, denmark. for those who have not heard about gbif, it is a multilateral intergovernmental initiative established in 2001 with 52 countries as members and 47 international organizations. gbif™s main objective is to facilitate free and open access to biodiversity data. our data is available through a portal and currently, includes 312,000,000 data records about existence of lands and animals across the globe from over 1800 data resources that has been contributed by 342 data publishers. why do we think that data citation is important? we believe that data citation will encourage our data publishers to publish more and more datasets. therefore, it will improve data discovery. it will also provide some kind of encouragement for data preservation. furthermore, it will provide incentives to those who use the data through improving the credibility of the interpretations that are based on the data. what is the current practice of data citation in the gbif network? let me explain this with an example. a user comes to gbif's portal and searches for the term ﬁpanthera tigris.ﬂ she gets 696 records from 37 different datasets, which are published by 31 different publishers. the current citation style just says ﬁaccess through gbif data portalﬂ and lists out all the access points of those 37 datasets. the problem with this practice is that it doesn't tell me what was the search string unless and until i can make an explicit statement about it, how many records were retrieved, how many data publishers contributed to the retrieved data, when search was carried out, who are the original contributors of the data, and who plays what role in the process from collection to publishing of the data? so, certainly there is a need to work around these challenges. what is needed is a data citation mechanism with a defined citation style that can provide recognition to all stakeholders involved with their roles, such as who is the producer of the data, who is the publisher, who is the aggregator, and who provided curation service to the data. given the complexity of our network, we require cascading citations, which are citations within the citations. furthermore, we need a data citation service whereby a publisher can go and register its citation and all documents of metadata. finally, we need a discovery service, which resolves to the fulltext citation and links to the underlying data. one of the first things that we think we need is a best practices guide for how to cite data. for that, we require two types of recommended styles. one is related to publisher supplied dataset citation and the other is related to query based citations. the publisher supplied dataset citation would obviously need to consider the types of publishers (e.g., an individual, a group of  1 presentation slides are available at http://sites.nationalacademies.org/pga/brdi/pga064019. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.114 developing data attribution and citation practices and standards individuals, or an institution). we also need to recognize individual's role in creating the dataset. we need also to identify when it was first released or whether it is a onetime release or frequently updated. also, the citation should link back to the primary uri of the dataset and then the citation itself needs to have a persistent identifier (preferably doi) so the entire citation string can be resolved. also, we need to consider the date of the first release, the latest updates, and the number of data records that we can actually access from a particular dataset. table 171 provides a sampling of gbif™s styles for potential citation strings or styles for the publisher supplied citations. complete formulation short formulation style 1 publisher (individual) with onetime release of dataset publisher (year), <title of the data resource>, <total nos. of records>, published <modes of publishing>, <primary access point>, released on<release date>, <persistent identifier>. publisher (year), <persistent identifier>. style 2 publisher (individual) with frequent update or release of dataset publisher (year). <title of the data resource>, <total nos. of records>, published <modes of publishing>, <primary access point>, first released on<release date>, <current version no. or last updated/released on (date)>, <persistent identifier>. publisher (year first published/released ). <version no., or last updated/released on (date)>, persistent identifier. style 3 publisher (group of individuals) with one time release of dataset publisher 1, ..... and publisher n (year). <title of the data resource>, <total nos. of records>, published <modes of publishing>, <primary access point>, released on <release date>, <persistent identifier>. publisher 1 et.al. (year). persistent identifier. style 4 publisher (group of individuals) with frequent update or release of dataset publisher 1, ..... and publisher n <year). <title of the data resource>, <total nos. of records>, published <modes of publishing>, <primary access point>, first released on<release date>, <current version no. or last updated/released on (date)>, <persistent identifier>. publisher 1 et.al. <year (year first published/released )>. <version no., or last updated/released on (date)>, persistent identifier. style 5 institute/consortium (multiple contributors) with one time release of dataset <publisher as institution / research group / consortium> (year), <title of the data resource>, <total nos. of records>, <contributed by contributor 1(role), contributor 2 (role)..... contributor n(role)>, <published (modes of publishing)>, <primary access point>, released on<release date>, <persistent identifier>. <publisher as institution / research group / consortium> (year), <persistent identifier> style 6 institute/consortium (multiple contributors) with frequent update or release of dataset. <publisher as institution / research group / consortium> <year (year first published / released )>, <title of the data resource>, <total nos. of records>, <contributed by contributor 1(role), contributor 2 (role)..... contributor n(role)>, <published (modes of publishing)>, <primary access point>,<version no., or last updated/released on (date)>, <persistent identifier>. <publisher as institution / research group / consortium> <year (year first published / released )>, <version no., or last updated/released on (date)>, <persistent identifier>  for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.data citation mechanism and service for scientific data 115 115 in the case of the query based citations, where we need to have citations within citations, there are two types of citations that we think are required. one is query based citations and the other publisher supplied dataset citations. such a citation needs to have multiple types of persistent identifiers that have been assigned or used by publishers themselves. so, going back to the example of the user who searched for the term ﬁpanthera tigrisﬂ, a hypothetical exemplification of this search is presented in figure 171. this query based citation will resolve to complete computer citation and it can also link back to the snapshot of the retrieved data, which are cited. this is how it will look like when you resolve the doi: http://data.gbif.net (2010). search string:panthera tigris, 696 records, contributed by 37 data resources, user doi: 09.1111/gbif.9.11.444, accessed on 04/11/2010, 10:03:30. 1. louisian state university (2007), museum of natural science: collection of mammal, 36000 records. contributed by patterson dn (principal investigator, architect, author), sandeep pk (author, curator), fieldman ln (author, developer), remsen d (curator, validator), published online http://www.museum.lsu.edu/mns/mammcoll.hml, released on october 2007, doi:09.1111/lsu.9.11.559. 2. michigan state university (2001 ), msu vertebrate collection, 76523 records. contributed by cook dk (principal investigator, author, curator, validator), hirsh l (author, architect, developer), lane mp (manager, author, curator)............, morris jh (curator), published online http://musuem.msu.edu/researchandcollections/dvnh, first released on 01/10/2001, last updated on 18/01/2010, urn:lsid:msu.org:observation:541. 3. cursada pk, bello j, and ajk moelicker (2006), natural history museum rotterdam: mammal collection, 1123 records, published online, http://www.nlbif.nl/nhmrmc/, released on 7 july 2006, http://nhmr.nl/ark:/1205/693xz693. .......................................................................................................................................................................................................................................................................................................................... 37. rumble kj (1998 ). vertebarte collection of rumble 19601999. 786 records, published online, http://www.sbnature.org/rumblecollection/, first released on 13/09/1998, last updated on 27/01/2010, http://hdl.oclc.gov/sbnature/5678. institutional dataset, onetime release, doi institutional dataset, frequent update, lsid full text composite citation multiple authors, frequent update, ark user driven citation single author, frequent update, handel http://data.gbif.net (2010). user doi:09.1111/gbif.9.11.444.  figure 171 hypothetical search result.  let me conclude with a summary of the implementation and next steps. the main challenge to implement this mechanism is the complexity of data management itself. how do we make sure that all our data publishers are going to follow through the citation style that is being proposed? there is also the complexity of the data network because many publishers publish the data through more than one access point.  therefore, we urgently need to have all these citation styles propagated in the form of a best practice guide. however, we also need to remember that there are social challenges related to updating the current practices. finally, somebody has to come forward to run the data citation service. these are some of the challenges that we are currently trying to address. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.117 18 how to cite an earth science dataset? mark parsons1 university of colorado i represent the federation of earth science information partners (esip). it is a federation of more than a hundred data centers and related organizations, predominantly in the united states. the primary sponsors of esip are federal science agencies nasa, noaa, epa, and there are several other sponsors such as nsf and usgs that are getting increasingly more involved in our work. i am going to focus my talk on best practices and guidelines of how to cite science data. i also want to mention that some of my presentation will be related to the international polar year (ipy), a very large international and interdisciplinary project that started to work on these issues. there is a lot of input going into the process of creating data citation and attribution guidelines at esip. we hope that these guidelines will be adopted by the general assembly in january of 20122. the main purposes of data citation as we see them are:  credit for data authors and stewards. accountability for creators and stewards.  track impacts of the dataset.  assist data authors in verifying how their data are being used. aid reproducibility of research results through a direct, unambiguous connection to the precise data used.  the last purpose is the primary, most important purpose and the most difficult to achieve. i also want to note that we see citation as a reference and a location mechanism, but not as a discovery mechanism, per se. data citation in the earth sciences is currently done using one of these approaches or styles: citation of traditional publication that actually contains the data, e.g., a parameterization value.  not mentioned, just used, e.g., in tables or figures. reference to name or source of data in text. url in text (with variable degrees of specificity). citation of related paper (e.g., the uk climate research unit recommends citing their wellknown surface temperature records using two old journal articles which do not contain the actual data or full description of methods)  citation of actual data set typically using recommended citation given by data center.  citation of data set including a persistent identifier or locator, typically a doi.   1 presentation slides are available at http://sites.nationalacademies.org/pga/brdi/pga064019. 2 the guidelines were adopted in january. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.118 developing data attribution and citation practices and standards the national snow and ice data center (nsidc) distributes a variety of different snow cover products derived from the moderate resolution imaging spectrometer (modis). the results of a quick analysis of how many scientific papers mention use of ﬂmodis snow cover dataﬂ (according to google scholar) and how often the data sets themselves are formally cited shows a huge disparity, illustrating the infrequency of proper data citation in practice. moreover, the lack of data citation standards introduces the possibility that informal references to data do not point to the exact data set actually used. 0!100!200!300!400!500!6002002!2003!2004!2005!2006!2007!2008!2009!ﬁmodis snow cover dataﬂ in google scholar1.3%1.0%0.7%0.7%1.3%0.9%1.3%1.7%formal citationtotal entries figure 181 modis snow cover data in google scholar.  there are a number of data citation guidelines available to scientists. these include the ones from the international polar year and datacite project. also, institutions such as nasa and noaa request acknowledgments. overall, approaches range from specific data citation, to general acknowledgment, to recommending citing a journal article or even a presentation. this is reflected also in the results of this study titled ﬁdata citation in the wildﬂ by enriquez et al. (2010): we found that few policies recommend robust data citation practices: in our preliminary evaluation, only onethird of repositories (n=26), 6% of journals (n=307), and 1 of 53 funders suggested a best practice for data citation. we manually reviewed 500 papers published between 2000 and 2010 across six journals; of the 198 papers that reused datasets, only 14% reported a unique dataset identifier in their dataset attribution, and a partiallyoverlapping 12% mentioned the author name and repository name. few citations to datasets themselves were made in the article references section.3 this shows clearly that the data author is not being fairly credited.  3 available at: http://openwetware.org/wiki/dataone:notebook/summer2010. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.how to cite an earth science dataset 119 in terms of measuring the impact of a data set, there are some measurement issues that make this process a bit challenging4. for example, data are not used in isolation. often different data are combined or used with models and other analytic techniques. also, impacts may be indirect ( i.e., resulting from development of information, papers, tools, etc. that relied on derived data or products); they may be delayed (i.e., months or years for a peerreviewed publication to be released, or a decision to be made and implemented); they may be unexpected (e.g., a new scientific discovery or a novel application of data collected for a different purpose); or they may be hard to compare (e.g., in scientific, economic, or ethical terms). nevertheless, it is still important to try to track the use and impact of a data set because we need to justify investment in data acquisition, maintenance, distribution, and longterm stewardship. we also need to help the community become more effective and efficient in data management and use. there are different possible citation metrics. these include: qualitative  examples of data use and impacts in key papers, discoveries, decisions. assessment of broader impacts such as influence of data on attitudes and thinking (e.g., the apollo 8 image of the earth). quantitative  counts of papers that cite data in peerreviewed journals. weighted indicators of data citations (e.g., type and quality of citation, impact of journal). quantitative and qualitative:  number of data citations in top peerreviewed scientific journals and key reports by decisionmakers.  data usage in other peerreviewed journals, textbooks, reports, magazines, documentary films, online tools, maps, blogs, twitters, and the like.  however, as heather piwowar notes, tracking dataset citations using common citation tracking tools does not work. traditional fields, such as author and date, are too imprecise and the web of science, scopus, and other scientific publisher tools do not handle identifiers.5 i think that we need two basic strategies. one is that archives and data centers need to provide consistent and precise recommendations on how their data should be cited. the other strategy is more of the social strategy trying to get the publishers and the educators on board with the whole concept of data citation. i am going to focus on the first strategy in this presentation.  4 see also chen, r. s. and downs, r. r. (2010). evaluating the use and impacts of scientific data. national federation of advanced information services (nfais) workshop, assessing the usage and value of scholarly and scientific output: an overview of traditional and emerging approaches. philadelphia, pa, november 10, 2010. http://info.nfais.org/info/chendownsnov10.pdf. 5 see piwowar™s blog at http://researchremix.wordpress.com/2010/11/09/trackingdatasetcitationsusingcommoncitationtrackingtoolsdoesntwork/. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.120 developing data attribution and citation practices and standards below is the basic esip data citation model shown in contrast to the datacite guidelines available at the time. per datacite: creator. publication year. title. [version]. publisher. [resource type]. identifier. per esip: author(s). release date. title [version]. [editor(s)]. archive and/or distributor. locator. [date/time accessed]. [subset used]. i will use the rest of the talk to describe some of these differences and why we think they are important. the first difference is that esip explicitly allows the recognition of roles other than the data creator or author. we call this ﬁeditorﬂ, but there are multiple data management roles that might be captured. whether or not they are appropriate can be open to question, but this approach gets a lot of traction with data stewards because particularly in earth sciences, data stewards frequently may have a significant role in developing and compiling the data sets and sometimes doing some quality control. they have similar levels of credit and accountability as the original authors do and i think that is important to recognize. for example, in the example below, the data authors were the designers of a large field experiment. the editors were responsible for managing the process of entering field data from notebooks, conducting manual and automated quality control, determining data formats, writing documentation, and so on. cline, d., r. armstrong, r. davis, k. elder, and g. liston. 2002, updated 2003. clpxground: isa snow depth transects and related measurements ver. 2.0. edited by m. parsons and m. j. brodzik. boulder, co: national snow and ice data center. data set accessed 20080514 at http://nsidc.org/data/nsidc0175.html.  another concept i want to present is the notion of the identifier versus the locator. the easiest way for us to understand these concepts is probably to look at the human example. human id: mark alan parsons (son of robert a. and ann m., etc.). ł every term defined independently (only unique in context/provenance). ł alternative like a social security number requires a very well controlled central authority.  human locator: 1540 30th st., room 201, boulder co 80303. ł every term has a naming authority. data set ids: data set title, filename, database key, object id code (e.g., uuid), etc. data set locators: url, directory structure, catalogue number, registered locator (e.g. doi), etc.  if we look at the human id, every term is defined independently and it is only unique in a certain context. we could use a title in combination with a location to find the relevant person, but it for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.how to cite an earth science dataset 121 would not necessarily be the right person. s/he might have retired. we could use his or her identifier but that may not describe her location or the person may have moved. this may be simplistic, but i see this same situation with data. there are data set ids, some of them are informal, like dataset titles, and others are very formal, like a uuid. there are also data set locators like urls or some registry based system like dois. the point is that the locator and identifiers are different things, but sometimes locator can be used as an identifier (e.g., the person working in this position at this address). hence the general use of the term ﬁidentifierﬂ such as in doi, is better described as a locator. indeed it is the registration of the location information in the doi scheme that makes it attractive to groups like datacite: one of the main purposes of assigning doi names (or any persistent identifier) is to separate the location information from any other metadata about a resource. changeable location information is not considered part of the resource description. once a resource has been registered with a persistent identifier, the only location information relevant for this resource from now on is that identifier, e.g., http:// dx.doi.org/10.xx. 6 duerr et al (2011)7 conducted an assessment of identification schemes for digital earth science data as summarized in this diagram i adapted from their paper.  id schemedata setitemdata setitemdata setitemdata setitemurl/n/ipurlxrihandledoiarklsidoiduuidunique identiþerunique locatorcitable locatorscientiþcallyunique idlocatorsidentiþersgoodfairpoor figure 182 assessment of identification schemes for digital earth science data. source: duerr et al (2011).   6 datacite metadata scheme for the publication and citation of research data, version 2.2, july 2011. 7 duerr, r., r. downs, c. tilmes, b. barkstrom, w. lenhardt, j. glassy, l. bermudez, and p. slaughter. 2011. on the utility of identification schemes for digital earth science data: an assessment and recommendations. earth science informatics: 122. http://dx.doi.org/10.1007/s1214501100836. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.122 developing data attribution and citation practices and standards the figure summarizes how different identifiers are more suitable for different purposes, and that often it depends on whether the scheme is actually a locator or an identifier. (note that the lsid is a locator; but also the objectid part of it is an identifier and most people use a uuid for the objectid part of it.) also the ark could be considered a bit better than the rest of the locators because it has additional trust value, but the doi stands out as the most appropriate locator for citation. why the doi? although the doi is not perfect, it is well understood and accepted by publishers, and datacite is working with thomson reuters to get data citations in their index. this broad acceptance gives dois a small edge, but, there are still some issues that need to be resolved. for example, what is the citable unit that should be assigned a doi? is it a file or a collection of files and, if so, how many? how do we handle different versions? when does a new version get a new doi? how do we handle data that have been retired and deleted? does their doi persist? what does it point to? overall, we believe these issues can be largely resolved by, a welldefined versioning scheme, good tracking and documentation of the versions, and due diligence in archive and release practices. so, it is not a technical problem so much as a social problem demanding good professional practices. here are some initial suggestions on versioning and locators. at my data center, we did a study looking at different types of data, from satellite data, modeling output, historical photographs, to interviews and transcripts. we have the notion of a major version, a minor version, and an archive version. the archive version is not publicly available, it is just for us to track any changes in the archive. what constitutes a major or a minor version has to be done on a casebycase basis. an individual steward has to work with their providers to figure it out, but in general, something that affects the entire dataset is going to be a major version. a small change such as changing a land mask might be a minor version. dois should be assigned to major versions. old dois for old versions should be maintained even if the data are no longer available. the old dois should point to some appropriate page that explains what happened to the old data if they were not archived. the older metadata record should remain with a pointer to the new version and with explanation of the status of the older version data. major and minor versions (after the first version) should be exposed with the data set title and recommended citation. and while minor versions don™t get a new doi, they should be explained in documentation, ideally in filelevel metadata. finally, applying uuids to individual files upon ingest aids in tracking minor versions and historical citations. the last difference between esip and datacite is the inclusion of ﬁsubset usedﬂ this is the concept of micro citation, which may be the most challenging aspect of data citation. in conventional literary citation this might take the form of citing a passage in a book and referencing a page number. we all know how to deal with page numbers in a book. but, how do we do it in a data set? maybe we can put an identifier to it. if we have a particular query, we could capture the query and maintain sort of a query id. those kinds of technical approaches are probably the way forward but that is not the way the vast majority of group science data is managed today. so instead, we consider the concept of a structural index. this is similar to citing ﬁchapter and verseﬂ in a sacred text. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.how to cite an earth science dataset 123 the key question then is what structure or structures can we use to organize data collections that might be common across earth sciences? the basic assumption of a ﬁchapterverseﬂ style of reference is that there is a canonical version of data set. this is also assumed in the approach using the unique numerical fingerprint. unfortunately, most earth science data lack a canonical version. for example, data could be in different digital formats, where the contents are scientifically equivalent, but they are not identical because of the different formats. therefore, we need to refer to ﬁequivalence classesﬂ not canonical versions, although we cannot deny the human readability of the chapterverse style approach. we probably need both approaches. we need the ﬁchapter and verseﬂ that makes sense to people and is easily conceived and communicated between people, but then we still need the precise location and identity of that rather mutable verse represented in a way that computers can readily understand and be precise about, i.e., the identifier. and then we cannot forget the fact that we have billions if not trillions of "verses" or "granules" that we are dealing with. our human approach needs to make sense at a high level of aggregation, while the computer approach needs to handle the volumes and precision. in earth science data, space and time can often serve as a structural index. we can simply refer to a spatial and temporal subset of the data. we might also consider what the open archives information system reference model8 calls archive information units: an archival information package whose content information is not further broken down into other content information components, each of which has its own complete preservation description information. neither of these approaches is fully satisfactory, but following are some examples of doing it as best we can: hall, dorothy k., george a. riggs, and vincent v. salomonson. 2007, updated daily. modis/aqua snow cover daily l3 global 500m grid v005.3, oct. 2007 sep. 2008, 84°n, 75°w; 44°n, 10°w. boulder, colorado usa: national snow and ice data center. data set accessed 20081101 at doi:10.1234/xxx. hall, dorothy k., george a. riggs, and vincent v. salomonson. 2007, updated daily. modis/aqua snow cover daily l3 global 500m grid v005.3, oct. 2007 sep. 2008, tiles (15,2;16,0;16,1;16,2;17,0;17,1). boulder, colorado usa: national snow and ice data center. data set accessed 20081101 at doi:10.1234/xxx. cline, d., r. armstrong, r. davis, k. elder, and g. liston. 2002, updated 2003. clpxground: isa snow depth transects and related measurements, version 2.0, shapefiles. edited by m. parsons and m. j. brodzik. boulder, co: national snow and ice data center. data set accessed 20080514 at doi:10.1234/xxx. we have not solved all the issues related to data citation and attributions, but we believe that approximately 80 percent of citation scenarios for 80 percent of earth system science data can be addressed with basic citations, i.e., [(author(s). releaseyear. title, version. [editor (s)].   8 ccsds (consultative committee for space data systems). 2002. reference model for an open archival information system (oais) ccsds 650.0b1 issue 1. washington, dc: ccsds secretariat.  for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.124 developing data attribution and citation practices and standards archive. locator. [date/time accessed]. [subset used]], and reasonable due diligence. we need to move forward with this now and not wait for the perfect solution. finally, as we go forward, i think that the concept of scientific equivalent is ripe for study and that we are beginning to look at the notion of how content equivalence and provenance equivalence can serve as a proxy for scientific equivalence. that is a big research question, but it should not stop us for moving forward on the citation issue in general. i want to emphasize that we can do something about data citation now and we should. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.125 19 citable publications of scientific data john helly1 university of california at san diego this presentation focuses on what we have learned from a history of developments in scientific data publication that began in 1993 and continue today. the first data publication work at the san diego supercomputer center started in 1993 related to natural resource management in san diego bay and evolved to an activity with the ecologic society of america (esa) in order to solve some problems related to the preservation of longterm ecological data. these data were at risk of being lost, but in 1998 we set up a website that was designed for publishing data papers by the esa. this effort then led to a letter in nature2, which suggested that the scientific community should raise data collections to the status of citable entities in journals. this was followed by an acm publication in 20023 and several other publications related to scientific data publication in the earth sciences and scalable models of data sharing. this meant that we were able to distill some basic principles and requirements for systems. these are the design principles that we employ in systems now in operation as well as new systems across disciplines and domains. the three earliest digital library systems in continuous operation since their inception are: 1 the scripps institution of oceanography (sio) sioexplorer, since 2001. 2 the site survey databank (ssdb) for integrated ocean drilling project (iodp), since 2003. 3 the national science foundation center for multiscale modeling of atmospheric processes (cmmap) digital library project in the atmospheric science, since 2005. these systems are designed to deal with data up to the multipetabyte range for data storage and transportation requirements. from these developments we have learned how to change the workflow for scholarly publication to achieve the goal of citable scientific data. the basic workflow for scientific scholarly research starts with collecting data, doing the research, writing and publishing a manuscript for which some of the people get credit for it through citations. within the past few years, it has become possible for individuals to obtain the authority to issue digital object identifiers (dois). previously this was an authority available only to commercial publishers. this new capability allowed us to introduce the use of dois for data to this workflow and make citable data publication a reality.  1 presentation slides are available at http://sites.nationalacademies.org/pga/brdi/pga064019. 2 j. helly. new concepts of publication. nature, 393, 1998. 3 j. helly, t. t. elvins, d. sutton, d. martinez, s. miller, s. pickett, and a. m. ellison. controlled publication of digital scientific data. cacm (accepted october 3. 2000), may, 2002. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.126 developing data attribution and citation practices and standards figure 191 basic scholarly workflow paralleling the new corresponding workflow for data publication. sccoos and ucmexus are acronyms pertaining to specific projects. we kept the same basic workflow with a path for data paralleling the manuscript path. the key here is to develop the training necessary to teach these steps to graduate students and expert scientists to ensure progress in this area for a number of reasons. only scientific experts can ensure data quality and provide sufficient metadata to enable this process. the federal agency archival requirements for data developed under federal grants are clearer than previously, but we need some incentives as well. there are also financial issues to deal with: how are longterm archives to be supported? noninteroperability of dois from different systems is also a looming problem. recent information has come to light that the main doi providers for data are not interoperating. this is a problem because the whole concept of changing the workflow hinges on the ability to resolve the doi issues across the different domains and publishing systems. the scientific community may need another solution that fully realizes the value of dois and warrants the effort to use them. it looks like many of the old players in the publication industry are moving to ﬁwalloffﬂ what they perceive as their intellectual property by sequestering their doi crossreferencing. let me now talk about the california coastal atlas. it is designed for data publication, with a focus on developing methods and training people to do highquality scientific data production, for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.citable publicatons of scientific data  127 primarily in the geospatial data area. the model is scalable by design. we know that science proceeds through research projects and that these projects have finite life times. the key people are the principal investigators, the research managers sponsoring the projects, and the other people who are doing the work. so, through cooperation between the chief editor of the atlas and the different projects teams, the projects agree to do their data management according to the atlas conventions and standards. by modifying that workflow slightly, though not dramatically, we were able to provide a platform for those scientific projects to have highquality data end products. the current projects are:  ucmexu: declining oxygenation and ph of the eastern pacific margin. us navy: a methodology for assessing the impact of sea level rise on military installations in the southwestern united states.  california environmental data exchange network: the 303dlisting dataset. the california spatial data infrastructure. we believe that the real keys to success in this process are a set of factors that can be summarized as follows:  changing scientific workflows in familiar, but powerful ways to attribute high quality data to the authors.  incentivize researchers to modify their existing workflows only slightly and provide tools to do it. integration into a wellestablished and trusted system of scholarly publication.  providing the basis for protecting intellectual property rights. the figure below provides the visual representation of our approach to automate the production of metadata. the intermediate products that are generated automatically include a bibliographic reference file, a metadata interchange file that talks only to oai pmh, and then the basic underlying metadata or the data content in the form of what we call an arbitrary digital object. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.128 developing data attribution and citation practices and standards figure 192 metadata production process emphasizing the modular nature of metadata organization to support the minimal needs for cataloging as well as the disciplinary needs for reuse of the data. we use conventional tools (latex/bibtex) that have seen a resurgence in the past five years to produce the content of the atlas and to ingest the bibliographic reference information using tools like bibtex, so that the data underlying an image, for example, could be directly cited within the context of the document in the california coastal atlas. the editorial policy is probably the most confusing part, especially in terms of how it is actually done. the following figure attempts to depict it. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.citable publicatons of scientific data  129 figure 193 the editorial workflow organized into levels with requirements to transition from one level to another. level 0 is raw data. level1 is data that has been quality controlled and provisioned with metadata. level 2 data is data that has been through peerreview and level 3 data has been used by others and may be combined with other data.  we define levels of data in the form of state machine transitions, since there are requirements for going from level zero to level one and then to level two. there is always a question of managing derived data, how to combine and track it and that is where dois play a powerful role. there is an ongoing question of user feedback when data anomalies are found in subsequent use and the project that produced the data has ended. how do anomalous reports get factored back into the maintenance of the data collection? i will conclude with this set of editorial requirements for data publication, which are essentially the instructions to authors. with editorial guidance, data authors should provide: derived data products in ccaconforming data format and packaging;  ccaconforming metadata (fullyprovenanced); procedural software for reading the data object; for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.130 developing data attribution and citation practices and standards corresponding output listing for verification of data contents; metadata for obtaining a digital object identifier;  manifest with summary description (e.g., readme) describing what is contained in the arbitrary digital object; and  licensing statement. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.131 20 the sagecite project monica duke1 the university of bath i am the project manager at "sagecite," a project funded by jisc (the expert organization on information and digital information for education and research in the united kingdom) through the managing research data program in the united kingdom. the project focuses on disease network modeling within sage bionetworks. sagecite was funded between august 2010 and july 2011 to develop and test a citation framework linking data, methods, and publications. the domain of bioinformatics provided a case study, and the project builds on existing infrastructure and tools: myexperiment and the sage commons. sage commons is an initiative of sage bionetworks to build a platform to share data in bioinformatics. citations of complex network models of disease and associated data will be embedded in leading publications, exploring issues concerning the citation of data including the compound nature of datasets, description standards, and identifiers. the project has international links with the concept web alliance and bio2rdf. the partners are ukoln, the university of manchester and the british library (representing datacite), with contributions from nature genetics and plos.2 the project was structured through a number of work packages comprising:  review and evaluation of options and approaches for data citation. understanding the requirements for citing largescale network models of disease and compound research objects.  demonstration of a citationenabled workflow using a linked data approach. http://blogs.ukoln.ac.uk/sagecite/demo/ benefits mapping using the krds2 (keeping research data safe) taxonomy. http://www.beagrie.com/sagecitekrdsbenefitsworksheet.pdf  technical and policy implications of citation by leading publishers. http://blogs.ukoln.ac.uk/sagecite/publisherinterviews/ dissemination across communities (bioinformatics and research and information communities).  sage bionetworks is a nonprofit organization located in seattle, wa that is creating resources for communitybased, dataintensive biological discovery. their work is motivated by the belief that it is necessary to have communitybased analysis to build accurate models. they are also driven by the fact that no one single body has all the data required to build accurate models, so different stakeholders come together and contribute. sage bionetworks provides the data infrastructure, the culture, and the norms to make this happen.   1 presentation slides are available at http://sites.nationalacademies.org/pga/brdi/pga064019. 2 see project description at: http://blogs.ukoln.ac.uk/sagecite/. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.132 developing data attribution and citation practices and standards figure 201 a digital commons for communitybased analysis. there are different types of data that the platform hosts and they come from different sources. for example, data can be obtained from pharmaceutical companies, disease consortia, investigators, patient advocacy organizations, and from government sponsored studies. there are seven stages in the data processing pipeline. the pipeline requires as input a combination of phenotypic, genetic, and expression data that need to be processed to determine a list of genes associated with diseases. the following figure shows an (idealized) description of these steps, each of which is likely to be performed by a different scientist who specializes in that area. one scientist acts as the project lead.  figure 202 stages in lifecycle.  stage 1: data curationšthis consists of basic data validation to ensure integrity and completeness of the data (although some files use common formats, others have considerable variety.) the datasets include microarray data and clinical data. this step ensures that the format of the data is understood and the required metadata is present. stage 2: statistical qcš actual values in data are validated for quality to check for experimental artifacts. the checks made are dependent on the type of data set and involves the for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.the sagecite project  133 use of r scripts (for statistical computing) or specialized gene analysis tools (like ﬁplink.ﬂ). the output is a normalized dataset. stage 3: genomic analysisš this involves identifying regions in the genome associated with clinical phenotypes and other molecular traits. the sage genetic analysis pipeline, which consists of a set of r and c programs, is used. statistical analysis is applied to identify interesting loci significantly associated with specific phenotypes (e.g., clinical phenotypes such as cqtl). stage 4: network constructionš this stage focuses on building a network using a statistical technique to capture how biological entities, such as genes, are related to each other. networks can contain up to 100 thousand nodes. in the network, nodes represent biological entities of some type (a gene, a protein, or even a physiological trait) and edges represent relationships between pairs of nodes. the output could be a correlation network (undirected graph) or a bayesian network (directed, acyclic graph). stage 5: network analysisš this involves examining the network to determine how its function can be modulated by a specific subset of biological nodes. the output may be a list of genes or a subnetwork. the networks from the previous steps are analyzed using techniques like key driver analysis to determine a subset of interest. stage 6: data miningš a report validating claims from network analysis is produced by a domain specialist with knowledge of the study domain. this stage uses resources from the literature and public databases to assess the predictions. the information is used to annotate network models to build the case for the involvement of genes in the functioning of the network. stage 7: experiment validationš in the final stage, laboratory experiments are devised and performed to test the claims of the model. validation is not carried out at sage bionetworks, but is completed in partnership with sage bionetworks collaborators. such a complex process presents challenges for reproducibility and citation. data curation is required as a first step to do basic data validation to ensure integrity and completeness, and to ensure that the format of the data is understood and the required metadata is present. agreed standards are also required for data sharing. we have to make sure that the data from different sources can be described, shared, used, and make the discovery process easier. the project has employed the workflow tool, taverna3, which helps to document the data processes and enables the workflow to be reenacted. the workflow can also be registered with a digital object identifier (doi). capturing the workflow and assigning an identifier supports better citation because the cited resource is more reusable, and strengthens the reproducibility and validation of the research. finally, we can describe the challenges for using data citation with the purpose of giving attribution and supporting reproducibility within this specific context. the challenges for attribution include:  3 http://www.vimeo.com/27287109. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.134 developing data attribution and citation practices and standards ł preserving a link with the original data. œ the data that enters the processing pipeline originates from several sources with different methods of identification of that data (or none). some disciplinebased repositories have their own identifiers that are culturally the norm within the discipline, but may not be wellknown within other communities, or may not fit in technically with global identifier infrastructures. œ creating bidirectional links. it is not sufficient to keep links which go in one direction only from processed data to the originating data. the originators of the data (e.g., discipline repositories) would also like to track usage. therefore links need to be maintained in both directions. however, systems of notification of usage and tracking have not yet been developed. ł attributing data creators. œ identifying the party that created or contributed the data may not be straightforward and may have confidentiality issues (e.g., where medical data identifies specific populations). the situation is made more complex through developments of sites like patientslikeme, where individuals are choosing to contribute their data. the range of entities and individuals who expect to be credited can be expected to grow and identifying new categories of data contributors (such as the individual patient) will create new challenges. ł defining creation of new intellectual objects, e.g., a curated dataset. with a complex process the community needs to agree what represents a new intellectual object that should be attributed. a curated dataset represents a significant input from the curator to make the object usable, but is the curated dataset a new distinct object that should be attributed and identified separately to the original data? ł cultural challenge in recognizing nonstandard contributions; microattribution.  traditionally there has been emphasis on publications as a measure of contribution for the purposes of career advancement and peer recognition. a culture change is required if other categories of contribution (such as curation effort and data sharing) are to be recognised. unless these contributions are recognised there will be little motivation to put in the effort to attribute them and create citation mechanisms around them. microattribution is a developing idea in data citation to recognise smaller contributions and was used in the description of genetic variation in a paper in nature genetics in march 2011.4 ł new metrics.   4 http://www.nature.com/ng/journal/v43/n4/full/ng.785.html. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.the sagecite project  135 as new types of contribution are recognised complementary metrics and mechanisms for measurement will be required. communities will need to decide what should be measured and services will need to be devised to track data citation and measurements.  ł identification of contributors. with multistep processes where individuals with different roles contribute, methods will be needed to describe the role of individuals and their contributions, particularly if nontraditional contributions such as data curation, data processing, data analysis, software, or process development are to be attributed. reproducibility: ł identification and granularity. œ discipline identifiers, global identifiers. sagecite has taken a workflow capture approach to preserving the steps of the process to make it reproducible. when assigning identifiers for citation purposes decisions are required to decide at which level of granularity unique identifiers should be issued. is it sufficient to identify the work flow or do individual steps need to be assigned their own identifiers? when should discipline identifiers be incorporated and how are these associated with identifiers assigned from a global system?  œ how much value has been added since the data entered the workflow? œ one argument for deciding when a new identifier is required is to assess the value added to the data since it has been in the pipeline. to ensure reproducibility and provenance tracking, links need to be kept between valueadded versions which have acquired a new identity in the pipeline and the original data.  ł identifying processes and software.  for the purposes of reproducibility it is not only the data that needs to be identified and cited. the tools and the workflow applied need to be referred to and accessed. the exact details are not always recorded and although some generic tools (such as r) are sometimes cited, the specific scripts used must be curated in order to become citable objects. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.137 discussion by workshop participants moderated by david kochalko participant: this session reminded me that the history of documentation is full of powerful systems that died because they were too much work to operate. mr. kochalko: i think it is difficult to draw meaningful distinctions between locators and identifiers. i also think it is important to understand that when some providers change a version number they will also change the identifier, which maintains parity so that each version of a dataset can both be located and uniquely cited. when providers refuse to issue new identifiers, they make it difficult to associate the version of work with its location and a unique citable identifier. i think that these factors have to be accommodated and that it is still possible to maintain version histories, which is a really fundamental. mr. parsons: quite honestly, most earth science data are not well versioned currently. what we have found is that an accurate citation is highly coupled with provenance and we, as a community, are just now beginning to fully address provenance. my data center recently got some money to develop a socalled climate data record, which is meant to be the gold standard of a long time series, in this case, brightness temperatures measured from passive microwave sensing satellites. what we discovered is that the dataset that can be perfectly reproduced was actually not the best dataset because scientists had made decisions over its 30year history that they were not necessarily documented in a way that could be reproduced by a machine. my point is that the provenance is really key and it is a developing field. participant: the major versions approach is good, but the other approach, which i believe the british oceanographic data center (bodc) is using, is periodic snapshots. either way, it is not an identifier. dr. callaghan: at the bodc and by extension the rest of the uk national environmental research centre data centers, when we post a doi on a dataset, we are saying that it is frozen in time and will not change as far as we can possibly manage it. if a dataset is still being updated, it will not have a doi. it will still be accessible and citable, using uris and urls, but it will not have a doi association. participant: when you say a ﬁdatasetﬂ, do you mean, for example, the time series of the history of the earth's temperature? dr. callaghan: for those situations where we do have an ongoing time series, we divide it into decades or years or even months, if appropriate. that kind of dataset, however, is picked because once you have recorded the data, they are not going to change. one will not go back after the fact to change what is in that particular time period, unless there is a major problem. in that case, you have to redo the dataset or revise the calibration and then you issue a new doi with a new version. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.138 developing data attribution and citation practices and standards participant: the data processed by datacite there are freely accessible by anybody. what happens if the commercial data curator decides to get out of the business? what happens to the data? dr. brase: first of all, we believe that the access to the data should be free of charge, but there is no strict rule that they always should be free of charge. we therefore work together with data centers that need to get some compensation for the data and we encourage them to make access as free as possible. the data centers do not seek to make profit, but there are some that do provide access to the data for a fee, or the data are only available to the members of some institution. now to your question about what happens to the data? that is always an issue and that is a situation for which we still have not found a perfect solution. one of the good things about assigning an identifier to a dataset is that you always can ensure that when somebody references the doi name of a dataset that is no longer available, they will not get a 404error, but they will receive a page describing that this dataset is no longer available and where the last known version can be accessed. this is always a possibility, but the idea of datacite is that, ideally, if we would have a situation where one data center would cease to exist, we would try to find other data centers to take over the data and ensure that doi name refers to the current version of the data. if that does not work, then we would direct the doi to a page describing why the data set is no longer available. participant: i wonder if anyone would like to reflect on the citation systems that have two parties who can be credited: the publishers and data providers. i just noticed that the people we have at this workshop are mostly from the provider side. are we designing a system that is driven by the data centers and their interests, but not necessarily by the data providers? dr. callaghan: basically, data providers are interested in getting data citation working. we know this because we have asked them, at least in the meteorological sciences. we also have had a few cases of data providers coming to us and asking, ﬁcan we get the doi for our dataset?ﬂ or ﬁwhen will the data be citable?ﬂ so, there is interest in the scientific community. as data center managers, our job is to get data from the data providers, but if they do not show any interest in data citation then it is not in our interest to do anything about it. mr. parsons: i will briefly add that if we have the identifier, we do not really need to include the role of the distributor or publisher in the citation. i would like people to think that they are getting a higher quality product out of the national snow and ice data center, but then i think we also have to be careful about citation being the credit mechanism. for example, i am getting pushback now from the funding agencies that want to have nasa or noaa in the citation. we never did that with literary citations. why do we have to do it now? dr. chavan: i think while it is clear that the basic motivation for data citation is mainly for the authors of the data, publishers of the data could get their work properly recognized as well. if you look at it from the usability perspective, there is equal responsibility on the part of the users of the data by making sure that they cite the data as adequately as possible. this brings in some complexity (e.g., when data are contributed by multiple parties) when the user actually uses records from each of the datasets or subsets of the data. this is exactly why we have been for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.discussion by workshop participants  139 promoting the practice of having userdriven citations on top of the publisher™s data citation. i think that it was technically vital previously to have authordriven citations or publisherdriven citations, but i think as the digital area progresses we will need to promote both. the userdriven citations will be the key to authenticate or verify the validity of the interpretations on the dataset that they have actually used. participant: is there a possibility that microsoft can tell me my true worth? if we are already indexing 24 million documents and doing many other things that are not measured in terms of scholarship, we might be able to begin to get at that through the microsoft bing index. i could actually come up with a number that was measured across all of my scholarship. this would seem to be something that could change the way people think about how scholarship is measured. it seems we need that kind of metric and maybe it is within reach. participant: this is the kind of metric that we would like to make available. we are not affiliated with the bing index, but we have the opportunity to work with that team, combine indexes, and run those kinds of searches and present data in a meaningful way. we are not doing that right now. if the community comes together and indicates that they would be very interested, that would be a good step forward. maybe there could be a largescale aggregator service of data citations?  for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.141 part five  institutional perspectives for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.143 21 developing data attribution and citation practices and standards: an academic institution perspective deborah l. crawford1 drexel university i have a somewhat unique perspective on this subject. until september 2010, i worked at the national science foundation (nsf), where i was involved in the fashioning of nsf's data management plan policy. shortly afterwards, i returned to academia, joining drexel university. i have the pleasure now of implementing the policies that i had a hand in preparing. it is an important topic with a lot of complexity.  today, i will try to share with you my view from a university administrator™s perspective œ but i will also touch on the respective roles and responsibilities of academic researchers as individuals and as members of research communities. i was asked to respond to the following question: how are university administrators thinking about data citation and related issues? what follows are some of my thoughts on this subject. in my role as a vice provost for research at drexel, i view the stewardship of research data as one of a number of responsibilities i have to create an environment that supports the responsible and ethical conduct of research in the public interest. developing such an environment has implications for the management of the increasingly digital research data that we collect or create.  let me first talk about the role of researchers, and the research communities to which they belong, in data stewardship. as is already quite well known, there are significant differences in practices among scientific communities, including the communities represented here at this workshop.  for example, some of our communities have, for a decade or more now, leveraged the economies afforded by data sharing, attribution, and citation. these tend to be the scientific and engineering communities, where data have been and continue to be created or collected with the intent to be shared broadly. these include, for example, environmental and astronomical sciences, and geosciences communitiesštypically those communities where data are collected on nationally or internationallysupported and communitygoverned instruments or facilities. and now, thanks to the ﬁomicsﬂ revolution, a number of the life sciences communities too are generating data with intent to be shared.  in other fields, cultures continue to be much more individual investigator oriented. in such domains, the independence of individual investigators is fiercely guarded and research data are rarely shared, except in relatively modest ways through peer review publications. i think it is useful to keep these cultural and research differences in mind as we think about how we move forwardœone size is unlikely to fit all.  1 presentation slides are available at http://sites.nationalacademies.org/pga/brdi/pga064019. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.144  developing data attribution and citation practices and standards  we need to develop explicit policies on data sharing, attribution, and citationœboth domainbased policies for the scientific and engineering communities, and institutional policies that complement and support community policies. it is important that we develop these policies and supporting practices in a collaborative way, bringing all stakeholder groups along so that we can fully leverage the added value of the enormous and growing quantities of digital data to advances in science and technology.  let us now turn our attention to the role of academic institutions. just as some communities have well developed data policies and practices and others do not, so some institutions have data sharing policies and others do notšat least, not yet. in tenure and promotion policies and practices that pertain to data sharing, citation and attribution, culture matters very much. for investigators in communities more accustomed to data sharing, data attribution and citation is likely to be valued in tenure and promotion decisions. this, however, is not true across the board. so in fashioning academic policies that promote data sharing, citation and attribution, we must be mindful of, and manage for, these differences.  institutions should help faculty understand what is expected of them in the responsible stewardship of research data in our increasingly digital scientific world. deans and department heads are major institutional stakeholders too, for they must provide leadership in raising awareness about this important topic and its implications in matters such as tenure and promotion (and others), and they must serve as advocates for change, where necessary.  i believe midcareer faculty play a very important role. we cannot expect our junior faculty, who are often pioneers or early adopters of new digital research modalities, to carry the weight of promoting the development of new data policies, for they have too many other pressures coming to bear on them, and in fact might be penalized for having pioneering views. midcareer faculty members are likely to be key to moving a conversation forward on these topics. they are the ones who typically are more engaged in research where progress demands an increasing reliance on the sharing and attribution of digital data, and these faculty members may be more willing and able to speak to and be heard about the importance of these issues.  let me now briefly address the issue of institutional repositories. many of us believed that institutional repositories, interoperable ones of course, would be a key to the future; they would enable universities to actively manage their digital assets, manage their intellectual property with appropriate controls, and explore new forms of scholarly communications. the role of institutional repositories is especially important in the later stages of the data lifecycle, as researchers focus on new and interesting scientific opportunities and worry less about the research data of their past interests. thus, institutional repositories were expected to play important roles in data curation and preservation. in practice, however, institutional repositories are not living up to our expectations, partly because researchers are not routinely depositing their digital objects in the repositories that their institutions are providing. many researchers do not see the value to their science and to their for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.an academic institution perspective  145 careers of doing so, which ultimately is the bottom line for most researchers. this is something we need to keep in mind as we think about the ways in which institutions encourage faculty to engage in conversations about the future of data sharing, citation, and attribution. it is important to note that it is not at all clear that academic institutions across the board are in a position to move boldly into this new world. for one thing, as we heard yesterday, universities have not been significantly engaged in the active longterm management of research data to date. traditionally, the majority of investigators have managed or have been responsible stewards of their own data, where community governance of data was essential to advances in certain fields. it is fair to say that there is much more evidence of communitybased initiatives, albeit in some fields more than others, than there are universitylevel initiatives.  equally important, or maybe more important today, the substantial cost implications of providing longterm stewardship of data is a very significant concern for research universities. in an increasingly difficult economic environment, where concerns already exist about the escalating costs of higher education and where the federal government is unable or unwilling to support the full cost of research in the academy, who bears the responsibility for paying to ensure longterm open, useful access to research data created in the public interest? this is a policy issue that the governmentuniversity partnership needs to resolve.  the discussions that we have been having in this workshop and in recent years raise important questions about who is or should be the champion for these issues at the university, and who should do what? unfortunately, there are no clear answers at most universities because of the complexities. faculty, researchers, and students have a voice and a role, but for the most part, they are not substantially involved in conversations because, for the most part, the value to their science and to their careers is not readily apparent to them. deans, colleges, and schools have voices as well, but for the most part, they are not serving as change agents. this is true, too, in university research offices, in part because they tend to reflect the cultures of the research communities they represent, and undoubtedly, because of concerns about costs. libraries have served as the strongest advocates for these kinds of changes, but they probably do not have the institutional power or authority to really effect change. so, if we are going to make a difference, there has to be a clear change mandate in institutions involving all institutional stakeholders. this is a critical topic of conversation for academic institutions, because it impinges upon their reputations as essential contributors to the national knowledge enterprise. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.147 22 data citation and data attribution: a view from the data center perspective bruce e. wilson1 oak ridge national laboratory and the university of tennessee one of the several things i am doing right now is pushing the question of how to enable scientific collaboration and, in particular, how to put in place the technology to do that. i am also working with my colleges at the university of tennessee to look at these issues from a sociological perspective. for three years, i was the manager of the oak ridge national laboratory (ornl) distributed active archive center (daac) for biogeochemical dynamics. it is one of nasa™s earth observing system data and information system (eosdis) data centers managed by the earth science data and information system (esdis) project. the ornl daac archives data produced by nasa's terrestrial ecology program, as well as data of particular interest to scientists funded by that program. the ornl daac provides data and information relevant to biogeochemical dynamics, ecological data, and environmental processes that are critical for understanding the dynamics relating to the biological, geological, and chemical components of earth's environment. i also spent eighteen years in private industry. i mention that because this experience influences in some ways my perspective on data citation and attribution issues. in addition, i have had some involvement in projects with the national biological information infrastructure, at the usgs, and i continue to work on the citizen science side of data submission and data citation for the u.s.a. national phenology network (usanpn). at the ornl daac, we make sure that the data generators get credit for what they have done. an incentive for data attribution is to ensure that the data center gets credit for hosting the data as well. we also want to understand what data are or are not being used. for example, we have good statistics about how many times datasets were downloaded. these data show that, on average, it takes about 18 to 24 months from when a dataset gets downloaded to when it gets cited, if it gets cited or referenced at all. we found that some datasets are downloaded more, but may be cited less. does that indicate that the data are hard to use, that there are barriers to being used, or that there is something wrong with the data? there is typically no single answer to that question, and it is something to investigate on a datasetbydataset basis. this kind of use metrics (not only of downloading, but of actually using a dataset) can be extremely valuable for the data center to understand its business model and operational environment. for example, it helps us understand how the data are being used outside of the scholarly context. tracking the use of an individual dataset is important, but what is the value for a data center if these data were used in a university study, for example? what is the value to the data center and   1 presentation slides are available at http://sites.nationalacademies.org/pga/brdi/pga064019. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.148  developing data attribution and citaiton pratices and standards  to the sponsor if we did something that made the data easier to use in undergraduate education classes? so, it is arguably valuable to the science and scientific community, but how do we measure that? how to we understand those kinds of uses? here is an example of dataset citation: gu j. j., e. a. smith, and h. j. cooper. 2006. lbaeco cd07 goes8 l3 gridded surface radiation and rain rate for amazonia: 1999. data set. available online [http://www.daac.ornl.gov] from oak ridge national laboratory distributed active archive center, oak ridge, tennessee, u.s.a. doi:10.3334/ornldaac/831. we started adding the doi to this citation style about five years ago. the major reason for adding the doi was that we had a citation without the doi and some journals rejected it because it was not a valid longterm citation. we put the doi into it because the doi is an established standard and one that the publishing community, in particular, uses. using the standard that makes sense to publishers helped to reduce the barriers to adoption of data citations. and a second key point is that the citation contains key information we need: the names of those who created the dataset, it tells where to find the data, and it has a persistent identifier let me now focus on the data center roles and responsibilities in this process. i think a key role here is the issue of stability. data centers need to provide stability through using persistent identifiers and ensuring technical, social, and organizational sustainability. data centers also have to encourage use and make it easy for users. we need to make it easy to download datasets like a bibliographic set. furthermore, we have to work on challenges such as what does the identifier points to, how to handle continuous data, subsetting, ondemand data, as well as issues related to scalability of the data management process. we have to work on some fundamentally different paradigms and be willing to take some risks about changing our business model. let us now see how our datasets actually get used. the figure 221 shows the growth in cited versus referred databases. compiling the information for this table was an effort of multiple weeks by the ornl library and the ornl daac staff. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.a view from the data center perspective 149 figure 221 growth in cited vs. referred databases.  this diagram shows an increased number of datasets cited versus referred. by ﬁcitedﬂ we mean that the ornl daac dataset is in the bibliography or in the requested format; and by ﬁreferredﬂ we mean that we infer, from the text of the article, that one or more ornl daac datasets were used in the work. ornl daac requests that data be cited in the list of references. this time series provides evidence of some changes in scientists™ behavior towards data citation and attribution. the best way to facilitate and promote this change is through having more champions. academicians and scientists are only going to do something if somebody who is successful tells them that it has worked. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.151 23 roles for libraries in data citation michael witt1 purdue university as a practicing librarian, i will be focusing on the roles for librarians and information professionals in data citation and attribution. i would like to start by answering the question, why are librarians involved in data, and why are they interested in data citation? if we go back to the workshop on ﬁnew collaborative relationships: the role of academic libraries in the digital data universeﬂ that was sponsored by the association for research libraries (arl) and the national science foundation (nsf) in september 2006, an important need was identified ﬁ–for new partnerships and collaborations among domain scientists, librarians, and data scientists to better manage digital data collections; necessary infrastructure development to support digital data; and the need for sustainable economic models to support longterm stewardship of scientific and engineering digital data for the nation™s cyberinfrastructure.ﬂ2 to follow up, in august 2010, the arl did a survey of its member institutions (approximately 130) and 57 of them responded. some of the findings include: (1) 21 of them currently provide infrastructure and services for escience and data support, and (2) 23 members are in the planning stages.3 this shows that libraries are involved in this area of data curation, at least in the context of academic and research libraries. that is not to say that any of these issues are exclusive to those libraries. in fact, i think that a lot of these needs extend to public libraries and citizen science, and other libraries outside of the university context. i propose that data citation has ﬁa last mile problem.ﬂ in communication networks it is usually easier to connect countries and cities than it is to connect to individual endnodes, such as houses, especially in rural areas. in the data citation arena, the challenge is: how do we reach and affect a change in practice among endusers of data? how can we reach people who will be writing papers and citing the data? those users could be students, faculty researchers, citizens, or government agencies, etc. i believe that a role that librarians can play here is rooted in libraries™ tradition of information literacy outreach and instruction. information literacy is a set of abilities requiring individuals to recognize when information is needed and have the ability to locate, evaluate, and use effectively the needed information.4 this includes the proper citation and attribution of sources.  1 presentation slides are available at http://sites.nationalacademies.org/pga/brdi/pga064019. 2 available at: http://www.arl.org/pp/access/nsfworkshop.shtml. 3 c. soehner, c. steeves, and j. ward, escience and data support services: a study of arl member institutions association of research libraries, 2010. http://www.arl.org/bm~doc/esciencereport2010.pdf. 4 american library association. 1989, presidential committee on information literacy. final report. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.152  developing data attribution and citation practices and standards  if you look at the information literacy competency standards for higher education5 from the association of college and research libraries (arcl), you can replace the word ﬁinformationﬂ with ﬁdataﬂ and the competencies make sense and remain relevant. where can users look for information on how to cite data? one natural place to turn would be style guides. i did a study with two colleagues, where we looked at 20 different style guides and performed content analysis to see what kind of instructions they are providing users explicitly to cite digital data. the answer is: they do not consistently address data citation and attribution.  figure 241 a description of data citation instructions in style guides. source: international digital curation conference, chicago, il. retrieved from http://docs.lib.purdue.edu/libresearch/121/. newton, mooney, & witt. (2010).  if you look at the above grid, it covers instructions for digital data, data in other formats (e.g., paperbased tables), and other electronic resources. the dark purple indicates the areas where the style guide provides explicit instructions for citation. the light colors (i.e., aqua or white) indicate that there are no explicit instructions. so, generally speaking, some style guides do a better job than othersšbut if this is where students and others are turning for instructions to properly cite data, they will undoubtedly be frustrated.   5 available at: http://www.ala.org/ala/mgrps/divs/acrl/standards/informationliteracycompetency.cfm. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.roles for libraries in data citation 153 one thing that we see happening on our university campuses is that librarians are stepping in to address this need by creating resource guides. this is a common practice of librarians to develop bibliographies and pathfinders to introduce topics and tools to users. here are some examples of resource guides on data citation that are appearing at universities from their libraries:  mit: http://libraries.mit.edu/guides/subjects/data/access/citing.html  msu: http://libguides.lib.msu.edu/citedata  minnesota: http://www.lib.umn.edu/datamanagement/cite  purdue: http://guides.lib.purdue.edu/datacitation  oregon: http://libweb.uoregon.edu/datamanagement/citingdata.html  cambridge: http://www.lib.cam.ac.uk/dataman/pages/citations.html  virginia: http://www2.lib.virginia.edu/brown/data/citing.html  these guides are written by librarians in most cases and tailored for their particular audience. they may be tailored for undergraduate or graduate students, faculty researchers, or others. one project that i would like to briefly talk about is databib.6 this project was funded through the institute for museum and library services (imls). here is the description of the project: the libraries of purdue university and penn state university will partner to create a new online information resource for research data producers, users, publishers, librarians, and funding agencies. this resource, databib, will be an annotated online bibliography of research data repositories, created and maintained by an online community of librarians. databib will be an important focal point for connecting librarians more closely with other research data stakeholders and demonstrating the significant contributions libraries can make to solving the challenges posed by digital datasets. the databib platform will also serve as a testbed for linking, integrating, and presenting information about datasets in new ways.7 databib is essentially a bibliography that describes data repositories. what we are doing is creating a platform for librarians to submit and enhance bibliographic entries that describe these data repositories and do it in a way that is maximally open, using the creative commons zero public domain protocol. if someone wants the list or the metadata, they are free to download and use them. also, if someone wants to enhance the metadata or annotate them, that is also possible. we are creating this resource for the community to help users find data as well as to help data producers identify repositories where they can submit their data, to share this information with funding agencies that mandate data management and tell them where data have been submitted, because these directions are unclear in many cases. we also want to test the notion of a bibliography. we will have bibliographic records that can be exported as marc records, so if someone wants to download them into their library catalogue, they can. also, if someone wants to integrate them with other web 2.0 tools, such as social tagging and social bookmarking, databib will facilitate sharing links and citations. finally, we want to use this platform to experiment with linked data. we want to create a platform where the descriptions of these data   6 databib website, http://databib.lib.purdue.edu. 7 imls press release, http://www.imls.gov/grantawardsannouncementsparksignitiongrants.aspx. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.154  developing data attribution and citation practices and standards  repositories can be linked in as many ways as possible to other things, whether it is in the same subject area, same agency that supports the data repository, or any other level of linkage. this project is a ninemonth project, and databib will be going online in the spring of 2012. going back to the potential role of libraries and librarians, libraries are a primary actor in the scholarly communication chain. i believe that libraries can promote persistence for links to data. jan brase talked about datacite yesterday. there are many libraries that are participating in this effort. i think that libraries need to adopt uri policies. we are creating a lot of digital content and making it available in a lot of different ways with links that break. so, in addition to minting and maintaining unique, global, and persistent identifiers, we can have more general uri policies, which we can advocate for web content across our institutions. are libraries presenting our own data in ways that facilitate or encourage citation? libraries maintain institutional repositories and other digital libraries where they are presenting digital objects, but do we have supporting documentation and faqs that give users instructions for citation? do we provide embedded, structured metadata within the web page, such as coins, microformats, or rdf? do we facilitate exportable citations? many of our libraries have data services that are doing outreach to faculty members to help them understand data management plans. before projects are funded and data are generated, there is the opportunity to have a conversation about datasharing with the different stakeholders. there is an opportunity for advocacy. i would like to raise awareness of the work being done by the international association of social science information services and technology (iassist). i cochair a special interest group on data citation with mary vardigan. among the over 300 members that iassist has, about 40 or 50 of them are involved in this special interest group. some of the activities that we have been engaged in include an effort to derive a common set of user instructions for citing data. we realize that we would not necessarily be able to use a perfect set of instructions for all cases, but if we can come up with a core set of instructions, that would be very useful. also, there has been some work to integrate datasets as a resource type in citation management software such as endnote or refworks. moreover, we are doing some advocacy. we have been writing letters to style guides editors and publishers to encourage them to articulate policies and instructions for data citation to their authors. also, like many other special interest groups, we are generating resources such as a website and brochure that are publicly available for use. to conclude, librarians and information professionals can play important roles in advocacy and outreach, and in the integration and citation of data. this includes data citation in reference services and information literacy instruction and standards. librarians should ask themselves: if we are publishing data, are we making our data citable, and are we incorporating data into information literacy? one last observation: many libraries are creating new data services units that can help raise awareness of and address issues related to data attribution and citation for their communities. promoting proper data use and citation should be a part of what we normally do in libraries, a part of our regular practices. there seems to be a trend of libraries addressing research data in a specialized manner, e.g., ﬁdata referenceﬂ and ﬁdata information literacyﬂ. i suggest that, after a period of time, the library profession will become more comfortable with data and will not need for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.roles for libraries in data citation 155 to qualify ﬁdataﬂ services as such. the same principles of library science that apply to traditional formats can be applied to data. the timing seems to be perfect for people to connect and collaborate to address data citation and attribution issues. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.157 24 linking data to publications: towards the execution of papers anita de waard1 elsevier labs and the university of utrecht, the netherlands first, i would like to say that i am not representing all commercial publishers and that i have not even coordinated this talk with my colleagues at elsevier, so this is my personal perspective on the issues being discussed here. i think it is useful when we are talking about integrating data with publications to look at where data fit within the scientific process. the kefed model developed by gully burns2 can help in this regard. figure 241 kefed model ﬁcycle of scientific investigation.ﬂ essentially, in doing research we start thinking about the background and making some hypotheses. this is basically experimental science. you do an experimental design, you manipulate some external objects, and then you have observations. from those observations, you gather what is called data. then you do some statistical analysis, and come up with some findings. in general, the data support your claims and findings. what happens in a publication is  1 presentation slides are available at http://sites.nationalacademies.org/pga/brdi/pga064019. 2 gully apc burns and thomas a. russ. 2009. biomedical knowledge engineering tools based on experimental design: a case study based on neuroanatomical tracttracing experiments. in proceedings of the fifth international conference on knowledge capture (kcap '09). acm, new york, ny, usa, 173174. doi=10.1145/1597735.1597768 http://doi.acm.org/10.1145/1597735.1597768. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.158  developing data attribution and citation practices and standards that you make a representation of your thoughts through language. these are the bases with which i would like to start. currently, the scientific community is storing data in repositories. we link to publications and viceversa. the example that is commonly used is that people add pdfs and spreadsheets to their papers. this is pretty useless because we are not doing anything with these documents. having them does not mean we can find the dataset. in general, i believe that datasets should all be available for server search and that sets and subsets of that data should be made freely accessible, whenever possible. overall, commercial publishers are not interested in owning or charging for research data or running those repositories. there might be exceptions, but in general, this is the case. in my view, most publishers are very interested in working with data repositories and believe that it would be very useful if there were one place where we can find data items. it would be useful if an identifier is persistent and unique and that if the content changes, the identifier changes as well. also, it would be very useful if the data would link back to the publication. it would be more interesting if we have data in a repository and can link them to some content from within a publication. not only from the top level, but from within the publication. there are some examples of this. what my lab has been doing currently is tagging entities and linking them to databases. this involves some manual as well as some automated work. more interesting, i think, is the fact that we can now create claim evidence networks that span across documents, so we can have a statement that can be backed up in a table or a reference in another publication or in another data center. at least at elsevier, we are very invested in the idea of linked data. we have developed something that we call a satellite, which is essentially a way to describe a linked data annotation, in rdf. we are using dublin core and swan™s provenance and authoring/version ontology to identify the provenance. we are very happy to develop this with people like paul groth and herbert van de sompel and others to have an ontology that connects to their work. the idea is that we can have some files that link to our xml at any level of granularity. there are files that sit outside the publication or the data center but we can still link one to the other. i think this is a very promising way to move forward. what would be really interesting is if we had the opportunity to completely rethink science publishing. why only change where the data is located: why not change the whole process? in my opinion, what is key is that scientists should be allowed to do their research process the way they want. we do not want to put more obstacles in front of the busy scientists who are already struggling to do their work. in fact, i think that the publishers would like to help them. so, if they have an experimental design, perhaps they can put a copy of it in the repository and put a link to it in their paper. similarly, there are reports of observations. perhaps there can be some way to deposit these reports in a repository and to pull them into their paper, code their statistics in a same way, and then draw the conclusions. for the publisher and probably for the reader, it is incredibly important to maintain the context that the data have (e.g., the experimental context, the reason you did the experiment, the time for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.linking data to publications  159 involved, and the like). there is a narrative context and we are using it to prove a point, so the data act as a key point for life scientists to communicate with other scientists. there are big questions that we are tackling and it is very important that the data are maintained and preserved. now let me ask this question: why do not the scientists themselves keep track of their own experimental design, their observed results and their code of statistics? they can share part of this with the publisher. similarly, they can share with the data repositories. they can share the experimental design, the data and the code of statistics, using cloud computing. imagine scientists using the cloud to store their research, find their results, experiments, and observations. i think it is truly important that as research keeps building, there are good systems in which researchers can keep track of their own data, store them, and add appropriate metadata. the assignment of unique identifiers plays a central role in the advertisement of these materials. data centers are able to connect datasets and promote them. they can also advertise them. the role of data centers in terms of quality control and access is very critical and, as we saw earlier in this meeting, this differs from one field to another. so, if we are publishing a paper with data, all we need to do is to deposit our document in a repository and allow access to an editor or somebody who we think can evaluate our work. then, we would have access to the collective thoughts as well as to links to the data, to the workflow, to the other science components, and to a publisher or somebody in the role of validating quality. i think these and similar practices will connect more in the future and publishers, data repositories, and perhaps software developers (e.g., microsoft, google, skype, twitter, or dropbox) will be involved in these processes. we all use commercial software all the time. these programs are very good at building tools that help us communicate. therefore, it is very useful to have such companies working with us on improving communication between scientists by encouraging them to build better software and applications. citizen science was mentioned earlier as well. citizens can also play a key role in these processes and we should be keen to involve them. again, some technological components and applications are now in place and can facilitate these processes. let me conclude by emphasizing that, in my view, publishers are not interested in owning or charging for data. we believe in identifiers and embrace open standards and i think that scientists should keep track of their own work. we certainly believe in a future where science is shared and stored in a better and productive way, as well as in working together with all stakeholders to make it happen.  for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.161 25 linking, finding, and citing data in astronomy michael j. kurtz1 smithsonian astrophysical observatory my presentation is focused on data citation and attribution issues in the field of astronomy. there are commercial astronomy journals, but most of them are not very important. basically, the entire system is operated through collaboration between data centers and publishers, where the publishers are the professional societies. i thought i would first share with you some information about a similar workshop held 25 years ago. this astrophysics data system (ads) workshop (1,2) was held on 1987 to discuss issues related to: 1. data accessibility 2. data format standards and quality 3. data analysis and reduction software 4. user scenarios 5. observation planning and operations there was a report from this workshop. one of the points that the report made was the following: there is an urgent need for a master directory for all nasa spacebased observations. it is recommended that the directory should include all past observations and currently planned observations from observatories, and the past and planned observations from groundbased observatories, where possible. nasa and nsf should enter into discussions regarding how this can be accomplished. if we think about what we are trying to do in this workshop, it is basically similar to this 1987 workshop. if we make a list of all the observations, give them names and addresses, that is essentially putting dois and addresses on every piece of data. however, this has never happened. they spent over the next seven years or so $25 million$30 million trying to make this happen. the reason it did not happen was primarily control. none of the archival systems were willing to give up the control necessary to make it happen. now, it is a quarter of a century later and that still is the case. the second issue that i would like to talk about is related to the american astronomical society's (aas) policy for dataset linking. their policy started seven or eight years ago, is active and people are joining up. the big data centers can create tags or names for their datasets. they are able to do it the way they want. there is no large system telling them how to do it. however, they have to agree that they will be able to resolve these tags with the datasets when asked. this is a great effort and all the big u.s. data centers are part of it, but the europeans did not join. we now will look at data citation and attribution in practice. we have been doing this for a couple of decades now. it pretty much works and it is growing organically. below is a 17year old image. it is the first image of a web browser used in our ads system.   1 presentation slides are available at http://sites.nationalacademies.org/pga/brdi/pga064019. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.162  developing data attribution and citation practices and standards figure 251 first web browser used in ads system. what you can see at the bottom is a complex literature query. to an astronomer, ﬁabundanceﬂ means the fraction of different elements in a star. it is also called metallicity. the next image is what it looks like today. it is basically the same query, except that we have changed the word abundance to metallicity. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.linking, finding, and citing data in astronomy  163 figure 252 complex literature query. what happens when you run a query is that a list of popular papers about the metallicity of m87 comes up. we are interested in data so we can ask the system to select only papers with links to data from the space telescope; this yields a list of seven papers concerning the metallicity of the galaxy m87 which have links to online data in the hst archive. we could just as well have chosen any (or all) of about a dozen other archives to obtain original (from the telescope) data, or chosen to retrieve tabular data from any of several dozen papers. the next image is from todd vision of dryad. it shows the two different kinds of data to which the ads is linked. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.164  developing data attribution and citation practices and standards figure 253 zipf™s law of datasets source: todd vision, dryad. this figure shows that the highestranking source in terms of size are the archives. the archives are enormous in the amount of data they hold and most of them are very well managed. they have people who curate their data and make links. some of these archives are better than others but in general, they are good. on the righthand side, there are small data tables from journals. there is a system for taking these tables numerically and keeping them online. most of these functionalities have been running pretty smoothly for about 20 years. finally, the middle part is where the problem is. there we find small and medium size datasets with no home; they are often too big for the journals, but are not part of the established archives. i will conclude by talking about a paper that margaret geller and i submitted to the astronomical journal last week. i am going to show you the references for a plot that we made in the paper. the first one is from the sloan digital sky survey (sdss). it shows that we used york, et al., but we did not use the paper, we used the database. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.linking, finding, and citing data in astronomy  165 figure 244 sloan digital sky survey notes. source: reproduced by permission of the aas.  york, et al., has about 3,000 citations, but has millions of downloads from the online database. there need to be ways to measure the impact of things like the sdss and not just citations. the next paper below is a secondary catalog of clusters created from the sdss. this is the part we cited but the catalog itself is not in the journal. the catalog is quite large, more than 50,000 clusters, with about a hundred measures per cluster.  figure 255 cluster catalog entries form sdss dr7. source: reproduced by permission of the aas.  as the catalog is not in the journal, the question then is, where is it? i did not know, so i queried google and it showed me that it is on the personal website of the first author. this is it where we got it from. this is the type of data that is not linked to in the ads. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.166  developing data attribution and citation practices and standards so, these are two problems in how astronomers link their data. references (1) astrophysics data system workshop. workshop report, annapolis, maryland, august 1820, 1987, pasadena: california institute of technology (caltech,cit), jet propulsion laboratory (jpl), 1987, edited by squibb, gael f. (2) squibb, g. f.; cheung, c. y. nasa astrophysics data system (ads) study eso conference workshop proceedings, no. 28, p. 489  496 http://adsabs.harvard.edu/abs/1988esoc...28..489s for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.167 discussion by workshop participants moderated by bonnie carroll dr. minster: this comment is for anita de waard. you said correctly that commercial software producers do a good job. however, give me any piece of software and i will show you that some of its thousands of files dating back to 1995 cannot open and will never open again. on the other hand, my linux mail from the 1980 is perfectly fine and i can open all my files with no problem. dr. de waard. you are right. i should not have said ﬁcommercialﬂ. i do not have any preference for any kind of software development. my point is not about how the software is being developed, but about the fact that software developersœcommercial or academicšhave an important role to play in the infrastructure of scientific communication. participant: my question is for bruce wilson. the last time i looked at the dataone project, they were using pieces of software from mercury to obtain metadata from different sources. is mercury now producing outputs that can be used directly as a citation? dr. wilson: what i showed with coins (complex objects in spans; see http://ocoins.info/) is embedded in the mercury results. it is the software that drives the search interface for the ornl back and about 15 other data centers. it is also being used for search in the dataone project. so yes, we are getting there in terms of using coins in the search results. the package itself is also using the open archives initiativeprotocol for metadata harvesting (oaipmh) and we have been extending it to expose oaipmh to other harvesters. participant: does it produce output that can be used directly as a citation? dr.wilson: coins produces output that can be used directly as a citation in the sense that what we are trying to do is to provide structured metadata in a format that can be used by citation tools. participant: my comment is for the commercial publishers. at least in my scientific community, biodiversity, there is a growing understanding that one of the ways to encourage the citation approach is to publish the data and to provide some incentives, something like a data paper. given the fact that there would not be any operational burden on them, how do you think the commercial publishers will respond to such a call from the community? would they produce a section in their journals dedicated to data papers where the datasets are described through the offering of metadata? will the journals be engaged in the peer review and publishing of such data papers? it is important for scientists to publish and make data available in the open public domain, and therefore i think it is important that the commercial publishing community come forward and introduce such sections in their existing journals and publications. do you think publishers would respond to that? how would this affect their business and operational models? participant: i think the peer review of datasets is a very difficult task. if we look at michael kurtz' data, for example, we will realize that it would take a lot of work and time to do a good job reviewing the data. someone needs to do that work and it costs money. this seems like the for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.168  developing data attribusion and citation practices and standards kind of work that governments usually fund. i do not think it is the publishers™ job. if a journal agrees to do it because it is critical in their field, then that would be great. i do not think there is any blanket statement to be made here except that everybody realizes it is a difficult, time consuming, and expensive process and that somebody needs to pay for it. participant: there is a good example from chemistry. there is a leading institute called the beilstein institute that does a lot of work in the data curation area. participant: there is also a commercial version of that. i think that we need either a strong mandate backed with funding from a government organization or a private business model to make sure that the data curation is done professionally and properly. participant: there is a journal that does that. it is called earth science data. i think it has been marginally successful. like any journal, they struggle to get reviewers but in this particular case, it has been more difficult and challenging. participant: i should start by saying that i am a total cynic. yes, we do have this session with a group of stakeholders in the research enterprise, and yes, data citation is the main theme, but i do not think that citation and attribution are going to solve all the important issues across this large spectrum. there needs to be a lot more outreach and interfacing. this is just an observation, not a criticism. this question is for bruce wilson. if i understood correctly, you said that downloads of data did not necessarily correlate with the citations of data. if you look at the literature, however, downloaded papers do actually correlate with the citations of these papers to some degree. it seems to me that there is a difference of views here and i think it would be important to understand why that might be the case. whether it is because the data are not properly attributed, because there is not enough metadata, or it is a function of different disciplines, it would seem to be an important point to understand. dr. wilson: i am not aware of broad studies on these issues. my statement was based on some observations of the roughly 1,000 datasets held by the ornl. this sample has some limitations, but what we found, through simply going out and asking questions, is that there were cases in which people stopped working with the dataset because it was too hard or because there was a problem with the data. these findings helped us to identify some of these issues and fix them or greatly lower the barrier to the datasets. after fixing some of these issues, the number of downloads increased and early indications suggested that the citation of that data has also gone up. we have seen cases where some of these datasets are now being routinely downloaded and used in the classroom for undergraduate education. that is also another issue, where downloads might be attributed to other kinds of uses of the data. that is why i am interested in the discussion about what are the impacts of the data outside of the scholarly community literature. dr. kurtz: downloads correlate with citations only when researchers use them. this is true because practitioners do not cite data. it seems that materials that are useful in practice are often never cited. also, not everybody who downloads the dataset is planning to immediately write a paper. there are many other uses of data. if you look at download statistics in google scholar versus citation statistics, there is no correlation, whatsoever, but if you look at download for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.discussion by workshop participants  169 statistics for research articles by astrophysicists through ads, the correlation is perfect. so, it really depends on what the use is. participant: i have two points. first, i am following up on phil bourne's point about whether we are overloading data citation. i think that despite our attempts to keep this meeting within a narrow range, we also wanted to surface the whole set of issues concerning data citation and attribution. second, i want to emphasize that describing the data for future reuses within the immediate discipline is hard enough. describing it for future uses in adjacent disciplines and beyond requires much more context. basically, the farther you want to go from the point of origin, the more interpretation is going to be required. potentially, this might be a librarian's fulltime job. allen renear and i are among the few people in this room who built courses and educated libraries around data archiving, but i do not see several dozen of our graduates being hired for these jobs. i am not seeing the growth yet. there are real infrastructure and human resource issues here. if the panel could address whom you are hiring to do this job and why, that would be really helpful, too. dr. chavan: i want to comment on michael kurtz's point that citation is not the only way to measure the usage of the data and that there are several uses of the data that often do not result in scholarly publications. the way to address this issue in our community is through building what we call ﬁa data usage indexﬂ. this is an index with several parameters, whereby download and use of data is one of the aspects of the data usage index. several coauthors and i proposed this index through a paper in 2009. the index has gone through community consultations and over the past 18 months and we will be advertising the algorithms auditors. we believe that this algorithm can be modified for different disciplines because of the different data usage patterns in different disciplines. so, in addition to data citation, we also need to promote and facilitate the creation of other forms of impact measurement. dr. smith: yesterday, someone talked about the importance of data citation to provide credit to researchers and that professional data centers also care about getting that credit. however, i do not hear that universities and libraries also require credit for the incredibly labor intensive work that they do to get the data managed and archived. it would be useful to discuss whether research universities and libraries also should get such credit. participant: i think it is important to parse this topic well here. let us start from a community perspective. in some communities, there is very clear value from having shared data and in this case there is an absolute requirement for standards for data attribution and citation. in other communities, on the other hand, the costbenefit analysis does not come out so clearly in favor of benefits. the second point i would like to make is about credit. some institutions choose to play a role in the community in the provision of data assets that extend beyond the institution. in those cases, it is very clear that those institutions expect to be rewarded for the role they are playing. when it comes to getting credit for having, for example, an institutional repository that serves the need of the researchers in one institution, i would not expect that institution to require credit for that work. this is simply the responsibility of this institution to its stakeholders. that maybe helps to tease out some of the issues in the discussions we have been having over the last couple of days, for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.170  developing data attribusion and citation practices and standards because when it comes to data attribution and citation, i do not think there is a onesizefitsall model. dr. witt: from a library perspective, i think it is more about supporting an institutional mission more so than credit. it is also about relevance. books and journals are going away and there is more focus now on how libraries are going to deal with data collections. so if we are not an actor in this process, whether through discussing data management plans, building repositories, or creating services to help people find and use data, libraries will lose relevance. as for the workplace and the kind of new organization or infrastructure that the libraries will need, i think that the principles are all there but the technology and the packages that are in place need to evolve to meet the requirements of the new tasks. take cataloging, for example. the people who understand the aacr2 (angloamerican cataloguing rules) and how to create records and descriptions will still very much be needed. such areas are still relevant to the data world. it is not what needs to be done, but how it is done to make these changes. we can look at the organizational chart of libraries and try to identify the relevant components, whether they are in informational literacy outreach, collection development, metadata and technical services, acquisitions, or archives. i currently see many libraries defining new positions related to data service and curation. so maybe we should take a hybrid approach, where a library will have folks who do the new data related work, and other folks, who still work on more traditional areas. participant: at our publishing house, we are hiring knowledge modelers. this category can include different domain specialists, such as economists and technologists. they do modeling, analysis, and visualization. mr. uhlir: i just want to point out that the nrc board on research data and information will be doing a consensus study on the future workforce and educational requirements for digital curation, starting in the fall of 2011. we will be looking at all of these issues. participant: i think that what will happen in the data citation context is similar in some ways to what happened when we went online and searching online became an important skill. libraries needed the expertise, so they hired some specialists who did searching for their patrons. in the library schools, they hired experts who did specialized courses on online searching that were very popular. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.discussion by workshop participants  171 dr. wilson: the one thing that i would add from a data center perspective regarding workforce needs is that we are frequently looking for what mark parsons has called the data wrangler, which is somebody who has domain expertise, information science expertise, and understands that what they are going to be doing in ten years, even though it may have nothing to do with what they are doing now. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.173 26 standards and data citations todd carpenter1 national information standards organization standards are very familiar in the distribution of information, even if we do not recognize them. they are things that we rely on every day. we probably do not give them much thought. however, when we pick up a book, we are really picking up an incredibly standardized object. everything from page numbers, table of contents, an index, cataloging information, title pages, organization structures, paper acidity, binding, paper sizes, ink, colors, font sizes, even the spelling of the words themselves are all best practices derived from the mass production of book publishing. these practices have developed over time and have been adapted to provide more efficient discovery and distribution of content. one challenge posed by our current environment and the transformation to digital content distribution is that a lot of the practices that have been resolved for decades have changed radically in this new digital environment. if we think about page numbers, for example, they do not really mean anything in the flow of a digital text, where you can size the font up to 96 points and a page might only have two or three words on it. so, how one cites those particular words within such a mutable digital object is certainly a problem. as i envision the citation of the future, it is no longer a string of words and textual descriptions of how to discover a referenced item. we are moving toward an era when a citation will likely be one or more identifiers for the specific information being referenced. the information distribution ecosystem is moving toward standardizing around a number of key identifiers. these identifiers provide us with actionable answers to a variety of questions: what is this thing? what is its relationship to other things? who created it? what is it packaged with? how can i locate it? if we can create a citation structure that is built on actionable links to entities that have additional information stored in accessible registries, there is great potential to aid discovery and distribution of information. after all, an actionable network of linkable text citations was, in part, the rationale for creating the entire world wide web and its followon the semantic web. one of the barriers to establishing this network infrastructure is that the machines needed to intermediate this world for us do not talk the way that human beings do. they rely on structure and markup and are not able to easily gloss over errors in coding, syntax, or semantics. however, they are incredibly well suited to navigating a structured world with incredible efficiency. this fact has implications on the access, use, and citation of data. instead of using a textformatted description of an information resource, an identifierbased citation format, built on universally adopted standards, could build upon and unleash the opportunities in our machineintermediated world.  1 presentation slides are available at http://sites.nationalacademies.org/pga/brdi/pga064019. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.174 developing data citation attribution and citation practices and standards we are almost to a point where we have identifiers for all of the elements of a citation. identifier standards exist, or are being developed, for names (orcid or isni), affiliations (institutional identification, i2), publications (isbn or issn), collections (isci), and persistent urls (doi, arc, or purl), and dates. each of these standards could be incorporated into actionable uris and those uris, along with the associated metadata, could be served to the community as part of linked data stores. the implications of this shift of meaningful connections and machine references to a wealth of additional information could be tremendous. for example, an unambiguous name identifier could bring a user more than just the name of the referenced the author; it could also provide links to everything else this author has published. another link in that uribased citation could connect to everything else in this package or collection. on several occasions during this meeting, we discussed the development of the open researcher and contributor identification (orcid) initiative whose goal is to establish a unique identifier for each researcher in the scholarly communication process. this project is closely related to the international standard name identifier (isni) standard (iso 27729). this standard was recently approved for publication and it defines an identifier for any ﬁpartiesﬂ involved in the content creation process across all media. both of these initiatives will probably launch in 2012 and will provide us a great opportunity for uniquely identifying content contributors and clearly distinguishing between people with the same or similar names. niso™s own institutional identifier (i2) project will be utilizing the isni and its infrastructure to identify institutions and to provide metadata about them, including its links to parent or suborganizations, such as departments. combining these new identifiers with existing standards, such as the isbn or issn, we are approaching a time where all of the information in a citation can be replaced with uris. so, when we talk about standards for data citation, what do we mean? there are a variety of things we could standardize that are related to, for example, discovering the data locating it, describing it, sharing it, preserving it, and for interoperating with it. but which are the most important to pursue? the problem with setting priorities is that each person or each field has different challenges and needs. what is a critical issue for one community is of secondary or tertiary concern to another. here is my list of the things that i believe is being a high priority for good citations in a digital world: 1 disambiguation of the item. 2 location of the item (either in physical or digital form or both). 3 attribution and disambiguation of the author. 4 ability to reuse and preserve. you may think there are other priorities; for example, ontologies and terminologies, privacy issues, rights and intellectual property issues, database size and complexity, and refresh pace and update frequency. identifying the most critical needs is really the first step. mark parsons said it well yesterday: if we can solve 80 percent of our problems with an 80/20 solution, we should do that. in large part, that is what standards do. perhaps the data citation group that organized this meeting should spend some time focusing on which issues are secondary to the bigger goal of sharing data and then focus its attention on those things most critical to creating a culture of digital data citation. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.standards and data citations 175 focusing on the core problem does provide a great deal of benefit even if it does not solve every issue for every community. one good example of this is the dublin core metadata standard, which is widely used. communities have been trying to develop a better schema for how to describe content, metadata, and bibliographic information and they keep going back to the dublin core and extending its basic model. the dublin core metadata set contains all the critical elements and that same approach is probably how we should start. perhaps what we need is a good framework of data elements that define core and secondary sets of metadata for describing data or data sets, which may be applicable in across a range of communities and leave the proper display question to each individual community. the problem with most standards is not that they are bad ideas or that there is not sufficient thought that went into their development. frequently, the problem stems from a limited amount of followup or commitment to promoting adoption. the point at which most standards fail is after consensus is reached. the failure point is often in the standard™s adoptionšor better said in the absence of adoption. figure 261 the standards development lifecycle. much like the sales adoption cycle of any other product, standards go through a lifecycle process of slow growth, growing adoption, broad acceptance, then eventual decline, which leads to revision or withdrawal. figure 261 is a graph of the life cycle of a standard with adoption on the vertical axis and time on the horizontal axis. projects start out in incubation stage, where a small group of people are interested in trying to solve a problem. then there is a period of consensus development, where people become aware of the project and agree on the solution. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.176 developing data citation attribution and citation practices and standards unfortunately, far too many standards development processes stop at this stage. group members might say or think, ﬁwe have developed the schema, so let us publish it, and then we are done.ﬂ but the problem is that getting people to use a standard is the most difficult part of the process. just as with the latest technology product, in order for it to reach massmarket appeal, it needs promotion, marketing, and encouragement to get people to use it. so, we need to spend a lot of time, effort, and energy on these later stages to make a standard succeed. if we do come up with a standard data citation format, schema for metadata about data sets, or publication policies, and after we have obtained consensus on the citation structure, we have to invest time and effort in getting people to actually use it. finally, i want to reflect on who is the audience for the project we are developing? ﬁwhoﬂ is one of the most critical questions regarding the adoption of a standard or best practice that has been developed. among those who need to be deeply engaged in data citation standards adoption are researchers, educators, data centers, publishers, promotion tenure committees, administrators, funding agencies, consumers of the data, and repositories. among the challenges as we move forward is ensuring that we have engaged the right communities to ensure that what we have done here and what we will continue to do over the next year will get adopted? i think that is probably the biggest challenge facing us now, as it is with any standards development organization, or any standards community. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.177 27 data citation and attribution: a funder™s perspective sylvia spengler1 national science foundation i should start by saying that i do not speak for all of the funding agencies and to emphasize that there may be differences of opinion within the national science foundation (nsf) itself regarding the issues being discussed here. nsf cares about data citation and attribution for a number of reasons. a primary reason is that the united states congress pays special attention to what happens in science and wants to see value for the money it allocates for science and education. that is a major determinant of why we want to encourage people to provide citations for their datašbecause it makes this effort more visible. it also helps convince the taxpayer, the people who actually provide the necessary funding, that there are good things coming out of this investment. i also believe that making data citations clear and a common practice will help promote the cutting edge interdisciplinary research, which in turn will help people in their career development and make their contributions to science and to the public good more visible and appreciated. the fact that the national science board is actually engaged in the issues of data policy, data citation, and data access gives us a big incentive as well. let me now talk briefly about what we are doing at nsf. everybody knows about the requirement for having a data management plan in the proposals submitted to nsf. it is important to note that we recognize that one size does not fit all. that is why individual review panels and their managers make decisions and recommend proposals for funding on a casebycase basis. let me give an example. i had a panel in which everyone liked the intellectual merit of the project. everyone thought it had incredible broader impact in terms of education. the principal investigator (pi) had cited his/her web page data policy. the panelists went to look at the webpage for data policy and said, "we think this is an intellectually stimulating and engaging idea that has incredible education outreach but because of their data policy, we do not recommend funding it." the pi was very responsive to this evaluation and i am sure that this will happen more in the future. we are also introducing some changes to the annual and final reports to recognize data contributions, specifically to recognize individuals™ role in data maintenance. finally, one of the pieces that pis have to provide when they write a proposal to the nsf is what they did with the money we gave them the last time. they must have the results of their data management plan (i.e., data access, preservation, use, and so on) available and listed in the references to stand higher chances of getting more funding. everything i will say now about the data management plan gets highly specific, sometimes at the program level, at the provision level, and at the directorate level. also, solicitations may have additional data management requirements. the nsf policy office has a searchable website that   1 presentation slides are available at http://sites.nationalacademies.org/pga/brdi/pga064019. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.178 developing data attribution and citation practices and standards links to relevant guidance documents and examples. it is available at: http://www.acpt.nsf.gov/bfa/dias/policy/dmp.jsp. the america competes authorization act that passed at the end of december 2010 required the formation of federal interagency groups to discuss two major issues: public access to publications and the data supported (in whole or in part) by federal funds. a group on digital data at the white house office of science and technology policy is specifically looking at data policies and data standards. i also want to underscore the role of university and other institutional libraries and repositories, not only in acting as repositories but in actively developing systems for dealing with what everyone recognizes as a major challenge of metadata, including minimum metadata, usage generated metadata systems, software metadata, and the like. i want to acknowledge as well schools of information science, which are helping to develop protocol software and systems that we use. the scientific societies also need to be acknowledged, since they are becoming clearer in their ethics statements and in their expectations for membership about the necessity of having not only citable publications, but also citable data. let me conclude by summarizing what i have heard over the last two days: basically, citation is a fundamental ethic in science and it is the right thing to do. there is a great enthusiasm and support for data access, sharing, use, and citation and attribution. technologies, per se, are not an urgent problem. it is the cultural and sociological challenges, since one size does not fit all and nobody pays attention to the instructions. we also should remember that there are both human and nonhuman communication mechanisms that need to be taken into consideration. we should not wait for the perfect solution for the issues under discussion: individual communities are making some good progress and they should collaborate and coordinate. finally, i would like to emphasize that i am interested in the different ways in dealing with granularity across different communities. i think this is an important issue about which i would like to hear some more discussion. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.179 discusssion by workshop participants moderated by christine borgman  participant: i want to ask about the bottomup standards approach, best practices, or conventions. i have heard a lot over the past couple of days about what seems like a growing convention on how to do data citation. what we have seen in some of our work is that whenever there is a convention that emerges, what we often do is invent the standards and then have to redo them so that we can embrace the convention that has been adopted. maybe someone could say something about what you think about data citation and convention. mr. carpenter: one of the issues with standards development is that if you are too forward thinking, people will not get behind it. sometimes it is better to let an ad hoc specification begin in a particular community and after it has gained some traction, move it into formal standards development for a broader audience. such an approach can be very useful because, ultimately, it is all about adoption. standards will not be helpful if they are not being used. part of the process should be getting the community™s buyin. i know it is a big problem, but it is a matter of timing and marketing. we have found with different standards that often what makes a standard popular is an application that shows the different things that you can do with it. i do not know what the best demonstration application might be for data citations and would like to know if someone has ideas in this regard. dr. spengler: one of the things that i have noticed is that when major leaders in the scientific community, whether it is research funders or journal publishers, have some requirements, it often helps with standards. so, if this group, for example, comes up with some recommended standards for the data citation, it might be useful to see whether or not some organization like the national science foundation (nsf) would welcome that. this might be one way to make the transition. participant: if someone were to write a proposal based on the discussions today and send it to the nsf, to what program should it be submitted? dr. spengler: i do not represent the entire nsf, but i would say either mimi mcclure from the office of cyberinfrastructure, or me from the directorate for computer and information science and engineering. it would fall between the two of us. participant: i want to make a suggestion related to standards and the usefulness of data citation. it would be good to be able to check the dataset and make sure that it was not changed since it was downloaded the first time. this would allow us to know if the generators of the dataset found anything wrong with it and if that they have recalibrated it. participant: i will ask a policy question. the nsf's approach with the data management plan is to enforce it via the proposed review process on the front end and then the reporting requirement on the back end. the national institutes of health (nih) has had such a data management plan requirement for large grants over a half million dollars and the plans have not for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.180 developing data attribution and citation practices and standards  been part of the peer review process, just between the investigator and the program officer. the economic and social research council (esrc) in the uk has gone a very large step further and requested that to submit any proposal to gather new data, an investigator must show that no other data exist that he or she can already use. this is a whole different kind of policy. what would happen if we tried to do something like that in the united states? that would certainly be a game changer. dr. spengler: yes, it would be a game changer. the question is how would you certify any of what the uk is requesting? is this accessible from my university? is this accessible with the adequate permissions? how can it be accessed? i think that the reason for the nsf to go for the review process and to include the community is because communities are part of nsf's highly individualized approach to funding science. program directors at nsf, except for some, come and go based on the twoyear and threeyear rotation model. what we want to do is to engage the community. we do not want to make it a topdown approach. we want to make it bottom up because that is our tradition and we want to have communities make clear what is adequate for them. i could possibly take the standards that the genomics community has for data and use the same approach for people who necessarily spend large portions of their lives in less than amiable environments, trying to push forward other areas of science. that would not be very fair of me as a program director or as a reviewer. i have to think about what my rights are versus their rights. participant: the esrc requirement to look for previous data was interesting. at one point, and i do not know if it is still the case now, the department of defense required that in order to do additional research, researchers had to prove that they searched the literature. most people do read the literature and that is why they have bibliographies when they are embarking on new research. they have to prove that they have searched the literature and there are systems to do that effectively. until we have good data repositories (i.e., clearinghouses, so we know how to find what data exists), it is going to be hard to request the same thing for data. it is the data discovery tool that we do not have yet. participant: i mentioned yesterday a catalogue of many resources in the bioscience area. we obtained all the urls and their papers in each issue. the attrition rate was about 10 percent per year. there seems to be some conflict between requesting researchers to deposit data and making more data available while they do not have the repositories they need to actually carry forth the policy. how is the nsf addressing that situation? dr. spengler: the directorate for biological sciences has put its resources on infrastructure in a variety of different places, but there is not any activity that is funded to do that specifically. it is a leadership challenge within the different directorates. availability and preservation are two very different issues and it is not at all clear to me how that is adequately dealt with and that is why i am speaking as sylvia spengler, not on behalf of the nsf. mr. carpenter: i think that we as a community are not investing enough time, effort, and particularly money in longterm preservation of content in all forms, not just data. for example, if a library holds a book, you can expect that that library will keep that book until eventually they run out of space and even then, you might still be able to get the book from some form of repository. we do not do that with electronic information. we are increasingly in an environment for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.discussion by workshop participants  181 where we lease content from organizations, but we do not own it. i think we might get to a point where we are living in a digital black hole a hundred years from now because we are not investing time and resources in preservation. participant: one thing that we have seen from private foundations in recent years with regard to the sharing of physical materials is to require researchers to demonstrate that the research that they are doing is novel. they will only give funding and access to some physical material resources, such as blood samples or spinal samples, if the researchers demonstrate that it is truly novel research, not just incremental. then researchers have to share the data back. this is something that we are starting to see some private foundations do. participant: one thing the big funders might need to consider is to create a condition in which universities and research institutions accept inbound policies from smaller funders, because there are 2500 disease foundations in the united states alone but very few of them can fight harvard to mandate a data sharing plan, format, or standard. guidance to those foundations and nontraditional funders can be very powerful in facilitating adoption in this difficult period where wellfunded scientists at top universities are not going to take that money, but a scientist at east tennessee state might look for such funding and adopt the standards as part of the deal. having the big foundations and funders lay the groundwork for adoption of that broader policy would be very useful. mr. carpenter: that is a good point. i think there are a variety of communities engaging in a very traditional landscape. keeping in mind who those new players are and how they communicate would be very useful. participant: this question is for sylvia spengler. i know that the nsf requirement for the data management plan is new, but i am wondering if there is any experience regarding reviewers and panelists, how they are accepting this added responsibility for reviewing the data management plan, and whether they feel they have adequate training to do it? dr. spengler: we actually have developed sets of materials to address these issues, both as instructions to reviewers when they start looking at the proposals and during the panels themselves. there are many issues involved here. there is an education process within the nsf for the program directors so that they become aware of the importance of these data plans. i think that part of the reason why it took so long to make the data management plan requirement visible is that there was a lot of concern about the additional effort that it would require not only in review, but also in award oversight. my guess is that in the long run, that will turn out to be part of submitting an annual report and, as we all know, the annual reports and the final reports enable researchers to continue to receive money. the funding agencies are not opposed to being a stick when pushed to do that. dr. callaghan: i thought i would give a different example of what is happening across the funding agencies in the uk. most of the money that is funding my work today comes from the national environment research council (nerc) and they are very keen on implementing data citation and publication. they also released the new data policy in january 2011, which essentially states that all data collected under research funded by them should be made publicly for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.182 developing data attribution and citation practices and standards  available through publication and environmental data centers. that is a good thing as far as we in the data centers are concerned, but we still have to convince the researchers who produce the data to deposit them appropriately. the other research councils of the united kingdom are following suit as well. there is pressure coming from the uk government, which decided a few years ago that if any scientific data or any data is collected as a result of public funding, it should be available to the public. so, there is pressure to do this, but it is up to us to tell nerc and the uk government what is the best way for us to get the data producers to comply with collecting, and then publicly sharing and archiving the data. dr. bourne: when someone mentioned the "stick", it made me think of the nih open access policies as something that could be considered for the nsf data requirements policy. it might be worth looking at how the nih policy is working and what additional lines and budgets to support it are expected. dr. spengler: i must clarify something about the nsf access policy. at the moment, you can get to an abstract, but you may or may not be able to get to the entire article. it is clearly something that is on the table, however. that is why there is an interagency task force or working group at the office of science and technology policy trying to deal with questions of public access to both publications and data. mr. carpenter: the publishing community is certainly interested in partnering with the data repository and scientific community because they recognize that they do not want to be performing those functions. the publishers are not interested in being the repository for any public domain data. it does not fit well with their business models. participant: i think that the publishers are listening and they want an access policy proposal that cuts across domains, obviously. they will have greater difficulties with different standards for different domains. while they will understand a diverse situation, the more generic the guidelines are, the easier it will be. mr. carpenter: as i mentioned earlier, there is a project currently within niso to look at how to tie together whatever supplemental materials are submitted with a paper, be that a dataset, video, audio file, and so on. the publishing community is already thinking about this and trying to address some of these concerns and issues. participant: i think the two key issues here are quality and discoverability. that is what the scientists and publishers care about. dr. kurtz: besides quality, reusability is very important to the operation of standards. in the astronomical virtual observatory movement, what we call the international virtual observatory alliance is basically a standards organization that is developing complex standards for characteristics such as at what time was the observation taken, what wavelengths are involved, and the like. it is a description of the observation so that it is machine readable and reusable by some kind of standard software tool. the increasingly complex data standards are clearly fielddependent, but they are necessary for machines to communicate and evaluate data so that people do not. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.discussion by workshop participants  183 mr. carpenter: i think there is a difference between the very domain specific intraoperability question and the more general 80 percent answer to how do we find, locate, interact with, and discover data. as a community, we need to be careful not to tread too closely into the domainspecific area because it very quickly gets bogged down and we will not be able to accomplish anything if we focus too much attention on those 20 percent solutions that are very domain specific. dr. spengler: i would like to go back to the comment on quality and discoverability. the national science board has had discussions about using data citations for biosketches and resumés of principal investigators. one of the points that todd carpenter made was about peer review and i was pleased to hear this point brought up yesterday. however, the reality is that there is nothing in any of the citation styles that i saw discussed yesterday that says whether or not something was actually peer reviewed. some researchers post their dataset online with very low quality. i know this is their issue, but where does the peer review come into the picture? i am hoping that the report that comes out of this workshop actually addresses that aspect. mr. carpenter: one of the really interesting conversations that the publishing community has been having within the joint nisonfais project on journal article supplemental materials is the difference between what is ﬁcore to understandingﬂ and what is ﬁsupplementalﬂ. if it is core to understanding then it should go through the same rigorous review process that the paper goes through. if the information is not really critical to understanding or is just supplemental, then the question is do we really have to review itšor even have it? this has actually been one of the most interesting philosophical conversations taking place among the publishers in the niso project in terms of defining what is supplemental. participant: i am glad you brought up the peer review issue again. there is nothing in the current citation practice and literature that implies peer review. it is all about norms. depending on the discipline, different materials get different levels of review and it is all very normsbased. it is the sort of thing you learn through your career as a scholar. mr. carpenter: in a print environment, we are relying on the reputation of publications such as nature and science, which has developed over decades. it is not perfect but we have a culture that has built up over time and we cannot simply replicate that today in a new environment because we have shifted to focus on data as opposed to publications. that is going to take additional time. participant: we should separate concerns and try to solve some fundamental problems first. citation and peer review are connected, but different. we have already heard that the journals that have started to do peer review of data are struggling. i want to point out that one of the current bases for the ranking of journals is how many citations refer to them. in the same way, we could start to build up a ranking system of the data centers, if that is a necessary outcome. the first step would be being able to count and track the number of referrals to a data center. i think that probably could be solved by concentrating on the citation element and then the quality of particular data centers would come out through those numbers and through other practices that are yet to be defined. there is a way of approaching this in small steps. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.184 developing data attribution and citation practices and standards  participant: i want to comment on the point regarding overreliance on the notion of peer review. when we have some of the larger fields with shared instruments like astronomy, that is very different from the folks who are in small areas of ecology. we do not have the kinds of agreed upon databases in all fields. those of us who like to call themselves interdisciplinary sometimes publish in computer science, social science, and information studies, for example. i publish both quantitative and qualitative work in these fields. i cannot even tell you who the peers are who would examine my data. there is consequently a huge long tail of fields where the community is not clear to develop its standards and policies. i am concerned that we are using peer review and community in a sense of big science, rather than this long tail. mr. carpenter: the peer review process is communitybased and the review criteria for computer science, astrophysics, and biology, for example, are somewhat different. if we have a database in a particular field that is core to our understanding, then it should go through the same process that a paper in that field goes through. dr. de waard: i am wondering why the concept of ﬁcore to our understandingﬂ seems totally wrong to me. it seems that there might be different use cases of data and it might be good to differentiate among them. one case is when you are convinced that the story that the author is telling is true. you need to look at the data and how they were obtained to be convinced. in this case, we can say that data are core to our understanding of the paper. there are other use cases and strong arguments for depositing data, however, even when it seems perhaps trivial for the authors themselves. this might allow others to do other types of research if the data are deposited in a usable format. gully burns proposed to deposit data in such a way so that someone can actually have metastudies that cut across different types of research. another example is einstein, who looked at michael morley's work because he was able to access the data that they could not interpret and this offered support for the theory of relativity. i think it is important to recognize that there are different use cases of any datasets. dr. bourne: i want to reiterate that talking about data citation together with peer review seems a very big activity and maybe something that should be addressed separately. if you look at the peer review of papers, the strain on that process is unbelievable. i get many requests to do peer review and i do not think i could do it for data. i can determine whether the data are good or not only when i use them. dr. spengler: people who get data online frequently have an almost instantaneous reflex to find out who funded the data and report any usability or quality issues. whether or not we consider that as act of peer review is open for discussion, but it does happen and you would be surprised how long people remember that they could not use a dataset. dr. callaghan: when it comes to peer review of data, we have been thinking about different levels of citations. we have what we call ﬁplastic citationﬂ, which is the case of researchers simply putting their datasets on excel spreadsheets and posting them on their personal web pages. it might not be usable as far as other users are concerned, because they might not be able to open the spreadsheets, but the datasets are citable. the next level that we call ﬁsilver citationﬂ is when the dataset is in a repository that is generally trusted by the members of the community. here, we can make certain assumptions about the quality of that for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.discussion by workshop participants  185 dataset simply because it is hosted in a repository. if we have done our jobs properly, the mere fact that it is there and cited means that it is in an appropriate format. even if the format is going to be migrated or changed, the metadata will be there and will be as complete as we can make it. moreover, when you open the file, you will be able to do that using standard tools. so, by the mere fact of the data being in a trusted repository, we are more confident about them. in terms of technical aspects, this is actually going to be quite helpful for the scientific reviewers because they know that if it is in the right repository, they would not have to worry about finding the right program to open the files. as for the scientific peer review itself, given that technical issues are taken care of, reviewers can focus on the quality, value, and other important attributes of the dataset. so, in a sense, we have got two levels of peer review. we have got the technical peer review, which is done by the data centers, and then we have got the scientific peer review, which is done by the domain experts as part of what we consider the formal scientific journal publication process. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.187 part six summary of breakout sessions for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.189 breakout session on technical issues moderator: martie van deventer rapporteur: franciel linares  the breakout session on technical issues for data citation focused on synthesizing and bringing together ideas from the individual participants. the purpose of this breakout session, like that of the other three breakout sessions, was to: identify the key issues that were raised during the workshop. identify those issues that are important to the topic that were not already discussed. discuss in greater depth the issues that the breakout group thinks are most important.  identify several issues for further work and choose one for discussion in the plenary session at the end of the workshop. what are major technical issues related to data citation, what are those that were not discussed and which ones may be more important? the group created a list of issues that were considered important regardless of the order in which they were discussed: determining the right versions to cite;  how we can use existing web conventions, such as landing pages; the need for standards in creating humanreadable and machineactionable landing pages (e.g., xml, rdf); how this relates to web mechanisms and how to leverage existing paradigms and not reinvent the wheel;  the need for a of a set of examples of existing technologies that are being used to illustrate;  how to put a dataset into a bibliographic tool, including syntax; views of data citations, and how to identify referenced datasets, granularity, and subsets; how to determine what is really being identified (e.g., the item itself, a descriptive landing page, or a journal article or the xml document representing that article);  identity (including scientific equivalence);  granularity of the database and citation; and location versus identification. a data citation might be something that simply identifies the data that has been used, or it might also provide a means to access the data. one key issue identified by several participants is how a data citation included in a paper might deal with both the identification of the data used (which has implications both for the issue of credit and for the issue of scientific reproducibility) and also provide a means to locate the datašwhich is often crucial to provide assurance of scientific reproducibility. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.190  developing data attribution and citation practices and standards  the group discussed the necessary and sufficient characteristics of an identifier. from the dataone research program perspective, the only necessary characteristic is that it be unique (within a particular name space). an underlying issue is the level of the information that is being identified. mod12qa1 collection 5 is an identifier for a particular concept, but does not identify the specific files used for a particular scientific analysis. it is an identifier that is sufficient for assigning credit, but it does not provide enough information to identify the source of the particular data. several participants noted that an identifier in a citation should provide enough information, perhaps through use of a resolver service, to be able to get to an online ﬁlanding pageﬂ that identifies the data and perhaps also provide information about access of the data. at minimum, as one participant observed, an identifier should be unique, not reusable, and not transferable. as several participants in the breakout observed, the data citation can have a number of characteristics. it can be in the format prescribed by the style guide used by the journal. it can include the doi to (the intermediary ﬁlanding pageﬂ of) the data being cited, so that data centers can use it as a fixed string to search on to find uses of their data. it may also include a doi to an authorcreated landing page, which for clarity we can call the ﬁcitation pageﬂ, and which contains information about how the dataset was further divided, processed, or otherwise manipulated. it ought not link directly to the data. it may be in humanreadable text until such time as we have domain standards to explain subsetting, processing, provenance, and the like in machinereadable form. it can be stored for the longterm and have a doi assigned to it. it also can link back to a data center's "landing page" for the data. what is being identified could be clearly defined (e.g., a landing page or a journal article or the xml). in a data citation there could be an identifier that identifies a landing page about the dataset of interest and it could involve a resolution mechanism. given that citation and given the associated identifier a user ought to be able to find the landing page that contains information about the dataset. since the citation itself should not go to the data, a suggested access paradigm is that there could be something in between the citation and the data. the string selected for an identifier could be provided by an organization with longterm longevity, and which has the authority to do so. versioning is also an important topic. a uniform resource identifier (uri) can lead a user to a landing page and then another uri that leads users to the granular version of the data. a certain version and granule may have its own uri. this information could be part of the citation created by the person who used the data. it reflects an attempt to recognize the scientific concept of the data that were used and also an attempt to encapsulate the specific subset of the dataset that was used, particularly for situations in which the entire dataset may not be completely reproducible. one of the participants raised three cases that could be considered. in the first case, the citation would be simple and would always point to the original landing page (or something similar). this would establish credit, but would not refer to the specific instantiation used for the particular paper. (that is, it would not address scientific reproducibility sufficiently). in the second case, each user could create (in some location with longterm longevity) a landing page that refers to the specific instantiation (granules, manipulations, subset) used in the particular study. this would handle the scientific reproducibility within the limits of the for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.breakout session on technical issues  191 longevity of the landing page, but would complicate the issue of credit, depending on the infrastructure by which the dependency tree is created. in the third case, one could have a more complicated citation, which would pull together both the original uri for the data and a uri that describes the specific instantiation used for the particular scientific study. this citation would be more complex, but would provide two uris: one uri that could be used from a credit perspective and refers to the concept of the scientific sense of the dataset, and another uri that could point to the specific instantiation of the scientific data used for the particular study. another participant noted that the group supplying or publishing the data could provide a page of information about the data. there could be standards created for these pages that specify the minimum information necessary for basic citation use, but that would be extensible for domainspecific information, or other valueadded services, such as linking to papers that use the data. citations could go to this intermediary "landing page," rather than directly to the data, as the data may be excessive in size, and not useful without the proper documentation or software to read it. the page could have information about the data suitable to create a citation in the various different citation standards. the url to this page could serve both humanreadable and machinereadable information. there also could be a standard for the machinereadable portion, and it would be best to avoid competing standards. the landing page could have information on how to obtain the data and how to use the data, such as links to papers about the data or the instrument that created the data, other grey literature documentation, or software necessary to read the data. the landing page could be stored for the longterm and have a doi assigned to it, although the landing page may change over time. it could be updated when the data are moved, or are no longer available. if the data are replaced by a new version, a user ought to be able to link to a page describing the dataset as a whole (without versions), that would then link to the most recent version, rather than linking to the next version (so we do not have as long a chain to resolve to find the data). this page could be an appropriate place to give credit to all of the people who are involved in the creation, validation, and maintenance of the data. there was further discussion of how to cite subsets of a dataset. one case could involve using the original identifier plus another identifier to describe the new subset. do we complicate the citation in order to make giving credit easier or do we make it simple to create the citation? in the latter instance, pointing to the new landing page identifying the subset could make giving credit more difficult if there were no way to trace back to the original dataset. if there is a landing page to a subset of the data, it could link back to the original data center, publisher, or landing page also. if it has been replaced by another version it could link to a landing page for the unversioned dataset. questions that could be discussed more in the future while most of the discussion in this breakout group covered data citations and how to identify referenced datasets, granularity, and subsets; other questions were identified by some participants as potential subjects of a subsequent meeting: for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.192  developing data attribution and citation practices and standards  how should we handle the aggregation of datasets (e.g., data from over 100 sources)? some of the current bibliographic systems might search extended methods supplements for references. is it sufficient to have only the data supplier™s landing page doi in the citation? would guidance similar to when you use ﬁet al.ﬂ be useful? for example, if one is citing three or fewer datasets, each one could be cited individually, but if four or more datasets are aggregated, then could one use the citation page to aggregate them? if as part of the research, a new dataset is derived or synthesized, and is going to be made available, are the researchers obligated to cite back to the original data source, or just to their "new" data, which then might have the link back to the landing page from the data source? for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.193 breakout session on scientific issues moderator: sarah callaghan rapporteur: matthew mayernik  what are major scientific issues related to data citation, which ones are general and which are field or context specific? before looking at some potential answers to those questions, some of the participants in the breakout group first tried to get a better grip on what a ﬁscientific issueﬂ meant. one thought was that a science issue is something that represents a disciplinary matter, in contrast to technical issues, which focus on how to do data citations. for example, determining what a ﬁdata aggregationﬂ is that can be cited would be a scientific matter. this could be decided based on disciplinary community norms. one participant noted that an important scientific issue is dealing with equivalence. the scientific equivalence of datasets is an outstanding problem because, ideally, the users of data would like to know when looking at two citations whether the citations are ﬁthe same thingﬂ from a scientific point of view. it can be challenging because data often lead to derived data, or they may be subsets of larger citable data collections. this points to a couple of key scientific issues with regard to data citation, data versions (how to cite data that change), provenance (how to track that citations are to data that have not changed), and data linking (how to link to data that are poorly bounded objects). another participant observed that many scientists would rather be using their data than managing them. this does vary by discipline. for example, bioinformatics is strongly based on using data created by someone else, which implies that people are making their data available to others. a distinction might be made between disciplines that are based on using others' data versus discipline based on providing others with data. different domains have different cultures, different funding mandates and norms, and different shared histories of practices. some data practices are also very dependent on individual personalities. another issue deemed important by some of the participants is that scientists do not have enough time to do all of the data work that is necessary to make their data usable by others. at data centers, there are people specifically responsible for cleaning and archiving data. what kind of partnerships might be available between scientists and data archives or centers? at a data center, it can be very difficult to work with scientists, who may be reluctant to collect appropriate data or provide full metadata. when possible, data centers may try to establish relationships and work procedures with scientists at the beginning of projects. many researchers are likely to be motivated by short term goals, not longterm goals, which is why, for example, documentation for the longterm is a low priority. researchers may deposit data and ancillary data (reduced data) into an archive, but have no guarantees that anybody will access these data except the people who deposited them and there may be few rewards even if others do access and use these data. what might be the "minimum metadata" for a data set? every domain faces this question. data citation initiatives also may face an analogous question: what is or ought to be the minimum metadata for data citations? with data sets that are created within large distributed for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.194  developing data attribution and citation practices and standards collaborations, identifying the data set ﬁauthorsﬂ could be difficult. in these cases, a data set may be attributed to a project, or, in analogy to movie credits, individuals may be attributed based on their individual contributions to a project. establishing minimum metadata standards for data citations, however, could be fairly domain specific, but in most cases they would probably look a lot like the dublin core metadata schema: who did it, when did they do it, what is covered, what it is called, how to find it, and the like. this is essentially the dublin core "kernel" metadata. data citations, however, can only include a small amount of metadata. extensive descriptions of a data set or the individual contributions to a large project may be best documented in other locations, such as a data set or project™s website. the entire data collection level is currently the most common data citation recommendation. do collection level data citations meet the minimum bar for all disciplines? within academic scientific projects, as one participant noted, data work usually defaults to graduate and postdoctoral students. data loss can thus be a significant issue. when students leave, their data can be lost to the broader project(s) in which they were situated. this can cause trouble in recreating experiments and is impossible for observations of unique phenomena. this data loss due to losing students is not new, however. this was already the case 30 years ago, and probably even further back. the significance of this issue is that if one cannot reproduce experiments from a lab, one cannot expect anybody else to reproduce them outside of one™s own lab. few papers that are read are actually replicated, however, and then frequently only at substantial cost and effort. replication also usually only happens if there is a suspicion that something was wrong with the original study. another participant said that there may be a need for documented workflows for provenance, but it ought to be easy to generate such provenance documentation or nobody will do it. with regard to ﬁworkflowﬂ tools, if they work with a "click" online, then they will probably be used, but otherwise, probably not. if one can take a snapshot of a laboratory via a workflow tool, then there is the problem of distinguishing what is relevant to a particular issue from what is not. many small steps in a data workflow pipeline may be purely of local interest and not really part of the science. data reductionšpulling the data relevant to a particular issue out of a larger set of datašis part of the scientific intellectual process. the question came up as to why it is that most work is not represented in workflows now. several participants commented that this is probably because most processes do not map to a workflow. workflows usually work best for repetitive processes. at best, in other scientific work settings, scripts and directories that a student may have left behind become the responsibility of the next person who is hired. most workflow tools are developed by computer scientists and have not fully penetrated the scientific fields. there are not many examples yet of workflows that have led to scientific breakthroughs. some scientific projects, within bioinformatics for example, are similar to software projects. in those situations, workflows may be more apt. the most used workflows, such as taverna, wings, and galaxy, are mostly used in computational sciences. within structural genomics, on the other hand, workflows never really got off the ground, even though some researchers would say that they are using them. in relation to data citation, workflows are a scientific notebook instantiated via a digital technology. are publishing and citing these workflows relevant to data citation? if one can make it easy, workflow tools can be a prerequisite to a data citation, enabling citations to be generated for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.breakout session on scientific issues 195 automatically. if a ﬁbuttonﬂ existed to generate data citations out of a workflow plan, scientists might use it, but currently there is no such shortcut. workflows might also allow some metadata to "fall out" for free. data centers often find themselves in the situation where the data that are coming in already have lost metadata. if those metadata were captured in workflows, metadata might be maintained more easily. however, workflows might also cause you to lose transparency by ﬁblackboxingﬂ the steps that take place in a data pipeline. many participants agreed that scientists need rewards to incentivize data management, and, correspondingly, data citations. if rewards existed, people might do it. for example, writing a research paper is just as timeconsuming as documenting data, but scientists write papers all the time because of the rewards given (or at least expected) for publishing them. citing data can be difficult, in part, because counting data sets is difficult. for example, one uk research assessment had the option of counting data sets, but not many were available to be counted. this is an issue in the united states as well. the nsf is talking about enabling people to cite data or their contributions to data in their résumés. peer review is also an issue here. are data citable only if they are peer reviewed, or can data be cited as long as they are accessible? one participant noted that most of our examples of data citation are from escience or big science. a significant exception is the dryad data repository, which has ecological and evolutionary biology data from smallscale projects. how do scientists in all fields relate to data? scientists might think of a research idea and then look for data to investigate the idea, or they might see what data are available and develop research ideas that those data can address. ﬁbig sciencesﬂ do tend to think about data more, partly because they have to have data management plans ahead of time in order to get funding. collaborative research versus research that is carried out by individual principal investigators shows a dichotomy with regard to data management. there might also be a dichotomy by career stage; for example, university deans might not care about data management because of where they are in their careers. diane harley™s presentation in an earlier session spoke to this issue more explicitly: scientists themselves have a responsibility to support data management and citation issues. for example, metrics on data use often are not released, in part because some high profile data sets might not be used much. another dichotomy is between disciplinary data repositories and institutional repositories, as one participant observed. different issues exist for each kind. institutional repositories tend to be library services. there are few successes with respect to institutional repositories. one reason for this lack of success is that people have to be vested in a repository in order to use it. it can be easier to convince people to submit to a repository if they know that there are people looking in that repository who are relevant to them. disciplinary repositories may have been more successful for that reason. ﬁdomain specific repositories,ﬂ however, can mean "silos of excellence". the question arose as to whether successful multidisciplinary repositories exist. even within disciplines, repositories can be problematic. a common issue is that many individual repositories may have been funded within a particular discipline. as services are added on top of those, answers cannot be found for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.196  developing data attribution and citation practices and standards within one repository. instead, the answers are across repositories. some groups, like the virtual observatory in astronomy are trying to develop services that cut across disciplinary repositories. as repository connections and consolidation take place, several participants observed that the location of data sets may need to change for the purpose of making scientific research easier. moving data from repository to repository might make any location binding in a data citation potentially dangerous, although the use of the digital object identifier (doi) is trying to mitigate this problem. identification is the first step to linking, but citation is more than linking. some of the participants discussed the fact that citations to journal articles perform many functions and that we are trying to shoehorn all of these functions into data citations. citations create the thread of science. they follow traces of use, both positive and negative uses of a resource. citations may be in support of or in refutation of a finding. one problem is that researchers rarely publish negative findings. for example, most crystallography research only reports positive results, when, in fact, most research attempts fail. publishing data, however, might make it easier to cite negative data; that is, data that show negative research results. in structural genomics, you do not get funding if you do not publish negative results in addition to positive ones. one participant observed that researchers learn over time about how to document research practices and software code. for example, one suggested practice when writing code is to insert tests into code that help to ensure quality. these quality checks, however, typically slow programmers down in the short term. the situation is similar when documenting and working with data. anecdotally, some scientists report that they spend too much time on working with data, and not enough time doing science. data citations seem simple, so why is it so hard for people to do them? bibliographic importing could make this a oneclick issue. creating metadata, however, might be harder. data centers require a lot of metadata, in some cases perhaps more than the scientific community may be willing to provide. another question that was identified in the breakout discussion focused on whether there is any field where data citation may be the norm. focusing on positive examples might help to illuminate the issue. one example that was raised is in geology. there is a fossil registry that generally everyone uses. they have a specific citation method with hundreds of years of history. this is not eresearch, however. fossil resources are not digital. they also have an extensive catalog of single objects, which is not typical in eresearch. what else is different here? these fossil data come slowly over time as new fossils are discovered. also, fossils are typically only uncovered via a large time and money investment. perhaps those resources are seen as having more value because of that investment. as a contrast, in crystallography, it used to take a whole lifetime to develop data sets, but now it has become very easy with digital techniques. perhaps there is a notion of "canonical" data that applies to fossils. other examples were raised by the participants. one concerned seasurface temperature data held in the data archive of the national center for atmospheric research. anybody who does research with seasurface temperature typically uses that data set because it is community developed, comprehensive, and maintained over time. similarly, the census data are widely used and cited. "benchmark" data are another example; that is, data that are used to evaluate algorithms in information retrieval or visual image processing research. behind these canonical for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.breakout session on scientific issues 197 data sets are methods, and these citable data sets are seen as "gold standards" for quality data. some of these canonical data sets are quite old, however, and it sometimes may be useful to update them using new technology. it takes ongoing community development efforts, however, to update such data. one of the participants noted that in some ways, data citation is a simple problem: provide people with the recommended citation formats and assign data sets dois. why would people still not cite data if these are available? in some cases, it might never occur to people that data have any value, even though they have used them. people may not recognize that they have used somebody else's data, but if you walk them through their data processes, in fact they have used data from other sources. for example, marine biologists forget about tide data, even though tide data are critical to their work. to scientists, asking them to cite data might be like asking them to cite where they got their laboratory chemicals. data might be seen as a tool, not as an intellectual resource to be cited. it was noted further that one of the biggest scientific challenges with regard to data citation may be changing the scientific culture so that citing data becomes a regular practice. scientific practices change gradually, so outreach is useful to implement good data citation practices. the "tipping point" for data citations might not be something obvious. in the united kingdom, the "freedom of information act" is having an impact, because researchers are more aware that their data may be requested. there is no equivalent requirement that reaches down to national science foundation (nsf) grantees. nsf grantees may be awarded exclusive legal protection for their data. several participants observed that the funding agencies™ data management planning policies might be a lever arm as they evolve. the business models are unclear for data sharing. in some circles (usually people outside of the research process) data are seen as so abundant that they must be easy to share, but this may not be the case. citation and free access are not directly connected. access does not imply "no cost." for example, there could be a "fee for serviceﬂ model in data archives, some cost of a grant would go to cyberinfrastructure that enables data archiving and sharing. when the nsf calls for new big infrastructure proposals, some fields may not respond well because they already have invested in infrastructure independently. there are no single repositories in discipline fields, so cost models might differentiate how users adopt them. the costs for a "fee for serviceﬂ model of data archiving could be front loaded; the initial users could take the biggest hits because of the small initial user groups. economies of scale might be slow to grow, as well. other questions were raised in this regard. do fee for service models exist and work? the interuniversity consortium for political and social research (icpsr) has a kind of ﬁfee for serviceﬂ model, specifically, a mixed membership model. some icpsr data are only available to members and some data are available to anyone. can we take lessons from the citation indexing business model? citation indexing took a few decades to become an accepted tool, and only after eugene garfield (and his collaborators) championed these tools in numerous settings and founded a company to enable their work. one of the participants noted that cloud computing models are gaining traction in scientific fields. evernote, basecamp, dropbox, and others, are widely used cloudbased tools. anyone can for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.198  developing data attribution and citation practices and standards get them and they are very easy to use. cloud tools are not necessarily interoperable; resources can be siloed in cloud tools just as easily as they can be siloed with conventional technologies. cloud computing can also introduce a whole new set of confidentiality issues, and is not without some costs as well. with regard to data citation, how do you cite data that are in the cloud? data may be distributed across computers, and in some cases mirrored or duplicated. one of the biggest scientific issues related to data citation identified in this breakout session was the culture shift that might be required in many research domains. currently, citation of data is not widespread in most research communities and is not the accepted thing to do. what is the "tipping point" for data citation? what will push researchers to cite data? one possibility is that a new reward structure for scholarship could be developed. for example, the tipping point for data citations might be when somebody starts counting data set citations. even if such counting becomes the norm, however, new data citation metrics ought to be developed within institutional structures such as data centers, libraries, universities, and other institutions that provide individual rewards to scientists. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.199 breakout session on institutional, financial, legal, and sociocultural issues moderator: vishwas chavan rapporteur: laura wynholds this group faced the challenge of wrapping institutional, financial, legal and sociocultural issues into a single session. given that the focus was broad, the conversations branched and circled around the dependencies of data citation. citation is one aspect of larger systems, such as scholarly communication, academic work, and data archives. arguably, it lies at the nexus of these established systems, all three of which are in the position of having a considerable installed base as well having practices in flux, so that the outcomes are speculative. one participant observed that a whole curatorial system is lacking in comprehensively addressing data citation, which some referred to as infrastructure. others were keen to point out that most people do not include workforce and best practices under the term ﬁinfrastructureﬂ, both of which are issues here. it was also noted that best practices are a collective responsibility that represent a twoway street between the users and the system. the following major issues were identified for further discussion:  resources for infrastructures and human resources for both data and metadata;  enhancing the recognition for data publication and citation;  financial sustainability of infrastructure for publishing data and metadata;  being able to appraise the value of data; costs versus benefits of data citation;  issues of intellectual property (ip), privacy, security, sensitive data, publicprivate data (confidentiality versus openness); and creating a culture of authoring good metadata.  from the outset, it was noted that a single approach for all of these issues is not likely to be effective. however, questions remained, such as what issues would be amenable to a collective approach? in which disciplinary approaches? what are the barriers to uptake? some of the participants thought that the disincentives to sharing data were paramount. others felt that culture change around describing and citing data was extremely important. finally, the issue of appraising the value of data versus the costs of curating it remains. external dependencies that impact data citation in the discussions, there were several major external systems noted that interact with data citation, namely scholarly communication, data sharing, academic work, and data archiving. moreover, these discussions were so intimately intertwined with data citation, that they were often conflated, such as in the discussion of barriers to data sharing being seen as a barrier to data citation. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.200 developing data attribution and citaiton practices and standards 1. scholarly communication one participant noted the usefulness of integrating data citation practices with an existing system of scholarly publications, which themselves are used to measure and track scholarly output. there has been increasing awareness of the importance of data publication, and increasing pressure from funders to make research data available. however, while there are a number of models of data publication in existence, the practices are still unstable. some journals are investing in supporting data in conjunction with the articles, while others are discontinuing the supplemental submissions after a trial period of a few years. institutions are also acting as publishers via institutional repositories, and have a need to get credit, but they cannot enforce compliance in the same way that journals can. the importance of the disciplinary community defining data citation policies came up again and again. the degree of uptake and implementation varies across disciplines, and crossdisciplinary issues lack attention. it was also noted that getting the buyin from key editors would be important. it was posited that currently the transaction costs are too high for data publishing, requiring too much work from too few users. in cases where network effects could be realized from aggregating data, then it could become worthwhile for journals or societies to archive data. data citation and publication themselves are metaphors taken from scholarly publication, some participants mentioned. there are tensions around applying print publication models to data, especially since ip rights are different for data and the protections offered vary significantly between countries. moreover, the law does not match what is being done in practice. in order for the metaphors of data citation and publishing to be useful, it can be useful to understand what it is that we want to count and how it is different from other kinds of publications. 2. data sharing understanding who shares what data and why is an underlying factor for understanding data citation practices. christine borgman™s ﬁconundrumﬂ paper (jasist, in press?) discusses these incentives and disincentives. it was observed by some participants in the group that there was a fair amount of good will towards sharing across the domains, with comments such as ﬁscholars will share because it is the right thing to do, as long as it is not too much work or too riskyﬂ and ﬁevery time i share data i learn somethingﬂ. data sharing is seen as part of moving the field forward, although funding agencies are requiring it as well. a large part of the discussion on data sharing was airing concerns about disincentives to such sharing. foremost were concerns expressed about the cost of curating data. a part of this was the observation that not all data are equal, nor should all data be shared. scientists have a general fear that their data will be misused, misrepresented, misconstrued, or used for purposes that are antithetical to the scientist. one of the discussants noted that there is currently a public relations attack going on about chronic fatigue syndrome that has escalated to threats against personal safety. within the issues about data sharing are also concerns about incentivizing data reuse to drive demand. data intensive fields may have more incentives to reuse data. there are some common issues across many disciplines, but as one approaches the next level of detail, the constraints for for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.breakout session institutional, financial, legal, and sociocultural issues 201  data reuse have variation. libraries do worry about the interdisciplinarity and the cross cutting issues, and then the more disciplinespecific concerns. the first major group of disincentives to data sharing dealt with legal issues and privacy. the legal issues were discussed first. it was asserted that while intellectual property rights have been developed into a maturing system of rights and responsibilities, privacy concerns are still an open problem. with pervasive mobile data collection possible at previously unimaginable scales, privacy has become a significant issue. it was observed that institutions will have to deal with the privacy concerns posed by data collection or face liability. there are some extant models of privacy around social science survey data, where the publically aggregated data is anonymized, but more detailed data must be accessed via a controlled process. however, it was also noted that many of the privacy issues are separate from data citation. in addition to privacy issues, there are other access barriers, such as national security, law enforcement, and sensitive data, all of which place limitations on data sharing. some have seen conflicts arise at the intersections of communities, for example when university faculty collaborated with a certain federal agency, in which the faculty was under a huge pressure to make their data available as soon as they were collected, ignoring the faculty™s right to first publication. there was the suggestion that more work needed to be done to set up practices that recognize the rights and responsibilities of individuals and the handling of sensitive data. finally, some scientists fear that their data will be misused or used for purposes that are antithetical to their own. others are concerned that the data can be manipulated to attack the researcher™s credibility, as with some of the climate science controversies, or misrepresented to support political agendas. in relation to this need, creative commons is working on a standard where any changes to the data are declared within the metadata. 3. data archives and repositories data citation is functionally dependent on a storage location for the data. on the surface, data citation is about giving credit for sources used. the persistence of those sources is assumed for purposes of credibility and reproducibility. ensuring access to a snapshot of the data is expected, both by funders and by publishers, although often not in perpetuity, but rather for a reasonable period of time. a reasonable time frame would present the opportunity for institutions and archives to harvest a copy for safekeeping. the question remains as to whether institutions may have a greater role to play in ensuring longterm access to data. in areas where data archives are lacking, some journals have been stepping up to the role of ensuring access and providing storage, such as the ecological society of america. some journals do see that as their role. some researchers said they have questions as to how long that will last, given the example presented earlier in the day of the journal jettisoning its supplemental materials entirely. others noted that aiming for ﬁpermanently accessibleﬂ data was unrealistic, that they would focus the discussion on ensuring access for a reasonable period of time. it was noted by one participant that this sense of a reasonable period of time (rather than in perpetuity) came from the nsf™s blue ribbon task force report on sustainability. it was not for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.202 developing data attribution and citaiton practices and standards modeled on the ﬁput it away foreverﬂ paradigm, and that material would be moved and reappraised regularly. however, the thing about data is that you will be dealing with more mobile artifacts than the traditionally archival perspective. data selection and appraisal were noted as an important feature of data curation with which data citation could assist. it was observed that data curation is in need of better heuristics to inform management decisions. nasa did a study looking for data sets that had never been used, and they discovered that it was about 80 percent of the data they held. these results were somewhat skewed by the fact that nasa keeps multiple versions of some datasets (raw, processed, reprocessed), where the raw data may take up a considerable amount of disk space, but it is the processed versions that receive the majority of use. libraries largely operate on a model of collective action that is based on redundancy, whereas data archives tend to hold data that has no redundancy, and thus the archival paradigm may be more appropriate for modeling selection and appraisal decisions. 4. academic work and workforce in some ways, the larger question remains as to who is going to do this work of ensuring access and availability to research data. creating a data curation workforce is an open discussion in the information science world. education and challenges remain, but another aspect is funding the work that employs the practitioners. some of the session participants raised questions such as, should the work be done within the library? or, should the work be done using embedded digital curation team members? such workforce issues will be important to consider as we move into a data intensive paradigm of science. a professional class may need to be supported to make the data accessible, citable, and persistent. 5. challenges of establishing the value of academic data one of the participants noted that there are difficulties with establishing the value of data citation is that it is also related to valuation of the science possible with the data. it is considered more valuable if the data supports new science as opposed to incremental science. there can be a prejudice against reuse because it is not considered as captivating as doing new science. it was explained in terms of being worthy of a nobel prize: if the data reuse is not nobel worthy, then it is really hard to attract good scientists to it. for scientists who work with reusing data in this paradigm, they are considered to be giving up their careers. the nsf is starting to try to incentivize data reuse, as seen with the new funding opportunity from nsf for reusing certain types of data. the united kingdom may also see some movement on this front with some of the legislation pending in parliament. much of the discussion about building credit for data producers was driven from the perspective of the tenure and promotion process in academia. the role of providing credit and the system of rewards are both different for those outside the ﬁpublish or perishﬂ system of academia. it was pointed out that another major stakeholder in data citation is the data center. as an organization whose mission is to produce data, as opposed to a professor in an academic for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.breakout session institutional, financial, legal, and sociocultural issues 203  perspective, the situation is quite different. the data itself are the end product. the nasa earth observing system data and information system (eosdis) is an example of this. all of this is important for the advancement of science. it would be interesting to consider what fraction of the data we are discussing comes from different types of sources. there is a lot of interest in the ways that a given set of data is cited in the peer reviewed literature. for example, there is a study to look at the scholarly impact of one of the instruments on the nasa eos satellites. a challenge with that is that there are important uses of data outside of the scholarly world. one of the presentations earlier in this workshop demonstrated an example of a citation of a data set in a study by a nongovernmental organization of a proposed reforestation project. what is the value of that particular citation (which is not in the peerreviewed literature)? as the presentation by bruce wilson indicated, the ornl daac cares about citationsšpartly because it ties back to giving credit to the people who provide us the data. it also reflects back on the value of the ornl daac as a data center, providing the role of that cadre of people who are doing the curation, discussed in diane harley's talk. ornl needs those citations and use metrics as a means to (a) understand what data are being used and why; that is, how to lower the barriers to the use of the data; and (b) justify their budget. models of data citation some of the participants suggested that the current practices of citing data have yet to coalesce around best practices and standards, so there are outstanding questions about how data citation fits in with data sharing and data publishing. citation as a scholarly practice and the citation of data within it present a variety of models for best practices. as discussed above, data publication itself presents challenges, but it was seen by many participants as central to getting data citation off the ground. it was also noted that there was likely no single solution to these challenges. of central importance to data citation was the intention to build credibility for creators throughout the lifecycle, but there were also technological dependencies around cost and ease of use. the discussion was twofold. on the one hand were concerns about what information was necessary for citation purposes, on the other was the question of how to leverage data citation. within the discussion of data citation models and standards, much of the concern was about fulfilling the functions performed by data citation. the functions of citation were not explicitly enumerated, but among those discussed were tracking usage via citation metrics, transaction costs and overhead for tracking usage, and whether citation standards impact the cost of implementation. it was noted that there was some tension between repositories that leaned toward including more elements within the citation, enumerating responsible parties and agencies, and the publishers who preferred a shorter, simpler template with as few elements as possible. there is also the tension of academic institutions as employers, but also as providers of services to other institutions and persons. there was also a fair amount of discussion around compliance, norms, and how to impose a mechanism such as data citation. one way to do that would be to have an open standard that has agreed upon elements, but the journals want short citations and the repositories want longer ones to help attract funders. there was some question about what the minimum number of elements for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.204 developing data attribution and citaiton practices and standards would be to satisfy the stakeholders: publishers, institutions, crossref, datacite, data authors, and so on. there was also some discussion about whether a human readable name was necessary. there was some discussion about whether it would be feasible to embed metadata in the resolved page. html was the example cited, using a simple, weak, extensible protocol, such as a landing page with all the necessary credits. there was the concern that having the doi resolve the full metadata. however, there also was concern that this type of approach would be too brittle for the long term. 1. current approaches current approaches to data citation largely follow disciplinary practices. it was suggested that one approach would be to agree on the purpose of citation, track the mechanisms, and see how they work for different disciplines. there was some concern over the splintering of standards across disciplines with that kind of approach. it was also pointed out that some disciplines have functioning practices already in place and whatever is implemented should not force changes on that which already works. however, as we have seen in this workshop, there is datacite, which is interdisciplinary and largely a library organization. the question was raised of how is datacite going to expand and do what it wants to do? it was noted that they are in collaboration with crossref and reaching out to the publishing community. crossref is largely focusing on more traditional document type of publications and datacite is focusing on data. thomson reuters would like to start indexing datasets and including them in their web of science. much of this activity is focusing on the sciences (rather than the humanities). it was also noted that many of the data centers achieved buyin from publishers by using dois, as it leverages the reputations and workflows of these identifiers. it was observed that there is some tension in aligning needs within datacite as the uc3 and purdue partners are the only academic institutions, with the rest being national libraries. dryad, for example, makes its data available under a creative commons zero (cc0) license, which receives a fair amount of resistance from depositors. cc0, much like traditional citation practices, relies on norms of scholarship, rather than on legally binding contractual language. it was observed that cco does not naturally port very well to scholarship and data and presents the potential to yield unintended consequences. the american geophysical union (agu) requires its journal authors to cite data and open their data by placing them in a data center. they also limit the citing of datasets, stipulating that one cannot cite datasets that are not permanently archived, but rather such data must be acknowledged like a personal communication. in this case, the term ﬁciteﬂ is a term of art. the agu is not unique in this regard. there are a number of journals that follow this model because of the discoverability and access issues that nonarchived materials present. in these cases, citations are used for formal audited sources; acknowledgements are for less formal sources. it was observed that this kind of stratification of sources could serve as a selection process for institutional repositories ingesting materials. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.breakout session institutional, financial, legal, and sociocultural issues 205  2. identity, data structure, and provenance identity and provenance are known challenges to both data citation and data sharing. these issues were brought up by the participants under concerns about taking subsets of data and ensuring reproducibility. how does the user know if they are accessing the same data? these issues of reproducibility assume that the data are static, but they also assume a stable repository. there are many examples of researchers taking raw data and manipulating them to such an extent that it is questionable whether they should even be considered the same data. identifiers were seen as a central feature of discoverability and access. the costs of registering dois with crossref and datacite were discussed, as well as some of the indexing services that make use of the metadata. some participants brought up the model of mandatory copyright deposit for national libraries in europe, as at the u.s. library of congress as a possible model for data curation, because it allows users and institutions to request copies. however, the library of congress has already decided that data are, generally speaking, outside of their scope. within that model is the assumption that what is taken is kept in perpetuity, which is a huge economic issue. 3. costs the question of how to determine the value of data citation was pondered, getting into the incentives and benefits of having the data cited. the cost of data citation is complex. on the one side is the cost of labor of creating the citation and the cost of minting the identifier. on the other side is the cost saving in labor in discoverability. there is also the cost of doing nothing and having the data be very difficult to find and access. thomson reuters, ebsco, and others are watching databases with an eye towards being able to start indexing them in their services. the cost of the identifiers specifically and data curation more generally was difficult to assess, given the potential cost savings in creating an economy of scale around data curation and discovery for scientists. it was advocated that the cost of the infrastructure and human resources was relatively small compared to the benefits. however, that assertion would need to be quantified, with the question of how data are being used is still outstanding. some studies suggest that the investments in data curation pay for themselves. other costs discussed by the group included the wasted opportunity costs of reinventing the wheel and redoing the same research because researchers were unaware that the research had already been done, the cost of redundant studies that place people and animals at risk, the cost of toxic experiments such as nuclear experiments, many of which have been accomplished by resifting old data. leveraging adoption a number of participants observed that given the complexity of the situation, leverage may be needed to encourage adoption, to change mindsets, and to change what is valued. examples of this have included setting examples of good practices for the younger individuals, collective value and emphasis  "it starts in your lab" (we need posters). data citation has a relationship to the role of credit for different stakeholders. in creating a culture of making data available and citing them, one also has to create a culture of valuing the data such that they can be considered for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.206 developing data attribution and citaiton practices and standards relevant for tenure and promotion decisions. however, it is also about valuing and rewarding reuse, as well as enabling reproducibility. several participants remarked that behavior change was part of what was needed. diane harley™s presentation noted that this happens through changes in expectations. there is a substantial literature on social changes, including the literature of technology adoption. there were questions about what can be learned from the literature that is relevant to this particular discussion. does it tie back to the earlier comments that the science work itself is potentially the subject of future work on the history and development of science? later discussions also pondered the importance of the policy environment, policy authorship, and policy compliance in data citation. there was also some discussion of who was responsible for setting best practices, and domain specific versus institution specific policies and practices. it was noted that since publishing practices center around disciplinary communities, data citation policies will also need to define data citation practices in different disciplines. many venues mandate that researchers must cite the data that they use as the result of what researchers are obligated to do when they receive data. in some cases, usage licenses are being written. (see also sarah callaghan™s example mandating data citation.) the question remains as to whether institutions should try to control citation via usage licenses which can demonstrate impact for the data™s expense. on the one hand, institutions need to provide clear best practices for their researchers, but on the other hand, they also need to provide compliance with using thirdparty data. some questioned whether any of these approaches were realistic for faculty to adopt. finally, others asserted that it really depends on the datasets and the use. if the dataset is used, one should be able to cite it. if someone spends two years collecting data, however, it is generally accepted that they can use them exclusively and not share them for a period of time. 1. the importance of disciplinary norms mirroring other discussions, the importance of the disciplinary community was key to this discussion as well. some in the breakout group noted that the disciplinary community has the power to instruct their constituents to cite data. one observation indicated that the pattern of data sharing was of small communities coming together to share, then getting approval from the journal editors. consensus for how data are to be cited has to be built at the disciplinary level. it was postulated that the publishers need to abide by the culture of describing their datasets well with voluntary compliance to citation. the notion of credit is also important for data citation. it was observed that many researchers seem to have stories about data citation problems and receiving credit, but that these problems do not seem to get addressed. in this area, the normative aspects do not seem to be as normative. 2. funding mandates funding mandates for citation and access was another major discussion point, especially with the recent discourse concerning data management plans. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.breakout session institutional, financial, legal, and sociocultural issues 207  3. tracking use citation is important as a scholarly activity because it provides a way to follow usage for people who contribute data. citation is an incentive in that way. it was asserted that the need for a system goes far beyond a citation, however. there is currently little incentive to cite data because netting citations to data is not considered for most academic tenure and promotion decisions. usage data has become quite important in other areas of scholarship and leads to impacts beyond the initial usage data. it raises the question as to whether when you cite a dataset you become complicit in the future funding. there were examples from business schools, where they were charging for use and access to their datasets. one question, partially discussed above, is whether the data work and resulting citation will rely on goodwill and norms for compliance or licenses that carry specific obligations. it was asserted that citation should have optout mechanisms that are trackable. that way you can discipline noncompliance. it was also observed that there is some tension between licenses and norms, with licenses having the potential to yield significant unintended consequences. if we could say that norms would be dominant, then we could talk scientists out of licenses, but we are seeing more licenses rather than fewer. it was also observed that if data are in a standardized structure, then it becomes advantageous to archive, as we see with the american chemical society. unfortunately, citation is a very lagging indicator of use. ornl sees an average of 1824 months from when the data are downloaded until they see a citation in the literature. a related example of barriers includes work that the ornl daac did to make some data available via ogc web services. data download rates increased on the order of 100 times for some of those data sets by making them available by ogc web services. part of this effect was caused by advertising. they are now starting to see some of that increase in downloads show up in citation rates. it is not 100 times, but the increase may well be significant. the main concern for data centers is demonstrating use of their data; nasa archives have a senior review every 23 years, and if they cannot show that their data are being used for peerreviewed publications, their funding gets cut, and the data might go offline or to a less costly storage and management system. missions have similar reviews once past their primary mission schedule, and if people are not using the data, the mission is terminated. although they also track download volume, this can be a bad metric, as the network bandwidth rates are not keeping up. the data centers are making a concerted effort to save people's time by facilitating more targeted downloads of data (e.g., reduced or lower cadence data to identify the periods and locations of interest, then serve subsets of the data rather than the full dataset). 4. accountability and transparency as mentioned above, citation provides a mechanism for tracking use. conversely, it also is a way to establish that you have shared your data. it provides a mechanism for accountability and transparency. at the ipcc, the notion of accountability has come to the fore, derived from false accusations of impropriety, but is being used to develop better transparency. however, this sense of accountability and transparency is not entirely an incentive. there are also concerns about data being scooped or stolen, with junior faculty and postdoctoral for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.208 developing data attribution and citaiton practices and standards researchers being particularly vulnerable. one participant cited an example of a colleague in los angeles who was interviewed by the press, but was not able to open his data as of yet due to a pending publication in nature. he received a nasty editorial in the press for being a public employee and not sharing his data. 5. embargos and proprietary periods for data embargos, which are also referred to as proprietary periods of exclusive use in some fields, are generally seen as an important tool for protecting data, for protecting postdoctoral researchers and junior faculty, for protecting dissertation work until the derivative publications are finished, and generally for maintaining the primacy of researchers. conversely, data registries are generally not on the researchers™ radars. the long term ecological research network has done some work on this, for example. there is some view that having an embargo set up at the time of deposit, with a particular sunset date, is best. the idea is that the embargo should be a standard length of time (e.g., 2 years). the researcher can extend the embargo, but the embargo will automatically end without an express action. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.209 breakout session on institutional roles and perspectives moderator: bonnie carroll rapporteur: jillian wallis  several participants began by focusing on the stakeholders and lowlevel details about the interaction between the stakeholders and the data citations. others then raised several questions: who is cited: the data center hosting the data, the data producer, or anyone who has added value to the data? this is really a question of whether the citation is for assigning credit or finding data. it should be noted that there are many stakeholders who add value to the data and it may not be feasible to acknowledge everyone. who is responsible for generating a citation: the data center hosting the data, some collaboration between the producer and archivist, or the data user consulting with the data producer to create a citation? the credit aspects of citation thus may conflict with the location and discoverability aspects, which have very different sets of requirements. a number of the participants identified issues that pulled apart the roles of data citation stakeholders. who should be the citation creator: the data creator responsible for providing a citable thing, or the data user responsible for citing that thing? who is responsible for collecting metrics? this led to plotting out the events that happen during the life of a data citation and assigning responsible parties. figure s1 presents one understanding of how data citations will come to be. rather than being a representation of the lifecycle of an individual data citation, it instead depicts the life cycle of how data citation practices in general will be created. in this case, lifecycle is perhaps a misnomer, and instead what is captured in the figure is a timeline for organizing all of the interested parties. it is important to further define the data citation lifecycle and the roles and responsibilities of institutions and people who act at each stage, in order to determine who is missing from this discussion and how we can get them involved.  figure s1 data citation lifecycle. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.210 developing data attribution and citation practices and standards prior to the actual creation and adoption of data citations, several participants suggested, one option is to develop an understanding of the social ramifications of the data citation and the frameworks with which data citations would need to interact. this understanding could come from academic research on data practices. at the top level, research funders, universities, and journal publishers could think about developing a data citation policy that supports their respective needs and creates incentives to encourage data citation. using such a base of understanding and policy, many parties may wish to work in parallel to make data citation a reality. research communities can define the data citation elements that are meaningful to them. journal publishers and standards bodies can define general data citation layouts that are both machine and humanreadable. in order for a data citation to be created: (i) the data need to have been generated by someone, and (ii) the data need to be available with enough information attached in order to create the data citation. the data generator or the data center hosting the data will then make the actual citation content available. the data users are responsible for actually using the data citation in their publications. the derivative data cycle here refers to the practice of creating derivative datasets from other datasets. a new form of data citation could be developed in order to take this practice into account, and can involve some combination of the original data generators or hosts and the data users in a new data citation or a data citation that expands into multiple data citations. once the various standards are in play, several participants remarked that training and education would be useful about how and when data citations can be used. the university libraries are perhaps well positioned to reach out to the academic communities they support. finally, commercial parties can aggregate data citations, much like citations are aggregated to characterize scholarly communication in the literature. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.211 appendix a: agenda  developing data attribution and citation practices and standards an international symposium and workshop august 2223, 2011 us codata and the board on research data and information in collaboration with codataicsti task group on data citation standards and practices agenda day one œ monday, august 22 9:00 am i. chair™s welcoming remarks and keynote: why are the attribution and citation of scientific data important? christine borgman, university of california at los angeles  9:20 ii.a. what are the major technical issues that need to be considered in developing and implementing scientific data citation standards and practices? moderator: john wilbanks, creative commons 1. how attribution and citation relate or differ: jeanbernard minster, university of california at san diego, scripps institution of oceanography 2. attribution and credit: johan bollen, indiana university 3. persistence, identification, and the actionability of data citations: herbert van de sompel, los alamos national laboratory 4. authenticity, provenance, and trust  maintaining the scholarly value chain: paul groth, vu university amsterdam, netherlands  discussion for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.212  developing data attribution and citation practices and standards  10:50 break œ 30 min 11:20 ii.b. what are the major scientific issues that need to be considered in developing and implementing scientific data citation standards and practices? which ones are universal for all types of research and which ones are field or context specific?  moderator: herbert van de sompel, lanl 1. life sciences: philip bourne, university of california at san diego 2. physical and earth sciences: sarah callaghan, rutherford appleton laboratory, uk 3. social sciences: mary vardigan, university of michigan, interuniversity consortium for political and social research  4. humanities: michael sperbergmcqueen, black mesa technologies  discussion 12:50 lunch (70 min, on site) 2:00 iii. what are the major institutional, financial, legal, and sociocultural issues that need to be considered in developing and implementing scientific data citation standards and practices? which ones are universal for all types of research and which ones are field or contextspecific?  moderator: paul uhlir, national research council 1. legal issues: sarah hinchliff pearson, creative commons 2. institutional/financial: mackenzie smith, mit 3. sociocultural: diane harley, university of california at berkeley  discussion 3:15 coffee break œ 30 min  3:45 iv. what is the status of data attribution and citation practices in individual fields in the natural and social (economic and political) sciences in united states and internationally? case studies. for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.appendix a 213 moderator: david kochalko, thomsonreuters 1. datacite: jan brase, national library of science and technology, germany 2. dataverse: micah altman, harvard university 3. microsoft academic search: lee dirks, microsoft research 4. international oceanographic data exchange and the scientific committee for oceanographic research:  roy lowry et al. (presentation given by sarah callaghan) 5. global biodiversity information facility: vishwas chavan, gbif 6. federation of earth science information partners: mark parsons, national snow and ice data center 7. scripps institution of oceanography: john helly, scripps 8. sagecite: monica duke, university of bath, ukoln  discussion 5:30 adjourn  reception  for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.214  developing data attribution and citation practices and standards  day two œ tuesday, august 23 hotel shattuck plaza whitecotton room, sixth floor 2086 allston way berkeley, ca  8:45 v. institutional roles and perspectives: what are the respective roles and approaches of the main actors in the research enterprise and what are the similarities and differences in disciplines and countries? the roles of research funders, universities, data centers, libraries, scientific societies, and publishers will be explored.  moderator: bonnie carroll, information international associates 1. universities: deborah crawford, drexel university 2. data centers œ bruce wilson, oak ridge national laboratory 3. libraries: michael witt, purdue/iassist  4. commercial scientific publisher: anita de waard, elsevier labs 5. scientific society publisher: michael kurtz, harvardsmithsonian center for astrophysics, astrophysics data system  discussion 10:30 break (30 minutes)  11:00 session v. (continued) moderator: christine borgman, ucla 6. standards: todd carpenter, national information standards organization 7. public research funder: sylvia spengler, national science foundation  discussion and wrap up for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.appendix a 215 12:15 lunch (i hour) workshop œ options on where do we go from here?  whitecotton room, sixth floor moderator: allen renear, university of illinois at urbanachampaign 1:151:25 introduction and charge to breakout groups, allen renear, university of illinois at urbanachampaign 1:30 3:30 breakout groups  6 groups @ 79 persons each, with moderator and rapporteur (meeting rooms to be assigned) breakout 1: why is the attribution and citation of scientific data important and for what types of data? is there substantial variation among disciplines? chair: jan brase, tbi and datacite, germany rapporteur: cheryl levey, nrc board on research data and information room: boiler room, section a breakout 2: what are the major technical issues that need to be considered in developing and implementing scientific data citation standards and practices? chair: martie van deventer, council for scientific and industrial research, south africa rapporteur: franciel linares, information international associates room: boiler room, section b breakout 3: what are the major scientific issues that need to be considered in developing and implementing scientific data citation standards and practices? which ones are universal for all types of research and which ones are field or context specific? chair: sarah callaghan, rutherford appleton laboratory, uk rapporteur: matthew mayernik, national center for atmospheric research room: boiler room, section c breakout 4: what are the major institutional, financial, legal, and sociocultural issues that need to be considered in developing and implementing scientific data citation standards and practices? which ones are universal for all types of research and for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.216  developing data attribution and citation practices and standards  which ones are field or contextspecific? chair: vishwas chavan, global biodiversity information facility, denmark rapporteur: laura wynholds, ucla room: crystal ballroom, section 1 breakout 5: what are some of the options for the successful development and implementation of scientific data citation practices and standards, both across the natural and social sciences and in major contexts of research? how can the different stakeholder groups be engaged in such a process? chair: bonnie carroll, information international associates, us rapporteur: jillian wallis, ucla main room, side 1 breakout 6: what issues would be useful to get additional feedback on from the scientific community in order to identify best practices for data citation practices and standards? who should be asked? what is the best way to get this information? chair: todd carpenter, national information standards organization, usrapporteur: daniel cohen, library of congress/nrc board on research data and information main room, side 2 3:30 break 4:00 plenary discussion of best practices and options, and wrapup chair: allen renear, uiuc 5:00 end of meeting for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.217 appendix b: speaker and moderator biographical information speaker affiliation url for biographical information micah altman harvard university http://mit.academia.edu/micahaltman johan bollen indiana university http://informatics.indiana.edu/jbollen/home.html christine borgman university of california at los angeles http://polaris.gseis.ucla.edu/cborgman/chrisssite/welcome.html philip bourne university of california at san diego http://www.sdsc.edu/~bourne/  jan brase national library of science and technology, germany http://sites.nationalacademies.org/pga/brdi/pga064146  sarah callaghan rutherford appleton laboratory, uk http://sites.nationalacademies.org/pga/brdi/pga064138  todd carpenter national information standards organization http://www.niso.org/about/directory/staff bonnie carroll information international associates http://www.codata.org/codata02/bios/biocarroll.htm  vishwas chavan gbif http://vishwaschavan.in/ deborah crawford drexel university http://sites.nationalacademies.org/pga/brdi/pga064137  anita de waard elsevier labs http://sites.nationalacademies.org/pga/brdi/pga064139 lee dirks microsoft research http://sites.nationalacademies.org/pga/brdi/pga064136 monica duke university of bath, ukoln http://sites.nationalacademies.org/pga/brdi/pga064142  paul groth vu university amsterdam, netherlands http://www.few.vu.nl/~pgroth/site/welcome.html for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.218  developing data attribution and citation practices and standards   diane harley university of california at berkeley http://sites.nationalacademies.org/pga/brdi/pga064151  john helly scripps institution of oceanographic research http://www.sdsc.edu/profile/jhelly.html david kochalko thomson reuters http://www.stmassoc.org/people/davekochalko/ michael kurtz harvardsmithsonian center for astrophysics https://www.cfa.harvard.edu/~kurtz/ roy lowry plymouth university, uk http://www.plymouth.ac.uk/staff/rlowry jeanbernard minster university of california at san diego, scripps institution of oceanography http://www.sio.ucsd.edu/profile/jbminster mark parsons national snow and ice data center http://sites.nationalacademies.org/pga/brdi/pga064147 sarah hinchliff pearson creative commons http://sites.nationalacademies.org/pga/brdi/pga064152 allen renear university of illinois at urbanachampaign http://people.lis.illinois.edu/~renear/renearcv.html  mackenzie smith mit http://sites.nationalacademies.org/pga/brdi/pga064149 sylvia spengler national science foundation http://www.nsf.gov/staff/staffbio.jsp?lan=sspengle&org=cise  michael sperbergmcqueen black mesa technologies http://sites.nationalacademies.org/pga/brdi/pga064153 paul uhlir national research council http://sites.nationalacademies.org/pga/brdi/pga059692  for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved.appendix b 219 herbert van de sompel los alamos national laboratory http://public.lanl.gov/herbertv/home/ mary vardigan university of michigan http://www.icpsr.umich.edu/icpsrweb/shared/icpsr/staff/vardiganjohn wilbanks creative commons http://sciencecommons.org/about/whoweare/wilbanks/ bruce wilson oak ridge national laboratory http://sites.nationalacademies.org/pga/brdi/pga064145 michael witt purdue/iassist http://sites.nationalacademies.org/pga/brdi/pga064140  for attribution: developing data attribution and citation practices and standards: summary of an international workshopcopyright national academy of sciences. all rights reserved. 
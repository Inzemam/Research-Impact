detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/10324technical, business, and legal dimensions of protectingchildren from pornography on the internet: proceedings of aworkshop144 pages | 6 x 9 | paperbackisbn 9780309083263 | doi 10.17226/10324committee to study tools and strategies for protecting kids from pornography andtheir applicability to other inappropriate internet content, board on children,youth, and families, national research counciltechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.committee to study tools and strategies forprotecting kids from pornography and their applicability toother inappropriate internet contentcomputer science and telecommunications boarddivision on engineering and physical sciencesnational research councilboard on children, youth, and familiesdivision of behavioral and social sciences and educationnational research council and institute of medicinenational academy presswashington, d.c.andoffromon thetechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.national academy press ¥ 2101 constitution avenue, n.w. ¥ washington, dc 20418notice: the project that is the subject of this report was approved by the governing board of the national research council, whose members are drawn fromthe councils of the national academy of sciences, the national academy of engineering, and the institute of medicine. the members of the committee responsiblefor the report were chosen for their special competences and with regard for appropriate balance.the study of which this workshop report was a part was supported by grant no.1999jnfx0071 between the national academy of sciences and the u.s. departments of justice and education; grant no. p0073380 between the national academy of sciences and the w.k. kellogg foundation; awards (unnumbered) fromthe microsoft corporation and ibm; and national research council funds. anyopinions, findings, conclusions, or recommendations expressed in this publication are those of the author(s) and do not necessarily reflect the views of the organizations or agencies that provided support for this project. any opinions, findings, conclusions, or recommendations expressed in this material are those of thesymposium presenters and do not necessarily reflect the views of the sponsors.international standard book number 0309083265additional copies of this report are available from:national academy press2101 constitution avenue, n.w.box 285washington, dc 20055800/6246242202/3343313 (in the washington metropolitan area)copyright 2002 by the national academy of sciences. all rights reserved.printed in the united states of americatechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.the national academy of sciences is a private, nonprofit, selfperpetuating society of distinguished scholars engaged in scientific and engineering research, dedicated to the furtherance of science and technology and to their use for the generalwelfare. upon the authority of the charter granted to it by the congress in 1863,the academy has a mandate that requires it to advise the federal government onscientific and technical matters. dr. bruce m. alberts is president of the nationalacademy of sciences.the national academy of engineering was established in 1964, under the charterof the national academy of sciences, as a parallel organization of outstandingengineers. it is autonomous in its administration and in the selection of its members, sharing with the national academy of sciences the responsibility for advising the federal government. the national academy of engineering also sponsorsengineering programs aimed at meeting national needs, encourages education andresearch, and recognizes the superior achievements of engineers. dr. wm. a.wulf is president of the national academy of engineering.the institute of medicine was established in 1970 by the national academy ofsciences to secure the services of eminent members of appropriate professions inthe examination of policy matters pertaining to the health of the public. the institute acts under the responsibility given to the national academy of sciences by itscongressional charter to be an adviser to the federal government and, upon itsown initiative, to identify issues of medical care, research, and education.dr. kenneth i. shine is president of the institute of medicine.the national research council was organized by the national academy of sciences in 1916 to associate the broad community of science and technology withthe academyõs purposes of furthering knowledge and advising the federal government. functioning in accordance with general policies determined by theacademy, the council has become the principal operating agency of both the national academy of sciences and the national academy of engineering in providing services to the government, the public, and the scientific and engineering communities. the council is administered jointly by both academies and the instituteof medicine. dr. bruce m. alberts and dr. wm. a. wulf are chairman and vicechairman, respectively, of the national research council.national academy of sciencesnational academy of engineeringinstitute of medicinenational research counciltechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.committee to study tools and strategies forprotecting kids from pornography andtheir applicability to other inappropriateinternet contentrichard thornburgh, kirkpatrick & lockhart llp, chairnicholas j. belkin, rutgers universitywilliam j. byron, holy trinity parishsandra l. calvert, georgetown universitydavid forsyth, university of california at berkeleydaniel geer, @stakelinda hodge, parent teacher associationmarilyn gell mason, independent consultantmilo medin, excite@homejohn b. rabun, national center for missing and exploited childrenrobin raskin, familypc magazinerobert schloss, ibm t.j. watson research centerjanet ward schofield, university of pittsburghgeoffrey r. stone, university of chicagowinifred b. wechsler, independent consultantstaffherbert s. lin, senior scientist and study directorgail pritchard, program officer (through june 2001)laura ost, consultantjoah g. ianotta, research assistantjanice sabuda, senior project assistantdaniel d. llata, senior project assistant (through may 2001)ivtechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.computer science and telecommunications boarddavid d. clark, massachusetts institute of technology, chairdavid borth, motorola labsjames chiddix, aol time warnerjohn m. cioffi, stanford universityelaine cohen, university of utahw. bruce croft, university of massachusetts at amherstthomas e. darcie, at&t labs researchjoseph farrell, university of california at berkeleyjeffrey m. jaffe, bell laboratories, lucent technologiesanna karlin, university of washingtonbutler w. lampson, microsoft corporationedward d. lazowska, university of washingtondavid liddle, u.s. venture partnerstom m. mitchell, carnegie mellon universitydonald norman, nielsen norman groupdavid a. patterson, university of california at berkeleyhenry (hank) perritt, illinois institute of technologyburton smith, cray inc.terry smith, university of california at santa barbaralee sproull, new york universityjeannette m. wing, carnegie mellon universitystaffmarjory s. blumenthal, directorherbert s. lin, senior scientistalan s. inouye, senior program officerjon eisenberg, senior program officerlynette i. millett, program officercynthia patterson, program officersteven woo, program officerjanet briscoe, administrative officerdavid padgham, research associatemargaret huynh, senior project assistantdavid drake, senior project assistantjanice sabuda, senior project assistantjennifer bishop, senior project assistantbrandye williams, staff assistantvtechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.board on children, youth, and familiesevan charney, university of massachusetts medical school, chairjames a. banks, university of washingtondonald cohen, yale universitythomas dewitt, childrenõs hospital medical center of cincinnatimary jane england, washington business group on healthmindy fullilove, columbia universitypatricia greenfield, university of california at los angelesruth t. gross, stanford universitykevin grumbach, university of california at san francisco, sanfrancisco general hospitalneal halfon, university of california at los angeles school ofpublic healthmaxine hayes, washington state department of healthmargaret heagarty, columbia universityren…e r. jenkins, howard universityharriet kitzman, university of rochestersanders korenman, baruch college, city university of new yorkhon. cindy lederman, juvenile justice center, dade county,floridavonnie mcloyd, university of michigangary sandefur, university of wisconsinmadisonelizabeth spelke, massachusetts institute of technologyruth stein, montefiore medical centerliaisonseleanor e. maccoby (liaison, division of behavioral and socialsciences and education), department of psychology (emeritus),stanford universitywilliam roper (liaison, iom council), institute of medicine,university of north carolina, chapel hillstaffmichele d. kipke, director (through september 2001)mary graham, associate director, dissemination andcommunicationssonja wolfe, administrative associateelena nightingale, scholarinresidencejoah g. iannotta, research assistantvitechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.prefacein response to a mandate from congress in conjunction with the protection of children from sexual predators act of 1998, the computer science and telecommunications board (cstb) and the board on children,youth, and families of the national research council (nrc) and the institute of medicine established the committee to study tools and strategies for protecting kids from pornography and their applicability toother inappropriate internet content.to collect input and to disseminate useful information to the nationon this question, the committee held two public workshops. on december 13, 2000, in washington, d.c., the committee convened a workshop tofocus on nontechnical strategies that could be effective in a broad range ofsettings (e.g., home, school, libraries) in which young people might beonline. this workshop brought together researchers, educators, policymakers, and other key stakeholders to consider and discuss these approaches and to identify some of the benefits and limitations of variousnontechnical strategies. the december workshop is summarized in nontechnical strategies to reduce childrenõs exposure to inappropriate material onthe internet: summary of a workshop.11national research council and institute of medicine, nontechnical strategies to reducechildrenõs exposure to inappropriate material on the internet: summary of a workshop, computerscience and telecommunications board and board on children, youth, and families, joahg. iannotta, ed., washington, d.c.: national academy press, 2001.viitechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.viiiprefacethe second workshop was held on march 7, 2001, in redwood city,california. this second workshop focused on some of the technical, business, and legal factors that affect how one might choose to protect kidsfrom pornography on the internet. the present report provides, in theform of edited transcripts, the presentations at that workshop. obviously,because the report reflects the presentations on that day, it is not intendedto be a comprehensive review of all of the technical, business, and legalissues that might be relevant to this subject. all views expressed in thisreport are those of the speaker (who sometimes is a member of the studycommittee speaking for himself or herself). most importantly, this reportshould not be construed as representing the views of the committee tostudy tools and strategies for protecting kids from pornography andtheir applicability to other inappropriate internet content; the computer science and telecommunications board; the board on children,youth, and families; the national research council; or the institute ofmedicine.the report contains 17 chapters, each of which is essentially an editedtranscript of the various briefings to the committee during the workshop.questions and comments from the audience and committee members areincluded as footnotes. the first four chapters are devoted to the basics ofinformation retrieval and searching. the next three (chapters 57) address some of the technology and business dimensions of filtering, theprocess through which certain types of putatively objectionable contentare blocked from display on a userõs screen. two chapters (chapters 89)then address technical and infrastructural dimensions of authenticationñthe process of proving that one is who one asserts to be. the next threechapters (chapters 1012) address automated approaches to negotiatingindividualized policy preferences and dealing with issues of intellectualproperty (and preventing unauthorized parties from viewing protectedmaterial). chapter 13 addresses the problems associated with a dotxxxdomain for òcordoning offó sexually explicit material on the internet.chapters 1416 cover various issues associated with business models forthe internet, and the final chapter, chapter 17, discusses one legal scholarõsperspective on regulating sexually explicit material on the internet.gail pritchard was largely responsible for assembling the speakers atthis workshop, and laura ost generated the first draft of the report.this report was reviewed in draft form by individuals chosen for theirdiverse perspectives and technical expertise, in accordance with procedures approved by the nrcõs report review committee. the purpose ofthis independent review is to provide candid and critical comments thatwill assist the institution in making the published report as sound as possible and to ensure that the report meets institutional standards for objectivity, evidence, and responsiveness to the study charge. the review comtechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.prefaceixments and draft manuscript remain confidential to protect the integrity ofthe deliberative process.we thank the following individuals for their participation in the review of these workshop proceedings:william aspray, computing research association,hinrich schtze, novation biosciences, andfrederick weingarten, american library association.although these individuals reviewed the report, they were not askedto endorse it, nor did they see the final draft of the report before its release. the review of this report was overseen by peter blair of the divisionon engineering and physical sciences. appointed by the national research council, he was responsible for making certain that an independent examination of this report was carried out in accordance with institutional procedures and that all review comments were carefullyconsidered. responsibility for the final content of this report rests entirelywith the authoring committee and the institution.herbert s. lin, senior scientist and study directorcomputer science and telecommunications boardtechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.contents1basic concepts in information retrieval1nicholas belkin1.1definitions and system design, 11.2problems, 22text categorization and analysis5david lewis and hinrich schtze2.1text categorization, 52.2advanced text technology, 73categorization of images11david forsyth3.1challenges in object recognition, 113.2screening of pornographic images, 123.3the future, 144the technology of search engines16ray larson4.1overview, 164.2boolean search logic, 174.3the vector space model, 184.4searching the world wide web, 19xitechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.xiicontents5cyber patrol: a major filtering product23susan getgood5.1introduction, 235.2why filter?, 245.3superscout and cyber patrol, 255.4the review process, 295.5the future, 316advanced techniques for automaticweb filtering33michel bilello6.1background, 336.2the wipe system, 347a critique of filtering36bennett haselton7.1introduction, 367.2deficiencies in filtering programs, 377.3experiments by peacefire.org, 387.4circumvention of blocking software, 458authentication technologies48eddie zeitler8.1the process of identification, 488.2challenges and solutions, 509infrastructure for age verification53fred cotton9.1the real world versus the internet, 539.2solutions, 569.3the extent of the problem, 5910automated policy preference negotiation62deirdre mulligan11digital rights management technology65john blumenthal11.1technology and policy constraints, 6511.2designing a solution to fit the constraints, 6711.3protecting children, 7311.4summary, 75technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.contentsxiii12a trusted third party in digital rightsmanagement76david maher12.1intertrust technologies, 7712.2countermeasures and hackers, 8012.3summary, 8413problems with a dotxxx domain85donald eastlake14business dimensions: the education market90irv shapiro14.1the role of teachers, 9014.2historical perspective, 9114.3the school marketplace, 9215business models: kidfriendly internetbusinesses96brian pass15.1building an internet business, 9615.2comparing business models, 9815.3the role of parents, 10316business models based on advertising104chris kelly16.1comparison of advertising models, 10416.2portals, advertising networks, and targeting, 10516.3choice of models, 10616.4advertising, regulation, and kids, 10717constitutional law and the law ofcyberspace110larry lessig17.1introduction, 11017.2regulation in cyberspace, 11117.3possible solutions, 11217.4practical considerations, 118appendix: biographies of presenters124technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.11basic concepts in information retrievalnicholas belkin1.1definitions and system designinformation retrieval and information filtering are different functions.information retrieval is intended to support people who are actively seeking or searching for information, as in internet searching. informationretrieval typically assumes a static or relatively static database againstwhich people search. search engine companies construct these databasesby sending out òspidersó and then indexing the web pages they find. bycontrast, information filtering supports people in the passive monitoringfor desired information. it is typically understood to be concerned withan active incoming stream of information objects.the problem in information retrieval and information filtering is thatdecisions must be made for every document or information object regarding whether or not to show it to the person who is retrieving the information. initially, a profile describing the userõs information needs is set upto facilitate such decision making; this profile may be modified over thelong term through the use of user models. these models are based on apersonõs behaviorñdecisions, reading behaviors, and so on, which maychange the original profile. both information retrieval and informationfiltering attempt to maximize the good material that a person sees (thatwhich is likely to be appropriate to the information problem at hand) andminimize the bad material.when people refer to filtering, they often really mean information retrieval. that is, they are not concerned with dynamic streams of documents but rather with databases that are already constructed and in whichtechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.2basic concepts in information retrievalthere is some way to represent the information objects and relate them toone another. thus, filtering corresponds to the boolean filter in information retrieval: a yes/no decision.most search engines designed for the world wide web use the principle of òbest match,ó that is, not making yes/no decisions but, rather,ranking information objects with respect to some representation of theinformation problem. thus, the basic processes in information retrievalor information filtering are the representations of information objects andof information needs, or more generally, the problem or goal that the person has in mind. the retrieval techniques themselves then compare needswith objects.the interaction of the user with other components of the system isimportant. in fact, the prevailing view in information retrieval research isthat the most effective approach for helping a user obtain the appropriateinformation is relevance feedback, in which the system takes into accountwhether a person likes or dislikes a document as it automatically rerepresents the userõs query. this leads to performance improvements of asmuch as 150 percentñmuch better than any other technique. thus, thepersonõs judgment of the information objects is an important part of theprocess. the user is an actor in the information retrieval system, becausemany of the processes depend on his or her expression and interpretationof the need. the relevance of a document cannot be determined unlessthe person is considered a part of the system.the second important part of the system is the information resource,a collection of information objects that has been selected, organized, andrepresented according to some schema. the third component is the intermediaryña device or person that mediates between the information resource and the user and that has knowledge of the user, the userõs problem, and the types of users that exist, as well as the information resource,the way the resource is organized, what it contains, and so on. the intermediary supports the interaction between people and the informationobjects and knowledge resource, through prediction and other means.1.2problemsthe representation of information problems is inherently uncertain,because people look for that which they do not know, and it is probablyinappropriate to ask them to specify what they do not know. the representation of information objects requires interpretations by a human indexer, machine algorithm, or other entity. the problem is that anyoneõsinterpretation of a particular text is likely to be different from anyoneelseõs, and even different for the same person at different times. as ourstate of knowledge or problems change, our understanding of a texttechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.nicholas belkin3changes. everyone has experienced the situation of finding a documentnot relevant at some point but highly relevant later on, perhaps for a different problem or perhaps because we, ourselves, are different. the easiest and most effective way to deal with this problem is to support usersõinteractions with information objects and let them take control.because of these uncertainties, the comparison of needs and information objects, or retrieval process, is also inherently uncertain and probabilistic. the understanding of information objects is subjective, and, therefore, representation is necessarily inconsistent. we do not know how wellwe are representing either the personõs need or the information object.an extensive literature on interindexer consistency shows that whenpeople are asked to represent an information object, even if they are highlytrained in using the same metalanguage (indexing language), they mightachieve as much as only 60 to 70 percent consistency in tasks such as assigning descriptors. we will never achieve òidealó information retrievalñthat is, all the relevant documents and only the relevant documents, orprecisely that one thing that a person wants.the implication is that we must think of probabilistic ways of representing information problems. even if computers were as smart as people,they probably could not do the job. a standard information retrieval result is that automatic indexingñin which algorithms do statistical wordcounting and indexingñleads to performance that is no worse, and oftenbetter, than systems in which people do manual indexing.there is no reason to suppose that people will do a better job thanmachines, and neither one will do a perfect job, ever. making absolutepredictions in an inherently probabilistic environment is not a good idea.algorithms for representing information objects, or information problems, do give consistent representations. but they give one interpretationof the text, out of a great variety of possible representations, dependingon the interpreter. language is ambiguous in many ways: polysemy,synonymity, and so on. for example, a bank can be either a financialinstitution or something on the side of a river (polysemy). the contextmatters a lot in the interpretation.the metalanguage used to describe information objects, or linguisticobjects, often is construed to be exactly the same as the textual languageitself. but they are not the same. the similarity of the two languages hasled to some confusion. in information retrieval, it has led to the idea thatthe words in the text represent the important concepts and, therefore, canbe used to represent what the text is about. the confusion extends toimage retrieval, because images can be ambiguous in at least as manyways as can language. furthermore, there is no universal metalanguagefor describing images. people who are interested in images for advertistechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.4basic concepts in information retrievaling purposes have different ways to talk and think about them than do arthistorians, even though they may be searching for the same images. thelack of a common metalanguage for images means that we need to thinkof special terms for images in special circumstances.in attempting to prevent children from getting harmful material, it ispossible to make approximations and give helpful direction. but in theend, that is the most that we can hope for. it is not a question of preventing someone from getting inappropriate material but, rather, of supporting the person in not getting it. at least part of the public policy concernis kids who are actively trying to get pornography, and it is unreasonableto suppose that information retrieval techniques will be useful in achieving the goal of preventing them from doing so.there are a variety of users. the user might be a concerned parent ormanager who suspects that something bad is going on. but mistakes areinevitable, and we need to figure out some way to deal with that. it isdifficult to tell what anything means, and usually we get it wrong. generally we want to design the tools so that getting it wrong is not as much ofa nuisance as it otherwise might be.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.52text categorization and analysis2.1text categorizationautomatic text categorization is the primary language retrieval technology in content filtering for children. text categorization is the sortingof text into groups, such as pornography, hate speech, violence, and unobjectionable content. a text categorizer looks at a web page and decidesinto which of these groups a piece of text should fall. applications of textcategorization include filtering of email, chat, or web access; text indexing; and data mining.why is content filtering a categorization task? one way to frame theproblem is to say that the categories are actions, such as òallow,ó òallowbut warn,ó or òblock.ó we either want to allow access to a web page,allow access but also give a warning, or block access. another way toframe the problem is to say that the categories are different types of content, such as news, sex education, pornography, or home pages. depending on which category we put the page in, we will take different actions.for example, we want to block pornography and give access to news.the automation of text categorization requires some input frompeople. the idea is to mimic what people do. two parts of the task needto be automated. one is the categorization decision itself. the categorization decision says, for example, what we should do with a web page. thesecond part to be automated is rule creation. we want to determine automatically the rules to apply.automation of the categorization decision requires a piece of software that applies rules to text. this is the best architecture because thentechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.6text categorization and analysiswe can change the behavior by changing the rules rather than rewritingthe software every time. this automatic categorizer applies two types ofrules. one type is extensional rules that explicitly list all sites that cannotbe accessed (i.e., òblacklistedó sites) or, alternatively, all sites that can beaccessed (e.g., kidsafe zones or òwhitelistedó sites). the second type,which is technically more complicated, is intentional rules or keywordblocking. we look at the content of the page, and, if certain words occur,then we take certain actions, such as blocking access to that page. it canbe more complicated than just a single word. for example, it can be logicbased, where we use and and or operators, or it can be a weightedcombination of different types of words.automated rule writing is called supervised learning. one or morepersons are needed to provide samples of the types of decisions we wishto make. for example, we could ask a librarian to identify which of 500texts or web pages are pornography and which ones are not. this provides a training set of 500 sample decisions to be mimicked. the rulewriting software attempts to produce rules that mimic those categorization decisions. the goal is to mimic the categorization decisions made bypeople. the selection of the persons who provide the samples is fundamental, because whatever they do becomes the gold standard, which themachine tries to mimic. everything depends on the particular personsand their judgments.research shows that supervised learning is at least as good as experthuman rule writing. (supervised learning is also very flexible. for example, foreign content is not a problem, as long as the content involvestext rather than images.) the effectiveness of these methods is far fromperfectñthere is always some error rateñbut sometimes it is near agreement with human performance levels. still, the results differ from category to category, and it is not clear how directly it applies to, for example, pornography. as discussed in the next presentation, there is aninevitable tradeoff between false positives and false negatives, and categories vary widely in difficulty. substantially improved methods arenot expected in the next 10 to 20 years.it is not clear which text categorization techniques are most effective.some recently developed techniques are not yet used commercially, sothere may be incremental improvements. nor is it clear how effectivesemiautomated categorization is, or whether the categories that are difficult for automated methods are the same as those that perplex people.with regard to spam email, it is possible to circumvent it, but there is nofoolproof way to filter it. the question is whether the error rate is acceptable.this all comes back to community standards. we can train the classitechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.david lewis and hinrich schƒtze7fier to predict the probability that a person would find an item inappropriate, and training can give equal weight to any number of communityvolunteers. in other words, we can build a machine that mimics a community standard. we take some people out of the community, get theirjudgments about what they find objectionable or not, and then build amachine that creates rules that mimic that behavior. but this does notsolve the political question of how to define the community, who to selectas representatives of that community, and where in that community toapply the filter. the technological capability does not solve the application issues in practice.2.2advanced text technologytrue text understanding will not happen for at least 20 or 30 years,and maybe never. therein lies the problem, because to filter content withabsolute accuracy we would need text understanding. as a result, therewill always be an error rate; the question is how high it is.the text categorization methods discussed above use the òbagofwordsó model. this is a simplistic machine representation of text. it takesall the words on a page and treats them as an unstructured list. if the textis òdick armey chooses bob shaffer to lead committee,ó then a representative list would be: armey, bob, chooses, committee, dick, lead, shaffer.the structure and context of the text is completely lost. this impoverished representation is the basis of text classification methods in existingcontent filters.there are problems with this type of representation. it fails, in manycases, because of ambiguous words. the context is important. ambiguous words such as òbeaveró have both a hunterõs meaning and a graphicmeaning. using the bagofwords model alone, you cannot tell whichmeaning is relevant. the bagofwords model is inherently problematicfor these types of ambiguous words. other words, such as òbreastó andòblow,ó are not ambiguous but can be used pornographically. again, ifwe use a bagofwords model, then we lose context and cannot deal withthese words properly. when context counts, the bagofwords model fails.the problem cannot be resolved fully by looking for adjacent words,as search engines do when they give higher weight to information objectsthat match the query and have certain words in the same sentence. thereis a distinction between search engines and classification. search enginescompute a ranking of pages. the end users look at the top 10 or maybethe top 100 ranked pages. because they are looking only at pages in whichthe signal is strongest and because they are making a relative judgment,this type of methodology works very well; the highestrated pages aretechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.8text categorization and analysisprobably very relevant to the query.1 but in classification, we have tomake a decision about one page by itself. this is a much more difficultproblem. by looking at the words that lie nearby, we cannot always makea decent statistical guess as to whether a situation is innocuous or not.when context is important, when the bagofwords model fails, pornography filters and content filters make errors. howeverñsurprisinglyñthe bagofwords model is effective in many applications, so it isnot a hopeless basis for pornography filters despite its error rate. it always comes down to what error rate is acceptable.2 to go beyond thebagofwords model, a number of technologies are currently available:morphological analysis, partofspeech tagging, translation, disambiguation, genre analysis, information extraction, syntactic analysis, and parsing. even using these technologies, thorough text understanding will remain in the distant future; a 100percentaccurate categorization decisioncannot be made today. but these advanced text technologies can increasethe accuracy of content filters, and this increased accuracy may be significant in some areas.the first area relates to overbroad filters that block material thatshould not be blocked, raising free speech issues. it is relatively easy tobuild an overbroad filter, which blocks pornography very well but alsoblocks a lot of good content, like dick armeyõs home page. these overbroad filters may suffice in many circumstances. for example, there maybe parents who would say, òas long as not a single pornographic pagecomes through, or it almost never happens, it is ok if my child cannot seea lot of good content.ó but these overbroad filters are problematic inmany other settings, such as in libraries, where there is an issue of freespeech. if a lot of good content is blocked, then that is problematic. advanced technology can really make a difference, because by increasingthe accuracy of the filter, less good content would be blocked.1milo medin said that various search engine companies have come with a number of techniques to filter adult content, so that you have to turn on the capability to see certain typesof references. most of it is ranking based, but there are some other obvious things as well.part of the challenge is that many adult sites are trying to get people to visit, so they fill theirheaders with all kinds of information that make it obvious what is going on. the question is,how practical is that?2milo medin said that the people who run search engines have an economic interest inmaking their results as accurate as possible, to satisfy their subscribers. normal large searchengines want the adultcontent filter to be as accurate as possible. if the filter is turned on,we basically want to eliminate adult content. the google folks, as an example, have devoteda lot of energy to these issues, but it is not aimed directly at pornography. they focus on abroader set of issues to which pornography is a business input.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.david lewis and hinrich schƒtze9the second area is pornography versus other objectionable content,such as violence and hate speech. the bagofwords model is most successful under two conditions: (1) when there are unambiguous words indicating relevant content and (2) when there are a few of these indicators.pornography has these properties; probably about 40 or 50 words, most ofthem unambiguous, indicate pornography. thus, the bagofwords modelis actually not so bad for this application, especially if you like overbroadfilters. however, in many other areas, such as violence and hate speech,the bagofwords model is less effective. often you must read four or fivesentences of a text before identifying it as hate speech. accuracy becomesimportant in such applications, and advanced technology can be helpfulhere.the third area is automated blacklisting. remember the distinctionbetween extensional and intentional rules; extensional rules are lists ofsites that you want to block. this is an effective contentfiltering technique, mostly driven by human editors now. this is a promising area forautomation. accuracy is important because blocking one site can blockthousands of pages; you want to be sure of doing the right thing. advanced text technology also can play a role here.a potential problem with these text technologies is their lack of robustness. they can be circumvented through changes in meaning. if apornographer wants to get through a filter that he knows and can test,then he or she will be able to get through itñit is simply a question ofeffort. but pornographers are not economically motivated to expend a lotof effort to get through these filters. i may be wrong, but my sense is that,because children do not pay for pornography, this is probably not a problem.in summary, true machineaided text understanding will not be available in the near term, and that means there always will be a significanterror rate with any automated method. the advanced text technologiesimprove accuracy, which may be important in contexts such as free speechin libraries, identification of violence and hate speech, and automatedblacklisting.the extent of the improvement from these technologies depends onmany parameters, and tests must be run.3 the latest numbers i know ofare from consumer reports,4 but they are aggregated and not broken down3milo medin said that it is difficult to do good experiments and that sloppy experimentation is rewarded in a strange way. first, you run a very large collection of text through yourfilter and determine how much of the material identified as pornographic was, in fact, not.second, you find out how much of the material identified as not pornographic was, in fact,a problem. if you do that analysis badly or carelessly, your filter looks better.4consumer reports, march 2001.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.10text categorization and analysisby area. there is probably a big difference in accuracy between pornography and the other objectionable areas. there is also a tradeoff betweenfalse positives and false negatives. the extent to which advanced techniques make a difference depends on where in the tradeoff you start out.if i had to give a number, i would expect a 20 to 30 percent improvementin accuracy over the bagofwords modelñif you want to let all good content through (if you do not want overblocking).technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.113categorization of imagesdavid forsyth3.1challenges in object recognitionthe process of determining whether a picture is pornographic involves object recognition, which is difficult for a lot of reasons. first, it isdifficult to know what an object is; things look different from differentangles and in different lights. when color and texture change, things lookdifferent. people can change their appearance by moving their headsaround. we do not look different to one another when we do this, but wecertainly look different in pictures.the state of the art in object recognition is finding buildings in pictures taken from satellites. computer programs sometimes can findpeople. we are good at finding faces. we can tellñsort ofñwhether apicture has nearly naked people in it. but there is no program that reliably determines whether there are people wearing clothing in a picture.the main way to look for people with clothes is to look for the ones without clothes. it is a remarkable fact of nature that virtually everyoneõs skinlooks about the same in a picture (even across different racial groups), aslong as we are careful about intensity issues. skin is easy to detect reliably in pictures, so the first thing we look for is skin. but we need torealize that photographs of the california desert, apple pies, and all sortsof other things also have a blank color. therefore, we need a pattern forhow skin is arranged.long, thin bits of skin might be an arm, leg, or torso. because thekinematics of the body is limited, certain things cannot be done with armsand legs. if i find an arm, for example, then i know where to look for atechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.12categorization of imagesleg. if i put enough of them together, then there is a person in the picture.if there is a person and there is skin, then they have no clothes on, andthere is a problem. we could reason about the arrangement of skin, or wecould simply say that any big blob of skin must be a naked person. wedid a classification based on kinematics.performance assessment is complicated. there are two things to consider: first, the probability that the program will say a picture is rude whenit is not (i.e., false positive) and, second, the probability that the programwill say a picture is not rude when it is (i.e., false negative). although it isdesirable to try to make both numbers as small as possible, the appropriate tradeoff between false positives and false negatives depends on theapplication, as described below. moreover, false positive and false negative rates can be measured in different ways. doing the experiments canbe embarrassing because a lot of pictures need to be handled and viewed,and all sorts of other things make it tricky as well. the experiments aredifficult to assess because they all use different sets of data. people usually report the experiments that display their work in a good light. inview of these phenomena, it is not easy to say what would happen if wedropped one of these programs on the web.3.2screening of pornographic imagesone way to reduce viewing of pornographic images is intimidation.a manager or parent might say to employees or children that internettraffic will be monitored. they might explain that the image categorization program will store every image it is worried about in a folder and,once a week, the folder will be opened and the contents displayed. if theimages are problematic, the manager or parent will have a conversationwith the employee or child. this approach might work, because whenpeople are warned about monitoring, they may not behave in a silly way.but it will work only if there is a low probability of false positives. noone will pay attention to monitoring if each week 1,500 òpornographicópictures are discovered in the folder, all being pictures of apple pies thatthe program has misinterpreted. the security industry usually says thatpeople faced with many false positives get bored and do not want to dealwith the problem.1 on the other hand, a high rate of false negatives is nota concern in this context. typically, in a monitoring application, letting1milo medin noted that the internal revenue service (irs) uses the intimidation approach.in the tax context, many false positives may not be a problem. certain behaviors cause theirs to expend a lot of energy to respond. if the consequences of an investigation are highenough, then the irs needs to do it only a few times to generate certain behaviors.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.david forsyth13one or two pictures sneak in is not a problem. if there is a high falsenegative rate, then we will get a warning. we might not see every one,but we will know there is an issue.another approach is to render every picture coming through a network. we could fill a building with banks of people looking at all thepictures and saying, òi donõt like this one.ó this is not practical. wecould take a òno porn shall passó attitude, but then we really care whetherthe possibility of a false negative is small, and there is a risk that we mightnot know what is being left out. large chunks of information might beruled as objectionable by the program without, in fact, being objectionable, and we would not know about it.yet another approach is site classification. we could look at a seriesof pictures from one site, and if our program thinks that enough of themare rude, then we could say that the whole site is rude. we need to becareful about such rules, however, because of a conditional probabilityissue, as discussed below.a program that i wrote with ida fleck marks about 40 percent of pornographic pictures, where a pornographic picture is an image that can bedownloaded from an adultoriented site. this program thinks picturesare pornographic if they contain lots of stuff that looks like skin that is inlong bits and in a certain arrangement. a picture that appears to have lotsof skin but in the wrong arrangement is not judged to be pornographic.pictures with little skin showing are not identified as pornographic. butpictures of things like deserts, cabins, the colorado plateau, cuisine, barbecue, salads, fruit, and the colors of autumn are sometimes identified aspornographic. spatial analysis is difficult and is done poorly. the program often identifies pies as torsos. but the program is not completelyworthlessñit does find some naughty pictures. sometimes the colors arenot adjusted correctly, so that the skin does not look like skin, but thebackground does. but this seldom happens because it makes people lookeither seasick or dead; usually, the people who scan the film adjust thecolors.this brings up the conditional probability issue. this program isslightly better at identifying pictures of puddings than it is at detectingpictures of naked people, because an apple tart looks like skin arranged inlines and strips. generally, if a web page contains pictures of puddings,then the program says each picture is a problem and, therefore, the webpage is a problem. this is a common conditional probability issue thatarises in different ways with different programs. there is no reason tobelieve that computer vision technology will eliminate it.mike jones and jim ray did some work on skin detectors. when theyfound skin, they looked for a big skin blob and, if it was big enough, theytechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.14categorization of imagessaid the picture was a problem. the program cannot tell if a person iswearing a little bathing costume or if the skin belongs to a dog instead ofa human. they plotted the probability of a false positive against the probability of detection. if you wanted only a 4 percent probability of a falsepositive, for example, then you would mark about 70 percent of pornographic pictures. i am not sure whether they used as many pictures ofpuddings or the colorado desert in their experiments as i did. densityalso affects the results; doing these experiments right is not easy. theyanalyzed text as well as images. i think they used a simple bagofwordsmodel with perhaps some conditional probability function. to markabout 90 percent of the pornographic pictures, you would get about8 percent false positives, which might be a very serious issue. unless youare in the business of finding out who is looking at rude pictures, then 8percent false alarms would be completely unacceptable.several things make it easier to identify pornography than you mightthink. first, people tend to be big in these pictures because there is notmuch else. there are also wild correlations among words, pictures, andlinks. most porn web sites are linked to most others. what you thinkabout a picture should change based on where you came from on theweb.filtering, or at least auditing, can be done in close to real time. acanadian product called porn sweeper audits in close enough to real timethat the producers claim that someone transmitting or receiving largenumbers of these pictures will get a knock on the door within the next dayor so, rather than the next month. but this is not fast enough to meeteveryoneõs needs.3.3the futureface detection is becoming feasible. the best systems recognize 90percent of faces with about 5 percent false positives. this is good performance and getting much better.2 in 3 to 5 years, the computer visioncommunity will have many good facedetection methods. this might helpin identifying pornography, because skin with a face is currently more ofa problem than skin without a face. face detection technology probablycan be applied to very specific body parts; text and image data and connectivity information also will help.2milo medin said that security software now on the market uses a camera in the computerto identify the user during sign on. bob schloss commented that it is much easier to compare an image to one or more known, authorized users than to an arbitrary person.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.david forsyth15however, i do not believe that the academic computer vision community will be highly engaged in solving this problem, for three reasons.first, it embarrasses the funding agencies. second, my students have beentolerant, but it is difficult to assign a job containing all sorts of problematicpictures. third, it embarrasses and outrages colleagues, depending ontheir inclinations.technical solutions can help manage some problems. i am convincedthat most practical solutions will have users in the loop somewhere. theuser is not necessarily a child trying to avoid pornography; he or she maybe a parent who backs up the filter and initiates a conversation when problematic pictures arise. what is almost certainly manageable, and going tobecome more so, is a test to determine whether there might be nakedpeople in a picture. the intimidation scenario described above could worktechnically in the not too distant future.what will remain difficult are functions such as distinguishing hardcore from softcore pornography. these terms are used as though theymean something, but it is not clear that they do. significant aspects of thisproblem are basically hopeless for now. there have been reasonable disagreements about the photographs of jock sturgess, for example. manydepict naked children. they are generally not felt to be prurient, butwhether they are problematic is a real issue. there is no hope that a computer program will solve that issue.another example of a dilemma is a composite photograph preparedby someone whose intentions were clearly prurient. one side shows children on a beach looking in excited horror at the other side of the frame,where a scuba diver is exposing himself. there was a legal debate overthis photo in the united kingdom and a legal issue in this country as well.one part of the photo showed kids pointing at a jellyfish on the beach; theother part was a lad with his shorts off. real people might believe that theintention of that photograph is prurient and seriously problematic, butthere is no hope that a computer program will detect that. it is not evenclear whether pictures such as this are legal or illegal in this country; reasonable people could differ on that question.based on my knowledge of computer vision and what appears to bepractically possible, any government interested in getting around filtersdesigned to censor things like voice of america is wasting its money.either that, or it is engaged in the essentially benevolent activity of supporting research. something like this could be regarded as a final courseproject in informationretrieval computer vision for a statistical englishprogram. this will remain true for the foreseeable future.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.164the technology of search enginesray larson4.1overviewmost search engine companies do not want to reveal what their technology is or does, because they consider that to be a trade secret. everycompany claims to do retrieval better than every other company, and theydo not want to lose their competitive edge. i will provide a broad overview of how search technology works in current engines, based on the oldstandard models of information retrieval.two players are involved: the information system and the people whowant the information stored in the system. the searchers go through aprocess of formulating a query, that is, describing what they seek in waysthat the system can process. the same sort of thing happens on the otherend, where the system has to extract information from the documentsincluded in its database. those documents need to be described in such away that someone posing a query can find them.in general, the emphasis in the design and development of searchengines has been to make the document finding process as effective aspossibleñtoday, however the goal seems to be to exclude some searchers.the idea is to prevent some people from getting things that we think theyshould not get. this is anathema to someone from a library background,where we tend to think that everyone should have access to everythingand that it is up to mom and dad to say no.in between the information system and the searcher are the searchengineõs processing functions (the òrules of the gameó)ñhow the languages are structured, all the information that can be acquired from thetechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.ray larson17documents that come in, and how that gets mapped to what a searcherwants. the usual outcome is a set of potentially relevant documents. thesearcher does not know whether a retrieved document is really relevantuntil he or she looks at it and says, òyes, that is what i wanted.ómuch of what happens in search engines, which generally use theòbagofwordsó model for handling data, is structure recognition. searchengines often treat titles differently than they do the body of a web page;titles indicate the topic of a page. if the system can extract structure fromdocuments, it often can be used as an indicator for additionally weightingthe retrieval process.often the search engine normalizes the text, stripping out capitalization and most other orthographic differences among words. some systems do not throw this information away automatically but rather attemptto identify things such as sequences of capitalized words possibly indicating a place or personõs name. the search engine then usually removesstop words, a list of words that it chooses not to index. this would be alikely place to put a filter. but this can become problematic because, whenusing a bagofwords model, one occurrence of a word does not indicateother nonproblematic occurrences of the same word. if the usual suspectwords were placed on the list of stop words, then suddenly the americankennel club web site no longer would be accessible, because of all of thewords that refer to the gender of female dogs, and so on. rarely, thesearch engine also may apply natural language processing (nlp) to identify known phrases or chunks of text that properly belong together andindicate certain types of content.4.2boolean search logicwhat is left is a collection of words that need to be retrieved in someway. there are many models for doing this. the simplest and mostwidely availableñused in virtually every search engine and the initialcommercial search modelñis the boolean operator model. simple boolean logic says either òthis word and that word occur,ó or òthis word orthat word occur,ó and, therefore, the documents that have those wordsshould be retrieved. boolean logic is simple and easy to implement. almost all search engines today, because of the volume of data on theinternet, include an automatic default setting that, in effect, uses the andoperator with all terms provided to the search engine. if the searcherturns this function off, then the search engine usually defaults to a ranking algorithm that attempts to do a òbest matchó for the query.all of these combinations can be characterized in a simple logic modelthat says that this word either occurs in the document or that it does not.if it does occur, you have certain matches; if not, you have other matches.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.18the technology of search enginesany combination of three words, for example, can be specified, such thatthe document has this word and not the other two, or all three together, orone and not the other of two. you can specify any combination of thewords. but if you do not specify the word exactly as it is stored in theindex, then you will not get it. it cannot be a synonym (unless you supplythat synonym), or an alternative phrasing, or a euphemism.4.3the vector space modelanother approach is the vector space model. this model was developed over 30 years of intensive research into a finely honed set of tools.probabilistic models are also being used much more commonly thesedays. many other models combine many of the same aspects, includingattempts to automatically recognize structures of information withindocuments that would indicate relevance. alternatively, one could lookat all of the documents in a collection and consider each individual wordthat occurs in any of those documents. but most large collections havetens of thousands of words, even hundreds of thousands. a large proportion of those words are nonsense, misspellings, or other problems thatoccur once or twice, whereas other words occur often (e.g., the, and, of).the vector space model attempts to consider each term that occurs ina document as if it were a dimension in euclidean space. (this is why weuse three terms as an example; if there are more than three dimensions, itbecomes difficult for people to think about.) in a vector space model, eachdocument has a vector that points in a certain direction, depending onwhether it contains a term or not. the documents are differentiated onthis basis. this example shows a system where there is a simple yes/noprocess; a document either has the term or does not have it. you also canconsider each term as having a particular weight, which can be measuredin a variety of ways, such as how frequently the word occurs in a particular document.in this model, you are calculating the cosine of the angle between twovectors in imaginary space. the smaller the angle between the vectors,the more similar the document is to the query. you can rank documentsbased on that closeness or similarity.1 therefore, in most vector spacemodels, you do not need to match all the words. as long as you match1nick belkin said that similarity in text documents is relatively easy to compute, assumingconstant meaning of words, whereas similarity of images is very difficult to compute. davidforsyth gave the example of the pope kissing a baby versus a picture of a politician kissinga baby; they are the same picture in some ways, but different in others.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.ray larson19many or even some of the words, you will get closer to a particular document that has those words in it.this model uses òterm frequency/inverse document frequencyó (tfidf), a measure of the frequency of occurrence of a particular term in aparticular document, as well as how often that term occurs in the entirecollection of interest. if a term occurs frequently in one document but alsooccurs frequently in every other document in the collection, then it is nota very important word, and the tfidf measure reduces the weightplaced on it. a common term is considered less important than rare terms.if a term occurs in every document, then the inverse document frequencyis zero; if it occurs in half of the documents, it will be 0.3; and if it occurs in20 of 10,000 documents, it will be 2.6. if a term occurs in just one document, then the idf measure would be 4ñthe highest weight possible.unfortunately, most pornographic words, given the distribution of pornon the internet, are not rare.once you have extracted the words from the documents, you have toput the words somewhere. they usually are placed in an inverted file,which puts the words into a list with an indication of which documentsthey came from. then the list is sorted to get all the terms in alphabeticalorder, and duplicates are merged; if there are multiple entries for a particular document or term, then you increment the frequency for that item.this is the simplest form of an inverted file. many search engines alsokeep track of where a word occurs in a document, to provide proximityinformation. they also keep track of many other things, such as howmany links there are to the page that a word is on.finally, you differentiate the file to make a unique list for every termthat occurs in the entire database, with pointers that say in which documents they occurred and how frequently. with that information, you canthen calculate the magicallooking formulas that provide a ranking for adocument.4.4searching the world wide webmost web search engines use versions of the vector space model andalso offer some sort of boolean ranking. some search engines use probabilistic techniques as well. others do little more than a coordinationlevelmatching, looking for documents that have the highest number of specified terms. some use natural language processing (lycos, for example,was based on some nlp work by michael mauldin). exciteõs conceptbased search may be a development of latent semantic indexing (developed at bell labs). the inktomi search engine formerly used a form ofretrieval based on logistic regression.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.20the technology of search enginesvirtually all search engines use the bagofwords of model.2 someuse additional page weight methods, looking not only at frequency of aword in a document, but also at other things like the number of links to apage. google uses inlinks, for example. if no one links to your page,then you would get a lower rank than someone who had the same wordsbut many inlinks. most search engines also include every string of characters on a page, even if they are total garbage. therefore, in addition tocomparing one word to another, you have to compare all of the numbers,which is difficult.exact algorithms are not available for most commercial web searchengines. most search engines appear to be hybrids of rank and booleansearching. they allow you to do a guessmatch symbolized by the vectorspace model and also very strict boolean matching. but most users neverclick to the òadvanced searchó page, which explains how to do all of thesethings; they usually just type in what they think would be an appropriatesearch. most people looking at search logs would say, òthatõs ridiculous.how are they ever going to find anything?óthe search engine obtains this material by sending out a òspideró toretrieve the pages from web sites. they retrieve only static pages, notpages that are hiding as databases or are dynamically generated. mostcrawlers also obey the robot.txt file on a web site; if the file says, òdo notindex this site,ó they do not index that site. they can store millions ofwords and hundreds of sites.there are different methods of crawling. in a depthfirst crawl, yougo down as deep as you can within any particular site before going on tothe next site. another way is a breadthfirst search, where you start acrossmany different sites and work your way down slowly.3 part of the reason2david forsyth observed that it might be logical to ask why people use the bagofwordsmodel, which they know to be bad. the answer is, it is very difficult to use anything else.most reasonable people know about 60,000 words. you need to count how often each oneappears in text. you need a lot of text to do this. if you are modeling the probability ofseeing a new word, given an old word, there are 60,000 choices for the old word and 60,000choices for the new word. the table would be 60,000 by 60,000, and it would be difficult tocollect enough data to fill the table. ray larson noted that 60,000 words is a very small sizecompared to the indexes used by search engines.3nick belkin noted that a crawler is limited by the size of its own memory. as soon as itfinds as much as it can hold, it stops. milo medin observed that this is not an ideal approach.rather, you want to rank order the types of things that you will either archive or not. if youcannot store all the useful things, then, rather than stop, a better approach is to go back andprune out some of the duplicate or irrelevant material. ray larson said finding duplicatesis a big deal, because many things either have the same name or have different names butare on the same pages. for database storage and efficiency reasons, it is important to findthose things.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.ray larson21for this is, if a spider comes to your web site and hits you 50,000 times ina row to get every single page that you have, you will get upset. instead,breadthfirst spiders spread out the hits over time among a number ofsites. the main message here is that the pages have to be connected somehow to the starting points or else you never will get themñthat is, unlesssomeone has sent you a pointer saying, òhere is the new starting point.hereõs our site, please index it.ó4 some people sell algorithms that ensurethat a given page gets ranked higher than others. search engine companies spend a lot of their time figuring out how to identify and counteractthe òspammedó pages from those people. it is an òarms race.ó5a paper published in nature in 1999 estimated the types of materialindexed, excluding commercial sites.6 scientific and educational siteswere the largest population. health sites, personal sites, and the sites forsocieties (scholarly or other) are all larger than the percentage estimatedfor pornography.7 no search engine has 100 percent coverage, and theyoften cover quite different things. there can be overlap, as well. thereare also issues of numbers of links. if one site indexes something, then4milo medin said that some sites generate indexes by asking other search engines andindexing what they already have. he also said that no catalog inventories show up insearches because the inventory is designed for a database query. the exception is when thatsite has created an index page with a set of stored queries.5winnie wechsler said that there seems to be a fundamental tension between search engines striving to provide the greatest accuracy to users in terms of retrieval or filtering andweb publishers trying to trick or mislead the search engines to make sure their sites arelisted as much and as high in rank as possible. how does this tension resolve itself? it doesnot seem resolvable, certainly in the case of pornography. nick belkin said one approach isto use more words in a query to make the conditions more restrictive. a query with 10words will get a much better result than one with only 2 words because it defines muchmore context. the difficulty is that, even though the average number of words per query onthe web has been going up, it is still only about 2.3 words, up from 1.7 words a few yearsago. with very simple search engine technology, it may help to encourage people to usemore words in their queries.6steve lawrence and c. lee giles, òaccessibility and distribution of information on theweb,ó nature 400(6740): 107109, july 8, 1999.7milo medin said that the declining cost of web servingñgenerally a good thingñhasmade it easier for amateur pornographers to get published. medinõs service offers free webhosting for a certain amount of material. subscribers are not allowed to post pornographyor objectionable material, but there is no cost or punishment if they do, so they take advantage of this situation. the company audits sites based on the amount of traffic to them.when a site attracts a certain amount of traffic, it triggers a red flag and generates a query tothe people in charge of investigating abuse. medin recalled that, when he worked for nasa,data on international links had to be controlled. when someone put up a porn site, the linkutilization to that region would rise. a wiretap would reveal where the traffic was going.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.22the technology of search enginesanother site will index it. things that are unique tend to stay uniquewithin a particular search engine.8in looking for images, text retrieval technology looks for text that isassociated with images. it looks for an image link tag within the htmland the sentences that surround it on either side. this can be highly deceptive. the words òoh, look at the cute bunniesó mean one thing on achildrenõs web site and something entirely different on playboyõs site.thus, the words alone may not indicate what those images are about.8milo medin emphasized the business dynamic, noting that creating the search capabilityto find an obscure web page may not be worth the cost in terms of its impact on the subscriber base. say a search engine fails to find 5 percent of the material on the internet. tosome people whose content is in that 5 percent, this is important. but if the cost of findingthat 5 percent is double the cost of finding the other 95 percent and the bulk of searchers aresatisfied with that performance, it may not be worth it. search engines are not librarians;they exist for a business purpose.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.235cyber patrol: a major filtering productsusan getgood5.1introductionsurfcontrol, inc., is the worldõs largest filtering company, with officesand companies throughout the world. the company attained this position through a combination of organic growth and growth by acquisition.in 1998 it got into the corporate filtering business, and in 1998 and 2000 itacquired both surfwatch and cyber patrol, the pioneers in filtering toprotect kids from inappropriate content.i will tell you what filtering software is and what it is not. it is safetytechnology, like a seatbelt for internet surfing. seatbelts are not 100 percent guaranteed to save a childõs life, but there is no responsible parent inamerica who does not buckle up a child in the car. we believe the situation is the same in protecting kids from inappropriate content online. filtering software puts the choice of how and when children can use theweb in the hands of the people who should have it: parents and educators. it is also the most effective way to safeguard kids from inappropriate web content without compromising first amendment rights, which isimportant. we are creating a solution that puts choice in the hands of thepeople who need it, while keeping the government out of those choices.filtering software is not a replacement for the guidance of parentsand educators. i doubt any filtering software company would suggestthat parents, teachers, educators, administrators, business people, or anyone use filtering software without clearly providing the guidance thatchildren need to understand what they see on the internet.web filtering products either block or allow access to web sites bytechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.24cyber patrol: a major filtering producteither ip addresses or domain names. most of the widely available commercial products are list based, with human reviewers. these productsalso use some artificial intelligence (ai) tools but not as the primarymechanism of filtering. technologies work for us in the research process,but they do not replace human review, which verifies that the content ona page is about, for example, a marijuana joint and not the joint chiefs ofstaff, or that a woman in a picture is not wearing a tan bathing suit. weneed human reviewers to make sure that content really is inappropriate.5.2why filter?about 30 million children in this country have access to the internet,and about 25 percent of them are exposed to some type of unwanted orinappropriate online content. although we are mostly concerned herewith sexually explicit content and pornography, it is important to remember that parents and educators are concerned about broader types of content, from hate sites and intolerance material to how to build a bomb andbuy a gun. parents and educators are the people with whom i deal mostin my job, which is running the cyber patrol brand.parents want this type of technology and they want it used both inschools and at home. in 2000, a study by digital media found that 92percent of americans want some type of filtering to be used in schools;they are concerned about the content that their children see. our job is tofind a way to make filtering an effective technology solution that does notget in the way of the educational experience, whether at home or in school.interestingly, we found that people do not always realize there is aproblem until they look at their hard drives and find miss april or missmay. as reported in the press recently, a teacher (a customer of one of ourcompetitors) checked the history of each computer and was appalled atwhat the students were able to access. they were accessing sexually explicit material, gambling, applying for credit cards, buying products without parentsõ permissionña whole host of things. there is clearly a problem out there in the world, and parents and schools want to do somethingabout it.corporations filter for four basic reasons: (1) productivity of employees; (2) legal liability for inappropriate content being available on networks; (3) issues of inappropriate surfing, which takes up room in theinformation pipeline; and (4) increasing demand for security to preventcompromise of confidential information. in schools, we tend to focus onfiltering to protect children from inappropriate content. but we havefound that network bandwidth increasingly is an issue in schools, especially with respect to federal mandates for filters, which we oppose. webelieve that schools purchase filtering software because it solves a widetechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.susan getgood25variety of problems, not just the simple, single problem of protecting kidsfrom inappropriate content.we mailed a quick email survey out last week to 1,200 customers andgot a 2.64 percent response rate, which is fairly good in this time frame.we asked them how important internet bandwidth was to them last yearversus this year. fiftyfive percent said it was very important or important last year, compared to 70 percent this year. similarly, 37 percentwere either neutral or thought it was an unimportant issue last year, compared to only 24 percent this year. this is what our customers are tellingus, both anecdotally and numerically. the bandwidth issue arises whenkids in the library go off to look at napster,1 free email accounts like hotmail and yahoo mail, and anything else not on task. even somethingotherwise appropriate, such as checking out sports scores, is not on taskat work or school. if napster is regulated, something else will come alongto replace it as the next big thing on the internet. we try to stay ahead ofwhat our customers need, and internet developments like napster proveto me that educators are looking at the whole issue of managing theinternet in the classroom, not just the management of sexually explicitcontent.5.3superscout and cyber patrolwe have two brands, superscout and cyber patrol. i will describesuperscout briefly and then concentrate on cyber patrol.superscout was developed to do filtering, monitoring, or reporting ina corporate environment. it uses an extensive list of nonbusinessrelatedweb sites. it has an optional ai tool that provides dynamic classificationof content, looking at the sites employees visit. some sites are on thesurfcontrol list, and some are not. if a site is not on the list, then the aiprogram uses pattern recognition and textual analysis. it can run thisinformation against the category definitions of the business product andgive the corporation an additional list that can act as a buffer against thecontent that people actually see. we do not plan to add this technology tothe home filtering products, although we use it in research before the reviewers look at something. we see a trend, especially in institutional settings but also in homes, toward managing access to the content that peopleactually are trying to seeñas opposed to having huge category lists ofwhich employees are trying to access only 1 percent.1milo medin said that the bandwidth issue is driven primarily by multimedia. manyinternet service providers have issues with napster traffic; about 1015 percent of bandwidth traffic on his companyõs interconnects is napster traffic.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.26cyber patrol: a major filtering productcyber patrol, which keeps kids safe, comes in standalone versionsfor the home and network versions for schools. the network version operates either on local area networks or through proxy servers. cyber patrol for schools focuses on blocking web access, and it goes through themicrosoft proxy server, microsoft internet security and accelerationserver 2000, or novell border manager. we incorporate elements withinthe software that address the whole scope of what parents are trying to doto protect their kids. we enhanced security and improved tamper resistance in the latest version for the home. parents can customize settingsfor multiple children or multiple grades. we also provide informationabout why a site is blocked, so that parents can explain to their childrenwhy they were not allowed to access something.cyber patrol works the same way if you are a subscriber to americaonline (aol). typically it is used in addition to aolõs parental controls,which are based on work that we did. other internet service providersalso offer these types of controls. an advantage to using a standalonefilter is that it works regardless of how children access the internet. itfollows the same set of rules regardless of whether a child uses aol, yourdialup modem to work, or a dialup modem they got from a friend, because the software is installed on the computer. we have many customers who use aol but also use cyber patrol specifically because they wantthe same settings and time management across multiple services.serverbased filters, the primary design used in schools and businesses, tend to be integrated with networks and users. when you log inas jimmy smith in the seventh grade, the filter knows that you are jimmysmith and how to apply the filtering rules. different rules can be appliedfor different users within a school system. in our user base, school districts have different rules in elementary school versus middle school versus high schoolñexcept for sexually explicit material, which tends to beblocked throughout the whole school system. as an example, you maynot want the fourth graders to access material about intolerance, but theseventh graders may be doing a project on hate groups. (setting rules indifferent ways is consistent with the new law against disabling filters.)eventually, as student identification (id) cards move toward becomingsmart cards, a childõs filter rules, lunch money, and library books will allbe on the id card.22milo medin said that user identification and signon always have been complicated because they involve sharing a password. but fingerprint scanners are becoming less expensive and are starting to appear in keyboards. this enables a userfriendly level of identification, because you no longer need to worry about getting your password right. this willbecome more common in the marketplace.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.susan getgood27we block lists of specific pages (identified by their uniform resourcelocator designations (urls)); we do not analyze the content of a page as itis downloaded to a subscriberõs computer. playboy.com is blocked because it is playboy, not because the program senses nude pictures or forbidden words. we can block an entire site or by page level. cyber patrolfor homes is based on a list called the cybernot list, reviewed in itsentirety by human reviewers. our team of professional researchers ismade up of parents and teachers. parents can then select the categories oflists that they want to use. we tailor the filtering levels to meet the needsof different children. ageappropriate filtering is possible; for example,we have a sex education category so that material that otherwise wouldbe considered sexually explicit can be made available to older children enmasse.there are 13 cybernot categories to choose from: violence and profanity, partial nudity, full nudity, sexual acts, gross depictions, intolerance, satanic and cult, alcohol and drugs, alcohol and tobacco, drugs anddrug culture, militant and extremist, sex education, and questionable/illegal material and material related to gambling. the definitions are published on our web site and in the product itself, so that parents can reviewthe definitions as they decide how to tailor the softwareõs settings to fittheir needs. about 7080 percent of the list content is violence or profanity, partial nudity, full nudity, sexual acts, and gross depictions. the othercategories make up 2030 percent; these categories are more difficult toresearch and much less obvious.we publish our content definitions and categories. we give you theability to override or allow based on your own preferences, but we do notpublish the sites that are on our category list. we have spent thousands ofdollars to build a proprietary list that cannot be duplicated by anyone; ihave yet to hear a commercial reason that makes sense why we shouldallow that. as a company devoted to protecting kids from inappropriatecontent, we will not publish a directory of dirty sites.we do not filter urls or web sites by keyword, which is an important point. we do use keywords as part of the research process to getsuspect material to look at. the training process is done on the job usinga shadowing technique. that is, a new researcher works with someonewho has been doing it for a while to understand the process. researcherswork in teams, which is important in identifying material, particularlywhen the material is difficult to classify and a discussion about it is helpful. most researchers have child development backgrounds, typicallywith some type of training, whether teaching certification or onthejobtraining as a parent. they are not child development specialists or psychologists, but they have an appreciation for why and how to classify thematerial.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.28cyber patrol: a major filtering productcyber patrol does not interfere with, or get involved in, the searchengine process. the software works purely in the browsing process. wecan block a search on sex if that is what the parent wishes, but we do notfilter search results. if a child tries to visit a blocked site, cyber patrolshows you that the site does exist but that you were not allowed to accessit, and tells the parent why. if you are trying to make this site available foryour family, you can go back and change that particular siteõs setting andknow that you are fixing the right thing, as opposed to stumbling aroundblindly, trying to figure out why a site was blocked.we deal with two kinds of chat. one is webbased chat, which weblock specifically by blocking the category of webbased chat. alternatively, you can use privacy features, which allow kids to go into chatroomsñif you want them to be allowed to talk about bird watching orwhateverñbut not to give out their names, addresses, or phone numbers.it cannot do anything about a 15yearold who is determined to tell someone his address. but if a naive 12yearold inadvertently gives out hisnumber, then the feature replaces it with a set of nonsense characters. wealso can block internet relay chat, which is used much less often now thanin the past, either completely or based on the chat channel name.surfcontrol gets a lot of feedback from customers. when a customerasks us to look at a site to see if it should be blocked for the larger population, not just for his or her own family, we spend more time on it than weotherwise might. often, however, such sites do not warrant being addedto a list that a large population uses.consumers can decide how well we make decisions by trying theproduct before they buy it.3 parents using cyber patrol can try to go to aweb site that is blocked and, if they think it should not be blocked, bypassthe filter and look at the site and make a personal decision about whethercyber patrol was right or wrong in putting that site on the list. (parentscan override the system, but children cannot, because, hopefully, they donot have the necessary password. picking the family dogõs name as thepassword is probably not a good idea.) there is an element of trust. ifthey believe that we offer them a good place to startñfiltering software isnot a replacement for parents, nor is it a solution for everythingñthen it isa reasonable place to start to protect their kids. we try to provide parentswith a solution that gives them the ability to implement their own choices.3david forsyth argued that it is easy to determine whether a dishwasher works becausethe plates either come out clean or dirty, but it is difficult to tell whether cyber patrol works,so the choice issue becomes problematic. milo medin noted that the average housewife isnot likely to figure out the difference between good and poor dishwashing fluid. rather, shemakes decisions based on brand, consumer reports, and other evaluations. medin said hedoes not make decisions about highly technical matters based only on his own experiments;third parties do these lab tests.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.susan getgood29we cannot guarantee 100 percent true positives, but we do the bestjob we can to build the tool. if there is a metric for deciding how muchaccuracy is enough, it is the market. the market decides what level ofaccuracy it wants by making product choices. if we have a good product,then presumably parents, schools, and businesses will continue to buy it.if we did not have a good product, then i truly believe that joe in hisgarage would come up with something better.one reason why we oppose mandatory filtering is that we believe theuse of these products should be a choice that parents and educators make,just as it is a choice for businesses. when you select and evaluate a productñin our case, you can try it for 14 days before you buy itñthen thechoice is yours. if it is mandated, then it is not a choice.5.4the review processto clarify, we have two review processes. one is the process of finding new material that comes onto the internet. we use a variety of mechanisms, from search engines to crawlers. that same group of people isinvolved in the rereview process to make sure that once something is onthe list, it should remain on the list.the cyber patrol team consists of about 10 people; most have beenwith us for at least 2 years and some more than 4 years. it is a good job fora parent who wants a parttime or supplementary job. we have workedhard to ensure that the job entails more than just looking at inappropriatecontent all day, which would be absolutely mind numbing. we also buildpositive lists. we have a yes list that we use. the job also has responsibility in the technical side of building these lists.it might sound like a great job, looking at porn all day. but afterabout a day, it becomes less fun. to understand what they are reading,the reviewers can spend anywhere from a minute or less on pornographicmaterial to upwards of 10 minutes on intolerance material or somethingthat requires textual analysis. a sexually explicit site can be judged fairlyquickly; a picture is a picture. if deeper probing into a site is required,that takes longer. we do not block sites simply because they domousetrapping,4 and we do not view this technique as a red flag for sitesto be reviewed. (i plan on suggesting it, however.)4mousetrappingña technique in which clicking on an item causes a second item to popupñis used by pornography and gambling sites. milo medin said that he would pay forblocking of sites that use mouse trapping, especially when it has multiple levels. herb linnoted that the underlying technology has legitimate purposes, such as in making surveys orquestionnaires pop up on consumer sites.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.30cyber patrol: a major filtering productit is a mistake to attribute political motives to surfcontrol or any othermajor filtering company. we add sites to our list based on their content.in the case of the gossip site the register,5 my understanding is that itpublished a detailed explanation of how people could use a loophole inanonymous proxies to get around the use of filtering softwareñto let kidsget pornography. this is why the site was added to the list.6 the ultimate example of a difficult case might be determining whether an imageis art or nudity. we would not consider work by rubins wake to benudity, because it is art. however, if you duplicated one of those imageson your own personal web page, using your own friends and family, thenthat probably would not qualify as art.we make sure that we rereview material, so that web sites that goout of existence do not stay on our list. we have regular rereviews of thelist categories, both as projects within the research department and as partof the customer feedback process. on average, we probably cycle throughthe whole cybernot list about once every year. some categories get morefrequent reviews. we look at some sites every month. a couple of organizations ask us to look at sites every month, and we do. after theheavenõs gate incident,7 we made an effort to go back through all thematerial on the cult. the same thing was done after the columbine highschool shooting.8 we do rereviews of the categories that are particularlyrelevant to these sorts of issues. the software comes with a yearõs subscription to daily updates, so it is updated on a regular basis.we are looking at ai to speed up some of the review processes. oneapproach is dynamic pattern matching. internal tests reveal up to 85 percent accuracy or agreement between what our reviewers find and whatthe tool finds. as that number starts to improve, we will be able to startrelying more on this tool. right now we do not believe that eliminatingthe human review process in cyber patrol is the right thing to do.here are two paraphrases of what reviewers say about their jobs.they take this job very seriously, which is one reason why we have beenable to keep some of these people for upwards of 4 or 5 years. they really5david forsyth said the register claimed it was blocked because it had said the financialbasis of the filtering market was not as sound as it looked and that surfcontrol might betaken over.6david forsyth said that this is a situation in which a legitimate discussion of a technological issue was cut short because useful, retrievable information was taken out of the publicdomain. susan getgood said that the company does not claim it never makes mistakes andthat perhaps the researcher who added the site to the list was being overzealous.7in march 1997, 39 members of the heavenõs gate cult committed suicide.8in april 1999, two students went on a shooting spree in their suburban high school injefferson county, colorado. thirteen people were killed and 21 were wounded.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.susan getgood31do believe that they are doing something that helps parents do their jobbetter.¥òbeing a researcher demands an open mind and an objective outlook at all times. we try to protect children and many adults from offensive and harmful material without encroaching on anyoneõs right to freespeech.ó¥òit can be both difficult and rewarding. at times, seeing the worstof what is on the internet can be difficult, but the reward comes when youknow that a small child, whose parents are responsible enough to usefiltering, will not ever have to see what i just saw when i put it in thedatabase.ó5.5the futureas part of surfcontrol, we take advantage of an active research anddevelopment department. we now have 40 researchers around the world,an increase from the time when cyber patrol alone had 10. this gives usan ability to deal with international content in a cultural context, ratherthan as americans looking at something in german or dutch or spanish.we are looking at the next generation of filtering and what we need tocontinue to do to build these products. we do not create the need forthese products; the need is out there. we are doing our best to developsoftware and products that meet the need.forty reviewers might seem like a small number if you were startingtoday.9 if you started this year and tried to do the whole web in 365 days,you probably would have a tough time. but we have been doing this for6 years, so there is a base that we are not repeating. we focus on theinappropriate content; we do not try to look at every single page on theinternet. to increase accuracy in dealing with material that is difficult tocategorize, it is not a question of hiring more people but rather of lookingat tools such as image recognition. we can manage the human costs andalso improve the frontend part of the research.clearly, there will be more bandwidth to homes in the future. thiswill allow us to use more robust ai technologies in these products. commands such as òdonõt show me more like this oneó rely on dynamic categorization. modems cannot handle this effectively; you need highspeed,9marilyn mason said that there are more than 1 billion sites total. winnie wechsler saidthat a couple of million new sites are added each year. david forsyth said that, given 1million new web sites a year (not an unreasonable number), then 40 reviewers have to review 25 sites an hour in a busy year to get them all done.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.32cyber patrol: a major filtering productbroadband connections. image filtering also is clearly part of the future,but there is not, as yet, a solution for this. we think the use of filteringalso will be changed by email, which is now available to just about everyone, and instant messaging. we will start looking at how to incorporateways to keep these methods safe for kids.privacy is of great interest to us, because protecting kidsõ private information goes handinhand with protecting them from inappropriatecontent. we already pay attention to both childrenõs rights for privacyand parentsõ decisions about their childrenõs privacy. we chose not to puta logging or monitoring feature into the cyber patrol home product because children have a right to privacy if they are looking at appropriatematerial. as rules on privacy preferencesñrules about going to web sitesthat collect information on kidsñbecome finalized, we will be able toimplement those rules in a technological fashion, so that parents can prevent kids from going to web sites that, for example, publish surveys. wewill be able to implement those types of thingsñif the market wants them.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.336advanced techniques forautomatic web filteringmichel bilello6.1backgroundas of 1999, the web had about 16 million servers, 800 million pages,and 15 terabytes of text (comparable to the text held by the library ofcongress). by 2001, the web was expected to have 3 billion to 5 billionpages.1to prevent kids from looking at inappropriate material, one solutionis to have dedicated, pornographyfree web sitesñsuch as yahoo!kidsand disney.comñand assign reviewers to look at those particular websites. this is useful in protecting children too young to know how to usea web browser.filtering is mostly text based (e.g., net nanny, cyber patrol,cybersitter). there are different methods and problems; for example,cyber patrol looks at web sites but has to update its lists all the time. youcan also block keywords, scanning the pages and matching the wordswith keywords. but keyword blocking is usually not enough, becausetext embedded in images is not recognized as text.2 you could block allimages, but then surfing an imageless web would become boring, especially for children. a group at the nippon electronic corporation (nec)1steve lawrence and c. lee giles, òaccessibility and distribution of information on theweb,ó nature 400(6740): 107109, july 8, 1999.2michel bilello said that his group has used a technique that pulls text off images, such aschest xrays used for research purposes. they process the xray image, detect the text, andthen remove, for example, the name of the patient, which the researcher does not need toknow.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.34advanced techniques for automatic web filteringtried to recognize the clustering communities within the web. you could,for example, keep the user away from particular communities or excludesome communities from the allowed web sites.6.2the wipe systemin the stanford wipe system,3 we use software to analyze image content and make classification decisions as to whether an image is appropriate or not. speed and accuracy are issues; for example, we try to avoidboth false positives and false negatives. the common imageprocessingchallenges to be overcome include nonuniform image background; textual noise in foreground; and a wide range of image quality, camera positions, and composition.this work was inspired by the fleckforsythbregler system at theuniversity of california at berkeley, which classifies images as pornographic or not.4 the published results were 52 percent sensitivity (i.e., 48percent false negatives) and 96 percent specificity (i.e., 4 percent false positives). the berkeley system had a rather long processing time of 6 minutes per image.in comparison, the wipe system has higher sensitivity, 96 percent,and somewhat less specificity (but still high) at 91 percent, and the processing time is less than 1 second per image. this technology is mostapplicable to automated identification of commercial porn sites; it alsocould be purchased by filtering companies and added to their products toincrease accuracy.in the wipe system, the image is acquired, feature extraction is performed using wavelet technology, and, if the image is classified as a photograph (versus drawing), extra processing is done to compare a featurevector with prestored vectors. then the image is classified as either pornographic or not, and the user can reject it or let it pass on that basis.there is an assumption that only photographsñand not manually generated images, such as an artistõs renderingñwould be potentially objectionable. manually generated images can be distinguished on the basis oftones: smooth tones for manually generated images versus continuoustones for photographs. again, only photographs would require the nextprocessing stage.3for a technical discussion, see james z. wang, integrated regionbased image retrieval,dordrecht, holland: kluwer academic publishers, 2001, pp. 107122. the acronym wipestands for wavelet image pornography elimination.4margaret fleck, david forsyth, and chris bregler, òfinding naked people,ó proceedingsof the european conference on computer vision, b. buxton and r. cipolla, eds., berlin, germany: springerverlag, vol. 2, 1996, pp. 593602.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.michel bilello35this work was based on an informationretrieval system that finds ina database all the images òcloseó to one selected image. from the selectedimage the software looks at thousands of images stored in the databaseand retrieves all the ones that are deemed òcloseó to the selected image.the images were tested against a set of 10,000 photographic images and aknowledge base. the knowledge base was built with a training system.for every image there is some trusted element, a feature vector can bedefined that encompasses all the information, texture, color, and so on.then images are classified according to the information in this vector.the database contains thousands of objectionable images of varioustypes and thousands of benign images5 of various types. in the trainingprocess, you process random images to see if the detection and classification are correct. you can adjust sensitivity parameters to allow tighter orlooser filtering. you could combine text and images or do multiple processing of multiple images on one site to decrease the overall error inclassifying a site as objectionable or not.a statistical analysis was done showing that, if you download 2035images for each site, and 2025 percent of downloaded images are objectionable, then you can classify the web site as objectionable with 97 percent accuracy.6 image content analysis can be combined with text and ipaddress filtering. to avoid false positives, especially for art images, youcan skip images that are associated with the ip addresses of museums,dog shows, beach towns, sports events, and so on.in summary, you cannot expect perfect filtering. there is always atradeoff between performance and processing effort. but the performance of the wipe system shows that good results can be obtained withcurrent technology. the performance can improve by combining imagebased and textbased processing. james wang is working on training thesystem automatically as it extracts the features and then classifying theimages manually as either objectionable and not.75to develop a set of benign images, david forsyth suggested obtaining the corel collection or some similar set of images known to be notproblematic or visiting web news groups,where it is virtually guaranteed that images will not be objectionable. he said this is a rarecase in which you can take a technical position without much trouble.6david forsyth took issue with the statistical analysis, because there is a conditional probability assumption that the error is independent of the numbers. in the example given earlier with images of puddings (in forsythõs talk in chapter 3), a large improvement in performance cannot be expected because there are certain categories in which the system will justget it wrong again. if it is wrong about one picture of pudding and then wrong again abouta second picture of pudding, then it will classify the web site wrong, also.7for more information, see <http://wwwdb.stanford.edu/image> (papers) and<http://wang.ist.psu.edu>.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.367a critique of filteringbennett haselton7.1introductioni have been running the peacefire.org site for about 5 years, and wehave become known as a source of mostly critical information about blocking software and filtering. i am biased in general against the idea of filtering, as well as the existing limitations, but that is fair because all intelligent people should have opinions about what they study. they simplyneed to design the experiments so that the person with the opinion willnot influence the outcome.the earlier presentations provided a general idea of how differenttypes of programs work. some programs examine the text on a downloaded page to look for keywords in the web page address (the uniformresource locator, or url) or in the body of the page. other programs aremainly list based; they do little analysis of the text on a page but have abuiltin list of sites that are blocked automatically. all the programs thati know of are some combination of the two types. they have some keyword filtering and some list filtering, but they can be slotted easily intoone of these categories.most mainstream commercial programs, such as cyber patrol, netnanny, and surfwatch, are list based. people often talk about a scenarioin which a site might get blocked if the word òsexó is in the title or firstparagraph. this scenario has not been accurate for years. sites can beblocked inaccurately, but this is not a correct way to describe what happens, because the most popular programs that look at words on the pagealso work off builtin lists of sites.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.bennett haselton377.2deficiencies in filtering programsthe mainstream commercial programs used in the homeñwhich filter and block pages on the fly (not for auditing or later review)ñdo notfilter images. we did a study involving the only commercial program atthe time that claimed to filter images on the fly, using 50 pornographicimages taken from the web and 50 nonpornographic images. we foundthat the software performed no better than random chance if the imageswere placed in a location that the software did not know about in advance. all the pornographic and nonpornographic images in the test remained accessible, so the claim of filtering based on image contents turnedout not to be true.the company later came out with some fixes so that the program began to filter based on skin tone, but it could not do complex object recognition. the best it could do was to count the number of pixels in thepicture that were skin toned and then block based on that. we did another test involving the 50 pornographic images and 50 nonpornographicpictures of peopleõs faces, and the software scored exactly the same foreach type; it was not able to tell the difference.cybersitter is mostly a contentbased program. cyber patrol ismainly a listbased program. the contentbased programs are notoriousfor errors that arise if you block sites based on keywords on the page or inthe url. it is nowhere near as advanced as the vector space model described earlier. yet, even though these programs are so sloppy, the examples of what they block are not very controversial, because the company justifiably can say it has no control in advance over what will beblocked. there is a certain phrase in the word filter, and if a site uses thatphrase, then it is not really the companyõs fault. blocking software got abad reputation initially because of examples like a page about the exploration of mars being blocked because the title was òmars explore,ó ormarsexpl.html.i have a friend named frank who made a web page about cyber patrol, and he later found that his page was blockedñnot because he wascriticizing the software, but because his name was frank, and òankó wason cyber patrolõs list of dirty phrase keywords. the list of blocked sitescould not be edited, but the list of dirty phrases was viewable and youcould add and remove terms from it. presumably to avoid offending theparents who had to deal with it, the company put in word fragments instead of whole words. the list contained phrases such as òuckó and òank,óthe latter apparently an abbreviation for òspankingó because the company wanted to block pages and chat channels about spanking fetishes.there are many other examples, some involving programs that evenremove words from the pages as they download them, without making ittechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.38a critique of filteringobvious that words were removed. sites blocked by these programs aremuch more controversial, because the company can control exactly whatis on the list. if you find something that is blocked, then they cannot claimthey did not know in advance. supposedly, everything on the list waschecked for accuracy in advance.we periodically do reports, published on the peacefire.org site, aboutwhat types of sites we have found blocked. we focus on sites blocked bythe listbased programs; finding sites blocked by the keywordbased programs is not very interesting, because you almost always find some partof almost every site blocked by something like cybersitter. if someonewants to know if they have standing to challenge a local library filteringordinance, and they want an example, i say: òwell, if you have 20 or moredocuments, i will just run it through cybersitter and one of them will befiltered.óthe main controversy regarding listbased programs is how they create the list of sites to block. the lists are divided into categories. if a siteis classified into one of these categories, then the site will become inaccessible. this gives the illusion of more flexibility than really exists. if youare using, say, surfwatch and you elect to block only sex sites, then youblock sites that surfwatch has classified under its sex category, whichmay or may not be accurate. even if it were accurate, it might not agreewith your views on what a sex site is. even if you did agree with thecompany on what qualified as a pornography site, the actual review process might not be accurate.7.3experiments by peacefire.orgwe are one of the third parties that designed experiments to test theaccuracy of the lists used by these companies. there are a couple of waysto do this. the list of blocked sites is supposed to be secret and is notpublished, but it is always stored in a file that comes with the software. aclientbased program has a local list, and periodically you update the listby downloading the latest version from the company that makes it. youcan try to break the code on the file and decrypt it, using either unsoftwareor something else. i wrote a decryption program for cybersitter in 1997,and two other programmers wrote a decoding program for cyber patrolin 2000. you run one of these programs on a computer that hascybersitter or cyber patrol installed, and it reads the file, decrypts it,and prints out the list of blocked sites into a text file.the digital millennium copyright act (p.l. 105304) was passed in1998. the library of congress was designated to set out regulations forhow parts of that act would be enforced. part of the act prohibitedtechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.bennett haselton39decryption of certain files perceived to be storing trade secrets of the company that produced them. the library of congress, which had been following the controversy regarding third parties decrypting lists of sitesblocked by blocking software and criticizing them, specifically said thatthe act of decrypting the list of sites blocked by a blocking program wouldbe considered exempt from this law. but at the time these programs cameout, there was no such exemption, so many people were worried aboutthe consequences.if you have a server product installed on the internet serviceproviderõs system, then you do not have access to the file where the list ofblocked sites is stored. in that case you need to do a traffic analysis instead of decrypting. the hard way is trial and error, looking at your favorite sites in a directory like yahoo. the easier approach is to run a list ofsites through the program. i have written scripts that run a large numberof urls through one of these programs and record exactly which onesare blocked. this takes some programming skill, and third parties whoreview this type of software generally do not go to this much trouble.reviewers for consumer reports or pc magazine usually just use the trialand error approach. the flaw in that approach is that if you want a smallsample of sites and you get them from a place like yahooñperhaps sitesin one of yahooõs pornography categoriesñthen you will get an overlygood impression of the software, because the software gets its list of pornography sites from the same type of place. any good program shouldblock 100 percent of those sites. you want to test a larger sample of sitesto get a more reliable accuracy rate.in one study, we took a cross section of 1,000 dotcom domain namesfrom the files of network solutions, which keeps track of all 22 million(and counting) dotcom sites. we wanted to do a random selection. theproblem was that if the blocking error rate came out too high with a random selection, then anyone could claim that we stacked the deck by nottaking a really random sample. this is a deeply politicized issue, and thecompanies knew me as someone who had strong feelings about it. itwould be too easy for them to say that we must have cheated by using adisproportionate number of sites that we knew were errors. therefore,we took the first 1,000 dotcom sites in an alphabetical list of all of thesites, because the first ones are not any more or less likely to contain errors than the rest of the list. they all began with òa1,ó i think.this report is linked to my subpage. you can see the 1,000 sites thatwe used and the ones that are blocked and which ones of those we classified as errors or nonerrors. the sites that we classified as inaccuratelyblocked were cases in which we believed that no reasonable person couldpossibly believe that they were accurately blocked. these sites were aboutthings like plumbing, aluminum siding, or home repair toolkits. theretechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.40a critique of filteringwas absolutely no doubt that these were errors; we did not encounter anyborderline cases at all. i did the analysis again using 1,000 random dotcom sites, and, for all cases, it looked like the result was within 10 percentof the error rate we got doing it the alphabetical way.we publicized this report with a strong caveat that the second digit ofthe error rate should not necessarily be taken as accurate. for example, ifthe error (false positive) rate is 50 percent, we are saying that 50 percent islikely to be close to the actual error rate. if a company claims that it is 99percent accurate, and we get 30 blocked sites and 15 of them are errors,we can determine with almost 100 percent accuracy that their 99 percentfigure is false. our 50 percent figure could indicate an error rate anywhere from 30 percent to 70 percent, but we definitely can say that 99percent accuracy is a false claim.of the 1,000 dotcom sites in the study, programs blocked anywherefrom 5 to 51 sites. of those blocked sites, how many do we feel wereerrors? in the case of the five blocked sites, the error number is not meaningful. in the case of 50 blocked sites, there is a certain spread of error.the intent was not so much to come up with a hard number for accuracybut rather to address the question of whether the ò99 percentó claims aretrue.here is what we found. cyber patrol blocked 21 sites, and 17 of themwere mistakes. these were not borderline cases at all; these were sitesselling tool hardware, home repair kits, and stuff like that.1 the examplesof blocked sites are listed on our page, so you can verify which sites fromthe first 1,000 were recorded as blocked or not blocked. we took screencapture images of the sites being blocked, showing the message, òthis sitehas been blocked by this software.ó obviously, screen capture is notproof, because it is trivial to fake an image. but there is a danger of people1bob schloss asked whether the same host might be hosting both a pornographic site anda hardware site, and, because of the way in which domain names, ip addresses, and portnumbers are mapped, the hardware site ends up blocked along with the pornographic site.susan getgood said cyber patrol formerly contained a bug that allowed this to happenñwhich peacefire.org may have known about and used in designing the test. she said thetechnical problem involving hosted servers has been solved in all network versions used inschools and libraries. bennett haselton noted that the companyõs web page specifically saidthat material does not have to be blocked because it shares an ip address with anotherblocked site; if it is true that ip address sharing is the cause of blocking, then this is a falseclaim. the web hosting issue has been around for several years and also applies to proxyservers. the bess filtering system and the parental controls of america online see the hostname, not the ip address, of the site that a user tries to access, so they should not have thisproblem.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.bennett haselton41being suspicious that the study was done incorrectly, that there was a bugin our scripts to record the number of sites blocked, or maybe a site wasdown at the time and we mistakenly entered it as being blocked.a rate of 17 errors out of the first 1,000 dotcom sites on the list extrapolated across the entire name space of 22 million dotcom sites yieldsa figure of several hundred thousand incorrectly blocked sites in the dotcom name space alone, not even counting dotorg and dotnet namespaces.surfwatchõs error (i.e., falsepositive) rate was 82 percent; it blocked42 sites incorrectly and 9 correctly. even though the same companyowned surfwatch and cyber patrol by that time, the lists of sites theyblocked turned out to be different. aolõs parental controls, which supposedly uses cyber patrolõs list, blocked fewer sites, possibly because itwas using an older version or because the list was frozen after they licensed it from cyber patrol. when we found the surf watch number, weknew that we had better get all the backup documentation we could possibly get, because there was such a high error rate. the reason that peopledo not get these high error rates when casually testing the software is thatthey test their favorite sites or sites that they know about, and errors inpopular sites already have been spotted and corrected. they get an overlygood picture of how well the software works.people spend a certain amount of time on sites that everyone elsespends time on; however, people also spend time on sites that are lesspopular. therefore, we are concerned about errors in the less popularsites, even though we know that the popular sites contain fewer errors.moreover, the surfwatch error rate is not okay if you are one of those 42sites blocked incorrectly. we plan to do a followup study in which welook at the error rates in a sample of 1,000 sites returned from a search ongoogle or alta vista, in which the more popular sites are pushed to thetop. i expect that the error rate in that sample will be lower, because thepopular sites are weighted more heavily.this study measured only the percentage of blocked sites that aremistakesñfalse positives. it did not measure the percentage of pornographic sites that are blocked, or the percentage of nonpornographic sitesthat are not blocked. if we use either of those numbers to judge a program, then we run into a problem. to determine how good the programsare at blocking pornography, we first would have to find out how manyof the 1,000 dotcom sites are pornographic and then see how many areblocked.we used the same 1,000 dotcom sites for every program except bess(a filter made by n2h2), which blocked 26 of 1,000 sites, 19 appropriatelyand 7 by mistake. we did the experiment first with surfwatch, and thatone was published first last august. we thought the other companiestechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.42a critique of filteringmight have heard about the first study and perhaps fixed their programsto block fewer sites incorrectly in that small 1,000 site sample. it turnedout that none of them apparently had heard about it, because their errorrates were the same as beforeñexcept for bess. in bess, we observed aclean break in the error rate pattern. we took the first 2,000 dotcom sites,and the first 1,000 contained no errors; but right after that, the error pattern appeared.2 technically, all they did was fix errors in their software,so can we accuse them of cheating or not? they removed errors from thesample that they knew we were using, so we used the second set of 1,000dotcom sites.our conclusion from this study was that the people are not actuallychecking every site before they put it on a list. if there are 42 errors in thefirst 1,000 dotcom sites in a list, then there is no way of knowing howmany errors will occur throughout the entire space of 22 million. thisdoes not necessarily mean there is a conspiracy at the highest levels in thecompany. the most innocent explanation may be that some intelligent,lowerlevel employee whose job it was to find these sites may have written a program that scoured these sites and added them to the list automatically, without the person having necessarily having to look at themfirst. there is not necessarily an explanation for how someone could havelooked at one of these sites and determined that it was offensive.the borderline cases receive a lot of attention, because someone bringsthem to the companyõs attention and they have debates about whether ornot the blocking is appropriate. this happened with an animal rightspage that was blocked by cyber patrol, for example. there was a discussion about whether the depictions of victims of animal testing were appropriate. but the vast majority of blocked sites that have not been viewedare moving targets, because if you raise the issue of these sites, then generally the company will fix the problems right away. then it becomes aquestion of finding more blocked sites. that was why we did the studyusing 1,000 dotcom sites, so that, even if these specific errors were fixed,the fact that we found them in this crosssection says something about thenumber of errors that exist in the list as a whole.sites can be blocked erroneously for reasons other than a lack of human review. in an incident that became the baseline in discussions aboutthe appropriateness of blocking software, time magazine wrote an onlinearticle about cybersitterõs blocking policies and the controversy over2david forsyth suggested that the substantial difference in results between tests of 1,000sites and tests of 2,000 sites means that 1,000 sites is too small a set with which to conduct anexperiment like this.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.bennett haselton43the blocking of a gay rights advocacy groupõs web pages. cybersitterput pathfinder.com, time magazineõs domain name, on its list. themagazineõs web site has an article written after cybersitter blocked thesite, which is good, because otherwise nobody would believe me. at theother end of the spectrum, i sent email to cyber patrol saying that theamerican family association (afa) web site, the home page of an extremely conservative organization, should be blocked as a hate site because of the amount of antigay rhetoric. because most programs that publish definitions of hate speech include discrimination based on race,gender, or sexual orientation, cyber patrol agreed to block the site. it isstill on the list today.this is an example of controversial blocking. many of cyber patrolõscustomers would not block this type of site themselves. many filteringcompanies, in their published definitions of hate speech, have paintedthemselves into a corner by including discrimination based on race, gender, and sexual orientation. there are many extremely conservative religious organizations, reasonably well respected, that publish speech denigrating people based on sexual orientation. it does not have to be hateful;it just has to meet the discrimination criteria. (òi hate rudy giulianió isnot a hate site.) even though antigay hate speeches generally are considered politically incorrect, it is not so politically incorrect that many peoplefavor blocking it in a school environment, the way they might favor blocking the ku klux klan web site.we did an experiment a couple of months ago in which we nominated some pages on geocities and tripod to be blocked by surfwatch,cyber patrol, net nanny, and some of the other companies, saying thatthe quotes on the pages constituted antigay hate speech. the quotes saidthings like, òwe believe that homosexuality is evil, unhealthy, and immoral and is disruptive to individuals and societies.ó the companiesagreed to block the pages. then we said we had created these pages, andthey consisted of nothing but quotes taken from the focus on the familyweb page or the dr. laura web page. we asked the companies if, to beconsistent, they also planned to block these sites as well. so far, all thecompanies have declined to do this. net nanny was the only one thatresponded, saying it would consider blocking the subpages of sites thatcontained the material that was blocked when copied to the other page.but about 6 months have passed since then, and the company still has notdone it.we concluded that an unspoken criterion for whether or not to blocka page is how much clout the organization that owns the page has andwhether it could incite a boycott against the filtering company. if dr.laura talked on her radio show about how cyber patrol or surfwatchblocked her web site, this has the potential to alienate a good proportiontechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.44a critique of filteringof potential customers, as well as possibly leading to a situation in whichsomeone sues a local school or library for blocking access to politicalspeech. if conservatives join forces to raise a legal challenge to speechblocked in a school or library, then it becomes a larger problem. evenwithout that experiment, the point is still valid. the companies say theyblock speech that is discriminatory based on race, gender, or sexual orientation. yet we have examples of unblocked sites run by large or wellfunded groups thatñno reasonable person could disagreeñmeet thatdefinition.3we recently published two reports about web sites blocked by various programs. these reports are linked to our main page. one is blindballots, about candidates in the u.s. elections in 2000 whose web siteswere blocked; these candidates included democrats, republicans, and onelibertarian, blocked by bess and cyber patrol. the other report is amnesty intercepted, about amnesty international israel and other humanrightsrelated web pages blocked by programs such as surfwatch, bess,cyber patrol, cybersitter, and some of the others.these reports were published just before the u.s. congress passed alaw requiring schools and libraries to use blocking software if they receive federal funding. i think the reports will still come in handy later asthe debate continues about the appropriateness of blocking software. justbecause these reports did not stop passage of the law does not mean thatthey will not be used as evidence in the court cases to be filed regardingthe legality of the law.there is a question about whether some of the more obvious mistakesmade by blocking software can be avoided if you disable the function thatdynamically examines pages as they are downloaded and blocks thembased on certain keywords. if the list of blocked sites was assembledusing keyword searches, and if the pages were not necessarily reviewedfirst, then the keyword blocking cannot be turned off if the software isinstalled in an environment (such as a library) in which the administratorwants to be extra careful about not blocking sites that should not beblocked.3susan getgood said that cyber patrol reviewed the four pages that peacefire.org createdand blocked them. the company also reviewed the four source sites but decided not to putthem on the list. cyber patrol does block afa.net and will continue to do so; afa promotesa boycott of disney because it offers samesex partner benefits. getgood said that cyberpatrol is not afraid of an organizationõs clout; she receives mail from the afa every 2 monthsasking for a site rereview, which is done. bennett haselton said that the afa is less mainstream than other groups focusing on the family, such as the family research council, whichhas a large lobbying group in washington, d.c.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.bennett haselton457.4circumvention of blocking softwareblocking software can be circumvented. the easiest way is to findpornography that is not blocked. if you run a search, it is not difficult tofind unblocked sites. everyone who runs a search, with small changes inthe query, will get a completely different list of results, so you often findat least one site that is not blocked. you also can disable the software,either by moving files around or by running programs to extract the password. i have written some of these programs. i wrote them because thestandards that people use to determine what is indecent and pornographicstrike me as arbitrary and silly. i have never heard an explanation forwhy a manõs chest, but not a womanõs chest, can be shown on tv. thecompanies that make the software are reinforcing those standards of decency.whether parents should have a right to filter is still a political issue. ithink that rights are more abstract; it is difficult to talk about them. iwrote these programs because i believe that no harm is done if you seesomething that your parents do not want you to see. all of us can think ofthings that our parents did not want us to see when we were growing up.all of us can think of examples of when we thought they were wrong, andsome of us still believe that they were wrong.people would not use a program like this just to find pornography,because it is trivially easier to find pornography than to disable the software. people use such a program if they need to access a specific site thathappens to be blocked. this is either a borderline case, like a sex education site, or something that you do not think should be blocked at all.people have asked me whether i think nothing ever should be blocked. iusually give the example that, if i had a friend whom i thought was depressed and likely to read something that might provoke suicide, then imight go out of my way to try and stop him or her from reading thatmaterial. what i would not do is say, òif theyõre under 18, then i have theright to interfere, but if theyõre over 18, i canõt stop them.ó i think thatcriterion is arbitrary and silly, and that itõs a red herring people use toavoid thinking about the real censorship issues at stake.anonymizer.com is a site that enables you to circumvent blockingsoftware. you can connect to a thirdparty web site through anonymizer,which has a policy of not disclosing who is being redirected to connect toa site. anyone can circumvent blocking software by going to anonymizerand typing in the site that they want to access, because blocking softwarelooks at the first site you connect to, not the url. however, all blockingsoftware blocks anonymizer. we never make a big deal out of this, because it is not something worth complaining about. safeweb is a site thatdoes the same type of thing.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.46a critique of filteringtranslator services also are blocked. babelfish.altavista.com is a sitewhere you can type in the url of a foreign language site and the wordsfrom that language will be translated to english, or vice versa. the rationale behind blocking this site was that otherwise the pictures would comethrough. but babelfish cannot be used to access images because it doesnot modify the image tags. (the images are loaded from the original location because babelfish does not want that data traffic.) the text comesthrough translated (poorly) but the images are blocked. we published ashort piece on why this was probably an unnecessary overreaction on thepart of the blocking software, because the text is converted and the images are not accessible.the third example is akamai.com, a content distribution service. ifyou sign up, then the images on your siteñinstead of being loaded fromyour siteñcan be loaded through akamaiõs server to save on your bandwidth costs. it is a caching service with servers distributed around thecountry. a person who requests one of these images will get it directlyfrom the server closest to them. it is a complex scheme that can shaveseconds off the load time of a page, so many people place a high value onit. the catch is that a loophole in the software allows you to put any urlon the end of the page, and it will fetch the page through akamai anddeliver it to you.4we pointed this out last august, but it still works. some people knewabout it before then; they had just published a page on how to use thistechnique and how often it works to unblock a blocked site. the problemis that if the blocking software companies were to block it, they also wouldblock many banner ads served by akamai. it is used mostly for bannerads to save on bandwidth costs. large sites, such as yahoo, also use it toserve their own images.programs installed on a network are more difficult to circumvent bymoving files around or disabling the software locally, but you can circumvent them by finding unblocked pornography or using the akamai trick.in addition, if you have the cooperation of someone on the outside willingto set up an anonymizertype program on a server, then you can gothrough that program to access whatever you want. this is becomingeasier to do, and people are starting to publish smaller and more lightweight versions of anonymizer that anyone can put on a web page as asecret source for them and their friends to use to tunnel through and ac4milo medin emphasized that this is a bug, which should be fixed, as opposed to a genericissue.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.bennett haselton47cess blocked sites. we are working on one of those. it does all kinds offancy things, such as scrambling the text on the source page and usingjava script code to unscramble the text and write it. the censoring proxyserver cannot block the page unless it parses the java script to figure outwhat the actual text is.to summarize, two points are important. first, a significant percentage of blocked sites have not been reviewed by humans. this situationmay be due to honest errors, such as ip address sharing or employeeswhose eyes are glazing over. but one way or another, significant amountsof content are blocked that should not be. second, it is easy to circumventblocking software.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.488authentication technologieseddie zeitleri work in information security and would like to provide a businessperspective on the difficult questions this committee is addressing. security implementations must resolve whether the measures are to protecthonest people from honest problems or are to provide ironclad solutions.the answer makes a big difference in what we implement. in addition,we are chasing technology. if i were trying to subvert a secure system, iwould wait for the next communications protocol to be implemented orthe next revision to the operating system to be installed. we have unlimited opportunities with computer systems to change whatever works today into something that will not work tomorrow.8.1the process of identificationi will approach identification and authentication from the perspectiveof the individual, that is, how a child or person is identified to a system.we prove who we are in a number of ways, such as with a driverõs license,passport, badge, signature, or fingerprint. when i provide an identifier toyou (or tell you who i am), that identifier needs to be authenticated. inthe computer world, we use something you know (e.g., a password),something you have (e.g., a credit card with a magnetic stripe), or something you are (e.g., a face, a fingerprint, a retinal scan) to authenticate anidentity. note that, usually, my possession of an identifier does not authenticate my identity.some authenticators are much more secure than others. we all knowand love our fourdigit personal identification number (pin) and passtechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.eddie zeitler49word authenticators. however, administrators of multigigabyte orterabyte databases have password authenticators that are necessarily 20or 30 characters long. the authenticator, whether weak or strong, needsto be verified.1 this is where we tend to run into trouble. the process ofverifying the authenticator requires a trusted source. in the example of adriverõs license, we trust the department of motor vehicles (dmv). thepicture on your driverõs license is the authenticator. to identify a personyou look at the picture on the license, you look at the person presenting it,and say, òyes, i have authenticated that this is your license and i nowbelieve your identity.ó the reason this works is that i trust the licensebecause i trust the dmv. if we did not trust the dmv licensing process,then we would not use a license for identification.if you sign something to authenticate yourself, i have to verify thatsignature against a trusted copy of your signature. the trusted copy i useto verify it against gives me the confidence that you are who you say youare. for example, a bankõs trust is based on properly issued signaturecards.a token typically is not a sufficient authenticator by itself because itcan be passed aroundñit is too mobile. but if implanted permanently insomeoneõs head, that token probably would have some validity. if i havea microchip embedded in my skull at birth by a national security agency(nsa) surgeon, and the nsa verifies the chip when i walk through magnetic readers, then i would trust it. but i cannot think of anything lessdraconian that would suffice to make a token a valid independent authenticator (we tend to use them in conjunction with other authenticatorssuch as pins).in summary, the ability to identify a person depends on confidence.you have to have confidence in the authenticator, the issuer and issuingprocess of the authenticator, the source of the information used to verifythe authenticator, and the process used to verify the authenticator. asystem that identifies millions of people must have very high confidence.for example, in the case of automated teller machine transactions, a verysmall error rate in identification would make them unacceptable. if youdo not have enormous confidence in the identification process, it is not1david forsyth gave the following example: he has a piece of paper given to him bysomeone trusted that says, òdavid forsyth knows the factors of this very long number.ó hegives someone else that piece of paper and tells the person these factors. in the authentication, that person says, òwell, if you cannot trust the person who gave you the piece of paper,then the whole thing will not work.ó eddie zeitler added that verification means that heknows that the piece of paper actually came from the person from whom forsyth said itcame. he has verified the òsignature.ótechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.50authentication technologiesappropriate for use by a large population (including some who may betrying to defeat the system).8.2challenges and solutionsin the digital world today, technology is rarely the problem. technology is changing so fast that, if a problem is not solved today, then it willbe solved next week. note that the opposite is also true. a technologythat is secure today may not be secure tomorrow. today we have veryhigh confidence in digital signatures based on public key cryptography.2the digital signing processes are good. we are able to identify, authenticate, and verify a person and his or her age very easily using digital signatures. however, the authentication and verification processes areproblematic. if they really worked, then the banking community, the brokerage community, and the rest of the financial world would have implemented them years ago. we have the technology to create digital signatures that we all trust, but we do not have an infrastructure in place thatmakes this process workable.the private key that you use to create your digital signature will be1,000 to 2,000 characters long. where will you put it? it has to be stored inan automated device of some sort. to date, smart tokens, or smart cards,are the best answer. note that if i put my private key in my computer, wewould be authenticating the computer, not me. what i want is somethingthat, wherever i am, can be plugged into any machine to identify me. i donot want it to identify the machine, because then others using that machine could also identify themselves as me if they knew how to use thesigning software, which, if they have possession of the machine, they canfigure out how to do.if we use cards, there must be universally compatible software, cardreaders, and signing processors. i have been involved in writing american national standards institute (ansi) standards for banking, and òuniversally compatibleó is more difficult to accomplish than it is to specify ina standard. we rarely achieve it. in software today, the signature processis fairly standard but the interfaces tend to be different.another thought is that if i have my secret key in a personal device(smart card), then i can use that secret key to create a signature. to au2a question was raised as to the applicability of zeroknowledge proofsñproving something to someone without revealing anything that you know. but this has not proved to bepractical. some years ago, i (zeitler) delved into zeroknowledge systems and found outthat, at least for the bank of america, they did not make a lot of sense.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.eddie zeitler51thenticate the person using that card to the signing system, we typicallyrequire a pin (usually four or six digits). remember, security is only asgood as its weakest link. we have sophisticated software, complex technology, and great cryptography, and it all depends on a pin.then we need a trusted authority to verify the digital signature, someone to say, òyes, that really is ed zeitlerõs signature.ó since it is a digitalsignature, it must be something more than comparing one piece of paperto another piece of paper. you would go to the agency that issued thesecret key and ask, òis this signature based on this personõs secret key?óthe agency would respond. note that i have to trust that agency.if i am the agency giving you a private key to use to create your signature, i had better know to whom i have given it. so far, the only way wehave found to accomplish this is in person. that is how you get a driverõslicense. banks want some verifiable form of identification from you intheir branch office. in the financial world, there are many stipulationsthat you know your customer. however, in the online world, banks andbrokerage firms do not strongly verify the identity of their customers anymore; they have necessarily resorted to less secure verification processes.a very secure process and database are necessary to assign cryptographic keys. the people who assign those keys had better have themlocked up tight and require strong authentication of a person requestingthem. a digital signature cannot be created with a fourdigit pin forauthentication. if we do not have a lot of trust in this process, it becomesa house of cards that comes apart, regardless of the zippy technology used.today we have digital signature software on all browsers, which isgreat. we were all applauding when that happened. but we still do nothave card readers. we do not have a practical way to issue private keys tomillions of people or a practical way to store those keys. the nsa andnational institute of standards and technology (nist) have ventured intothis area and have not been successful.we do not have a trusted party to issue cryptographic keys and verifydigital signatures at the national level. u.s. government intelligence agencies would not be satisfactory to the private sector. the trusted party doesnot have to be a government agency, but what other organization has thepresence? when we started developing public key cryptography, wetalked about the u.s. postal service issuing keys. there are also liabilityissues. for example, if the post office managed the keys and a majorbreakin occurred and the whole country lost the ability to process publickeys (or digital signatures), whom would you sue? on the other hand, ifit were a private concern, that probably would be the end of that privateconcern. what type of liability do companies such as verisign, whichissues cryptographic keys to the public, have? they have been addressing this issue for years and are comfortable that they have a workabletechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.52authentication technologiessolution. but i am not comfortable with that, because if verisignõs datacenters were to blow up, people would have little recourse.despite the security flaws, electronic banking works fairly well. iworked in a retail company as the chief technology officer years ago, andi moved to a bank from there. i was amazed to find that the retail databases and systems had much more security than the banking systems atthat time. interbank wire transfers and the like were done in a rudimentary fashion. anyone who knew the system could break it or cause damage. but the reality is that there was very little loss. there were reciprocalagreements between banks. if i sent you a $100 million transfer and realize this afternoon that, oops, it was fraudulent, then the receiving bankwill give it back, in most cases. in banking, when you get to the top, onlya few people are necessary to make a phone call to gain agreement that,òyes, weõll take care of that.ó although real attacks have been madeagainst our systems, if you want to steal a million dollars, it is still mucheasier to make friends with the branch manager than to figure out how tobreak into the automated money transfer systems. security technologyhas tended to stay a step ahead of what is practical in the world of financial fraud.to get back to the beginning of this talk, the definition of ògoodenoughó security depends on the problem to be solvedñfourdigit pinsmay be sufficient in many cases. however, for the purpose of this study,limiting the solution to school or public library computers is vastly different from the problem of identifying a 9yearold using any computer toaccess the web. most of the computers to which children have accessprobably will not be run by federal, state, or local governments.3 a strongidentification process will be required.3bob schloss suggested that there are more incentives for people to steal $100 million or toget the right to launch a nuclear weapon than there are for a 9yearold to use a schoolcomputer to see something that his teacher does not want him or her to see. ordinarily, theschool district gives the smart card to the teachers, who use it to set filters. you cannot forgethe pin. but will one kid who is a computer genius write a device driver that he loads intothe computer so that it steals the secret number? milo medin suggested wryly that he couldsimply download it from peacefire.org.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.539infrastructure for age verificationfred cottonmy background is predominantly law enforcement, so i come to thisissue having tried to clean up the results of many societal problems, and isee what is going on in the streets. i agree with eddie zeitler about authentication and verification. you have to watch it work in the real worldwith driverõs licenses. you can book an individual into the county jail andrely on fingerprint information that does not come back to the right person. you will face these problems anytime you try to superimpose authentication of age onto the real world.9.1the real world versus the internethow, and to what extent, is interaction with a human being needed tovalidate identity? who will validate the validator? who is it that youtrust to say who somebody else is? that level of trust does not exist in anylevel of government these days. what level of confidence is needed forthe accuracy of an assertion of age to pass the legal requirements? thelaw will define that for you. if you foul it up, you will know. just as withany other problem in society throughout history, the lawyers will solve it.they will find the tort in the problem, find the person or persons responsible for the tortñeither directly or vicariouslyñand then sue their shortsoff. the necessary level of confidence will be defined rapidly as soon asthe legal community determines that there is money to be made from it.what infrastructure is needed to support age checks outside theinternet? we have an existing infrastructure for dealing with credit cards,fingerprints, biometrics, chips in your head, and other things that can betechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.54infrastructure for age verificationused today. but these things are cost prohibitive and not widely disseminated. to have any kind of authentication process, it has to be globallydisseminated; otherwise, there is no standardization. the problem is dissemination. credit cards are great in the united states but not in themiddle of africa and other places around the global internet where theinfrastructure does not exist. in third world countries that are developing sites that deal with child pornography and child exploitation, implementing online authentication and age verification technologies is a wholedifferent business.cops dealing with problems online tell us that the problem is that ourlaws only extend as far as our borders, and, historically, our ability toregulate or influence things extends only as far as our laws. our laws arebased on how much territory we can hold with a standing army. this hasno application on the global internet. it is a totally new environmentñabrave new world. there is little we can do other than talk about it, because nobody owns the internet and nobody runs it. nobody has any sayover it other than the people who use it. it is truly a democratic society.when the people who use the internet get tired enough of something,they will do something about it, independent of government.has the internet environment changed the necessary infrastructure?obviously, we cannot superimpose the existing structure on the internet,because of its global and nebulous nature. if you are going to validateidentification online, then it has to be standardized to some extent. if youare validated through abc signature company, and i am a retail merchant who subscribes to xyz but not abc, does that mean that you do notget to buy from me? this is probably not going to work well, and something will need to be done about standardization.what are the costs to the user and to the government? who will maintain the database of validation? this is a huge responsibility, a huge cost,and a huge security risk. if you blow that one, you are guaranteed to getthe legal community involved.how reliable is the technology? it is reliable today, but tomorrowbrilliant little johnny in the class will figure it out. it only takes one littlejohnny to figure it out, and then he automates it and gives it to all theothers. we have seen this in computer security for years. it does notrequire much skill to hack. all you have to do is download the tools thatsomebody who had the skills to write them made available. it is a pointand shoot operation.other things that we do in the real world in age verification may ormay not have application here. a driverõs license is an official id becausewe have an official government entity. it is well funded and well staffed,and it requires that you show up to prove who you are before you get thetoken or identification. it is very difficult to do that on the internet. i cantechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.fred cotton55apply for a credit card through the mail, and i call the issuing company toactivate it, and no one there ever actually sees my face. but credit cardfraud is easy to commit. people just throw those forms in the garbage. icould go through your garbage and pick up those applications, fill themout, put in a change of address, and charge things in your name. thishappens daily. identity theft is huge. once you are in that particularloop, getting out of it is next to impossible.biometric technologies and fingerprint scans are possible, but it is costprohibitive for both the user and authentication organization at this time.in addition, the initial validation is always a problem with anything thatyou superimpose here. tokens are too mobile. we see that with identitiesnow. we have juveniles buying alcohol over the counter with false ids,which are not difficult to forge.historically, law enforcement protection is a threelegged triangle. itinvolves enforcement, education, and prevention. of the three, educationis probably the cheapest. this is where you get the most bang for thebuck. you simply get people to change their ways by telling them thatsomething is not right, and that it is not in their best interests. so far wehave not been very successful with things like narcotics. if we could getpeople to stop wanting children to access pornography on the internet,then it would go away.that leaves you with the other two legs of the triangle. preventioninvolves giving parents and teachers some tools that they can use to try tostem the flow. the tools will not stop it but will give them some controlover their own part of the environment. the third aspect is enforcement.we find the people who are bringing this grief on us and we bring grief onthem, or we find the biggest offenders and put their pelts on the fence as awarning to others. historically, that is what enforcement is about. we getthem to the point where they do not know if they will be next, and theykeep their heads down. if they all decide to do bad things at once, there isno law enforcement agency in the world that can prevent it. but we cankeep them on their toes enough that they will think twice before they doit.everything i have talked about so far deals with the web, the leastoffensive of the content problems. how does any of this technology affectemail or usenet? the worst offender is internet relay chat (irc), whenkids are involved in that arena. i train 30 task forces around the countryto do nothing but go after online predators, people who will get on anairplane and go find a child for sex. they spend months and monthscultivating that situation. you would not believe the astronomical numbers involved. in that type of environment, all of the screening softwareand age verification do no good. technology will not solve this particulartechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.56infrastructure for age verificationproblem. right now, the only thing that is having an effect is enforcement. we are at least identifying the offenders and taking them out ofcirculation as fast as we canñsurgically removing them from society bywhatever means is currently socially acceptable.if you could keep kids off email and internet relay chat1ñthat is, ifkids accessed the internet in a way that worked only through the web,but portedñthen it would eliminate access to children for most of thesepreferential sexual offenders. but you would also eliminate a lot of thingsthat kids use the internet for; it would be like keeping kids out of the parkor off the telephone. irc has replaced the telephone after school, and thatglobal circle of friends is a strong social draw. for latchkey kids afterschool, this is their way of communicating nowadays. with usenet, if theywant to surf for porn, then they will find a public news server and pull offwhatever they want. screening does little about that, particularly with allthe things that are mislabeled.9.2solutionsany successful effort to keep pornography away from children willhave to draw from all available solutions; you need a bit of everything tomake it work. no one model will be successful by itself, but, when combined, they likely will have some impact. the degree of impact will depend on the social acceptance of this effort in the long run. the availablemodels include the following:¥age verification and validation is a positive id model. before i canget in somewhere, i must prove that i am an adult. this lends itself to theuse of tokens, or what i have and what i know. but this leaves us with theproblems mentioned earlier concerning who controls that database andwho keeps track of that information.¥the supervision model does nothing at the technological level, butrather has parents supervise kids online. if you put your kids online, thenyou do not throw them into an electronic pool hall without supervision.you move the computer out into the family room; you do not let kids sitin the back room and do these things all by themselves. unfortunately,the reality is that most parents do not take the time to do this.¥the software model involves the screening softwareñnet nanny,cyber patrol, and the others. with the false positives and so on, this is1milo medin said that he could build a system to do this; the question is whether anyonewould want such a product.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.fred cotton57problematic, but, when combined with the two approaches mentionedabove, it may offer some reassurance.¥the law enforcement model says we go out there and increase ourpresence online, so it keeps the predatorsõ heads down and keeps themfrom doing what we want to prevent. they will think twice before engaging someone online, for fear that they are engaging me. this keeps themguessing. this is a fear model.¥the intervention model says we identify the people causing theproblem and enlist the aid of the cybernetwork neighborhood and crimeprevention types so that people who see this activity do not ignore it.they step in and do something about itñthey report it, and somethinghappens as a result. this works with burglaries and territorial crimes.we have to rely on the community to tell us how things are going.¥the education model involves improving education to the pointwhere people see that something is wrong and change their behavior.when you change the behavior pattern, it no longer will be socially acceptable or tolerated by the majority of society.we also need to remove roadblocks in law enforcement that severelylimit what i can do online. the rules currently applied to online situationswere written for telephones, not the internet. we work within very narrow parameters. for example, a recent case in the ninth circuit dealtwith a supervisor going onto a passwordprotected web page under theauspices of a pilot during a pilotõs strike. the ninth circuit said that wasnot right. if i am a law enforcement officer working undercover, whatdoes that mean for me when i try to access a child pornography web site?they do not think about the ramifications and how it affects our ability tofunction online.i cannot just take your computer, go through it, and find out what ison it. i have to write a search warrant, convince a judge that i have probable cause to believe that what i seek will be there, and show proof of thatbefore someone will give me a search warrant. this is wise, of course. wehave these protocols and procedures because you do not want us runningamuck and grabbing everything. however, at some point you have toremove some roadblocks if we are to address new technologies based onlaws for old technology. we have to remove some of roadblocks so thatwe can become effective; but we also have to keep parameters in place tokeep it from getting out of hand. there is a balance.the roadblocks have not been collected and presented in an article orpublication. they are buried in case lawñnot even codified law. theyare buried in the decisions of the u.s. supreme court, district courts, andtechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.58infrastructure for age verificationcourts of appeal, in a variety of cases, and in civil lawsuits.2 agencies areless concerned about protecting you as a citizen than about getting sued.but we have advocates for change. the u.s. department of justice has thetools to do that.the first thing i would do is to protect children online. we have tofind the most egregious cases out there of the providers. i would identifywho is causing the problem. second, if i cannot arrest that person for aviolation of law, then i would sic a whole battery of attorneys and lawfirms on them for a tort violation, basically a violation of my rights. atsome point, they will get a clue that this is not acceptable behavior. all ofthis has to be done within the parameters of the law, but the people causing this problem have to fix the problem. they are causing a problem forthe rest of society and they will have to own up to their part and face theconsequences.criminal prosecution is generally the least effective approach. usingthe law is always available; the pen is mightier than the mouth. but thebottom line is, you need to change behaviors. there is no law west of themodem. look at the development and rapid growth of the internet, andcompare it to the westward expansion of this country in the early 1800s.the same type of thing is happening.behavioral changes will be required on both sides. it will requiredifferent behavior on the part of people being victimized now. they needto realize that they cannot continue to do these things online without thepotential of being a victim. the other behavior we have to change is thatof people who look at the internet as the wild and woolly west, who donot care what they do to anyone else online. you have to change the behavior of children who use the internet at some point and, by default,change their parentsõ behavior. i am not picking on any one group. society as a whole will have to look at this problem and say, òdo we reallywant this to continue?ó3the group causing the biggest problem right now are the offenders,2dick thornburgh said that someone should read all the cases, collect them, and developa strong argument for a remedy.3eddie zeitler said that, as long as society keeps developing new technologies, these problems will arise. a problem is created when someone puts digitized music in a file and thensays you cannot copy it. you cannot commercialize it in the united states, but you can gosomewhere else where there is no law against this. no one can tell you that you cannotmake copies, because you can, and no one can tell you that you cannot use the internet,because you can. fred cotton noted that, if you send a picture of women without veils tosaudi arabia, you have sent pornography. in other words, there is also a nebulous community standards issue.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.fred cotton59the ones sending material to kids unsolicited, targeting kids, going afterthem in a planned and concerted manner. that is the first behavior to bechanged. they need to wise up and realize that this is not appropriate orface the consequences, because what they are doing is a violation of thelaw. sending 12 or 13yearold kids horrific graphic images is unacceptable to me because the kids do not get a choice. if you tell them, òhey, donot go over there, because there is bad stuff,ó and they stay away, then itis fine. but keep the bad stuff over there.you cannot dry up the supply by somehow taking the money out ofit. the sexual predator is not motivated by money but rather by access tochildren. this cannot be managed like the banking model, in which aconcerted effort is made in multiple areas that largely prevents a problem.there are few predators within the banking community, and we tend toget our wagons in a circle when under attackñwe control where moneygoes electronically. on the internet, nobody controls the pornographysupply. you have a widely dispersed supply and a widely dispersed demand, with no central point at which you can install controls.9.3the extent of the problemwhen talking about protecting children online, it makes no differencewhether it is protection from a sexual predator or a pornographer,4 because predators use pornography as a tool to lower the inhibitions of children. i have seen them with cartoons of homer simpson and fredflintstone, telling little kids, òsee, wilma thinks itõs okay.ó there is nodifference; pornography is still being put out there and accessed by children. if children are hooked into it and able to go to another site and feedthat paraphilia (i.e., unusual sexual preference), then it simply serves tolower the inhibitions further. (some sexual preferences are illegal; someare not. child pornography paraphilia happens to be illegal.)this is like watching violence on tv; eventually, you get numb to it.most law enforcement officers see the same thing. finding a dead bodyon the street is not horrific to me any longer; i have seen too many ofthem. to the average citizen, it is absolutely horrific, but i have beendesensitized to it over the last 27 years. this is sad to say, but it is true of4marilyn mason asked whether there are two different, but related, aspects to the pornography issue. on the one hand, there are sexual predators who are trying to make contactwith juvenilesñthe scariest part. on the other hand, there are creators of sexually explicitmaterial who are trying to make a buck by selling it, presumably to adults, but sometimesthey solicit children as well.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.60infrastructure for age verificationmany people who are exposed, over and over again, to things that societydoes not wish to deal with. pornography is one of those things.just because someone possesses or distributes child pornography doesnot necessarily make them a predator. but every single predator whom ihave dealt with in my 27 years in law enforcement had child pornographyñthey possessed it, collected it, and used it to entice someone. predators also use sexually explicit material that is not illegal. the process doesnot take place overnight. my investigators work on these cases formonths. a predator meets a child in a chat room and becomes a friendñtalking about things that they cannot talk about with their parents, lowering their inhibitions. the whole object is to get physical access to the kid.these are the people whom i would go after first, because they are themost dangerous. but there is also a group of them who have set up anindustry that supports this paraphilia. when they cannot get access tochildren, they get access to child pornography, because it is the next bestthing.david finkelhor5 put together a study for the office of juvenile justice and delinquency prevention, published early this year. it was anempirical study of young teenagers online and their contact with sexualpredators. of young girls in the 14yearold age range that were online,90 percent of those interviewed had been contacted with unwanted sexualadvances. several went on to further levels. they were interviewed incontrol groups, too. the numbers were shocking, amazing.how much of this is unique to the internet, and how much is justreflective of society in general? for about the first half of my 27 years, icould count on my hands the number of child sexual abuse cases that ihandled. with the advent of the internet, it has grown exponentially. ihandled 10 to 15 cases in 1989, the first year that i realized there was aproblem. when we started looking at the agencies dealing with it, everyone thought that they were the only one. a segment of our society hasthis paraphilia or would like to explore it or act it out. they use theinternet as the mask they hide behind. they can play whatever personathey want online, because there is no validation of who they are.i think we have had child sexual offenders in our society from thebeginning, but they used to have to go to extraordinary measures to getaccess to children. the internet has made it easy for them. those whomay never have thought of acting out in the real world now have no com5finkelhor, of the university of new hampshire, testified at the committeeõs first meeting,in july 2000.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.fred cotton61punction about doing it on the internet. it is the borderline cases that arecoming out now; this is part of the problem.there is also a phenomenon called validation. if you are into sexuallyassaulting children, then you are universally disdained in almost everysociety in the world. you are the lowest form of bottom feeder; if you goto prison, murderers will kill you because you went after a kid. therefore, when child sexual offenders have the ability to get together in affinity groups they say, òoh, iõm not the only one. i thought i was the onlyone, but there are thousands of me out here. and now we can validate it.we can exchange information about children and target children online.we can find out where they live and go and meet with them. this is awonderful tool.ójust because they talk to one another does not make them easier tocatch. it has made for an interesting enforcement environment, but westill have roadblocks that prevent us from catching them. their internetcommunications are in transit, so, technically, we are using forms of wireintercepts. the law was written for the old days of wiretapping the telephone; it does not apply to an internet chat room. the courts have notdefined this well enough. they have not told us what we can and cannotdo as far as this new communications medium. as a result, law enforcement is more concerned about getting sued over these types of things.we have to be careful how we proceed.but these people are coming out in droves. the numbers are astronomical; i have never seen anything like it, and i see no end to it. children are at risk. can the risk be managed? yes, if we implement a varietyof different approaches, not just technology, we may be able to manage orlimit that risk. but can we eliminate it? absolutely not. can we controlthe global internet? probably not. can we change how people use theinternet through education, prevention, and enforcement? probably.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.6210automated policypreference negotiationdeirdre mulligani worked for a long time on the platform for privacy preferences (p3p),which gives parents some control over the data collection practices at websites visited by their children. there are instances in which children disclose information about themselves that can be used to contact and communicate with them. p3p has no application in the context of limitingchildrenõs access to pornography and other content that might be considered inappropriate.p3p is a project of the world wide web consortium (which also developed the platform for internet content selection (pics)), which enables web sites to express privacy practices in a standard format. thismeans that a web site can make an extensible markup language (xml)statement about how it uses personal data.the basic functionality of p3p is as follows. say that a web site collects information such as name, address, and credit card number for thepurchase of goods, or it uses clickstream data (i.e., the data left behindwhen surfing a web page) to target or tailor information on the web siteto your interests. on the client site, either through a browser or someplugin to a browser, p3p allows individuals to set parameters for thetypes of web sites their kids can visit based on the siteõs data collectionpractices. for example, a child might try to enter a web site that collectsdata from children and sells itñwhich is generally illegal in this countrywithout parental consent, under the childrenõs online privacy protectionact (coppa).1 the browser could be set up either to limit access to web1coppa, which regulates the collection of personal information from children under age13, was signed into law in 1998 and went into effect in 2000.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.deirdre mulligan63sites that engage in that type of data collection or to supply a prompt,notifying the child that òthis web site collects data that your parents havedecided you should not disclose.óseveral products incorporating p3p are being developed. most arebrowser plugins. microsoft will have some p3p functionality in the nextgeneration of internet explorer. as with other web standards, p3p can becombined with other tools and you can plug in certain things, such astrust symbols. you can envision a digital certificate built as an addon toa p3p application. but the p3p specification itself deals with data collection, not access to different types of content.the adoption of p3p had little to do with coppa. tim bernersleeand i gave the first public presentation on p3p at a federal trade commission (ftc) meeting in 1995, several years before the enactment ofcoppa. the technology was not specifically designed to deal withchildrenõs privacy issues; rather, it was designed to address the need forweb sites to be up front about how they handle data, and the need toimplement, on the clientõs side, tools for individuals to make informeddecisions about the disclosure of personal information without having toread all the fine print. p3p is an effort to use the interactivity of the web toget around some of the barriers and costs associated with privacy protection in the offline world.the notion of rating is not part of the p3p specification. there is astandard way of talking in a descriptive fashion, which is different from anormative fashion, about privacy. a p3p statement allows a web site tomake descriptive statementsñnot that their privacy policy is good, bad,or the best, but simply, òwe collect this type of information, and we dothis with it.ó clearly, someone could build a program that makes a judgment. for example, a web site could say, òwe collect everything that wepossibly can about you and sell it to everyone in the world.ó someonecould develop a tool that says that statement equals a bad privacy policy.that tool, in effect, could make a rating based on the descriptive statements.in many ways, pics was an effort to provide the capability to makedescriptive statements about content. p3p does not provide anything newor special in that area. but descriptive information is not necessarily whatpeople are looking for in the content context; they are looking for normative judgments about what is appropriate, and this is much more difficultto build into a specification. there are constitutional, cultural, and hegemony reasons that make such decisions suspect. it is not as straightforward or factual as statements about what data are collected and how theyare used.whether p3p leads to more negotiation and customization of contenttechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.64automated policy preference negotiationdelivery2 will depend on the implementations. there are a wide varietyof implementation styles, and it is unclear how the products will work.part of it will be driven by consumer demand. survey after survey hasdocumented enormous public concern with privacy and a real anxietyabout disclosing personal information, because people feel that web sitesare not forthright about what they do with data.a tool that allows people to gain better knowledge about how thedata are used certainly may allow more personalization. some peoplewill choose personalization because they are comfortable having certaintypes of data collected; if data collection and the personalization it enables are done with the individualõs consent, it will advance privacy protection. if a web site offers the news or sports scores, you might be comfortable telling it which state or county you live in, or your zip code,because the site provides a service that you think is worthwhile. but today you might be anxious about what the site does with the data. if therewere a technical platform that allowed you to know ahead of time thatonly things you were comfortable with would be done with your data,then certainly it might facilitate personalization. but it would be personalization based on your privacy concerns and your consent to the datacollection.with regard to the truth of a siteõs privacy statements, the question ofbad actors is one that we have in every context. there is nothing aboutp3p that provides enforcement, but it does provide for some transparency, which could facilitate enforcement. in this country, people who saysomething in commerce that is designed to inform consumers run the riskof an enforcement action by the ftc or a state attorney general if they failto do what theyõve said. in other countries, there are similar laws prohibiting deceptive trade practices, and, in addition, many countries have lawsthat require businesses to adhere to a set of fair information practices designed to protect privacy. collaborative filteringña process that automates the process of òwordofmouthó recommendations by developingresponses to search queries based on the likes and dislikes of others whoshare interests, buying habits, or another trait with the searcherñis independent of p3p. i have not seen a discussion of its applicability in theprivacy area.2bob schloss gave the hypothetical example of sports illustrated warning that some of itscontent shows people in skimpy bathing suits, and a user agent (or client) saying it does notwant to see sites like this. sports illustrated could offer to present a subset of its contenthonoring the request. but why would the magazine go through such complex programming if only 10 people had user agents that could negotiate? to what extent would there benegotiations in which a site would either collect data or provide a subset of its functionwithout collecting data?technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.6511digital rights management technologyjohn blumenthali am a security architect specializing in digital rights management(drm) systems. i am engaged now in the music and publishing space,but i have a history of looking at rights management in terms of digitalproducts and messaging, email in particular, dealing with issues such asthe unauthorized forwarding of emails in the sense of how conversationsare considered under copyright law and the ability to abuse conversations. i have both a technological hacker perspective and a policy approach that includes a focus on risk management in terms of how to control content.11.1technology and policy constraintshow do we prevent particular types of content floating around on theinternet from reaching certain classes of users? we would like to implement a technological restriction. how do we implement these controls oncontents to contain propagation? the internet is all about propagation.this question raises not only the issue of viewing but also the issue ofownership and superdistribution or forwarding. on the policy and legalside, can this be implemented in a legal structure once you achieve thisònirvanaó of a universal technological solution?is this really any different from the mp3 debate? there may be socialor psychological issues as to why people consume and propagate this typeof content, but fundamentally, to look at the mp3 debate is to stare in theface of the problem. the current crisis in the music industry is that thistechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.66digital rights management technologyformat, mp3, which compresses and renders audio,1 is not associated withany type of use controls. napster posts these files, or references to them,such that users can send and swap the files without any control, effectively undermining the music distribution channel, typically compact diskwith readonly memory (cdrom). the publishers chose not to encryptthe data on cds, for cost and other reasons.2 music on a cd is storeddigitally in a totally unencrypted way, which is why you can make copiesto play in your car.there is no way to control this problem technologically; we can onlycontinue to raise the bar, effectively placing us in the domain of risk management. this is the core problem, which i refer to here as the trustedclient security fallacy. i have complete ownership of this device, literally,physically, and in every aspect, when it is on a network. this means that,with the proper tools, i can capture that content no matter what type ofcontrols you place on me. there are people within @stake who are experts in reverse engineering, which allows them to unlock anything thathas been encrypted. if we attempt a technological solution, then therewill be ways to circumvent it, which then will propagate and becomemuch easier for the masses to use.i believe that policy drives technology in this problem, simply because technology does not offer a complete solution. the only way toattempt a solution to mitigate risk is to adopt a hybrid approach, mixingtechnology and policy. whatever system you come up with in the digitalrights space must be sensitive to these policy constraints. you have todistinguish the type of content in attempting to invoke rights on it andcontrol it. this is a fundamental premise of the way a drm system isdesigned and applied.3these policy constraints create the archenemy of security and contentcontrolñsystem complexities. there are serious economic consequencesfor the technology industry in general, because you are imposing on theend user experience. you are disrupting and removing things, such asfree use of and access to information, that i have become accustomed tousing on the internet. decisions regarding how to implement the policyand technology will affect this industry.1to render means to convert a format into a humanconsumable elementñdisplaying dataas images, playing data as sound, or streaming data as video.2milo medin pointed out that the music publishers themselves created the unencryptedformat in which cds are published, effectively creating this problem. he said we cannotexpect people to use a digital management format that offers them fewer capabilities thanthe native format in which the material originally was published.3references for drm and clientside controls can be found at <http://www.intertrust.com>, <http://www.vyou.com>, and <http://www.oracle.com>.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.john blumenthal67the policy constraints causing these problems are privacy, the firstamendment and free speech, censorship, the legal jurisdiction issue, rating systems (which will become difficult to implement and maintain),copyright and fair use, and compliance and enforcement. these are alldifficult issues.11.2designing a solution to fit the constraintsthis is how i would approach designing a system that conforms tothe policy constraints. some of this is very technical. first, we have todesign a system to operate across all the consuming applications: chat, email, web browsers, file transfer protocol, and so on. this is a massiveinfrastructure. then, given all of the policy constraints, how can we authenticate ageñto determine if a user is 18ñand only age without stomping on privacy issues? the only thing that i could come up with is biometric authentication. a biometric approach can detect who you are. ihave heard that devices exist that can take a biometric measurement anddetermine the age of that measurement, but i do not believe it.4the collector of the information is responsible for enforcing the privacy issues. if you are willing to go deeper into the privacy issue andmaybe involve socalled trusted third parties, porn sites often performage authentication through the submission of a credit card number. thus,if you release some of the constraints, you get more of what you want toachieve. but the problem of hacking is inescapable.5 gaining access topornñsomething forbiddenñis probably one of the most deeprootedpsychological motivations for becoming a hacker in the early stages. talkto any hacker; if there is lurid content, then they want access to it. musicprobably brings them into the same psychological realm.the bigger issue is, now that you provide access, do you permit propagation? in other words, is the authorized user allowed only to view thecontent? this issue has more to do with content consumption than con4herb lin said that he does not believe this; his 6yearold daughter just had a boneagescan, which said she is threeandahalf. milo medin suggested that a blood test probablycould determine age. david forsyth suggested counting the rings in a section of a longbone. herb lin noted that, to be useful legally, a biometric would have to change suddenlyin a significant way between age 17 years and 364 days and age 18 years and 1 day. milomedin countered that a realworld system need not be accurate to within 1 day. gailpritchard summed up the problem by saying, òthe minute i turn 18, i want access.ó shenoted that there are other means for checking a personõs birthday.5david forsyth pointed out the conundrum of òanything i own, i can attack.ó in otherwords, if a parent has an age verification system and a technically creative offspring, thenthe system is essentially meaningless.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.68digital rights management technologytent access. you want to prevent the propagation of certain types ofinternet material. there is a subtle, more hidden issue here. if content isprovided to someone who is authorized and authenticated, and it is rendered, then you are heavily into drm. should the user be permitted topropagate that material to another party such that it is rendered, in effect,in an uncontrolled fashion? the system needs to consider both consumption and propagation issues to provide a whole solution.in the system that i am designing, i will install a virtual vchip. someof you may be familiar with the vchip initiative,6 which led to manydebates and various laws. as of january 2000, new television sets havethis capability. there is a twin effort in the vchip analogy, in which thesocalled client side (i.e., the television, desktop) and the publisher side(i.e., the broadcasters) are driven by policy makers not only to implementthis bar to maintain risk on the client rendering side, but also to come upwith a rating system so that the vchip can look at a stream of art or videoand say whether it is inappropriate content. the parents have set up thisvirtual ratings wall to prevent the rendering of, and access to, the content.as applied to television, the vchip impedes the user experience soonerously that people do not use it. instead, they police the use of television by simply physically being in their childrenõs presenceñor they donot police it at all.7 a lot of work would need to be done with both the6see <http://www.cep.org/vchip.html>, <http://www.fcc.gov/vchip.html>, <http://www.webkeys.com>.7janet schofield said that parents typically do not police their childrenõs television usesystematically. linda hodge said that parents do not trust the filtering system because thebroadcasters themselves set vchip ratings, which are voluntary, and they have no incentiveto use them. janet schofield said that many parents do not believe that the violence seen ontelevision is really a problem, at least not to the degree that they donõt watch things theywant to see because their children will be exposed to it. when she talks to kids about experiments on the connection between television violence and kidsõ behavior, she loses their interest. she said parents or adults would take pornography issues more seriously than theydo violence, so there may be a difference in motivation to use the filter. sandra calvertnoted that the vchip is not designed to censor violence only; it also screens sex and language. it has about five different ratings: fantasy violence, real violence, sex, language, andso on. robin raskin said parents are not using filters on their pcs or aolõs parental controls either, because they do not see the link between entertainment and behavior. part ofthe problem is that the research on this link is 20 years old and not very good. sandracalvert said that people who watch violence but are not incited to kill by it tend to disbelievethe general findings in the literature about the connection, which depends on the individual.but there is a new review article showing a link between playing aggressive video gamesand being aggressive personally, for both males and females. people can become desensitized to violence and no longer pay attention to it. at this time, the culture is not so desensitized to pornography, but this could become a problem.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.john blumenthal69purveyors of this technology (microsoft and intel) and the publishers onthe server side offering up the content. the complexity and impossibilityof this problem starts to avalanche here.a precedent to frame thinking in this debate is encased in an interesting act of 1990 that ultimately led to this technology. the first initiative tolook at is the platform for privacy preferences (p3p).8 i argue that extensions to this initiative, in effect, could implement a rating system. thiswould be done using the extensible markup language (xml), a revolution in the industry and the treatment of content. xml is a natural evolution from html.9 it provides more power and will be the native formatin which all microsoft documents are stored. (today, word is stored in aformat proprietary to microsoft.) the xml processing engines sit insidethe operating system, at least in forthcoming versions of windows; virtually every device in the world will be capable of parsing that type of content. the idea is to modify the processing engine to require a p3p rating.if the description of the p3p rating is not in the content, the processingengine will not render it. this would force everyone in the industry toadopt this standard on a global basis.this idea is not that farfetched. html achieved global status over aperiod of time; xml will achieve similar status over a period of time.xml already is being applied in various ways that have a global effect.the idea of modifying client applications that already use the underlyingxml processing engine is not a stretch either. xml even could be extended to handle commerce material (e.g., from napster). this initiative,which is in front of the world wide web consortium, is achieving standards that are unprecedented. p3p is not a burdensome implementation,either, technologically. it is in line with where the vendors are going witha whole slew of other initiatives.next, you would need to start applying pressure on software industry giants and possibly hardware industry giants, too. in doing so, theentire clientside security fallacyñthat you can control the rendering ofcontent on an untrusted and unsecured hostñmust be recognized. theonly way to compensate for it is through policy, by going after the peoplewho create compromises in reverse engineering of the system itself. the8see <http://www.w3c.org/p3p>.9nick belkin said that, so far, xml has done only what http has doneñformal characterization. no one has had any significant experience with content characterization. if this isdone, then a database is needed that incorporates ontology that describes the whole thing,and someone has to construct and maintain the database. bob schloss said there would bean announcement soon related to this issue by a consortium of companies.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.70digital rights management technologydigital millennium copyright act of 1998 outlaws some of these techniques. it did not stop the decss10 model, but it did end up in court.reverseengineering techniques would permit me to create controlsaround the content of any type of system. reverse engineering unleashescontent across all of computing. it is one of those difficult problems thathave not been solved in the computer science field. embedded systemsraise the bar,11 but you create a cottage industry of reverse engineers whowill get down to assembly level code and remove the actual execution seton the chip and replace it. this is done widely now. there are ways ofraising the bar continually;12 the question is, how far you want to raisethe bar and, in doing so, affect the industry in many different ways.if we implement such a solution in the turbulent waters of the industry now, we would create an interesting and difficult problem. some giants, such as microsoft, want to dominate the contentrendering space,and whoever wins that battle effectively dominates digital entertainment.microsoft is the best positioned to do this, as america online and everyone else knows. the interesting economic and political issue is that theoperating system vendor would dominate this area. if this solution wereimplemented in the interests of policy, then the vendors would scramble10decss is software that breaks the content scrambling system (css), which is weak encryption used for movies on digital versatile disks (dvds).11herb lin said it would be very difficult, although not impossible, to do onscreendecryption. in principle, you could build into the display processor some hardware thatdecrypts data on the fly before they are put on the screen. milo medin noted that suchtechnology is used for highdefinition television. david forsyth said the problem with raising the bar is that you only raise it for one person. the federal courts say that decss isnaughty, but he has dvds stolen from a macintosh that required no programming to obtain.12milo medin said the problem with standards is that computer power increases. a dvdplayer cannot send out raw, highdepth material; it has to be encoded in some way. (a pcdoes not have this constraint.) this requirement is in the license signature process for dvds.all consumer devices have the same fundamental issue. you want to build a standard thatconsumer electronics companies can blast into hardware, make cheap, and make widelyavailable. you want that standard to last for 10 to 20 years. to make an affordable devicewhen the standard is released, there must be a manageable level of complexity and security.but 10 years later, a computer is much faster, and the standard cannot change. anythingthat uses a fixed standard for cryptography is doomed. directv dealt with this problem inthe right way. people often steal the modules and clone them. one superbowl sunday, thecompany turned off about half a million to 1 million pirate boxes. over time, the companysent down little snippets of code and then, all at once, decrypted the code and ran it, and itchanged the way the bits are understood. a flexible crypto scheme is the only way to address this problem. however, it is very difficult to implement in consumer electronics whenyou do not have a data link; it may be easier in the future when everything is internetconnected.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.john blumenthal71to provide a solution, not so much to solve this very ugly problem,13 butrather to control the rendering of music, documents, and images.of course, the client security fallacy continues to hold.14 once someone has developed a way to circumvent the system, he or she can packageit into an application or executable and put it on the internet, and anyoneelse who wants to shut the whole system off just clicks on this application.15 the goal is to raise the bar to a level of hassle so high that only avery motivated individual would engage in cracking it. such safeguardsare all hardware related.16 any solution not hardware related will endup with a oneclick compromise. when you have to crack open a device13milo medin said many stupid ideas are circulating in this space. one idea is to putcontrols in the logic of hard drives so that they will not store or play back files. but as longas the industry wants a cheap, easytodisplay, and easytoimplement consumer electronicsstandard, security will remain elusive, because you cannot have all these things and securitytoo. this is a problem that the industry has made for itself.14milo medin noted that, as long as a generalpurpose operating system is used, someonecan circumvent the system by changing a device driver. in fact, a network makes suchchanges automatically. as long as people can make a change between the xml renderingengine and the underlying hardware, they can get around anything. dan geer said anotherfuture trend is automatic updating by manufacturers on a regular basis. this is done for tworeasons: to ease the burden of updating on the average user, and to handle security problems that cannot wait for system updates. the question of whether the software will run ona desktop internally and belong to the user, or whether there has to be an opening for otherselsewhere to reach in and change it as part of a contract or lease, is outside the scope of thepresent discussion. herb lin noted that automatic updates already are made to nortonantivirus, word, and windows. milo medin emphasized that both the software programsand users can do automatic updates. a provider can trigger an update on the desktop of asubscriber at homeña capability built into the software. but the provider cannot preventthe user from also doing an update.15john rabun said this would be a problem for law enforcement, because many pedophileswould get the chip needed to circumvent the system. however, the system would preventnormal exposure of children to pornography. milo medin disagreed. unless the industrychanges the architecture of pcs completely, there will be a way to intervene in instructionsby loading executables into an operating system and running them between the hardwareand renderer. by contrast, a cell phone is an intelligent device running software that isrelatively secure. people cannot make calls with someone elseõs cell phone because theycannot download programs into it. in the case of the cable modem, the network operator,not the user, controls the code. the problem with pcs is that the user controls the code, andthe operating system does not have trusted segments that interplay with the hardware toprevent circumvention. the situation is different with a settop box, because the operatingsystem is embedded and is managed and downloaded remotely. a user cannot get aroundit because there is no hook to execute.16david forsyth gave the example of region codes in the dvd world. if someone wantedto convert a dvd player into a nonregioncoded player, he or she would have to fiddlearound in the guts of the device. clear instructions can be obtained from the internet onhow to do this, but most people are inhibited from changing the firmware on their dvdplayers.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.72digital rights management technologycase and replace a chip or do something else that involves hardware, youraise the bar pretty significantly. but this is a generalpurpose computer,and the idea of shipping a chip associated with digital rights, which inteltried to do, has not worked.17i am creating a futuristic scenario, drawing on themes in the industryand technology that are moving toward what i am describing. the oldersystems that remain in legacy states would not be able to participate inthe system; they would not be able to render content as easily as newersystems. the king holding all the cards is microsoft, because it is the oneentity that can modify the operating system to require tags on content forrendering. if microsoft took that step, then, in effect, you would drive thepressure back to the publishers, who are saying, òif i donõt rate, then idonõt render.ó microsoft can drive this issue, but this brings you back fullcircle to the question of whether you give it the power to do that.let us fantasize about this world in which content is legislated andrated, effectively much like the vchip. the whole argument over ratingsalready has been conducted on capitol hill, so you would end up with aninteresting and difficult technological problem. how do i know that content is accurately rated and that my p3p profile on my browser rendersthat? how do i enforce the association between the content being postedand the rating that it is purported to have?18there would need to be a law that defines the answers. technology ispart of the solution, but this is difficult technologically. a crawler or pieceof software could wander around the internet, looking at your p3p ratingand then descending into your web site to determine what that contentreally is and whether it is accurately rated. this is feasible, and it is probably an interesting project for some of the best computer scientists in thiscountry. there are things like this on the internet today, not necessarilylooking at porn, but providing other search engine capabilities. this technology will improve over time. you would have to build a componentthat is highly complex and globally capable of crawling around theinternet.17milo medin said intel would still fail if it tried this approach again today, because peopledo not want someone else controlling their computers. robin raskin argued that it is atradeoff between service and privacy; if intel can make the usersõ lives easier, then userswill comply. milo medin said the problem is that consumer electronic companies want tobuild cheap devices without elaborate internal workings. all it takes is for one or two peopleto crack the code and post it to usenet, and it will be replicated all over the place. providingaccess to the content (as opposed to the algorithm) is illegal because of copyright.18milo medin said this is a federal trade commission (ftc) issue. there must be a negative consequence for rating aberrations to change behavior. in the privacy arena, everyoneposted something in the deal with the ftc, and the ftc said it would pursue anyone whoviolated the agreement. bob schloss suggested a default rating, so that if actual rating information is absent, the content is assumed to be x rated and for adults only.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.john blumenthal73realistically, to achieve this system, you would go after microsoftbased on its market dominance in the rendering device itself. if you control that, then you effectively control how things get published to thosedevices. this would be the creation of a vchiplike initiative that goes tothe heart of a much more homogeneous environment than what the vchip vendors were concerned about. technologically, it fits with the p3pprotocol and borrows from classification models, such as label securityimplemented by oracle 9i, that define data and how the rendering clientshould treat them. but it is still futuristic and requires huge global change.another layer you can add is policybased filtering in the network itself.the only way you can approach this problem holistically is with a modelthat layers additional components of control from the network to the client application and operating system to the publisher.the publishers will oppose this because it will limit their market reach.yet they have an incentive to protect copyrights and to have a controlmodel in place. they are all trembling in the wake of the napster crisis.this is why i hold out hope that solving this problem also solves some ofthose issues for them.11.3protecting childreni say it is up to the parent to define a childõs user profile during theinstallation of an application. many applications do this today: aolaccounts, netscape, and internet explorer offer a profiled login. this way,when a child sits down to use that computer, he or she is constrained bythe user profile, which technically becomes intertwined with the p3p profile. once the child gets past a profile login, his or her internet world isconstrained by the definition of that profile.this is in line with how you operate today. the difference is that thecontent you would access in my system would be controlled by the definition of your profile. this link is not strong today; there are no presetrules as to what renders in a browser. i am suggesting that you have todeal with the login issue to gain access to a profile based on your age.this comes back to the question of how you authenticate just age withoutviolating other policy constraints and privacy and so forth. the p3p negotiation occurs at the machine level. for the level of detail in the profile,imagine a sliding bar representing content acceptable to the parent.1919robin raskin said the more granular the p3p negotiation, the less it will be used. systems do not work when they ask parents to make distinctions among, for example, fullfrontal nudity, partial nudity, and halfrevealed nudity; in such cases, parents decide to lettheir kids see everything. a good profile requires a lot of granularity, but to convince aparent to use it, it cannot have any nuances.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.74digital rights management technologythe privacy issue arises not when a person provides access to personal information but rather when someone else records it. if you focuson the client side, then at least you can throw to the privacy advocates abone that says, òall of that information is stored locally.ó but there aresystems in which you need a connection to a remote server, and yourprivate informationñlike a credit card number or some other authenticating tokenñgoes somewhere else. once you do that, the privacy advocates will descend on this like vultures and pick it apart.the adult entertainment industryõs age verification services move theissue of trust somewhere else. when you give your age, you get a challenge response asking you to prove your age by filling out a form. youmight do that with a credit card number or other personal information.you repose this information with the trusted third party. this information could be loaded to say, òyour p3p profile now permits you to see thistype of material.ó but because you send your private information somewhere else, this age verification service, over time, now becomes a list ofnames of people who want access to porn.20 you can see the privacypeople going crazy about the fact that this database is being used for thatpurpose.there is another industry trend that relates to age verification. dangeer is probably one of the worldõs leading experts on this, because hedesigned the system that wall street uses, identrus, which issues digitalcertificates to own identity. the forms that describe the identities in thosecertificates have an age field. there are initiatives concerning the issuance of multiple certificates based on multiple types of identities and useof identity. there is talk in various committees in front of the internetengineering task force about the issuance of agespecific certificates.to obtain an agespecific certificate, you would prove to verisign thatyou were born on the following date and your social security number isx. then you can be issued a certificate to be loaded onto your computer.there is discussion in the public key infrastructure community thatverisign might fill the trusted thirdparty role, in which it would gain nofurther knowledge about you other than your age. verisign has a bunkerthat enforces the limits in physical and legalistic ways. i would feel comfortable proving my age to verisign, knowing that it is legally bound. in20herb lin noted that whoever is verifying the age information does not have to keep alist, even though it would be valuable. if people could be sure that no list was being kept,then the privacy issue would disappear. the difference between cyberspace and the realworld is that, if a person goes into an adult bookstore and shows a driverõs license as proofof age, then the clerk just looks at it and says, òok.ó the clerk does not make a photocopyof it and file it away.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.john blumenthal75fact, verisign exists on a foundation of trust that is assumed when youuse and obtain its certificates.this system might indeed provide the trusted third party for age authentication, and it fits with the public key infrastructure. the problemñand simpson garfinkel and others have pointed to this in the privacydebatesñlies in the metaaggregation that will come in the future. i willget that database; verisign sells data like that. i also will get theclickstream from all the porn sites, and interesting data mining techniqueswill be used to aggregate and combine these data to trace it back to meand say, òyou were the person who did this.ó there is widespread compromise on the server sideñlook at egghead and cd now. this is anuncontainable problem that you do not encounter until after the compromise has occurred.2111.4summarythere are many threats to the system i just designed.22 compliance isa major issue, which the search engine industry is addressing to someextent. bots will be required to crawl the internet for serverside ratingsimplementation; antibots can be created to defeat compliance checking.clientside trojans, worms, and viruses all can be injected into this machine to modify the xml processor. if it has memory, then i can hack it. ifit has a processor, then good reverse engineers can create a oneclick compromise. ratings can be stripped off of content, or interesting techniquescan be used to create content that appears grated to the rendering enginebut is actually xrated. in the secure digital music initiative, they tried towatermark the content to control it; this was hacked within days. thesame thing would happen here. finally, you would face widespread dissemination of a oneclick compromise created by one hacker. òscript kittiesó enable people to click on an attack that someone else created to automate everything i described. the scenario is not very hopeful.21david forsyth said you could prohibit people from possessing certain types of data orusing them in certain ways. you also could punish violators. but the chances of actuallycatching them might be very small. someone could keep a database in a way such that itwould be difficult to find.22herb lin summarized the presentation as follows: to control distribution of content toonly ageappropriate people, you would have to make many changes in the existing technology and policy infrastructure, going far beyond the issue of age verification for inappropriate content. this would offer some benefits but would not necessarily solve the problem.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.7612a trusted third party indigital rights managementdavid maheri designed the secure telephone unit that first used the infamous clipper chipñwhich further illustrated, to me, many of the issues involvedwith trusted third parties. i agree that there are major problems withtrying to control what people do on their opensystem pcs. but we shouldnot give up just because we cannot design a perfect system to prevent ahacker from hacking pcs. there are techniques that can make hackingdifficult, and in particular techniques that can allow business models tobe supported in spite of security breakdowns. when i saw css severalyears ago, my colleagues and i in the secure systems world shook ourheads and said, òas soon as itõs rolled out, it (the crack) will be on a tshirt.ó in fact, it was. but bad security design does not have to be the rule.i agree that a lot of infrastructure will have to be rolled out to takeadvantage of some of the methods and techniques discussed here at thesemeetings, and many things will have to change. we will become moreoriented to digital rights and responsibilities and policies. there will bemotivation to roll out some of these techniques, methods, and standards,not only because of digital rights management for the control of copyrighted material in the media and entertainment industry, but also practically for asset management (in enterprises), where some of the challengesare not quite the same. there is a lot of movement and demand to set upthe infrastructure for policy and control of the deployment of assets, bothwithin an enterprise and among enterprises.the context for digital rights management (drm) has a lot to do withcommerce automation, where you have a publisher who wants to publishinformation, which could be entertainment, pricing information, or a contechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.david maher77tract, and the publisher wants to give access to the right people, who areallowed to exercise the provisions of the contract. just about any piece ofinformation that has some value that someone can exercise some rightwith regard to is the type of thing that you want to be able to control inthis sort of system.12.1intertrust technologiesat intertrust technologies, we give the publishers tools that allowthem to place the content in a container that provides any type of protection that the publisher wants. it can be encrypted or not; it can have integrity protection or not. there could be rules associated with the information placed in the container. there also could be other containers linkedto that first container that contain additional rules, such as rules that thepublisher thought of later on or rules that say that the previous rules arerevoked.then you go through a distribution chain, which may have severaltiers. according to the rules, people can do various things. they couldchange the unit price of an object that has commercial value, for example,or they could decide that you can forward it to someone else. just aboutany action can be controlled at any level of the distribution chain.eventually, however, these things get back to the consumer. in ourspace, the consumer has to agree to rules, either implicitly or en masse.for example, if there is a license associated with something, then the usermust agree to the license, which may make an implicit agreement formany other transactions that might happen down the road. but somehowor other, the consumer must be informed about the rules associated withthe things that impinge on the consumer.as an example, a rule might say that an audit record will be created ifyou engage in a specific transactionñan audit record that itself becomesprotected content. this is done in a way such that the consumer is told,òyou can have this piece of content for free. we will collect some unlinked, anonymous information about it, but we need to aggregate thatinformation with information from other people.óintertrustõs role is to ensure that such things are done in a fair andaccurate manner. for example, if someone says, òi will not collect data foran audit record about your use of this,ó we can tell whether that statement is true, because we designed many of the mechanisms. the rulessay that if an audit record is supposed to be created but instead ananomaly occurs, then the transaction will not go through. the idea is tohave automation not just within the web, but within any local area networks or personal area networks, such that the consumer could, for extechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.78a trusted third party in digital rights managementample, have some of this content moved into various other types of devices.thus, the commerce networkñat least in the way that we representdrmñcontains just about any type of digital information. there are alsoloosely coupled rules, meaning the rules do not have to be packed withthe information in the same file. the file can be delivered in one spaceand the rules delivered in another. in addition, the rules can change; theycan expire and things of that sort.another important concept is identity attributes, which are applied toprincipals who may use the information. rules can refer to those identityattributes. there is a coding system for identity attributes, and a trustmanagement system for determining which identity attributes are associated with what. the identity attributes also could be associated withpieces of information. for example, a rule might say that if you are a bookof the month club member, you get a 25 percent discount. there also hasto be something, such as labels, that identifies book of the month clubselections. these labels are identity attributes in that space.events and consequences are an essential part of the drm system.the content owner identifies the events; for example, if you want to playthis particular game, then you have to pay for it. in such cases, contentowners might want to see proof of authorization or payment, or theymight prefer to say that a meter in some device is decremented orincremented. or they may want to have, anonymously or explicitly, theidentitylinked information or a record of what happened. some of theseevents and consequences are practical. in the medical information arena,for example, people are resistant to hardcoded policies on access to medical records, because in emergency situations these policies would not beappropriate.therefore, you need exception mechanisms, which are difficult toimplement. the exception mechanism might say, òyou can have emergency access if you say who you are; then an audit record will be collectedand will flow upstream to a clearinghouse, and later on someone may askyou why you did this.ó at least this approach tends to ensure that theexception mechanism is not abused. such a mechanism could be useful inthe context of labeling content so that children can have access to something on which they are doing a report, even though something like p3por some browsing policy enforcement software, or whatever, otherwisewould deny them access. creating an audit record is problematical, butat least the parent can say, òi understand that you exercise that exceptionin a fairly straightforward way and i am still monitoring what you aredoing in absentia.ó when these techniques are applied, the recording oftechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.david maher79events, logging, and especially exception mechanisms are absolutely required.an audit mechanism can be defeated by an attack on the communication between the auditor and desktop. the mechanism that we use assumes that you are not always online (most people are not). we can tellwhether or not people tamper with the protected database, up to certainlimits. there are thresholds that say, òi must deliver my cache of auditrecords to wherever their destination is.ó the audit server could be partof an enterprise, or you could contract with an isp to host the clearinghouse for the audit records. or it could be part of a home network or partof the same machine such that the parent has access to the audit recordsbut the children do not. it is difficult to implement but conceptuallystraightforward.we have a network of protected processing environments. we workdirectly with chipmakersñsuch as texas instruments, and chip platformmakers, such as arm, and other companies making chips that go in settop boxes, cell phones, or personal digital assistantsñto put in securitymechanisms (e.g., trust management) so that we can have a protected processing environment. this is highly problematic for a pc, as observed byothers earlier. the mechanisms that we use for the pc are quite different;they have to do with the concept of renewability, also alluded to earlier.trust management, or delegation of trust, involves who and what aretrusted to do what, and who determines policy. this has do with, forexample, those things you delegate to a parent versus a child, and howyou arrange the user interface so that people actually understand thepolicy on what might be delegated to themña difficult problem in thisspace. a couple of years ago, at&t labs did a demonstration of p3ppolicy with a user interface, which i thought was the most crucial aspectof the research done at at&t labs on p3p. a user interface is how youmake all of this material understandable. they made a few policies visible. but these were not granular policies, which are difficult to makepeople understand. straightforward policies might be difficult to changeon a daily basis, but they can at least be tuned, perhaps when installed,using a somewhat more complicated user interface.there is also the distribution of policies and rules, which can be broken up into three areas of intent: what you want to do with the content,under what conditions you are allowed to do those things, and what theconsequences are. another important concept is action inquiries, that statethe conditions under which i even ask the question, òam i allowed to dothis?ó there is also governance of transactions, the overseer that ensuresthat a transaction is carried out. when the answer to an action inquiry is,òyes, this is allowed, but . . . ,ó then often it is allowed if you pay or if anaudit record is created or whatever. this is the concept of a transaction.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.80a trusted third party in digital rights managementconcurrent events either all occur or do not occur together. there aretwophase approaches to ensuring that governance is enforced that arepart of the drm system but distinct from the trust management system.12.2countermeasures and hackersanother part of drm is renewability, which i think is key to trying todefeat someone who is determined to circumvent the system. i have beeninvolved in the design of protection for satellite entertainment systems,and the sophistication of attacks on these systems might astound somepeople. one of the best books on defeating these systems is the blackbook, which has a skeleton and crossbones on the cover. you can order iton the internet and it is freely available, published by a charming irishmannamed john mccormac. it is humorous, but it also has a lot of code anddiagrams of how to defeat various satellite receivers. he also publishes aweb site, the hack watch news (at <http://www.iol.ie/~kooltek/>),which has been up for years and is probably still there. at one time thissite was filled with hacks and boasts of hacks, but now the hacking isuninteresting, and the hackers seem to be having far less fun.a number of these satellite systemsñthe predecessors of directv, forexampleñwere mercilessly attacked. i asked them how they designedsystems that could be attacked so easily. the answer was something likethe following: òour contract with the service provider just says to keepthe piratesõ success rate below a certain level.ó this is all they reallyneeded to do. more aggressive approaches were either more expensive ormore intrusive to the legitimate consumers. for years, they have beenplaying that game of keeping the piracy below a certain level while ensuring that the protection measures are not that expensive in a generalizedsense, and that includes intrusion on legitimate rights.the hack watch news, which i used to monitor quite a bit, coveredwhat happened when the purveyors of one of these protection systemstried using a renewal technique. as described in an exercise recently withdirectv, some people had businesses selling hacker versions of smartcards, which were better designed than some of the legitimate smart cards.they gave you access to material that you should not have been allowedto access.1 then the algorithms were changed, and the hackers defeatedthe countermeasure. the algorithms were changed again the secondmonth.2 after the third time, the hack watch news said there was a pallof defeat. the hackers basically gave up.1milo medin said there was a market for these cards in canada, because residents therecould not subscribe to the programming legally.2bob schloss noted that this approach works for new content only. new content requiresthe new algorithm, which may never be broken or may take a few months to be broken.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.david maher81i taught a course on some of these things, and i had a cartoon in whicha little kid is crying, òmommy, mommy, i canõt get the cartoon channelany more.ó the mother says, òwell, weõll just have to wait until nextmonth when the solution to the next countermeasure is available.ó theidea is to keep the legitimate service level, for most people, better thanthat available from the pirate. there are things that we can learn fromthat approach, although this problem was different from the one at handhere.3 the satellite pirates were commandeering part of the legitimatesystem, either for their own benefit as individuals or, in some cases, aspart of a business selling smart cards.we use a secured virtual machine that is independent of the browser.4we keep changing it to defeat the hackers. this method is problematicbecause we have to get that thing on the desktop. we are arranging to getthat capability in all of the forwardlooking systems, but we do not have adeal with microsoft so it is problematic within internet explorer. there isreasonably good technology such that, as long as you are connected intermittently, it will allow you to do that. marimbaõs castanet software doesa good job; you tune in to an upgrade channel. i think real network useseither castanet or something similar. it tells you if an upgrade is available, and then gives you an option, which is the standard way of dealingwith this. to make our system effective, you would not allow the optionfor the upgrade. the problem is raising the stakes on who gets the update, so renewability and tamper resistance are essential.napster is having a problem now with legacy content. they are trying to put together a system that will use name tagging to prevent distribution of copyrighted material through napster. of course, there are already dozens of ways to counteract that approach. but there is also theconcept of requiring proof of origination. there are sophisticated systemsthat check for proof using cryptography techniques. (hackers do not target these techniques, but rather try to turn off the structure of the securesystem, the key management and things like that.) in the case of something like proof of origination, you must have a policy that says, òthissystem will not read or present any data that lacks proof of origination.óin which case you would have secure labels and so on. you will still havethus, even with a great system, all the old pornography produced before a certain dateñalot of materialñstill would be available for everyone to see.3david forsyth said the problem is different because the satellite pirates are òvicariousócontent providers who are not doing anything to their own satellites. they might hack yourchips, whereas anyone who gains access to pornography on the internet can distribute it.4david forsyth suggested that software vendors might give out new browsers everycouple of months to defeat the hackers. but it is not clear that everyone is jumping on therendered software bandwagon.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.82a trusted third party in digital rights managementthe issue of what to do about unlabeled content, whether legacy materialor not. you need a policy that deals with unlabeled content.people believe they should get satellite programming or music fromnapster for free5 because the data are not stored in any encrypted waywhen someone buys it. this is a fundamental issue. for new things, youcan use the lack of an òin the clearó distribution path as the exclusionmechanism; this is the issue with the record industry. but from the perspective of media, do you believe that this type of structure, which, inessence, rents content or distributes rights according to content, will beany more successful outside of the commerce space, where you can basically say, òif you want to do this, then you have to do it this wayó? doyou believe that this will ever be successful given all the history?i am making an actual personal bet that it will. but the path to gettingthere will not be easy. i look at the forces that resist success and wonderwhether they can be overcome. i have spent a lot of time thinking aboutprivacy because of the issue of collecting information about events in distributed systems. i do not think we will have a truly productive distributed computing system unless we know how we can collect informationabout those events. we are dealing with that in the embedded systemscommittee. at my company, we say, òcollected information about thosethings is protected, and we have techniques and policy mechanisms to dothat.ó how effective we can make them and how can we use distributedtrust mechanisms? we know that we cannot do it perfectly, but this doesnot mean we do not try.there is also interplay between law enforcement and policy at thegovernment level. in the drm field, we depend on things such as thedigital millennium copyright act, with which i was not completelyhappy because of its impact on research. but certain aspects of it are reasonable. its provisions are importantñaddressing issues associated withcountermeasures, and what risks you take when you try to defeat a countermeasure. if we could get the research aspect right, then i would behappy. there are also other things, such as copyright and patent law.if you are a purveyor of mechanisms that defeat countermeasures,what consequences do you face? what are the risks? my house does not5winnie wechsler said that in the mid1980s, when encryption was introduced to the backyard satellite dish market for the first time (before directv), there was an uproar amongpeople who owned eband satellite dishes, because they felt it was their right to have accessto this programming, which had always been free. they bought the dish, and the free programming was part of the proposition. then suddenly programmers started to use encryption, and there was a huge backlash involving piracy. she suggested that this is a fundamental hurdle in developing any solution to piracy.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.david maher83have a lot of security systems. many other people have all types of security systems on their houses. yet it is very simple to deal with them; youcould level a house with a bulldozer, for example, and grab the jewelry.this does not happen because we have laws and law enforcement. thesame type of situation will occur here. the cost of the systems clearly hasto fall,6 and you need a shared infrastructure so that, instead of just a fewpeople paying for it, a lot of people pay a much smaller perperson pricefor it. this is why the techniques will not be rolled out just yet.there are solutions coming in a couple of years that will use moresophisticated distributed trust management techniques to increase thebarriers to unauthorized redistribution of content.7 this will be done onthe basis of actions that firms can insist that you do as a condition of receiving their material. i believe this to be true because many larger publishersñincluding entertainment publishers, such as time warner, universal, bertelsmann, and reutersñare funding the establishment of someof these mechanisms.6robin raskin said the cost of the system would exceed the costs of the music or televisionshow that one tried to protect. he gave the example of publishers dealing with authorsõcontracts. in looking at drm, he decided it was cheaper in the short term (the next 2 years)to pay all the authors more money than to implement a rights management system, the costsof which, for a big publishing company, would be astronomical. herb lin said representatives of the adult online industry told the committee that they have problems with peoplecopying their content and redistributing it without paying. he said it seemed doubtful thatany single provider could afford to implement a drm system. bob schloss said drm wouldwork in the music industry because the major labels believe that each artist is unique, suchthat almost nothing is a substitute. this may not be the case for other types of content,including pornography. if danni ashe (who testified before the committee at a previousmeeting) required a special browser plugin or keyword every time someone visited her site,and no one else had such a requirement and her competitors were comparable, then peoplewould go elsewhere. john rabun said most of asheõs images are copied all over the place.the people who copy them do not even bother to change the titles, even though you wouldexpect that someone violating a copyright would at least do this. rabin said ashe expressedconcern about new talent, but this constitutes probably less than 1 percent of all adult pornography sites.7john blumenthal said he checked the web site of danni ashe to see how she did ageverification and how she contained her content to her site. then he went to usenet, wheresome news groups focus on her. the news groupsñat least three or four different usenetserversñcontained no images of her. somehow she is creating a barrier between her website and usenet. herb lin said he asked ashe these questions and she is very concernedabout redistribution; she also hired her own technical staff to deal with the issue. davidforsyth said he does not understand why she does this, because it is valuable when peopleredistribute lowresolution or inconvenient versions of good content. forsyth is finishing atextbook, which can be downloaded in pdf format and printed. it is much less convenientto print an 800page book than to buy it, but availability of the pdf version means thateveryone gets to look at it.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.84a trusted third party in digital rights management12.3summarycarrying out the concepts of trust and policy management is nottrivial. we need languages and ways in which we can identify principals.in some of this space, we need to identify principals in an anonymousway. p3p addresses some of this, but i am not sure whether it will doeverything that we want without things like exception mechanisms. weneed credentials and an artificial intelligence compliance checker. theseare not universally available, but there is a drive to make them more available because of their usefulness in commerce. until these things are embedded in such a way that people interact with automated systems in anatural fashion, it is difficult to believe that the mechanisms will havewidespread effectiveness. some of the research needs to focus on howpeople interact with these systems.intertrust has embedded a trust management system that adheres tothese principles into the systems deployed on behalf of its partners. wealso play another important role. there must be an administrator; someone has to be copyrighted as the root source of trust. this must be autilitylike function, that is, carried out by someone who specializes indoing these types of things and does not compete with the people forwhom these mechanisms are deployed, because there could be bias.do we have competitors? yes, we have competitors. in spaces such asmusic, our main competitor is microsoft, which, interestingly enough,does not have the utilitylike attribute. microsoft competes with manyservice providers, which is what they (the service providers) are afraid of(in making microsoft a gatekeeper, through their drm). people expectintertrust, as an impartial trusted party, not to compete with them as wedeploy these types of mechanisms. we are putting legal structures inplace to ensure that this happens. drm is all that we do. we charge autility fee, which i think is 60 basis points on transactions that use thetechnology. the reason the universal music group, bertelsmann, and afew others have looked kindly on us is because of our impartiality in thatwe do not compete with them. but we have also heard that they thinkthat 60 basis points is a òcheap date.ótechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.8513problems with a dotxxx domaindonald eastlakei cowrote a personal internet draft in the internet engineering taskforce (ietf) about the problems with mandatory labeling.1 people oftencome up with ideas about how to segregate or label all bad material tomagically solve the content selection or child protection problem. thisidea is simple and easy to understand, but it does not work. there are alot of problems with it, which this draft tries to summarize. the problemscan be divided into several categories.the first category of problems is philosophical. the idea of finding away to categorize content in the global context of the internet is absurd.there are 200 countries and they all have different laws. for example,laws on nude modeling differ. in one country you can have a magazineconsisting entirely of nude pictures of 17yearolds, but this is obviously afelonious and criminal act in another country, where nude models have tobe 18. yet another country might not permit any noticeable amount of thefemale body under any circumstances in a magazine or publication. thereis no hope of getting a consistent point of view on this sort of thing. andthis is just one criterion.moreover, there are more cultures than there are countries. thereare literally thousands of cultures, all of which have their own particular1see <ftp://ftp.ietf.org/internetdraft/drafteastlakexxx00.txt>. personal internet draftshave no formal status and are not endorsed by the ietf or any other group. the draft isintended to become an informational request for comments (rfc), a document that is issuedunder the auspices of the ietf.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.86problems wth a dotxxx domainquirks and ideas regarding what sorts of things children should be allowed to access or the age at which children become adults. going onestep further, the concept of community has made it easier to developstandards, one way or another. but there are literally millions ofcommunities.another category of problems is legal. if you require everyone whohas a certain type of content to be in the dotxxx name space, then you are,in effect, forcing speech on them. this seems to be a problem with respectto certain legal rights in the united states and some other countries. itobviously depends on the circumstances and whether this sort of speechis commercial or noncommercial, and so on. but, in effect, you are requiring people to label themselves, which runs into legal problems and effectively limits their free speech.one difficulty in thinking about this sort of thing is the malleable nature of the internet. some parts of it are similar to commercial broadcasttelevision, which, at least in the united states, currently has a system oflabeling. but other parts of the internet are more like someone strollingthrough a park and talking to whomever they bump intoñactivities thatare entirely noncommercial, spontaneous, and unorganized. imagine, ifyou are strolling through a property and bump into someone and youwant to say something that some people could construe as objectionable,that you had to wear a large, yellow star. i think people would considerthis to be objectionable. in some respects, labeling of internet contentcould be considered similar to the yellow star.another category of problems is technical. the labeling system has tobe realistic. the use of dotxxx is not linguistically complicated. but ifyou try to label in an understandable way the various different axes ofheresy or derogatory speechñwhatever people object toñthen you wouldhave problems with the language from which to select the labeling. inaddition, the internet is not technically structured for things to be done inthis way. the internet has a hierarchically distributed control structure,so that one entity controls dotcom, for example, and other entities controlthe subzones below dotcom. there are multiple levels. typically what isidentified by one of these names is an ip address for some machine thatcan store data. of course, we worry about causing a name to somehowcorrespond to some characteristic of the data in that machine. in fact, thepeople controlling these different name zones are likely to be independent organizations, and there is no way to stop other people from pointing at your material.in other words, if you post material on a web site with a name, thereis no technical mechanism to stop someone else who has independentcontrol of a different zone on the internet from posting a pointer to yourip address under any name that they choose. if you have innocent matetechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.donald eastlake87rial, there is no way to stop someone from creating a dotxxx name thatpoints to your project. similarly, if you have material that is placed correctly in dotxxx, there is no way to stop someone from creating an innocentsounding name that points to you. if we had global laws, we couldmake this practice illegal and go round up all of the people who do it andfine them.all of these tricks are affordable. it is very simple, for example, totake an arbitrary mailing list, one that is entirely innocent and devoted tosome light topic, and create an alternative address that you can send mailto, an address with terrible things about òxxxó in its name. you can havethis bad sounding address automatically forward messages to the real,innocent mailing list and change the envelope informationñthings notnormally seen around a messageñand the headers. there is no softwarethat checks on these functions, so it is easy to cause things to be distributed to individuals or mailing lists while making it appear that the mailing list has a name that is actually forged. in principle, a few of theseproblems could be solved by globally distributing changed software, butthis is unlikely to happen.there are other things on the internet that have domain names thatare not really domain names. for example, there is net news, which hasnews groups that are hierarchically named but not hierarchically structured. they are more anarchic than domain names because they do nothave a root and so on. they are more like a conversation, in that anyonecould post anything to any of these news groups and, except for the fewthat are moderated, it is not clear how you can enforce much control overthe names. similarly, names are used in internet relay chat and chat roomsthat are also very conversationlike. given all of this, you wonder if youcan reasonably come up with an approach that would meet reasonablelinguistic criteria and somehow affect all of these different namingschemes in any reasonable fashion.there is nothing wrong with the mere existence of a dotxxx domainname,2 or with just anybody getting a dotxxx site. but i feel that, if sucha category existed, it would greatly increase the probability of laws requiring people to register there. this is not a technical problem, and there2milo medin said that some companies want to brand themselves in such a way, and thismechanism is convenient. logically, if there were a generic law that said people had to labelthemselves, it would be universally agreed that, if people put their content into dotxxx,they should not be prosecuted if a child happened to get in there and the filtering softwarefailed. dotxxx is not the way to enforce mandatory labeling; this should be done with picsor something page dependent. however, someone could be prosecuted, either civilly orcriminally, if they put notforminors content into a dotkids domain.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.88problems wth a dotxxx domainis certainly no technical difficulty with the mere existence of that utilityand the ability of people to get names there, as long as some organizationruns a registry for it. there is a slippery slope argument, but it is notcurrently mentioned in our draft. the main thrust of our draft is to provide a convenient, precompiled answer for people who assert that a mandatory dotxxx domain name will magically solve the problem they perceive in the categorization of internet content.the idea of a dotkids domain may have a different spin in variousways. it still has the problem that the criteria for what kids are and whatis appropriate material for them differ widely among nations, cultures,and communities. but in some sense it is a little better than dotxxx.maybe if you put something in dotkids that is not considered appropriate for children, you would be prosecuted.i also want to comment on the idea, which is mentioned less often, ofcategorizing content with a bit of the ip address. all hosts on the internethave either 32bit addresses under ipv4 or 128bit addresses under ipv6, which is not widespread but is getting some attention. there are manyproblems with this approach. it is, in some ways, coarser than the domainnames (sometimes the main name structures can be used to address asubset of material for the host). in some sense, like the address of a building, it refers to everything in that building. one problem is that there areno extra bits in ipv4. taking even one bit away would cause havoc; thereare not enough addresses to go around. the whole reason for the creationof ipv6 was to overcome the limit of 32 bits in ipv4.another problem is that these bits are not arbitrary. they are topologically significant. as packets are sent through the network, they arerouted by comparing the prefix bits on these numbers with a routing table.essentially, the longest match determines how the packet is sent. i amsimplifying this a bit, but at the top level of the internet, routing tablescurrently have on the order of 40,000 or 50,000 entries, and this determines where things go at the top level, and they trickle down from thereuntil they get to a particular local machine. if you assign addresses randomly, then you need billions of routes at the top level or else it wouldnot work. there is no feasible hardware today that understands how todo this. for the internet to work and get the data around, the address bitshave to be assigned in a topologically meaningful way, directly related tothe actual structure of the internet and how the ips are connected to eachother.ipv6 might sound more hopeful, but it is not. one popular proposal,intended to enable wide deployment, effectively would reduce the routing part of the ipv6 address to half of the full size. in this scheme, 64 bitswould be used for all of the routing control, and the other 64 bits wouldbe used as a unique endpoint identifier. conceivably, you could sometechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.donald eastlake89how get one bit out of the bottom of the 64. but once you consider theneed to label things along all the different dimensions and categories youmight need on a globally meaningful basis, there is no way to do it in thebits in an ip address.there is some hope for a technical solution. pics has multiple modes.the mode in which you have to put a fixed label on your web page or sitehas all types of similar problems as does forced speech, and not enoughcategories, and so on. but pics does have a mode in which you haveseparate servers, like a separate rating service. you can ask the serversabout certain data, certain sites, and so forth. this, at least, seems not tohave the problems of forced speech or the limitations of other labels. youcould have literally thousands or millions of different pics servers thatpainted the world in different ways, and they would enable you to askquestions as to whether certain parts of the network are approved or notby the vendor of that particular pics rating service, which could be someparticular church, culture, or country. i am not saying that this necessarily would work wonderfully, but it does seem to have at least some technical practicality.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.9014business dimensions:the education marketirv shapiroi am the chief executive officer for edventions, which provides a suiteof software services and training to introduce technology transparentlyinto schools. let me define òtransparentó very simply. when you gotinto your car this morning, all you needed to do was hold onto the steering wheel, push two pedals (maybe three, if you are an advanced driver),and you were done. you did not think about what type of engine was inthe car, why it worked, or any of those kinds of issuesñthe car was transparent technology.114.1the role of teachersi am most interested in the role of teachers in elementary schools,which are very different from high schools or universities. from a business perspective, teachers are both an asset and a liability. that asset andliability may be the solution to some of the questions posed earlier today(described earlier in these proceedings).for at least 2,600 years, from the time of the greek academies, whenadults have wanted to introduce children to new material, they have sent1sandra calvert noted that driving a car is not transparent for a new driver. when firstlearning to drive, she was concerned about what to do if she had to sneeze. this is something that requires thought; it is not an automatic skill. even today, she carries an americanautomobile association card so that she can call emergency services if she runs into anyproblems.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.irv shapiro91them to school. teachers are expected to teach more than reading, writing, and arithmetic. we also expect teachers to make decisions. teachershave immense classroom autonomy. in elementary schools, the numberof supervisory staff is small compared to the number of teaching staff,and teachers in the classroom are mostly on their own. they decideñwetrust them to decideñwhat our children should learn each day. in thatprocess, they make many selections.the same processes are at work in the elementary school library. thelibrary does not have a million books in it. even if the school could afforda million books, having a million books would not be a good idea. forexample, if a third grader is writing a book report on george washingtonand goes to the library and finds a thousand books on the shelf about him,the student will sit on the ground and begin to cry. i have four children; iknow this to be a fact. school librarians and teachers select books for thelibrary under the direction of the school board, state and federal standards, and recommendations from organizations.14.2historical perspectivethe challenge is how to provide the tools that teachers need to leadand teach children in the internet age. the present rate of technologychange is unprecedented in history. the impact of information technology is comparable to the impact of gutenbergõs printing press at the endof the 1400s, but today the impact is being manifested over several yearsinstead of several decades.how do we empower teachers? let us look at the last 30 years. overthe last 10 years, there has been universal agreement that the economyhas been robust. even with the adjustments occurring now (i am no expert, and i do not know if they are permanent or if this is a recession),times have been good for 10 years.when economists looked at this period of time, they were baffled initially, because, as i learned years ago in economics 101, you cannot haveboth low inflation and low unemployment. you cannot have robustgrowth, low interest rates, and a full employment economy. those thingsdo not happen together; they have to be kept in balance. the federalreserve board kept them all in balance, and taxes kept them in balance.economists eventually concluded that there was a dramatic increase inproductivity over that period of time as a result of the introduction ofcomputer technology into the american economy. that increase in productivity allowed us to produce more goods for less cost.this sounds wonderful. but i was in steel mills in the mid1970s installing computers, and i guarantee you that there was no increase in productivity. when we walked into the mills, they laughed about all thetechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.92business dimensions: the education marketpeople they were going to have to hire to take care of the computers doingtheir payroll, general ledger, and accounts receivable. maybe the computers were controlling a couple of machines, monitoring temperatures offurnaces, and doing process control, but there was no increase in productivity.let us assume, for the sake of argument, that there was no increase inproductivity in the 1970s. yet in the 1990s, the economy was robust. whatchanged? some very smart people and organizations, such as sap,microsoft corporation, apple computer, and sun microsystems, recognized that the computers in the plants, factories, and offices of americawould not account for the difference. nor would it come from the infrastructure. no, the difference was that these companies began to buildspecialized software for industry, and businesses invested hundreds ofmillions of dollars in training their workforces. in the 1970s, we put inlots of wires and computers; in the 1980s, we introduced new softwaredesigned to revolutionize the process of manufacturing. the word processor changes peoplesõ lives.i have two children in college who would not even know how to writea paper in longhand. this is a technologically revolutionary time. sowhere does technology stand in the schools? because of the erate andother successful programs, we have put lots of computers and wires intothe schools. but it seems to me that the schools are stuck in the 1970sbecause we have not retrained our teachers. we have not introduced newsoftware specifically designed for these marketsñespecially for elementary school. instead, we have taken software designed for the businesscommunity, universities, or high schools, and tried to roll it downhill to asecondgrade classroom.teachers in second grade do not have $5,000 projectors. they mayhave laptop computers, but powerpoint and excel are not tools for them.the teachers need something different. thus, the opportunity for the business community now is the same opportunity that existed at the end ofthe 1970s for the traditional computer and software companies. there is aneed for software and training in the schools. there is a need for helpdesks so that teachers can pick up a telephone and talk to a real person at8:00 p.m. or 11:00 p.m. without being put on hold for an hour, as they try toprepare an assignment for the next day. this is a wonderful businessopportunity, which is why i got involved with it 2 1/2 years ago.14.3the school marketplacethe other side of the story is that teachers are scared. they are underpaid and overworked. when a teacher gives our children more hometechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.irv shapiro93work, the teacher has more homework the next night, too. they get callslate at night. they work in a complex environment. quite candidly, theskills that make someone a phenomenal secondgrade teacher are probably not skills that would enable them to deal with such complexity.change in the elementary school education marketplace is difficult, because teachers do not want anything to do with it. our company has beeninvolved in many districts where the superintendents and principalsbrought in a program but the teachers dug in their heels and said, òno,we will not use this stuff. we do not even want to learn it.ó2it will take some time. unfortunately, the cost of time is dollars. inthis economy that has just survived the dotcom world, think about timein terms of months, maybe a year and a fraction. when you talk to theinvestment community about going into a marketplace in which you mayhave to spend 2 years in a sales and educational process, providing education at a subsidized rate, the investment community says, òthere areeasier places to put our money.ówhy should they do it? because switching costsñto use economictermsñin the schools are very high. once a program is in a school, it doesnot go away. if i had a magic wand, i would look at how to pump dollarsinto teacher education and the creation of software and technology specifically targeted to this marketplace, even though we know the paybackprobably will take 5 years instead of 18 months. the reason to do it is thatthe marketplace is very large. look at the presidentõs budget and see thelarge numbers going into education. when you are in the market and aresuccessful, you provide very good returns to investors.whether you do that as a nonprofit, whether the government does it,or whether the government provides funds to a forprofit to do it, it is afundamental issue. as a forprofit attempting to address that need, wefind it very difficult to raise capital, because the return on investmenttakes longer than the current capital markets want. this is not just a private market problem. look at the allocation of federal funds. as an example, erate was strictly a program for lines and hardware. the wayyou get title 1 dollars to apply to technology is to repackage the technology as reading, math, and basic learning. the overall challenge is to finda way to retrain the teachersñnot put dollars into curriculum, hardware,2sandra calvert said teachers today are expected to do much more than teach. they areexpected to solve social problems, such as parents getting divorced. then the computer isthrown in. a teacher using a computer to give a presentation needs to become a technicalexpert in case something goes wrong. if it breaks down, then usually a whole classroom ofkids is left sitting there, because technical support is seldom available in the classroom.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.94business dimensions: the education marketand lines, which is where i see the majority of the dollars going. somefederal money is targeted specifically to professional development, butlook at the order of magnitude difference between professional development and hardware and infrastructure.over the past 2 years, many businesses looked at the size of the pot inthe education marketplace and attempted to fill the gap by using advertising revenues or other nontraditional revenue sources. they failed. weare left with two models, which may be fine. very large corporationshave a vested interest in the current model. they would like teachers touse textbooks in the exact same way as in the past; they are not interestedin the technology changing too rapidly. these parties have deep pockets,which is okay. there is also the continual opportunity marathon, in whichsomeone can start a small business and leave it as a small business. in anumber of sectors of elementary schoolsõ infrastructure, there are manysmall òmom and popó operations that never grow beyond serving thetechnology needs of a couple of communities.teachersõ unions have no effect, positive or negative. in the long term,they could have a slight positive influence. but in the situations that wehave seen over the past 30 months, this has rarely been packaged as aunion issue. every once in a while we hear, òour contract is coming up in6 months and we do not want any change until the contract is renegotiated.ó there are many fearful teachers out there, and getting them overthat fear is as massive an undertaking as the complete erate undertaking. this is much more expensive than what we have done on the technology side. unions could be a positive force in helping their membership to overcome this fear.3there is another positive force coming. the statistics indicate thatabout 50 percent of the teachers in america are approaching retirementage, and as many as 50 percent will retire over the next 5 years. thepeople going into those jobs probably recently came from universitieswhere they got all of their homework online and computers were usedtransparently, so they may demand this in the schools. teachers become3janet schofield suggested that unions could be helpful in negotiating, for example, discounts for teachers buying home computers. in studying teachers, she has found that, ifthey have computers at home, they are more likely to get over their initial reluctance. maybesons or daughters train them, and they have more time in the home environment. unionscould reduce the economic barriers and create centers for their members to get home computers. she also suggested that teacher training relates directly to other issues at hand. forexample, teachers seldom know enough about the internet to realize how they might prepare kids to surf safely and responsibly. teachers may not know how to locate good sitesthat will draw the kids in.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.irv shapiro95obsolete because we have not done our job of training them. if we haddone our job better as a society of providing teachers with the expertiseand training that they needed, then society would not have to solve thisother issue of kidsõ access to inappropriate material. teachers are veryinfluential, at least with very young children.we need to develop a business model that takes a patient approach tothe retraining of the teacher workforce.4 over the last 9 months, we haveheld training in 200 schools in internet access, how to select good sites,how to use our particular tools, and a variety of related topics. we nolonger will be doing onsite, inservice training. instead, we are movingto a model in which we will train a trainer in the school and provide avariety of multimedia materials for the teachers. we have found that whatis most effective with teachers is òjust in timeó training, rather than bringing them into an inservice for a day at the beginning of the year and then4 months later when they go to use the materials. providing that type oftraining and support mechanisms is expensive. it is a challenge to develop business models that will support the teachers so that they can provide the education that will cut down on some of the bad things that happen in this networked world.4marilyn mason said that when libraries began using the internet, entire staffs were retrained. librarians are neither more nor less reluctant to use technology than are teachers.but if a library had something very specific that it wanted the staff to do, and if librarianssaw this as a way to make their jobs easier and make themselves more effective, then theycould embrace the technology as a new tool. the education profession has not sorted outhow the internet can be a tool for improving education. mason suggested looking at whereone can intervene in a cycle. one opportunity may be the emphasis on test scores, becausethey provide some measure of effectiveness. there are software packages that help childrenlearn to read, and they can be effective if used in libraries. the key is to make sure there isa common understanding of how teachers are supposed to use technology.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.9615business models:kidfriendly internet businessesbrian passuntil yesterday, i was president, chief executive officer, and cofounder of passport new media, which created a product called òyourown worldó (yow for short), standalone software designed to enablechildren to experience thirdparty internet content in a protected, offlineenvironment. for parents, we offered peace of mind that their kids, whenusing our software, would never be exposed to the dangers of the internet.for kids, we dramatically improved the performance of the internet byeliminating bandwidth constraints and putting all of the content on thepersonal computer (pc).we founded the company in january 1999. we were a year in development, building this software from scratch. we launched the productlast spring but, when we went to raise our third round of capital andmarket the product nationwide, we were hit by the financing problemsthat face many companies these days. bankruptcy papers were filed justyesterday. nonetheless, we are proud of the product, which drew a lot ofpraise from parents, especially, and from critics who covered the space.i am also a father of two girls aged 5 and 7, and many of my comments are informed by the fact that i am a concerned parent.15.1building an internet businesswhat are the primary challenges of building a business based on theidea of attracting kids to safe and appropriate internet content? buildingany internetbased business is difficult, but especially in the kidsõ space.the kidsõ companies suffer from all the same problems that the adulttechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.brian pass97content companies do, but the problems are exacerbated. the problemsare not necessarily different in nature, except for the safety area.the first and biggest challenge is the internet itself, which is not necessarily an effective medium for young children aged 2 to 12, especiallyfor those under 10. the bandwidth constraints pose one of the most significant problems. even at broadband speeds, children find content coming over the internet frustrating. adults do, too. if you try to watch avideo or animation, especially over a dialup connection but even overbroadband connections, the experience is not pleasant. it is tolerable foradults but becomes intolerable for kids. this is a business challenge because of the competition. you are competing with tv, video games thatperform extremely well, and pc software that works well. when youclick on a pc game, something happens right away; the same cannot besaid for content coming over the internet.a snowballing series of other business challenges arise out of thesebandwidth constraints. there are creative limitations on what you can doin a space. if you want to do something that works well over the internet,chances are you will make creative sacrifices that make your content fareworse than your competition. this applies to entertainmentbased content and educational content. our product was somewhere in the middle,in the edutainment space. the creative tradeoffs pose real challenges.many companies have tried to develop original educational contentand deliver it exclusively over the internet. for example, mamamedia innew york tried to create bandwidthintensive educational (but fun) content for kids. they were challenged from a business perspective becausethey spent a lot of money marketing this product. there was a majormassadvertising campaign of which my kids were well aware; they askedme if we could buy fruit rollups so that they could get the secret codefor a game on a mamamedia siteñnotwithstanding the fact that they arenot allowed on the internet and have never seen mamamedia. this was asuccessful campaign and it drove millions of unique visitors to the site.but from a business perspective, those kids did not visit the site often orstay very long, and the performance results were probably among theworst in the industry of the companies that i am aware.at passport, we tried to address this very issue by bringing the content off the internet and making it perform well. as a consequence, wedid not have the same problems. on average, our kids visited 10 times amonth and stayed 25 minutes each time they sat down, about 10 times theindustry rate (kids visiting less than twice a month and staying maybe 25minutes during the entire month). we had other problems, but bandwidth clearly is holding kids back from embracing the internet in important ways.the other major limitations of the internet include the safety and pritechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.98business models: kidfriendly internet businessesvacy concerns. i will address them from a business perspective. the firstissue is the cost of complying with regulations. the childrenõs onlineprivacy protection act governs this space. there have been many discussions since the law was enacted about the costs, in dollars, that these regulations impose on content providers. these are just some of the costs ofdoing business in this space.the more important cost is the primal fear factor. i do not wish toquestion parentsõ judgment, because i share a lot of those concerns. butparentsõ fear of the internet makes it a less than great medium for thesimple reason they do not allow their younger kids online in great numbers. (i am not referring to teenagers, who embrace the internet in muchhigher numbers.) when you combine this fact with the unpleasant, bandwidthconstrained online experience for kidsñif they are allowedonlineñit explains why fewer than one in three kids who have internetaccess at home are actually online. (this number does not include kidswho access the internet from schools.)another major challenge to a business seeking to provide content tokids in a safe way is financing. this is obviously the biggest issue facinginternet companies of any type today, but even when we got started inearly 1999, during the glory days of the internet, the kidsõ segment wasdifficult for the venture capital community. i cannot tell you how manytimes i was in a venture capital meeting and was told, òit is very difficultto monetize kids.ó as repugnant as that sounds, it gets to the heart of theproblem. there is no bigger challenge than getting a business funded andoff the ground. even in the late 1990s, the industries serving childrenwere not doing especially well. this includes television production, historically a difficult business, and the cdrom business, which is very hitdriven and a difficult retail model. the learning company, then undermattel, was struggling in those days, and i read just recently that, sincethe company was sold, it has reached the breakeven point.15.2comparing business modelsafter the stock market crash of last year, i did not hear about the issueof monetizing children anymore in meetings, because i was not gettingany meetings. i could not have presented a worse business model to theventure capital community last yearñi think the same still holds for todayñbecause the model embraces content for kids and has an advertisingsupported revenue stream.one might argue that the business case has not yet been made forproviding content to kids in a safe way. but many people have tried. thebusiness models today can be categorized by two variables. the first varitechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.brian pass99able is the market that you are targeting, such as kids in the home, theconsumer market, or kids in schools. these are different markets and aredividing lines among business models. the second variable is the revenue model, whether adsupported or feebased subscription or licensing. i am excluding ecommerce.if you constructed a matrix using those variables, you would haveconsumer adsupported companies, consumer subscriptionfee companies, schoolbased adsupported companies, and schoolbased subscriptionfee companies. we were in the first of those four categories, with aconsumer product for the home supported by advertising. other examples of this type are mamamedia, zeeks, freezone, and probably ahost of others.the problems here with the business case are similar to those facingsites for adults: the high cost of creating content, slow acceptance by advertisers, and limitations of the internet medium with respect to advertising. not only does it make for a poor entertainment content experience,but it also makes for a poor advertising experience. the traditional formof advertising on the web is a banner ad, which you click and it takes youto another site. for a kid, especially over a dialup modem, that form ofadvertising is a nonstarter. the kid gets lost when transferred to anothersite. even the content provider loses out, because now the kid is no longerat the original site. it is a losing proposition all the way around.we tried to address this problem with offline capability. instead ofkids clicking on a banner ad and going to another site, they got a richmedia payoff right away. they could play a game instantly. they couldwatch the full, 2 1/2minute rocky and bullwinkle movie trailer behind abanner ad that played in real time with no bandwidth constraints. notsurprisingly, we got a very high response to that ad. but with a small userbase, you cannot make a lot of money doing this. this was our big challenge; we could not build a base big enough to get large advertisers onboard, even though they were excited about the product. we did nothave enough kids for them to reach. we did not build the base quicklyenough before we ran out of cash. timing is everything, and that had alot to do with it.there are many examples of the consumersubscription model in thekidsõ space, such as juniornet, probably the closest technically to what wewere doing, and disney blast. these companies have tried to offer subscriptionbased services to kids in the home, such that the subscriptiontakes the form of a monthly or yearly fee. the problem is that the subscription model never has worked for any internet company, as far as iknow. many people have tried to charge for content, but people at homefeel that internet content should be free of charge. this has been the funtechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.100business models: kidfriendly internet businessesdamental problem of the internet for all companies, not just those cateringto kids.an example of a school business model that adopted an advertisingapproach would be zap me, which offered to wire schools and build infrastructure in exchange for being able to advertise or market to childrenin those schools. this brings up difficult issues in terms of the commercialization of schools. zap me found that it was unworkable and the company no longer deals with schools or kids; it is now offering network services under a different name, rstar networks.the fourth model in the matrix is schoolbased services that use asubscription or licensing model. this is the predominant model. classroom connect, light span, and others have developed online, feebasedservices for schools. we have heard a lot about the obstacles and difficulties of working in schools; i will highlight just a few.one difficulty is the great variability in how networks and computersare structured. every school is a little bit different in ways that affect howyou bring content into that school. statistics show a very high penetrationof internet access in schools, but i doubt that any one school is like anyother in the way that kids use and experience the internet. some havecomputers in the classrooms, others have them only in the library, andstill others have a separate computer lab. this makes it very difficult tocreate curriculumbased content.in addition, there is an underlying assumption that learning from thepc or the internet is a good thing, especially in schools. this remains tobe shown. i believe that, on the whole, my kids are better off. they arelearning to use software and have had positive experiences on computers.but at least some studies suggest that this is not necessarily a good thing,so this becomes a barrier to successfully putting content into those schools.ultimately, the successful model (if there is one) will do the followingthings: it will work well within the bandwidth limitations of the internet.it will focus on what the internet does well, which is deliver content andexchange text. it will meet the demands of parents. it will be safe, secure,and private. and, above all, it will meet the demands of kids, the toughest ones to please in this market. it will entertain, it will educate, and itwill be well done so that they will accept it.no one has tried yet to shrinkwrap a contentbased web productñthe publisherõs model. cdrom developers are trying to incorporate theinternet into their offtheshelf products. we could have shrinkwrappedour product and put it on a shelf. but at the time, we looked at the companies doing this and saw the difficulties that they were having. the learning company and others in the educational space had difficult distribution models and had to provide incentives for purchases by offering verysubstantial rebates. the publishing model was not attractive to us at thetechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.brian pass101time. maybe netscape tried this model when they first introduced thenavigator.1there are also other issues. one is whether a company in this spacecan be grown organically while avoiding some of the venture capital funding issues. it probably can. somewhere, there is probably someone creative enough to make their own educational or entertainment content,post it on the web, and build a business that can pay for itself over time. iwas not smart enough to go about it this way, but i think someone maysucceed.sadly, some of the best sites for kids on the web are probably thecommercial ones pushing products. nabisco, lifesavers, and kelloggõsare examples of dynamic, welldone sites that exist purely to promoteproducts. they have great activities. the most popular game that circulated around our office was a tetrislike game with fruit loops; it was alot of fun. unfortunately, this is where the money is. they have a different purpose in bringing that content to kids, and they can afford to createbeautiful stuff.businesses targeting 12 to 18yearolds would face a lot of the samechallenges. the web applications are differentñmore chat, more instantmessagingñand the content is different. i have not seen as much educational content going to teens. the content is more like the back streetboys, surfing, and skateboarding. the companies operating in this spacehave had very mixed results. a notable company in san francisco, kibu,recently closed before it ever launched. bandwidth is less of an issue forteens, who are more tolerant than younger kids and understand the medium better. they are looking to the internet for different things. thereare also more homework issues. teens who go home and do their homework want to do research and access those positive aspects of the internet.any technology change has both good and bad aspects.21marilyn mason suggested that this model is going in the direction of a journal for a different level of reader.2irv shapiro said his company targets the ages between very young children and teens,primarily kids aged 6 to 12. he uses a subscription model paid for by the schools. hismotivation is simple: he had good fortune in a previous career, planned to donate about100 computers to schools, walked around to see how they were planning to use them, andwas appalled. this led to the creation of edventions. the goal was to integrate computersinto schools just as calculators had been integrated into the math curriculum, based on theidea that children will use calculators to do arithmetic when they become adults. in theearly years, elementary school math teachers were against any use of calculators. now,calculators are integrated into the curriculum. a division of texas instruments is devoted toselling calculators to schools. similarly, children will use computers as teens in high schooland as adults, so the societal motivation is to find the proper way to provide a safe, secureenvironment for these children to learn about computers. shapiroõs solution is to try toleverage the talents of teachers to do this. sandra calvert said dan geer sent around atechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.102business models: kidfriendly internet businessesthere has been a lot more business activity in the teen space, and afew companies have gone public. sites like bolt, alloy, and snowball arereally going after this market and these advertising dollars, because teenshave more disposable income. they can make decisions. then the questions become whether they are staying away from pornography andwhether marketing to them is good or bad.i spoke about a year ago at a conference at which there was a heateddiscussion about the commercialization of the web and kids. someoneasked why there is nothing like a public broadcasting system (pbs) forkids on the internet. the discussion went on for about 5 or 10 minutes,and it was heated. no one pointed out that pbs is the pbs of the webñitis out there online. maybe not enough people know about it, but this maybe a good model going forward (it is one that i was toying with late in thegame). we could create nonprofit organizations that license commercialtechnology and work in that space, and corporations that want to do goodwork can sponsor good educational content. we can have something likepbs; it is not out of the realm of possibility.in the course of licensing content from major media companies and indealing with their kidsõ divisions in separate internet operating groups, idid not think those separate internet groups did very well.3 my sense isthat nickelodeon, for example, went through two or three massiverestructurings of its internet group over the last 2 years. another exampleis warner brothers, whose online site just folded itself back into the company. fox is withdrawing from having separate internet divisions, including fox for kids, and wrapping them back up in the network. television is a great driver. but it is interesting that sites like nickelodeon orfox for kids do no better than the industry averages in terms of repeatvisitation and total minutes of use. the media company is making moneyfrom the tv show and not necessarily from the web. they are not thatdifferent from life savers, which is promoting products online and doingit well.memo about the use of calculators, especially among minority children, who do not understand the fundamentals of math but can use a calculator. this approach needs to be tempered with more basic knowledge. calculators alone are not a magic bullet for doing math.3winnie wechsler suggested that web sites linked to television networks or other preexisting media seem to do well. whatever her kids watch on television, they also use on theinternet. in other words, the business model that works involves a web site that augmentsviewership on television, which, in turn, draws traffic to the web. to address the problemof drawing traffic, what is more powerful than a 24hour ad on television?technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.brian pass10315.3the role of parentsthe question of how to deal with inappropriate material goes back tothe role of responsible parents. this burden falls on parents, teachers,and librarians by default because the technologies are not strong enough,and the regulatory responses generally run into first amendment issuesabout free speech and have a tough time in the courts. by default, responsible adults have to stand up and take the lead in combating inappropriate material.the central role of responsible adults is the reason why, as businessmen, we made a product that would appeal to parents as the primarydecision makers. we demonstrated with the product adoption rates thatthere is a lot of demand for solutions from parents. parents are concerned;they want their kids to have a positive internet experience, and they aresearching for solutions.i do not let my kids go on the internet without my presence. ofcourse, they are young (5 and 7), so we will see how vigilant i am in 2 or 3years. i have a cable modem, and my kids are examples of how bandwidth constraints are a problem. even when my kids go with me onlineand we look at something together, they get frustrated and go back totheir rooms to play with barbie dolls. the internet is slow.there is concern about whether we want 2 and 3yearolds on theinternet. by being offline, we could make a completely simplified interface that could be used by 2yearolds, who did use our service withoutknowledge of how to use the internet. i will not say whether this is rightor wrong, but the childrenõs educational software industry targets kidsstarting at that age and even younger. a year or two ago, the learningcompany introduced software that teaches toddlers how to bang on keyboards. my kids were using the computer with multimedia software at 18months. they are not gifted children. but they happened to be the typesof kids who would just as soon be playing outside and would do a little ofboth. but this is a concern, and it goes back to the assumption that theinternet is a good medium for educating kids. that assumption should bechallenged.44sandra calvert said the issue should be researched. the discussion points to the lack ofa database on whether and how little kids should use the internet. she has seen 4yearoldswho have been online for 2 years, and they are not òhunched over.ó they are curious; theywant to know where the òbackó button is. they are knowledgeable about the internet. shedoes not think it is damaging them, but she would pay attention to the sites they visited andwhether their parents were with them at the time.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.10416business models based on advertisingchris kellymy presentation will focus on the business models for advertisingand commerce on the internet, still viable despite the general pessimismabout the way things are going on the internet these days. all of the bigplayers have had problems. but there will be a workable business model;the question is how to figure out what it will look like, and how thosemodels can be put to use in protecting kids online.16.1comparison of advertising modelsadvertising will continue to be a significant part of internet businessmodels, despite what you may hear. there are four basic models for thesale of advertising. the most common models are cost per impressionand revenue share, although costperclick and costperacquisition dealsare gaining in popularity.costperimpression (cpm) deals are usually experienced as bannerads while you surf the web. you go to a site such as excite, and thebanner ad is presented to you as part of the page. this is still the breadand butter of the industry, the way most sites generate their major revenues, but it is in serious trouble. every major internet portal has seen aserious decline in revenue coming from advertising, and offline businessesdependent on advertising revenues have seen similar thinning.when banner ads first came out on the internet, people clicked onthem 15 to 20 percent of the time, because nobody knew what they wereand everyone was trying to bounce around and figure out this excitingnew medium. things have stabilized now to below half a percent in termstechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.chris kelly105of click rates for a basic banner ad. this has been a disaster in terms ofconvincing offline advertisers to move some of their budgets onlineñaneffect that everyone has seen on the nasdaq. in talking about these lowclickthrough rates, i am referring to runofthemill ads; i will discuss targeting later.because of this lower perceived effectiveness, a few other models aregaining greater prominence, such as òcost per click.ó instead of payingfor the presentation of your product in a banner advertisement, you payfor the actual clickthrough on the ad. this is less popular and more difficult to negotiate, because internet networks are reluctant to accept thesedeals. they say, òif you pay us only on a conversion, on a move, on aredirection to your site, then we cannot forecast what the revenue fromthis deal is going to be.ó advertisers (i.e., ad space owners) are lookingfor guaranteed paymentsñgenerally targeted banner ads.cost per lead is a slightly different model. a lead is a conversion sothat someone agrees to provide a service or to accept to further direct mailor emailñroughly analogous to the response card in a magazine thatsays, òcircle here for more information.óthe revenue share, as i mentioned earlier, is also a popular type ofdeal. the problem with revenue share deals is that you are depending onactual commerce to pay the bill. if there is no transaction at the end of theday, then revenue does not flow back to the advertising presenter, who isthus not happy about the way the ad space has been used.16.2portals, advertising networks, and targetingin discussing advertisingbased business models, itõs important tonote that the big playersñamerica online, excite@home, yahooñsellmany of their own ads but not all of them, which is important. we havean ad sales force that spends a lot of time going to large advertisers andsaying, òfor x million dollars, you can get this many impressions on ournetwork. they will be on these particular channels on the network.ósmaller players and some of the big ones outsource that type of ad sales toad networks. the biggest one is double click. other large ones arematchlogic, a wholly owned subsidiary of our company; engage; and24/7 media. these are thirdparty networks that operate on a variety ofsites across the internet. double click has 2,500 to 3,000 sites from whichit serves ads across the internet. match logic has about 1,000 sites. a bigconcern is the placing of cookies on userõs browsers and computers, totrack behavior across those different sites.targeting is, in many ways, the holy grail of the industry. most adtargeters use profiles based on your behavior across a number of siteswithin an hour. if you visit 10 or 20 of the 2,500 sites within a doubletechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.106business models based on advertisingclick network, then you get scores associated with each site indicatingmale or female, likely age, presence of children in the household, andother things like that. once that profile is established, when you visit asite where ads are served by double click, it will read the cookie on yourbrowser and say, òthis person is probably between 24 and 35, probablyhas kids in the household, is probably female, and may have an interest inx.ó then you get served an ad that double click has sold to an advertiserthat matches this demographic profile.these are usually anonymous, which is an important point. this isone of the biggest sources of confusion and discussion in the privacyarena. the federal trade commission (ftc) took action against doubleclick because the company had plans to start personally identifying without user permission. as it turned out, they never did that and the ftcinquiry was properly stopped. they had planned something that probably would have violated the law and it would have been a false incentiveadvertising practice. but they did not do it.all of this happens because of the need to drive the click rates up, toactually reach the people that you are trying to target. to the extent thatthese things are done anonymously, they are, arguably, wonderfully beneficialñand one of the business models that will work. if you can get tothe types of people that you want, then it is much easier to present to anadvertiser who has x number of dollars to spend to reach this audience,and say, òyou should pay this rate, this cpm or whatever, to get thesepeople. because we know, based on the technology that weõve set up,that we can get to people who meet these characteristics.óa number of companies have tried to generate revenues this way. iam sure that a number of the big networks are very involved in ad targeting. this is similar to what grocery stores have been doing by giving outdiscount cards. the major difference is that the grocery discount cardshave personally identifiable data, so that they can send you coupons inthe mail.16.3choice of modelsdifferent types of internet content providers favor different ad models. the quintessential example of the lengths to which some companieswill go to drive traffic is that, if you end up accidentally on a porn site,you cannot even close your browserñthe site just keeps showing up.mainstream advertisers are starting to use these technologies, too; if youtry to close a window, then ads pop up on a number of different sites.without having done a full economic study of the porn industry, i cannotsay this definitively, but my guess is that they will get hit with some of thesame advertising doldrums that everyone else has. the ones makingtechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.chris kelly107money are probably the ones with subscription models. porn seems to beone of the few things that people will pay for. the problem in avoidingthe content is probably related to promo pages, which are designed todraw people in to pay for a subscription. filters definitely need to catchthose pages.most nonporn sites are not trying to show pictures or video, just animations and banner ads, so there is less concern about bandwidth cost inthe presentation of screens. one reason why the ad networks have managed to prosper is precisely because their costs are so low.1 there is ahigh cost to build servers to push things out and to negotiate the firstarrangements with web sites to build them into the network. but oncethat happens, you can just serve it out. you added potential customerleads and lowered your customer acquisition cost by expanding your network, because you can send a cookie when a new browser visits a site thathas, for example, a double click ad. that unique identifier will be carriedacross every site in the double click network and be registered in doubleclick. high startup cost and low marginal cost make a big difference interms of overall advertising cost.16.4advertising, regulation, and kidsthere are many questions to be asked about advertising as a modelfor paying for software or services that would protect kids. the biggestplayer in filtering in the schools has now abandoned advertising despitethe potential for real benefits in terms of a business model and potentiallymodifiable ad space that could pay for technology that would help toavoid indecent material. what drives these choices are worries about privacy. the childrenõs online privacy protection act (coppa) requiresparental permission for any personally identifiable information collected1brian pass said that, when his company delivered large, richmedia adsñsuch as themovie trailer mentioned in his presentationñbandwidth costs were an issue, because theentire file was shipped all the way to the userõs computer on a nightly basis. if richmediatechnology starts to take hold in advertising structures, then bandwidth costs will be a factor. the myth that bandwidth is so inexpensiveñthat it is effectively unlimitedñcausesengineering decisions to be made. milo medin said market data show that retail pricing forinternet transport runs about $400 monthly for one megabit per second. a new entrantmight get a competitive price in the range of $200. if a site draws a lot of traffic, then network providers discount substantially. for example, a yahoo colocation facility might payonly $50, even without fiberoptic systems. if a company is willing to put content into ahosting facility that a network charges for, then the network virtually gives the bandwidthaway because it provides leverage in interconnection discussions. over the long term, theprice probably will stabilize at about $150, medin said.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.108business models based on advertisingabout children under 13 and thus severely limits business models thatwould target kids.a number of other potential privacy laws and regulations also arecoming that could affect the choice of advertisingbased models for onlinesafety efforts. one is selfregulation by the industry through the networkadvertising initiative (nai), part of a response to the double click ruling. a number of industry players, including match logic, double click,24/7 media, and engage, got together to find a fair way to give peoplenotice if we want to merge personally identifiable data with ad information. the group came up with strict permission and selfregulatory standards. they worked and negotiated with the ftc to establish these standards, which were unanimously approved by the ftc and sent to thecongress and are now in force.in discussing the data models that advertisers use and particularlythe potential effect on a childrensõ market, the meaning of òpersonallyidentifiableó is a huge issue. the question is how far you can move backup the chain to make data personally identifiable. according to the nai,there will not be a move to make data nonanonymous without permission. if a hacker took the information and could match it geographically,then perhaps this could be done without permission, but it is difficult toget all the crumbs together and link them back to an actual person. personally identifiable information usually is defined as information to beused to contact an individual directlyñsuch as full name and physicaladdress. email address generally is defined as personally identifiable aswell. some interesting discussions are going on in the european unionabout whether internet protocol addresses should be considered personally identifiable information. it is always difficult to figure out what willhappen in the eu and which body is acting on which day.senators john mccain and john kerry have proposed privacy legislation that would require web site notice, which would affect potentialchildrenõs advertisers along with everyone else, in terms of fully disclosing the facts and the privacy laws. there are also a number of other possibilities. some in the industry favor a weakening of coppa because ofits effects in cutting off under13s from a socially beneficial communication source. our network does not favor a weakening of coppa. but ithas a real effect on our site. we have completely cut off under13s from email and chat, because these mechanisms can be used to spread personally identifiable information, and the costs of getting parental permissionand maintaining verifiable parental permission were not justified by therevenue. kids on our network can get to the personalization features anduse them, but we keep only the first name and birth dateñeverything elseis deleted.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.chris kelly109on privacy, including kidsõ privacy, the corporate position that wehave taken is that we are comfortable with further enforceable regulationssaying what companies can and cannot do, as long as they are done carefully and do not forbid legitimate consumerserving uses of data. selfregulation, in which companies talk about their practices and exposethemselves to both public scrutiny and government scrutiny for false anddeceptive trade practices, will also be a major part of coming up with aprivacy solution. there also will be new technology, which is the x factor.some technologies will allow complete masking of information and covering of footsteps. this is difficult to implement. a number of advertiserswill rely on the fact that people will find it difficult to use. furthermore,not everyone wants to be anonymous at the end of the day. for instance,you want toothpaste if you run out. it is okay for most people thatwebvan knows that fact because you want it to bring the toothpaste sothat you do not have to leave home or worry about it. you want yourrefrigerator company to know when your compressor isnõt operatingproperly so that it can come out and service it.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.11017constitutional law andthe law of cyberspacelarry lessig17.1introductioni am a professor at stanford law school, where i teach constitutionallaw and the law of cyberspace. i have been involved from the beginningin this debate about how best to solve the problem of controlling childrenõsaccess to pornographic material. i got into a lot of trouble for the positions i initially took in the debate, which made me confident that i mustbe on to something right.this is, necessarily, a question about the interaction between a certaintechnological environment and certain rules that govern that environment. this question about childrenõs access to materials deemed harmfulto minors obviously was not raised for the first time in cyberspace; it wasraised many years prior in the context of real space. in real space, asjustice oõconnor said in reno v. aclu, 521 u.s. 844, 887 (1997), a majorityof the states expressly regulate the rights of purveyors of pornography tosell it to children. this regulation serves an important purpose because ofcertain features of the architecture of real space.it is helpful to think this through. you could suppose a communitythat has a law that says that if you sell pornography or other materialharmful to minors, then you must assure that the person purchasing it isabove the age of 18. but in addition to a law, there are clearly also normsthat govern even the pornographer in his willingness to sell pornographyto a child. the market, too, participates in this zoning of pornographyfrom children; pornography costs money, and children obviously do nothave a lot of money. yet the most important thing facilitating this regulation is that, in real space, it is relatively difficult to hide the fact that youtechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.larry lessig111are a child. a kid might use stilts and put on a mustache and dark coat,but when the kid walks into a pornography store, the pornographer probably knows that this is a kid. in real space, age is relatively selfauthenticating.this is the single feature of the architecture of cyberspace that makesthis form of regulation difficult to replicate there. even if you have exactly the same laws, exactly the same norms, and a similar market structure, the character of the original architecture or technology of cyberspaceis such that age is not relatively selfauthenticating.17.2regulation in cyberspacethe question, then, is how to interact with this environment in a waythat facilitates the legitimate state interest of making sure that parentshave the ability to control their childrenõs access to this stuff, while continuing to preserve the extremely important first amendment values thatexist in cyberspace. the initial reaction of civil libertarian groups was tosay the government should do nothing hereñthat if the government didsomething, it would be censorship, which is banned by the first amendment. instead, we should allow the private market to take care of thisproblem.although the u.s. congress passed the communications decency act(cda) of 1996, there is fairly uniform support among civil liberty organizations to strike it down for that very reason. when bruce ennis arguedthis case before the supreme court, he said, òprivate systems, these private technologies for blocking content, will serve this function just as wellas law.ó and the court avers the fact that there exists private technologythat could serve this purpose as well as law.but the thing to keep in focus is that just as law regulates cyberspace,so does technology regulate cyberspace. law and code together regulatecyberspace. just as there is bad law so, too, there is bad code for regulating cyberspace. in my codeobsessive state of california, we say there isbad east coast codeñthis is what happens in congressñand bad westcoast code, which is what happens when people write poor technologyfor filtering cyberspace. the objective of someone who is worried aboutboth free speech in cyberspace and giving parents the right type of controlshould be to find the mix between good east coast and good west coastcode that gives parents this ability while preserving the maximum amountof freedom for people who should not be affected by this type of regulation.in my view, when the civil liberties organizations said governmentshould do nothing, they were wrong. they were wrong because it created a huge market for the development of bad west coast codeñblocktechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.112constitutional law and the law of cyberspaceing software, or censorware, which made it possible for companies tofilter out content on the web. the reason i call this type of technologyòbad codeó is that it filters much too broadly relative to the legitimatestate interest in facilitating the control of parents over their childrenõs access to materials that are harmful to them.there is a lot of good evidence about how poorly this technologyfilters cyberspace: how it filters the wrong type of material. there arealso more insidious examples of what the companies that release thissoftware do. for example, if you become known as a critic of that software, mysteriously your web site may appear on the list of blocked websites, which becomes an extraordinary blacklist of banned books. theproblem with this blacklist of banned books is that the public cannot lookat it. it is a secret listña secret list of filtered sites that is being sold to thepublic on account of parentsõ legitimate desire to find a way to protecttheir children.17.3possible solutionsmy view is that there is a mixture of government and market actionsthat could help facilitate the type of control that parents deserve whileminimizing the bad effects of this west coast code. i will describe twoversions of it. one is more problematic; the other is more invasive.imagine a browser that allows you to select grated surfing. as thebrowser perused the web, the client would signal to the server that thisperson wants grated browsing. this means that, if you have materialthat is harmful to minors on your site, you cannot serve that gratedbrowser this material. the necessary law to make the regime work issimply a requirement that sites respect the request that only grated material be sent to a particular client. all that is required is that you forbidpeople from sending socalled òharmfultominorsó material to a browserthat says, òi want grated material.óif there were such a lawñand only that lawñthen there would be astrong incentive for the market to develop many browser technologiesthat would signal efficiently, òi want grated material.ó a family in aparticular house could have many different accounts on the browser, sothat children have grated accounts and the parents do not. the marketwould provide the technology to make that system work.one problem with this system is that, by going around and raisingyour hand and saying, òi want grated browsing material,ó you are alsosaying, òi am likely to be a child.ó people who want to abuse children canthen take advantage of that handwaving in ways that we obviously donot want. there is a way around this problem, but let us move to thesecond solution, which i think solves it more directly.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.larry lessig113imagine a law that says, òyou must, if you have a web site, have acertain tag at the server or the page level that signals the presence of material that is harmful to minors.ó this is the type of judgment that bookstores have to make now. it is not an easy judgment, but it is one alreadyentrusted to booksellers today. an incentive is thereby created in themarket for the development of a grated browser, but this time it does notsignal its use by a child. it simply looks for this particular tag. if it findsthis tag, then it does not give the user access to the web site.this, too, is a mixture of a certain amount of regulation, which saysòyou must tag this content,ó and a certain expectation about how the market will respond. to the extent that parents want to protect their children,they will adopt versions of the browser that facilitate this blocking on thebasis of age. to the extent they do not want to protect their children, theywill not use these types of browsers. but the power either to adopt thetechnology to block access or not will be within the hands of parents.obviously, browsersñat least in the current browser warñare inexpensive; microsoft has promised they will be free forever. thus, the cost ofthe technology implemented from the parentsõ side is very low.the advantage to this approach is that the only people blocked by thissystem are either parents who opt to use the blocking or schools that adoptbrowsers that facilitate blocking to protect children from harmful contentwhile at school. it does not have the overinclusiveness problem that theother solutions tend to have. because the incentive is structured so thatall we need to worry about is material harmful to minors, it does not create an incentive to block much more broadly than what the law legitimately can require.1if geoff stone2 were here, he would say, òyes, but arenõt you forcingweb sites to speak, by forcing them to put these little tags on their systems? and so isnõt this a compelled speech, and isnõt that a violation ofthe constitution?ó i think the answer is no, because the relevant compelled speech is not that you must display on your web site a banner that1milo medin said he likes this scheme because there are many ways of implementing itñnot only in a browser, but also as a service that a user could buy from a network provider.the provider would be able to look at the tags as part of the caching process, and peoplewould not be subject to the usual workarounds on the software side. another appealingaspect is that it puts all the people who want to cooperate on one side of the issue. the otherpeople do not want to cooperate and do not want their stuff to be restricted. the question is,what incentives do these people have? many personal publishers, who publish just becauseit is fun, would be affected directly by this. it would not affect the large companies, becausethey would act rationally.2geoff stone, from the university of chicago, spoke on the first amendment at thecommitteeõs first meeting, in july 2000.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.114constitutional law and the law of cyberspacesays, òthis is material harmful to minors.ó it is not that you must, in anypublic way, advertise this characteristic. you simply enable the web siteto label itself properly through the html code in the background. thesupreme court has upheld the right of states to force providers of material harmful to minors to discriminate in the distribution of this material.it seems to me perfectly consistent with that opinion to say that sites thathave this type of material must put a hidden tag in it that facilitates thetype of blocking that would enable parents to regain some kind of control.geoff stone taught me the first amendment, so i understand his perspective toward it. but i think he is undercounting how this action looksin light of the other things congress has done. there is a certain pragmatic character to how the supreme court decides cases; the court willnot say the congress can never do anything until the end of time. thistype of regulation seems to me to be a relatively slight intrusion thatwould facilitate a better freespeech environment than would exist in theabsence of any federal regulation. if we had no federal regulation at all,the result would be, for example, the blocking of many sites about contraception using private filters. in this way, the first amendment world isworse without this regulation than with it.the necessary condition for success is not an agreement about whatmaterial is harmful to minors but rather what the language of the harmfultominor tag would be. the former would be left to the ordinary system of letting people decide what the character of the material is and selfrating. the standard imposed by the supreme court is that you mustadopt the leastrestrictive means. cda1 failed because it was overlybroad in trying to regulate things that were clearly not speech harmful tominors and because it created too much of a burden on users by requiringthem to carry ids around if they wanted to use the web. i think cda2will be struck down because it continues to require that you carry an id.these burdens would have to be borne by everyone who wanted to usethe web, just so that children could be protected.in my scenario, the burden is borne by web site administrators, whoalready are spending extraordinary amounts of money developing theirweb pages. it is just one more tag. no one can argue that the marginalcost of one more tag is expensive. what is expensive is making a judgment about your web site. but if you are in the pornography business,then it is an easy judgment. if you are in the business of advising childrenabout access to contraception, then i think it is an easy judgment. thestarr report3 is not harmful to minors. there would be difficult cases, butthe law passed by congress requires these difficult decisions anyway.3this is a reference to the 1998 report by independent counsel kenneth starr on presidentclintonõs relationship with a white house intern.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.larry lessig115i envision the grating feature as an optin setting on a browser. itcould be a default instead,4 but i contend that if parents do not know howto turn on the grating feature, then they ought to learn. constitutionally,opting out clearly is different from opting in. the way to analyze theconstitutional balancing test is as follows: is the additional burden placedon the 100 million people who do not have children and do not care aboutprotecting children worth the advantage of making sure that the 60 million people who do want to protect children do not have to take any extrasteps? i cannot predict how this type of judgment would be made. but asthe market develops, people will start branding themselves, much likeaol has done. one reason why aol likes the existing system so much isthat the company draws a lot of parents to its content, because it has takenmany steps to provide for them.age verification would be performed by the family in switching thebrowser on or off the grating setting. this is the big difference betweenthis type of a solution and the cda type of solution, in which age verification is done over the internet. with age verification over the internet, theincentives for cheating are big, so the system needs to be sophisticatedenough to prevent it.my proposal suggests a twotier system in a library setting,5 with onetier available to children and either available to adults. just as librariesnow might have an adult section that is not accessible to children, you canimagine having some browsers that are grated and others that are not. itis difficult to know the libraryõs role in enforcing the rule on children,however. some libraries have adopted the practice of requiring a childõslibrary card to be marked. i am less concerned about libraries enforcingthis rule when only a tiny fraction of speech is being regulated, as opposed to many types of speech. it does suggest some minimal role forlibrarians.64linda hodge noted that most parents are not using filters and suggested that the grating feature be a default, requiring action to opt out. to disable the grating feature, a usercould change the default setting. milo medin said the isps supply browsers and provide anoption either at startup or in an upgrade panel that asks the user to òcheck this or that.ó5marilyn mason said that one of the most troublesome things about the current legislationis that it puts the burden of deciding what is harmful to minors on the shoulders of everyschool and library. she said aspects of lessigõs proposal are appealing: the leastrestrictivesetting becomes the norm, the list of what is grated or not is public, a challenge is a publicevent, public agencies are removed from the middle, and millions of people are relieved ofthe burden of deciding what òharmful to minorsó means.6marilyn mason said the tier system could be handled with a library card or smart card.an adult has an adult card so there is no problem. children have their parents sign for theircards. if a parent wants a child to have unlimited access, then the card can be so coded. thecards can be read by machine. david forsyth said librarians have told the committee thattechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.116constitutional law and the law of cyberspacethere are problems with the system i have described, but only withthose involving a state regulation that attempts to guarantee that materialharmful to minors is not handed over to children without the permissionof parents. my concept is more complicated because it involves theinternet, but it anticipates the same type of problem that exists in the majority of states now, when material like this is distributed.sites would have to do selfrating. importantly, the selfrating wouldnot go beyond this category of harmful to minors. pics technology, theplatform for internet content selection, enables site rating in a wide rangeof circumstances. pics is the same technology as p3p, the platform forprivacy preferences project, but is applied to material harmful to minors.(i am skeptical of pics because it enables general labeling, which is muchbroader than the legitimate interest at issue when dealing with materialharmful to minors. its architecture is such that the label or filter can beimposed anywhere in the distribution chain. if the world turned out theway the pics author wanted, you would have many rich filtering systemsthat could become the tools of censors who wanted to prevent access tospeech about china or the like. my proposal involves a much narrowerlabel.)to avoid asking a site to slander itself, the label could be an equivalent to the one on cigarette packets. this label does not say, òi think this isharmful to your health.ó it says only that the surgeon general thinkscigarettes are harmful to your health. an equivalent entity could findmaterial harmful to minors. the label would not actually say thisñitwould be a computer code, of course. on the other hand, i could revealthe code and see it, so you might say that this is equivalent to selfslander,although i am not sure where the harm is. the label means that the speechis of a class that can be restricted. we could make up a word and call itòxyz speech.ó i can be required to block childrenõs access to xyz speech.the law cannot force me to keep the speech away from my own children.all this does is improve the vocabulary of the space so that people canmake decisions in a relatively consistent frame.they already monitor library activity and discourage users who are making others uncomfortable or behaving inappropriately. it might not be necessary for a library to require children to identify themselves before using the internet; the òtap on the shoulderó mechanismprobably can deal with it. milo medin said this approach moves the incentive for labeling ordoing the labor to the content publishers, as opposed to the people who do not want to beaffected. this localizes the problem and trims a wide range of responsibility. labelingprovides the negative incentive needed for the system to work.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.larry lessig117we cannot simply create a dotxxx space for material harmful to minors because there are other types of potentially harmful speech besideshardcore pornography. here geoff stone would appear in full force, andi am behind him now. the fact that you force me to go into a dotxxxspace is harmful to me if i do not convey hardcore pornography but ratherother material that perhaps should not be given to children. you are forcing me to associate with a space that has a certain kind of meaning. if thatwere the only option, then maybe it would be constitutionally acceptable.but there is no reason to force me to associate with the hardcore pornographers when an invisible filtering/zoning system, such as the p3p labelsin the html tag, can be employed instead. i can be a dotcom and betagged. some of my web pages would be blocked to a child, whereasothers would not. because i have both types of content, i contend that ishould be free to be a dotorg or dotcom and not be forced into the dotxxx ghetto.of course, a site might take the position that the first amendmentprotects it in delivering my material to children, regardless of what theparents think. the parents might have a different view, thinking theyshould be allowed to block access to that site. the point about this structure is that the question would be resolved in a public context. if theparents believe that this material properly is considered harmful to minors, and the site refuses to label it as such, then there would be an adjudication of whether this is material harmful to minors. i am much happierto have this adjudication in the context of a first amendment tradition,which does limit the degree to which you can restrict speech, as opposedto a cyberspace board meeting, where the real issue is, òhow is this goingto play in the market if people think weõre accepting this kind of speech?óin my view, we can ensure more protection of free speech if we have thatargument in the context of adjudicators, who understand the tradition offree speech that we are trying to protect.i want to emphasize that it would be stupid and probably unconstitutional to make the requirement to label punishable through a criminalsanction. we want to keep the punishment low in order to preserve thisproposed system against constitutional challenge. to the extent that youraise the punishment, the supreme court is likely to say, òthis is too dangerous, and it will chill speech if you threaten 30 years in jail becausesomeone failed to properly tag a site.ó alternatively, i like causes of action. i push this in the context of spam all the time. a cause of actionmight be one in which bounty hunters were deployed to find sites thatthey believe are harmful to minors. they would then employ some system for adjudicating this issue. then you would get lots of efficient enforcement technology out there, for people who really care about this issue, and the enforcement would be enforced in a context in which thetechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.118constitutional law and the law of cyberspacefirst amendment is the constraint as opposed to a corporate context inwhich the board worries about public relations.you have to implement this solution step by step. you have to beopen to the fact that we do not understand well enough how the differentfactors interact. we can make speculations, but we need to use real datato analyze it, and this requires some experience in taking one step andevaluating it. the web is the first place to worry about. you could playwith that for a year or more and see what works, and then decide whereelse you need to deploy this solution. usenet is a network that uses anntp protocol. an isp can decide which protocols to allow across its network. it might say, i am a grated isp and will not allow any usenetservices to come across. sometimes people get access to the usenetthrough the web. in these cases, you can still require the same kind offiltering. it is only in the context of getting access to usenet outside of theweb that a problem arises.717.4practical considerationslet me map out a sample proceeding. let us say there has been afailure to properly tag something that is, in fact, harmful to minors. imagine that something like a bounty is available. the bounty hunter bringsan action: hopefully not a federal court action. in principle, anyone couldbring the action. the person says, òthis site by playboy has material thatis properly considered harmful to minors, and they have not implementedthis tag.ó then there has to be a judgment about whether the material is,in fact, harmful to minors. a court must make this type of judgment, asthey always have done. it is difficult in some cases, but the public haslong survived this judgment being made in real space. if the court findsthat this is material harmful to minors and the site has not put up this tag,then there would be some sanction. i think the sanction should be a civilsanction, such as a fine, sufficient to achieve compliance, that is, set at a7dick thornburgh said the person doing the conversion from usenet to the web wouldend up doing the labeling, not the person who posts the content. in this example, the problem is not difficult to solve. but the generic issue is that there is some level of restriction onthe connection; it is not necessarily a complete removal of either an intermediary or softwareon the pc, although it greatly facilitates things. there is no reason why you could notenforce the same type of labeling requirement on the publisher. there is usually a way oflabeling files available via file transfer protocol or other types of protocols, for example. itcould apply to chat groups, instant messaging traffic, and so on. the key point is to shift theburden, make it general enough that people have an incentive to cooperate, and enablebounty hunters so the marketplace can police it and you would not necessarily need lawenforcement.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.larry lessig119level such that a rational businessperson thinks, òitõs cheaper for us tocomply.óyou could assume that no one would comply with this law, that therewould be thousands of these prosecutions, and it would bog down thecourts and end up like the war on drugs. this situation would be similarto a denialofservice attack8 and would prove that this system is terrible.on the other hand, you could assume that people will behave rationallybased on what they expect the consequences and cost of compliance willbe. then the world segregates into a vast majority that are willing tocomply because it is cheaper and they do not wish to violate the law anyway and a smaller number that we have to worry about controlling.a bounty action could be structured so that the first to file gets tolitigate, and, after a judgment is rendered, that is the end of it. if a frivolous action is filed, it should be punishable by a filing for malicious prosecution. a class action analogy is possible, but the cumbersome nature ofclass actions now might make it simpler to have just a single action. i donot think it is possible to eliminate the possibility of a proliferation ofactions, but there are ways to try. for instance, we could limit it by geographic district, for example, to avoid the problem of trying to sue someone across the country and imposing that type of burden. a lot of creativethinking will be needed. a qui tam action9 could be troubling constitutionally. there are people who believe that a party should be found tolack standing unless there is a demonstration of harm.10 but there is sucha long tradition of qui tam that, like bounty actions, it will survive.the one area of this jurisprudence that has not been developed iswhether and how the community standards component of the traditionalobscenity doctrine applies in the context of material harmful to minors.there is a need for the courts to figure out something new. the decisionin the third circuit, aclu v. reno, 217 f.3d 162 (3rd cir. 2000), striking8david forsyth sought to draw an analogy to a denialofservice attack in which a largenumber of people do a small inappropriate thing on a network and overload the systemadministrator. in the legal context, a sufficient number of small bountyseeking actionsfrom enough different people would bring the system to a halt.9a qui tam action is one filed in court by a private individual who sees some misconductthat is actionable under the law. if the individual prevails in court, he or she is entitled tosome of the proceeds that the transgressor must pay.10david forsyth questioned whether bounty hunters could participate in civil actions,because he thought that some harm had to be demonstrated in order to sue. dickthornburgh said that, in a qui tam case, the evidence brought forth as the basis of the actionmust be something peculiar to the individual. a person cannot walk in off the street andbring a qui tam claim by showing a simple fact such as a lack of a tag on a program. theseclaims are numerous within an industry where evidence has been accumulated and there isonly one person or a small group of people who could bring an action.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.120constitutional law and the law of cyberspacedown the most recent action of congress made it sound as if there is nopossible way to get over the community standards problem when tryingto regulate this material in cyberspace, because there are so many different communities and problems associated with applying different typesof tests. what if the architecture requires you to label or unlabel depending on where, geographically, a person is coming from? the way thearchitecture is now, it is relatively difficult to figure out where a user islocated. this is where the additional layer of community standards becomes difficult to architect. i confess that i do not know how to solve thisproblem.the supreme court is difficult to predict. my confidence in predicting what this court will do has dropped dramatically in the last year, so iwill not predict how the court will resolve this issue. but i cannot believethat it will decide that nothing can be done. the resolution will not bethat one standard fits the whole nation either; the court will instead attempt to find some compromise. in a sense, it has struck the same balancein real space through the same legal standard applied to realspace materials.this leads to the question of how the community standards issuewould play out in a place like a library, which serves a wide range ofpeople, presumably with different ideas of what is harmful. if there werethousands of lawsuits, this could create a chilling effect on free speech,because people would think, òwell, every time i have a certain type ofspeech on my site, iõm going to get into a lawsuit. it will be blocked, soiõm not going to have that speech on this site (without labeling).ó yet weoften forget that, with the existing censorware, web sites already makethe same judgment. they say, òhmm. i want to avoid getting on thecybersitter list. i want to include this interesting information about howto get contraception in certain cases, but itõs too dangerous, because thisspeech will be filtered. when my speech is filtered in the context ofcybersitter, there is no court to which i can go to order that it is improper to filter my speech. i am stuck.óin other words, there is already a chilling effect on free speech createdby these invisible blacklists that spread across cyberspace. i do not thinkwe can avoid some chilling effect. the question is how to minimize it.focusing on a legal standard that is interpreted in a legal context is a wayto minimize the chilling effect and maximize the amount of speech thatcan be protected.òchilló has a more precise meaning than just causing you to not postmaterial. it means that you are uncertain and afraid of punishment, soyou choose not to post what otherwise you should be allowed to post. itis the variance (the uncertainty in application) that we are concernedabout. given the range of private censors, the variance that we need totechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.larry lessig121consider is much greater than it would be if there were a single standarddefining material harmful to minors. thus, i think that òchilló has greatermeaning in the private censorship context than in the government context. this is not to say that we could not imagine the court developing adoctrine such that people are terrified and do not do anything. that is anunavoidable consequence if you screw it up and would be terrible for freespeech. maybe this is a lawyercentric view, but i am much happier if thatbattle occurs in court, because then i have the right to argue that this standard is wrong and inconsistent. when it is done in the private censorshipcontext, i do not have the right to make that argument.here is the disingenuous part of my scenario. it is extremely difficultto say what the standard òharmful to minorsó means. the burden is onthe government or prosecutor to demonstrate that this material is harmfulto minors. i have the right to free speech until the state can demonstratethis. but what does the government actually have to show? the government does not need to show data that demonstrate the harm. the waythese cases are typically litigated involves comparisons to òlike kindsó ofmaterial. obscenity is harmful to minors. as the court said, the sort ofsexually explicit speech that appropriately is kept from children is likeobscenity to children.to date, òharmful to minorsó has been interpreted by the supremecourt to include sexually explicit speech only. it does not include hatespeech, for example. there is a lower court judgment that expands theinterpretation, but i donõt believe that interpretation will be sustained.therefore, in my view, the legitimate interest of the government has beenprescribed to include only sexually explicit speech. i am sure that peoplewill try to bring other types of speech to the courts. but i am also sure thatthe supreme court would look at ku klux klan (kkk) speech, for example, and say, òit is terrible speech, i agree, but this is the core of firstamendment type of speech that we must protect.ó we will get into anargument about whether 6yearolds should see kkk speech, and thiswill be difficult for the court.i have no kids and i do not look at this material. i have no way offiguring out how to draw the line. but part of the solution is to realizethat no one will have a complete solution. we depend on the diversity ofinstitutions to contribute their parts. some part has to be contributed bypeople making judgments. in a paper that i wrote with paul resnick,who was originally on this committee, we described techniques for minimizing the cost of determining what òharmful to minorsó means. geoffstone would look at some of these techniques and say, òno, no, the constitution would forbid them.óimagine a site asking a government agency, òcan you give me a signthat this material is okay?ó this is like a promise not to prosecute, and ittechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.122constitutional law and the law of cyberspaceis done now. it amounts to preclearance of material that is on the borderline. it is not saying that you cannot publish unless you get permission. itis not saying that if you do not get permission, you cannot publish. all itmeans is that if you get preclearance, there is a guarantee that you will notbe punished. it is a safe harborñit takes care of the òchilló problem. if thegovernment says, òwe canõt give you a safe harbor here,ó then you have aproblem. then you must decide whether it is worth the risk to speak.but, again, this is a problem we face now. people currently make thisdecision when they decide how to distribute material in more than half ofthe states. we should minimize the cost of that problem, but i do notthink we can say the constitution requires us to make that cost zero.as times and standards change, crude standards help, because a finegrained system would become outofdate.11 because this discriminatoris so crude, i think that what happens in cyberspace would mirror whatwould happen in real spaceñpeople only worry about and prosecute theextreme cases. there is a lot of material floating around that nobodywastes time worrying about. but, in principle, we would have to worryabout how things are updated over time. in cyberspace, 10 years is a longtime. i am not sure what the burden of that is. my personal preference isthat we do as little as possible but enough to avoid the problem of toomuch private censorship. the system also needs to be sensitive to whatwe learn about the consequences of what we do.this solution will not eliminate all private filtering. but my view isthat a significant amount of demand for private filtering results from thelack of any lessrestrictive alternative. if you asked the filtering companies, 90 percent of them would say, òwhat lessig is talking about is terrible and unconstitutionalóñbecause it would drive 90 percent of themout of business. but there still would be parents who are on the christianright, for example, and who want to add another layer of protection ontop. we will not go from a world of perfect censorship to perfect freespeech, but a balance is needed between the two. under the existing system, we have so many examples of overreaching and private censoringthat some way to undermine it is needed.given the international context for the internet, this solution is not a11bob schloss asked who would label orphan content, which is floating around on theinternet or on hard disks but whose publisher is dead or not paying attention, and how thebinary indicatorña yes or no answer to the question of whether something is harmful tominorsñwould hold up over time as community standards changed. it might work for 10years, but in the end, to deal with the problem of both shifting standards and orphan content, the system could end up with a thirdparty rating process again.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.larry lessig123complete one. but our nation is very powerful. when you set up a simplesystem for people to comply with, and there is some threat that they willbe attacked by the united states if they are not in compliance, then it willbe easier for most people to comply. tiny sanctions and tiny compliancecosts actually have a significant effect on convincing people to obey.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.124appendix:biographies of presentersnicholas belkin has been professor of information science in the schoolof communication, information, and library studies at rutgers university since 1985. prior to that appointment, he was lecturer and then seniorlecturer in the department of information science at the city university,london, from 1975. he has held visiting positions at the university ofwestern ontario, the free university of berlin, and the integrated publication and information systems institute of the german national researchcenter for computer science and was visiting scientist at the institute ofsystems science of the national university of singapore and a fulbrightfellow at the department of information studies, university of tampere,finland. professor belkin was chair of the acm special interest group oninformation retrieval from 1995 to 1999 and is a member of the steeringcommittee for the acm/ieee joint conferences on digital libraries. hereceived his ph.d. in information studies from the university of londonin 1977 and a masterõs in librarianship from the university of washingtonin 1970.michel bilello holds a ph.d. degree in electrical engineering and an m.d.degree, both from stanford university. his most recent research includessecurity mediation for secure dissemination of medical information. hehas recently completed his internship in internal medicine.fred cotton is director of training services for search, the nationalconsortium for justice information and statistics. he provides technicalassistance and training to local, state, and federal criminal justice agenciesnationwide in information systems, including assistance in computertechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.biographies of presenters125crimes investigations and examining seized microcomputers. he instructsa variety of technology crimes courses that search offers at its nationalcriminal justice computer laboratory and training center in sacramento, california, and at other sites nationwide, and he oversees a training staff of eight. he has also taught advanced officer courses and officersafety subjects in the basic police academy and was an invited guest ofnorwayõs national bureau of criminal investigation, where he providedtraining on computer investigations. mr. cotton has 13 years of fulltimelaw enforcement service as a field supervisor with experience in operations, investigations, records, training, and data processing. in addition tohis duties at search, he is a reserve police officer with the yuba city,california, police department, where he is assigned to the sacramentovalley hightech crimes task force, and a specialist reserve officer withthe los angeles police department, where he is assigned to the organized crime and vice division. mr. cotton is a member of the floridacomputer crime investigators association, the forensic association ofcomputer technicians, the northern california chapter of the high technology crime investigation association (htcia), the national technicalinvestigators association, the georgia hightech crime consortium, themidwestern electronic crime investigation association, the americansociety of law enforcement trainers, and police futurist international.he is a former member of the national board of directors of htcia. inseptember 1999, the international board of directors of htcia selectedhim as the first recipient of its distinguished achievement award. mr.cotton is certified by the california commission on peace officer standards and training as a òcomputer / whitecollar crime investigatoró forthe state of california through the robert j. presley institute of criminalinvestigation (ici), and he is an icicertified instructor. he is also a graduate of and has been a guest instructor at the òseized computer evidencerecovery specialistó training course offered through the federal law enforcement training center in glynco, georgia, and he has qualified andtestified as an expert witness on computer investigations in both countyand federal courts. mr. cotton holds a degree in administration of justiceand is an adjunct professor in the forensic computer investigation certificate program of the university of new haven, connecticut.donald eastlake has over 30 years of experience in the computer fieldand was one of the principal architects and specification authors for thedomain name system security protocol. he cochairs the joint ietf/w3cxml digital signature working group, chairs the ecommerceorientedietf trade working group, is a member of and coeditor of the specification for the w3c xml encryption working group, and a member ofthe java community group developing xml security apis. he is a memtechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.126appendixber of the technical staff at motorola and has previously worked for ibm,cybercash, digital equipment corporation, transfinite systems company, computer corporation of america, and the massachusetts instituteof technology.susan getgood is one of corporate americaõs leading experts on education and internet safety. she has testified on behalf of the industry beforecongress, the national research council, the federal trade commission,and the childrenõs online protection act commission. prior to assuming her current role, she served as the learning companyõs director ofcorporate communications, where she developed a broad knowledge ofthe role of technology in teaching both children and adults. she previously was director of marketing at microsystems software, thecompany that developed cyber patrol internet filtering software. she represented microsystems software when it was among the coalition of plaintiffs who successfully challenged the constitutionality of the communications decency act in 1996. in december 1997, she was part of a panel oninternet filtering and other technologies to protect children at the whitehousebacked òinternet/online summit: focus on kids.ó she has beeninvolved in issues related to creating positive digital content for childrenand the development of quality educational content for homes andschools. she is on the board of directors of mass networks and was recently named to the executive committee of this publicprivate partnership dedicated to enhancing education through technology in the state ofmassachusetts.bennett haselton has been publishing reports on the workings of internetblocking software since 1996. his web site at peacefire.org has functionedas a clearinghouse of information related to internet blocking softwareand has been featured in reports on cnn, courttv, cnnfn, mtv news,and msnbc. bennett holds an m.a. in mathematics from vanderbiltuniversity and lives in seattle. he currently works as a òcontract hacker,ófinding security holes in internet applications on a commission basis.chris kelly has served in a variety of business, educational, and governmental roles over the past 10 years aimed at bettering the ways we provide customer service, teach our children, and live our everyday livesonline. currently chief privacy officer for excite@home, the leadingbroadband online service provider, he is responsible for the companyõsprivacy policy and practices, working with public policy makers and industry leaders on consumer privacy initiatives and educating the publicregarding the companyõs commitment to online privacy. he brings extensive experience in information technology, law, and public policy tohis role at excite@home. at kendara, a 40person nextgeneration digitaltechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.biographies of presenters127marketing startup acquired by excite@home in 2000, he served as one ofthe online industryõs first chief privacy officers, overseeing product architecture development and data management practices to ensure consumerprivacy. prior to kendara, he was an attorney in the antitrust and intellectual property groups at palo alto law firm wilson, sonsini, goodrich &rosati, where he counseled numerous internet companies on privacy policies, terms of service, and other web site service concerns. at wilsonsonsini, he also advised numerous silicon valley companies on the implications of the governmentõs antitrust suit against microsoft and on theapplication of intellectual property concepts in the digital age. as a fellow of the berkman center for internet and society at harvard lawschool, he worked on a variety of internet public policy issues, includingspam prevention, privacy protection, and technologyõs impact on education. he has also taught cyberspace law as an adjunct professor at thecalifornia western school of law in san diego. in the community, heserves on the board of directors of greatschools.net, a notforprofit onlineòzagatõs guideó to every school in california and arizona, assisting thecompanyõs nationwide expansion efforts. he also participates in the paloalto area bar associationõs lawyers in the schools program, teachinghigh school students basic legal concepts through interactive role playingexercises. he is a member of the state bar of california and the americanbar association. he has served as a policy analyst for the white housedomestic policy council and as a special assistant at the u.s. departmentof education. he holds a j.d. from harvard law school, a masterõs degree in political science from yale university, and a b.a. from georgetownuniversity, where he was elected to phi beta kappa. at harvard, he waseditor in chief of the harvard journal of law and technology and was part ofthe founding team for the berkman center for internet & society.ray larson specializes in the design and performance evaluation of information retrieval systems and the evaluation of user interaction with thosesystems. his background includes work as a programmer/analyst withthe university of california (uc) division of library automation, wherehe was involved in the design, development, and performance evaluationof the uc public access online union catalog (melvyl). his research hasconcentrated on the design and evaluation of information retrieval systems. he is the designer of the cheshire ii information retrieval system,which is being used as a search engine at numerous sites in the unitedstates and europe. the ranking algorithms developed in the cheshire iiproject are the basis of the inktomi search engine used by yahoo and otherworld wide web search portals. he was a faculty investigator on thesequoia 2000 project, where he was involved in the design and evaluationof a verylargescale, networkbased information system to support thetechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.128appendixinformation needs of scientists studying global change. he is also a faculty investigator on the uc berkeley environmental digital libraryproject (sponsored by the national science foundation (nsf), the national aeronautics and space administration, and the defense advancedresearch projects agency (darpa)), where the work is continuing on avery large environmental information system providing access to information on the california environment. he is the principal investigator onthe international digital libraries initiative sponsored by nsf and thejoint information systems committee in the united kingdom. he is a coprincipal investigator on other projects sponsored by darpa and the institute for museum and library studies. he has consulted on informationretrieval systems and automatic classification methods with major corporations, including sun microsystems, american express, and inktomi. hehas also consulted on international information system projects in theunited states and the united kingdom, including the networked socialscience tools and resources project and the òarchives hubó linking archival collections in u.k. research libraries.david lewis is a consultant based in chicago, illinois. he works in theareas of information retrieval, machine learning, and natural languageprocessing. prior to taking up consulting, he was a researcher at at&tlabs and bell labs and a research faculty member at the university ofchicago. lewis received his ph.d. in computer science from the university of massachusetts at amherst in 1992 and has undergraduate degreesin computer science and mathematics from michigan state university.he has published more than 40 papers, holds 5 patents, and helped todesign the u.s. government message understanding conference and textretrieval conference evaluations of language processing technology.david maher has served as chief technology officer of intertrust sincejuly 1999. before joining intertrust, he was an at&t fellow, divisionmanager, and head of the secure systems research department at at&tlabs, where he was working on secure ip networks and secure electroniccommerce protocols. he joined bell labs in 1981, where he developedsecure wideband transmission systems, cryptographic key managementsystems, and secure communications devices. he was chief architect forat&tõs stuiii secure voice, data, and video products used by the whitehouse and u.s. intelligence and military personnel for top secret communications. in 1992 maher was made a bell labs fellow in recognition of hiswork on communications security. he was also chief scientist for at&tsecure communications systems overseeing secure systems r&d at belllabs, gretag data systems in zurich, and datotek systems in dallas. in1993, maher designed the information vending encryption system usedto provide a òvirtual vcró video payperview system for cable networks.technical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.biographies of presenters129in 1995, he worked with at&t universal card services, where he designed and analyzed a number of electronic payment systems and servedas a member of the mondex international security group. he has published papers in the fields of combinatorics, cryptography, number theory,signal processing, and electronic commerce. he has been a consultant tothe national science foundation, national security agency, national institute of standards and technology, and the congressional office oftechnology assessment. he has a ph.d. in mathematics from lehigh university, and he has taught electrical engineering, mathematics, and computer science at several institutions and was an associate professor ofmathematics at worcester polytechnic institute. he currently serves onthe computer science and telecommunications board committee investigating networked systems of embedded computers.deirdre mulligan is acting clinical professor of law and director of thesamuelson law, technology, and public policy clinic at the boalt hallschool of law, university of california at berkeley. prior to joining boalt,she was staff counsel at the center for democracy and technology, whereshe focused on privacy and first amendment issues. she serves on thecomputer science and telecommunications board committee studyingauthentication techniques and their implications for privacy.brian pass is a partner with the law firm of brown, raysman, millstein,felder, and steiner llp, heading the firmõs west coast technology practice from its los angeles office. mr. pass represents clients in the licensing, development, and distribution of computer software; hardware development and oem relationships; new media and web site licensing,development, and marketing; intellectual property and trade secret protection; broadband communications; interactive television; and ecommerce. mr. pass counsels companies on startup formation and venturecapital finance, joint venture formation, and mergers and acquisitions. healso advises companies on internet privacy and other regulatory issuesaffecting new media and ecommerce. before joining brown raysman, heserved as president, chief executive officer, and cofounder of passportnew media, where he led the development of passportõs critically acclaimed childrenõs internet service, your own world. at passport, heraised $7.5 million in venture capital and led a team of over 30 employees,while concluding numerous thirdparty content partnerships and negotiating key technology and distribution relationships. he also served asvice president and general counsel at americast, a joint venture of thewalt disney corporation and several of the baby bell telephone companies, to develop interactive digital television systems. in addition to advising the americast partnership and its board on all general corporatematters, he negotiated and administered numerous technology purchastechnical, business, and legal dimensions of protecting children from pornography on the internet: proceedings of a workshopcopyright national academy of sciences. all rights reserved.130appendixing and licensing agreements, including a $1 billion settop box purchaseagreement; an $80 million dollar hardware purchase agreement; a multimillion dollar intellectual property licensing agreement; and numeroussoftware development and licensing agreements. he graduated fromwesleyan university in 1986 with high honors in the college of socialstudies and received his j.d. from the ucla school of law in 1991.is chief technical officer and cofounder of novation biosciences, a data and text mining company serving the pharmaceutical industry. he was formerly cofounder and vice president of advanced development for outride, inc., where he applied stateoftheart relevancetechnology to the challenge of information retrieval.eddie zeitler was a senior vice president at charles schwab & co., inc.,through march 2001, where for 5 years he managed the information security department, which comprised six specialized units: information access and protection, information security technology, information security risk management, information security strategy and architecture,business contingency planning, and security awareness and training.mr. zeitler has a varied background in computers and information processing. prior to charles schwab, he managed the information securityfunctions at fidelity investments, bank of america, and security pacificnational bank. other management positions include the capacity planning function for security pacific national bankõs computer centers, technical services (operating systems and software) and computer center operations for the national data center of federated department stores,and data center performance and configuration for transamerica information services. he began his career developing the operating systemused on the shuttle orbiter at rockwell international and radar systemcontrols at itt gilfillan. external activities include participation on various committees such as the los angeles county computer crime taskforce, the department of the treasuryõs financial management servicessecurity advisory panel, the ansi x9.e9 and x9.f2 working groups forsecurity of financial systems, the u.s. treasuryõs electronic funds transfer task force subcommittee on interoperability, the aba informationsystems security committee, the (isc)2 qualifications review committee, the national computer system security and privacy advisory board,and the national research councilõs panel for information technologythat annually reviews the national institute of standards andtechnologyõs information technology program. mr. zeitler is a registeredbrokerage representative (series 7 and series 63) and is a certified information systems security professional. he holds a b.s. in mathematics andan m.s. in systems engineering from the university of arizona. he alsocompleted his ph.d. candidacy in computer science at the university ofalberta.
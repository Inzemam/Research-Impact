detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/10831information and communications: challenges for the chemicalsciences in the 21st century208 pages | 6 x 9 | paperbackisbn 9780309087216 | doi 10.17226/10831organizing committee for the workshop on information and communications;committee on challenges for the chemical sciences in the 21st century; board onchemical sciences and technology; division on earth and life studies; nationalresearch councilinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.organizing committee for the workshop oninformation and communicationscommittee on challenges for the chemical sciencesin the 21st centuryboard on chemical sciences and technologydivision on earth and life studiesthe national academies presswashington, d.c.www.nap.educhallenges for the chemical sciencesin the 21st centuryinformation andcommunicationsinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.the national academies press ¥ 500 fifth street, n.w. ¥ washington, d.c. 20001notice: the project that is the subject of this report was approved by the governingboard of the national research council, whose members are drawn from the councils ofthe national academy of sciences, the national academy of engineering, and the institute of medicine. the members of the committee responsible for the report were chosenfor their special competences and with regard for appropriate balance.support for this study was provided by the national research council, the u.s. department of energy (deat010ee41424, bes defg0200er15040, and deat0103er15386), the national science foundation (cts9908440), the defense advancedresearch projects agency (dod mda97201m0001), the u.s. environmental protection agency (r82823301), the american chemical society, the american institute ofchemical engineers, the camille and henry dreyfus foundation, inc. (sg00093), thenational institute of standards and technology (na13410121070 and43nanb010995), and the national institutes of health (ncin01od42139 andnigmsn01od42139), and the chemical industry.all opinions, findings, conclusions, or recommendations expressed herein are thoseof the authors and do not necessarily reflect the views of the organizations or agencies thatprovided support for this project.international standard book number 030908721x (book)international standard book number 0309526876 (pdf)additional copies of this report are available from:national academies press500 fifth street, n.w.box 285washington, dc 2005580062462422023343313 (in the washington metropolitan area)http://www.nap.educopyright 2003 by the national academy of sciences. all rights reserved.printed in the united states of americainformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.the national academy of sciences is a private, nonprofit, selfperpetuating society ofdistinguished scholars engaged in scientific and engineering research, dedicated to thefurtherance of science and technology and to their use for the general welfare. upon theauthority of the charter granted to it by the congress in 1863, the academy has a mandatethat requires it to advise the federal government on scientific and technical matters.dr.bruce m. alberts is president of the national academy of sciences.the national academy of engineering was established in 1964, under the charter of thenational academy of sciences, as a parallel organization of outstanding engineers. it isautonomous in its administration and in the selection of its members, sharing with thenational academy of sciences the responsibility for advising the federal government.the national academy of engineering also sponsors engineering programs aimed at meeting national needs, encourages education and research, and recognizes the superiorachievements of engineers. dr. wm. a. wulf is president of the national academy ofengineering.the institute of medicine was established in 1970 by the national academy of sciencesto secure the services of eminent members of appropriate professions in the examinationof policy matters pertaining to the health of the public. the institute acts under the responsibility given to the national academy of sciences by its congressional charter to be anadviser to the federal government and, upon its own initiative, to identify issues of medicalcare, research, and education. dr. harvey v. fineberg is president of the institute ofmedicine.the national research council was organized by the national academy of sciences in1916 to associate the broad community of science and technology with the academyspurposes of furthering knowledge and advising the federal government. functioning inaccordance with general policies determined by the academy, the council has become theprincipal operating agency of both the national academy of sciences and the nationalacademy of engineering in providing services to the government, the public, and thescientific and engineering communities. the council is administered jointly by bothacademies and the institute of medicine. dr. bruce m. alberts and dr. wm. a. wulf arechair and vice chair, respectively, of the national research council.www.nationalacademies.orginformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.ivorganizing committee:workshop on information and communicationsrichard c. alkire, university of illinois, urbanachampaign, cochairmark a. ratner, northwestern university, cochairpeter t. cummings, vanderbilt universityjudith c. hempel, university of texas, austinkendall n. houk, university of california, los angeleskenny b. lipkowitz, north dakota state universityjulio m. ottino, northwestern universityliaisonsignacio e. grossmann, carnegie mellon university (steering committee)peter g. wolynes, university of california, san diego (steering committee)sangtae kim, eli lilly (bcst)john c. tully, yale university (bcst)staffjennifer j. jackiw, program officersybil a. paige, administrative associatedouglas j. raber, senior scholardavid c. rasmussen, program assistanteric l. shipp, postdoctoral associateinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.vcommittee on challenges for the chemical sciencesin the 21st centuryronald breslow, columbia university, cochairmatthew v. tirrell, university of california at santa barbara, cochairmark a. barteau, university of delawarejacqueline k. barton, california institute of technologycarolyn r. bertozzi, university of california at berkeleyrobert a. brown, massachusetts institute of technologyalice p. gast,1 stanford universityignacio e. grossmann, carnegie mellon universityjames m. meyer,2 dupont co.royce w. murray, university of north carolina at chapel hillpaul j. reider, amgen, inc.william r. roush, university of michiganmichael l. shuler, cornell universityjeffrey j. siirola, eastman chemical companygeorge m. whitesides, harvard universitypeter g. wolynes, university of california, san diegorichard n. zare, stanford universitystaffjennifer j. jackiw, program officerchristopher k. murphy, program officersybil a. paige, administrative associatedouglas j. raber, senior scholardavid c. rasmussen, program assistanteric l. shipp, postdoctoral associatedorothy zolandz, director1committee member until july 2001; subsequently board on chemical sciences and technology(bcst) liaison to the committee in her role as bcst cochair.2committee member until march 2002, following his retirement from dupont.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.viboard on chemical sciences and technologyalice p. gast, massachusetts institute of technology, cochairwilliam klemperer, harvard university, cochairarthur i. bienenstock, stanford universitya. welford castleman, jr., the pennsylvania state universityandrea w. chow, caliper technologies corp.thomas m. connelly, jr., e. i. du pont de nemours & co.jean de graeve, institut de pathologie, lige, belgiumjoseph m. desimone, university of north carolina, chapel hill, and northcarolina state universitycatherine fenselau, university of marylandjon franklin, university of marylandmary l. good, university of arkansas, little rockrichard m. gross, dow chemical companynancy b. jackson, sandia national laboratorysangtae kim, eli lilly and companythomas j. meyer, los alamos national laboratorypaul j. reider, amgen, inc.arnold f. stancell, georgia institute of technologyrobert m. sussman, latham & watkinsjohn c. tully, yale universitychihuey wong, scripps research institutestaffjennifer j. jackiw, program officerchristopher k. murphy, program officersybil a. paige, administrative associatedouglas j. raber, senior scholardavid c. rasmussen, program assistanteric l. shipp, postdoctoral associatedorothy zolandz, directorinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.viiprefacethe workshop on information and communications was held in washington,d.c., on october 31november 2, 2002. this was the third in a series of six workshops in the study challenges for the chemical sciences in the 21st century. thetask for each workshop was to address the four themes of discovery, interfaces,challenges, and infrastructure as they relate to the workshop topic (appendix a).the workshop on the information & communications brought together adiverse group of participants (appendix f) from the chemical sciences who wereaddressed by invited speakers in plenary session on a variety of issues and challenges for the chemical sciences as they relate to computational science and technology. these presentations served as a starting point for discussions and comments by the participants. the participants were then divided into small groupsthat met periodically during the workshop to further discuss and analyze the relevant issues. each group provided its discussions to the workshop as a whole.this report is intended to reflect the concepts discussed and opinions expressed at the workshop on information and communications, and it is not intended to be a comprehensive overview of all of the potential challenges thatexist for the chemical sciences in the area of computing. the organizing committee has used this input from workshop participants as a basis for the findingsexpressed in this report. however, sole responsibility for these findings rests withthe organizing committee.this study was conducted under the auspices of the national researchcouncils board on chemical sciences and technology, with assistance providedby its staff. the committee acknowledges this support.richard c. alkire and mark a. ratner, cochairs,organizing committee for the workshop on information andcommunicationschallenges for the chemical sciences in the 21st centuryinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.viiiacknowledgment of reviewersthis report has been reviewed in draft form by individuals chosen for theirdiverse perspectives and technical expertise, in accordance with procedures approved by the national research councils (nrcs) report review committee.the purpose of this independent review is to provide candid and critical commentsthat will assist the institution in making the published report as sound as possibleand to ensure that the report meets institutional standards for objectivity, evidence,and responsiveness to the study charge. the review comments and draft manuscriptremain confidential to protect the integrity of the deliberative process. we wish tothank the following individuals for their participation in the review of this report:c. gordon bell, microsoft bay area research centerbruce a. finlayson, university of washingtonsharon c. glotzer, university of michiganpeter gund, ibm life scienceskenneth m. merz, jr., the pennsylvania state universitydavid h. west, dow chemical companyalthough the reviewers listed above have provided many constructive comments and suggestions, they were not asked to endorse the conclusions or recommendations nor did they see the final draft of the report before its release. thereview of this report was overseen by joseph g. gordon ii (ibm almaden research center). appointed by the national research council, he was responsiblefor making certain that an independent examination of this report was carried outin accordance with institutional procedures and that all review comments werecarefully considered. responsibility for the final content of this report rests entirely with the authoring committee and the institution.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.ixcontentsexecutive summary1background and method, 2findings, 31introduction: the human resource72accomplishments12major themes, 12some specific enabling accomplishments, 143opportunities, challenges, and needs21current status, 22challenges, 234interfaces: cooperation and collaborationacross disciplines29overarching themes, 31targeted design and openended discovery, 31flow of information between people within and among disciplines, 34multiscale simulation, 39collaborative environments, 44education and training, 475infrastructure: capabilities and goals49research, 50education, 51information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.xcontentscodes, software, data and bandwidth, 53anticipated benefits of investment in infrastructure, 55appendixesastatement of task63bbiographies of the organizing committee members64cworkshop agenda67dworkshop presentations,charles h. bennett, 71anne m. chaka, 73juan j. de pablo, 81thom h. dunning, jr., 86christodoulos a. floudas, 116richard friesner, 125james r. heath, 132dimitrios maroudas, 133linda r. petzold, 136george c. schatz,146larry l. smarr, 152ellen stechel, 157dennis j. underwood, 170ebiographies of workshop speakers177fparticipants182greports from breakout session groups185information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.1executive summarysince publication of the national research council (nrc) reports on chemistry in 1985 and chemical engineering in 1988,1,2 dramatic advances in information technology (it) have totally changed these communities. during this period,the chemical enterprise and information technology have enjoyed both a remarkably productive and mutually supportive set of advances. these synergies sparkedunprecedented growth in the capability and productivity of both fields includingthe definition of entirely new areas of the chemical enterprise. the chemical enterprise provided information technology with device fabrication processes, newmaterials, data, models, methods, and (most importantly) people. in turn, information technology provided chemical science and technology with truly remarkable and revolutionary resources for computations, communications, and datamanagement. indeed, computation has become the strong third component of thechemical science research and development effort, joining experiment and theory.sustained mutual growth and interdependence of the chemical and information communities should take account of several unique aspects of the chemicalsciences. these include extensive and complex databases that characterize thechemical disciplines; the importance of multiscale simulations that range frommolecules to technological processes; the global economic impact of the chemical industry; and the industryõs major influence on the nationõs health, environment, security, and economic wellbeing. in planning the future of the chemical1 opportunities in chemistry, national research council, national academy press, washington,d.c., 1985.2frontiers in chemical engineering: research needs and opportunities, national researchcouncil, national academy press, washington, d.c., 1988.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.2information and communicationsciences and technology, it is crucial to recognize the benefits already derivedfrom advances in information technology as well as to point the way to futurebenefits that will be derived.background and methodin october 2002, as part of challenges for the chemical sciences in the 21stcentury, the board on chemical sciences and technology convened a workshopin washington, d.c., on information & communications. the charge to the organizing committee (appendix a) addressed four specific themes:¥discovery: what major discoveries or advances related to informationand communications have been made in the chemical sciences during the lastseveral decades?¥interfaces: what are the major computingrelated discoveries and challenges at the interfaces between chemistry/chemical engineering and other disciplines, including biology, environmental science, information science, materialsscience, and physics?¥challenges: what are the information and communications grand challenges in the chemical sciences and engineering?¥infrastructure: what are the issues at the intersection of computing andthe chemical sciences for which there are structural challenges and opportunitiesñin teaching, research, equipment, codes and software, facilities, and personnel?the workshop organizing committee assembled a group of top experts todeliver plenary lectures (appendix c), and recruited an outstanding group ofchemical scientists and engineersñfrom academia, government, national laboratories, and industrial laboratoriesñto participate in the workshop (appendix f).through extensive discussion periods and breakout sessions, the entire group ofparticipants provided valuable input during the course of the workshop. the results of the breakout sessions appear in appendix g, and written versions of thespeakersõ presentations are provided in appendix d. in combination with otherreferences cited in this report, the data collected at the workshop provide the basisfor this report.the structure of the workshop on information & communications followedthat of the parent project and each of the other workshops that were held as part ofthe study of challenges for the chemical sciences in the 21st century (materialsand manufacturing, energy and transportation, national security and homelanddefense, the environment, and health and medicine).information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.executive summary3findingsthe information presented in this report enabled the organizing committee toreach a series of conclusions. advances at the interface between information technology and chemical technology and science are today revolutionizing the waythat chemists and engineers carry out their work. chapter 2 describes accomplishments in the professional development and teaching of people; in methods,models, and databases; and in processes and materials. new tools offered byinformation technology are fundamentally reshaping research, development, andapplication activities throughout the chemical sciences. the traditional boundaries between chemistry and chemical engineering are becoming more porous,benefiting both disciplines and facilitating major advances.finding: boundaries between chemistry and chemical engineering arebecoming increasingly porous, a positive trend that is greatly facilitatedby information technology.this report contains numerous examples of ways in which databases, computing, and communications play a critical role in catalyzing the integrationof chemistry and chemical engineering. the striking pace of this integrationhas changed the way chemical scientists and engineers do their work, compared to the time of publication of the previous national research councilreports on chemistry (1985) and chemical engineering (1988).finding: advances in the chemical sciences are enablers for the development of information technology.breakthroughs from molecular assembly to interface morphology to processcontrol are at the heart of nextgeneration it hardware capabilities. theseadvances impact computer speed, data storage, network bandwidth, and distributed sensors, among many others. in turn, effective deployment of itadvances within the chemical enterprise will speed discovery of yet morepowerful it engines.some of the major challenges to the chemical community can be advancedby it. opportunities are plentiful, and challenges and needs remain for furtherprogress. chapter 3 examines the current status of the research arena in the contexts of computational methodology, training, databases, problem solving, optimization, communications capabilities, and supplychain modeling. the challenges are then described that will certainly arise, as computing capabilities andinformation technology continue to grow, and the modeling tasks for the chemical community become more complex.finding: there are major societal and civic problems that challenge thechemical community. these problems should be addressed by chemistryand chemical engineering, aided by it advances.these societal issues include providing stewardship of the land, contributinginformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.4information and communicationto the betterment of human health and physical welfare, ensuring an informedcitizenry through education, facilitating more thoughtful and informed decision making, and protecting and securing the society.finding: the nationõs technological and economic progress can be advanced by addressing critical needs and opportunities within the chemical sciences through use of new and improved information technologytools.bringing the power of it advances to bear will greatly enhance both targeteddesign through multidisciplinary team efforts and decentralized curiositydriven research of individual investigators. both approaches are important,but they will depend upon it resources in different ways.finding: to sustain advances in chemical science and technology, newapproaches and it infrastructures are needed for the development, support, and management of computer codes and databases.significant breakthroughs are needed to provide new means to deal withcomplex systems on a rational basis, to integrate simulations with theory andexperiment, and to construct multiscale simulations of entire systems.pervasive computing and data management will complement and aid the roleof intuition in allowing science and engineering to take full advantage of humanresources. chapter 4 addresses the ways in which information technology andcomputation can provide new capabilities for cooperation and collaboration acrossdisciplines. training chemical scientists and engineers to take strategic advantage of advances in information technology will be of particular importance.overarching themes such as targeted design, curiositydriven research, flow ofinformation, multiscale simulation, and collaborative environments will becomeincreasingly significant as information technology becomes yet more capable andthe chemical community undertakes even more intricate problems.finding: computation and information technology provide a key enabling force for lowering barriers among the disciplines that comprisethe chemical enterprise and closely related fields.identification of mutual interests among disciplines and removal of the barriers to successful communication among constituencies are essential for increasing the overall effectiveness of the system. the processes of identification and removal are still in their infancy.finding: addressing critical challenges at the interfaces with other scientific and engineering disciplines will enable chemistry and chemicalengineering to contribute even more effectively to the nationõs technological and economic progress.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.executive summary5the most important challenge involves people. advances in it that facilitateselforganization of problemsolving groups with common interests acrossdisciplinary boundaries will impact strongly both understandingbased andapplicationdriven projects. the essential resource driving the interface ofit and the chemical sciences is human ingenuity.finding: the capability to explore in the virtual world will enable society to become better educated and informed about the chemical sciences.conveying the intellectual depth, centrality, societal benefits, and creativechallenges of molecular systems will be greatly facilitated by the use ofmodeling, visualization, data manipulation, and realtime responses. all ofthese new capabilities will provide unparalleled environments for learning,understanding, and creating new knowledge.finding: the growing dependence of the chemical enterprise on use ofinformation technology requires that chemical professionals have extensive education and training in modern it methods.this training should include data structures, software design, and graphics.because data and its use comprise such important aspects of chemistry andchemical engineering, and because appropriate use of it resources can empower unprecedented advances in the chemical arena, it is crucial that theappropriate training, at all levels, be a part of chemical education.looking to the future, we need to build upon these advances to enable computational discovery and computational design to become standard components ofbroad education and training goals in our society. in this way, the human resourceswill be available to create, as well as to realize and embrace, the capabilities, challenges, and opportunities provided by the chemical sciences through advanced information technology. chapter 5 deals with capabilities and goalsñstructural challenges and opportunities in the areas of research, teaching, codes, software, dataand bandwidth. major issues of infrastructure must be addressed if the nation is tomaintain and improve the remarkable economic productivity, scientific advances,and societal importance of the chemical sciences and technology.finding: federal research support for individual investigators and forcuriositydriven research is crucial for advances in basic theory, formalisms, methods, applications, and understanding.history shows that the investment in longterm, highrisk research in thechemical sciences must be maintained to ensure continued r&d progressthat provides the nationõs technological and economic wellbeing. largescale, largegroup efforts are complementary to individual investigatorprojectsñboth are crucial, and both are critically dependent on nextgeneration it infrastructure.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.6information and communicationfinding: a strong infrastructure at the intersection with informationtechnology will be critical for the success of the nationõs research investment in chemical science and technology.the infrastructure includes hardware, computing facilities, research support,communications links, and educational structures. infrastructure enhancements will provide substantial advantages in the pursuit of teaching, research,and development. chemists and chemical engineers will need to be ready totake full advantage of capabilities that are increasing exponentially.recommendationsthese findings show that the intersection of chemistry and chemical engineering with computing and information technology represents a frontier ripewith opportunity. major technical progress can be expected only if additionalresources are provided for research, education, and infrastructure. while this report identifies many needs and opportunities, the path forward is not yet fullydefined and will require additional analysis.recommendation: federal agencies, in cooperation with the chemicalsciences and information technology communities will need to carry outa comprehensive assessment of the chemical sciencesðinformation technology infrastructure portfolio.the information provided by such an assessment will provide federal funding agencies with a sound basis for planning their future investments in bothdisciplinary and crossdisciplinary research.recommendation. in order to take full advantage of the emerging gridbased it infrastructure, federal agenciesñin cooperation with thechemical sciences and information technology communitiesñshouldconsider establishing several collaborative dataðmodeling environments.by integrating software, interpretation, data, visualization, networking, andcommodity computing, and using web services to ensure universal access,these collaborative environments could impact tremendously the value of itfor the chemical community. they are ideal structures for distributed learning, research, insight, and development on major issues confronting both thechemical community and the larger society.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.71introduction: the human resourceinformation technology is transforming american and global society. theavailability of deep information resources provides a fundamentally new capability for understanding, storing, developing, and integrating information. information and communication tools in chemical science and technology have alreadyprovided an unprecedented capability for modeling molecular structures and processes, a capability that has contributed to fundamental new understanding aswell as new technological products based on the physical and life sciences.chemistry and chemical engineering are being transformed by the availability of information technology, modeling capabilities, and computational power.the chemical sciences in the twentyfirst century will include information, computation, and communications capabilities as both assets and challenges. the assets are clear in terms of what we already can accomplish: we can model manysystems with accuracy comparable to or exceeding that of experiment; we canrapidly and effectively extend theoretical conceptual development toward modeling capabilities; and we can store, retrieve, integrate, and display informationeffectively and helpfully.the challenges come at several levels. major exploration will be needed todevelop new and better tools, educational techniques, computational and modeling strategies, and integrative approaches. the exploration will create demands intwo areas: chemical information technology and the people who will do the work.the two traditional components of the scientific method, observation andhypothesis, have led to a formidable array of experimental and theoretical tools.since the end of world war ii, computation and modeling has advanced to become the strong third component, one that can integrate experiment and theorywith application. advances in information technology (it) in communications,information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.8information and communicationmodeling, and computing have substantially increased the capabilities of chemistry and chemical engineering. effectively harnessing new and future it advanceswill present a great challenge for chemical science, but success will provide bothcontributions to fundamental knowledge and benefits to our society in health,welfare, and security.looking to the future, we need to build upon these advances to enable computational discovery and computational design to become standard componentsof broad education and training goals in our society. in this way, the human resources will be available to create, as well as to realize and embrace, the capabilities, challenges, and opportunities provided by the chemical sciences throughadvanced information technology.chemists and chemical engineers, and the processes and goods that they produce, have probably the largest impact of any science/engineering discipline onour economy and on our environment. the chemical industry employs over onemillion workers in the united states and indirectly generates an additional fivemillion jobs; this business of chemistry contributes nearly $20 billion annually tofederal, state, and local tax revenues.1 investment in chemical r&d is estimatedto provide an annual return of 17% after taxes.2 chemical manufacturing (including chemicals, allied products, petroleum, coal products, rubber, and plastics)produces 1.9% of the u.s. gross domestic product (gdp) and approximately 17%of the for the manufacturing sector.3 the chemical industry supplies nearly $1out of every $10 of u.s. exports,4 and in 2002 its total exports of $82 billionranked second among exporting sectors.5it is therefore especially important that we, as a society, take steps to assurethat the chemical enterprise maintain its cuttingedge capability in teaching, research, development, and production. it is also important that the chemical enterprise provide leadership in economic growth and environmental quality. all of thesegoals require increased capability for chemists and chemical engineers to utilize,efficiently and creatively, the capabilities offered by information technology.advances in the chemical sciences enabled major achievements in medicine,life science, earth science, physics, engineering, and environmental science. theseadvances in the productivity, quality of life, security, and economic vitality of oursociety flowed directly from the efforts of people who work in those fields. how1guide to the business of chemistry, american chemistry council, arlington, va, 2002; http://www.accnewsmedia.com/docs/300/292.pdf.2measuring up: research & development counts in the chemical industry, council for chemicalresearch, washington, d.c., 2000; http://www.ccrhq.org/news/studyindex.html.3u.s. department of commerce, bureau of economic analysis, industry accounts data , grossdomestic product by industry: http://www.bea.doc.gov/bea/dn2/gposhr.htm.4u.s. department of commerce, technology administration: the chemical industry: http://www.technology.gov/reports.htm.5chemical & engineering news 2003, 81(27), 64.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.introduction: the human resource9will we as a community utilize the remarkable capabilities provided by it toteach, train, inspire, challenge, and reward not only the professionals within ourdiscipline but also those in allied fields whose work depends on understandingand using concepts and ideas from the chemical sciences?this report is written by the committee that organized a workshop held inwashington, d.c., in october 2002, to address ways in which chemists andchemical engineers could focus their r&d efforts on the solution of problemsrelated to computing and information technology. a series of speakers (appendixe) presented lectures (appendix d) on topics that covered different aspects of theproblem, and they addressed issues in all areas of chemical science and engineering. considerable input for the report was also provided by a series of breakoutsessions (appendix g) in which all workshop attendees participated (appendixf). these breakout sessions explored the ways in which chemists and chemicalengineers already have contributed to solving computationally related problems,the technical challenges that they can help to overcome in the future, and thebarriers that will have to be overcome for them to do so. the questions addressedin the four breakout sessions were:¥discovery: what major discoveries or advances related to informationand communications have been made in the chemical sciences during the lastseveral decades?¥interfaces: what are the major computingrelated discoveries and challenges at the interfaces between chemistryðchemical engineering and other disciplines, including biology, environmental science, information science, materialsscience, and physics?¥challenges: what are the information and communications grand challenges in the chemical sciences and engineering?¥infrastructure: what are the issues at the intersection of computing andcommunications with the chemical sciences for which there are structural challenges and opportunitiesñin teaching, research, equipment, codes and software,facilities, and personnel?the world of computing has grown at an extraordinary pace in the last halfcentury.6 during the early stages, the impact of this growth was restricted to asmall segment of the population, even within the technical community. but as theexpanded power of computer technology made it possible to undertake significant new areas of research, the technical community began to embrace this newtechnology more broadly. perhaps the seminal event in changing the culture was6for example, òmooreõs law,ó originally stated as òthe complexity for minimum component costshas increased at a rate of roughly a factor of two per year,ó moore, g. e., electronics 1965, 38 (8)114117. this has been restated as òmooreõs law, the doubling of transistors every couple of yearsó;(http://www.intel.com/research/silicon/mooreslaw.htm).information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.10information and communicationthe introduction of personal computers in the 1980s. by 1994 the number of u.s.households with personal computers had reached 24%,7 and this increased to 54%by 1999.8 for japan, the analogous numbers are 12% in 1986, 49% in 1999, and88% in 2002.9the key to the future is the human resource.10 computers are extraordinarilypowerful tools, but they do only what people tell them to do. there is a remarkable synergy between humans and computers, because high levels of human creativity are needed to push the capabilities of computers in solving research problems. at the same time, computers have enabled an astonishing increase in humancreativity, allowing us to undertake problems that previously were far too complex or too timeconsuming to even consider. our technical future is stronglylinked to our ability to take maximum advantage of the computer as a way ofdoing routine tasks more rapidly, beginning to undertake tasks that we could notdo before, and facilitating the creativity of the human mind in ways that we havenot yet imagined.like so many other aspects of the information technology universe, the useof computational resources for addressing chemical systems has been growingrapidly. advances in experiment and theory, the other two principal research anddevelopment modes in chemical science, have also developed rapidly. the advances in the chemical sciences enabled by exponential growth of computationalcapability, data storage, and communication bandwidth are by far the most striking and profound change in the past two decades. this remarkable growth hasbeen stressed elsewhere,11,12and is clearly stated by jack dongarra, one of theworldõs foremost experts in scientific computing, who has argued thatéthe rising tide resulting from advances in information technology shows norespect for established order. those who are unwilling to adapt in response tothis profound movement not only lose access to the opportunities that the infor7national telecommunications and information administration, falling through the net, towarddigital inclusion, 2000, http://www.ntia.doc.gov/ntiahome/digitaldivide/.8arbitron, pathfinder study, 1999, new york, http://internet.arbitron.com/main1.htm.9http://www.jinjapan.org/stat/stats/10liv43.html.10beyond productivity: information, technology, innovation, and creativity, mitchell, w. j.;inouye, a. s.; blumenthal, m. s., eds. national research council, the national academies press,washington, dc, 2003.11revolutionizing science and engineering through cyberinfrastructure, report of the nationalscience foundation blueribbon advisory panel on cyberinfrastructure, alliance for communitytechnology, ann arbor, mi, 2003 (the atkins committee report); http://www.communitytechnology.org/nsfcireport/.12science and engineering infrastructure for the 21st century: the role of the national sciencefoundation, national science board, arlington, va, 2003; this report lists as one of its key recommendations to òdevelop and deploy an advanced cyberinfrastructure to enable new s&e in the 21stcentury.óinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.introduction: the human resource1113see t. dunning, appendix d.mation technology revolution is creating, they risk being rendered obsolete bysmarter, more agile or more daring competitors.13at the current rate of change, communications and computing capabilitieswill increase tenfold every five years. such rapid increase of capability meansthat some problems that are unsolvable today will be straightforward in five years.the societal implications are powerful. to deal with these assets, opportunities,and challenges will require both an awareness of the promise and a commitmentof financial and human resources to take advantage of the truly revolutionaryadvances that information technology offers to the world of chemical science.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.122accomplishmentsthe chemical sciences begin the twentyfirst century with an enviable recordof accomplishment in education, research, technology development, and societalimpact. the record also extends to information technology (it), where it works inboth directionsñthe chemical sciences are impacted by, as well as exert impactupon, information technology.major themesthe chemical sciences and information technology form a mutually supportive partnership. this dates to the early years when it was still in the vacuumtubeage. the chemical sciences have provided construction modalities for computers,ranging from polyimide packaging to organic photoresists and chemical vapordeposition. today chemical sciences contribute to the partnership in three majorareas. the chemical sciences provide1.people who develop and extend information technology through expertisethat ranges from algorithm development to programming, from process and materials research through database and graphics development, and from displaytechnology research to mobile power sources;2.theoretical models and methods on which software programs can be based,along with a huge amount of significant, wideranging, and unique data to construct crucial databases; and3.processes and materials for construction of information networks and computersña microelectronic fabrication facility involves many processing operations that are familiar to chemical engineers.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.accomplishments13in turn, information technology has provided for the chemical sciences aseries of major enablers of processes and schemes for enriching both what chemists and chemical engineers can do and the ease with which they can do it.¥languages and representations: in addition to the traditional upper levelprogramming languages, new approachesñincluding scripting languages,markup languages, and structural representationsñhave greatly facilitated thesuccessful use of it within the chemical sciences. these languages make it possible to use distributed computers very efficiently. this can be useful in exploringmultidimensional design, structure, and dynamics problems.¥informatics and databases: starting with the remarkably complete andpowerful informational database represented by chemical abstracts,1 we havewitnessed in the last 20 years a striking development of databases, database relationships, datamining tools and tutorial structures. these enable the kind of research to be done in the chemical sciences that could never have been done before. from something as straightforward as the protein data bank (pdb)2 to themost subtle datamining algorithms, information technology has permitted allscientists to use the huge resource of chemical data in a highly interactive,reaonably effective fashion. other contributions include the developments ofstring representations for chemical structures.¥computing capability, integration, and access: these have enabled cuttingedge work in the chemical sciences that resulted in a number of nobel prizesin chemistry, national medals of technology, and national medals of science.more broadly, they have enabled research modalities within the chemical sciences that could not have been accomplished before the advent of computers.much of this report is devoted to computational modeling and numerical theoryñareas of research and development that didnõt exist before 1950. striking examples such as quantum chemical calculations of molecular electronic structure,monte carlo calculations of equations of state for gases and liquids, or moleculardynamics simulations of the structures of hightemperature and highpressurephases represent major extensions of the traditional concepts of chemistry. integrated models for plant design and control, monte carlo models for mixtures,polymer structure and dynamics, and quantum and classical dynamics models ofreaction and diffusion systems provide chemical engineers with an ability to predict the properties of complex systems that, once again, was simply unobtainablebefore 1950.¥bandwidth and communication capabilities: these have enabled newlevels of collaboration for chemical scientists and engineers, who now have in1a product of the american chemical society, http://www.cas.org/.2the pdb is a repository for threedimensional biological macromolecular structure data; http://www.rcsb.org/pdb/; berman, h. m.; westbrook, feng, z.; gilliland, g.; bhat, t. n.; weissig, h.;shindyalov, i. n.; bourne, p. e. nucleic acids research 2000, 28, 235242.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.14information and communicationstantaneous access to information, to measurements, and to the control of entiresystems. the impact of increased bandwidth typically has received less attentionthan that of advances in informatics and computing. however, it is key to muchof the future societal and educational development of the chemical sciencesñtothe processes that will allow chemists and chemical engineers to interact andcollaborate with one another, with other scientists, with industrial and medicalpractitioners, and with users that require chemical information, methods, andmodels. similarly, increases in network and memory bus speed have made computation a more powerful tool for modeling in chemical science.the emphasis on mooreõs law as a measure of steady increase in computingpower is well known. equally remarkable growth has also occurred in algorithmdevelopment. figure 21 shows performance increases over the past three decades derived from computation methods as well as from supercomputer hardware, noting approximate dates when improvements were introduced. it may berecognized that growth in algorithm speed and reliability has had a significantimpact on the emergence of software tools for the development and integration ofcomplex software systems and the visualization of results.some specific enabling accomplishmentsthe chemical sciences have heavily influenced the field of information andcommunications during the past five decades. examples are so common that wenow take them for granted or even dismiss them as simply being a part of modernsociety. but in retrospect it is clear that scientists, engineers and technologistshave used training in the chemical sciences to contribute significantly to information technology and its use. chemical scientists have built methods, models, anddatabases to take advantage of it capabilities, and they have explored and developed materials from knowledge based at the molecular level. some examplesfollow.1.people:professionals with backgrounds in chemical science have provided a major intellectual resource in the industrial, government, and academic sectors ofsociety. such chemical problems as process design, optimization ofphotorefractive polymers, or organization and searching of massive databases areprecisely the sort of complicated and demanding environment to provide excellent training for work in information technology policy and development.2.methods, models, and databases¥modeling: a series of model chemistries has been developed and partially completed. this has facilitated the use of computational techniquesñin a relatively straightforward wayñfor specific problems such as molecularinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.accomplishments15structure. the availability of quantum chemical codes has made it possible tosolve, with high accuracy and precision, many problems associated with theground state for organic molecules. this has been used extensively to solveproblems spanning the range from molecules in outer space to drug design.¥multiscale computational integration: the beginnings of multiscaleunderstanding and methodologies are being developed. these include hierarchical approaches such as using molecular dynamics to compute diffusioncoefficients or materials moduli that in turn can allow extended scale descriptions of real materials, or using quantum chemistry to define electricaland optical susceptibility that can then be used in full materials modeling.such efforts mark the beginnings of multiscale computational integration.¥integrated data management: computational bandwidth and extantdatabases are being utilized to develop userfriendly and integrated data management and interpretation tools. such utilities as web of science, chemicalsparse gaussian eliminationfigure 21speedup resulting from software and hardware developments. updatedfrom charts in grand challenges: high performance computing and communications,office of science and technology policy committee on physical, mathematical and engineering sciences, 1992; siam working group on cse education, siam rev. 2001, 43:1,163177; see also l. petzold, appendix d.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.16information and communicationabstracts online, and the structural libraries developed at cambridge,3brookhaven,4 and kyoto5 are examples of using information technologycombined with chemical data to facilitate research and understanding at alllevels.3.processes and materials¥optical fibers: the development of optical fibersñprepared fromsilica glass purified by removal of bubbles and moisture and capable of transmitting light over great distancesñhas made possible the communicationbackbone for the early twentyfirst century.¥photoresist technology: the development of photoresist technology,based on polymer chemistry, has been an integral part of chip, packaging,and circuit board manufacturing for the past four decades.¥copper electrochemical technology: the manufacture of complexmicrostructures for onchip interconnects requires multiple layers of metallization. copper electrochemical technology was introduced by ibm in 1999and is now used widely as a basic chip fabrication process. the process depends critically on the action of solution additives that influence growth patterns during electrodeposition.¥magnetic films: magnetic data storage is a $60 billion industry that isbased on the use of thin film heads fabricated by electrochemical methods.since their introduction in 1979, steady improvements in storage technologyhave decreased the cost of storage from $200/mbyte to about $0.001/mbytetoday.6examples such as the preceding demonstrate that the remarkable it advanceswe have seenñin speed, data storage, and communication bandwidthñhave beenfacilitated in no small way by contributions from the chemical sciences. a recentarticle by theis and horn, of ibm, discusses basic research in the informationtechnology industry and describes the ongoingñindeed growingñimportance ofnanoscale chemistry in the it community.73the cambridge structural database (csd) is a repository for crystal structure information fororganic and metalorganic compounds analyzed by xray or neutron diffraction techniques, http://www.ccdc.cam.ac.uk/prods/csd/csd.html.4the pdb; see footnote 3.5the kyoto encyclopedia of genes and genomes (kegg) is a bioinformatics resource for genomeinformation, http://www.genome.ad.jp/kegg/.6romankiw, l.t. j. mag. soc. japan 2000, 24:1.7theis, t. n.; horn, p. m. physics today 2003, 56(7); http://www.physicstoday.org/vol56/iss7/p44.html.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.accomplishments17finding: advances in the chemical sciences are enablers for the development of information technology.breakthroughs from molecular assembly to interface morphology to processcontrol are at the heart of nextgeneration it hardware capabilities. theseadvances impact computer speed, data storage, network bandwidth, and distributed sensors, among many others. in turn, effective deployment of itadvances within the chemical enterprise will speed discovery of yet morepowerful it engines.the flow of information and influence goes in both directions; just as chemistry and chemical engineering have had a significant influence on the development of computing, information technology has helped to produce major advancesin chemical science and engineering. examples include the following:¥ computeraided drug design: this has become a significant contributorin the discovery and development of new pharmaceuticals. some examples ofmolecules created with the direct input of computational chemistry include theantibacterial agent norfloxacin, the glaucoma drug dorzolamide hydrochloridemarketed as trusopt, indinavir sulfate marked as crixivan, the protease inhibitorfor aids, marketed as norvir, the herbicides metamitron and bromobutide, andthe agrochemical fungicide myclobutanil.¥simulation and computational methods for design and operation: thesetechniques are used for something as simple as a new colorant or flavor species tosomething as complicated as the integrated design and realtime optimization of achemical plant or a suicide enzyme inhibitor. predictive capabilities are now goodenough to obtain phase diagrams for real gases with accuracies exceeding mostexperiments. simulations are now beginning to address more complex systems,including polymers, biomolecules, and selfassembling systems. molecular dynamics (md) simulation of billions of atoms is now possible, permitting bothunderstanding and prediction of such phenomena as phase and fracture behavior.¥integrated control and monitoring systems: this approach recently permitted the ibm corporation to dedicate a multibilliondollar semiconductor fabrication line in east fishkill, new york, that is controlled almost entirely bycomputers and employs remarkably few people to maintain production.¥chemoinformatics: a new area of chemical science has begun, one thatoften is called chemoinformatics (one negative aspect of information technologyis the ugliness of some of the new words that it has engendered).chemoinformatics is defined as the òapplication of computational techniques tothe discovery, management, interpretation and manipulation of chemical information and data.ó8 it is closely related to the growth of such techniques as high8naturejobs 2002, 419, 47.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.18information and communicationthroughput screening and combinational chemistry, and is a strong growth areaboth in employment and in its ability to use fundamental knowledge to yieldpractical advances.¥molecular electronic structure calculations: this capability has becomeavailable to the community in a number of integrated computer codes. thesemake it possible for chemists, engineers, physicists, astronomers, geologists, andeducators to model the structure and properties of a given molecule (within limits) to high accuracy. this ability has led to a new way of doing science, based noton artificial models but on accurate quantum calculations of actual chemical species. in some cases, accurate calculations can replace experiments that are expensive or dangerous or involve animals.in a curious way, some of the important accomplishments in chemical science with respect to information technology involve realization of strengths anddefinition in other areasñfields that can (or perhaps must) in the future takeadvantage of exponential advances in it implied by mooreõs law.9 these accomplishments address fundamental issues or enabling methods to solve majorproblems, often outside the chemical sciences, as illustrated by the followingexamples:¥the chemical industry does a far better job than either universities orgovernment laboratories of integrating capabilities and training across disciplinesand backgrounds. the ability to integrate expertise is absolutely crucial to thesuccess of modeling efforts in the chemical, pharmaceutical, and energy sectors.¥areas that have data streams rich enough to require truly massive datamanagement capability have undergone major development. these areas includecombinatorial chemistry, the use of microfluidics to study catalysts, and the rapidly expanding capability to label enormous numbers of individual objects, suchas cans of soup or shipping parcelsñusing, for example, inexpensive polymerbased transistor identification tags. remarkable advances in security, economy,and environmental assurance can follow from computational monitoring, modeling, and communicating in real time the flow of these material objects withinsociety.¥supplychain optimization and process optimization are showing initialsuccess. full utilization of such capabilities can have significant positive impacton the competitiveness and capability of industrial production, homeland defenseand security, and the quality of the environment.¥integrated modeling efforts are proving highly valuable to industry in suchareas as drug design and properties control. richard m. gross, vice president and9moore, g.e., electronics 1965, 38 (8) 114117; http://www.intel.com/research/silicon/mooreslaw.htm.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.accomplishments19director of research and development for the dow chemical company, describedat another workshop10 the critically important role of computational chemistry indesigning the lowk (low dielectric constant) resin, silk. the dielectric, mechanical, and thermal properties of a large group of polymers were predictedcomputationally in order to identify a much narrower group of targets on whichsynthetic efforts were focused. close integration of computation and chemistryduring very early stages of discovery and process invention were key for gettingsample material into the marketplace within six months of the original goaheadfor pursuing the idea.in many areas, simulation capabilities now make it possible to go beyondsimplistic models to truly integrated, highaccuracy simulations that can providean accurate guide to the properties of actual structures. simulations combine thescientistõs search for truth with the engineerõs desire for targeted design, the deterministic solution of welldefined sets of equations with the use and understanding of stochastic and probabilistic arguments, and the targeted strengths ofthe research institute with the multilevel demands of r&d business. sophisticated simulation tools that combine all of these aspects are beginning to be developed, and they constitute a major advance and opportunity for chemical science.perhaps the most significant accomplishment of all is the fundamental reshaping of the teaching, learning, and research and development activities thatare likely to be carried out in the chemical sciences by taking strategic advantageof new information technology tools. within chemical engineering and chemistry, we are approaching an era of òpervasive computing.ó in this picture, computation and information will be universal in the classroom, the laboratory, andmanufacturing areas. already, organic chemists use databases and data mining tosuggest molecular structures, quantum chemistry to predict their stability, andstatistical mechanics methods (monte carlo, molecular dynamics) to calculatetheir properties and interactions with other species. what once was the esotericdomain of the theoretical chemist now encompasses scientists and engineers fromhigh schools to seasoned professionals. this integration of modeling, simulation,and data across many sectors of the society is just beginning, but it is already amajor strength and accomplishment.finding: boundaries between chemistry and chemical engineering arebecoming increasingly porous, a positive trend that is greatly facilitatedby information technology.this report contains numerous examples of ways in which databases, computing, and communications play a critical role in catalyzing the integration10reducing the time from basic research to innovation in the chemical sciences, a workshopreport to the chemical sciences roundtable, national research council, the national academiespress, washington, d.c., 2003.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.20information and communicationof chemistry and chemical engineering. the striking pace of this integrationhas changed the way chemical scientists and engineers do their work, compared to the time of publication of the previous national research councilreports on chemistry11 (1985) and chemical engineering12 (1988).among the many examples of this trend is the study on challenges for thechemical sciences in the 21st century. the integration of chemistry and chemical engineering is a common thread that runs throughout the report beyond themolecular frontier: challenges for chemistry and chemical engineering,13 aswell as the six accompanying reports on societal needs (of which this report isone).14,15,16,17,18 these describe both the progress and the future needs for increasing cooperation that will link chemical science and chemical technology.many of the barriers and interfaces are discussed in this report, but furtheranalysis and action will be needed on many frontsñby individual scientists andengineers and by administrators and decision makersñin universities and individual departments, in companies and federal laboratories, and in those agenciesthat provide financial support for the nationõs research investment in the chemicalsciences.11opportunities in chemistry, national research council, national academy press, washington,d.c., 1985.12frontiers in chemical engineering: research needs and opportunities, national research council, national academy press, washington, d.c., 1988.13beyond the molecular frontier: challenges for chemistry and chemical engineering, nationalresearch council, the national academies press, washington, d.c., 2003.14challenges for the chemical sciences in the 21st century: national security & homeland defense, national research council, the national academies press, washington, d.c., 2002.15challenges for the chemical sciences in the 21st century: materials science and technology,national research council, the national academies press, washington, d.c., 2003.16challenges for the chemical sciences in the 21st century: energy and transportation, nationalresearch council, the national academies press, washington, d.c., 2003 (in preparation).17challenges for the chemical sciences in the 21st century: the environment, national researchcouncil, the national academies press, washington, d.c., 2003 ..18challenges for the chemical sciences in the 21st century: health and medicine, national research council, the national academies press, washington, d.c., 2003 (in preparation).information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.213opportunities, challenges, and needsinformation technology (it) is a major enabler for the chemical sciences. ithas provided the chemical scientist with powerful computers, extensive databasestructures, and widebandwidth communication. it permits imagination, envisioning, information integration and probing, design at all levels, and communicationand education modalities of an entirely new kind.the richness and capability of effective data management and data sharingalready permit, and in the future will facilitate even more successfully, entirelynew kinds of understanding. combining modeling with integrated data may permit the community to predict with higher reliability issues of risk, environmentalimpact, and the projected behavior of molecules throughout their entire life cyclein the environment. major progress on issues of societal policyñranging fromenergy to manufacturability, from economic viability to environmental impact,and from sustainable development to responsible care to optimal use of matterand materialsñmight all follow from the integrated capabilities for data handlingand system modeling provided by advances in information technology.this field is beginning to see the development of cooperative environments,where learning, research, development, and design are carried out utilizing bothmodeling and data access: this cooperative environment for understanding maybe the most significant next contribution of it within the chemical sciences.the questions posed by chemists and chemical engineers are often too complex to be solved quantitatively. nevertheless, the underlying physical laws provide a framework for thinking that, together with empirical measurement, hasallowed researchers to develop an intuition about complex behavior. some ofthese complicated problems will yield to quantitative solution as computationalpower continues to increase. most will not, however, at least in the foreseeableinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.22information and communicationfuture. these will continue to be addressed by intuition and experiment. thedatahandling and visualization capabilities of modern computing will increasingly become an essential aid in developing intuition, simple models, and underlying physical pictures. thus, information technology has changed and will continue to change the way we think about chemical problems, thereby opening upnew vistas well beyond the problems that can be addressed directly by largescalecomputation.current statuscomputational modeling currently excels in the highly accurate computationof small structures and some gasphase properties, where experiment can be reproduced or predicted when adequate computational power is available. techniques for the computation of pure solvents or dilute solutions, macromolecules,and solidstate systems are also advanced, but accuracy and size are serious limitations. many efforts in chemical science have merged with biology or with materials science; such modeling requires accuracy for very large systems, and theability to deal with complex, macromolecular, supramolecular, and multicomponent heterogeneous systems. the nanosystems revolution is very small from theperspective of size, but huge from the viewpoints of chemical engineering andchemistry because it will allow wholly different design and manufacture at themolecular level. this will comprise a new subdiscipline, molecular engineering.computational, communications, and data storage capabilities are increasingexponentially with time. along with massive increases in computational power,better computational algorithms for large and multicomponent systems are neededif computations are to have a major role in design. some of the major targetopportunities for exploration in the near term include¥computational methods: the implementation of theoretical models withinsoftware packages has now become excellent for certain focused problems suchas molecular electronic structure or simple monte carlo simulations. very largechallenges remain for extending these methods to multiscale modeling in spaceand time.¥education and training: the community has not made as much progressas might be desired in the training both of chemical professionals and professionals in other disciplines. training techniques tend to focus on particular packages,lack integration, and be concentrated too narrowly within particular subdisciplines. education in chemistry and chemical engineering has not yet utilized itadvances in anything approaching a reasonable way.¥databases: the database resource in the chemical sciences is rich butfragmented. for example, the structural databases for small molecules and forbiological systems are in an early stage of development and integration. often thedata have not been verified, and federated databases are not in widespread use.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.opportunities, challenges, and needs23¥ease of use: the field has a very bad record of resuscitating òdeadcodeóñcode that has been modified so that some branches will never be executed. too many of the software packages in current use, both commercial andfreeware, are very poor from the viewpoints of reliability, user friendliness, portability, and availability. maintenance and improvement of codes is not wellhandled by the field in general.¥problem solving: particular areas such as drug design, molecular structure prediction, prediction of materials properties, energetics of particular reactions, and reaction modeling have shown major successes. these successes willprovide the springboard for designing the collaborative modelingdata environments that constitute a major theme of this report.¥optimization: largescale nonlinear optimization techniques for continuous and discrete variables are just beginning to make their way into every part ofthe chemical sciences, from the molecular level to the enterprise level. at themolecular level these techniques are being used to design molecular structures,while at the enterprise level they are being used to optimize the flow of materialsin the supply chain of the chemical industry. issues such as sensitivity analysis,parameter estimation, model selection, and generalized optimization algorithmsare particularly importantñbut are not yet in common use.¥supplychain modeling: in this crucial area, security, privacy, and regulatory issues must be addressed. supplychain structures are characteristic oflargescale problems of importance both within chemical science and engineering and across the larger society. just as the impact of such work will be major, sowill the importance of assuring accuracy, security, privacy, and regulatory compliance. often the data used in such simulations are proprietary and sensitive;thus, access to both the data and the models themselves must be controlled so thatthe effectiveness and societal acceptance of such modeling are not jeopardized.challengesin the future, modeling and data management will become unified. research,development, education, training, and understanding will be done in a comprehensive multiscale problemposing and problemsolving environment. targeteddesign, as well as our understanding of simple systems, can profit by investigation within such a holistic research environment.although the exponential increases in capability and utilization cannot continue forever, the advances already attained mean that the capabilities available tothe chemical sciences are not limited to commodity computing and the constraintsof more, faster, and cheaper cycles. major challenges exist in maximizing theability of chemical scientists to employ the new methods and understandings ofcomputer science:¥develop interdisciplinary cooperation with applied mathematicians andinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.24information and communicationcomputer scientists to approach chemical problems creatively. particular targetswould include use of informatics, data mining, advanced visualization and graphics, database customization and utility, nonlinear methods, and software engineering for optimization and portability of chemical codes.¥develop computational chemical sciences, as a focused subdiscipline ofchemistry, chemical engineering, computer sciences, and applied mathematics.crossdisciplinary training grants, planning workshops, symposia, programminggroups, and other forms of interaction sponsored by professional societies andfunding agencies should materially help in bringing computer science and applied math expertise to solving advanced problems of chemical modeling anddata utilization.¥better integrate the two disciplines of chemistry and chemical engineering. these face nearly identical issues, concerns, advantages, and itbased challenges. integrated software, communications, database, and modeling capabilities can be used as a pathway for closer linking of these disciplines.¥generate graphical user interfaces. current advances in computer science permit adaptation of existing complex codes into a larger problemsolvingenvironment. tools for semiautomatic generation of new graphical user interfaces could facilitate calling or retrieving data from these extant programs. eventually, one can envision an objectoriented approach that would include interfaces to a suite of components for integrating chemical science modeling taskssuch as electronic structure calculation, smart monte carlo simulation, moleculardynamics optimization, continuum mechanics flow analysis, electrochemical process modeling, and visualization. the disciplinary expertise in chemistry andchemical engineering is a key element in making the interfaces broadly usefulwithin the discipline.¥develop more general methods for manipulating large amounts of data.some of the data will be discrete, some stochastic, some bayesian, and someboolean (and some will be unreliable)ñbut methods for manipulating the datawill be needed. this integrative capability is one of the great markers of humanintelligence; computational methods for doing such integration are becomingmore widespread and could transform the way that we treat the wonderful bodyof data that characterizes the chemical sciences.¥help the it community to help the chemical world. one issue is the needto modify the incentive and reward structure, so that optimization experts or datastructure engineers will want to attack chemical problems. another issue is therapid exporting of it offshore, while national security and economic considerations require that extensive it expertise and employment also remain in theunited states. a national award for it professionals working in the chemicalsciences would be useful as one component for addressing these issues. anotherpossibility would involve new postdoctoral awards for it experts to work withinthe chemical sciences.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.opportunities, challenges, and needs25the preceding challenges involve bringing tools from information technology, in a much more effective fashion, to the solution of problems in the chemicalsciences. there are also major problems within the chemical sciences themselves,problems that comprise specific challenges for research and technology development. the largestscale challenges deal with the ability of chemistry and chemical engineering to address major issues in society. we should focus on majorissues:¥provide stewardship of the land. we need to develop new methods ingreen chemistry for manufacturing in the twentyfirst century, and we need toaccelerate environmental remediation of sites around the world that have beenpolluted over the previous century. examples includeincorporating computational methods into sensors for realtime analysisand assimilating sensormeasurementinformation data in simple yet technically correct formats for use by public policy decision makers; andexpanding our environmental modeling effortsñespecially atmosphericand oceanic modelingñto account for the impact and fate of manufacturedchemicals; to assess how changes in air and water chemistry affect our healthand wellbeing; and to develop alternative, efficient, and clean fuel sourcesso that we need not rely on imported hydrocarbons as our major energysource.¥contribute to betterment of human health and physical welfare. this challenge extends from fundamental research in understanding how living systemsfunction as chemical entities to the discovery, design, and manufacture of newproductsñsuch as pharmaceuticals, nanostructured materials, drug delivery devices, and biocompatible materials with lifetimes exceeding patientsõ needs. thischallenge is especially relevant to computing and informationbased solutions,because we will need to develop smart devices that can detect and assess the earlyonset of disease states. moreover, we need to convey that information in a reliable manner to health care professionals. we will also need to develop largescalesimulations that describe the adsorption, distribution, metabolism, and excretionof asyetunsynthesized drugs. we need to do this to account for naturally occurring phenomena such as bacterial evolution that make extant drugs ineffective,and we must be able to do it in quick response to potential homeland securitydisasters that affect our health and lives.¥ensure an informed citizenry through education. our democratic societydepends on an informed citizenry, and a major challenge facing the chemicalsciences community involves education. this includes reaching out to our colleagues in other disciplines who are beginning to study how things work at themolecular level. a need exists to integrate seamlessly chemistry with biology andmaterials science and to strengthen the connections between chemistry and chemical engineering. the challenges described above concerning environmental,information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.26information and communicationhealth and welfare issues are also challenges to the community of educators. theeducational challenge includes public relations, where it will be necessary to address the negative connotations of the word òchemical.ó educational efforts thatuse such information technology tools as websites and streaming video couldhelp to deliver accurate information and to inform the level of discourse withinthe public domain.¥facilitate more thoughtful decision making. we need informed policy anddecision makers to guide our society in times of increasingly complex technological issues. examples include international policies on health, disease, environment, natural resources, and intellectual property, to name a few. issues thatarise include mining databases for relevant information, developing and evaluating model scenarios, and assessing uncertainty and risk. the goal òevery child ascientistó should be attainable, if by òscientistó we mean someone with an understanding of what science is.¥protect and secure the society. of all the scientific and engineering disciplines, the chemical sciences and technology have perhaps the strongest influence on the economy, the environment, and the functioning of society. the community of chemical engineers and scientists must retain responsibility both formaintaining the traditional intellectual and methodological advances that chemistry and chemical engineering have provided and for responsibly managing thefootprint of chemistry. issues of privacy and security are crucial here, as are scrupulous attention to responsible care and continued responsible development. using information technology to achieve these goals will greatly enhance our abilityto protect and secure both the principles of our discipline and the civilization inwhich that discipline is used and practiced.finding: there are major societal and civic problems that challenge thechemical community. these problems should be addressed by chemistryand chemical engineering, aided by it advances.these societal issues include providing stewardship of the land, contributingto the betterment of human health and physical welfare, ensuring an informedcitizenry through education, facilitating more thoughtful and informed decision making, and protecting and securing the society.the chemical sciences need to develop datadriven, natural teaching andinformationcapture methods, preferably including some that do not require equationbased algorithms (what might be called natural learning). the communityshould develop means for using assemblyknowledge methods to produceweighted, customized datasearch methodology (which would, effectively, correspond to designing a chemistryengineering google search engine). the community also should utilize such popular it advances as web searching and 3dimensional graphics developed for games. finally, a verification or standardizationscheme for data is needed.within the overall challenge themes just expressed, it is useful to focus oninformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.opportunities, challenges, and needs27several specific, targeted challenges that the chemistry and chemical engineeringcommunity will need to address:¥multiscale methodologies are crucial. the community needs to incorporate advanced theoretical ideas to generate nearly rigorous techniques for extending accurate simulations to deal with phenomena over broad ranges of time andspace. it also needs to attack such methodological problems as modelbased experimental design, virtual measurement, quantum dynamics, integration with continuum environments, dispersion energetics, excited states, and response properties. all of these are part of the multiscale modeling capability that will be crucialif the chemical community is to utilize advanced it capabilities in the most effective and productive fashion.¥optimization must go beyond studies of individual systems and deal withchemical processes. the community must develop effective design, planning,and control models for chemical processes. it will be necessary to address theintegration of these models across long time scales, as well as their accuracy andutility.¥the community must develop enterprisewide optimization models. usingadvanced planning and scheduling methods, the models must allow fast and flexible response of chemical manufacturing and distribution to fluctuations in market demands. the economic implications and societal importance of this effortcould be enormous.finding: the nationõs technological and economic progress can be advanced by addressing critical needs and opportunities within the chemicalsciences through use of new and improved information technology tools.bringing the power of it advances to bear will greatly enhance both targeteddesign through multidisciplinary team efforts and decentralized curiositydriven research of individual investigators. both approaches are important,but they will depend upon it resources in different ways. among the needsto be addressed are:sampling is a key bottleneck at present in obtaining accurateresults in molecular modeling simulations. obtaining convergencefor a complex condensedphase system is extremely challenging.this is the area in my opinion where prospects are most uncertainand where it is critical to support a lot of new ideas as opposed tojust improved engineering of existing approaches. some advanceswill come about from faster hardware, but algorithmic improvementshould contribute even more if sufficient effort is applied.richard friesner (appendix d)information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.28information and communication¥take strategic advantage of exponential growth of it resources, whichwill revolutionize education, research, and technology in chemical science andengineering.¥develop a rational basis for dealing with complex systems.¥embrace simulation, which is emerging as an equal partner with experiment and theory.¥utilize webbased collaborative problem solving: collaborative modelingdata environments).¥recognize the increasing importance of multiscale, multiphenomena computing.¥maintain support for the fundamental aspects of theory that will remainessential for progress.¥support an approach to algorithm development that is application driven.¥facilitate the development of technologies for building reliable and secure multiscale simulations. multiscale computational methodologies are crucialfor extending accurate and useful simulations to larger sizes and longer times.fundamental advances in the formalisms and methodologies, as well as in algorithmic software and visualization advances, will be needed to make effectivemultiscale modeling a reality.finding: to sustain advances in chemical science and technology, newapproaches and it infrastructures are needed for the development, support, and management of computer codes and databases.significant breakthroughs are needed to provide new means to deal withcomplex systems on a rational basis, to integrate simulations with theory andexperiment, and to construct multiscale simulations of entire systems.it will be necessary to¥develop methods for semiautomatic generation of user interfaces forcodes and design modules in the chemical sciencesñwith the eventual goal of asemiautomated objectoriented modeling megaprogram(s) containing modules forspecific capabilities;¥develop reliable error estimators for computational results;¥develop enhanced methodology for data mining, data management, anddatarich environments, because databased understanding and insights are keyenablers of technical progress; and¥develop improved methods for database management, including assurance of data quality.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.294interfaces: cooperation and collaborationacross disciplinesthe research and technology enterprise of chemistry and chemical engineering involves a dazzling multitude of intellectual opportunities and technologicalapplications. the community includes academia, industry, and government andindependent laboratories, and it embraces the disciplines of chemistry, chemicalengineering, and many others that interact in complex ways. working at the interfaces between disciplinary domains offers reinforcing synergies and beneficialblurring of boundaries, but may also be accompanied by barriers that hinder theeffectiveness of information flow. information technology (it) assets are rapidlyaccepted in the chemical sciences, and they are used today mainly to increase thespeed of what we already know how to do. the strategic use of computing resources requires deeper integration of chemical engineering and chemistry withit, a process that is in its early stages.finding: computation and information technology provide a key enabling force for lowering barriers among the disciplines that comprisethe chemical enterprise and closely related fields.identification of mutual interests among disciplines and removal of the barriers to successful communication among constituencies are essential for increasing the overall effectiveness of the system. the processes of identification and removal are still in their infancy.cooperation and collaboration across disciplines expand the intellectual freedom that people enjoyñthe freedom to access the tools and capabilities to dooneõs work. the interface between information technology and the activities ofchemists, chemical engineers, and allied disciplines is still in its infancy. theopportunity to build that architecture, which will revolutionize the future of theinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.30information and communicationchemical enterprise, is exciting. it is essential that advanced information systemsdraw on and reinforce the intuitions and creative intellectual patterns that havebeen so successful in research, discovery, and technology innovation in the chemical enterprise. moreover, it is critically important to ensure the security of transactions that involve sharing of tools, data, and resources in a manner that provides responsibility for privacy, intellectual property, economic growth, and thepublic trust.intuition based on physical understanding of the system is an essential component for integrating computation and information technologies with the chemical sciences. the essential resource driving the interface of it and the chemicalsciences is human ingenuity, and the supply of this resource is unbounded. indeed, throughout this report, the importance of òpeople issuesó emerges. cooperation and collaboration depend on the attitudes of the individuals who participate. an important first step toward cooperation and collaboration is training thatnurtures a creative, problemsolving approach. the crossdisciplinary interfacebetween it and the chemical sciences should be capable of rejecting preconceptions about the way things òought to be,ó while also providing stable tools thatsupport advances in research and technology. people, not software or data repositories, will recognize the revolutionary breakthrough and how to apply it to theright problem. people will recognize how a suite of interlocking technologies canbe combined with revolutionary impact.finally, it is important to have realistic expectations for success.this is a difficult challenge because what one measures and rewards is what one gets. there are many ways to impact applications and technology with any level of sophistication in a simulation.some of the important ways lend themselves only to intangible measures,but oftentimes these may be the most important. again quoting einstein,ònot everything that counts can be counted, and not everything thatcan be counted counts.óellen stechel (appendix d)research discoveries and innovative technological advances invariably accompany breakthroughs in computation and information technology. the advances to date in these areas make possible an even faster pace of discovery andinnovation. virtually every one of the discovery and application opportunitiesmentioned in the previous chapter depends on sustained advances in informationtechnology.the opportunities at the chemical scienceðit interface are enormous. advances in each of the it areas discussed here will benefit virtually all of theinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.interfaces: cooperation and collaboration across disciplines31discovery and application areas described. profound advances will result whendevelopments in the it area are brought to bear on discovery and applicationsareas. it is critically important to learn how to build teams that bring the component disciplinary skills to bear. it is equally important to maintain depth and disciplinary skills in the fundamentals. clearly the impact gained by solving a keyit challenge in one particular area can be significantly multiplied if it is carriedout in an informed manner that benefits related areas.the process of bringing it developments to bear on discovery areas can servealso to guide new research within the it discipline. the close interaction of itexperts with science and engineering application experts requires a managementapproach that is in part maintaining the value systems that cross multiple disciplines, and in part òenlightened selfinterest.ó the major challenge is to set clearinvestment priorities, and to maximize the benefits across many applications.the computationinformation infrastructure within chemical sciences is variegated and effective. new directions, based on both it advances and the growthof interdisciplinary understanding, offer striking possibilities for research development, learning, and education. one specific new endeavor, the establishmentof integrated, multicomponent gridbased collaborative modelingdata environments (cmdes), is a promising opportunity for federal investment.overarching themesfour key themes emerge repeatedly at the heart of discussions on integratinginformation technology with the chemical sciences. these themes provide aframework for relating research areas with the enabling it technologies. they are(1) targeted design and openended discoveryñtwo routes to solving complexproblems; (2) flow of information between people within and among disciplinesñpeople issues and barriers to solving complex problems; (3) multiscale simulationñone of the key itcentric technical approaches to solving complex problems; and (4) collaborative environmentsñintegrating it methodology and toolsfor doing the work. appropriate educational initiatives linked with these themesare a major need.targeted design and openended discoverytargeted design. targeted design involves working backward from the desired function or specifications of a molecule or product and determining theunderlying structure and the process conditions by which it can be fabricated. thegeneral features of the approach have been used successfully in cases for whichthe need was exceptional and the target was clear: for example, synthetic rubber,aids treatment, and replacements for chlorofluorocarbon refrigerants. extraordinary recent advances in computer speed, computational chemistry, processsimulation and control, molecular biology, materials, visualization, magnetic storinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.32information and communicationage, bioinfomatics, and it infrastructure make possible a wholly new level ofimplementation. by building on past investments, targeted design could pay enormous dividends by increasing the efficiency of the discovery effort and speedingup the innovation process. the process can also provide fundamental researcherswith intuitive insights that can lead to more discoveries, however, a number ofquestions will need to be addressed as this approach is used more widely:¥structurefunction relationships: given the molecular structure, how canits properties be estimated? given the properties, how can they be related to thedesired structure?¥combinatorial search algorithms: how do we evaluate many alternatives to find the ones that meet specifications?¥target identification: what are the criteria for specifying the target?¥roles of participants: how does the technical problem translate into whatthe participants must do to address the issue successfully?¥collaborative problem solving: what tools are needed to connect individuals from different disciplines, and how can these be made to reflect the waypeople work together?¥trust and confidence: appropriate security is essential if collaborationsinvolve proprietary information or methodology. how can the key elements oftrust and confidence be assured?openended research and discovery. openended research and discoveryhas a long tradition and spectacular record of success in the chemical sciences.curiositybased discovery has led to research discoveries (such as oxygen, nuclearmagnetic resonance, dna structure, teflon, and dimensionally stable anodes)that have truly changed the world. the broadbased research investment of thepast half century in curiositydriven research is still paying great dividends intechnological developments in such areas as nanoscience and nanotechnology,microelectromechanical systems (mems), environmental chemistry, catalysis,and the chemicalbiological interface. such an approach requires:¥access to information in a way that informs intuition;¥the flexibility for the individual investigator to modify the approach (anavenue of research in which opensource software can be of tremendous value);and¥data assimilation and visualization (where better and more efficient algorithms for mining data are needed, and where the chemical sciences must learnfrom the information sciences).targeted design builds on the foundation of previous curiositydriven discoveries, but its progress also drives the need for new fundamental understanding. to view the technological developments that grow from openended researchinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.interfaces: cooperation and collaboration across disciplines33as a series of coincidences is to miss the point of the past investment strategy ofmany federal funding agencies. both are essential and both benefit from advancesin information technology. to take full advantage of the discoveries in such areasas chemical biology, nanoscience, environmental chemistry, and catalysis requiresboth approaches. it is important to distinguish between them:¥targeted design does not work if we do not know the target; it is not openended.¥curiositydriven research rarely hits a technology target on its own without guidance; the rate of innovation can be very slow.¥the common ground between these approaches can be expanded and enriched by it advances.the targeted design approach is beginning to be used in the it community.examples are available that are built on several decades of real success in computer science and in pure and applied mathematics:¥very advanced specialpurpose computers, such as the earth simulator injapan; its raw speed is high, as is effectiveness of delivered performance. themachine takes advantage of memoryusage patterns in science and engineeringapplications by way of fast data paths between the central processing unit andmain memory (cf. t. dunning, appendix d).¥algorithmic advances: todayõs supercomputers use commercially oriented commodity microprocessors and memory that are inexpensive but haveslow communications between microprocessor and memory. they try to minimize slow access to main memory by placing a fastcache memory between processor and main memory. this works well if algorithms can make effective use ofcache, but many chemical science and engineering algorithms do not. cachefriendly algorithms are needed to take full advantage of the new generation ofsupercomputers.¥algorithm development based on understanding of the physical system aswell as the computer architecture.¥development of òmodeló sciences to obtain consistent results withoutunnecessary computation: this approach is illustrated by the gaussian2 (g2)model chemistry developed by popleõs group.1the use of targeted design to bring it resources to bear on problems in chemical science and technology is likely to become more widespread. that approachwould provide progress for virtually every point identified earlier in this chapter.many of the applications require capacity computingñlinked with robust, reli1curtiss, l. a.; raghavachari, k.; pople, j. a. j. chem. phys. 1995, 103, 4192.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.34information and communicationable, and accessible problemsolving tools that gain leverage from rapid advancesin network bandwidth, storage, and algorithm efficiency. the need for increasedcapability highperformance computing will also continue to be critically important for the long term, as larger, more complicated, and more accurate simulationsbecome needed. in most cases, increases in performance are a result of adroitalgorithms based on physical understanding of the application in addition to rawprocessor speed.flow of information between people within and among disciplinesadvances in communication and information technology have helped makeit possible to address complex systems with unprecedented success. such systems exhibit nonlinear relationships, large experimental domains, and multipleinteractions between many components. moreover, for most complex systems,the underlying model is generally unknown although some components may bewellcharacterized. it is therefore necessary to employ various kinds of knowledge, theory, insight, experimental data, numerical codes, and other tools. in general, the flow of information passes among communities that may be organizedby skill set (experimentalist, theorist, modeler, programmer, etc.); institution(academia, industry); discipline (chemistry, chemical engineering, computer science); or other such broad groupings.appropriate computational levelsi have a comment about your discussion of the convergence withrespect to basis set in the computation or calculation of molecularenergies. probably the most notable feature of that is it becamevery expensive as you moved to the larger and larger basis sets butthe last order of magnitude in computing that you in fact spent gaveyou almost no information, a result differing very little from the previous numbers, and in fact it seems to me that one is in somedanger of exaggerating the need for very large computation to achievechemical accuracy.many of the results that we have been able to find, we have foundcan be achieved by the judicious use of small empirical correctionsor simple extrapolation schemes. i think that was already evidentfrom your presentation. so i feel there is a slight danger in sayingthat you can only achieve this high a level of accuracy by usingmassive resources.john a. pople, northwestern university(comments following presentation by thom dunning, appendix d)information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.interfaces: cooperation and collaboration across disciplines35figure 41 describes some aspects of information flow in the analysis of complicated problems. the boxes in the figure represent specialized methods and toolsthat contribute to the investigation. these include, for example, experimental techniques, computer codes, numerical methods, mathematical models of wellcharacterized fundamental scientific systems, and numerical simulations of complex components of technological systems. several general observations can be made:¥the tools used in one box often are not easily used by specialists workingin another box.¥robust, reliable methods of remote access are now beginning to emerge:shared equipment (e.g., nmr), data (e.g., bioinfomatics), and codes (e.g., opensource). such beginnings strengthen the flow of information between some of theboxes.physicalsystemnoncontinuumphenomenacontinuumphenomenahypothesesandexperimental dataadditional experiments that test and exploit hypotheses:  multiscale multiphenomena simulationscontinuummodelfinite differencefinite elementnoncontinuummodelmonte carlomolecular dynamicsdesign, control, and optimizationdevices, products, and processesfigure 41flow of information between science and technology in multiscale,multiphenomena investigations.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.36information and communication¥between other boxes, however, barriers still hinder the effectiveness ofthe information flow as noted in the next paragraphs.for the purpose of discussion, consider a specific starting point: ensuringproduct quality at the molecular level requires a new generation of science andengineering tools. this statement applies to many of the areas discussed in thisreport, from nanoscience to biotechnology to sensors and microelectronic devices. the following material considers how this statement translates into needsand barriers between two groups: the technical experts (who create the methodsand tools inside the boxes) and the users (who use the methods and tools to solveproblems). first consider the roles of different kinds of technical experts. it isabundantly clear that the following characterizations are incomplete and overlysimplified, but they nevertheless serve to illustrate some of the salient needs of,and barriers between, different communities of technical experts:¥experimentalists want to produce reliable data. to interpret and validatethe data with sophisticated computational codes, they need modeling expertise.to test the data and to make it accessible to others, they need computer scienceexpertise in database structures.¥modelers want to create simulation codes that compile and run efficiently.from experimentalists, they need uptodate mechanisms and data. from computer scientists and engineers, they need to understand the language, advancingprogramming methods, protocols, data management schemes, and computing architectures, to name a few.¥computer scientists and engineers want to push the envelope of it capabilities, and create structures for efficient workflow and resource utilization. theyneed metadata from experimentalists in order to characterize the data and facilitate use of the data by others. they need an understanding of the physical basis ofmodeling goals, which are invariably the key to improving efficiencies.figure 42 provides a schematic representation in which the three kinds oftechnical expertise are at the corners of a triangle, and the needs that they havefrom each other are summarized along the sides. barriers are shown that blockinformation flow between the experts.once the barriers are removed, each group will want to change the way itdoes things in order to take full and strategic advantage of the others. in keepingwith a focus on the statement that ensuring product quality at the molecular levelrequires a new generation of science and engineering tools, the following examples serve as illustrations:¥experimentalists will produce data at small scales on wellcharacterizedsystems to validate numerical simulations, and also provide measurements atinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.interfaces: cooperation and collaboration across disciplines37multiple scales on entire complex systems to characterize sources of uncertainty.they will document error bars on datasets including images so that others can usethem with confidence. they will be able to suggest simulations to test or resolvemultiple reasonable hypotheses of mechanisms and use uptodate data from others in those simulations.¥modelers will produce portable objectoriented codes so others (includingnonexperts) can modify the original hypotheses and assumptions. they will combine theories at different scales with metadata schema to link continuum andnoncontinuum codes so that multiscale simulations can be done routinely. theywill be able to draw on different experimental methods and data for improvedsimulation including parameter sensitivity and estimation, hypothesis selection,lifetime probability, and risk assessment. they will tune code performance basedon understanding of computer architecture and algorithm strategies.¥computer scientists and engineers will develop robust, reliable, userfriendly collaborative modelingdata environments for linking codes, computers, research data, fieldsensors, and other inputs for use by scientists and engineers. they will provide semiautomatic generation of graphical user interfaces sothat older codes will continue to be useful for design and synthesis as new computing resources become available. in addition, improved security, authentication, authorization, and other measures will be developed that protect privacy andintellectual property associated with commercial applications. the use of webservices will be crucial for the utility of this work.modelersneed: understanding of mechanisms and access to data need: physical insight to steer improvements in efficiency and capabilityneed: understand language, protocols, architecture need: modify and use models and codes to interpret dataneed: expertise in database structuresexperimentalistscomputer scientists and engineersneed: metadata barrierbarrierbarrierfigure 42needs and barriers for interactions among technical experts.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.38information and communicationin order to reduce barriers several steps must be addressed:¥promote and ensure decentralized autonomy of creative technical expertsin the chemical sciences, while facilitating a multidisciplinary team approach totargeted design.¥continue the integration of it with the chemical sciences, which havebeen quick to accept it and use it throughout the discipline.¥increase multidisciplinary training and provide financial mechanisms tosupport it.¥develop modular, portable, and extensible programming methods, including objectoriented component architectures for portable modeling (i.e., softwarethat can be reused in multiple applications).¥expand use of opensource software for community codes where it is appropriate for many individuals to participate in code development.¥strengthen the scientific method of testing hypotheses by experiment bylowering barriers to flow of information among modelers, experimentalists, andcomputer scientists.¥optimize the research infrastructure for multidisciplinary team approachesto problems.¥develop a permanent, ongoing interaction between the chemical sciencesand it communities to guide investments in infrastructure for the chemical sciences.next consider the roles of users who use the methods and tools developed bytechnical experts to solve problems.¥users may include other technical experts, industrial technicians and trainees, students, and learners.¥most users will want to input their own variablesñwhich may be confidentialñand will want return to their work leaving no fingerprints for others toview.¥such users may have limited interest in technical details, but will need aclear understanding of assumptions and limitations on use as well as robust, reliable, userfriendly access.¥users will require a feedback mechanism so they can direct the experts toareas where improvements are needed.finding: addressing critical challenges at the interfaces with other scientific and engineering disciplines will enable chemistry and chemicalengineering to contribute even more effectively to the nationõs technological and economic progress.the most important challenge involves people. advances in it that facilitateinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.interfaces: cooperation and collaboration across disciplines39selforganization of problemsolving groups with common interests acrossdisciplinary boundaries will impact strongly both understandingbased andapplicationdriven projects. the essential resource driving the interface of itand the chemical sciences is human ingenuity.multiscale simulationafter a century of success in understanding the fundamental building blocksand processes that underlie our material world, we now face another, even greater,challenge: assembling information in a way that makes it possible to predict thebehavior of complex, realworld systems. we want to predict properties of physical, environmental, and biological systems over extended length and time scales;understand how component processes interact; fill gaps in experimental data withreliable computational predictions of molecular structure and thermodynamicproperties; and develop reliable methods for engineering process design and quality control. here is a scientific frontier for the twentyfirst century. with virtuallyany of the examples emphasized throughout this report, multiscale simulationwill play a critical role.figure 43 shows the broad range of time and length scalesñsome 15 ordersof magnitude or moreñthat are encountered for one particular field of application involving electrochemical processing for chip manufacture. the figure isdivided into three vertical bands in which different kinds of numerical simulationtools are used: noncontinuum, continuum, and manufacturing scale. each of thetopics indicated in the figure is placed in the approximate position to which itcorresponds. for some of the topics, the space and/or time axis may be more orless approximate and appropriate. there is today a sophisticated understanding ofeach of these topics. multiscale simulation involves linking the pieces in order tounderstand interactions within an entire system.other topics such as drug development and manufacture, environmentalremediation, or coatings technology show similar multiscale aspects. a few examples serve to illustrate the very broad range of activities for which multiscalesimulations are important:¥the internal combustion engine involves the engine itself, as well as thefluid dynamics of the fuelair mixture as it flows through the combustion regionand the chemical process of ignition and burning of the fuel, which can involvehundreds of chemical species and thousands of reactions, as well as particulatematter dynamics and the ultimate fate of combustion products through atmospheric chemistry.¥creating a virtual simulation of a periodic living cell involves a substantial challenge. the organelles in such a cell include about 4 million ribosomes.there are about 5 billion proteins drawn from 500010,000 different species, ameter of dna with several billion bases, 60 million transfer rnas, and vastinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.40information and communicationnumbers of chemical pathways that are tightly coupled through various feedbackloops in which the proteins are turning the genes on and off in the regulationnetwork. all of the relevant detail would have to be captured.¥the design and degradation of materials involves multiple scalesñfromatoms to processes, as well as stress and environmental degradation from corrosion, wear, fatigue, crack propagation, and failure.multiscale simulation involves the use of distinct methods appropriate fordifferent length and time scales that are applied simultaneously to achieve a comprehensive description of a system. figure 44 illustrates some of the computational methods that have been developed over many decades in order to deal withphenomena at different time and length scales to compute properties and modelphenomena. these include quantum mechanics for accurate calculation of smalllength scale (m)time scale (s)101210121010104102100102104106108109106103100103106109101210151014noncontinuummodelscontinuummodelselectrontransferadditivesmonolayerssurface filmsdoublelayerinterconnectsmorphologyboundary layerspotential fieldcurrent distributionwaferdeposition toolsfabfacilitiesmanufacturingfigure 43schematic of the time and length scales encountered in multiscale simulations involving electrochemical processing for chip manufacture.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.interfaces: cooperation and collaboration across disciplines41and fast phenomena, statistical mechanics and semiempirical modeling formechanistic understanding, the mesoscopic scale where new methods are nowbeginning to emerge for multiscale modeling, continuum mechanics for macroscopic reaction and transport modeling, process simulation for characterizing andoptimizing entire process units and their interconnection, and supply chain modeling that integrates over multiple plants, customers, and global logistics. in manycases, modeling and simulation techniques that were originally developed in othercommunities, such as computational physics and materials science, are now being used in the chemical sciences, and vice versa. specific examples can be foundin appendix d in presentations by depablo, dunning, maroudas, and petzold.ultimately what one would like to do from an engineering viewpoint wouldbe to use all of these methodologies to explore vast regions of parameter space,identify key phenomena, promote these phenomena that lead to good behavior,avoid the phenomena that lead to failure, and so on. the range of scales mayextend from quantum chemistry to atomistic and molecular modeling tomesoscopic to continuum macroscopic mechanics to largescale integration ofprocesses. progress over the past decade at each of these scales suggests that itmay now be possible to combine methods from different scales to achieve a comprehensive description of a system. such techniques will address important issuesmicroscale design issuesthe major development that is driving change, at least in myworld, is the revolution at the micro scale. many people are workingin this area now, and many think it will be as big as the computerrevolution was. of particular importance, the behavior of fluid flownear the walls and the boundaries becomes critical in such smalldevices, many of which are built for biological applications. we havelarge molecules moving through small spaces, which amounts tomoving discrete molecules through devices. the models will oftenbe discrete or stochastic, rather than continuous and deterministicña fundamental change in the kind of mathematics and the kind ofsoftware that must be developed to handle those problems.for the engineering side, interaction with the macro scale world isalways going to be important, and it will drive the multiscale issues.important phenomena occurring at the micro scale determine thebehavior of devices, but at the same time, we have to find a way tointeract with those devices in a meaningful way. all that must happen in a simulation.linda petzold (appendix d)information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.42information and communicationthat include highthroughput, combinatorial modeling and highdimensional design space. the simulation of systems that involve such broad ranges of lengthand time scales naturally requires a multidisciplinary approach.the current generation of multiscale modeling activity represents the beginningsof a new approach for assembling information from many sources to describe thebehavior of complicated systems. the current methodology will need steady improvement over the next decade. for any multiscale model, what is missing in the earlystages of development is less important. what is important is what is not missing.multiscale modeling is a kind of targeted design activity, and it requires physicalintuition to decide what does not matter. these examples point to several critical areaswhere advances are needed to provide improved multiscale capabilities:¥computing power: the underlying progress of the past decade has resulted in part from the increasing power of computers and from important theoretical and algorithmic developments. bringing these together for commoditycomputing places significant demands on the information infrastructure.¥computers: multiple architectures will be needed for different types ofcalculations. major gains in the chemical sciences will require accuracy and anunderstanding of the error bounds on the calculations. highend computing willincreasingly serve to validate simpler approaches.figure 44modeling elements and core capabilities.continuum mechanics:¥finiteelement & difference methods¥boundaryintegral methodsmacroscopic modelingmodeling for mechanisticunderstandingstatistical mechanics:¥semiempirical hamiltonians¥molecular statics, lattice dynamics,molecular dynamics, monte carloquantum mechanics:¥ab initio, electronic structure¥density functional theory, ¥first principles molecular dynamicsaccurate calculations ofmaterials propertieslengthtimeprocess simulation:¥equationbased models¥control and optimizationprocessing units/facilitiessupply chain modeling:¥planning & scheduling¥mixed integer linear programmingglobal logisticsmesoscopic scale:¥coarsegrained quantum and statistical mechanics¥mixed/coupled atomisticcontinuum methodsmesoscale modeling information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.interfaces: cooperation and collaboration across disciplines43¥formalisms: actual multiscale models are nearly always based on approximate separations of length and time scales (e.g., langevin equations forcoarse graining in time, or quantum mechanicsðmolecular mechanics algorithmsfor large systems). formal approaches are needed for understanding and improving such approximations.¥software: investments in software are needed, especially in the area ofautomated programs for software development and maintenance. for multiscalesimulations, componentbased methods will be needed to develop simulations ofentire systems. the funding portfolio for algorithm development should includeapplicationdriven projects that can establish the scientific and technological foundations necessary for a software component industry.¥interoperability: protocols and data structures are needed to achieveinteroperability of applications that run concurrently.¥web services: programs and associated data that can be accessed by othercomputers.¥computational steering: the modeler should be able to interact with asimulation in real time to steer it towards a particular goal. this will require bothalgorithmic and hardware advances.¥uncertainty and reliability: methods are needed for assessing sources oferror in components and their aggregation into multiscale systems, for example,systems that combine computational results with experimental data and images.¥accessibility: objectoriented or other portable component architecture isneeded for models that can be modified and reused by others.¥standardization: experts need certification standards for producing robust, reliable components that have a clear description of approximations andlimits so that nonexperts can use them with confidence.¥people: a robust supply of workers with multidisciplinary training willbe needed in response to increasing demand.many of the processes of interest in chemistry and chemical engineering occur on much longer time scales (e.g., minutes or hours);it is unlikely that the several orders of magnitude that now separateour needs from what is possible with atomisticlevel methods will bebridged by the availability of faster computers. it is therefore necessary to develop theoretical and computational methods to establisha systematic connection between atomistic and macroscopic timescales. these techniques are often referred to as multiscale methods or coarsegraining methods.juan de pablo (appendix d)information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.44information and communicationcollaborative environmentsthe most difficult challenges in advancing human welfare through knowledge of the chemical sciences involve a level of complexity that can only nowbegin to be addressed through information technology. such efforts are inherently multidisciplinary and involve working in teams to assemble informationand solve problems. new methods for collaborative discovery and problem solving in chemical science and technology are needed so that individual researcherscan participate in distributed collective action and communicate effectively beyond their discipline. these methods should be a significant component of education in the chemical sciences.prototype collaborative environments are emerging, with examples such assimple webbased browsers that provide access to applications. the environmentsextend to more sophisticated demonstrations of gridbased services that utilizeadvanced middleware to couple, manage, and access simulation codes, experimental data, and advanced tools, including remote computers. current early prototype examples include the doe science grid, eurogrid (european union),unicore (german federal ministry for education and research), informationpower grid (nasa), teragrid alliance portal (nsf/ncsa), punch (purdue),and many other projects, applications, and libraries including some commercialactivities. such environments promise an integrating structure for composing substantial applications that execute on the grid. they offer great promise for sharing codes, data, and expertise, as well as for linking the pieces and keeping themup to date. many of the prototype environments utilize complex gridbased protocols that are still in early stages of development or consist of demonstrationsthat are not easily reused or modified. if these prototypes are to fulfill their promise for effective problem solving, pioneering chemical scientists and engineersmust be brought into close working relationship with computer scientists andengineers to develop and tune the architecture. infrastructure expenditures in information technology will be investments only if they can eventually be used forsolving problems faced by society.to support growing demand for and dependence on information infrastructure for applications in the chemical science and technology, advances are neededin many different dimensions:¥software: for collaborations, there will be increasing needs for userfriendly and robust software that provides assured standards for quality, reliability, access, interoperability, and sustained maintenance. standards are needed tofacilitate collaborative interactions between applications personnel who are notexperts in computer science. use of software to upgrade legacy codes (or dustydecks)ñcodes that have been around a while (1015 years), usually written byothers who are no longer available, run on currentgeneration computers and systems for which it was not optimized, and therefore difficult to modify and deinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.interfaces: cooperation and collaboration across disciplines45bugñwill be increasingly important for sustaining the effective lifetime of collaborative tools so as to recover a larger portion of the initial investment.¥computers: because the range of applications and collaborative dimensions is large, access to a wide range of computing platforms is essential for thechemical sciences. in the past, federal investments in highend computing and networking have provided a successful path for pioneering and eventually developingcapabilities for a very broadbased user community. as the pace of change quickens in the future, it will become increasingly important to maintain even closer tiesbetween pioneering efforts and their rapid dispersal in broad user communities.¥simulation codes: today, the portfolio of commercial, opensource, andother specialty simulation codes constitutes a rich but unconnected set of tools.while some commercial and opensource codes are userfriendly, many specialtycodes are idiosyncratic and excessively fragile. advanced techniques, such as objectoriented programming, are needed to facilitate code reuse by individuals otherthan the creators. working in collaborative groups, experimental scientists will wantto modify codes (to test alternative hypotheses, assumptions, parameters, etc.) without the need to recompile. in addition, technologists will want to explore operatingconditions and design variables, as well as scientific hypotheses.¥data and knowledge management: hypotheses often continue to influence decisionsñeven after being proven wrong. legacy simulations too oftenoperate on the basis of disproved or speculative hypotheses rather than on recentdefinitive data. the chemical enterprise has created extensive data crypts of flatfiles (such as chemical abstracts). it now needs to develop datarich environments (such as fully normalized databases) to allow new crossdisciplinary research and understanding. within certain industries, such capabilities already exist, at least in early form. an example is offered by the pharmaceutical industry,where informatics hubs integrate biological, chemical, and clinical data in a singleavailable environment. the science and technology enterprise needs a way tomanage data such that it is relatively easy to determine what the community doesand does not know, as well as to identify the assumptions underlying currentknowledge.¥uncertainty and risk: in collaborative efforts it is important to assesssources of error in both experimental and numerical data in order to establishconfidence in the conclusions reached. managing the uncertainty with which dataare known will reduce the risk of reaching unwarranted conclusions in complicated systems. improved, easytouse risk assessment tools will assist in identifying the weakest link in collaborative work and help steer efforts toward developing improvements where they are most needed.¥multiscale simulation: many collaborative efforts involve multiscalesimulations, and the comments about uncertainty and risk apply here as well.¥quality: models and data inform intuition but rarely persuade individualsthat they are wrong. careful design of collaborative modelingdata environmentsñto facilitate appropriate access to codes, data, visualization tools, cominformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.46information and communicationputers, and other information resources (including optimal use of web services)ñwill enable participants to test each otherõs contributions.¥security: industrial users in the chemical sciences require security in various ways, such as avoiding unwanted fingerprints on database searches, preventing inappropriate access through firewalls, and providing confidence that codeswork and that data are accurate. collaborative environments must facilitate openinquiry about fundamental understanding, as well as protect intellectual propertyand privacy while reducing risk and liability. economic models differ amongvarious user and creator communities for software and middlewareñfrom opensource, to large commercial codes, legacy codes behind firewalls, and fleetfootedand accurate specialty codes that may, however, be excessively fragile. the security requirements of the industrial chemical technologies thus represent both achallenge and an opportunity for developing or marketing software and codes. inmultidisciplinary collaborations, values including dogmatic viewpoints established in one community often do not map onto other disciplines and cultures.¥people: collaborations are inherently multidisciplinary, and the point hasbeen made throughout this report that the pool of trained individuals is insufficient to support the growing opportunities.collaborations bring many points of view to problem solving and offer thepromise of identifying barriers rapidly and finding new unanticipated solutionsquickly. anticipated benefits include the following:¥bringing in simulation at the beginning of complex experiments to helpgenerate hypotheses, eliminate dead ends, and avoid repeated failures from a trialanderror approach;¥arriving at approximate answers quickly with a realistic quantification ofuncertainty so that one has immediate impact, deferring highly accurate answersto a later time when they are needed; bringing in data at the beginning of simulation efforts allows simple calculations to help frame questions correctly and assure that subsequent detailed experiments are unambiguous; and¥attaining new capability for dealing with the realistic, not overly idealized, problems at various levels of sophistication; some problems lend themselvesonly to intangible measures, but often these may be the most important approaches.federal support for one or more collaborative modelingdata environmentscould impact tremendously the value of it for the chemical community. thesewould be ideal structures for advancing learning, research, insight, and development on major issues confronting both the chemical community and the larger society. the cmdes can offer advantages of scale and integration for solving complicated problems from reaction modeling to corrosion. these are also uniquelyinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.interfaces: cooperation and collaboration across disciplines47promising for computational tools and methods development, for developing integrated gridbased modeling capabilities, for building appropriate metaprogramsfor general use, and for providing powerful computational/understanding structuresfor chemical professionals, students, and the larger community.education and trainingfor the chemical sciences to be able to train and enable people in optimalfashion, we must understand and employ information and communications in afar more sophisticated and integrated fashion than we have done to date. this useof information and communications as an enabler of peopleñas a developer ofhuman resourcesñis a great challenge to the chemical sciences in the area ofinformation and communications.in broad terms, information and communications methods should permit thechemical scientist to create, devise, think, test, and develop in entirely new ways.the interactions and crossfertilization provided by these methods will lead tonew materials and products obtained through molecular science and engineering.they will lead to understanding and control of such complex issues as combustion chemistry and clean air, aqueous chemistry and the water resources of theworld, protein chemistry and drug design, and photovoltaics and hydrogen fuelcells for clean energy and reduced petroleum dependence. they can help to design and build structures from microelectronic devices to greener chemical plantsand from artificial cells and organs to a sustainable environment.finding: the capability to explore in the virtual world will enable society to become better educated and informed about the chemical sciences.conveying the intellectual depth, centrality, societal benefits, and creativechallenges of molecular systems will be greatly facilitated by the use ofmodeling, visualization, data manipulation, and realtime responses. all ofthese new capabilities will provide unparalleled environments for learning,understanding, and creating new knowledge.finding: the growing dependence of the chemical enterprise on use ofinformation technology requires that chemical professionals have extensive education and training in modern it methods.this training should include data structures, software design, and graphics.because data and its use comprise such important aspects of chemistry andchemical engineering, and because appropriate use of it resources can empower unprecedented advances in the chemical arena, it is crucial that theappropriate training, at all levels, be a part of chemical education.the collaborative modelingdata environments could both provide thesecapabilities and educate people from 8 to 88. the proposed cmdes would allowinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.48information and communicationunique learning by permitting students to use different parts of the metaprogramõscapabilities as their understandings and abilities advance. this could give students specific problemsolving skills and knowledge (òi can use that utilityó) thatare portable, valuable, and shared. all of these new capabilities would provideunparalleled environments for learning, understanding, and creating.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.495infrastructure: capabilities and goalsremarkable advances in information technologiesñcomputer speed, algorithm power, data storage, and network bandwidthñhave led to a new era ofcapabilities that range from computational models of molecular processes to remote use of oneofakind instruments, shared data repositories, and distributedcollaborations. at the current pace of change, an orderofmagnitude increase incomputing and communications capability will occur every five years. advancesin information technology (it) allow us to carry out tasks better and faster; inaddition, these sustained rapid advances create revolutionary opportunities. weare still at the early stages of taking strategic advantage of the full potential offered by scientific computing and information technology in ways that benefitboth academic science and industry. investments in improving chemicalbasedunderstanding and decision making will have a high impact because chemicalscience and engineering are at the foundation of a broad spectrum of technological and biological processes. if the united states is to maintain and strengthen itsposition as a world leader, chemical science and technology will have to aggressively pursue the opportunities offered by the advances in information and communication technologies.at the intersection of information technology and the chemical sciences thereare infrastructural challenges and opportunities. there are needs for infrastructure improvements that could enable chemical scientists and engineers to attainwholly new levels of computingrelated research and education and demonstratethe value of these activities to society. these needs extend from research andteaching in the chemical sciences to issues associated with codes, software, dataand storage, and networking and bandwidth.some things are currently working very well at the interface of computinginformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.50information and communicationand the chemical sciences. networking and internet highspeed connectivity havebeen integrated into the chemical sciences, changing the landscape of these fieldsand of computational chemistry. commercial computational chemistry softwarecompanies and some academic centers provide and maintain computational andmodeling codes that are widely used to solve problems in industry and academia.however, these companies and centers do not, and probably cannot, provide theinfrastructure required for the development of new scientific approaches andcodes for a research market that is deeply segmented. the development of newcodes and applications by academia represents a mechanism for continuous innovation that drives the field and helps to direct the choice of application areas onwhich the power of computational chemistry and simulation is brought to bear.modern algorithms and programming tools have speeded new code developmentand eased prototyping worries, but creating the complicated codes typical ofchemical science and engineering applications remains an exceedingly difficultand timeconsuming task. defining new codes and applications is potentially agrowth area of high value but one that faces major infrastructure implications if itis to be sustained.successful collaborations between chemists and chemical engineers, as wellas broadly structured interdisciplinary groups in general, have grown rapidly during the past decade. these have created the demand for infrastructure development to solve important problems and new applications in ways never beforeenvisioned. the current infrastructure must be improved if it is to be used effectively in interdisciplinary team efforts, especially for realizing the major potentialimpact of multiscale simulations. infrastructure developments that support improved multidisciplinary interactions include resources for code development,assessment, and lifecycle maintenance; computers designed for science and engineering applications; and software for data collection, information management,visualization, and analysis. such issues must be addressed broadly in the way thatfunding investments are made in infrastructure, as well as in crossdisciplinaryeducation and in the academic reward structure.the overarching infrastructure challenge is to provide at all times the neededaccessibility, standardization and integration across platforms while also providing the fluidity needed to adapt to new concurrent advances in a time of rapidinnovation.researchsignificant gains in understanding and predictive ability are envisioned toresult from the development of multiscale simulation methods for the investigation of complicated systems that encompass behavior over wide ranges of timeand length scales. such systems usually require a multidisciplinary approach.often, multiscale simulations involve multiple phenomena that occur simultaneously with complex, subtle interactions that can confound intuition. while muchinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.infrastructure: capabilities and goals51is known about simulating aspects of behavior at individual scales (e.g., ab initio,stochastic, continuum, and supplychain calculations), integration across scales isessential for understanding the behavior of entire systems.a critical component in achieving the benefit implied by multiscale modeling will be funding for interdisciplinary research for which effective, collaborative webbased tools are required. the integration of computational results withexperimental information is often necessary to solve multiscale problems. in someinstances, creating opportunities to access shared equipment will be as critical asaccess to shared computers or software. especially important is the ability torepresent and understand the uncertainties, not only in the underlying scientificunderstanding, but also in experimental data that may come from extremely heterogeneous sources. the infrastructure to achieve these research goals must include definition of standard test cases for software and experiments.basic infrastructure needs include highbandwidth access to highperformance computational facilities, further increased network and bus speed, diversecomputer architectures, shared instruments, software, federated databases, storage, analysis, and visualization. computers designed with a knowledge of thememory usage patterns of science and engineering problems will be useful, aswill algorithms that take full advantage of the new generation of supercomputers.continuation of the successful trend towards clusters of commodity computersmay result in further opportunities for improved computational efficiency andcost effectiveness. software should be characterized by interoperability and portability so that codes and computers can talk to each other and can be moved in aseamless manner to new systems when they become available.educationthe need for student learning in basic mathematics at the intersection ofcomputing and the chemical sciences is essential because it provides the foundation for computational chemistry, modeling, and simulation as well as associatedsoftware engineering. although many entrylevel students in the chemical sciences are familiar with the use of computers and programs, they often have littleor no understanding of the concepts and design principles underlying their use.the integration of these topics in interdisciplinary courses is essential for thedevelopment of a skilled workforce.1 educational activities will require the investment of time and resources to develop new content in the curriculum forchemists and chemical engineers. new pedagogical approaches at both the undergraduate and graduate levels will be needed to address subjects at the interface ofdisciplines linked by scientific data, programming, and applications areas. training students to adopt a problemsolving approach is critically important for good1building a workforce for the information economy, national research council, national academy press, washington, dc, 2001.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.52information and communicationsoftware engineeringñand especially for producing codes and data structuresthat are useful to other people. a national community of educational opensourcesoftware would help speed development of training tools.just as training in mathematics and physics has been needed for work inchemical sciences and engineering, so will specific education in the use of modern it tools, software design, and data structures be needed by the chemical professional of the twentyfirst century. such education will help in the rapid development of new approaches, crossdisciplinary integration, and integrated datahandling and utilization.interdisciplinary research and development at the itðchemical science interface are areas of great excitement and opportunity. nevertheless, people trainedto carry out such projects are in short supply. the continued capability of individuals requires both deep competence and the ability to interact across disciplines. the emphasis in graduate training therefore must be balanced betweenspecialization within a discipline and crossdisciplinary collaboration and teamwork. transfer of information between fields remains difficult when evaluatingperformance, particularly for tenure and promotion of faculty who focus on interdisciplinary projects or hold joint appointments in multiple departments. suchevaluation of scholarship will require attentive administrative coordination to resolve cultural differences. creating highquality educational programs to trainpeople to work at interdisciplinary interfaces is currently a ratelimiting step inthe growth of the field. recognizing and rewarding the success of interdisciplinary scientists at different stages in their careers is becoming critically importantfor the sustained development of the field.computational chemistry and simulation methods should be incorporated intoa broad range of educational programs to provide better understanding of thescope and limitations of various methods, as well as to facilitate their applicationover the full range of interdisciplinary problems to which they apply. both science and engineering applications have to be addressed, since these can havedifferent goals and methods of pursuitñwith widely differing levels of sophistication. these include simple applications that can be helpful in early stages, complicated applications that require greater skill, and applications to truly complexnonlinear systems that represent the current focus of many experts in the field.such training will benefit industry, where there is a need for computational specialists who understand the goals and objectives of a broad interdisciplinary problem and know how and when computational chemistry and systemslevel modeling can provide added value. in academia, infrastructure support to facilitate bettercommunication and interaction between chemists and chemical engineers willenhance the training of computational experts of the future. the field will be wellserved by establishing commonality in understanding and language between thecreators and users of codes as well as the skilled computer science and engineering nonusers who develop the it methods.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.infrastructure: capabilities and goals53an increasingly important part of the infrastructure will be the skilled workers who maintain codes, software, and databases over their life cycle. the widevariety of tasks that require sustained management may necessitate a combination of local (funded through research grants) and national (funded through centergrants) support to address the overall portfolio of needs.advances in the chemical sciences have permitted major advances in medicine, life science, earth science, physics and engineering, and environmental science, to name a few. advances in productivity, quality of life, security, and economic vitality of global and american society have flowed directly from theefforts of people who work in these fields. looking to the future, we need to buildon these advances so that computational discovery and design can become standard components of broad education and training goals in our society. in thisway, the human resources will be available to create, as well as to realize andembrace, the capabilities, challenges, and opportunities provided by the chemicalsciences through advanced information technology.information and communication, data and informatics, and modeling andcomputing must become primary training goals for researchers in chemical science. these skills have to be accessible to effectively serve others in the societyñfrom doctors to druggists, ecologists to farmers, and journalists to decisionmakersñwho need an awareness of chemical phenomena to work effectively andto make wise decisions. such skills provide liberating capabilities that enableinteractions among people and facilitate new modes of thought, totally new capabilities for problemsolving, and new ways to extend the vision of the chemicalprofession and of the society it serves.codes, software, data and bandwidtha critical issue for codes, software, and databases is maintenance over thelife cycle of use. in the academic world, software with much potential utility canbe lost with the graduation of students who develop the codes. moreover, ascodes become more complicated, the educational value of writing oneõs own codemust be balanced against the nontrivial effort to move from a complicated idea, toalgorithm, and then to code. increasing fractions of student researchers are tending to develop skills with simpler practice codes, and then to work with andmodify legacy codes that are passed down. yet at the same time, working in a bigcoding environment with codes written by people who have long gone is difficultand often frustrating. development of software that uses sourcecode generationto automatically fix dusty decks will be increasingly important for decreasing thetime and effort associated with extending the useful life of codes. also, the development of semiautomatic methods for generation of improved graphical user interfaces will reduce a significant barrier to sustaining the use of older code. although the opensource approach works well for communities where thousandsinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.54information and communicationof eyes help remove bugs, it is unable to accommodate certain applicationsñforexample, when proprietary information is involved.growth in multiscale simulation may be expected to drive development ofimproved tools for integration of different software systems, integration of different hardware architectures, and use of shared code by distributed collaborators.an increasing need will steadily result for improved interoperability and portability and for users to be able to modify codes created by others. advances in objectoriented programming and component technology will help. examples such asthe portable, extensible toolkit for scientific computing (petsc) library atargonne national laboratory represent the kind of infrastructure that will support growth in strategic directions.central to the vision of a continuously evolving code resource for new applications is the ability to build on existing concepts and codes that have been extensively developed. however, at present, academic code sharing and support mechanisms can at best be described as poorñsometimes as a result of perceivedcommercialization potential or competitive advantage. moreover, code development and support are not explicitly supported by most research grants, nor ismaintenance of legacy codes. consequently, adapting academic codes from elsewhere may generate a risk that the code will become unsupported during its useful life cycle. avoiding this risk results in continual duplication of effortñtoproduce trivial codes that could be better served by opensource toolkits and libraries maintained as part of the infrastructure.the assurance of code verification, reliability, standardization, availability,maintenance, and security represents an infrastructure issue with broad implications. sometimes commercial software has established a strong technical base,excellent interfaces, and userfriendly approaches that attract a wide range ofusers. commercial software can be valuable when market forces result in continuous improvements that are introduced in a seamless manner, but generally,commercial code development is not well matched to the needs of small groupsof research experts nor to many large potential markets of nonexperts. thereforea critical need exists to establish standards and responsibilities for code.the rapid growth of data storage per unit cost has been accompanied byequally significant increases in the demand for data, with the result that there israpid increase in emphasis on data issues across chemical science and engineering. bioinformatics and pharmaceutical database mining represent areas in whichsophisticated methods have been effective in extracting useful knowledge fromdata. newly emerging applications include scientific measurements, sensors inthe environment, processengineering data, manufacturing execution, and supplychain systems. the overall goal is to build data repositories that can be accessed easily by remote computers to facilitate the use of shared data amongcreative laboratory scientists, plant engineers, processcontrol systems, businessmanagers, and decision makers. achieving this requires improved proceduresthat provide interoperability and dataexchange standards.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.infrastructure: capabilities and goals55the integration of federated databases with predictive modeling and simulation tools represents an important opportunity for major advances in the effectiveuse of massive amounts of data. the framework will need to include computational tools, evaluated experimental data, active databases, and knowledgebasedsoftware guides for generating chemical and physical property data on demandwith quantitative measures of uncertainty. the approach has to provide validated,predictive simulation methods for complicated systems with seamless multiscaleand multidisciplinary integration to predict properties and to model physical phenomena and processes. the results must be in a form that can be visualized andused by even a nonexpert.in addition to the insightful use of existing data, the acquisition of new chemical and physical property data continues to grow in importanceñas does the needto retrieve data for future needs. such efforts require careful experimental measurements as well as skilled evaluation of related data from multiple sources. itwill be necessary to assess confidence with robust uncertainty estimates; validatedata with experimentally or calculated benchmark data of known accuracy; anddocument the metadata needed for interpretation.there is a need to advance it systems to provide scientific data and availablebandwidth in the public arena. highquality data represent the foundation uponwhich public and proprietary institutions can develop their knowledgemanagement and predictive modeling systems. it is appropriate that federal agencies participate in the growing number of data issues that are facing the chemical scienceand engineering communityñincluding policy issues associated with access todata. improved access to data not only will benefit research and technology butwill provide policy and decision makers with superior insights on chemical datacentric matters such as environmental policy, natural resource utilization, andmanagement of unnatural substances. expanded bandwidth is crucial for collaborations, data flow and management, and shared computing resources.you might ask òwhat is the twentyfirst century grid infrastructurethat is emerging?ó i would answer that it is this tightly opticallycoupled set of data clusters for computing and visualization tiedtogether in a collaborative middle layer. é so, if you thought youhad seen an explosion on the internet, you really havenõt seen anything yet.larry smarr (appendix d)anticipated benefits of investment in infrastructurechemical science and engineering serve major sectors that, in turn, have awide range of expectations from infrastructure investments. at the heart of theseinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.56information and communicationis the development and informed use of data and simulation tools. the use ofinformation technology to facilitate multidisciplinary teams that collaborate onlarge problems is in its infancy. sustained investment in information technologiesthat facilitate the process of discovery and technological innovation holds trulysignificant promise, and the chemical sciences provide a large number of potential testbeds for the development of such capabilities.in science and engineering research, the complex areas identified in chapter4 are clear points of entry for computer science, engineering, and applied mathematics along with chemical science and engineering. one of the great values ofsimulation is the insight it gives into the inner relationships of complicated systemsñas well as the influence this insight has on the resulting outcome. the keyenabling infrastructure elements are those that enhance the new intuitions andinsights that are the first steps toward discovery.the advances being made in grid technologies and virtual laboratories will enhance our ability to access and use computers, chemical data, and firstofakind or oneofakind instruments to advancechemical science and technology. grid technologies will substantially reduce the barrier to using computational models to investigate chemical phenomena and to integrating data from various sourcesinto the models or investigations. virtual laboratories have alreadyproven to be an effective means of dealing with the rising costs offorefront instruments for chemical research by providing capabilitiesneeded by researchers not colocated with the instrumentsñall weneed is a sponsor willing to push this technology forward on behalfof the user community.the twentyfirst century will indeed be an exciting time for chemical science and technology.thom dunning (appendix d)in industrial applications, tools are needed that speed targeted design andimpact business outcomes through efficient movement from discovery to technological application. valuing it infrastructure tools requires recognizing how theyenhance productivity, facilitate teamwork, and speed timeconsuming experimental work.finding: federal research support for individual investigators and forcuriositydriven research is crucial for advances in basic theory, formalisms, methods, applications, and understanding.history shows that the investment in longterm, highrisk research in thechemical sciences must be maintained to ensure continued r&d progressthat provides the nationõs technological and economic wellbeing. largescale, largegroup efforts are complementary to individual investigatorinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.infrastructure: capabilities and goals57projectsñboth are crucial, and both are critically dependent on nextgeneration it infrastructure.finding: a strong infrastructure at the intersection with informationtechnology will be critical for the success of the nationõs research investment in chemical science and technology.the infrastructure includes hardware, computing facilities, research support,communications links, and educational structures. infrastructure enhancements will provide substantial advantages in the pursuit of teaching, research,and development. chemists and chemical engineers will need to be ready totake full advantage of capabilities that are increasing exponentially.to accomplish this we must do the following:¥recognize that significant investments in infrastructure will be necessaryfor progress.¥enhance training throughout the educational system (elementary throughcomputeraided design of pharmaceuticalscomputeraided molecular design in the pharmaceutical industryis an application area that has evolved over the past several decades. documentation of success in the pharmaceutical discoveryprocess now transcends reports of individual applications of varioustechniques that have been used in a specific drug discovery program. the chemistry concepts of molecular size, shape, and properties and their influence on molecular recognition by receptors ofcomplementary size, shape, and properties are central unifying concepts for the industry. these concepts and computational chemistryvisualization tools are now used at willñand without hesitationñbyvirtually all participants, regardless of their core discipline (chemistry, biology, marketing, business, management). such ubiquitoususe of simple chemical concepts is an exceedingly reliable indicatorof their influence and acceptance within an industry. the conceptsthat unify thinking in the pharmaceutical discovery field seeminglyderive little from the complexity and rigor of the underlying computational chemistry techniques. nevertheless, there is little reason toassume that these simple concepts could ever have assumed acentral role without the support of computational chemistry foundations. in other words, having a good idea in science, or in industry,does not mean that anyone will agree with you (much less act on it)unless there is a foundation upon which to build.organizing committeeinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.58information and communicationpostgraduate) for computational approaches to the physical world. assuring thatchemists and chemical engineers have adequate training in information technology is crucial. programming languages have been the traditional focus of sucheducation; data structures, graphics, and software design are at least as importantand should be an integral component (along with such traditional fundamentalenablers as mathematics, physics, and biology) of the education of all workers inchemistry and chemical engineering.¥maintain national computing laboratories with staff to support researchusers in a manner analogous to that for other user facilities.2¥develop a mechanism to establish standards and responsibilities for verification, standardization, availability, maintenance, and security of codes.¥define appropriate roles for developers of academic or commercial software throughout its life cycle.¥provide universal availability of reliable and verified software.the findings and recommendations outlined here and in previous chapters showthat the intersection of chemistry and chemical engineering with computing andinformation technology is a sector that is ripe with opportunity. important accomplishments have already been realized, and major technical progress should be expected if new and existing resources are optimized in support of research, education, and infrastructure. while this report identifies many needs and opportunities,the path forward is not yet fully defined and will require additional analysis.recommendation: federal agencies, in cooperation with the chemicalsciences and information technology communities, will need to carry outa comprehensive assessment of the chemical sciencesðinformation technology infrastructure portfolio.the information provided by such an assessment will provide federal funding agencies with a sound basis for planning their future investments in bothdisciplinary and crossdisciplinary research.the following are among the actions that need to be taken:¥identify criteria and appropriate indicators for setting priorities for infrastructure investments that promote healthy science and facilitate the rapid movement of concepts into wellengineered technological applications.¥address the issue of standardization and accuracy of codes and databases,including the possibility of a specific structure or mechanism (e.g., within a federal laboratory) to provide responsibility for standards evaluation.2cooperative stewardship: managing the nationõs multidisciplinary user facilities for researchwith synchrotron radiation, neutrons, and high magnetic fields, national research council, national academy press, washington, d.c., 1999.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.infrastructure: capabilities and goals59¥develop a strategy for involving the user community in testing and adopting new tools, integration, and standards development. federal investment in itarchitecture, standards, and applications are expected to scale with growth of theuser base, but the user market is deeply segregated, and there may not yet be adefined user base for any specific investment.¥determine how to optimize incentives within peerreviewed grant programs for creation of high quality crossdisciplinary software.during the next 10 years, chemical science and engineering willbe participating in a broad trend in the united states and across theworld: we are moving toward a distributed cyberinfrastructure. thegoal will be to provide a collaborative framework for individual investigators who want to work with each other or with industry onlargerscale projects that would be impossible for individual investigators working alone.larry smarr (appendix d)recommendation. in order to take full advantage of the emerging gridbased it infrastructure, federal agenciesñin cooperation with thechemical sciences and information technology communitiesñshouldconsider establishing several collaborative dataðmodeling environments.by integrating software, interpretation, data, visualization, networking, andcommodity computing, and using web services to ensure universal access,these collaborative environments could impact tremendously the value of itfor the chemical community. they are ideal structures for distributed learning, research, insight, and development on major issues confronting both thechemical community and the larger society.collaborative modelingdata environments should be funded on a multiyearbasis; should be organized to provide integrated, efficient, standardized, stateoftheart software packages, commodity computing and interpretative schemes; andshould provide opensource approaches (where appropriate), while maintainingsecurity and privacy assurance.this report should be seen in the context of the larger initiative, beyond themolecular frontier: challenges for chemistry and chemical engineering,3 aswell as in the six accompanying reports on societal needs (of which this report is3beyond the molecular frontier: challenges for chemistry and chemical engineering, nationalresearch council, the national academies press, washington, d.c., 2003.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.60information and communicationone).4,5,6,7,8 this component on information and communications, examines perhaps the most dynamically growing capability in the chemical sciences. the findings reported in the executive summary and in greater depth in the body of thetext constitute what the committee believes to be viable and important guidanceto help the chemical sciences community to take full advantage of growing itcapabilities for the advancement of the chemical sciences and technologyñandthereby for the betterment of our society and our world.4challenges for the chemical sciences in the 21st century: national security & homeland defense, national research council, the national academies press, washington, d.c., 2002.5challenges for the chemical sciences in the 21st century: materials science and technology,national research council, the national academies press, washington, d.c., 2003.6challenges for the chemical sciences in the 21st century: energy and transportation, nationalresearch council, the national academies press, washington, d.c., 2003 (in preparation).7challenges for the chemical sciences in the 21st century: the environment, national researchcouncil, the national academies press, washington, d.c., 2003.8challenges for the chemical sciences in the 21st century: health and medicine, national research council, the national academies press, washington, d.c., 2003 (in preparation).information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.appendixesinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.63 astatement of taskthe workshop on information and communications is one of six workshopsheld as part of òchallenges for the chemical sciences in the 21st century.ó theworkshop topics reflect areas of societal needñmaterials and manufacturing,energy and transportation, national security and homeland defense, health andmedicine, information and communications, and environment. the charge foreach workshop was to address the four themes of discovery, interfaces, challenges, and infrastructure as they relate to the workshop topic:¥discoveryñmajor discoveries or advances in the chemical sciences during the last several decades¥interfacesñinterfaces that exist between chemistry/chemical engineeringand such areas as biology, environmental science, materials science, medicine,and physics¥challengesñthe grand challenges that exist in the chemical sciences today¥infrastructureñinfrastructure that will be required to allow the potentialof future advances in the chemical sciences to be realizedinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.bbiographies of the organizingcommittee members64richard c. alkire (cochair) is the charles and dorothy prizer professorof chemical engineering at the university of illinois at urbanachampaign. hehas recently served as vice chancellor for research and dean of the graduate college. his current research is in the area of electrochemical deposition and dissolution of metals, including corrosion, and the strategic use of highperformancecomputing in collaborative problem solving. he received his b.s. from lafayettecollege and ph.d. from university of california at berkeley. he is a member ofthe national academy of engineering.mark a. ratner (cochair) is professor of chemistry, department of chemistry, northwestern university, evanston, illinois. he obtained his b.a. fromharvard university (1964) and his ph.d. (chemistry) from northwestern university (1969). his research interests are in nonlinear optical response properties ofmolecules, electron transfer and molecular electronics, dynamics of polymer electrolyte transport, selfconsistent field models for coupled vibration reaction dynamics, meanfield models for extended systems, and tribology and glassy dynamics. he has some 312 professional publications. ratner is a fellow of theamerican physical society and the american association for the advancementof science and has received numerous teaching awards from northwestern university. he is a member of the national academy of sciences.peter t. cummings is the john r. hall professor of chemical engineeringat vanderbilt university. in addition, he is on the staff of the chemical sciencesdivision at oak ridge national laboratory, where he also holds the position ofdirector of the nanomaterials theory institute of the center for nanophase materials science. he received his bachelorõs degree in mathematics from the university of newcastle (new south wales, australia) and his ph.d. from the univerinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.appendix b65sity of melbourne (victoria, australia) in applied mathematics. his research expertise is in statistical thermodynamics, molecular modeling, computational science, and chemical process design.ignacio e. grossmann (steering committee liason) is rudolph h. and florence dean professor and head of the chemical engineering department atcarnegie mellon university. he received his b.sc. (1974) from universidadiberoamericana, mexico, and his m.sc. (1975) and ph.d. (1977) degrees fromimperial college, london. he joined carnegie mellon in 1979 and has focusedhis research on the synthesis of integrated flow sheets, batch processes, andmixedinteger optimization. the goals of his work are to develop novel mathematical programming models and techniques in process systems engineering.he was elected to the mexican academy of engineering in 1999, and he is amember of the national academy of engineering.judith c. hempel was assistant chair of the department of biomedical engineering at the university of texas, austin, 20022003. previously, she wasassociate director of the molecular design institute at the university of california, san francisco, and was a member of the scientific staff of the computationalchemistry software company biosym/msi and a founding member of the computeraided drug design group at smithkline. she received her b.s., m.a., andph.d. degrees in chemistry at the university of texas, austin. she served as amember of the computer science and telecommunications board from 1996 to2001. her research expertise is in theoretical and computational chemistry.kendall n. houk is professor of chemistry in the department of chemistryand biochemistry at the university of california, los angeles. he obtained hisa.b. in 1964 and his ph.d. in 1968, both from harvard university. his researchinvolves the use of computational methods for the solution of chemical and biological problems, and he continues experimental research as well. he has some560 professional publications. he served on the board on chemical sciences andtechnology from 1991 to 1994. from 1988 to 1991 houk was director of thensf chemistry division. he was elected to the american academy of arts andsciences in 2002.sangtae kim (board on chemical sciences and technology liaison) is vicepresident and information officer of the lilly research laboratories. prior to elililly, he was a vice president and head of r&d information technology for theparkedavis pharmaceutical research division of warnerlambert company. priorto his positions in industry, he was the wisconsin distinguished professor of chemical engineering at the university of wisconsinmadison. he received his b.s. andm.s. degrees from the california institute of technology (1979) and his ph.d.degree in chemical engineering from princeton university (1983). his researchinterests include computational fluid dynamics, computational biology and highperformance computing. he is a member of the national academy of engineering.kenny b. lipkowitz is chairman and professor of chemistry at north dakota state university. prior to assuming this position in 2003, he was professor ofinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.66appendix bchemistry at the purdue university school of science and associate director ofchemical informatics in the indiana university school of informatics. he receivedhis b.s. in chemistry from the state university of new york at geneseo in 1972and his ph.d. in organic chemistry from montana state university in 1975.lipkowitzõs work in informatics focuses on the derivation of threedimensionaldescriptors of molecules for use in quantitative structure activity relationships(qsar), one of several techniques used for computerassisted molecular design.he is associate editor of the journal of chemical information and computerscience, and editor of reviews in computational chemistry.julio m. ottino is currently robert r. mccormick institute professor andwalter p. murphy professor of chemical engineering at northwestern university in evanston, illinois. he was chairman of the department of chemical engineering from 1992 to 2000. ottino received a ph.d. in chemical engineering fromthe university of minnesota and has held research positions at the californiainstitute of technology and stanford university. his research interests are in thearea of complex systems and nonlinear dynamics with applications to fluids andgranular matter. he is a fellow of the american physical society and the american association for the advancement of science. he is a member of the nationalacademy of engineering.john c. tully is arthur t. kemp professor of chemistry, physics, and applied physics in the department of chemistry at yale university. tully is a leading theorist studying the dynamics of gas surface interactions. he develops theoretical and computational methods to address fundamental problems and thenworks with experimentalists to integrate theory with observation. energy exchange and redistribution, adsorption and desorption, and dissociation and recombination are among surface phenomena he has elucidated. he uses mixedquantumclassical dynamics, which allow the extension of conventional molecular dynamics simulation methods to processes involving electronic transitions orquantum atomic motion. he is a member of the national academy of sciences.peter g. wolynes (steering committee liaison) is professor of chemistryand biochemistry at the university of california, san diego. he was previouslyprofessor of chemistry at the university of illinois at urbanachampaign. hereceived his a.b. from indiana university in 1971 and his ph.d. from harvarduniversity in 1976. his research area is physical chemistry with specialized interests in chemical physics of condensed matter, quantum dynamics and reactionkinetics in liquids, dynamics of complex fluids, phase transitions and the glassystate, and biophysical applications of statistical mechanics, especially proteinfolding. he is a member of the national academy of sciences.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.cworkshop agendaagendaworkshop on information and communicationschallenges for the chemical sciences in the 21st centurynational academy of sciences2101 constitution avenue, nwwashington, dcthursday, october 31, 20027:30breakfastsession 1. overview and impact8:00introductory remarks by organizersñbackground of projectdouglas j. raber, national research councilronald breslow, matthew v. tirrell, cochairs, steering committee on challenges for the chemical sciences in the 21st centuryrichard c. alkire, mark a. ratner, cochairs, information &communications workshop committee8:20james r. heath, university of california, los angelesthe current state of the art in nanoscale and molecular informationtechnologies8:50discussion9:10thom h. dunning, jr., oak ridge national laboratory and university of tennesseeinformation & communication technologies and chemical sciencetechnology67information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.68appendix c9:40discussion10:00break10:30juan j. de pablo, university of wisconsin, madisonthe evolution of structure modeling11:00discussion11:20charles h. bennett, ibm researchquantum information11:50discussion12:10lunchsession 2. data and informatics1:30christodoulos a. floudas, princeton universitysystems approaches in bioinformatics and computational genomics2:00discussion2:20anne m. chaka, national institute of standards and technologyhow scientific computing, knowledge management, and databasescan enable advances and new insights in chemical technology2:50discussion3:10breakout session: discoverywhat major discoveries or advances related to information andcommunications have been made in the chemical sciences duringthe last several decades?4:15break4:30reports from breakout sessions and discussion5:30reception6:00banquetspeaker: larry l. smarr, university of california, san diegofriday, november 1, 20027:30breakfastsession 3. simulations and modeling (part 1)8:00dennis j. underwood, bristolmyers squibb companydrug discovery, a game of 20 questions8:30discussion8:50george c. schatz, northwestern universitysimulation in materials science9:20discussioninformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.appendix c699:40breakout session: interfaceswhat are the major computingrelated discoveries and challengesat the interfaces between chemistry/chemical engineering and otherdisciplines, including biology, environmental science, informationscience, materials science, and physics?10:45break11:00reports from breakout sessions and discussions12:00lunchsession 4. simulations and modeling (part 2)1:00ellen b. stechel, ford motor companymodeling and simulation as a design tool1:30discussion1:50linda r. petzold, university of california, santa barbarathe coming age of computational sciences2:20discussion2:40breakout session: challengeswhat are the information and communications grand challenges inthe chemical sciences and engineering3:45break4:00reports from breakout sessions and discussion5:00adjourn for daysaturday, november 2, 20027:30breakfastsession 5. accessibility, standardization, and integration8:00dimitrios maroudas, university of massachusetts, amherstmultiscale modeling8:30discussion8:50richard friesner, columbia universitymodeling of complex chemical systems relevant to biology andmaterials science: problems and prospects9:20discussion9:40breakout session: infrastructurewhat are the two issues at the intersection of computing and thechemical sciences for which there are structural challenges andopportunitiesñin teaching, research, equipment, codes and software, facilities, and personnel?information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.70appendix c10:45break11:00reports from breakout sessions (and discussion)12:00wrapup and closing remarksrichard c. alkire, mark a. ratner, cochairs, information andcommunications workshop committee12:15adjourninformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.71dworkshop presentationsquantum informationcharles h. bennettibm researchthis paper discussed how some fundamental ideas from the dawn of the lastcentury have changed our understanding of the nature of information. the foundations of information processing were really laid in the middle of the twentiethcentury, and only recently have we become aware that they were constructed a bitwrongñthat we should have gone back to the quantum theory of the early twentieth century for a more complete picture.the word calculation comes from the latin word meaning a pebble; we nolonger think of pebbles when we think of computations. information can be separated from any particular physical object and treated as a mathematical entity. wecan then reduce all information to bits, and we can deal with processing these bitsto reveal implicit truths that are present in the information. this notion is bestthought of separately from any particular physical embodiment.in microscopic systems, it is not always possible (and generally it is notpossible) to observe a system without disturbing it. the phenomenon of entanglement also occurs, in which separated bodies could be correlated in a way thatcannot be explained by traditional classical communication. the notion of quantum information can be abstracted, in much the same way as the notions of classical information. there are actually more things that can be done with information, if it is regarded in this quantum fashion.the analogy between quantum and classical information is actually straightinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.72appendix dforwardñclassical information is like information in a book or on a stone tablet,but quantum information is like information in a dream: we try to recall the dreamand describe it; each description resembles less the original dream than the previous description.the first, and so far the most practical, application of quantum informationtheory is quantum cryptography. here one encodes messages and passes them on.if one thinks of photon polarization, one can distinguish vertical and horizontalpolarization through calcite crystals, but diagonal photons are in principle notreliably distinguishable. they should be thought of as a superposition of verticaland horizontal polarization, and they actually propagate containing aspects ofboth of these parent states. this is an entangled structure, and the entangled structure contains more information than either of the pure states of which it is composed.the next step beyond quantum cryptography, the one that made quantuminformation theory a byword, is the discovery of fast quantum algorithms forsolving certain problems. for quantum computing, unlike simple cryptography, itis necessary to consider not only the preparation and measurement of quantumstates, but also the interaction of quantum data along a stream. this is technicallya more difficult issue, but it gives rise to quite exciting basic science involvingquantum computing.the entanglement of different state structures leads to socalled einsteinpodolskyrosen states, which are largely responsible for the unusual propertiesof quantum information.the most remarkable advance in the field, the one that made the field famous, is the fast factor algorithm discovered by shor at bell laboratories. itdemonstrates that exponential speedup can be obtained using a quantum computer to factor large numbers into their prime components. effectively, this quantum factorization algorithm works because it is no more difficult, using a quantum computer, to factor a large number into its prime factors than it is to multiplythe prime factors to produce the large number. it is this condition that renders aquantum computer exponentially better than a classical computer in problems ofthis type.the above considerations deal with information in a disembodied form. ifone actually wants to make a quantum computer, there are all sorts of fabrication,interaction, decoherence, and interference considerations. this is a very rich areaof experimental science, and many different avenues have been attempted.nuclear magnetic resonance, ion traps, molecular vibrational states, and solidstate implementations have all been used in attempts to produce actual quantumcomputers.although an effective experimental example of a quantum computer is stillincomplete, many major theoretical advances have suggested that some of theobvious difficulties can in fact be overcome. several discoveries indicate that theeffects of decoherence can be prevented, in principle, by including quantum errorinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.anne m. chaka73correcting codes, entanglement distillation, and quantum faulttolerant circuits.good quantum computing hardware does not yet exist. the existence of thesemethodologies means that the hardware does not have to be perfectly efficient orperfectly reliable, because these programming techniques can make arbitrarilygood quantum computers possible, even with physical equipment that suffersfrom decoherence issues.although most of the focus has been on quantum cryptography, quantumprocessing provides an important example of the fact that quantum computers notonly can do certain tasks better than ordinary computers, but also can do differenttasks that would not be imagined in the context of ordinary information processing. for example, entanglement can enhance the communication of classical messages, by augmenting the capacity of a quantum channel for sending messages.to summarize, quantum information obeys laws that subtly extend those governing classical information. the way in which these laws are extended is reminiscent of the transition from real to complex numbers. real numbers can be viewed asan interesting subset of complex numbers, and some questions that might be askedabout real numbers can be most easily understood by utilization of the complexplane. similarly, some computations involving real input or real output (by òrealó imean classical) are most rapidly developed using quantum intermediate space inquantum computers. when iõm feeling especially healthy, i say that quantum computers will probably be practical within my lifetimeñstrange phenomena involving quantum information are continually being discovered.how scientific computing knowledge managementand databases can enable advances and new insightsin chemical technologyanne m. chakanational institute of standards and technologythe focus of this paper is on how scientific computing and information technology (it) can enable technical decision making in the chemical industry. thepaper contains a current assessment of scientific computing and it, a vision ofwhere we need to be, and a roadmap of how to get there. the information andperspectives presented here come from a wide variety of sources. a general perspective from the chemical industry is found in the council on chemicalresearchõs vision 2020 technology partnership,1 several workshops sponsored1chemical industry of the future: technology roadmap for computational chemistry, thompson,t. b., ed., council for chemical research, washington, dc, 1999; http://www.ccrhq.org/vision/index/.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.74appendix dby nsf, nist,2 and doe, and the wtec report on industrial applications ofmolecular and materials modeling (which contains detailed reports on 91 institutions, including over 75 chemical companies, plus additional data from 55 u.s.chemical companies and 256 worldwide institutions (industry, academia, andgovernment).3 my own industrial perspective comes from my position as coleader for the lubrizol r&d it vision team for two years, and ten years as thehead of computational chemistry and physics prior to coming to nist. it shouldbe noted that although this paper focuses primarily on the chemical industry,many of the same issues apply to the biotechnology and materials industry.there are many factors driving the need for scientific computing and knowledge management in the chemical industry. global competition is forcing u.s.industry to reduce r&d costs and the time to develop new products in the chemical and materials sectors. discovery and process optimization are currently limited by a lack of property data and insight into mechanisms that determine performance. thirty years ago there was a shortage of chemicals, and customers wouldpay premium prices for any chemical that worked at all. trial and error was usedwith success to develop new chemistry. today, however, the trend has shifteddue to increased competition from an abundance of chemicals that work on themarket, customer consolidation, and global competition that is driving commodity pricing even for highperformance and fine chemicals. trial and error havebecome too costly, and the probability of success is too low. hence it is becomingwidely recognized that companies need to develop and finetune chemicals andformulations by design in order to remain competitive, and to screen chemicalsprior to a long and costly synthesis and testing process. in addition, the chemicalsproduced today must be manufactured in a way that minimizes pollution andenergy costs. higher throughput is being achieved by shifting from batch processing to continuous feed stream, but this practice necessitates a greater understanding of the reaction kinetics, and hence the mechanism, to optimize feedstream rates. simulation models are needed that have sufficient accuracy to beable to predict what upstream change in process variables are required to maintain the downstream products within specifications, as it may take several hoursfor upstream changes to affect downstream quality. lastly, corporate downsizingis driving the need to capture technical knowledge in a form that can be queriedand augmented in the future.2nist workshop on predicting thermophysical properties of industrial fluids by molecular simulations (june, 2001), gaithersburg, md; 1st international conference on foundations of molecularmodeling and simulation 2000: applications for industry (july, 2000), keystone, co; workshop onpolarizability in force fields for biological simulations (december 1315, 2001), snowbird, ut.3applying molecular and materials modeling, westmoreland, p. r.; kollman, p. a.; chaka, a.m.; cummings, p. t.; morokuma, k.; neurock, m.; stechel, e.b .; vashishta, p., eds. kluwer academic publishers, dordrecht, 2002; http://www.wtec.org/.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.anne m. chaka75data and property information are most likely to be available on commoditymaterials, but industrial competition requires fast and flexible means to obtaindata on novel materials, mixtures, and formulations under a wide range of conditions. for the vast majority of applications, particularly those involving mixturesand complex systems (such as drugprotein interactions or polymernanocomposites), evaluated property data simply do not exist and are difficult,time consuming, or expensive to obtain. for example, measuring the density of apure liquid to 0.01% accuracy requires a dual sinker apparatus costing $500,000,and approximately $10,000 per sample. commercial laboratory rates for measuring vaporliquid equilibria for two state points of a binary mixture are on theorder of $30,000 to 40,000. hence industry is looking for a way to supply massive amounts of data with reliable uncertainty limits on demand. predictive modeling and simulation have the potential to help meet this demand.scientific computing and information technology, however, have the potential to offer so much more than simply calculating properties or storing data. theyare essential to the organization and transformation of data into wisdom that enables better technical decision making. the information management pyramidcan be assigned four levels defined as follows:1.data: a disorganized, isolated set of facts2.information: organized data that leads to insights regarding relationshipsñknowing what works3.knowledge: knowing why something works4.wisdom: having sufficient understanding of factors governing performance to reliably predict what will happenñknowing what will workto illustrate how scientific computing and knowledge management convertdata and information into knowledge and wisdom, a real example is taken fromlubricant chemistry. polysulfides, rsnr, are added to lubricants to prevent wearof ferrous metal components under high pressure. the length of the polysulfidechain, n, is typically between 2 and 6. a significant performance problem is thatsome polysulfide formulations also cause corrosion of coppercontaining components such as bronze or brass. to address this problem, a researcher assemblesdata from a wide variety of sources such as analytical results regarding composition, corrosion and antiwear performance tests, and field testing. it makes it easyfor the right facts to be gathered, visualized, and interpreted. after analysis of thedata, the researcher comes to the realization that longchain polysulfides (n = 4 orgreater) corrode copper, but shorter chains (n = 2 to 3) do not. this is knowingwhat happens. scientific computing and modeling can then be used to determinewhy something happens. in this case, quantum mechanics enabled us to understand that the difference in reactivity of these sulfur chains could be explained bysignificant stabilization of the thiyl radical delocalized over two adjacent sulfuratoms after homolytic cleavage of the ss bond: rssssr 2rss¥. theinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.76appendix dmonosulfur thiyl radical rs¥ was significantly higher in energy and therefore ismuch less likely to form. hence copper corrosion is consistent with the formationof stable thiyl radicals. this insight led to a generalization that almost any sulfurradical with a low energy of formation will likely corrode copper, and we wereable to reliably predict copper corrosion performance from the chemical structureof a sulfurcontaining species prior to testing. this understanding also led to improvements in the manufacturing process and other applications of sulfur chemistry, and is an example of what is meant by wisdom (i.e., reliably predicting whatwill happen in novel applications due to a fundamental understanding of the underlying chemistry and physics).what is the current status of scientific computing and knowledge management with respect to enabling better technical decisions? for the near term, databases, knowledge management, and scientific computing are currently most effective when they enable human insight. we are a long way from hitting a carriagereturn and obtaining answers to tough problems automatically, if ever. wetware(i.e., human insight) is currently the best link between the levels of data, information, knowledge, and wisdom. there is no substitute for critical, scientific thinking. we can, however, currently expect an idea to be tested via experiment orcalculation. first principles calculations, if feasible on the system, improve therobustness of the predictions and can provide a link between legacy data andnovel chemistry applications. computational and it methods can be used to generate a set of possibilities combinatorially, analyze the results for trends, andvisualize the data in a manner that enables scientific insight. developing thesesystems is resource intensive and very application specific. companies will invest in their development for only the highest priority applications, and the knowledge gained will be proprietary. access to data is critical for academics in theqsprqsar method development community, but is problematic due to intellectual property issues in the commercial sector.4 hence there is a need to advance the science and the it systems in the public arena to develop the fundamental foundation and building blocks upon which public and proprietaryinstitutions can develop their own knowledge management and predictive modeling systems.what is the current status of chemical and physical property data? publishedevaluated chemical and physical property data double every 10 years, yet this iswoefully inadequate to keep up with demand. obtaining these data requires meticulous experimental measurements and/or thorough evaluations of related datafrom multiple sources. in addition, data acquisition processes are time andresourceconsuming and therefore must be initiated well in advance of an anticipated need within an industrial or scientific application. unfortunately a significant part of the existing data infrastructure is not directly used in any meaningful4comment by professor curt breneman, rensselaer polytechnic institute.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.anne m. chaka77application because data requirements often shift between the initiation andcompletion of a data project. analysis and fitting, such as for equationofstatemodels, must be reinitiated when significant new data become available.one vision that has been developed in consultation with the chemical andmaterials industries can be described as a òuniversal data and simulation engine.ó this engine is a framework of computational tools, evaluated experimentaldata, active databases, and knowledgebased software guides for generatingchemical and physical property data on demand with quantitative measures ofuncertainty. this engine provides validated, predictive simulation methods forcomplex systems with seamless multiscale and multidisciplinary integration topredict properties and model physical phenomena and processes. the results arethen visualized in a form useful for scientific interpretation, sometimes by a nonexpert. examples of highpriority challenges cited by industry in the wtec report to be ultimately addressed by the universal data and simulation engine arediscussed below.5how do we achieve this vision of a universal data and simulation engine?toward this end, nist has been exploring the concepts of dynamic data evaluationand virtual measurements of chemical and physical properties and predictive simulations of physical phenomena and processes. in dynamic data evaluation, all availableexperimental data within a technical area are collected routinely and continuously,and evaluations are conducted dynamicallyñusing an automated systemñwhen information is required. the value of data is directly related to the uncertainty, so òrecommendedó data must include a robust uncertainty estimate. metadata are also collected (i.e., auxiliary information required to interpret the data such as experimentalmethod). achieving this requires interoperability and data exchange standards. ideally the dynamic data evaluation is supplemented by calculated data based on validated predictive methods (virtual measurements), and coupled with a carefully considered experimental program to generate benchmark data.both virtual measurements and the simulation engine have the potential tomeet a growing fraction of this need by supplementing experiment and providingdata in a timely manner at lower cost. here we define òvirtual measurementsóspecifically as predictive modeling tools that yield property data with quantifieduncertainties analogous to observable quantities measured by experiment (e.g.,rate constants, solubility, density, and vaporliquid equilibria). by òsimulationówe mean validated modeling of processes or phenomena that provides insight5these include liquidliquid interfaces (micelles and emulsions), liquidsolid interfaces (corrosion,bonding, surface wetting, transfer of electrons and atoms from one phase to another), chemical andphysical vapor deposition (semiconductor industry, coatings), and influence of chemistry on thethermomechanical properties of materials, particularly defect dislocation in metal alloys; complexreactions in multiple phases over multiple time scales. solution properties of complex solvents andmixtures (suspending asphaltenes or soot in oil, polyelectrolytes, free energy of solvation rheology),composites (nonlinear mechanics, fracture mechanics), metal alloys, and ceramics.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.78appendix dinto mechanisms of action and performance with atomic resolution that is notdirectly accessible by experiment but is essential to guide technical decision making in product design and problem solving. this is particularly crucial for condensedphase processes where laboratory measurements are often the average ofmyriad atomistic processes and local conditions that cannot be individually resolved and analyzed by experimental techniques. it is analogous to gasphasekinetics in the 1920s prior to modern spectroscopy when total pressure was theonly measurement possible. the foundation for virtual measurements and simulations is experimental data and mathematical models that capture the underlyingphysics at the required accuracy of a given application. validation of theoreticalmethods is vitally important.the council for chemical researchõs vision 2020 states that the desiredtarget characteristics for a virtual measurement system for chemical and physicalproperties are as follows: problem setup requires less than two hours, completiontime is less than two days, cost including labor is less than $1,000 per simulation,and it is usable by a nonspecialist (i.e., someone who cannot make a fulltimecareer out of molecular simulation). unfortunately, we are a long way from meeting this target, particularly in the area of molecular simulations. quantum chemistry methods have achieved the greatest level of robustness andñcoupled withadvances in computational speedñhave enabled widespread success in areas suchas predicting gasphase, smallmolecule thermochemistry and providing insightinto reaction mechanisms. current challenges for quantum chemistry are accuratepredictions of rate constants and reaction barriers, condensedphase thermochemistry and kinetics, van der waals forces, searching a complex reaction space,transition metal and inorganic systems, and performance of alloys and materialsdependent upon the chemical composition.a measure of the current value of quantum mechanics to the scientific community is found in the usage of the nist computational chemistry comparisonand benchmark database (cccbdb), (http://srdata.nist.gov/cccbdb). thecccbdb was established in 1997 as a result of an (american chemical society)acs workshop to answer the question, how good is that ab initio calculation?the purpose is to expand the applicability of computational thermochemistry byproviding benchmark data for evaluating theoretical methods and assigning uncertainties to computational predictions. the database contains over 65,000 calculations on 615 chemical species for which there are evaluated thermochemicaldata. in addition to thermochemistry, the database also contains results on structure, dipole moments, polarizability, transition states, barriers to internal rotation,atomic charges, etc. tutorials are provided to aid the user in interpreting data andevaluating methodologies. since the cccbdbõs inception, usage has doubledevery year up to the current sustained average of 18,000 web pages served permonth, with a peak of over 50,000 pages per month. last year over 10,000 separate sites accessed the cccbdb. there are over 400 requests per month for newchemical species not contained in the database.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.anne m. chaka79the cccbdb is currently the only computational chemistry or physics database of its kind. this is due to the maturity of quantum mechanics to reliablypredict gasphase thermochemistry for small (20 nonhydrogen atoms or less),primarily organic, molecules, plus the availability of standardreferencequalityexperimental data. for gasphase kinetics, however, only in the past two yearshave highquality (<2% precision) rateconstant data become available for h¥and ¥oh transfer reactions to begin quantifying uncertainty for the quantum mechanical calculation of reaction barriers and tunneling.6 there is a critical needfor comparable quality rate data and theoretical validation for a broader class ofgasphase reactions, as well as solution phase for chemical processing and lifescience, and surface chemistry.one of the highest priority challenges for scientific computing for the chemical industry is the reliable prediction of fluid properties such as density, vaporliquid equilibria, critical points, viscosity, and solubility for process design.empirical models used in industry have been very useful for interpolating experimental data within very narrow ranges of conditions, but they cannot be extendedto new systems or to conditions for which they were not developed. models basedexclusively on first principles are flexible and extensible, but can be applied onlyto very small systems and must be òcoarsegrainedó (approximated by averagingover larger regions) for the time and length scales required in industrial applications. making the connection between quantum calculations of binary interactions or small clusters and properties of bulk systems (particularly systems thatexhibit highentropy or longrange correlated behavior) requires significant breakthroughs and expertise from multiple disciplines. the outcome of the first industrial fluid properties simulation challenge7 (sponsored by aicheõs computational molecular science and engineering forum and administered by nist)underscored these difficulties and illustrated how fragmented current approachesare. in industry, there have been several successes in applying molecular simulations, particularly in understanding polymer properties, and certain direct phaseequilibrium calculations. predicting fluid properties via molecular simulation,however, remains an art form rather than a tool. for example, there are currentlyover a dozen popular models for water, but models that are parameterized to givegood structure for the liquid phase give poor results for ice. others, parameter6louis, f.; gonzalez, c.; huie, r. e.; kurylo, m. j. j. phys. chem. a 2001, 105, 15991604.7the goals of this challenge were to: (a) obtain an indepth and objective assessment of our currentabilities and inabilities to predict thermophysical properties of industrially challenging fluids usingcomputer simulations, and (b) drive development of molecular simulation methodology toward acloser alignment with the needs of the chemical industry. the challenge was administered by nistand sponsored by the computational molecular science and engineering forum (aiche). industryawarded cash prizes to the champions to each of the three problems (vaporliquid equilibrium, density, and viscosity). results were announced at the aiche annual meeting in indianapolis, in, november 3, 2002.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.80appendix dized for solvation and biological applications, fail when applied to vaporliquidequilibrium properties for process engineering. the lack of òtransferabilityó ofwater models indicates that the underlying physics of the intermolecular interactions is not adequately incorporated. the tools and systematic protocols to customize and validate potentials for given properties with specified accuracy anduncertainty do not currently exist and need to be developed.in conclusion, we are still at the early stages of taking advantage of the fullpotential offered by scientific computing and information technology to benefitboth academic science and industry. a significant investment is required to advance the science and associated computational algorithms and technology. theimpact and value of improving chemicalbased insight and decision making arehigh, however, because chemistry is at the foundation of a broad spectrum oftechnology and biological processes such as¥how a drug binds to an enzyme,¥manufacture of semiconductors,¥chemical reactions occurring inside a plastic that makes it burn faster thanothers, and¥how defects migrate under stress in a steel ibeam.a virtual measurement system can serve as a framework for coordinatingand catalyzing academic and government laboratory science in a form useful forsolving technical problems and obtaining properties. there are many barriers toobtaining the required datasets that must be overcome, however. corporate dataare largely proprietary, and in academia, generating data is òperceived as dull soit doesnõt get funded.ó8 according to dr. carol handwerker (chief, metallurgydivision, materials science and engineering laboratory, nist), even at nistjust gathering datasets in general is not well supported at the moment, because itis difficult for nist to track the impact that the work will have on the people whoare using the datasets. one possible way to overcome this barrier may be to develop a series of standard test problems in important application areas where thevalue can be clearly seen. the experimental datasets would be collected and theoretical and scientific computing algorithms would be developed, integrated, andfocused in a sustained manner to move all the way through the test problems. thedata collection and scientific theory and algorithm development then clearly become means to an end.8comment by professor john tully, yale university.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.juan j. de pablo81on the structure of molecular modeling: montecarlo methods and multiscale modelingjuan j. de pablouniversity of wisconsin, madisonthe theoretical and computational modeling of fluids and materials can bebroadly classified into three categories, namely atomistic or molecular, mesoscopic, and continuum or macroscopic.1 at the atomistic or molecular level, detailed models of the system are employed in molecular simulations to predict thestructural, thermodynamic, and dynamic behavior of a system. the range of application of these methods is on the order of angstroms to nanometers. examples of this type of work are the prediction of reaction pathways using electronic structure methods, the study of protein structure using molecular dynamicsor monte carlo techniques, or the study of phase transitions in liquids and solidsfrom knowledge of intermolecular forces.2 at the mesoscopic level, coarsegrained models and mean field treatments are used to predict structure and properties at length scales ranging from tens of nanometers to microns. examples ofthis type of research are the calculation of morphology in selfassembling systems (e.g., block copolymers and surfactants) and the study of macromolecularconfiguration (e.g., dna) in microfluidic devices.3,4,5 at the continuum or macroscopic level, one is interested in predicting the behavior of fluids and materialson laboratory scales (microns to centimeters), and this is usually achieved bynumerical solution of the relevant conservation equations (e.g., navierstokes, inthe case of fluids).6over the last decade considerable progress has been achieved in the threecategories described above. it is now possible to think about òmultiscale modelingó approaches, in which distinct methods appropriate for different length scalesare combined or applied simultaneously to achieve a comprehensive descriptionof a system. this progress has been partly due to the everincreasing power ofcomputers but, to a large extent, it has been the result of important theoreticaland algorithmic developments in the area of computational materials and fluidsmodeling.much of the interest in multiscale modeling methods is based on the premisethat, one day, the behavior of entirely new materials or complex fluids will be1de pablo j. j.; escobedo, f. a. aiche journal 2002, 48, 27162721.2greeley j.; norskov, j. k.; mavrikakis, m. ann. rev. phys. chem. 2002, 53, 319348.3fredrickson g. h., ganesan, v.; drolet, f. macromol. 2002, 35, 1639.4hur j. s.; shaqfeh, e. s. g.; larson, r. a. j. rheol. 2000, 44, 713742.5jendrejack r. m.; de pablo, j. j.; graham, m. d. j. chem. phys. 2002, 116, 77527759.6bird, r. b.; stewart, w. e.; lightfoot, e. n. transport phenomena: 2nd ed., john wiley: newyork, ny; 2002.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.82appendix dconceived or understood from knowledge of their atomic or molecular constituents. the goal of this brief document is to summarize how molecular structureand thermodynamic properties can be simulated numerically, to establish thepromise of modern molecular simulation methods, including the opportunitiesthey offer and the challenges they face, and to discuss how the resulting molecularlevel information can be used in conjunction with mesoscopic and continuummodeling techniques for study of macroscopic systems.as a first concrete example, it is instructive to consider the simulation of aphase diagram (a simple liquidvapor coexistence curve) for a simple fluid (e.g.,argon) from knowledge of the interactions between molecules; in the early 1990s,the calculation of a few points on such a diagram required several weeks ofsupercomputer time.7 with more powerful techniques and faster computers, it isnow possible to generate entire phase diagrams for relatively complex, industrially relevant models of fluids (such as mixtures of large hydrocarbons) in relatively short times (on the order of hours or days).8a molecular simulation consists of the model or force field that is adopted torepresent a system and the simulation technique that is employed to extract quantitative information (e.g., thermodynamic properties) about that model. for molecular simulations of the structure and thermodynamic properties of complexfluids and materials, particularly those consisting of large, articulated molecules(e.g., surfactants, polymers, proteins), monte carlo methods offer attractive features that make them particularly effective. in the context of molecular simulations, a monte carlo algorithm can be viewed as a process in which random stepsor displacements (also known as òtrial movesó) away from an arbitrary initialstate of the system of interest are carried out to generate large ensembles of realistic, representative configurations. there are two essential ingredients to anymonte carlo algorithm: the first consists of the types of òmovesó or steps that areused, and the second consists of the formalism or criteria that are used to guide analgorithm toward thermodynamic equilibrium.the possibility of designing unphysical òtrial moves,ó in which moleculescan be temporarily destroyed and reassembled, often permits efficient exploration of the configuration space available to a system. in the case of long hydrocarbons, surfactants, or phospholipids, for example, configurationalbias techniques9,10 can accelerate the convergence of a simulation algorithm by severalorders of magnitude. in the case of polymers, rebridging techniques permit simulation of structural properties that would not be accessible by any other means.117panagiotopoulos, a. z. mol.sim. 1992, 9, 123.8nath s.; escobedo, f. a.; patramai, i.; de pablo, j. j. ind. eng. chem. res. 1998, 37, 3195.9de pablo, j. j.; yan, q.; escobedo, f. a. ann. rev. phys. chem. 1999, 50, 377411.10frenkel d.; smit, b. understanding molecular simulation, 2nd ed., elsevier science: london,uk, 2002.11karayiannis n. c.; mavrantzas, v. g.; theodorou, d. n. phys., rev. lett. 2002, 88 (10), 105503.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.juan j. de pablo83the development of novel, clever monte carlo trial moves for specific systems isa fertile area of research; significant advances in our ability to model fluids andmaterials will result from such efforts.a monte carlo simulation can be implemented in a wide variety of ways. inthe most common implementation, trial moves are accepted according to probability criteria (the socalled metropolis criteria) constructed in such a way as toresult in an ensemble of configurations that satisfies the laws of physics. there is,however, considerable flexibility in the way in which such criteria are implemented. in expanded ensemble methods, for example, fictitious intermediatestates of a system are created in order to facilitate transitions between an initialand a final state; transitions between states are accepted or rejected according towelldefined probability criteria. in paralleltempering or replicaexchange simulation methods, calculations are conducted simultaneously on multiple replicas ofa system. each of these replicas can be studied at different conditions (e.g., different temperature); configuration exchanges between distinct replicas can be proposed and accepted according to probability criteria that guarantee that correctensembles are generated for each replica. within this formalism, a system orreplica that is sluggish and difficult to study at low temperatures (e.g., a highlyviscous liquid or a glassy solid), can benefit from information generated in hightemperature simulations, where relaxation and convergence to equilibrium aremuch more effective.12 more recently, densityofstates techniques have been proposed as a possible means to generate all thermodynamic information about asystem over a broad range of conditions from a single simulation.13,14,15 theseexamples serve to illustrate that if recent history is an indication of progress tocome, new developments in the area of monte carlo methods will continue toincrease our ability to apply first principles information to thermodynamic property and structure prediction, thereby supplementing and sometimes even replacing more costly experimental work. such new developments will also facilitateconsiderably our ability to design novel and advanced materials and fluids fromatomistic and molecularlevel information.as alluded to earlier, the results and predictions of a molecular simulationare only as good as the underlying model (or force field) that is adopted to represent a system. transferable force fields are particularly attractive because, in thespirit of ògroupcontributionó approaches, they permit study of molecules andmanybody systems of arbitrary complexity through assembly of atomic or chemicalgroup building blocks. most force fields use electronic structure methods togenerate an energy surface that is subsequently fitted using simple functional12yan, q.; de pablo, j. j.; j. chem. phys. 1999. 111, 9509.13wang, f. g.; landau, d. p.; phys. rev. lett. 2001, 86 (10), 20502053.14yan, q. l.; faller, r.; de pablo, j. j. j. chem. phys. 2002, 116, 87458749.15yan, q. l.; de pablo, j. phys. rev. lett. 2003, 90 (3), 035701.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.84appendix dforms. the resulting functions and their corresponding parameters are referred toas a force field. it is important to note that òrawó parameters extracted from electronic structure calculations are generally unable to provide a quantitative description of the structure and properties of complex fluids and materials; theirsubsequent optimization by analysis of experimental data considerably improvestheir applicability and predictive capability. improvements in the efficiency ofsimulation techniques have rendered this last aspect of force field developmentmuch more tractable than it was a decade ago. reliable force fields are now beingproposed for a wide variety of systems, including hydrocarbons, carbohydrates,alcohols, polymers, etc.6,16,17,18 accurate force fields are the cornerstone of fluids and materials modeling; much more work in this area is required to reach astage at which modeling tools can be used with confidence to interpret the resultsof experiments and to anticipate the behavior of novel materials.the above discussion has been focused on monte carlo methods. such methods can be highly effective for determining the equilibrium structure and properties of fluids and materials, but they do not provide information about timedependent processes. molecular dynamics (md) methods, which are the techniqueof choice for study of dynamic processes, have also made considerable progressover the last decade. the development of multipletimestep methods, for example, has increased significantly the computational efficiency of md simulations. unfortunately, however, the time scales amenable to molecular dynamicssimulations are still on the order of tens or hundreds of nanoseconds. many of theprocesses of interest in chemistry and chemical engineering occur on much longertime scales (e.g., minutes or hours); it is unlikely that the several orders of magnitude that now separate our needs from what is possible with atomisticlevelmethods will be bridged by the availability of faster computers. it is thereforenecessary to develop theoretical and computational methods to establish a systematic connection between atomistic and macroscopic time scales. these techniques are often referred to as multiscale methods or coarsegraining methods.while multiscale modeling is still in its infancy, its promise is such thatconsiderable efforts should be devoted to its development in the years to come. afew examples have started to appear in the literature. in the case of solid materials, the challenge of coupling atomistic phenomena (e.g., the tip of a crack) withmechanical behavior (e.g., crack propagation and failure) over macroscopic domains has been addressed by several authors.19,20 in the case of fluids, molecularlevel structure (e.g., the conformation of dna molecules in solution) has been16jorgensen, w. l.; maxwell, d. s.; tiradorives, j. j. amer. chem. soc. 1996, 118, 1122511236.17errington, j. r.; panagiotopoulos, a. z. j. phys. chem b 1999, 103, 63146322.18wick, c. d. martin, m. g.; siepmann, j. i. j. phys. chem. b 2000, 104, 80088016.19broughton, j. q. abraham, f. f.; n. bernstein; phys. rev. b 1999, 60, 23912403.20smith, g. s., e. b. tadmor, n. bernstein; kaxiras, e. acta mater. 2001, 49, 40894101.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.juan j. de pablo85solved concurrently with macroscopic flow problems (fluid flow through macroand microfluidic geometries).21,22 multiscale methods for the study of dynamicprocesses currently rely on separation of time scales for various processes. one ofthe cornerstones of these methods is the averaging or coarsegraining of fast,local processes into a few, wellchosen variables carrying sufficient informationcontent to provide a meaningful description of a system at longer time and lengthscales. the reverse is also true, and perhaps more challenging; information froma coarsegrained level must be brought back onto a microscopic level in a sensible manner, without introducing spurious behavior. this latter òreverseó problem is often underspecified and represents a considerable challenge. new andbetter numerical schemes to transfer information between different descriptionlevels should be developed; this must be done without adding systematic perturbations to the system and in a computationally robust and efficient way. a betterunderstanding of coarse graining techniques is sorely needed, as are better algorithms to merge different levels of description in a seamless and correct manner.21jendrejack, rm, de pablo, j. j.; graham, m. d. j. nonnewton fluid 2002, 108, 123142.22jendrejack, r. m.; graham. m. d.; de pablo, j. j. multiscale simulation of dna solutions inmicrofluidic devices, unpublished.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.86appendix dadvances in information & communicationtechnologies: opportunities and challenges inchemical science and technologythom h. dunning, jr.university of tennessee and oak ridge national laboratoryintroductionthe topics to be covered in this paper are the opportunities in chemical science and technology that are arising from the advances being made in computingtechnologiesñcomputers, data stores, and networks. that computing technologycontinues to advance at a dizzying pace is familiar to all of us. almost as soon asone buys a pc, it becomes outdated because a faster version of the microprocessor that powers that pc becomes available. likewise, tremendous advances arebeing made in memory and data storage capacities. the density of memory isincreasing at the same rate as computing speed, and disk drive densities are increasing at an even faster pace. these advances have already led to a new era incomputational chemistryñcomputational models of molecules and molecularprocesses are now so widely accepted and pcs and workstations so reasonablypriced that molecular calculations are routinely used by many experimental chemists to better understand the results of their experiments. for someone like theauthor, who started graduate school in 1965, this transformation of the role ofcomputing in chemistry is little short of miraculous.dramatic increases are also occurring in network bandwidth. bandwidth thatwas available only yesterday to connect computers in a computer room is becoming available today between distant research and educational institutionsñthenational light rail is intended to link the nationõs most researchintensive universities at 10 gigabits per second. this connectivity has significant implicationsfor experimental and computational chemistry. collaborations will grow as geographically remote collaborators are able to easily share data and whiteboards,simultaneously view the output of calculations or experiments, and converse withone another face to face through audiovideo links. data repositories provided byinstitutions such as the national institute of standards and technology, althoughnot nearly as large in chemistry as in molecular biology, are nonetheless important and will become as accessible as the data on a local hard drive. finally,network advances promise to make the remote use of instruments, especially expensive oneofakind or firstofakind instruments, routine, enabling scientificadvances across all of chemical science.at the current pace of change, an orderofmagnitude increase in computingand communications capability will occur every five years. an orderofmagnitude increase in the performance of any technology is considered to be the threshold for revolutionary changes in usage patterns. it is important to keep this ininformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.thom h. dunning, jr.87mind. in fact, professor jack dongarra of the university of tennessee, one of theworldõs foremost experts in scientific computing, has recently stated this fact instark terms:the rising tide of change [resulting from advances in information technology]shows no respect for the established order. those who are unwilling or unable toadapt in response to this profound movement not only lose access to the opportunities that the information technology revolution is creating, they risk beingrendered obsolete by smarter, more agile, or more daring competitors.never before in the history of technology have revolutionary advances occurredat the pace being seen in computing. it is critical that chemical science and technology in the united states aggressively pursue the opportunities offered by theadvances in information and communications technologies. only by doing sowill it be able to maintain its position as a world leader.it is impossible to cover all of the opportunities (and challenges) in chemicalscience and technology that will result from the advances being made in information and communications technologies in a 30minute presentation. instead, i focus on three examples that cover the breadth of opportunities that are becomingavailable: (i) advances in computational modeling that are being driven by advances in computer and disk storage technology, (ii) management of large datasetsthat is being enabled by advances in storage and networking technologies, and(iii) remote use of oneofakind or firstofakind scientific instruments resultingfrom advances in networking and computer technologies. in this paper, i focus onapplications of highend computing and communications technologies. however,it is important to understand that the òtrickledown effectó is very real in computing (if not in economics)ñthe highend applications discussed today will be applicable to the personal computers, workstations, and departmental servers oftomorrow.computational modeling in chemical science and technologyfirst, consider the advances in computational modeling wrought by the advances in computer technology. almost everybody is aware of mooreõs law, thefact that every 1824 months there is a factor of 2 increase in the speed of themicroprocessors that are used to drive personal computers as well as many oftodayõs supercomputers. within the u.s. department of energy, the increase incomputing power available for scientific and engineering calculations is codifiedin the òasci curve,ó which is a result of mooreõs law compounded by the use ofincreased number of processors (figure 1). the result is computing power that iswell beyond that predicted by mooreõs law alone. if there is a factor of 2 increaseevery 1824 months from computer technology and the algorithms used in scientific and engineering calculations scale to twice as many processors over thatsame period of time, the net result is a factor of 4 increase in computing capabilinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.88appendix dity. at this pace, one can realize an orderof magnitudeincrease in computingpower in just five years.the increase in computing power in the past five years, in fact, follows thispattern. in 1997, òasci redó at sandia national laboratories was the first computer system capable of performing more than 1 trillion arithmetic operations persecond (1 teraflops). asci red had a total of 9216 pentium pro processors, a peakperformance of 1.8 teraflops, and 0.6 terabytes of memory. in 2002, the japaneseearth simulator is the worldõs most powerful computer with 5120 processors, apeak performance of 40.96 teraflops, and more than 10 terabytes of memory (figure 2). this is an increase of a factor of over 20 in peak performance in just fiveyears! but, the increase in delivered computing power is even greater. on ascired a typical scientific calculation achieved 1020% of peak performance, or 100200 gigaflops. on the japanese earth simulator, it is possible to obtain 4050% ofpeak performance, or 16 to 20 teraflops. thus, in terms of delivered performance,the increase from 1997 to 2002 is more like a factor of 100.the earth simulator is the first supercomputer in a decade that was designedfor science and engineering computations. most of the supercomputers in usetoday were designed for commercial applications, and commercial applicationsplace very different demands on a computerõs processor, memory, i/o, and interconnect subsystems than science and engineering applications. the impact of thisis clearly evident in the performance of the earth simulator. for example, theperformance of the earth simulator on the linpack benchmark1 is 35.86 teraflops0.11101001,00019982000200220041996microprocessors2x increase in performance every 1824 months(moores law)parallelismmore processors per smp more smpsinnovative designsspecialized computerscellular architecturesprocessorsinmemoryhtmtcomputing power (tflops)mooreõs lawasci curvefigure 1mooreõs law and beyond.1see: http://www.top500.org/list/2002/11/.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.thom h. dunning, jr.89or 87.5% of peak performance (figure 3). the earth simulator is a factor of 5faster on the linpack benchmark than its nearest competitor, asci white, eventhough the difference in peak performance is only a factor of three. the performance of the earth simulator on the linpack benchmark exceeds the integratedtotal of the three largest machines in the united states (asci white, lemieux atthe pittsburgh supercomputing center, and nersc3 at lawrence berkeley national laboratory) by a factor of nearly 2.5.&&'figure 2japanese earth simulator.0.010,000.020,000.030,000.040,000.0earthsimulatorasciwhitepsclemieuxnersc3rmax(gflops)figure 3linpack benchmarks.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.90appendix dthe performance of the earth simulator is equally impressive when real scientific and engineering applications are considered. a general atmospheric global circulation benchmark ran at over 26 teraflops, or 65% of peak performance,on the earth simulator.2 on the commercially oriented machines that are currently being used in the united states, it has proven difficult to achieve more than10% of peak performance for such an application.3 so, not only is the raw speedof the japanese earth simulator very high, it is also very effective for scientificand engineering applications. the earth simulator is expected to run a wide rangeof scientific and engineering applications 10100 times faster than the fastestmachines available in the united states. as professor dongarra noted, drawing acomparison with the launching of sputnik by the soviet union in 1958, the earthsimulator is the òcomputernikó of 2002, representing a wakeup call illustratinghow computational science has been compromised by supercomputers built forcommercial applications.the above discussion has focused on general computer systems built usingcommodity components. computer companies now have the capability to designand build specialized computers for certain types of calculations (e.g., moleculardynamics) that are far more costeffective than traditional supercomputers. examples of these computers include mdgrape4 for molecular dynamics calculations and qcdoc5 for lattice gauge qcd calculations. priceperformance improvements of orders of magnitude can be realized with these specializedcomputers, although with some loss of flexibility and generality. however, withthe exception of the lattice gauge qcd community, specialized computers havenot gained widespread acceptance in science and engineering. this is due largelyto the rapid increases in the computing capabilities of generalpurpose microprocessors over the last couple of decadesñwith an increase of a factor of 2 every1824 months, the advantages offered by specialized computers can rapidly become outdated. in addition, the limited flexibility of specialized computers oftenprevented the use of new, more powerful algorithms or computational approaches.however, given the increasing design and fabrication capabilities of the computer industry, i believe that this is a topic well worth reexamining.2s. shingu, h. takahara, h. fuchigami, m. yamada, y. tsuda, w. ohfuchi, y. sasaki, k.kobayashi, t. hagiwara, s. habata, m. yokokawa, h. itoh, and k. otsuka, òa 26.58 tflops globalatmospheric simulation with the spectral transform method on the earth simulator,ó presented atsc2002 (nov. 2002).3òcapacity of u.s. climate modeling to support climate change assessment activities,ó climateresearch committee, national research council, (national academy press, washington, 1999).4see: http://www.research.ibm.com/grape/.5d. chen, n. h. christ, c. cristian, z. dong, a. gara, k. garg, b. joo, c. kim, l. levkova, x.liao, r. d. mawhinney, s. ohta, and t. wettig, nucl. phys. b (proc. suppl.) 2001, 94, 825832; p. a.boyle, d. chen, n. h. christ, c. cristian, z. dong, a. gara, b. joo, c. kim, l. levkova, x. liao, g.liu, r. d. mawhinney, s. ohta, t. wettig, and a. yamaguchi, nucl. phys. b (proc. suppl.) 2002,106, 177183.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.thom h. dunning, jr.91in summary, it is clear that over the next decade and likely longer, we willsee continuing increases in computing power, both from commercially orientedcomputers and from more specialized computers, including followons to theearth simulator. with the challenge offered by the japanese machine, u.s. computer companies can be expected to refine their designs in order to make theircomputers more appropriate for scientific and engineering calculations, althougha major change would be required in the business plans of companies such asibm and hp to fully respond to this challenge (the scientific and engineeringmarket is miniscule compared to the commercial market). these increases in computing power will have a major impact on computerbased approaches to scienceand engineering.opportunities in computational chemistrywhat do the advances occurring in computing technology mean for computational chemistry, especially electronic structure theory, the authorõs area of expertise? it is still too early to answer this question in detail, but the advances incomputing technology will clearly have a major impact on both the size of molecules that can be computationally modeled and the fidelity with which they canbe modeled. great strides have already been achieved for the latter, although asyet just for small molecules (the size of benzene or smaller). consider, for example, the calculation of bond energies (figure 4). when i started graduate school1101001970198019902000error (kcal/mol)bond energies critical for describing many chemical phenomenaaccuracy of calculated bond energies increased dramatically from 19702000 due to advances intheoretical methodologycomputational techniquescomputing technologyfigure 4opportunity: increased fidelity of molecular models.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.92appendix din 1965, we were lucky to be able to calculate bond energies accurate to 50 kcal/mol. such predictions were not useful for much of anything and, in fact, lookingback on the situation, i am amazed that experimental chemists showed such tolerance when we reported the results of such calculations. but the accuracy of thecalculations has been improving steadily over the past 30 years. by 2000, bondenergies could be predicted to better than 1 kcal/mol, as good as or better than theaccuracy of most experimental measurements.this increase in the accuracy of calculated bond energies is not just due toadvances in computing technology. computing advances were certainly important, but this level of accuracy could never have been achieved without significant advances in theoretical methodology (e.g., the development of coupledcluster theory) as well as in computational techniques, for example, the development of a family of basis sets that systematically converge to the completebasis set (cbs) limit. coupledcluster theory, which was first introduced intochemistry in 1966 (from nuclear theory) by j. cizek and subsequently developed and explored by j. paldus, i. shavitt, r. bartlett, j. pople, and their collaborators, provides a theoretically sound, rapidly convergent expansion of thewave function of atoms and molecules.6 in 1989, k. ragavachari, j. pople andcoworkers suggested a perturbative correction to the ccsd method to accountfor the effect of triple excitations.7 the accuracy of the ccsd(t) method isastounding. dunning8 has shown that it provides a nearly quantitative description of molecular binding from such weakly bound molecules as he2, which isbound by only 0.02 kcal/mol, to such strongly bound molecules as co, which isbound by nearly 260 kcal/molña range of four orders of magnitude. one of themost interesting aspects of coupled cluster theory is that it is the mathematicalincarnation of the electron pair description of molecular structureña modelused by chemists since the early twentieth century. true to the chemistõs model,the higherorder corrections in coupled cluster theory (triple and higher excitations) are small, although not insignificant.the accuracy of the ccsd(t) method for strongly bound molecules is illustrated in figure 5. this figure provides a statistical analysis of the errors in thecomputed de values for a representative group of molecules.9 the curves represent the normal error distributions for three different methods commonly used tosolve the electronic schrıdinger equation: secondorder m¿llerplesset perturbation theory (mp2), coupled cluster theory with single and double excitations, and6a brief history of coupled cluster theory can be found in r. j. bartlett, theor. chem. acc. 2000,103, 273275.7k. raghavachari, g. w. trucks, j. a. pople, and m. headgordon, chem. phys. lett. 1989, 157,479483.8t. h. dunning, jr., j. phys. chem. a 2000, 104, 90629080.9k. l. bak, p. j¿rgensen, j. olsen, t. helgaker, and w. klopper, j. chem. phys. 2000, 112, 92299242.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.thom h. dunning, jr.93ccsd with the perturbative correction for triple excitations, ccsd(t). the boxat the top right lists the average error (ave) and the standard deviation (std). ifthe schrıdinger equation was being solved exactly for this group of molecules,the standard error distribution would simply be a function at zero (0.0). it isclear that the error distributions for the mp2 and ccsd methods do not have thisshape. not only are the average errors far from zero (ave = 6.0 and ð8.3 kcal/mol,respectively), but the widths of the error distributions are quite large (std = 7.5and 4.5 kcal/mol, respectively). clearly, neither the mp2 nor the ccsd methodis able to provide a consistent description of the molecules included in the test set,although the ccsd method, with its smaller std, can be considered more reliablethan the mp2 method. on the other hand, if the perturbative triples correction isadded to the ccsd energies, the error distribution shifts toward zero (ave = ð1.0kcal/mol) and sharpens substantially (std = 0.5 kcal/mol). the accuracy of theccsd(t) method is not a fluke, as studies with the ccsdt and ccsdtq methods, limited though they may be, show.the above results illustrate the advances in our ability to quantitatively describe the electronic structure of molecules. this advance is in large part due toour ability to converge the solutions of the electronic schrıdinger equation. inthe past, a combination of incomplete basis sets and lack of computing powerprevented us from pushing calculations to the complete basis set (cbs) limit. forany finite basis set calculation, the error is the sum of the error due to the use of anapproximate electronic structure method plus the error due to the use of an incomplete basis set. these errors can be of opposite sign, which can lead to confusing òfalse positivesó (i.e., agreement with experiment due to cancellation ofde(kcal/mol)0.00.20.40.60.81.040.030.020.010.00.010.020.030.040.0mp2ccsd(t)ccsdmp2ccsd+(t)6.08.31.0std7.54.50.5(de)figure 5theoretical advances: coupled cluster calculation of de values.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.94appendix derrors).8 at the cbs limit on the other hand, the remaining error is that due to themethod itself. thus, the ability to push calculations to the cbs limit has allowedus to separate the methodological errors from the basis set truncation errors,greatly advancing our understanding of the accuracy of the various approximatemethods used to solve the schrıdinger equation.the computing resources needed to push calculations to the cbs limit areillustrated in figure 6. this figure displays the results of mp2 calculations10 onthe water hexamer by xantheas, burnham, and harrison.11 in addition to its intrinsic importance, an accurate value of the binding energy for the water hexamerwas needed to provide information for the parameterization of a new manybodypotential for water.12 this information is not available from experiment and thusits only source was highlevel quantum chemical calculations. the energy plottedin the figure is that required to dissociate the water hexamer at its equilibriumgeometry into six water molecules at their equilibrium geometries: ee[(h2o)6]ñ6ee(h2o). with the augmented double zeta basis set (augccpvdz), the calculations predict that the hexamer is bound by 39.6 kcal/mol. calculations with theaugccpvdz augccpvtz augccpvqz augccpv5z47.045.043.041.039.01032~1,000x2461552~50x1722~10,000xnbf=de(kcal/mol)44.8 kcal/molmp2/cbs limitfigure 6opportunity: converged molecular calculations (data from xantheas s.;burnham, c.; harrison, r. j. chem. phys. 2002, 116, 1493).10in contrast to chemicallybound molecules, the mp2 method predicts binding energies for hydrogenbonded molecules such as (h2o)n comparable to those obtained with the ccsd(t) method (seeref. 8).11s. s. xantheas, c. j. burnham, and r. j. harrison, j. chem. phys. 2002, 116, 14931499.12c. j. burnham and s. s. xantheas, j. chem. phys. 2002, 116, 15001510; c. j. burnham and s.s. xantheas, j. chem. phys. 2002, 116, 51155124.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.thom h. dunning, jr.95quintuple zeta basis set (augccpv5z), on the other hand, predict an equilibriumbinding energy of 44.3 kcal/mol, nearly 5 kcal/mol higher. as can be seen, thevariation of the binding energy with basis set is so smooth that the value can beextrapolated to the cbs limit. this yields a binding energy of 44.8 kcal/mol.from benchmark calculations on smaller water clusters, this number is expectedto be accurate to better than 1 kcal/mol.pushing molecular calculations on molecules such as (h2o)6 to the cbs limitrequires substantial computing resources. if the amount of computer time requiredfor a calculation with the double zeta set is assigned a value of 1 unit, then acalculation with the triple zeta set requires 50 units. the amount of computingtime required escalates to the order of 1000 units for the quadruple zeta set andthe order of 10,000 units for the quintuple zeta set. clearly, advances in computing power are contributing significantly to our ability to provide quantitative predictions of molecular properties. another opportunity provided by greatly increased computing resources isthe ability to extend calculations such as those described above to much largermolecules. this is important in the interpretation of experimental data (experimentalists always seem to focus on larger molecules than we can model), to characterize molecules for which experimental data is unavailable, and to obtain datafor parameterizing semiempirical models of more complex systems. the latterwas the driving force in the calculations on (h2o)n referred to above.11 this wasalso what drove the study of water interacting with cluster representations ofgraphite sheets by feller and jordan13 (figure 7). the mp2 calculations reportedby these authors considered basis sets up to augccpvqz and graphitic clustersup to (c96h24). by exploiting the convergence properties of the correlation consistent basis sets as well as that of the graphitic clusters, they predicted the equilibrium binding energy for water to graphite to be 5.8 0.4 kcal/mol. the largestmp2 calculation reported in this study was on h2oc96h24 and required 29 hourson a 256processor partition of the ibm sp2 at pnnlõs environmental molecular sciences laboratory.the ultimate goal of feller and jordanõs work is to produce a potential thataccurately represents the interaction of water with a carbon nanotube. their nextstudy will involve h2o interacting with a cluster model of a carbon nanotube.however, this will require access to the next generation massively parallel computing system from hewlettpackard that is currently being installed in emslõsmolecular science computing facility.1413d. feller and k. jordan, j. phys. chem. a 2000, 104, 99719975.14see: http://www.emsl.pnl.gov:2080/capabs/mscfcapabs.html and http://www.emsl.pnl.gov:2080/capabs/mscf/hardware/confighpcs2.html.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.96appendix dchallenges in computational chemistryalthough the opportunities offered by advances in computing technologiesare great, many challenges must be overcome to realize the full benefits of theseadvances. most often the focus is on the computational challenges because theseare indeed formidable. however, we must also quantify the limitations of theexisting theories used to describe molecular properties and processes as well asdevelop new theories for those molecular phenomena that we cannot adequatelydescribe at present (theoretical challenges). in addition, we must seek out andcarefully evaluate new mathematical approaches that show promise of reducingthe cost of molecular calculations (mathematical challenges). the computationalcost of current electronic structure calculations increases dramatically with thesize of the molecule (e.g., the ccsd(t) method scales as n7, where n is thenumber of atoms in the molecule). of particular interest are mathematical techniques that promise to reduce the scaling of molecular calculations. work is currently under way in all of these challenge areas.as noted above, the theoretical challenges to be overcome include the validation of existing theories as well as the development of new theories. one of thesurprises in electronic structure theory in the 1990s was the finding that m¿llerplesset perturbation theory, the most widely used means to include electron correlation effects, does not lead to a convergent perturbation expansion series. this˛need interaction potentials to model nanoscale processeslittle data from experiment, need accurate calculationsfirst step: h2oc96h24; next step h2o with segment of nanotube.˛ !"˛˘figure 7opportunity: larger, more complex molecules. courtesy of d. feller, pacific northwest national laboratory; see also feller, d.; jordan, k. d. j. phys. chem. a2000, 104, 9971.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.thom h. dunning, jr.97nature of the perturbation expansion is illustrated in figure 8, which plots thedifference between full ci and perturbation theory energies for the hf moleculein a double zeta and augmented double zeta basis set as a function of the order ofperturbation theory. as can be seen, at first the perturbation series appears to beconverging, but then, around fifth or sixthorder perturbation theory, the seriesbegins to oscillate (most evident for the augccpvdz set) with the oscillationsbecoming larger and larger with increasing order of perturbation theory.15 theperturbation series is clearly diverging even for hydrogen fluoride, a moleculewell described by a hartreefock wave function. dunning and peterson16 showedthat even at the complete basis set limit, the mp2 method often provides moreaccurate results than the far more computationally demanding mp4 method. thus,the series is not well behaved even at low orders of perturbation theory. howmany other such surprises await us in theoretical chemistry?although we now know how to properly describe the ground states of molecules,17 the same cannot be said of molecular excited states. we still await an30.025.020.015.010.05.00.05.010.015.020.025.001020304050ccpvdzaugccpvdze(fci)ðe(mpn), mehfigure 8theoretical challenges: convergence of perturbation expansion for hf (datafrom olsen, j.; christiansen, o.; koch, h.; j¿rgensen, p. j. chem. phys. 1996, 105, 50825090).15j. olsen, o. christiansen, h. koch, and p. j¿rgensen, j. chem. phys. 1996, 105, 50825090.16t. h. dunning, jr. and k. a. peterson, j. chem. phys. 108, 47614771 (1998).17this is not to say that the ccsd(t) method provides an accurate description of the ground statesof all molecules. the coupled cluster method is based on a hartreefock reference wave function andthus will fail when the hf wave function does not provide an adequate zeroorder description of themolecule. the development of multireference coupled cluster methods is being actively pursued byseveral groups.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.98appendix daccurate, general, computationally tractable approach for solving for the higherroots of the electronic schrıdinger equation. without such a theory, many molecular phenomena, such as visibleuv spectroscopy and photochemistry, willremain out of modeling reach.as noted above, current molecular electronic structure calculations scale as ahigh power of the number of atoms in the molecule. this is illustrated in figure 9.for example, the hartreefock method scales as n4, while the far more accurateccsd(t) method scales as n7. thus, when using the ccsd(t) method, doublingthe size of a molecule increases the computing time needed for the calculation byover two orders of magnitude. even if computing capability is doubling every 18 to24 months, it would take another 10 years before computers would be able to modela molecule twice as big as those possible today. fortunately, we know that, forsufficiently large molecules, these methods donõt scale as n4 or n7. for example, ithas long been known that simple, controllable mathematical approximations suchas integral screening can reduce the scaling of the hf method to n2 for sufficientlylarge molecules. the impact of this is illustrated by the dashed curve in the figure.using more advanced mathematical techniques, such as the fast multipolemethod (fmm) to handle the longrange coulomb interaction,18 and a separate101100101102103104105106100101102103104number of atomsteraflopshfccsd(t)~n7~n4figure 9mathematical challenges: scaling laws for molecular calculations.18l. greengard and v. rohklin, acta numerica 6 (cambridge university press, cambridge, 1997),pp 229269 and references therein.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.thom h. dunning, jr.99treatment of the exchange interaction, it is possible to develop a hf algorithmthat exhibits linear scaling in the very large molecule limit.19 combining thesetechniques with fourier methods provides improved scaling even for small systems in highquality basis sets. these techniques can be straightforwardly extended to dft calculations and similar reductions in the scaling laws are possiblefor correlated calculations, including ccsd(t) calculations. as in the hf method,it is possible to exploit screening in calculating the correlation energy. however,to take advantage of screening, the equations for the correlated methods must berewritten in terms of the atomic orbital basis rather than the molecular orbitalbasis. this has recently been done for the coupled cluster method by scuseria andayala,20 who showed that, with screening alone, the ccd equations could besolved more efficiently in the ao basis than in the mo basis for sufficiently largemolecules. since the effectiveness of screening and multipole summation techniques increase with molecule size, the question is not whether the use of aoswill reduce the scaling of coupled cluster calculations but, rather, how rapidly thescaling exponent will decrease with increasing molecular size.the impact of reduced scaling algorithms is just beginning to be felt in chemistry, primary in hf and dft calculations.21 however, reduced scaling algorithms for correlated molecular calculations are likely to become available in thenext few years. when these algorithmic advances are combined with advances incomputing technology, the impact will be truly revolutionary. problems that currently seem intractable will not only become doable, they will become routine.numerical techniques for solving the electronic schrıdinger equation arealso being pursued. another paper from this workshop has been written by r.friesner, who developed the pseudospectral method, an ingenious half numericalhalf basis set method. another approach that is being actively pursued is theuse of wavelet techniques to solve the schrıdinger equation. r. harrison andcoworkers recently reported dftlda calculations on benzene22 that are themost accurate available to date (figure 10). unlike the traditional basis set expansion approach, the cost of waveletbased calculations automatically scales linearly in the number of atoms. although much remains to be done to optimize thecodes for solving the hartreefock and dft equations, not to mention the development of wavelet approaches for including electron correlation effects via methods such as ccsd(t), this approach shows great promise for extending rigorouselectronic structure calculations to large molecules.finally, there are computational challenges that must be overcome. thesupercomputers in use today achieve their power by using thousands of proces19c. ochsenfeld, c. a. white, and m. headgordon, j. chem. phys. 1998, 109, 16631669.20g. e. scuseria and p. y. ayala, j. chem. phys. 1999, 111, 83308343.21see, e.g., the feature article by g. e. scuseria, j. phys. chem. a 1999, 103, 47824790.22r. j. harrison, g. i. fann, t. yanai, and g. beylkin, òmultiresolution quantum chemistry: basictheory and initial applications,ó to be published.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.100appendix dsors, and computers are already in the design stage that use tens of thousands tohundreds of thousands of processors. so, one challenge is to develop algorithmsthat scale well from a few processors to tens of processors, to hundreds of processors, to thousands of processors, and so on. this is a nontrivial challenge and is anarea of active research in computational science, computer science, and appliedmathematics. in some cases, this work has been very successful with calculationsusing more than 1,000 processors being reported with nwchem23 and namd.24for other scientific applications, we donõt know when such algorithms will bediscoveredñthis is, after all, an issue of creativity and creativity respects noschedules.one of the surprises that i had as i began to delve into the problems associated with the use of the current generation of supercomputers was the relativelypoor performance of many of the standard scientific algorithms on a single processor. many algorithms achieved 10% or less of peak performance! this efficiency then further eroded as the calculations were scaled to more and more processors. clearly, the cachebased microprocessors and their memory subsystemsused in todayõs supercomputers are very different than those used in the supercomputers of the past. figure 11 illustrates the problem. in the ògood ole daysó(just a decade ago), cray supercomputers provided very fast data paths betweenthe central processing unit and main memory. on those machines, many vectordft/lda energy103230.194105230.19838107230.198345for comparisonpartridge3 primitive set +augccpvtz polarization set:230.158382 hartreesfigure 10mathematical challenges: multiwavelet calculations on benzene. courtesyof r. harrison, g. fann, and g. beylkin, pacific northwest national laboratory.23see benchmark results at the nwchem web site http://www.emsl.pnl.gov:2080/docs/nwchem/nwchem.html.24j. c. phillips, g. zheng, s. kumar, and l. v. kale, ònamd: biomolecular simulation on thousands of processors,ó presented at sc2002 (nov. 2002). see also http://www.ks.uiuc.edu/research/namd/.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.thom h. dunning, jr.101arithmetic operations, such as a daxpy, which requires two words to be drawnfrom memory plus one written to memory every processor cycle, could be run atfull speed directly from main memory. the supercomputers of today, on the otherhand, are built from commercially oriented microprocessors and memory. although this greatly reduces the cost of the supercomputers, the price is slow communications between the microprocessor and memory. thus, it may take tens ofcycles to transfer a word from memory to the processor. computer designersattempt to minimize the impact of slow access to main memory by placing a fastcache memory between the processor and main memory. this works well if thealgorithm can make effective use of cache, but many scientific and engineeringalgorithms do not. new cachefriendly algorithms must be developed to take fulladvantage of the new generation of supercomputersñagain, a problem in creativity.one of the reasons for the success of the japanese earth simulator is that itwas designed with a knowledge of the memory usage patterns of scientific andengineering applications. although the bandwidth between the processor andmemory in the earth simulator does not match that in the cray computers of the1990s (on a relative basis), it is much larger (four times or more) than that forllnlõs asci white or pscõs lemieux. in late 2002, cray, inc. announced itsnew computer, the cray x1. the cray x1 has the same bandwidth (per flops) asthe earth simulator, but each processor also has a 2 mbyte cache with a bandwidth that is twice the bandwidth to main memory. the scientific and engineeringcommunity is eagerly awaiting the delivery and evaluation of this new computermemorycpunodememorythe good ole dayscirca19901s10s100s101,000s10,000syesterday1001,000 processorstoday~10,000 processorsvrcpu1s1,000s10,000snodememoryvrcpucpufigure 11computational challenges: keeping the processors busy. the numbers superimposed on the arrows refer to the number of processor cycles required to transfer abyte of data from the indicated memory location to the processor.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.102appendix dfrom cray, inc. oak ridge national laboratory is leading this effort for doeõsoffice of science.the figure also illustrates the speed with which data can be transferred fromthe memory associated with other processors. this can require thousands or tensof thousands of processor cycles. the trick to developing scalable algorithms is tokeep the data close to the processor(s) that need it. this is, of course, easier saidthan done.computational modeling of complex chemical systemsi donõt know how many in the audience read john horganõs book the end ofscience: facing the limits of knowledge in the twilight of the scientific age(little brown & company, 1997). given the amazing advances in scientificknowledge that are being made each day, i began reading this book in a state ofdisbelief. it wasnõt until i was well into the book that i realized that horgan wasnot talking about the end of science, but rather the end of reductionism in science.these are not the same thing. a physicist might be satisfied that he understandschemistry once the schrıdinger equation had been discovered, but for chemiststhe job has only begun. chemists are interested in uncovering how the basic lawsof physics become the laws that govern molecular structure, energetics, and reactivity. molecules are complex systems whose behavior, although a result of thefundamental laws of physics, cannot be directly connected to them.although i donõt think we have yet reached the end of reductionism (muchstill remains to be discovered in physics, chemistry, and biology), i do think thatwe are in a period of transition. scientists spent most of the twentieth centurytrying to understand the fundamental building blocks and processes that underlieour material world. the result has been a revolutionñchemists now understandmuch about the basic interactions between atoms and molecules that influencechemical reactivity and are using that knowledge to create molecules that oncecould be obtained only from nature (e.g., cancerfighting taxol); biologists nowunderstand that the basic unit of human heredity is a large, somewhat monotonous molecule and have nearly determined the sequence of aõs, tõs, gõs and cõsin human dna and are using this knowledge to pinpoint the genetic basis ofdiseases. as enlightening as this has been, however, we are now faced with another, even greater, challengeñassembling all of the information available onbuilding blocks and processes in a way that will allow us to predict the behaviorof complex, realworld systems. this can only be done by employing computational models. this is the scientific frontier of the twentyfirst century. we arenot at the end of science, we are at a new beginning. although horgan may notrecognize this activity as science, the accepted definition of science, òknowledgeor a system of knowledge covering general truths or the operation of general lawsespecially as obtained and tested through the scientific method,ó indicates thatthis is science nonetheless.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.thom h. dunning, jr.103one example of a complex chemical system is an internal combustion engine. to predict the behavior of such a system, we must be able to model a widevariety of physical and chemical processes:¥simulate the mechanical device itself, including the dynamical processesinvolved in the operation of the device as well as the changes that will occur inthe device as a result of combustion (changes in temperature, pressure, etc.);¥simulate the fluid dynamics of the fuelair mixture, from the time it isinjected into the combustion chamber, through the burning of the fuel and consequent alteration of its chemical composition, temperature, and pressure, to thetime it is exhausted from the chamber; and¥simulate the chemical processes involved in the combustion of the fuel,which for fuels such as nheptane (a model for gasoline) can involve hundreds ofchemical species and thousands of reactions.the problem with the latter is that many of the molecular species involved incombustion have not yet been observed and many of the reactions have not yetbeen characterized (reaction energetics and rates) in the laboratory. from this it isclear that supercomputers will play a critical role in understanding the behaviorof complex systems. it is simply not possible for scientists to understand how allof these physical and chemical processes interact to determine the behavior of aninternal combustion engine without computers to handle all of the bookkeeping.it will not be possible for experimental chemists to identify and characterize all ofthe important chemical species and reactions of importance in flames; reliablecomputational predictions will be essential to realizing the goal.an example of cuttingedge research on the fundamental chemical processesoccurring in internal combustion engines is the work being carried out at thecombustion research facility of sandia national laboratories. for example,jackie chen and her group are working on the development of computationalmodels to describe autoignition. autoignition is the process that ignites a fuel bythe application of heat, not a flame or spark. the fuel in diesel engines is ignitedby autoignition; a spark plug is not present. autoignition is also the basis of avery efficient internal combustion engine with extremely low emissionsñtherevolutionary homogeneous charge, compression ignition, or hcci engine. thecatch is that hcci engines can not yet be used in cars and trucks because it is notyet possible to properly control the òflamelessó combustion of fuels associatedwith autoignition.figure 12 illustrates recent results from j. chen and t. echekki at sandianational laboratories (unpublished). this figure plots the concentration ofperoxyl radical (hoo), which has been found to be an indicator of the onset ofautoignition, as a function of time in a h2air mixture from twodimensional,direct numerical simulations. these results led chen and echekki to identify newchemical pathways for autoignition and led them to propose a new approach toinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.104appendix ddescribing autoignition in terms of relevant flow and thermochemical parameters.the simulations show that autoignition is initiated in discrete kernels or structures that evolve differently, depending strongly upon the exact local environment in which they occur. this is easily seen in the figure. but, this work is onlya beginning. turbulent fluctuations as well as the autoignition kernels are inherently three dimensional, and more complex fossil fuels will undoubtedly give riseto new behaviors that must be studied in a full range of environments to gain thescientific insight needed to develop predictive models of realworld autoignitionsystems. however, the computing requirements for such calculations require software and hardware capable of sustaining tens of teraflops.an internal combustion engine is just one example of a complex system.there will be many opportunities to use our fundamental knowledge of chemistryto better understand complex, realworld systems. other examples include a widevariety of processes involved in industrial chemical production, the molecularprocesses that determine cellular behavior, and the chemical processes affectingthe formation of pollutants in urban environments. at the workshop, jim heathmade a presentation on nanochemistry, i.e., nanoscale molecular processes. oneof the most intriguing aspects of nanoscale phenomena is that nanoscale systemsare the first systems to exhibit multiple scales in chemistryñthe molecular scaleand the nanoscale. phenomena at the molecular scale are characterized by timescales of femto to picoseconds and distance scales of tenths of a nanometer to ananometer. nanoscale phenomena on the other hand often operate on micro tomillisecond (or longer) time scales and over distances of 10100 (or more) nm.so, in addition to whatever practical applications nanochemistry has, it also represents a opportunity to understand how to describe disjoint temporal and spatialevolution of hydroperoxy, ho2, at different fractions of autoignition induction time.3/2 tind1/2 tind2 tind1 tindturbulent mixingstrongly affects ignition delay by changing chemical branching/ termination balancenew modelsmay assist innovative engine design (hcci)limited by existing computer capabilitiesfigure 12coupling chemistry and fluid dynamics: h2 autoignition in turbulent mixtures. courtesy of j. h. chen and t. echekki, sandia national laboratories.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.thom h. dunning, jr.105scales in computational modeling. achieving this goal will require close collaboration between experimental chemists synthesizing and characterizing nanochemical systems and computational chemists modeling these systems. the concepts developed in such studies may be applicable to scaling to the meso andmacroscales.data storage, mining, and analysis in genomics and related scienceslet me turn now to a very different type of problemñthe data problem inscience. the research universities in north carolina, like most major researchuniversities in the united states, want to become major players in the genomicsrevolution. this revolution not only portends a major advance in our understanding of the fundamentals of life, but also promises economic rewards to thosestates that make the right investments in this area. however, the problem with theònew biologyó is that it is very different than the òold biology.ó it is not just morequantitative: it has rapidly become a very dataintensive activity. it is not dataintensive nowñmost of the data currently available can be stored and analyzedon personal computers or small computerðdata servers. but, with the quantity ofdata increasing exponentially, the amount of data will soon overwhelm all ofthose institutions that have not built the information infrastructure needed to manage it. few universities are doing this. most are simply assuming that their faculty will solve the problem on an individual basis. i donõt believe this will workñthe problem is of such a scale that it cannot be addressed by point solutions.instead, to be a winner in the genomics sweepstakes, universities or even university consortia must invest in the integrated information infrastructure that will beneeded to store, mine, and analyze the new biological data.the next two figures illustrate the situation. figure 13 is a plot of the numberof gigabases (i.e., linear sequences of aõs, tõs, cõs, and gõs) stored in genbankfrom 1982 to the present.25 as can be seen, the number of gigabases in genbankwas negligible until about 1991, although i can assure you that scientists werehard at work from 1982 to 1991 sequencing dna; the technology simply did notpermit largescale dna sequencing and thus the numbers donõt show on the plot.the inadequacy of the technology used to sequence dna was widely recognizedwhen the office of science in the u.s. department of energy initiated its humandna sequencing program in 1986 and substantial investments were made in thedevelopment of highthroughput sequencing technologies. when the human ge25there are three database respositories that store and distribute genome nucleotide sequence data.genbank is the repository in the u.s. and is managed by the nih (see: http://www.ncbi.nlm.nih.gov);ddbj is the dna data bank of japan managed by the japanese national institute of genetics (see:http://www.nig.ac.jp/home.html); and the embl nucleotide database is maintained by the europeanbioinformatics institute (see: http://www.ebi.ac.uk/). sequence information is exchanged betweenthese sites on a daily basis.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.106appendix dnome project, a joint effort of the national institutes of health and the office ofscience, began in 1990, a similar approach was followed. we are now reaping thebenefits of those investments. at the present time, the doubling time for genbankis less than one year and decreasingñin the first 10 months of 2002, more sequences were added to genbank than in all previous years.the other òproblemó that must be dealt with in genomics research is thediversity of databases (figure 14). there is not just one database of importance tomolecular biologists involved in genomics, proteomics, and bioinformatics research; there are many, each storing a specific type of information. each year thejournal nucleic acids research surveys the active databases in molecular biologyand publishes information on these databases as the first article of the year. injanuary 2002, 335 databases were identified. many of these databases are derivedfrom genbank; others are independent of genbank. whatever the source, however, all are growing rapidly. the challenge to the researcher in genomics is tolink the data in these databases with the data that he/she is producing to advancethe understanding of biological structure and function. given the size and diversity of the databases, this is already intractable without the aid of a computer.putting all of the above together, we find ourselves in a most unusual situanumber ofgigabasesgenbank¥number of base pairs increasing rapidly (exponentially)¥so far, more than 15 gigabases have been added in 2002 (as of oct. 30th)19821986199019941998200205101520figure 13background: exponential growth in genbank.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.thom h. dunning, jr.107"the molecular biology database collection: 2002 update,"andreas d. baxevanis, a.d.nucleic acids research 2002,30 (1) 112.major public sequence repositoriesvaried biomedical contentdna data bank of japan (ddbj)http://www.ddbj.nig.ac.jpall known nucleotide and protein sequences . . .. . .. . . . . . . . .. . . . . . . . .. . . . . .. . .. . . . . .viroligohttp://viroligo.okstate.eduvirusspecific oligonucleotides for pcr and . . .335 databases!figure 14background: number and diversity of databases.timecomputational loadgenome data8x growth / 1824 monthsmoore's law2x growth / 1824 monthsx multipliertion in biology (figure 15, from timelogic26). mooreõs law is represented atthe bottom of the graph; this represents a doubling of computing capability every1824 months. the middle band illustrates the rate at which the quantity of genomic data is increasing. note that, as seen above, it outstrips the growth in comfigure 15background: mining and analysis of genomic data. courtesy of timelogiccorporation.26see: http://www.timelogic.com/.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.108appendix dputing power from mooreõs law. the upper band represents the rate of growth incomputing resources needed for genomic research (i.e., the amount of computingpower that will be needed to mine and analyze genomic data). clearly, the day ofreckoning is coming. although most biologists involved in genomic and relatedresearch are very comfortable with the situation they are currently in, when storage and computing requirements are increasing at an exponential rate, one can bequite comfortable one year and absolutely miserable the next.of course, those biologists most heavily involved in genomics research understand that substantial computing power and data storage capabilities will beneeded to support their research. even so, many still underestimate the growingmagnitude of the problem. faculty in duke university, north carolina state university, and the university of north carolina at chapel hill are attempting tosolve the problem by building pcbased clusters (usually linux clusters) to storeand analyze their data. there are many such clusters all over the three universitieswith more to come. if there ever was an argument for collective action, this is it.not only will it be difficult to grow the clusters at the pace needed to keep up withthe proliferation of data, but it will be difficult to house and operate these machines as well as provide many essential services needed to ensure the integrity ofthe research (e.g., data backup and restore).one difficulty with taking collective action is that each researcher wants tohave his/her data close at hand, but these data need to be seamlessly integratedwith all of the other data that are available, both public data being generated in theuniversities in north carolina and data being stored in databases all over theworld. in addition, the researchers want to be confident than their private data aresecure. one way to achieve this goal is to use grid technologies. with grid technologies, a distributed set of computing and data storage resources can be combined into a single computing and data storage system. in fact, my idea was tospread this capability across the whole state of north carolina. all of the 16campuses of the university of north carolina plus duke university and wakeforest university would be tied together in a grid and would have ready access tothe computing and data storage resources available throughout the grid whetherthey were physically located in the research triangle, on the coast, or in themountains. although the biggest users of such a grid would likely be the researchuniversities, all of the universities need to have access to this capability if they areto properly educate the next generation of biologists.some of the attributes of the north carolina bioinformatics grid27 are summarized in figure 16. at the top of the list are general capabilities such as singlesignon (a user need only log onto the grid; thereafter all data and computingresources to which they are entitled become available) and a systemwide approach to security rather than a universitybyuniversity approach. the latter isparticularly important in a multicampus university. other capabilities include fine27for more information on the nc biogrid, see: http://www.ncbiogrid.org.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.thom h. dunning, jr.109grain control of resource sharing based on policies set at the university and campus level (controlling who has access to which resources for what length of time).this not only protects the rights of the individual campuses, but allows the performance of the biogrid to be optimized by siting computing and data storageresources at the optimal locations on the network. the grid software underlyingthe biogrid provides a unified view of all of the resources available to a userwhether they are located on his/her campus or distributed across the state. in fact,one of the chief goals of grids, such as the biogrid, is to simplify the interactionof scientists with computers, data stores, and networks, allowing them to focus ontheir scientific research rather than the arcana of computing, data management,and networking. finally, grids, unlike other attempts at distributed computingsuch as nfs and afs, are designed to efficiently handle large datasets. at times,to optimize performance, the grid management software may decide to cache adataset locally (e.g., if a particular dataset is being heavily used by one site) orreplicate a dataset, if it is being heavily used on an ongoing basis.there are many economies and benefits that will be realized with the ncbiogrid. for example, at the present time all of the universities maintain theirown copies of some of the public databases. although this is not a significantexpense now, it certainly will be when these databases hold petabytes of datarather than hundreds of gigabytes of data. in addition to the hardware required tostore and access such massive amounts of data, a seasoned staff will be requiredto manage such a resource. important data management services such as backupand restore of research data can be assured more easily in a grid environmentattributessingle signon, systemwide securitypolicybasedresourcesharingunified view of resourcescomputers and data manages large data setsefficient caching and replicationfigure 16the north carolina bioinformatics grid.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.110appendix dthan in a loosely coupled system of computers. finally, economies will berealized by allowing each campus to size its computing and data storage system to meet its average workload and using resource sharing arrangementswith the other campuses on the biogrid to offload its higherthanaveragedemands.so, how is a biogrid constructed? like many applications in computing, agrid is best viewed as a layered system (figure 17). at the lowest level are thedistributed computing and data resources. it is assumed that these resources aregeographically distributed, although one particular site may be dominant. this isthe case in north carolina where the dominant site will be the north carolinasupercomputing center (terascale supercomputer and petascale data store); yet significant resources will also be needed at the three major campuses, and some levelof resources will be needed at all campuses. the next level is the network that linksthe distributed computing and data storage resources. if the datasets are large, ahighperformance network is needed. north carolina is fortunate is having a statewide, highperformance networkñthe north carolina research and educationnetwork (ncren). this network serves all 16 campuses of the university of northcarolina plus duke university and wake forest university and is currently beingupgraded to ensure that no university is connected to ncren at less than oc3(155 mbps) and many will be connected at oc12 (622 mbps). the next layer is theoperating system for the grid, something called grid middleware. it is themiddleware that adds the intelligence associated with the grid to the network. thereare currently two choices for the grid operating system, globus28 and legionavaki.29 although globus, which was first developed by researchers atportal, interfacesdistributed computingand data resourcesnetworkgrid middleware(gridos)bioapp#1bioapp#2bioapp#3–globus, legion/avaki, –ncren3gridaware or gridenabled bioinformatics applicationsbiogrid portal, bioapplications interfacesncsc plusmember™s computing centersfigure 17elements of the north carolina biogrid.28http://www.globus.org.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.thom h. dunning, jr.111argonne national laboratory and the university of southern california, currently has the greatest mindshare, it is a toolkit for building grids and requiresa considerable amount of expertise to set up a grid. avaki, a commercial product that grew out of legion, a grid environment developed at the university ofvirginia, provides an allinone system. the north carolina biogrid testbedhas both grid operating systems running. both globus and avaki are currentlymigrating to open standards (open grid services infrastructure and open gridservices architecture30).the top two layers of the biogrid are what the user will see. they consist ofapplications such as blast, charmm, or gaussian, modified to make themaware of the services that they can obtain from the grid, and the portals and webinterfaces that allow access to these applications and data resources in a userfriendly way. portals can also provide access to workflow management services.often a scientist doesnõt just perform an isolated calculation. in genomics research, a typical òcomputer experimentó might involve comparing a recently discovered sequence with the sequences available in a number of databases (blastcalculation). if related sequences are found in these databases, then the scientistmay wish to know if threedimensional molecular structures exist for any of theproteins coded by these related sequences (data mining) or may want to knowwhat information is available on the function of these proteins in their associatedbiological systems (data mining). and, the list goes on. the development of software that uses computers to manage this workflow, eliminating the time that thescientist has to spend massaging the output from one application to make it suitable for input to the next application, is a major opportunity for advancing research in molecular biology.when we announced that we were going to build a biogrid in north carolina, we were immediately approached by several biologyrelated software companies stating that we didnõt have to build a biogrid, they already had one. afterparrying several such thrusts, chuck kesler at mcnc decided that we needed tomore carefully define what we meant by a grid. this is done in figure 18. although the companies were able to provide bits and pieces of the functionalityrepresented in this figure, none were able to provide a significant fraction of thesecapabilities.so, this is a grid. i challenge the participants in this workshop to think aboutapplications for grid technologies in chemical science and technology. clearly,chemists and chemical engineers are not yet confronted with the flood of data thatis becoming available in the genomics area but the amount of data that we have inchemistry is still substantial. ann chaka discusses the issue of chemical datastorage and management in her paper.29http://www.avaki.com.30http://www.gridforum.org/documents/drafts/default.htm.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.112appendix dvirtual laboratories in chemical science and technologythe last topic that i want to discuss is the concept of virtual laboratories. advances in networking and computing have made remote access to instruments notonly possible but (reasonably) convenient, largely removing the inconveniencesassociated with distance (e.g., travel to the remote instrument site, specified timeslots for instrument availability, limited time to consult with site scientists on experimental details, etc.). the concept of virtual laboratories is quite advanced inastronomy, where the instruments (large telescopes) are very expensive and oftenlocated in remote regions (on the tops of mountains scattered all over the world). inchemical science and technology, on the other hand, remote access to instruments islargely a foreign concept. however, as instruments for chemical research continueto increase in cost (e.g., the highest field nmrs and mass spectrometers currentlyavailable already cost several million dollars), it will soon become desirable, if notnecessary, for universities and research laboratories to share these instruments ratherthan buy their own. investments in software and hardware to enable remote accessto these instruments can dramatically decrease the barriers associated with the useof instruments located at a remote site. one of the major investments that i made as associate director for computing and information science (and later as director) in the environmental molecular sciences laboratory at pacific northwest national laboratory was in the development of collaboratories, which were defined by wm. a. wulf as31grid user portal(webbased interface)grid workflow management(job sequencing and support)app #1(e.g., blast)app #2(e.g., lion srs)égrid service management(service instantiation, brokering, and monitoring,qos, é)grid dataservices(file and databaseaccess, data cachingand replication, é)grid processingservices(global queuing,coscheduling, é)grid messagingservices(interprocessor,communication, é)app #ngridregistryservices(resourcediscovery,interfacedescription,é)gridsecurityservices(authentication,authorization,accounting,auditing,encryption,é)figure 18no, what you have is not a grid!31wulf, wm. a. the national collaboratory: a white paper in towards a national collaboratory;unpublished report of a nsf workshop, rockefeller university, ny. march 1718, 1989.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.thom h. dunning, jr.113a òcenter without walls,ó in which the nationõs researchers can perform theirresearch without regard to geographical locationñinteracting with colleagues,accessing instrumentation, sharing data and computational resources, and accessing information in digital libraries.(see also national collaboratories: applying information technologies for scientific research32). i did this because pnnl is located in the pacific northwest,a substantial distance from most of the major centers of research in the unitedstates. since emsl is a national user facility, much like the advanced photonsource at argonne national laboratory or the advanced light source atlawrence berkeley national laboratory, we needed to minimize the inconvenience for scientists wishing to use emslõs facilities to support their researchprograms in environmental molecular science. allowing scientists access toemslõs resources via the internet was key to realizing this goal.in emsl there are a number of firstofakind and oneofakind instrumentsthat could be made available to the community via the internet. we decided tofocus on the instruments in the highfield nuclear magnetic resonance facilityas a test case. when we began, this facility included one of the first 750mhznmrs in the united states; it now includes an 800mhz nmr, and a 900mhznmr is expected to come online shortly. the nmrs were very attractive candidates for this test since the software for field servicing the nmrs provided muchof the basic capability needed to make them accessible over the internet.development of the virtual nmr facility (vnmrf, figure 19) was a truecollaboration between nmr spectroscopists and computer scientists. one of thefirst discoveries was that simply implementing secure remote control of the nmrswas not sufficient to support the vnmrf. many other capabilities were neededto realize the promise of the vnmrf, including¥realtime videoconferencing,¥remotely controlled laboratory cameras, and¥realtime computer displays sharing a webbased electronic laboratorynotebook and other capabilities.of particular importance was the ability of remote researchers to discussissues related to the experiment with collaborators, scientists, and technicians inthe emsl before, during and after the experiment as well as to work together toanalyze the results of the experiment. these discussions depended on the availability of electronic tools such as the electronic laboratory notebook and theteleviewer (both are included in pnnlõs core2000 software package33 forsupporting collaborative research).32national collaboratories: applying information technologies for scientific research, nationalresearch council, national academy press, washington, dc, 1993.33http://www.emsl.pnl.gov:2080/docs/collab/.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.114appendix dusing the software developed or integrated into core2000 by emsl staff,a remote scientists can schedule a session on an nmr, discuss the details of theexperiment with emsl staff, send the sample to emsl, watch the technicianinsert the sample into the nmr, control the experiment using a virtual controlpanel, and visualize the data being generated in the experiment. the scientists cando everything from the virtual control panel displayed on his/her computer display that he/she can do sitting at the real control panel. currently, over half of thescientists using emslõs highfield nmrs use them remotely, a testimony to theeffectiveness of this approach to research in chemical science and technology. amore detailed account of the vnmrf can be found in the paper by keating etal.34 see also the vnmrf home page.35there are a few other virtual facilities now in operation. a facility for electron tomography is operational at the university of california at san diego36 andhas proven to be very successful. there are electron microscopy virtual laboratofigure 19the virtual nmr facility at the environmental molecular sciences laboratory (pacific northwest national laboratory).34k. a. keating, j. d. myers, j. g. pelton, r. a. bair, d. e. wemmer, and p. d. ellis, j. mag. res.2000, 143, 172183.35http://www.emsl.pnl.gov:2080/docs/collab/virtual/emslvnmrf.html.36http://ncmir.ucsd.edu/telescience.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.thom h. dunning, jr.115ries in operation at argonne national laboratory37 and oak ridge national laboratory.38 despite these successes, however, fullfledged support for virtual laboratories has not yet materialized. those federal agencies in charge of buildingstateoftheart user facilities for science and engineering rarely consider the computing, data storage, and networking infrastructure that will be needed to makethe facility widely accessible to the scientific community. even less do they consider the investments in software development and integration that will be neededto provide this capability. this is a lost opportunity that results in much wastedtime on the part of the scientists who use the facility to further their researchprograms.conclusioni hope that this paper has given you an appreciation of the potential impact ofadvances in computing, data storage, and networking in chemical science andtechnology. these advances will profoundly change our field. they will greatlyenhance our ability to model molecular structure, energetics, and dynamics, providing insights into molecular behavior that would be difficult, if not impossible,to obtain from experimental studies alone. they will also allow us to begin tomodel many complex systems in which chemical processes are an integral part(e.g., gasoline and diesel engines, industrial chemical production processes, andeven the functioning of a living cell). the insights gained from these studies notonly will deepen our understanding of the behavior of complex systems, but alsowill have enormous economic benefits.the advances being made in grid technologies and virtual laboratories willenhance our ability to access and use computers, chemical data, and firstofakind or oneofakind instruments to advance chemical science and technology.grid technologies will substantially reduce the barrier to using computationalmodels to investigate chemical phenomena and to integrating data from varioussources into the models or investigations. virtual laboratories have already provento be an effective means of dealing with the rising costs of forefront instrumentsfor chemical research by providing capabilities needed by researchers not colocated with the instrumentsñall we need is a sponsor willing to push this technology forward on behalf of the user community.the twentyfirst century will indeed be an exciting time for chemical scienceand technology.37http://www.amc.anl.gov/.38http://www.ms.ornl.gov/htmlhome/mauc/magrem.htm.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.116appendix dsystems approaches in bioinformatics andcomputational genomicschristodoulos a. floudasprinceton universitythe genomics revolution has generated a plethora of challenges and opportunities for systems approaches in bioinformatics and computational genomics. theessential completion of several genome projects, including that of the humangenome, provided a detailed map from the gene sequences to the protein sequences. the gene sequences can be used to assist and/or infer the connectivitywithin or among the pathways. the overwhelmingly large number of generatedprotein sequences makes protein structure prediction from the amino acid sequence of paramount importance. the elucidation of the protein structures throughnovel computational frameworks that complement the existing experimental techniques provides key elements for the structurebased prediction of protein function. these include the identification of the type of fold, the type of packing, theresidues that are exposed to solvent, the residues that are buried in the core, thehighly conserved residues, the candidate residues for mutations, and the shapeand electrostatic properties of the fold. such elements provide the basis for thedevelopment of approaches for the location of active sites; the determination ofstructural and functional motifs; the study of proteinprotein and proteinligandcomplexes and proteindna interactions; the design of new inhibitors; and drugdiscovery through target selection, lead discovery and optimization. better understanding of the proteinligand and proteindna interactions will provide importantinformation for addressing key topology related questions in both the cellular metabolic and signal transduction networks. in this paper, we discuss two componentsof the genomics revolution roadmap: (1) sequence to structure, and (2) structure tofunction. in the first, after a brief overview of the contributions, we present astrofold, which is a novel ab initio, approach for protein structure prediction. in thesecond, we discuss the approaches for de novo protein design and present an integrated structural, computational, and experimental approach for the de novo designof inhibitors for the third component of complement, c3. we conclude with a summary of the advances and a number of challenges.sequence to structure: structure prediction in protein foldingstructure prediction of polypeptides and proteins from their amino acid sequences is regarded as a holy grail in the computational chemistry and molecularand structural biology communities. according to the thermodynamic hypothesis1 the native structure of a protein in a given environment corresponds to the1anfinsen, c. b. science 1973, 181, 223.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.christodoulos a. floudas117global minimum free energy of the system. in spite of pioneering contributionsand decades of effort, the ab initio prediction of the folded structure of a proteinremains a very challenging problem. the existing approaches for the protein structure prediction can be classified as : (1) homology or comparative modeling methods,2,3,4,5,6 (2) fold recognition or threading methods,7,8,9,10,11,12,13 (3) ab initiomethods that utilize knowledgebased information from structural databases (e.g.,secondary and/or tertiary structure restraints).14,15,16,17,18,19,20 and (4) ab initiomethods without the aid of knowledgebased information.21,22,23,24,25,26,27in the sequel, we introduce the novel astrofold approach for the abinitio prediction of the threedimensional structures of proteins. the four stagesof the approach are outlined in figure 1. the first stage involves the identificationof helical segments24 and is accomplished by partitioning the amino acid sequenceinto overlapping oligopeptides (e.g., pentapeptides, heptapeptides, nonapeptides).2bates, p. a., kelley, l. a.; maccallum, r. m.; sternberg, m. j. e. proteins 2001, s5, 3946.3shi, j. y.; blundell, t. l.; mizuguchi, k. j. mol. biol. 2001, 310, 243257.4sali, a.; sanchez, r. proc. natl. acad. sci. u.s.a. 1998, 95, 1359713602.5fischer, d. proteins 1999, s3.6alazikani, b., sheinerman, f. b.; honig, b. proc. natl. acad. sci. u.s.a. 2001, 98, 1479614801.7fischer, d.; eisenberg, d. proc. natl. acad. sci. u.s.a. 1997, 94, 1192911934.8skolnick, j.; kolinski, a. adv. chem. phys. 2002, 120, 131192.9mcguffin, l. j.; jones, d. t. proteins 2002, 48, 4452.10panchenko, a. r.; marchlerbauer, a.; bryant, s. h. j. mol. biol. 2000, 296, 13191331.11jones, d. t. proteins 2001, s5, 127132.12skolnick, j.; kolinski, a.; kihara, d.; betancourt, m.; rotkiewicz, p.; boniecki, m. proteins2001, s5, 149156.13smith, t. f.; loconte, l.; bienkowska, j.; gaitatzes, c.; rogers, r. g.; lathrop, r. j. comp.biol. 1997, 4, 217225.14ishikawa, k.; yue, k.; dill, k. a. prot. sci. 1999, 8, 716721.15pedersen, j. t.; moult, j. proteins 1997, s1, 179184.16eyrich, v. a.; standley, d. m.; friesner, r. a. adv. chem. phys. 2002, 120, 223264.17xia, y.; huang, e. s.; levitt, m.; samudrala, r. j. mol. biol. 2000, 300, 171185.18standley, d. m.; eyrich, v. a.; an, y.; pincus, d. l.; gunn, j. r.; friesner, r. a. proteins 2001,s5, 133139.19standley, d. m.; eyrich, v. a.; felts, a. k.; friesner, r. a.; mcdermott, a. e. j. mol. biol. 1999,285, 16911710.20eyrich, v.; standley, d. m.; felts, a. k.; friesner, r. a. proteins 1999, 35, 41.21pillardy, j.; czaplewski, c.; liwo, a.; wedemeyer, w. j.; lee, j.; ripoll, d. r.; arlukowicz, p.;oldziej, s.; arnautova, y. a.; scheraga, h. a. j. phys. chem. b 2001, 105, 72997311.22pillardy, j.; czaplewski, c.; liwo, a.; lee, j.; ripoll, d. r.; kazmierkiewicz, r.; oldziej, s.;wedemeyer, w. j.; gibson, k. d.; arnautova, y. a.; saunders, j.; ye, y. j.; scheraga, h. a. proc.natl. acad. sci. u.s.a. 2001, 98, 23292333.23srinivasan, r.; rose, g. d. proteins 2002, 47, 489495.24klepeis, j. l.; floudas, c. a. j. comp. chem. 2002, 23, 245266.25klepeis, j. l.; floudas, c. a. j.comp. chem. 2003, 24, 191208.26klepeis, j. l.; floudas, c. a. j. global optim. unpublished.27klepeis, j. l.; schafroth, h. d.; westerberg, k. m.; floudas, c. a. adv. chem. phys. 2002, 120,254457.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.118appendix dthe concept of partitioning the amino acid sequence into overlappingoligopeptides is based on the idea that helix nucleation relies on local interactionsand positioning within the overall sequence. this is consistent with the observation that local interactions extending beyond the boundaries of the helical segment retain information regarding conformational preferences.28 the partitioning pattern is generalizable and can be extended to heptapeptides, nonapeptides,figure 1overall flow chart for the ab initio structure prediction using astrofold.(global optimization & molecular dynamics)28baldwin, r. l.; rose, g. d. tibs 1999, 24, 7783.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.christodoulos a. floudas119or larger oligopeptides.29 the overall methodology for the ab initio prediction ofhelical segments encompasses the following steps:24 the overlappingoligopeptides are modeled as neutral peptides surrounded by a vacuum environment using the ecepp/3 force field.30 an ensemble of low potential energy pentapeptide conformations, along with the global minimum potential energy conformation, is identified using a modification of the global optimizationapproach31 and the conformational space annealing approach.32 for the set ofunique conformers z, free energies (fharvac) are calculated using the harmonicapproximation for vibrational entropy.31 the energy for cavity formation in anaqueous environment is modeled using a solventaccessible surface area expression fcavity = a + b, where a is the surface area of the protein exposed to thesolvent. for the set of unique conformers z, the total free energy ftotal is calculated as the summation of fharvac, fcavity, and fsolv, which represents the difference in polarization energies caused by the transition from a vacuum to a solvatedenvironment, and fionize, which represents the ionization energy. the calculationof fsolv and fionize requires the use of a poissonboltzmann equation solver.33 foreach oligopeptide, total free energy values (ftotal) are used to evaluate the equilibrium occupational probability for conformers having three central residues withinthe helical region of the  space. helix propensities for each residue are determined from the average probability of those systems in which the residue in question constitutes a core position.in the second stage, strands,  sheets, and disulfide bridges are identifiedthrough a novel superstructurebased mathematical framework originally established for chemical process synthesis problems.25,34 two types of superstructureare introduced, both of which emanate from the principle that hydrophobic interactions drive the formation of structure. the first one, denoted as hydrophobicresiduebased superstructure, encompasses all potential contacts between pairsof hydrophobic residues (i.e., a contact between two hydrophobic residues mayor may not exist) that are not contained in helices (except cystines, which areallowed to have cystinecystine contacts even though they may be in helices).the second one, denoted as strandbased superstructure, includes all possiblestrand arrangements of interest (i.e., a strand may or may not exist) in addition to the potential contacts between hydrophobic residues. the hydrophobicresiduebased and strandbased superstructures are formulated as mathematical29anfinsen, c.; scheraga, h. adv. prot. chem. 1975, 29, 205.30n”methy, g.; gibson, k. d.; palmer, k. a.; yoon, c. n.; paterlini, g.; zagari, a.; rumsey, s.;scheraga, h. a. j. phys. chem. 1992, 96, 6472.31klepeis, j. l.; floudas, c. a. j. chem. phys. 1999, 110, 74917512.32lee, j.; scheraga, h.; rackovsky, s. biopolymers 1998, 46, 103.33honig, b.; nicholls, a. science 1995, i, 111441149.34floudas, c. a. nonlinear and mixedinteger optimization; oxford university press: new york,ny, 1995.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.120appendix dmodels that feature three types of binary variables: (1) representing the existenceor nonexistence of contacts between pairs of hydrophobic residues; (2) denotingthe existence or nonexistence of the postulated strands; and (3) representing thepotential connectivity of the postulated strands. several sets of constraints inthe model enforce physically legitimate configurations for antiparallel or parallel strands and disulfide bridges, while the objective function maximizes the totalhydrophobic contact energy. the resulting mathematical models are integer linear programming (ilp) problems that not only can be solved to global optimality,but also can provide a rankordered list of alternate sheet configurations.25the third stage determines pertinent information from the results of the previous two stages. this involves the introduction of lower and upper bounds ondihedral angles of residues belonging to predicted helices or strands, as well asrestraints between the c atoms for residues of the selected sheet and disulfidebridge configuration. furthermore, for segments that are not classified ashelices or strands, freeenergy runs of overlapping heptapeptides are conductedto identify tighter bounds on their dihedral angles.24,27,35the fourth stage of the approach involves the prediction of the tertiary structure of the full protein sequence.26 formulation of the problem relies on the minimization of the energy using a full atomistic force field, ecepp/330 and on dihedral angle and atomic distance restraints acquired from the previous stage. toovercome the multiple minima difficulty, the search is conducted using the global optimization approach, which offers theoretical guarantee of convergenceto an global minimum for nonlinear optimization problems with twicedifferentiable functions.27,36,37,38,39this global optimization approach effectively brackets the global minimumby developing converging sequences of lower and upper bounds, which are refined by iteratively partitioning the initial domain. upper bounds correspond tolocal minima of the original nonconvex problem, while lower bounds belong tothe set of solutions of convex lower bounding problems, which are constructed byaugmenting the objective and constraint functions by separable quadratic terms.to ensure nondecreasing lower bounds, the prospective region to be bisected isrequired to contain the infimum of the minima of lower bounds. a nonincreasingsequence for the upper bound is maintained by selecting the minimum over all thepreviously recorded upper bounds. the generation of lowenergy starting points35klepeis, j. l.; pieja, m. t.; floudas, c. a. comp. phys. comm. 2003, 151, 121140.36adjiman, c. s.; androulakis, i. p.; floudas, c. a. computers chem. engng. 1998, 22, 11371158.37adjiman, c. s.; androulakis, i. p.; floudas, c. a. computers chem. engng. 1998, i 11591179.38adjiman, c. s.; androulakis, i. p.; floudas, c. a. aiche journal 2000, 46, 17691797.39floudas, c. a. deterministic global optimization: theory, methods and applications,nonconvex optimization and its applications; kluwer academic publishers: dordecht, 2000.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.christodoulos a. floudas121for constrained minimization is enhanced by introducing torsion angle dynamics40 within the context of the  global optimization framework.26two viewpoints provide competing explanations of the proteinfolding question. the classical opinion regards folding as hierarchic, implying that the processis initiated by rapid formation of secondary structural elements, followed by theslower arrangement of the tertiary fold. the opposing perspective is based on theidea of a hydrophobic collapse and suggests that tertiary and secondary featuresform concurrently. astrofold bridges the gap between the two viewpoints byintroducing a novel ab initio approach for tertiary structure prediction in whichhelix nucleation is controlled by local interactions, while nonlocal hydrophobicforces drive the formation of structure. the agreement between the experimentaland predicted structures (rmsd, root mean squared deviation: 46 † for segmentsup to 100 amino acids) through extensive computational studies on proteins up to150 amino acids reflects the promise of the astrofold method for generictertiary structure prediction of polypeptides.structure to function: de novo protein designthe de novo protein design relies on understanding the relationship betweenthe amino acid sequence of a protein and its threedimensional structure.41,42,43,44,45,46 this problem begins with a known protein threedimensionalstructure and requires the determination of an amino acid sequence compatiblewith this structure. at the outset the problem was termed the òinverse foldingproblemó46,47 since protein design has intimate links to the wellknown proteinfolding problem.48experimentalists have applied the techniques of mutagenesis, rational design, and directed evolution49,50 to the problem of protein design, and althoughthese approaches have provided successes, the searchable sequence space ishighly restricted.51,52 computational protein design allows for the screening of40gntert, p.; mumenthaler, c.; wthrich, k. j. mol. biol. 1997, 273, 283298.41ventura, s.; vega, m.; lacroix, e.; angrand, i.; spagnolo, l.; serrano, l. nature struct. biol.2002, 9, 485493.42neidigh, j. w.; fesinmeyer, r. m.; andersen, n. h. nature struct. biol. 2002, 9, 425430.43ottesen, j. j.; imperiali, b. nature struct. biol. 2001, 8, 535539.44hill, r. b.; degrado, w. f. j. am. chem. soc. 1998, 120, 11381145.45dahiyat, b. i.; mayo, s. l. science 1997, 278, 8287.46drexler, k. e. proc. natl. acad. sci. u.s.a. 1981, 78, 52755278.47pabo, c. nature 1983, 301, 200.48c. hardin, t. v. p.; lutheyschulten, z. curr. opin. struc. biol. 2002, 12, 176181.49bowie, j. u.; reidhaarolson, j. f.; lim, w. a.; sauer, r. t. science 1990, 247, 13061310.50moore, j. c.; arnold, f. h. nat. biotechnol. 1996, 14, 458467.51degrado, w. f.; wasserman, z. r.; lear, j. d. science 1989, 243, 622628.52hecht, m. h.; richardson, d. s.; richardson, d. c.; ogden, r. c. science 1990, 249, 884891.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.122appendix doverwhelmingly large sectors of sequence space, with this sequence diversitysubsequently leading to the possibility of a much broader range of properties anddegrees of functionality among the selected sequences. allowing for all 20 possible amino acids at each position of a small 50residue protein results in 2050combinations, or more than 1065 possible sequences. from this astronomical number of sequences, the computational sequence selection process aims at selectingthose sequences that will be compatible with a given structure using efficientoptimization of energy functions that model the molecular interactions. the firstattempts at computational protein design focused only on a subset of core residues and explored steric van der waalsbased energy functions, although overtime they evolved to incorporate more detailed models and interaction potentials.once an energy function has been defined, sequence selection is accomplishedthrough an optimization based search designed to minimize the energy objective.both stochastic53,54 and deterministic55,56 methods have been applied to the computational protein design problem. recent advances in the treatment of the protein design problem have led to the ability to select novel sequences given thestructure of a protein backbone. the first computational design of a full sequenceto be experimentally characterized was the achievement of a stable zincfingerfold () using a combination of a backbonedependent rotamer library withatomisticlevel modeling and a deadend eliminationbased algorithm.45 despitethese breakthroughs, issues related to the stability and functionality of these designed proteins remain sources of frustration.we have recently introduced a combined structural, computational, and experimental approach for the de novo design of novel inhibitors such as variants ofthe synthetic cyclic peptide compstatin.57 a novel twostage computational protein design method is used not only to select and rank sequences for a particularfold but also to validate the stability of the fold for these selected sequences. tocorrectly select a sequence compatible with a given backbone template that isflexible and represented by several nmr structures, an appropriate energy function must first be identified. the proposed sequence selection procedure is basedon optimizing a pairwise distancedependent interaction potential. a number ofdifferent parameterizations for pairwise residue interaction potentials exist; theone employed here is based on the discretization of alpha carbon distances into aset of 13 bins to create a finite number of interactions, the parameters of whichwere derived from a linear optimization formulated to favor native folds over53wernisch, l.; hery, s.; wodak, s. j. j. mol. biol. 2000, 301, 713736.54desjarlais, j. r.; handel, t. m. j. mol. biol. 1999, 290, 305318.55desmet, j.; maeyer, m. d.; hazes, b.; lasters, i. nature 1992, 356, 539542.56koehl, p.; levitt, m. nature struct. biol. 1999, 6, 108.57klepeis, j. l.; floudas, c. a.; morikis, d.; tsokos, c. g.; argyropoulos, e.; spruce, l.; lambris,j. d. 2002, submitted.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.christodoulos a. floudas123optimized decoy structures.58,59 the resulting potential, which involves 2730parameters, was shown to provide higher z scores than other potentials and placenative folds lower in energy.58,59the formulation allows a set of mutations for each position i that in thegeneral case comprises all 20 amino acids. binary variables yij and ykl can beintroduced to indicate the possible mutations at a given position. that is, the yijvariable will indicate which type of amino acid is active at a position in the sequence by taking the value of 1 for that specification. the objective is to minimize the energy according to the amino acid pair and distance dependent energyparameters that multiply the binary variables. the composition constraints require that there is at most one type of amino acid at each position. for the generalcase, the binary variables appear as bilinear combinations in the objective function. this objective can be reformulated as a strictly linear (integer linear programming) problem.57 the solution of the ilp problem can be accomplishedrigorously using branch and bound techniques,34 making convergence to the global minimum energy sequence consistent and reliable. finally, for such an ilpproblem it is straightforward to identify a rankordered list of the lowlying energy sequences through the introduction of integer cuts34 and repetitive solutionof the ilp problem.once a set of lowlying energy sequences has been identified via the sequence selection procedure, the fold validation stage is used to identify an optimal subset of these sequences according to a rigorous quantification of conformational probabilities. the foundation of the approach is grounded in thedevelopment of conformational ensembles for the selected sequences under twosets of conditions. in the first circumstance the structure is constrained to vary,with some imposed fluctuations, around the template structure. in the second casea freefolding calculation is performed for which only a limited number of restraints are likely to be incorporated, with the underlying template structure notbeing enforced. the distance constraints introduced for the templateconstrainedsimulation can be based on the structural boundaries defined by the nmr ensemble, or simply by allowing some deviation from a subset of distances provided by the structural template; hence they allow for a flexible template on thebackbone.the formulations for the folding calculations are reminiscent of structureprediction problems in protein folding.26,27 in particular, a novel constrained global optimization problem first introduced for structure prediction of compstatinusing nmr data,60 and later employed in a generic framework for the structureprediction of proteins, is utilized.26 the folding formulation represents a general58tobi, d.; elber, r. proteins 2000, 41, 4046.59tobi, d.; shafran, g.; linial, n.; elber, r. proteins 2000, 40, 7185.60klepeis, j. l.; floudas, c. a.; morikis, d.; lambris, j. d. j. comp. chem. 1999, 20, 13541370.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.124appendix dnonconvex constrained global optimization problem, a class of problems forwhich several methods have been developed. in this work, the formulations aresolved via the deterministic global optimization approach, a branch andbound method applicable to the identification of the global minimum of nonlinearoptimization problems with twicedifferentiable functions.27,36,37,38,39,60,61in addition to identifying the global minimum energy conformation, the global optimization algorithm provides the means for identifying a consistent ensemble of lowenergy conformations.35,61 such ensembles are useful in derivingquantitative comparisons between the free folding and templateconstrained simulations. the relative probability for template stability ptemp is calculated by summing the statistical weights for those conformers from the freefolding simulationthat resemble the template structure and dividing this sum by the summation ofstatistical weights over all conformers.compstatin is a 13residue cyclic peptide and a novel synthetic complementinhibitor with the prospect of being a candidate for development as an importanttherapeutic agent. the binding and inhibition of complement component c3 bycompstatin is significant because c3 plays a fundamental role in the activation ofthe classical, alternative, and lectin pathways of complement activation. althoughcomplement activation is part of the normal inflammatory response, inappropriate complement activation may cause host cell damage, which is the case in morethan 25 pathological conditions, including autoimmune diseases, stroke, heartattack, and burn injuries.62 the application of the discussed de novo design approach to compstatin led to the identification of sequences with predicted sevenfold improvements in inhibition activity. these sequences were subsequently experimentally validated for their inhibitory activity using complement inhibitionassays.57summary and challengein the two components of the genomics revolution: (1) sequence to structure,and (2) structure to function, we discussed two significant advances. the firstone, the ab initio structure prediction approach astrofold, integrates thetwo competing points of view in protein folding by employing the thesis of localinteractions for the helical formation and the thesis for hydrophobichydrophobicresidue interactions for the prediction of the topology of sheets and the locationof disulfide bridges. astrofold is based on novel deterministic global optimization and integer linear optimization approaches. the second advance, a novelapproach for de novo protein design, introduces in the first stage an explicit mathematical model for the in silico sequence selection that is based on distance61 klepeis, j. l.; floudas, c. a. j. chem. phys. 1999, 110, 74917512.62sahu, a.; lambris, j. d. immunol. rev. 2001, 180, 3548.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.richard friesner125dependent force fields and a rigorous treatment of the combinatorial optimizationproblem, while in the second stage, full atomisticlevel folding calculations areintroduced to rank the proposed sequences, which are subsequently validated experimentally.several important challenges exist; these include new methods for the protein structure prediction that will consistently attain resolutions of about 46 † forall, all,, and / proteins of medium to large size; new approaches thatwill lead to resolution of protein structures comparable to the existing experimental techniques; novel global optimization methods for sampling in the tertiarystructure prediction and refinement; new approaches for the packing of helices inglobular and membrane proteins; new computational methods for the structureprediction of membrane proteins; improved methods for proteinprotein and proteindna interactions; new methods for the determination of active sites andstructural and functional motifs; new methods for protein function prediction;new approaches for the design of inhibitors; and systemsbased approaches forimproved understanding of gene regulatory metabolic pathways and signal transduction networks.acknowledgmentsthe author gratefully acknowledges financial support from the national science foundation and the national institutes of health (r01 gm52032).modeling of complex chemical systemsrelevant to biology and materials science:problems and prospectsrichard friesnercolumbia universityoverviewin this paper, i discuss the future of computational modeling of complex,condensedphase systems over the next decade, with a focus on biological modeling and specifically structurebased drug design. the discussion is organized asfollows. first, i review the key challenges that one faces in carrying out accuratecondensed phase modeling, with the analysis centered on the core technologiesthat form the essential ingredients of a simulation methodology. next, i examinethe use of molecular modeling in structurebased drug design applications. in mypresentation, i briefly discussed issues associated with software development,and space limitations do not allow elaboration on that area.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.126appendix dcore technologiesi focus here on a condensedphase simulation problem involving the interactions of macromolecular structures, small molecules, and solvent. periodic solidstate systems have their own special set of difficulties and simplifications, whichi do not discuss. the problems of protein structure prediction, proteinligand binding, and enzymatic catalysis, which are discussed in the next section, fall into thiscategory.any condensedphase simulation protocol that is going to address the aboveproblems requires three fundamental components:1.a function describing the energetics of the macromolecule(s) and smallmolecule(s) as a function of atomic positions. such a function can involve directquantum chemical computation (e.g., via density function theory or mp2 secondorder perturbation theory methods), a molecular mechanics force field, a mixedqmmm model, or an empirical scoring function parameterized against experimental data.2.a description of the solvent. if explicit solvent simulation is to be employed, this can simply be another term in the molecular mechanics force field,for example. however, explicit solvent models are computationally expensiveand can introduce a substantial amount of noise into the evaluation of relativeenergies unless extremely long simulation times are used. therefore, considerable effort has been invested in the development of continuum solvent models,which are relatively inexpensive to evaluate (typically between one and two timesthe cost of a gasphase force field evaluation for one configuration) and canachieve reasonable accuracy.3.a protocol for sampling configuration space. if structure prediction isdesired, then the objective is to find the minimum freeenergy configuration. ifone is calculating binding affinities or rate constants, some kind of statisticalaveraging over accessible phase space configurations is typically required. thetotal cost of any given calculation is roughly the number of configurations required to converge the phase space average (or to locate the global minimum)multiplied by the computational cost of evaluating the energy (including the contribution of the solvent model if continuum solvation is employed) per configuration. thus, a key problem is to reduce the number of required configurations bydevising more effective sampling methods.the ability to treat realworld condensedphase simulation problems, suchas structurebased drug design, is dependent on making progress in each ofthese three areas, to the point where none of them represents an unavoidablebottleneck to achieving adequate accuracy in reasonable cpu times. we discuss each area briefly below, summarizing the current state of the art and prospects for the future.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.richard friesner127energy modelsquantum chemical methods. quantum chemistry represents the most fundamental approach to computation of the energy of any given atomic configuration.if the electronic schrıdinger equation is solved accurately, the correct answerwill be obtained. since an exact solution is not possible for systems containingmore than one electron, the problem here is to solve the schrıdinger equationwith approximations that are tractable and yet yield good quantitative accuracy.following is a summary of the most useful methods that have been developed fordoing this:1.density function theory provides respectable results for bond energies (23 kcal/mol), activation barriers (34 kcal/mol), conformational energies (0.51.5kcal/mol), and hydrogen bonding interactions (0.51 kcal/mol) with a scaling withsystem size in the nn2 regime (n being the number of electrons in the system). acrucial strength of dft is its ability to deliver reasonable results across the periodic table with no qualitative increase in computational effort. dft is currentlythe method of choice for calculations involving reactive chemistry of large systems, particularly those containing transition metals, and is useful for a widerange of other calculations as well. systems on the order of 300500 atoms can betreated using current technology.2.secondorder perturbation theory (mp2) can provide more accurate conformational energies, hydrogen bond energies, and dispersion energies thandft (which fails completely for the last of these, at least in current functionals).this makes it the method of choice for computing conformational or intermolecular interactions and developing force fields. the computational scaling is inthe n2 n3 range. systems on the order of 100200 atoms can be treated usingcurrent technology. the use of large basis sets and extrapolation to the basis setlimit is necessary if high accuracy is to be achieved for the properties discussedabove.3.coupled cluster methods (ccsd (t) in particular) provide highaccuracyresults (often within 0.1 kcal/mol) for many types of molecules (e.g., organicmolecules), but have more difficulties with transition metalcontaining species.the method scales as n7 and at present can conveniently be applied only to smallmolecules, where it is however quite valuable in producing benchmark results.overall, current quantum chemical methods are adequate for many purposes.improvements in speed and accuracy are ongoing. my personal view is that thisdoes not represent the bottleneck in accurate predictive condensed phase simulations. if one accepts the idea that entirely quantum chemical simulations are notthe optimal approach for most problems (and almost certainly not for biologicalproblems), current methods perform well in the context of qmmm modelingand in producing results for force field development.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.128appendix dmolecular mechanics force fields. there are a number of different philosophies that have been the basis for various force field development efforts. somegroups have relied primarily on fitting parameters to experimental data, whileothers have introduced a larger component of ab initio quantum chemical data.my view is that extensive use of ab initio data is necessary if one is going toachieve high accuracy and broad coverage of chemical space. experimental datacan be used in conjunction with ab initio data and also used to test the resultingmodels.there are two basic issues associated with force field development. the firstis the functional form to be used. the simplest cases involve building models fororganic molecules that do not consider reactive chemistry. in this case, the standard models (valence terms for stretches, bends, torsions, electrostatic, and vander waals terms for intermolecular interactions) are sufficient, with the provisothat the electrostatics should be described by higher order multipoles of some sort(as opposed to using only atomic point charges) and that polarization should beintroduced explicitly into the model. simplified models that do not include polarization or higher multipoles appear to yield reasonable structural predictions butmay not be able to provide highly precise energetics (e.g., binding affinities, although the improvements attainable with a more accurate electrostatic description have yet to be demonstrated). the second problem is achieving a reliable fitto the quantum chemical and experimental data. this is a very challenging problem that is tedious, labor intensive, and surprisingly difficult technically. while ibelieve that better solutions will emerge in the next decade, this is an area thatdefinitely could benefit from additional funding.current force fields are lacking in both accuracy and coverage of chemicalspace. improvements in the past decade have been incremental, although real.with current levels of funding, continued incremental improvement can be expected. largerscale efforts (which perhaps will be financed in private industryonce people are convinced that bottom line improvements to important problemswill result) will be needed to produce improvement beyond an incremental level.empirical models. the use of empirical models is widespread in bothacademia and industry. there are for example a large number of òstatisticalópotential functions that have been developed to address problems such as proteinfolding or proteinligand docking. the appeal of such models is that they can bedesigned to be computationally inexpensive and can be fitted directly to experimental data to hopefully yield immediately meaningful results for complex systems. the challenge is that systematic improvement of empirical models beyonda certain point is extremely difficult and becomes more difficult as the basis forthe model becomes less physical.a recent trend has been to combine approaches having molecular mechanicsand empirical elements; an example of this is bill jorgensenõs work on the cominformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.richard friesner129putation of binding free energies. this is a direction in which much more workneeds to be done, much of it of an exploratory variety. in the end, it is unlikelythat brute force simulations will advance the solution of practical problems in thenext decade (in another 1020 years, this may actually be a possibility). a combination of physically meaningful functional forms and models with empirical insight gained from directly relevant experiments is more likely to work. computational chemistry needs to become a full partner with experiment, not try to replaceitñthe technology to do that simply is not there at this time.solvation modelsi concentrate here on continuum models for aqueous solution, although thebasic ideas are not very different for other solvents. a great deal of progress hasbeen made in the past decade in developing continuum solvation models based onsolution of the poissonboltzmann (pb) equation, as well as approximations tothis equation such as the generalized born (gb) model. these approaches properly treat longrange electrostatic interactions and as such are significantly moreaccurate than, for example, surfaceareabased continuum solvation methods.progress has also been made in modeling the nonpolar part of the solvation model(i.e., cavity formation, van der waals interactions) for both small molecules andproteins.the current situation can be summarized as follows:¥computational performance of modern pb and gbbased methods isquite respectable, no more than a factor of 2 more expensive than a gasphasecalculation.¥accurate results for small molecules can be obtained routinely by fittingexperimental data when available.¥the significant issues at this point revolve around the transferability ofthe parameterization to larger structures. how well do continuum methods describe a small number of waters in a protein cavity, for example, or solvationaround a salt bridge (provided that the individual functional groups are well described in isolation)? our most recent results suggest that there are cases in whicha small number of explicit waters is essential and that the transferability problemhas not yet been completely solved.the above problems, while highly nontrivial, are likely to be tractable in thenext decade. thus, my prediction would be that given sufficient resources, robustcontinuum solvent models (using a small number of explicit waters when necessary) can and will be developed and that this aspect of the model will cease tolimit the accuracy. however, a major investment in this area (public or private) isgoing to be necessary if this is to be accomplished.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.130appendix dsampling methodsa wide variety of sampling methods are being developed to address the problems of both global optimization and freeenergy calculations. traditional methods using molecular modeling programs include molecular dynamics, montecarlo, conformational searching, and freeenergy perturbation techniques. morerecently, there have been many new ideas including parallel tempering, replicaexchange, quantum annealing, potential smoothing techniques, and so forth. testsof the newer methods on realistic problems (as opposed to the model problemswhere they typically are validated initially) have not yet been extensive, so we donot really know which approaches will prove optimal. it does seem likely, however, that significant progress is going to be made beyond that provided by fasterprocessors.sampling is a key bottleneck at present in obtaining accurate results in molecular modeling simulations. obtaining convergence for a complex condensedphase system is extremely challenging. this is the area in my opinion whereprospects are most uncertain and where it is critical to support a lot of new ideasas opposed to just improved engineering of existing approaches. some advanceswill come about from faster hardware, but algorithmic improvement should contribute even more if sufficient effort is applied. until we can converge the sampling, it is going to be very difficult to improve the potential functions and solvation models reliably using experimental input, because there are always questionsabout whether the errors are due to incomplete sampling as opposed to the modelitself.structurebased drug designthe key problems in structurebased drug design can be enumerated as follows:1.generation of accurate protein structures, whether in a homology modeling context or simply enumerating lowenergy structures given a highresolutioncrystal structure. the use of a single protein conformation to assess ligand binding is highly problematic if one wants to evaluate a large library of diverse ligandsso as to locate novel scaffolds capable of supporting binding. this is particularlythe case for targets such as kinases where there is considerable mobility of loopsand side chains in the active site of the protein.progress on this problem will come from the ability to rapidly samplepossible conformations using an accurate energy functional and continuum solvation model. there is good reason to believe that this can be accomplished in thenext three to five years. while perfect accuracy will not be achieved, the generation of structures good enough to dock into and make predictions about binding isinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.richard friesner131a realistic possibility. experimental data can be used to refine the structures oncegood initial guesses can be made.2.highthroughput docking of ligands into the receptor for lead discovery.the first objective is to correctly predict the binding mode of the ligand in thereceptor. considerable progress has been made in this direction over the pastdecade, and continued improvement of the existing technology, as well as use offlexible protein models when required, should yield successful protocols in thenext 35 years. the second objective is scoring of the candidate ligandsñat thisstage, one simply wants to discriminate active from inactive compounds. to dothis rapidly, an empirical component is required in the scoring. this in turn necessitates the availability of large, reliable datasets of binding affinities. atpresent the quality and quantity of the publicly available data is problematic; amuch greater quantity of data resides in a proprietary form in pharmaceutical andbiotechnology companies, but this is inaccessible to most of those working on thedevelopment of scoring functions. this problem must be addressed if more robust and accurate scoring functions are to be produced. given the availability ofsufficient data, it seems likely that excellent progress can be made over the nextseveral years.3.accurate calculation of binding free energies for lead optimization. froma technical point of view, this is the most difficult of the problems that have beenposed. at present we do not really know whether the dominant cause of errors inrigorous binding affinity computation should be attributed to the potential functions, solvation model, or sampling; perhaps all three are contributors at a significant level. all one can do is attempt to improve all three components and carryout tests to see whether better agreement with experiment is obtained. my ownintuition is that on a 5 to 10year time scale, there will be some real successes inthis type of calculation, but achieving reliability over a wide range of chemistriesand receptors is going to be a great challenge.overall, there is a good probability that in the next 510 years, computationalmethods will make increasingly important contributions to structurebased drugdesign and achieve some demonstrable successes.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.132appendix dthe current state of research ininformation and communicationsjames r. heathuniversity of california, los angelesthis paper focuses on an area of nanoelectronics to which chemists have beenmaking substantial contributions in the past five to ten years. it discusses what thechemical challenges are and what the state of the art is in the field of nanoit.we have recently prepared a series of aligned semiconductor (silicon) wires,5 nm in diameter. these are at 16 nm pitch, and 5 mm long; the aspect ratio is 107.this means that if a crossbar junction of the silicon were pdoped at a reasonablelevel, there would be only a 1% chance of the dopant atom being at the wirecrossing. consequently, if the electronics of doped silicon could be brought downto these dimensions, the statistical fluctuations in the density simply mean thatclassical electronics analysis would failñone cannot use ohmic assumptions,because one does not know the doping level locally. from a classical computingviewpoint, there are major problems in working at this scale. each device nolonger acts like every other device, leakage currents can dominate in thenanodimension area, and they can cause tremendous parasitic power losses due tovery thin gate widths.one can use molecules to augment the capabilities of the cross wires. it is notdifficult to make small structures down to 60 nm, but it is difficult to bring themclose together, so density becomes the most challenging synthetic capability.work in molecular electronic construction of logic and memory devices hasadvanced substantiallyñthe hewlett packard (hp) group a year ago prepared a64bit electronicbased randomaccess memory. our group at the university ofcalifornia, los angeles, and the california institute of technology has also madea randomaccess memory, but it is substantially smaller (and fits within a singlebit of the work reported by hp). crossbar structures can still be used for logic.threeterminal devices can be developed at a single molecule level, and we havedone so. i believe that these structures will be the way in which chemists canextrapolate structureproperty relationships that go back to synthesis and simplereduced dimensionality circuits.there are major problems involved in working at this scaleñthe traditionalconcepts of marcus theory may fail because there is no solvent to polarize. traditional analytical methodology such as nmr, mass spectrometry, and optical techniques are very difficult to use on this length scale. the direct observables are theconductance in twoterminal devices and the gated conductance in threeterminaldevices. using these, we have been able to analyze hysteresis loops and cyclingdata to indicate that the molecules are stable. very recent work in our group hasshown that one can prepare threemolecule gates and analyze the behavior ofthese molecular quantum dots that also have the internal structures.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.dimitrios maroudas133other concepts become complicated. is it clear that the molecules behaveohmically in such junctions? how do the energies align? how does gating workwithin this molecular structure? there are problems with organics, which havemobilities that are many orders of magnitude lower than that of singlecrystalsilicon. consequently, there will be a major challenge in making molecules behave like silicon.the molecular junctions that we have prepared have several advantages: first,they are almost crystallike, and therefore it seems that they could be chemicallyassembled. second, they are quite tolerant of defective components, and are therefore appropriate for the world of chemistry, where reactions never go 100 percent.both the hp structure and the structures that we have prepared are reallyextremely dense: ours are roughly 1012 bits per centimeter. the volume density ofinformation storage in the brain is roughly 1012 per cubic centimeter, so the density of these molecular structures is extremely high.finally, it is important to think about architectures. molecular devices havenow been demonstrated, and the fabrication of several molecular devices hasbeen clarified over the past five years. however, architecture is more complicated, and it requires the fabrication, alignment, interaction, and behavior of manydevices. key questions are now being addressed by scientists in places such ascarnegie mellon, stanford, hp, and caltech: how big should the memory be?how big should the wires be? how much should be devoted to routing? and howmuch gain needs to be put in?some of the structures that i showed you of crossbars had 2200 junctions;they were all made; there werenõt any broken components. so at the level of 104bits, it seems possible to do highfidelity fabrication. this is the start of a truemolecular electronics.multiscale modelingdimitrios maroudasuniversity of massachusettsin the next decade, multiscale modeling will be a very important area of chemical science and technologyñin terms of both needs and opportunities. the field hasemerged during the past 10 years in response to the need for an integrated computational approach toward predictive modeling of systems that are both complex andcomplicated in their theoretical description. the applications to date have involvedcomplexity in physical, chemical, or biological phenomena or have addressed problems of material structure and function. the intrinsic characteristics of all suchsystems are the multiplicity of length scales due to, e.g., multiple structural features, multiplicity of time scales due to multiple kinetic phenomena that governprocessing or function, the strong nonlinear response as these systems operate awayinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.134appendix dfrom equilibrium, and the large number or broad range of operating parametersthat are involved in practical engineering applications.from the viewpoint of theory and computation, the major challenge in thisarea will be to establish rigorous links between widely different theoretical formalismsñquantum mechanics, classical statistical mechanics, continuum mechanics, and so onñthat span a very broad range of space and time scales and areused to explore broad regions of parameter space. the practical goal will be toderive, as rigorously as possible, relationships between processes, structure, function, and reliability, and to use them to develop optimal engineering strategies.the core capabilities of multiscale modeling consist of computational methods that have been developed over many decades and are now used to computeproperties and model phenomena. figure 1 illustrates some of these methods in aschematic diagram of length versus time: computational quantum mechanicsfor accurate calculation of properties and investigation of small and fast phenomena, statistical mechanics for semiempirical modeling for mechanistic understanding and, at the much larger scale, continuum mechanics for macroscopicmodeling. between these last two is the mesoscopicmicrostructural scale, whichhas been an important motivator for the development of multiscale modeling techniques in the area of materials science. ultimately, what one would like to do,from an engineering viewpoint at least, is use all these methodologies to explorevast regions of parameter space, identify critical phenomena, promote criticalphenomena that improve the behavior of a system, and avoid critical phenomenathat lead to failure.continuum mechanics:¥finiteelement & difference methods¥boundaryintegral methodsmacroscopic modelingmodeling for mechanisticunderstandingstatistical mechanics:¥semiempirical hamiltonians¥molecular statics, lattice dynamics,molecular dynamics, monte carloquantum mechanics:¥ab initio, electronic structure¥density functional theory, ¥first principles molecular dynamicsaccurate calculations ofmaterials propertieslengthtimeprocess simulation:¥equationbased models¥control and optimizationprocessing units/facilitiessupply chain modeling:¥planning & scheduling¥mixed integer linear programmingglobal logisticsmesoscopic scale:¥coarsegrained quantum and statistical mechanics¥mixed/coupled atomisticcontinuum methodsmesoscale modeling figure 1modeling elements and core capabilities.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.dimitrios maroudas135strict requirements must be imposed if multiscale modeling is to become apowerful predictive tool. in particular, we need to deal with issues of accuracyand transferability through connection to first principles because phenomenological models are not transferable between widely different environments. thequestion is unavoidable: is system level analysisñstarting from first principlesñfeasible for complex systems? in certain cases, the realistic answer would be animmediate òno.ó in general, the optimistic answer is ònot yet,ó but the stage hasbeen set for tremendous progress in this direction over the next decade. in addition to developing novel, robust multiscale computational methods, fundamentalmechanistic understanding will be invaluable in order to enable computationallyefficient schemes and to steer parametric studies and design of experiments.i suggest classifying the approaches for multiscale modeling into two categories:1.serial strategies: differentscale techniques are implemented sequentiallyin different computational domains at different levels of discretization.2.parallel strategies: differentscale techniques are implemented simultaneously in the same computational domain that is decomposed appropriately.a number of significant trends are noteworthy:¥a variety of multispacescale techniques are emerging from efforts ondifferent applications where phenomena at the mesoscopic scale are important.¥efforts to push the limits of the core capabilities continue as illustrated bymoleculardynamics (md) simulations with hundreds of millions of atoms.¥kinetic monte carlo (kmc) methods are used increasingly to extend thetimescale limitation of atomistic simulations. these, in turn, are driving the creation of methodology for properly treating structural complexity in kmc schemesand for assessing the completeness and accuracy of the required database, i.e.,determining whether all of the important kinetic phenomena are included andaccurately calculating transition probabilities.¥methods are being developed for accelerating the dynamics of rareeventsñeither by taking advantage of physical insights about the nature of transition paths or by using numerical methods to perform atomistic simulations forshort periods and then project forward over large time steps.¥multiscale models are making possible both the integration of insight between scales to improve overall understanding and the integration of simulationswith experimental data. for example, in the case of plastic deformation of metals,one can incorporate constitutive theory for plastic displacements into macroscopicevolution equations, where parameterization of the constitutive equations is derived from analysis of md simulations.¥methods are being explored for enabling microscopic simulators to perform systemlevel analysisñmainly numerical bifurcation and stability analyinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.136appendix dsisñto predict and characterize system instabilities and effectively compute thestability domains of the system. the central question is, can we predict the onsetof critical phenomena that lead to phase, structural, or flow transitions? suchtransitions may lead to function improvement (e.g., prediction of disordertoorder transitions, or failure).in conclusion, over the past decade, various multiscale methods and modelshave been developed to couple widely different length scales, accelerate rareevent dynamics, and explore the parametric behavior of complex systems. thesemultiscale methods have been applied successfully to various problems in physical chemistry, chemical engineering, and materials science. over the next decade, these and other new methods will enable truly predictive analyses of complex chemical, material, and biological systems. these will provide powerful toolsfor fundamental understanding, as well as technological innovation and engineering applications. the development of multiscale methods will generate tremendous research opportunities for the chemical sciences, and the integration ofmultiscale methods with advances in software and highperformance computingwill be strategically important. the new opportunities also will present an educational challengeñto enable students, researchers, and practitioners to understanddeeply what is going on throughout the physical scales and parameter space sothey can develop intuition and understanding of how best to carry out simulationsof complex systems.the coming of age of computational scienceslinda r. petzolduniversity of california, santa barbarathe workshop program overstated the title of my contribution. rather thanòthe coming age of computational sciences,ó i focus on this disciplineõs coming of age. by this, i mean that computational science is like a teenager who hasa large potential for rapid growth and a great future but is in the midst of veryrapid change. it is like a love affair with the sciences and engineering that is reallygoing to be great.i explain how i see computational scienceñwhat it is, where it is going, andwhat is driving the fundamentals that i believe are taking place, and finally, howall this relates to you and to the chemical sciences.i start with some controversial views. many of us see computation as thethird mode of science, together with theory and experiment, that is just coming ofage. what could be driving that? other presentations at this workshop have notedthe tremendous growth of the capabilities of both hardware and algorithms. theni discuss what i see as the emerging discipline of computational science andinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.linda r. petzold137engineeringñor maybe itõs the antidiscipline, depending on how one looks at it.i hope that discipline and antidiscipline donõt interact in the same way as matterand antimatter!next i turn to the revolution in engineering and the sciences at the microscale, what this means for computational science, the implications for the kindsof tools we are going to need, and what is already happening in response to therapidly increasing application of engineering to biology. finally, i address somecurrent trends and future directions, one of which is going beyond simulation. inmany cases, simulation has been successful, but now people are getting moreambitious. as algorithms and software get better and the computer gets faster, wewant to do moreñand this will be a big theme in computational science. whatwill be the role of computer science in the future of computational science?figure 1 illustrates computational speedup over the last 30 years. the lowerfigure 1speedup resulting from software and hardware developments. (updated fromcharts in grand challenges: high performance computing and communications, executive office of the president, office of science and technology policy committee on physical, mathematical and engineering sciences, 1992; siam working group on cse education, siam rev., 2001, 43:1, 163177).information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.138appendix dplot, which shows speedup (relative to computer power in 1970) derived fromsupercomputer hardware, is just another restatement of mooreõs law.1 the threelast data points are asci red and asci white (the big doe machines) and thefamous earth simulator system (ess).the ess is right on schedule for the computer power that we would expectbased on the trends of the last 30 years. the architecture is vector parallel, so itõsa combination of the kind of vectorization used 20 years ago on craysupercomputers, with the parallelization that has become common on distributedmachines over the last decade. itõs no surprise that this architecture leads to a fastcomputer. probably the main reason that itõs newsworthy is that it was built injapan rather than the united states. politicians may be concerned about that, butas scientists i think we should just stay our course and do what we feel is right forscience and engineering.the upper graph in figure 1 is my favorite because it is directly related to myown work on algorithms. it shows performance enhancement derived from computational methods, and it is based on algorithms for solving a special problem atthe heart of scientific computationñthe solution of linear systems. the datesmay seem strangeñfor example, gaussian elimination came earlier than 1970ñbut they illustrate when these algorithms were introduced into productionlevelcodes at the doe labs. once again, the trend follows mooreõs law.my view on what these numbers mean to us, and why computational scienceand engineering is coming of age, relates to the massive increases in both computer and algorithm power. in many areas of science and engineering, the boundary has been crossed where simulation, or simulation in combination with experiment, is more effective in some combination of time, cost, and accuracy, thanexperiment alone for real needs. in addition, simulation is now a key technologyin industry. at a recent conference, i was astonished to see the number of companies using computer applications to address needs in chemical processing. thereis also a famous example in my field, the design of the boeing 777ñan incredibly complex piece of machineryñin which simulations played a major role.the emerging discipline of computational science and engineering is closelyrelated to applied mathematics (figure 2). there were early arguments that itshouldnõt be called mathematics but applied mathematicsñillustrating the disciplinary sensitivities of the fieldsñbut computer science and engineering alsooverlaps strongly with science, engineering, and computer science. it lies in thetwilight zone between disciplines. this is a great area of opportunity, becausethere is a lot of room there for growth!1moore originally stated that òthe complexity for minimum component costs has increased at arate of roughly a factor of two per year,ó moore, g. e., electronics 1965, 38 (8) 11417; this has beenrestated as òmooreõs law, the doubling of transistors every couple of years,ó (http://www.intel.com/research/silicon/mooreslaw.htm).information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.linda r. petzold139in an academic setting, itõs easy to become mesmerized by disciplinaryboundaries. we identify with those in our own discipline, but here iõm just looking from the outside. some have suggested that the structure is like a religion, soyour department would be like a branch of the church of science. consequently, itfeels a little bit heretical to suggest that there is a lot of value in integrating thedisciplines. nevertheless, i believe there is a lot of science and technology to bedone somewhere in that murky area between the disciplines.the major development that is driving change, at least in my world, is therevolution at the micro scale. many people are working in this area now, andmany think it will be as big as the computer revolution was. of particular importance, the behavior of fluid flow near the walls and the boundaries becomes critical in such small devices, many of which are built for biological applications. wehave large molecules moving through small spaces, which amounts to movingdiscrete molecules through devices. the models will often be discrete or stochastic, rather than continuous and deterministicña fundamental change in the kindof mathematics and the kind of software that must be developed to handle theseproblems.appliedmathematicscomputersciencescience andengineeringcsefigure 2computer science and engineering (cse) focuses on the integration ofknowledge and methodologies from computer science, applied mathematics, and engineering and science.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.140appendix dfor the engineering side, interaction with the macro scale world is alwaysgoing to be important, and it will drive the multiscale issues. important phenomena occurring at the micro scale determine the behavior of devices, but at thesame time, we have to find a way to interact with those devices in a meaningfulway. all this must happen in a simulation.many multiscale methods have been developed across different disciplines.consequently, much needs to be done in the fundamental theory of multiscalenumerical methods that applies across these disciplines. one method is famous instructural materials problems: the quasicontinuum method of tadmor, ortiz, andphilips.2 it links the atomistic and continuum models through the finite elementmethod by doing a separate atomist structural relaxation calculation on each cellof the finite element method mesh, rather than using empirical constitutive information. thus, it directly and dynamically incorporates atomisticscale information into the deterministic scale finite element method. it has been used mainly topredict observed mechanical properties of materials on the basis of their constituent defects.another approach is the hybrid finite elementmolecular dynamicsquantummechanics method, attributed to abraham, broughton, bernstein, and kaxiras.3this method is attractive because it is massively parallel, but itõs designed forsystems that involve a central defective region, surrounded by a region that isonly slightly perturbed from the equilibrium. therefore it has limitations in thesystems that it can address. a related hybrid approach was developed by nakano,kalia, and vashista.4 in the totally different area of fluid flow, people have beenthinking about these same things. there has been adaptive mesh and algorithmrefinement, which in the continuum world has been very successful. itõs a highlyadaptive way of refining the mesh that has been useful in fluid dynamics. theseresearchers have embedded a particle method within a continuum method at thefinest level (using an adaptive method to define the parts of the flow that must berefined at the smaller scale) and applied this to compressible fluid flow.finally, one can even go beyond simulation. for example, ioanniskevrekidis5 has developed an approach to computing stability and bifurcationanalysis using time steppers, in which the necessary functions are obtained directly from atomisticscale simulations as the overall calculation proceeds. this2shenoy, v. b.; miller, r.; tadmor, e. b.; phillips, r.; ortiz m. physical review letters 1998, 80,742745; miller, r.; tadmor, e. b., journal of computeraided materials design 2003, in press.3abraham, f. f.; broughton, j. q.; bernstein, n.; kaxiras, e. computers in physics 1998, 12, 538546.4nakano, a.; kalia, r. k.; vashishta, p. vlsi design 1998, 8, 123; nakano, a.; kalia, r. k.;vashishta, p.; campbell, t. j.; ogata, s.; shimojo, f.; saini, s. proceedings of supercomputing 2001(http://www.sc2001.org).5kevrekidis, y. g.; theodoropoulos, k.; qian, y.h. proc. natl. acad. sci. u.s.a. 2000, 97, 9840.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.linda r. petzold141is difficult because it must take into account the boundary conditions on the smallscale simulations. there are so many of these multiscale problems and algorithmsthat the new multiscale modeling and simulation journal is devoted entirely tomultiscale issues.another big development is taking place where engineering meets biology(box 1). these two different worlds will vastly augment each other, but whatdoes it mean for computation? a huge number of multiscale problems exist alongwith problems that involve understanding and controlling highly nonlinear network behavior. at a recent meeting in the area of systems biology, someonesuggested that it would require 140 pages to draw a diagram for the networkbehavior of e. coli. a major issue is uncertainty. the rate constants are unknown,and frequently the network structure itself is not well understood. we will need tolearn how to deal with uncertainty in the network structure and make predictionsabout what it means. it would be useful to be able to tell experimentalists that wethink something is missing in this structure.box 1engineering meets biology: computational challenges¥multiscale simulation¥understanding and controlling highly nonlinear network behavior (140pages to draw a diagram for network behavior of e. coli)¥uncertainty in network structure¥large amounts of uncertain and heterogeneous data¥identification of feedback behavior¥simulation, analysis, and control of hybrid systems¥experimental designwe have large amounts of both uncertain and heterogeneous data. biologistsare obtaining data any way they can and accumulating the data in differentformsñdiscrete data, continuous data, old data, and new data. we must find away to integrate all these data and use them as input for the computations.a key challenge in systems biology is identification of the control behavior.biology is an engineered systemñit just was engineered by nature and not by us.however, we would like to understand it and one way to do that is to treat it as anengineered system. it is a system that has many feedback loops, or else it wouldnõtbe nearly as stable as it is. we need to identify the feedback behavior, which isdifficult using only raw simulations and raw data. again, there are many variables arising from different models, leading to something that control engineerscall a hybrid system. such a system may have continuous variables, booleaninformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.142appendix dvariables, and bayesian variables. until now, computational scientists havenõtdealt with such systems, but we must learn to do so quickly.experimental design offers a unique opportunity for computation to contribute in a huge way. often it is not possible, or nobody has conceived of a way, todo experiments in biology that can isolate the variables, because itõs difficult todo in vivo experimentsñso the data come out of murkier experiments. it wouldbe of great value to the experimental world if we could design computations thatwould allow us to say, òhere is the kind of experiment that could yield the mostinformation about the system.óan important multiscale problem arises from the chemical kinetics within biological systems, specifically for intracellular biochemical networks. an example isthe heat shock response in e. coli, for which an estimated 20 to 30 sigma32 molecules per cell play a key role in sensing, in the folding state of the cell, and inregulating the production of heat shock proteins. the method of choice is the stochastic simulation algorithm, in which molecules meet and react on a probabilisticbasis, but the problem becomes too large because one also must consider the interactions of many other molecules in the system. it is a challenge to carry out suchsimulations in anything close to a reasonable time, even on the largest computers.two important challenges exist for multiscale systems. the first is multipletime scales, a problem that is familiar in chemical engineering where it is calledstiffness, and we have good solutions to it. in the stochastic world there doesnõtseem to be much knowledge of this phenomenon, but i believe that we recentlyhave found a solution to this problem. the second challengeñone that is evenmore difficultñarises when an exceedingly large number of molecules must beaccounted for in stochastic simulation. i think the solution will be multiscalesimulation. we will need to treat some reactions at a deterministic scale, maybeeven with differential equations, and treat other reactions by a discrete stochasticmethod. this is not an easy task in a simulation.i believe that some trends are developing (box 2), and in many fields, we aremoving beyond simulation. as a developer of numerical software, in the past 10years, iõve seen changes in the nature of computational problems in engineering.investigators are asking questions about such things as sensitivity analysis, optimum control, and design optimization. algorithms and computer power havereached a stage where they could provide success, but we are ambitious, and wantto do more. why do people carry out simulations? because they want to designsomething, or they want to understand it.i think the first step in this is sensitivity analysis, for which there has been alot of work in past decade. the forward method of sensitivity analysis for differential equations has been thoroughly investigated, and software is available. theadjoint method is a much more powerful method of sensitivity analysis for someproblems, but it is more difficult to implement, although good progress has beenmade. next it moved to pdes, where progress is being made for boundary conditions and adaptive meshes.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.linda r. petzold143uncertainty analysis was mentioned many times at the workshop, and still tocome are multiscale systems, fully stochastic systems, and so on. and then wewill move toward design optimization and optimal control. there is a need formore and better software tools in this area. even further in the future will becomputational design of experiments, with the challenge of learning the extent towhich one can learn something from incomplete information. where should theexperiment be done? where does the most predictive power exist in experimentspace and physical space? right now, these questions are commonly answered byintuition, and some experimentalists are tremendously good at it, but computations will be able to help. some examples are shown in box 3.i think computer science will play a much larger role in computational science and engineering in the future. first, there are some pragmatic reasons. all ofthe sciences and engineering can obtain much more significant help from software tools and technology than they have been receiving. a really nice exampleis source code generationñcodes that write the codes. a very successful application is that of automatic differentiation codes developed at argonne nationallaboratory. these codes take input functions written in fortran or c, whileyou specify where you have put your parameters and instruct them to differentiate the function with respect to those parameters. in the past, this has been a bigheadache because we had to do finite differences of the function with respect toperturbations of the parameter. in principle this a trivial thingñfirstsemestercalculusñbut itõs actually very difficult to decide on the increment for the finitebox 2beyond simulation: computational analysissensitivity analysis¥forward methodñodedae methods and software; hybrid systems¥adjoint methodñodedae methods and software¥forward and adjoint methodsñpde methods with adaptive mesh refinement¥multiscale, stochasticñstill to comeuncertainty analysis¥polynomial chaos, deterministic systems with uncertain coefficients;still to come: multiscale¥stochastic¥many other ideasdesign optimizationðoptimal controlinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.144appendix ddifference. there is a very tenuous tradeoff between roundoff and truncation error, for which there sometimes is no solutionñparticularly for chemistry problems,which tend to be badly scaled. the new approach writes the derivative function infortran or c, and the result is usually perfect, eliminating all of the finitedifferencing errors. moreover, it doesnõt suffer from the explosion of terms thatwere encountered with symbolic methods. the methodology uses compiler technology, and you might be surprised at what the compiler knows about your program. itõs almost a big brother sort of thing. this has enabled us to write muchmore robust software, and it has greatly reduced the time needed for debugging.another thing we can do use sourcecode generation to fix the foolish mistakes that we tend to make. for example, when i consult with companies, i oftenhear the problem of simulation software that has worked for a long time butsuddenly failed when something new was introducedñand the failure appears tobe random. i always ask the obvious question, òdid you put an ôifõ statement inyour function?ó and they always say, òno, i know about that potential problem:i wouldnõt be stupid enough to do that.ó but after many hours of looking at multiple levels of code (often written by several generations of employees who haveall gone on to something else), we find that òifó statement.in any case, what paul barton has done to source code generation is to go inand just automatically fix these things in the codes, so that we donõt have to go inbox 3more computational analysis¥design of experiments: to what extent can one learn something fromincomplete information? where is the most predictive power?¥determination of nonlinear structure: multiscale, stochastic, hybrid,bayesian, boolean systemsbifurcationmixinglongtime behaviorinvariant manifoldschaoscontrol mechanisms: identifying feedback mechanisms¥reduced and simplified models: deterministic, multiscale, stochastic,hybrid systems, identify the underlying structure and mechanism¥data analysis: revealing the interconnectedness, dealing with complications due to data uncertaintiesinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.linda r. petzold145and do it. and sometimes itõs a really hard thing to go into some of these codes,dusty decks, or whatever, written by people who have departed long ago, so thisis really making things easier.another big issue is that of user interfaces for scientific software. user interfaces generate many complaints, and with good reason! i am embarrassed to admit that my own codes and the scientific engineering software with which i comein contact regularly are in the dinosaur age. if our interfaces were betterñup tothe standards of microsoft office, for exampleñwe would have an easier timeattracting students into scientific and engineering fields. it certainly canõt excitethem to deal with those ugly pieces of 30yearold codes. on the other hand, wearenõt going to rewrite those codes, because that would be tremendously tediousand expensive.there have been some good developments in user interface technology forscientific computing, and some exceptions to the sad state of most of our softwareinterfaces. in fact, i think that the first one is obvious: matlab.6 it is an excellent example of the value of a beautiful interface, but it has been directed primarily at relatively small problems.computer science technology can be used to enable the semiautomatic generation of graphical user interfaces. this is also being done at sandia nationallaboratories and at lawrence livermore national laboratory with the mauicode. there are ugly codes in chemistry. we all know it. and there are manydusty decks out there in industry and national laboratories. weõd like to just keepthose codes but use compiler technology to bring up the kind of interfaces for theuser. a collaboration with those users will be necessary, which is why the methodis semiautomatic; but it can become a reality without every user needing to learnthe latest userinterface technology.finally, my major point is that computer science will play a much larger role.there is a deep reason for that. it isnõt just machines, compilers, and sourcecodegeneration that will help you with your research. those will be nice and undoubtedly will be useful, but they are not the main issue.at the smaller scalesñand this is the fundamental changeñwe are dealingwith and manipulating large amounts of discrete stochastic, bayesian, and boolean information. the key word is information. in the past, we manipulated continuum descriptions, which are averages. those were nice, but now we must manipulate discrete datañheterogeneous data. who has been thinking about that forthe last 20 or 30 years? these problems form the core of computer science.living simultaneously in both a computer science department and a mechanical engineering department, i know that the communications between engineeringand computer science have not always been as good as they could be. but we are allresearchers, and the needs of science will make the communication happen.6originally developed as a òmatrix laboratoryó to provide access to matrix software, matlabintegrates mathematical computing, visualization, and a programming language for scientific andtechnical computing: http://www.mathworks.com/information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.146appendix dsimulation in materials sciencegeorge c. schatznorthwestern universityintroductionthe primary goal of this paper is to provide examples of problems innanomaterials where computational methods are able to play a vital role in thediscovery process. three areas of research are considered, and in each of theseareas, an important scientific problem is identified. then we consider the currentcomputational and theoretical tools that one can bring to bear on the problem andpresent representative results of applications to show how computation and theoryhave proven to be important. the three areas are (1) optical properties of goldnanoparticle aggregates, (2) melting of dna in the gold nanoparticle aggregates,and (3) oxygen atom erosion of polymers in low earth orbit conditions. the threetheories and computational methods being highlighted are (1) computational electrodynamics, (2) empirical potential molecular dynamics, and (3) direct dynamicsðcarparinello molecular dynamics.optical properties of gold nanoparticle aggregatesalthough colloidal gold has been known for thousands of years as the redcolor of stained glass windows, only very recently has it found a use in the medical diagnostics industry as a sensor for dna. in this work by chad mirkin andcolleagues at northwestern, singlestranded oligonucleotides are chemically attached to colloidal gold particles, typically 10 nm in diameter. if an oligonucletide(that is from dna that one wishes to detect and is complementary to the singlestranded portions attached to the gold particles) is added to the system, dnahybridization results in aggregation of the particles, inducing a color change fromred to blue that signifies the presence of the complementary oligonucleotide.smallangle xray scattering studies of these aggregates indicate that although theaggregates are amorphous, the nearestneighbor spacing between particles is approximately equal to the length of the linker oligonucleotide.the challenge to theory in this case is to develop an understanding of theoptical properties of the aggregates so that one can determine the optimum particle size and optimum linker length, as well as the variation in optical propertiesthat can be achieved using other kinds of nanoparticles. the basic theory neededfor this problem is classical electrodynamic theory because this is known to describe the optical properties of metal particles accurately, provided only that wehave the dielectric constants of the particles and surrounding medium and that wehave at least a rough model for the aggregate structure.computational electrodynamics has made great progress during the past deinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.george c. schatz147cade, with the result that codes are now available for describing the optical properties of assemblies of nanoparticles with upwards of tens of thousands of particles. however there are many choices for methods and codes, including a variety of finite element (gridbased) methods (discrete dipole approximation, finitedifference time domain, multiple multipoles, dyadic greenõs function) and also avariety of coupled particle methods, of which the most powerful is the tmatrixmethod but which in the present case can be simplified to coupled dipoles formost applications. the codes for this application are largely public domain, butimportant functionality can be missing, and we have found it necessary to makerevisions to all the codes that we have used.fortunately the nanoparticle aggregate electrodynamics problem is relativelyeasy to solve, and many of the codes that we examined give useful results that notonly explain the observed color change, but tell us how the results vary withparticle size, dna length, and optical constants. figure 1 shows a typical comparison of theory and experiment. we learned enough from our studies of thisproblem that we were able to develop a simple, almost analytical theory based ona dynamic effective medium approximation. in addition, the methods that weconsidered have been useful for studying more challenging electrodynamics problems that involve ordered assemblies of nonspherical particles in complex dielectric environments.figure 1absorption spectra of dnalinked gold nanoparticle aggregates for 24, 28and 72 basepair dna linkers, comparing theory (light scattering calculations, right) andexperiment (left).information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.148appendix dmelting of dna that links nanoparticlesthe dnalinked gold nanoparticle aggregates just described have anotheruse for dna detection that is both very important and distinctly nanoscale innature. when the aggregates are heated, it is found that they òmeltó (i.e., thedouble helix unravels into singlestranded oligonucleotides) at about 50 oc over anarrow temperature range of about 3 oc. this melting range is to be contrastedwith what is observed for the melting of dna in solution, which typically has a20oc range for the same length oligonuclides. this is important to dna testing,as single base pair mismatches, insertions, and deletions can still lead to aggregate formation, and the only way to identify these noncomplementary forms ofdna is to subject the aggregates to a thermal wash that melts out the noncomplementary linkers but not the complementary ones. the resolution of this process is extremely sensitive to the melting width. in view of this, it is important toestablish the mechanism for the narrow melting transition.recently we have developed a cooperative melting model of dna meltingfor dnalinked gold nanoparticle aggregates. this model considers the equilibrium between different partially melted aggregates, in which each stage involvesthe melting of a linker oligonucleotide, but there are no other structural changes(at least initially). in this case, the equilibria can be writtendn = dnð1 + q + ns . . . . . .d2 = d1 + q + nsd1 = d0 + q + nswhere dn is the aggregate with n linkers, q is the linker, and s stands for freecounterions that are released with each linker. the key to the cooperative mechanism is to realize that each successive step becomes easier due to the loss ofcounterions from the aggregate. here, we are guessing that when the dnas aresufficiently close that their ion clouds overlap, the ion clouds mutually stabilizethe hybridized state (due to improved screening of the phosphate interactions).with this assumption, the equilibrium collapses to a single expression:dn = d0 + nq + nnsin which n targets and their complement of counterions come out cooperatively.equilibrium concentrations based on this expression are consistent with a varietyof measurements on the gold nanoparticle aggregates. figure 2 shows the sharpening of the melting curve that occurs as the number of dna linkers pernanoparticle is increased, while figure 3 shows a fit to experimental data usingthis model.while this works, the key assumption of the theory, that the ion clouds overinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.george c. schatz149cooperative melting modelt1m=320, t2m=350h1=h2=50 kcal/mol0.000.200.400.600.801.00280300320340360380temperaturen=2n=1349fraction meltedtemperature50.052.054.056.058.060.0fraction melted0.000.200.400.600.801.00target: ctccctaataacaatttataactattcctathermodynamic model (n=1.6)experimentfigure 3cooperative melting mechanism: fit to experiment.figure 2cooperative melting mechanism: dependence of melting curves on numberof dna linkers per nanoparticle.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.150appendix dlap and the dna is stabilized needs to be proven, and crucial information aboutthe range of the cooperative mechanism needs to be determined. to do this, wemust simulate the ion atmosphere around dna dimers, trimers and other structures, ideally with gold particles also nearby. this could probably be done withan electrostatic model, but there is concern about the validity of such models forsingly charged counterions. to circumvent this, we have considered the use ofatomistic molecular dynamics with empirical force fields. such methods havealready been used to determine charge cloud information concerning a singleduplex, but the calculations had not been considered for aggregates of duplexes.fortunately the methodology for simulating dna and its surrounding chargecloud is well established based on the amber suite of programs. this suite allowsfor the inclusion of explicit water and ions into the calculation, as well as particlemesh ewald sums to take care of longrange forces. previous work with amber,explicit ions, and ewald summation has demonstrated that the ion distributionaround duplex dna is consistent with available observations in terms of thenumber of ions that are condensed onto the dna, and the extent of the counterioncloud around the dna. given this, it is therefore reasonable to use the sametechnology to examine the counterion distribution around pairs (and larger aggregates) of dna duplexes and how this varies with bulk salt concentration.my group has now performed these simulations, and from this we have established the relationship between bulk salt concentration and the counterion concentration close to the dna. by combining these results with the measured variation of dna melting temperature with bulk salt concentration, we havedetermined that the decrement in melting temperature associated with loss of adna target from an aggregate is several degrees, which is sufficiently large thatcooperative melting can occur. in addition we find that the range of interaction ofthe counterion atmospheres is about 4 nm, which establishes the dna densityneeded to produce cooperative melting.oxygen atom erosion of polymers in low earth orbit conditionsthe exterior surfaces of spacecraft and satellites in low earth orbit (leo)conditions are exposed to a very harsh environment. the most abundant speciesis atomic oxygen, with a concentration of roughly 109 atoms per cm3. when onefactors in the spacecraft velocity, one finds that there is one collision per secondper exposed surface site on the spacecraft, and the oxygen atoms typically have 5ev of energy, which is enough to break bonds. when this is combined with otherleo erosion mechanisms that involve ions, electrons, uv photons, and otherhighenergy neutrals, one finds that microns of exposed surface can be eroded perday unless protective countermeasures are imposed. this problem has been knownsince the beginning of space flight, but the actual mechanism of the erosion process has not been established. in an attempt to rectify this situation, and perhapsto stimulate the development of new classes of erosion resistant materials, i haveinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.linda r. petzold151been collaborating with a team of afosr funded facultyñsteven sibener,luping yu, tim minton, dennis jacobs, bill hase, barbara garrison, johntullyñto develop theory to model the erosion of polymers and other materials,and to test this theory in conjunction with laboratory and satellite experiments. ofcourse this research will also have spinoffs to other polymer erosion processesimportant in the transportation and electronics industries.we are only at the beginning of this project at the moment, so all of ourattention thus far has been directed at understanding the initial steps associatedwith the impact of highvelocity oxygen atoms with hydrocarbon polymer surfaces. even for this relatively simple class of materials, there has been muchconfusion as to the initial steps, with the prevailing wisdom being that o(3p), theground state of oxygen, can only abstract hydrogen atoms from a hydrocarbon,which means that in order to add oxygen to the surface (as a first step to theformation of co and co2), it is necessary for intersystem crossing to switch fromtriplet to singlet states, thereby allowing the oxygen atom to undergo insertionreactions. this wisdom is based on analogy with hydrocarbon combustion, wherehydrogen abstraction reactions are the only mechanism observed when oxygeninteracts with a saturated hydrocarbon.in order to explore the issue of oxygenatom reactions with hydrocarbons inan unbiased way, we decided to use òdirect dynamicsó (dd), i.e., molecular dynamics simulations in which the forces for the md calculations are derived fromelectronic structure calculations that are done on the fly. although this idea hasbeen around for a long time, it has become practical only in the last few years as aresult of advances in computational algorithms for electronic structure calculationsand the advent of cheap distributed parallel computers. we have studied two approaches to this problem: the carparinello molecular dynamics (cpmd) methodusing plane wave basis functions and ultrasoft pseudopotentials, and conventionalbornoppenheimer molecular dynamics (bomd) using gaussian orbitals. bothcalculations use density functional theory, which is only capable of 5 kcal/molaccuracy for stationary point energies, but should be adequate for the high energiesof interest to this work. both methods give similar results when applied to the sameproblem, but cpmd is more useful for atomperiodic surface dynamics, whilebomd is better for cluster models of surfaces or disordered surfaces.our calculations indicate that there are important reaction pathways that arisewith 5ev oxygen atoms that have not previously been considered. in particularwe find that oxy radical formation is as important as hydrogen atom abstractionand that cc bond breakage also plays a role. both of these pathways allow fortriplet oxygen to add to the hydrocarbon directly, without the need for intersystem crossing. thus, it appears that previously guessed mechanisms need to berevised. further studies are under way in john tullyõs group to incorporate theseresults into kinetic monte carlo simulations that will bridge the gap betweenatomicscale reaction simulations and the macroscale erosion process.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.152appendix dconclusionthese examples indicate that an arsenal of simulation approaches is needed fornanoscale materials research including electrodynamics, empirical potential md,direct dynamics, and cpmd. fortunately, many of the necessary tools are available, or can be developed by straightforward modifications of existing software.these examples also show that simulation is an important tool for manynanoscale materials problems. although brute force simulation is not usually effective because the time scales are too long and the number of particles is toolarge, a combination of simulation in conjunction with analytical theory andsimple models can be quite effective.acknowledgmentsthe research described here is supported by the afosr (muri anddurint programs) and the national science foundation (nsec program). important contributors to these projects have been my students and postdocs (annelazarides, guosheng wu, hai long, diego troya, ronald pascual, lance kelly,and linlin zhao) as well as tim minton (montana state) and chad mirkin(northwestern).distributed cyberinfrastructure supporting thechemical sciences and engineeringlarry l. smarruniversity of california at san diegoduring the next 10 years, chemical science and engineering will be participating in a broad trend in the united states and across the world: we are movingtoward a distributed cyberinfrastructure. the goal will be to provide a collaborative framework for individual investigators who want to work with each other orwith industry on largerscale projects that would be impossible for individualinvestigators working alone. i have been involved in examining the future of theinternet over the next decade, and in this paper i discuss this future in the contextof the issues that were dealt with at the workshop.consider some examples of grand challenges that are inherently chemical innature:¥simulating a micronsized living cell that has organelles composed of millions of ribosomes, macromolecules with billions of proteins drawn from thousands of different species, and a meter of dna with several billion basesñallinvolving vast numbers of chemical pathways that are tightly coupled with feedinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.larry l. smarr153back loops by which the proteins turn genes on and off in regulatory networks;and¥simulating the starforming regions in which radiation from newly forming stars causes a complex set of chemical reactions in diffuse media containingcarbon grains as catalytic surfaces moving in turbulent flow. according to datafrom the hubble space telescope, the size of the òreactoró would be on the orderof one light year.both examples require solving a similar set of equations, although the regime in which they are applied is quite different from traditional chemical engineering applications.other areas of science have similar levels of complexity, and i would arguethat those areas have progressed farther than the chemical sciences in thinkingabout the distributed information infrastructure that needs to be built to make thescience possible.¥medical imaging with mri: the nih has funded the biomedicalinformatics research network (birn), a federated repository of threedimensional brain images to support the quantitative science of statistical comparisonof the brain subsystems. the task includes integration of computing, networks,data, and software as well as training people.¥geophysics and the earth sciences: the nsf is beginning to roll outearthscape, a shared facility which will ultimately have hundreds of highresolution seismic devices located throughout the united states. the resulting data willprovide threedimensional images of the top several kilometers of the earth to theacademic community.¥particle physics: the physics community is developing a global data gridthat will provide tens of thousands of users, in hundreds of universities, in dozensof countries throughout the world with a distributed collaborative environmentfor access to the huge amount of data generated at cern in geneva.many additional examples exist across scientific disciplines that have beeninvolved as a community for 5 or 10 years in coming to consensus about the needfor ambitious largescale, dataintensive science systems. you will be hearingmore and more about a common information infrastructure that underlies all oftheseñcalled distributed cyberinfrastructureñas it rolls out over the next fewyears. here i want to impart a sense of what will be in that system. it seems to methat chemical engineering and chemistry share many aspects of data science withthese other disciplines, so chemists and chemical engineers should be thinkingabout the problem.for 20 years, the development of information infrastructure has been drivenby mooreõs lawñthe doubling of computing speed per unit cost every 18information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.154appendix dmonths.1 but now, storage and network bandwidth are rising even faster thancomputing speed. this means that everything we thought we knew about integrated infrastructure will be turned completely upside down. it is why the gridmovement described earlier by thom dunning2 is now taking off. we are goingthrough a phase transitionñfrom a very loosely coupled world to a very tightlycoupled worldñbecause the bandwidth between computers is growing muchfaster than the speed of the individual computers.the rapid growth in data handling is being driven by the same kind of effectthat influenced computing over the last 10 years: that is, not only did the individual processors become faster but the amount of parallelism within highperformance computers also increased. each year or so, as each processor becametwice as fast, you also used twice as many processors, thereby increasing theoverall speed by a factor of 4ñthis leads to hyperexponential growth. we haveentered a similar era for bandwidth that enables data to move through opticalfibers. it is now possible to put wavelength bins inside the fiber, so that instead ofjust a single gigabitpersecond channel it is possible to have 16, 32, or manymore channels (called lambdas), each operating at a slightly different wavelength.every year both the number and the speed of these channels increase, therebycreating a bandwidth explosion. as an aside, the capability of optical fiber tosupport these multiple channels, ultimately enhancing the chemical sciences using the fiber, is itself a victory for chemical engineering. it was the ability, particularly through the contributions of corning, to remove almost every moleculeof water from the glass (water absorbs in the infrared wavelengths used by thefiber to transmit information) that makes this parallel revolution possible withoptical fiber.without going into more detail, over the next few years each campus and statewill be accumulating òdark fiberó to support such parallel dedicated networks foracademic research or governmental functions. each campus will have to start worrying about where the conduit is, so that you can pull the dark fiber from building tobuilding. once this owneroperated dark fiber is in place, you will get more bandwidth every year for no money other than for increasing the capability of the electronic boxes at the ends of the fiber. consequently, you will use the increasinglambda parallelism to get an annual increase in bandwidth out of the same installedfiber. today, this is occurring on a few campuses such as ucsd, as well as in somestates, and there are discussions about building a national òlight railó of dark fiberacross the country. when that happens, we will be able to link researchers togetherwith data repositories on scales we havenõt imagined.coming back to the science drivers, stateoftheart chemical supercomputing1moore, g. e., electronics 1965, 38 (8) 11417; http://www.intel.com/research/silicon/mooreslaw.htm.2see t. dunning, appendix d.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.larry l. smarr155engineering simulations of phenomena such as turbulence (or experiments withlaser readouts of fluid) are comparable in scale to the highest resolution mris orearth seismic images, namely gigazone data objects. such giant threedimensionalindividual data objects (103 103 103, or larger) often occur in timed series, soone has a whole sequence of them. furthermore the data are generated at a remotesupercomputer or laboratory and stored there in federated data repositories. theremay be some important consequences if you are the investigator:¥the data piece you want is somewhere you arenõt, and you would like tointeractively analyze and visualize these highresolution and data objects.¥because nature puts so many different kinds of disciplines together inwhat it does, you actually need a group of different kinds of scientists to do theanalysis, and they are probably spread out in different locations, requiring collaborative networked facilities.¥individual pcs donõt have enough computing power or memory to handlethese gigazone objects, requiring scalable linux pc clusters instead.¥you wonõt have enough bandwidth over the shared internet to interactwith these objects; instead, you need dedicated lambdas.you might ask, òwhat is the twentyfirst century grid infrastructure that isemerging?ó i would answer that it is this tightly optically coupled data grid ofpc clusters for computing and visualization with a distributed storage fabric, tiedtogether a software middle layer that enables creation of virtual metacomputersand collaboration. in my institute we try to look out to see how this is all happening, by creating òliving laboratories of the future.ó we do this by deployingexperimental networked test beds, containing bleeding edge technological components that will become mass market in 3, 5, or 10 years, and using the systemnow with faculty and students. that is the way supercomputers developedñfrom15 years ago when a few people ran computational chemistry on a gigahertzmachine like the cray2 that cost $20 million, to today, when everybody workson a gigahertz machine pc that costs only $1000.let me summarize a few major trends in this emerging distributedcyberinfrastructure:¥wireless internet: it took 30 years of exponential growth for the numberof fixed internet users, sitting at their pcs, to grow to two hundred million. weare likely to double that number with mobile internet users who use cell phonesor portable computers in the next three years. consider that today there are probably a few hundred million automobiles in the united states, each having a fewdozen microprocessors and several dozen sensors and actuators. when the processors on these cars get on the net they will vastly dominate the number ofpeople in this country on the net. so, if you thought you had seen an explosion onthe internet, you really havenõt seen anything yet.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.156appendix d¥distributed grid computing: wisconsinõs pioneering condor3 project,which creates a distributed computing fabric by tying hundreds of unix workstations together, was discussed during the workshop. today, companies such asentropia or united devices are emulating that basic concept with pcs. in thissense, pcs are the dark matter of the internetñthere is an enormous level ofcomputing power that is not being used but could be tied together by the grid.¥sensorsnets: one of the problems with doing largescale chemistry in theenvironment is that you donõt know the state of the system. i think this may begenerally true if you look at largescale chemical plants. if you have wirelesseverywhere you can insert devices to measure the state of the system and obtain afinergrained set of data. if you are simulating the future of the system, yoursimulation can be much more useful, and the approach will afford a tighter coupling between those who do simulation and those who gather data.¥miniaturization of sensors: one of the big trends that is taking place is theability to take chemical sensing unitsñsensors that detect the chemical state ofair, water, and so forthñand make them smaller and cheaper so you can put themin more places. i think chemistry and chemical engineering will be a frontier forsensornets based on nanotechnology and wireless communication. moreover,there is a revolution going on in the kinds of sensor platforms that can be used,such as automous vehicles the size of hummingbirds that fly around, gather, andtransmit data.¥nanoscale information objects: consider the human rhinovirus, whichhas genetic information coded for the several surface proteins. these proteinsform an interlocking coat that serves to protect the nucleic acid which is a software programñwhich, of course, is interpreted to generate the protein coat thatprotects the software. so, in addition to being a nanoobject, the virus structure isalso an information object as well. so when you talk about nanotechnology, iwant you to remember that it is not just the physics, chemistry, and biology that issmall, but there is a new set of nano information objects that are also involved.for science, all of these trends mean that we are going to be able to know thestate of the world in much greater detail than any of us ever thought possibleñand probably on a much shorter time scale than most of us would expect as wefocus on this report that looks at òthe next 10 years.ó3see: http://www.cs.wisc.edu/condor/.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.ellen stechel157modeling and simulation as a design tool1,2ellen stechelford motor companythe lens from which my comments arise is an industrial perspective, albeitfrom a limited experience base. the experience base reflected is one of four yearsin industrial research and development, during which about 60% of my time hasbeen in research and 40% in development. over those four years, my observations and experiences have caused my thinking to evolve dramatically.the first section of this paper is focused on the chemical and materials simulation section in ford research laboratory. i describe research that has significant academic character but simultaneously has a path of relevance to automotiveapplications. the second section focuses on some crosscutting themes, and thethird section describes three specific examples that illustrate those themes andreflect some challenges and opportunities for chemical sciences.chemical and materials simulation in anautomotive industrial research laboratorythe unifying strategy of the chemical and materials simulation (cams)group at ford research laboratory is the use of atomistic and higher length scalechemical and materials simulation methods to develop a knowledge base with theaim of improving materials, interfacial processes, and chemistry relevant to fordmotor company. excellence and fundamental understanding in scientific researchare highly valued; but relevance and potential impact are most important. thereare three focus areas of research:1.surface chemistry and physics for such applications as environmental catalysis, exhaust gas sensors, mechanical wear and fatigue.2.materials physics, such as lightweight alloys and solid oxide fuel cells.3.molecular chemistry, which includes particulate matter dynamics, atmospheric chemistry, and combustion chemistry.the tools used by the chemical and materials simulation group are commercial, academic, and homegrown simulation and analysis software, in additionto dedicated compute servers. the development of some of those òhomegrownó1copyright reserved by ford motor company. reproduced by permission.2acknowledgement: the author is indebted to drs. john allison, peter beardmore, alex bogicevic,william green, ken hass, william schneider, and chris wolverton for many enlightening conversations and for use of their work to illustrate the points in this manuscript.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.158appendix dtools occurred in nonindustrial settings, since one of the proven ways to transfertechnology is simply to hire a person, who often arrives with tools as well asexpertise.catalytic materialsexhaust gas catalytic aftertreatment for pollution control depends heavily onchemistry and on materials. hence, in one cams project, the goal is to guide thedevelopment of improved catalytic materials for leanburn, exhaustnox (noand no2) aftertreatment, for gas sensors, and for fuel cells. one driver for improved materials is to enable cleaner leanburn and diesel engine technologies.the first principles, atomisticscale calculations have been able to elucidate noxbinding to an mgo surface, providing a knowledge base relevant to the fundamentals of a nox storage device (figure 1).3 using ab initio calculations, fordresearchers have been able to map out the energy landscape for nox decomposition and n2o formation on a model cu(111) surface, relevant to exhaust noxcatalysis.43schneider, w. f.; hass, k. c.; militec, m.; gland, j. l. j. phys. chem. 2002, b 106, 7405.4bogicevic, a.; hass, k. c. surf. sci. 2002, 506, l237.figure 1nox storage: ab initio calculations of nox binding to an mgo surface provide a knowledge base.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.ellen stechel159ionic conductorsa second example from the cams group focuses on ionic conductors, withthe goal of improving the activity and robustness of oxide conductors for fuelcells and sensors.5 the scientific goal of the project is to understand with theintent of tailoring ion diffusion in disordered electrolytes. this work has developed a simple computational screen for predicting ionic conductivity of variousdopants. the work has also established the only known ordered yttriastabilizedzirconia phase (figure 2).crosscutting themeswhile the two examples briefly described above have direct relevance to theautomotive industry, the scientific work is indistinguishable from academic basicresearch. nevertheless, successful industrial research generally reflects and buildsfigure 2ab initio calculations on yttriastabilized zirconia to improve activity androbustness of oxide conductors for fuel cells and sensors.5bogicevic a.; wolverton, c. phys. rev. b 2003, 67, 024106; bogicevic a.; wolverton, c.europhys. lett. 2001, 56, 393; bogicevic a.; wolverton, c.; crosbie, g.; stechel, e. phys. rev. b2001, 64, 014106.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.160appendix don four recurring and broadly applicable themes. the first theme is the false dichotomy that arises when trying to distinguish applied from basic research. thesecond theme is a definition of multidisciplinary research that is broader thanwhat is conventional. the application of this broader definition leads to the challenge of integration across research fields within the physical sciences as well asacross science, engineering, technology, and business. the third theme concernscomplex systems and requires establishing an appropriate balance between reductionism (solvable òbitesizeó pieces) and holism (putting the pieces back together again). finally, the fourth theme is one of a fourlegged stool, a variationof the oftenreferenced threelegged stool. a successful application of industrialresearch necessarily adds a leg to the stool and puts something on the seat of thestool. the three legs are theory and modeling, laboratory experimentation, andcomputer simulation. the fourth leg is subsystem and fullsystem testing (verification and validation of laboratory concepts), and what sits on top of the stool isthe realworld application in the uncontrolled customerõs hands.false dichotomythe structure of science and technology research sometimes is assumed tohave a linear progression from basic research (which creates knowledge that isput in a reservoir) to application scientists and engineers (who try to extractknowledge from the reservoir and apply it for a specific need), and finally todevelop technology. few people believe that the science and technology enterprise actually works in this simplistic way. nevertheless, with this mental modelunderpinning much of the thinking, unintentional adverse consequences result.as an alternative mental model, pasteurõs quadrant,6 generalizes the linearmodel to a twodimensional model, where one axis is consideration and use, andthe other axis is the quest for fundamental understanding. the quadrant in whichthere is both a strong consideration of use and a strong quest for fundamentalunderstanding (òpasteurõs quadrantó) has basic and applied research rolled together. this is the quadrant with the potential for the greatest impact in industrialresearch. the false dichotomy between basic and applied research arises whenthe onedimensional model ignores feedbackñwhere the application drives aneed for new fundamental understanding that is not yet in the reservoir. indeed,in implementing new technology, one is likely to find that what keeps one awakeat night is not the known but the unknownñmissing basic research that is applicable to the technology. industrial researchers quickly find no shortage of challenging, intellectually stimulating, and poorly understood (albeit evolutionary)research problems that are relevant and have the potential for great impact on theindustry.6pasteurõs quadrant, stokes, d. e. brookings institution press: washington, dc, 1997.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.ellen stechel161another way of looking at the twoway relationship between science andapplication is to recognize that the difference is in the starting and ending points,but not necessarily in the road traveled. if the primary motivation is scienceñaquest for understandingñthen the world in which the researcher lives is one thatis reasonably deterministic, predictive, and well controlled. the goal is generallyone of proof of conceptña consistent explanation or reproducibility of observations. new and novel knowledge and wisdom, new capabilities, and new verification and validation methodologies are outputs of the science.however, if the motivation stems from an industrial or application perspective, the researcher begins first by considering the use of what he or she is studying. as the researcher digs beneath the surface as if peeling away layers of anonion, he or she finds that it is necessary to ask new questions; this drives newscience, which drives new knowledge, and, which in turn, drives new uses andnew technology. additionally, the two researchers (understandingdriven andapplicationdriven) frequently meet in the middle of their journeyñwith bothfinding the need to develop new and novel knowledge and wisdom, new capabilities, and new verification and validation methodologies.it does not matter where the researcher starts the journey: whether he or shecomes from an industrial or academic perspective, there is common ground inthe middle of the journey. in industry, science is a means to an end, but it is noless science. the world the industrial researcher strives for is a world definedby performance within specifications, robustness, reliability, and cost. however, in trying to achieve those goals, the researcher necessarily asks new questions, which drive new science in the process of deriving verifiable answers tothose questions.integrated, complex systems, perspectiveto strike the appropriate balance between reductionist approaches and integrated approaches is a grand challenge. it may be that as social creatures, peopleare simply illequipped to go beyond reductionist approaches, to approaches thatintegrate science, engineering, technology, economics, business, markets, andpolicy. right now, there is a strong tendency to view these disparate disciplinesin isolation. nevertheless, they interrelateñand interrelate stronglyñin the realworld.even when starting with a problemdriven approach, a goal or problem canalways be broken down into solvable pieces. the very basis for the success oftwentieth century science has been such reductionism. nevertheless, to obtainoptimal solutions, it is necessary to appropriately integrate disparate pieces together. within any system, if one only optimizes subsystems, then one necessarily suboptimizes the entire system.typically, there will be many ways to assemble pieces. industry needs areassembled state that is practical, costeffective, robust, adaptable, strategic, andinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.162appendix dso on. although the constraints are many, it is necessary to consider all of themseriously and intellectually.complex systemsthe term òcomplex systemsó means different things to different people. anoperational definition is used here: complex systems have, nonlinear relationships, large experimental domains, and multiple interactions between many components. for complex systems, the underlying model is generally unknown, andthere is no analytical formula for the response surface. it may be that solutionscome in the form of patterns rather than numerical values. most importantly, thewhole is not the sum of the parts.a system can be very complicated and can be ordered, or at least deterministic. one might be able to write down a model and use a very powerful computerto solve the equations. science can make very good progress in elucidating solutions of such a system. a system can also be random enough that many variablesmay not be explicitly important, but their effects can be adequately accounted forwith statistical science. it is important not to ignore such effects, but often theyare describable as noise or random variables.somewhere between the ordered state and the fully random state lie manycomplex systems of interest. significant progress depends on both the understandingdriven and the applicationdriven researcher comprehending and becoming comfortable with such complexity.the fourlegged stool and statisticstheory, experiment, and simulation form a threelegged stool. the additionof subsystem and fullsystem testing produces a fourlegged stool. the ultimategoal is to have a firm grasp of how a product will perform in the customerõshands, and that is what sits on top of the stool. the four legs must support thedevelopment of a firm understanding of mean performance variables, natural variability, and the sources of variability as well as the failure modes and the abilityor not to avoid those failure modes. one does not need to be able to predictperformance under all conditions (something which is often impossible), but onedoes need the ability to determine if the anticipated stresses can or cannot lead toa hard failure (something breaks) or a soft failure (performance degrades to anunacceptable level.)consequently, it is critically important that the four legs do not ignore statistical performance distributions. statistical science is a powerful tool for comprehending complex systems; however, it does not appear to be a wellappreciateddiscipline, despite its importance to product development. in addition, of note arethe tails of performance distributions, which become quite significant in a population of millions for highvolume products.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.ellen stechel163examplesthree automotive examples that can and do benefit from simulation and fromthe chemical sciences are aluminum casting for engine blocks and heads, exhaustaftertreatment catalysts for air quality, and homogeneous charged compressiveignition (hcci) enginesña potential future technology for improving fueleconomy and maintaining low emissions. in the following discussion, the focusfor each example is on why the industry has an interest in the application, whatthe industry is trying to accomplish, and the nature of some of the key ongoingscientific challenges. the emphasis is on how the chemical sciences fit in andhow simulation fits in, but the challenge in each example involves significantlymore than chemical sciences and simulation.virtual aluminum castingaluminum casting research at ford motor company is the most integrated ofmy three examples. i chose it because this research illustrates good working relations across disciplines and research sectors. the research in the aluminumcasting project spans the range from the most fundamental atomicscale science tothe most applied in the manufacturing plant, and the project exploits all legs ofthe fourlegged stool. the research approach displays a heavy reliance on simulation that unimpeachably is having significant impact.modeling and simulation of new materials receives most of the attentionthese days, but breakthroughs on mature material may have greater potential impact. peter beardmore, a nowretired director from ford research laboratory,argued [ref. p. beardmore, nae, 1997] that refinements made to mature materials and processes would have a much greater probability of being implementedñand therefore would have far greater impactñthan all new materials conceptscombined. while his focus was on automotive structural materials, i believe thestatement is broadly applicable and is likely true of virtually all technology.automobiles are mostly steel, by weight. however, the automotive industrycomprises a large percentage of the consumer market for aluminum, iron, andmagnesium. the automotive industry does not dominate the plastics market.materials usage (measured by weight) did not changed much from 1977 to2001. vehicles are still mostly steel, which accounts for the high recyclability ofthe automobile. aluminum and plastic content have increased some. however,with emerging energy issues, further change will be necessary. fuel economy andemission standards are very strong drivers for lightweight materials, since to afirst approximation fuel economy is inversely proportional to the weight of avehicle. any other avenues to increase fuel economy pale in terms of what arisesfrom lightening the vehicle. however, from a consumer perspective, it is criticalthat the vehicle is reliable, costeffective, and high quality: consumer demandsthus present considerable challenges to lightweight materials.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.164appendix dto attack the problem from a technology perspective, the researcher mustconsider the materialõs usage, performance, reliability, and manufacturability. theautomotive industry makes a large number of cylinder heads and engine blocks;so reducing weight creates a driver to replace cast iron with cast aluminum. tohave impact, the researcher must set goals and meet specifications, such as reducing product and process development time by a large percentage through simulation, improving quality, reducing scrap, and ensuring highmileage durability.the ford research virtual aluminumcasting project has developed uniquecapabilities for simultaneous process and product optimization. generally, process optimization and product optimization are not integrated. in the stereotypicalprocess, a design engineer generates a product design, and then throws it over thefence to the manufacturing division. the manufacturing engineer says, òwell, ifyou made this change in your design it would be easier to manufacture,ó and anargument begins. both parties remain focused on their subsystem and neither isable to recognize that the design and manufacturability of the product is a system,and that system optimization as opposed to subsystem optimizations is oftenwhere opportunities emerge.the virtual aluminumcasting project has developed a comprehensive andintegrated set of simulation tools that capture expertise in aluminum metallurgycasting and heat treatmentñyears of experience captured in models. in additionto enabling calculations, an oftenoverlooked characteristic of models, is that theycan organize knowledge and information and thereby assist in knowledge dissemination. the models in the toolset have unsurpassed capabilities. john allison,the project leader for virtual aluminum casting uses the phrase òcomputationalmaterials science plus.ó the project, which links physics, materials science, mechanics, theory, modeling, experiment, engineering, fundamental science, andapplied science, is achieving notable successes.the structure of this virtual aluminum casting project could serve as a readilygeneralized example. one of the fundamental premises is that computationalmodels should be able to solve 80 percent of the problem quickly through simulation, and the models should not be trusted completely. experimental verificationand validation on subsystem and fullsystem levels are critical. another fundamental premise is that new knowledge should be transferred into the plant andimplemented. in other words, it is important to go beyond proof of concept topractical application.the virtual aluminum casting project is comprehensive and multiscale;7 itreaches from initial product design based on property requirements, all the way toinfluencing actual manufactured components (figure 3). the project involvesexperiments and models at all length scales and all levels of empiricism thatpredict properties, service life, damage, stress, and the like.7vaithyanathan, v.; wolverton, c.; chen, l.q. phys. rev. lett. 2002, 88, 125503.).information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.ellen stechel165figure 3multiscale calculations of aluminum alloys to guide the development of improved engine blocks.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.166appendix dthe aluminum alloy material has roughly 11 components (as a recycled material, the alloy actually has an unknown number of components that depend onthe recycling stream), and some of the concentrations are quite small. some components have more impact than others do. impurities can have enormous effectson the microstructure and thereby affect the properties. precipitates form duringheat treatment, and manufacturers have the opportunity to optimize the aluminumheat treatment to achieve an optimized structure. first principles calculations andmultiscale calculations are able to elucidate opportunities, including overturning100 years of metallurgical conventional wisdom (figure 4).8several key challenges must be overcome in this area. in the chemical sciences, methodologies are needed to acquire quantitative kinetic information onreal industrial materials without resorting to experiment. it is also necessary todetermine kinetic prefactors and barriers that have enough accuracy to be useful.another key challenge is the need for seamless multiscale modeling includinguncertainty quantification. properties such as ductility and tensile strength arestill very difficult to calculate. finally, working effectively across the disciplinesstill remains a considerable barrier.figure 4firstprinciples calculations can provide input to models downstream.computationalthermodynamics(x,t)precipitation hardening modelsthermal growth modelf (mev/atom)df(qq')free energy of al2cuunexpected temperaturedependenceof free energyfirstprinciples calculationq™ stableqstable'(x,t)f(x,t)df(qq')free energy of al2cuof free energyfirstq™ stableqstabledf(qq')free energy of al2cuq™ stabledf(qq')free energy of al2cudf(qq')free energy of al2cudf(qq')free energy of al2cutemperature (c)f(ð')free energy of al2cu' stablestable0200400151050510151510505101515105051015151050510151510505101515105051015151050510158wolverton, c.; ozolins, v. phys. rev. lett. 2001, 86, 5518; wolverton, c.; yan, x.y.;vijayaraghavan, r.; ozolins, v. acta mater. 2002, 50, 2187.)information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.ellen stechel167catalysis of exhaust gas aftertreatmentexhaust aftertreatment catalysts provide a second, lessintegrated exampleof the use of chemical sciences modeling and simulation to approach an applicationrelated problem. despite large investments in catalysis, both academia andfunding agencies seem to have little interest in catalysis for stoichiometric exhaust aftertreatment. perhaps the perception of a majority of the scientific community is that stoichiometric exhaust aftertreatment is a solved problem. althoughthere is a large empirical knowledge base and a cursory understanding of theprinciples, from my vantage point the depth of understanding is far from adequateespecially considering the stringency of tier 2 tailpipe emissions regulations.this is another challenging and intellectually stimulating research area with critical issues that span the range from the most fundamental to the most applied.after the discovery in the 1960s that tailpipe emissions from vehicles wereadversely affecting air quality, there were some significant technological breakthroughs, and simply moving along a normal evolutionary path, we now haveobtained threeway catalysts that reduce nox and oxidize hydrocarbons and coat high levels of efficiency with a single, integrated, supportedcatalyst system.unfortunately, the threeway catalyst works in a rather narrow range of airfuelratio, approximately the stoichiometric ratio. this precludes some of the opportunities to improve fuel economyñfor example, a leaner airfuel ratio can yieldbetter fuel economy but, with current technology, only at the expense of greaternox emissions. also, most exhaust pollution comes out of the tailpipe in the first50 seconds of vehicle operation, because the catalyst has not yet reached a temperature range in which it is fully active.the threeway catalyst is composed of precious metals on supported aluminawith ceriabased oxygen storage, coated on a highsurfacearea ceramic or a metallic substrate with open channels for the exhaust to pass through and over thecatalyst. as regulations become increasingly stringent for tailpipe emissions, theindustry is transitioning to higher cell densities (smaller channels) and thinnerwalls between channels. this simultaneously increases the surface area, decreasesthe thermal mass, and reduces the hydraulic diameter of the channels; all threeeffects are key enablers for higherefficiency catalysts. the highsurfaceareananostructured washcoat is another key enabler, but it is necessary to maintainthe high surface area of the coating and highly dispersed catalytically active precious metals for the life of the vehicle despite hightemperature operation andexposure to exhaust gas and other chemical contaminants. in other words, thematerial must have adequate thermal durability and resist sintering and chemicalpoisoning from exhaust gas components. neither thermal degradation nor chemical degradation is particularly well understood beyond some general principles;i.e., modeling is difficult without a lot of empirical input.the industry also must design for end of life, which is a very large designspace. simulation currently uses empirically derived, agedcatalyststate input.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.168appendix dwhat the industry really needs is a predictive capability. the industry also has tominimize the cost and the amount of the active catalytic platinum, palladium, andrhodium used, since precious metals are a rare and limited commodity.again a key chemical science challenge is kinetics. how does one obtainquantitative kinetic information for real industrial, heterogeneous catalysts without resorting to timeconsuming experiments? simulations currently use empirically derived simplified kinetics.the science of accelerated aging has generally been ignored. the automotiveindustry must be able to predict aging of catalysts and sensors without runningmany vehicles to 150,000 miles. the industry does currently take several vehiclesand accumulate miles, but driving vehicles to that many miles is a particularlyslow way to get answers and does not provide good statistics. the industry doesutilize accelerated aging, but it is done somewhat empirically without the benefitof a solid foundation of basic science.homogeneous charge compression ignitionthe third and final example arises from the ford motor companyðmit alliance, a partnership that funds mutually beneficial collaborative research. the principal investigators, bill green and bill kaiser, are from mit and ford, respectively.the project spans a range from the most fundamental chemistry and chemical engineering to the most applied. in contrast to the first two examples, this project focuseson a technology in development, as opposed to a technology already in practice.homogeneous charged compression ignition is similar in concept to a dieselengine. it is high efficiency and runs lean. it is compression ignited, as opposed tospark ignited, but it has no soot or nox because it runs much cooler than diesel. itis similar to a gasoline engine in that it uses premixed, volatile fuels like gasoline, and it has similar hydrocarbon emissions. but an hcci engine has a muchhigher efficiency and much lower nox emissions than a gasoline engine, whichcould eliminate the need for the costly threeway precious metal catalyst.however, hcci is difficult to control. there is no simple timing mechanism,which can control ignition, as exists for a spark or diesel fuel injection engine.hcci operates by chemistry and consequently is much more sensitive to the fuelchemistry than either sparkignition or diesel engines. the fuel chemistry in aninternal combustion engine largely ignores the details of the fuel. hcci looksvery promising, but researchers do not yet know what the full operating range isor how to reliably control the combustion with computation. yelvington andgreen have already demonstrated that hcci can work well over a broad range ofconditions demonstrating the promise that computation and simulation can andmust play a continuing and large role in resolving the hcci challenges.99yelvington, p. e.; green, w. h.; sae technical paper 2003, 2003011092.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.ellen stechel169final wordsthe complexity of industrial design problems requires that one must be ableto deal with the realistic, not overly idealized, system. the complexity demands amultidisciplinary approach and working in teams. access to and understanding ofthe full range of methods are generally necessary if the researcher is going tohave impact by solving relevant problems, and individual industrial researchersmust be able to interact and communicate effectively beyond their disciplines. toachieve integration from fundamental science to realworld applications is truly achallenge of coordination, integration, and communication across disciplines,approaches, organizations, and research and development sectors.in general, approximate answers or solutions with a realistic quantificationof uncertaintyñif arrived at quicklyñhave greater impact than highly accurateanswers or solutions arrived at too late to impact critical decisions. often, there isno need for the increased level of accuracy. to quote einstein, òthings should bemade as simple as possible, but not any simpler.ó sometimes researchers in industry, just out of necessity, oversimplify. that is when the automotive development engineer might lose sleep, because it could mean that vehicles might haveunexpected reliability issues in the field, if the oversimplification resulted in awrong decision.simulations with varying degrees of empiricism and predictive capabilityshould be aligned closely with extensive experimental capabilities. it is also important to bring simulation in at the beginning of a new project. too often, experimentalists turn to a computational scientist only after repeated experimental failures. many of these failures would have been avoided had the consultationoccurred at an early stage. the computational expert could help generate hypotheses even without doing calculations or by doing some simple calculations, andthe perspective of the computational researcher can frequently eliminate deadends. in addition, framing questions correctly so that the experiments yield unambiguous results, or reasonably unambiguous results, is crucial. obtaining reasonably definitive answers in a timely manner is equally crucial, but too often, theredoes not seem to be a time scale driving needed urgency.data and knowledge management offer additional overlooked opportunities.hypotheses that have been proven wrong often continue to influence decisions.decision makers too often operate on the basis of disproved or speculative conjectures rather than on definitive data. the science and technology enterpriseneeds a way to manage data such that it is relatively easy to know what thecommunity does and does not know as well as what the assumptions are thatunderlie current knowledge.finally, it is important to have realistic expectations for success. this is adifficult challenge because what one measures and rewards is what one gets.there are many ways to impact applications and technology with any level ofsophistication in a simulation. some of the important ways lend themselves onlyinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.170appendix dto intangible measures, but oftentimes these may be the most important. againquoting einstein, ònot everything that counts can be counted, and not everythingthat can be counted counts.ódrug discovery: a game of twenty questionsdennis j. underwoodinfinity pharmaceuticals, inc.introductionthere is a revolution taking place in the pharmaceutical industry. an era inwhich nearly continuous growth and profitability is taken for granted is comingto an end. many of the major pharmaceutical companies have been in the newslately with a plethora of concerns ranging from the future of the biotechnologysector as a whole to concerns over the availability of drugs to economically disenfranchised groups in the developed and the developing world. the issues forthe pharmaceutical sector are enormous and will likely result in changes in thehealthcare system, including the drug discovery and development enterprises.central challenges include the impact that drugs coming off patents have had onthe recent financial security of the pharmaceutical sector and the need for improved protection of intellectual property rights. historically, generic competition has slowly eroded a companyõs market share. there is a constant battle between the pace of discovering new drugs and having old drugs going off patent,providing generic competition opportunities to invade their market share. recentpatent expirations have displayed a much sharper decline in market share, making new drugs even more critical.the 1990s were a decade in which the pharmaceutical giants believed theycould sustain growth indefinitely by dramatically increasing the rate of bringingnew medicines to market simply by increasing r&d spending and continuing toutilize the same research philosophies that worked in the past. it is clear from therapid rise in r&d expenditure and the resultant cost of discovering new drugsthat the òold equationó is becoming less favorable. there is a clear need to become more efficient in the face of withering pipelines and larger and more complex clinical trials. for large pharmaceutical companies to survive, they mustmaintain an income stream capable of supporting their current infrastructure aswell as funding r&d for the future. the cost of drug development and the lowprobability of technical success call for improved efficiency of drug discoveryand development and further investment in innovative technologies and processesthat improve the chances of bringing a compound to market as a drug. alreadythere has been quite a change in the way in which drugs are discovered. largepharmaceutical companies are diversifying their drug discovery and developmentinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.dennis j. underwood171processes: they are relying more on the inventiveness of smaller biotechnologycompanies and licensing technology, compounds, and biologicals at a faster, morecompetitive rate. to meet critical time lines they are outsourcing components ofresearch and development to contract research organizations, enabling greaterefficiencies by providing added expertise or resources and decreasing development time lines. the trend toward mergers and acquisitions, consolidating pipelines, and attempting to achieve economies of scale is an attempt by large pharmaceutical companies to build competitive organizations. although this may helpshortterm security, future ongoing success may not be ensured solely with thisstrategy.one of the most valuable assets of a pharmaceutical company is its experience in drug discovery and development. of particular importance are the data,information and knowledge generated in medicinal chemistry, pharmacology, andin vivo studies accumulated over years of research in many therapeutic areas.this knowledge is based on hundreds of personyears of research and development; yet most companies are unable to effectively capture, store, and search thisexperience. this intellectual property is enormously valuable. as with the otheraspects of drug discovery and development, the methods and approaches used indata, information and knowledgebase generation and searching are undergoingevolutionary improvements and, at times, revolutionary changes. it is imperativefor all data and informationdriven organizations to take full advantage of theinformation they are generating. we assert that those companies that are able todo this effectively will be able to gain and sustain an advantage in a highly complex, highly technical, and highly competitive domain. the aim of this overviewis to highlight the important role informatics plays in pharmaceutical research,the approaches that are currently being pursued and their limitations, and thechallenges that remain in reaping the benefit of advances. we are using the termòinformaticsó in a general way to describe the processes whereby information isgenerated from data and knowledge is derived as our understanding builds.informatics also refers to the management and transformation of data, information, and assimilation of knowledge into the processes of discovery and development.there has been much time, money, and effort spent in attempting to reducethe time it takes to find and optimize new chemical entities. it has proven difficultto reduce the time it takes to develop a drug, but the application of new technologies holds hope for dramatic improvements. the promise of informatics is toreduce development times by becoming more efficient in managing the largeamounts of data generated during a long drug discovery program. further, withmanaged access to all of the data, information, and experience, discoveries aremore likely and the expectation is that the probability of technical success willincrease.why is the probability of technical success of drug discovery and development so low? what are the issues in moving compounds through the drug discovinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.172appendix dery and development pipeline? it is clear from existing data that the primarybottlenecks are pharmacokinetic problems and lack of efficacy in man. in addition there are problems of toxicity in animal models and the discovery of adverseeffects in man. clearly there are significant opportunities to improve the probability of technical success and, perhaps, to shorten the time line for development. the current strategies bring in absorption, distribution metabolism, excretion and toxicity (admetox) studies earlier in the process (late discovery) anduse programs based on disease clusters rather than a single target.1drug discovery and development is a difficult business because biology andthe interaction with biology is complicated and, indeed, may be classifiable as acomplex system. complexity is due partly to an incomplete knowledge of thebiological components of pathways; the manner in which the components interact and are compartmentalized; and the way they are modulated, controlled, andregulated in response to intrinsic and environmental factors over time. mostly,biological interactions are important and not accounted for in screening and assaying strategies. often model systems are lacking in some way and do not properly represent the target organism. the response of the cell to drugs is very dependent on initial conditions, which is to say that the response of the cell is verydependent on its state and the condition of many subcellular components. typically, the behavior of complex systems is different from those of the components,which is to say that the processes that occur simultaneously at different scales(protein, nucleic acid, macromolecular assembly, membrane, organelle, tissue,organism) are important and the intricate behavior of the entire system is dependent on the processes but in a nontrivial way.2,3 if this is true, application of thetools of reductionism may not provide us with an understanding of the responsesof an organism to a drug. are we approaching a òcausality catastropheó wheneverwe expect the results of in vitro assays to translate to clinical data?what is an appropriate way to deal with such complexity? the traditionalapproach has been to generate large amounts of replicate data, to use statistics toprovide confidence, and to move cautiously, stepwise, toward higher complexity:from in vitro to cellbased to tissuebased to in vivo in model animals and then toman. in a practical sense, the drug discovery and development odds have beenimproved by a number of simple strategies: start with chemically diverse leads,optimize them in parallel in both discovery and later development, back them upwith other compounds when they reach the clinic and follow on with new, structurally different compounds after release. approach diseases by focusing, in parallel, on clusters rather than on a single therapeutic target, unless the target hasproven to be clinically effective. generate more and betterquality data focusingon replicates, different conditions, different cell types in different states, and dif1kennedy, t. drug discovery today 1997, 2 (10), 436444.2vicsek, t. nature 2001, 411, 421.3glass, l. nature 2001, 410, 27784.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.dennis j. underwood173ferent model organisms. develop good model systems, such as for admetox,and use them earlier in the discovery process to help guide the optimization process away from difficulties earlier (fail early). the increase in the amount anddiversity of the data leads to a òdata catastropheó in which our ability to fullyextract relationships and information from the data is diminished. the challengeis to provide informatics methods to manage and transform data and informationand to assimilate knowledge into the processes of discovery and development;the issue is to be able to integrate the data and information from a variety ofsources into consistent hypotheses rich in information.the more information, the better. for example, structurebased design hashad many successes and has guided the optimization of leads through a detailedunderstanding of the target and the way in which compounds interact. this hasbecome a commonly accepted approach, and many companies have large, activestructural biology groups participating in drug discovery teams. one of the reasons that this is an accepted approach is that there is richness in data and information and there is a wealth of methodology available, both experimental and computational, that enables these approaches. the limitations in this area areconcerned primarily with the challenges facing structural biology such as appropriate expression systems, obtaining sufficient amounts of pure protein, and theability to crystallize the protein. these limitations can be severe and can preventa structural approach for many targets, especially membranebound protein complexes. there is also a question of relevancy: does a static, highly ordered crystalline form sufficiently represent dynamic biological events?there are approaches that can be used in the absence of detailed structuralinformation. further, it is often instructive to use these approaches in conjunctionwith structural approaches, with the aim of providing a coherent picture of thebiological events using very different approaches. however, application of thesemethods is extremely challenging primarily due to the lack of detailed data andinformation on the system under study. pharmacophore mapping is one such approach. a complexity that is always present is that there are multiple ways inwhich compounds can interact with a target; there are always slight changes inorientation due to differences in chemical functionality and there are always distinctly different binding modes that are possible. current methods of pharmacophore mapping find it difficult to detect these alternates. further, the approachoften relies on highthroughput screening approaches that are inherently ònoisyómaking the establishment of consistent structure activity relationships difficult.in addition, the methodology developed in this area is often limited to only partsof the entire dataset. there has been much effort directed to deriving two, three, and higherdimensional pharmacophore models, and the use of these models inlead discovery and development is well known.44agrafiotis, d. k.; lobanov, v. s.; salemme, f. r. nat. rev. drug discov. 2002, 1, 33746.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.174appendix dthe key issue in these methods is the manner in which compounds, theirstructure, features, character and conformational flexibility are represented. thereare many ways in which this can be achieved, but in all cases the abstraction ofthe chemistry for computational convenience is an approximation. each compound, in a practical sense, is a question that is being asked of a complex biological system. the answer to a single question provides very little information; however, in an manner analogous to the game of òtwenty question,ó the combinedresult from a wellchosen collection of compounds (questions) can provide anunderstanding of the biological system under study.5 the manner in which chemistry is represented is essential to the success of such a process. it is akin to posinga wellinformed question that, together with other wellformed questions, is powerful in answering or giving guidance to the issues arising from drug discoveryefforts. our approach is to use the principles of molecular recognition in simplifying representation of the chemistry: atoms are binned into simple types such ascation, anion, aromatic, hydrogenbond acceptor and donor, and so forth. in addition, the conformational flexibility of each molecule is represented. the result isa matrix in which the rows are compounds and the columns are bits of a verylarge string (tens of millions of bit long) that mark the presence of features. eachblock of features can be the simple presence of a functional group such as phenyl,chlorophenyl, piperidyl, or aminoethyl, or it can be as complex as three or fourpint pharmacophoric features that combine atom types and the distances betweenthem. the richness of this representation along with a measure of the biologicalresponse of these compounds enables methods that use shannonõs maximumentropy, informationbased approach to discover ensembles of patterns of features that describe activity and inactivity. these methods are novel and have beenshown to capture the essence of the effects of the sar in a manner that can beused in the design of informationbased libraries and the virtual screening ofdatabases of known or realizable compounds.6the power of these methods lies in their ability to discern relationships indata that are inherently ònoisy.ó the data are treated as categorical rather thatcontinuous: active (yes), inactive (no) and a variable category of unassigned data(maybe). these methods are deterministic and, as such, derive all relationshipsbetween all compounds at all levels of support. the relationships or patterns arescored based on their information content. unlike methods such as recursive partitioning, pattern discovery is not ògreedyó and is complete. the discriminationability of pattern discovery depends very much on the quality of the data generated and on the type and condition of the compounds; if either is questionable, the5underwood, d .j. biophys. j. 1995, 69, 21834.6beroza, p.; bradley, e. k.; eksterowicz, j. e.; feinstein, r., greene, j.; grootenhuis, p. d.; henne,r. m.; mount, j.; shirley, w. a.; smellie, a.; stanton, r. v.; spellmeyer, d. c. j. mol. graph.model. 2002, 18, 33542.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.dennis j. underwood175signaltonoise ratio will be reduced and the quality of the results will be jeopardized. these approaches have been used in the study of gprotein coupled receptors7,8 and in admetox studies.9,10these methods have also been used in the identification of compounds thatare able to present the right shape and character to a defined active site of aprotein11,12 in cases where the protein structure is known and the potential binding sites are recognized, the binding site can be translated into a bitstring that isin the same representational space as described above for the compounds. thistranslation is done using methods that predict the character of the space availablefor binding compounds. the ability to represent both the binding site(s) and compounds in the same way provides the mechanism to discriminate between compounds that are likely to bind to the protein. these approaches have been used forserine proteases, kinases and phosphatases.13the game of 20 questions is simple but, almost paradoxically, has the abilityto give the inquisitor the ability to read the subjectõs mind. the way in which thisoccurs is well understood; in the absence of any information there are many possibilities, a very large and complex but finite space. the solution relies on a tenetof a dialectic philosophy in which each question provides a thesis and an antithesis that is resolved by a simple yes or no answer. in so doing, the number ofpossibilities are dramatically reduced and, after 20 questions, the inquisitor isusually able to guess the solution. the solution to a problem in drug discoveryand development is far more complex than a game of 20 questions and should notbe trivialized. even so, the power of discrimination through categorization ofanswers and integration of answers from diverse experiments provides an extremely powerful mechanism for optimizing to a satisfying outcomeña potentialdrug.these approaches have been encoded into a family of algorithms known aspattern discovery (pd).14 pd describes a family of novel methods in the categoryof data mining. one of the distinguishing features of pd is that it discovers rela7wilson, d. m.; termin, a. p.; mao, l.; ramirezweinhouse, m. m.; molteni, v.; grootenhuis, p.d.; miller, k.; keim, s.; wise, g. j. med. chem. 2002, 45, 21236.8bradley, e. k.; beroza, p.; penzotti, j. e.; grootenhuis, p. d.; spellmeyer, d. c.; miller, j. l. j.med. chem. 2000, 43, 27704.9penzotti, j. e.; lamb, m. l.; evensen, e.; grootenhuis, p. d. j. med. chem. 2002, 45, 173740.10clark, d. e.; grootenhuis, p. d. curr. opin. drug discov. devel. 2002, 5, 38290.11srinivasan, j.; castellino, a.; bradley, e. k.; eksterowicz, j. e.; grootenhuis, p. d.; putta, s.;stanton, r. v. j. med. chem. 2002, 45, 2494500. 12eksterowicz, j. e.; evensen, e.; lemmen, c.; brady, g. p.; lanctot, j. k.; bradley, e. k.; saiah,e.; robinson, l. a.; grootenhuis, p. d.; blaney, j. m. j. mol. graph. model. 2002, 20, 46977.13rogers, w. t.; underwood, d. j.; argentar, d. r.; bloch k. m.; vaidyanathan, a. g. proc. natl.acad. sci. u.s.a., submitted.14argentar, d. r.; bloch, k. m.; holyst, h. a.; moser, a. r.; rogers, w. t.; underwood, d. j.;vaidyanathan, a. g.; van stekelenborg, j. proc. natl. acad. sci. u.s.a., submitted.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.176appendix dtionships between data rather than relying on human interpretation to generate amodel as a starting point; this is a significant advantage. another important advantage of pd is that it builds models based on ensembles of inputs to explain thedata and therefore has an advantage in the analysis of complex systems (such asbiology2,3). we have developed a novel approach to pd that has been applied tobiosequence, chemistry, and genomic data. importantly, these methods can beused to integrate different data types such as those found in chemistry and biology. pd methods are quite general and can be applied to many different areassuch as proteomics, text, etc.validation of these methods in biosequence space has been completed usingwelldefined and wellunderstood systems such as serine proteases13 and kinases.pd in biosequence space provides a method for finding ensembles of patterns ofresidues that form a powerful description of the sequences studied. the similaritybetween patterns expresses the evolutionary family relationships. the differencesbetween patterns define their divergence. the patterns express key functional andstructural motifs that very much define the familial and biochemical character ofthe proteins. embedded in the patterns are also key residues that have particularimportance with respect to the function or the structure of the protein. mappingthese patterns onto the xray structures of serine proteases and kinases indicatesthat the patterns indeed are structurally and functionally important, and further,that they define the substratebinding domain of the proteins. this leads to thecompelling conclusion that since the patterns describe evolutionary changes (divergence and convergence) and also describe the critical features of substratebinding, the substrate is the driver of evolutionary change.13a particular application of pd is in the analysis of variations of genetic information (single nucleotide polymorphisms, or snps). analysis of snps can leadto the identification of genetic causes of diseases, or inherited traits that determine differences in the way humans respond to drugs (either adversely or positively). until now, the main method of snp analysis has been linkage disequilibrium (ld), which seeks to determine correlations among specific snps. a keylimitation of ld however is that only a restricted set of snps can be compared.typically snps within a local region of a chromosome or snps within genes thatare thought to act together are compared. pd on the other hand, through its uniquecomputational approach, is capable of detecting all patterns of snps, regardlessof the genomic distances between them. among these will be patterns of snpsthat are responsible for the disease (or trait) of interest, even though the individual snps comprising the pattern may have no detectable disease (or trait)correlation. this capability will greatly accelerate the exploitation of the genomefor commercial purposes.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.177ebiographies of workshop speakerscharles h. bennett is an ibm fellow at ibm research, where he has workedon various aspects of the relation between physics and information. he receivedhis bachelorõs degree from brandeis university, majoring in chemistry, and hisph.d. from harvard in 1970 for molecular dynamics studies (computer simulation of molecular motion). his research has included work on quantum cryptography, algorithmic information theory, and òquantum teleportation.ó he is an ibmfellow, a fellow of the american physical society, and a member of the nationalacademy of sciences.anne m. chaka is the group leader for computational chemistry at the national institute of standards and technology in gaithersburg, maryland. she received her b.a. in chemistry from oberlin college, her m.s. in clinical chemistry from cleveland state university, and her ph.d. in theoretical chemistry fromcase western reserve university. in 19992000, she was maxplancksocietyfellow at the fritzhaberinstitut in berlin. she spent 10 years at the lubrizolcorporation as head of the computational chemistry and physics program andpreviously was technical director of icn biomedicals, inc., an analytical researchchemist for ferro corporation, and a cray programming consultant to case western reserve university for the ohio supercomputer center. active areas of herresearch include atomistic descriptions of corrosion, pericyclic reaction mechanisms, freeradical chemistry, heterogeneous and homogeneous catalysis, thermochemistry, and combustion and oxidation.juan j. de pablo is h. curler distinguished professor of chemical engineering at the university of wisconsinmadison. he received his b.s. fromuniversidad nacional autšnoma de m”xico and his ph.d. from the university ofcalifornia at berkeley. his research interests include thermodynamics, phaseinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.178appendix eequilibria, statistical mechanics, molecular modeling and simulation, and polymer physics. molecular simulations play an important role in his research, inwhich he uses powerful methods and advanced computational techniques to studymolecular motion and to probe the microstructure of fluids and solids with theaim of explaining and predicting macroscopic behavior. professor de pablo haspublished 50 journal articles in the areas of polymer physics, molecular simulations, thermodynamics, and statistical mechanics. he has received the nationalscience foundationõs national young investigator award.thom h. dunning, jr., is distinguished professor of chemistry and chemical engineering at the university of tennessee (ut) and distinguished scientistin computer science and mathematics at oak ridge national laboratory(ornl). he is also director of the joint institute for computational sciences,which was established by ut and ornl to create advanced modeling and simulation methods and computational algorithms and software to solve the most challenging problems in science and engineering. he has authored nearly 150 scientific publications on topics ranging from advanced techniques for molecularcalculations to computational studies of the spectroscopy of highpower lasersand the chemical reactions involved in combustion. dr. dunning received hisb.s. in chemistry in 1965 from the university of missourirolla and his ph.d. inchemical physics from the california institute of technology in 1970. he wasawarded a woodrow wilson fellowship in 19651966 and a national sciencefoundation fellowship in 19661969. he is a fellow of the american physicalsociety and of the american association for the advancement of science.christodoulos a. floudas is professor of chemical engineering at princetonuniversity, associated faculty in the program of applied and computationalmathematics at princeton university, and associated faculty in the department ofoperations research and financial engineering at princeton university. heearned his b.s.e. at aristotle university of thessaloniki, greece, and his ph.d. atcarnegie mellon university. he has held visiting professor positions at imperialcollege, england; the swiss federal institute of technology; the university ofvienna, austria; and the chemical process engineering research institute(cperi), thessaloniki, greece. his research interests are in the area of chemicalprocess systems engineering and lie at the interface of chemical engineering, applied mathematics, operations research, computer science, and molecular biology. the principal emphasis is on addressing fundamental problems in processsynthesis and design, interaction of design and control, process operations, discretecontinuous nonlinear optimization, deterministic global optimization, andcomputational chemistry, structural biology and bioinformatics. he is the recipient of numerous awards for teaching and research that include the nsf presidential young investigator award, 1988; the bodossaki foundation award in applied sciences, 1997; the aspen tech excellence in teaching award, 1999; andthe 2001 aiche professional progress award for outstanding progress in chemical engineering.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.appendix e179richard a. friesner is professor of chemistry at columbia university. hereceived his b.s. degree in chemistry from the university of chicago and hisph.d. from the university of california, berkeley. following postdoctoral workat the massachusetts institute of technology, he joined the chemistry department at the university of texas at austin before moving to columbia in 1990.his research, involving both analytical and computational theory, includes theapplication of quantum chemical methods to biological systems, development ofmolecular mechanics force fields and models for continuum solvation, computational methods for protein folding and structural refinement, prediction of proteinligand binding affinities, and calculation of electron transfer rates in complex molecules and materials.james r. heath is the elizabeth w. gilloon professor of chemistry at thecalifornia institute of technology and he is the director of the californiananosystems institute, which was formed by california governor grey davis indecember 2000. until 2003, he was professor of chemistry and biochemistry atthe university of california, los angeles (ucla). he received a b.sc. degree inchemistry from baylor university and a ph.d. degree in chemistry from riceuniversity. following postdoctoral work at the university of california, berkeley, he was a research staff member at the ibm t.j. watson research laboratories in yorktown heights, new york, from 1991 until 1994 when he joined theucla faculty. heathõs research interests focus on òartificialó quantum dot solidsand quantum phase transitions in those solids; molecular electronics architecture,devices, and circuitry; and the spectroscopy and imaging of transmembrane proteins in physiological environments. he is a fellow of the american physicalsociety and has received the jules springer award in applied physics (2000), thefeynman prize (2000), and the sackler prize in the physical sciences (2001).dimitrios maroudas is professor of chemical engineering at the universityof massachusetts, amherst. prior to accepting his present position in 2002, hewas professor of chemical engineering at the university of california, santa barbara, and a visiting associate professor in the department of chemical engineering at the massachusetts institute of technology for the academic year 20002001. he graduated from the national technical university of athens with adiploma in chemical engineering, received his ph.d. in chemical engineering witha minor in physics from the massachusetts institute of technology, and didpostdoctoral research at ibmõs t.j. watson research center, yorktown heights,new york. professor maroudasõ research interests are in the area of theoreticaland computational materials science and engineering. his research aims at thepredictive modeling of structure, properties, dynamics, processing, and reliabilityof electronic and structural materials, especially semiconductor and metallic thinfilms and nanostructures used in the fabrication of electronic, optoelectronic, andphotovoltaic devices. he has received a faculty early career development (career) award from the national science foundation, a camille dreyfus teacherscholar award, and several teaching awards.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.180appendix elinda r. petzold is professor in the departments of mechanical and environmental engineering, and computer science, and director of the computational science and engineering program at the university of california, santabarbara. from 1978 to 1985 she was a member of the applied mathematics groupat sandia national laboratories in livermore, california, and from 1985 to 1991she was group leader of the numerical mathematics group at lawrencelivermore national laboratory. from 1991 to 1997 she was professor in thedepartment of computer science at the university of minnesota. she receivedher ph.d. in computer science in 1978 from the university of illinois. her research interests include numerical ordinary differential equations, differentialalgebraic equations, and partial differential equations, discrete stochastic systems,sensitivity analysis, model reduction, parameter estimation and optimal controlfor dynamical systems, multiscale simulation, scientific computing, and problemsolving environments. dr. petzold was awarded the wilkinson prize for numerical software in 1991 and the dahlquist prize for numerical solution of differential equations in 1999.george c. schatz is professor of chemistry at northwestern university. hereceived a b.s. degree from clarkson university and a ph.d. in chemistry fromcalifornia institute of technology in 1976. his research is aimed at understanding the interaction of light with nanoparticles and with nanoparticle aggregates.he is also actively working on structural modeling of dna melting and of molecular self assembly processes. in addition, he is interested in timedependentchemical processes such as bimolecular reactions and collisional energy transfer.he is a fellow of the american physical society, the american association forthe advancement of science, the international academy of quantum molecularscience, and the american academy of arts and sciences. he is a recipient of themax planck research award and serves as senior editor of the journal of physical chemistry.larry l. smarr is the harry e. gruber professor of computer science andinformation technologies at the jacobs schoolõs department of computer science and engineering at the university of california, san diego (ucsd). he isthe founding institute director of the california institute for telecommunicationsand information technology, which brings together more than 200 faculty fromucsd and the university of california, irvine and more than 50 industrial partners to research the future development of the internet. prior to moving to ucsdin 2000, he was on the faculty of the university of illinois at urbanachampaigndepartments of physics and of astronomy, where he conducted observational,theoretical, and computationalbased research in relativistic astrophysics. in 1985he was named the founding director of the national center for supercomputingapplications (ncsa) at the university of illinois, and in 1997, he also becamethe director of the national computational science alliance, comprised of morethan 50 universities, government labs, and corporations linked with ncsa in anationalscale virtual enterprise. smarr was an undergraduate at the university ofinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.appendix e181missouri; he earned a masterõs at stanford university and his doctorate from theuniversity of texas at austin. he did postdoctoral work at princeton universityand was a junior fellow in the harvard university society of fellows. he is afellow of the american physical society and the american academy of arts andsciences, and he received the 1990 delmer s. fahrney gold medal for leadership in science or technology from the franklin institute. dr. smarr is a memberof the national academy of engineering.ellen b. stechel is manager of the emissions compliance engineering department for ford motor company, north america engineering, where she recently served as implementation manager for fordõs accelerated catalyst costreduction opportunity program. formerly, she was manager of the chemistryand environmental science department in the scientific research laboratoriesat ford motor company she received her a.b. in mathematics and chemistryfrom oberlin college, her m.s. in physical chemistry from the university ofchicago, and her ph.d. in chemical physics also from the university of chicago.after a postdoctoral research position at ucla, she joined the technical staff atsandia national laboratories, becoming manager of the advanced materials anddevice sciences department in 1994. she left sandia to accept a position at fordmotor company in 1998. she has served as a senior editor for the journal ofphysical chemistry and has been active in several professional societies, including the american vacuum society (where she is currently a trustee).dennis j. underwood is vice president for discovery informatics and computational sciences at infinity pharmaceuticals, inc., in cambridge, massachusetts. before assuming his current position in 2003, he was a director at bristolmyers squibb, wilmington (formerly dupont pharmaceuticals). he received hisb.sc. degree and his ph.d. degree in physical organic chemistry at adelaide university. following postdoctoral work at rockefeller university and cornell university, he returned to australia for additional postdoctoral work at the commonwealth scientific and industrial research organisation (csiro) division ofapplied organic chemistry and then at the australian national university. in1985 he joined the molecular modeling group at the merck research laboratories where he remained as head of molecular modeling until 1998. during thistime, his research included the structurebased design of human leukocyte elastaseinhibitors, virtual screening methodologies and studies of gprotein coupled receptors. dr. underwood moved to dupont pharmaceuticals as a senior director in1998 and was responsible for both discovery informatics and the molecular design group. in his current position, he is responsible for structural biology (protein crystallography) and molecular design. in this role he has continued withdevelopment of novel computer methodologies aimed at drug discovery and development and has continued his work on structurebased design on gproteincoupled receptors as drug targets.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.fparticipantschallenges for the chemical sciences in the 21stcentury: workshop on information andcommunicationsoctober 31november 2, 2002richard c. alkire, university of illinois at urbanadaniel auerbach, ibm researchpaul i. barton, massachusetts institute of technologycharles h. bennett, ibm research corporationdavid l. beveridge, wesleyan universitycurt m. breneman, rensselaer polytechnic instituteronald breslow, columbia universitydonald m. burland, national science foundationstanley k. burt, national cancer instituteanne m. chaka, national institute of standards and technologydennis i. chamot, the national academiesthomas w. chapman, national science foundationhongda chen, u.s. department of agriculturezhongying chen, saicdean a. cole, u.s. department of energypeter t. cummings, vanderbilt universitydavid a. daniel, american chemical societyjuan j. de pablo, university of wisconsin, madisonbrett i. dunlap, u.s. naval research laboratorythom h. dunning, jr., joint institute for computational sciences, university oftennessee, oak ridge national laboratory182information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.appendix f183gregory farber, national institutes of healthchristodoulos a. floudas, princeton universityrichard friesner, columbia universitymichael frisch, gaussian, inc.christos georgakis, polytechnic universitystephen gray, argonne national laboratorymihal e. gross, rand, science and technology policy instituteignacio e. grossmann, carnegie mellon universitypeter gund, ibm life sciencescarol handwerker, national institute of standards and technologywilliam l. hase, wayne state universityjames r. heath, university of california, los angelesjudith c. hempel, university of texas at austinrichard l. hilderbrandt, national science foundationdaniel hitchcock, u.s. department of energypeter c. jurs, pennsylvania state universitymiklos kertesz, georgetown universityatsuo kuki, pfizer global research & developmentmichael kurz, illinois state universityangelo lamola, worcester, pennsylvaniakuyen li, lamar universitykenneth b. lipkowitz, north dakota state universitymehrdad lordgooei, drexel universityandrew j. lovinger, national science foundationpeter ludovice, georgia institute of technologydimitrios maroudas, university of massachusetts, amherstglenn j. martyna, ibm t.j. watson research laboratorypaul h. maupin, u.s. department of energygregory j. mcrae, massachusetts institute of technologyjulio m. ottino, northwestern universitydimitrios v. papvassiliou, university of oklahomalinda r. petzold, university of california, santa barbarajohn a. pople, northwestern universitylarry a. rahn, sandia national laboratorymark a. ratner, northwestern universityrobert h. rich, american chemical society, petroleum research fundceleste m. rohlfing, national science foundationdavid rothman, dow chemical companygeorge c. schatz, northwestern universitypeter p. schmidt, office of naval researchmichael schrage, massachusetts institute of technologyrobert w. shaw, u.s. army research officelarry l. smarr, university of california, san diegoinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.184appendix fellen b. stechel, ford motor companywalter j. stevens, u.s. department of energyterry r. stouch, bristolmyers squibb companymichael r. thompson, pacific northwest national laboratorym. silvina tomassone, rutgers universitymark e. tuckerman, new york universityjohn c. tully, yale universitydennis j. underwood, bristolmyers squibb companyernesto vera, dow chemical companydion vlachos, university of delawarealfons weber, national science foundationphilip r. westmoreland, university of massachusettsralph a. wheeler, university of oklahomawilliam t. winter, state university of new yorkinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.185greports from breakout session groupsa key component of the workshop on information and communicationswas the set of four breakout sessions that enabled individual input by workshopparticipants on the four themes of the workshop: discovery, interfaces, challenges,and infrastructure. each breakout session was guided by a facilitator and by theexpertise of the individuals as well as the content of the plenary sessions (tableg1). participants were assigned one of three groups on a random basis, althoughindividuals from the same institution were assigned to different breakout groups.each breakout group (colorcoded as red, yellow, and green) was asked to address the same set of questions and provide answers to the questions, includingprioritization of the voting to determine which topics the group concluded weremost important. after every breakout session, each group reported the results ofits discussion in plenary session.the committee has attempted in this report to integrate the information gathered in the breakout sessions and to use it as the basis for the findings containedherein. when the breakout groups reported votes for prioritizing their conclusions, the votes are shown parenthetically in this section.information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.186appendix gdiscoverywhat major discoveries or advances related to information and communications have been made in the chemical sciences during the last several decades?red group reportprioritized list:analytical instrumentation and data visualization (11)computational materials science (10)process design and optimization (6)drug design and bioinformatics (5)environmental chemistry and modeling (5)materials applications (4)advances in quantum mechanics (3)table g1organization of breakout sessions.breakout sessiongroupfacilitatorsession chairrapporteurdiscoveryredd. raberj. ottinop. jursyellowp. cummingsj. tullye. veraj. hempelgreenj. jackiwk. lipkowitzc. brenemaninterfacesredp. cummingsj. tullyg. mcreaj. hempelyellowj. jackiwk. lipkowitzr. friesnergreend. raberj. ottinop. westmorelandchallengesredj. jackiwk. lipkowitzg. mcreayellowd. raberj. ottinol. rahng. martynagreenp. cummingsj. tullyp. westmorelandj. hempelinfrastructureyellowredd. raberj. tullym. tuckerman(with g. mcrea)greenredj. jackiwk. lipkowitzp. gundj. ottinoc. brenemaninformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.appendix g187yellow group reportmolecular simulation of chemical systemsmaterials, polymers, advanced materials, crystals and nanostructures, simulation techniques (dft, monte carlo and md)microfabrication by chemical methods (e.g., lithography)synthesis, design, and processing of new materialscomputeraided drug designqsar, molecular modelingnumerical simulation of complex chemical processes involving reactiontransportozone layer, combustion, atmospheric chemistry, chemical vapor depositionprocess simulation, optimization, and controlsupplychain optimization, plant design, and remote control of plantsgreen group reportelectronic structure theory (9)common language of electronic structure theory, validating density functional theory, standardization of gaussian basis sets, semiempirical methods,b3lyp functional (beckeõs threeparameter hybrid functional using the lypcorrelation functional), welldefined procedures for qm algorithmsresearch simulation software for all science (8)gaussian software, archiving chemical information, software developmentfor the community, process control and process design softwarecomputeraided molecular design (7)combinatorial chemistry for drug studyinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.188appendix gchemical kinetics and dynamics (5)coupling electronic structure theory with dynamics (direct dynamics), kinetics from firstprinciplesñrate constants, stochastic simulation theorypotential functions and sampling methods (4)monte carlo and molecular dynamics for statistical thermodynamics, potential energy functions and applications, discovery of applicability of potential energy functions, reactive empirical bond order potentialsdatadriven screening (2)application of statistical learning theory and qspr, data mining and combinatorial highthroughput methods, virtual highthroughput screeningsynthesis of materials for computing (1)photolithography polymers, metallization, optical fibersinterfaceswhat are the major computingrelated discoveries and challenges at the interfaces between chemistryðchemical engineering and other disciplines, including biology, environmental science, information science, materials science, andphysics?red group reporttargeted designhow do we go backward from the desired specifications and functions toidentify the molecules and processes?a matrix view is neededrows (discovery opportunities): biosytems modeling, proteins, drug design,materials design, environmental modeling (lifecycle analysis), molecular electronics, nanosciencescolumns (challenges to achieving discoveries): combinatorial search algorithms, data assimilation, multiscale modeling, structurefunction and structureactivity relationships, largescale computation, how to create interdisciplinaryteams as well as support the critical need for disciplinary researchinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.appendix g189results: the matrix is dense (every cell is a yes); people issues are critical;opportunities exist for both methodology development and application; there aremany rows and columns, so the challenges are prioritization and maximizing theintersections; we need to show relevance, examples and benefits of what we aretrying to do.yellow group reportmolecular design (8)modeling at an atomic scale, small molecules vs. larger structures (one levelof multiscale modeling); examples include design of drugs, nanostructures, catalysts, etc.materials design (18)more complex structures, additional length scales, property prediction (e.g.,microelectronics devices, microfabrication, polymers, drug delivery systems,chemical sensors)multiscale modeling (17)incorporates all length scales, important in all disciplines; examples includemodeling of a cell, process engineering, climate modeling, brain modeling, environment modeling, combustiondata analysis and visualization (6)analysis of high throughput data, seamlessness of large datasets, modelingof highthroughput datasets, information extraction, component architectures forvisualization and computingnew methods development (6)green group reportcomputer science and applied mathhumancomputer interface, expert systems, software engineering, highperformance computing, databases, visualizationinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.190appendix genvironmental sciencefate of chemicals, computational toxicology, climate change, speciationbiologyprotein structurefunction; protein sequencestructure; networks (kinetic,regulatory, signaling); bioinformatics; dynamics in solvent environmentphysicschemical dynamics, scattering theory, molecular astrophysics, hightemperature superconductorsmaterials science and engineeringpolymer property prediction, largesystems modeling (mesoscale simulation), nanostructures, corrosionearth sciencewater chemistry; binding of chemical species to illdefined substrates; hightemperature, highpressure chemistry; atmospheric chemistrycrosscutting issuesthe issues are pervasive: every matrix entry is a yes.matrix columns (computingrelated problem areas): software and interfaces(expert systems, development and maintenance and incentives); systems biology(chemistry, physics, and processesñcomplexity); molecular dynamics with quantitative reactive potentials; chemical and physical environment of calculations(e.g., solvation); simulationdriven experiments; specialpurpose computing hardware; closing the loop for computational chemistry with analytical instrumentsand methods; database access, assessment, and traceabilitymatrix rows (disciplines that interact with chemistryðchemical engineering):computational science and mathematics; environmental science; biology; physics; materials science and engineering; earth scienceschallengeswhat are the information and communications grand challenges in the chemical sciences and engineering?information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.appendix g191red group reportkey messagereducing the time to solutionsome general pointsthere are many possible information and communications challenges; society: no chemicals in this product; education: communicating excitement; technical: information gathering and exchange; new disciplines can emerge or haveemerged from taking a systematic approach to knowledge storage and representation (e.g., genomics); if we are to be successful in our recommendations we needto address the challenges, opportunities and benefits (particularly with examples).a matrix viewrows: grand challenge problems; columns: information and knowledgebased methodologiesdiscovery opportunities (rows)simulating complex systems, cell, combustion, polyelectrolytes and complex fluids, atmospheres, hydrogeology, catalysts under industrial conditions,drug design, proteindna interactions, proteinprotein, rna folding, proteinfolding, metabolic networks, conformational samplingknowledge issuesknowledge database and intelligent agents; assimilating sensorðmeasurementðinformation data, data exchange standards (xml, etc.); representation ofuncertainties in databases, building and supporting physical property archives(nist, webbook, etc.); collaborative environmentsmethodology issuesmodelbased experimental design, long time scale dynamics, virtual measurements, force field validation and robustness, quantum dynamics, dispersion,excited statessystems approaches to complex problemsintelligent and distributed databases (the òchemistry googleó), nextgenerainformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.192appendix gtion òlaboratory notebooksó for models, databases, programs, and experimentaldata, nextgeneration databases, communication, sampling, and interaction acrossscales; this is not a new issue.yellow group reportlongterm challengescomprehensive data and computational problemsolving environment (12):seamless chemical information, computer human interfacesimultaneous product and process development (12): for example, drug discovery, cradletograve chemical productiondesign of new molecule given required functionality (11): design lowestenergy consumption path, proof of concept for computational drug designvirtual cell (4)design biocompatible materials (3): artificial tissue and bone, artificial organ, posttranslational protein modificationshort term challengesdesigncontrolled directed selfassembly (10)selfassembly of protein in membrane (5)chemical plant on a chip (4)predict crystal structures (small organics) (4)computational electrochemistry (4)accurate freeenergy calculation of medium size molecules (3)green group reportmolecularly based modelingelectronic structure methods for 104+ atoms, binding energies in arbitrarilycomplex materials, molecular processes in complex environments, computationaltools to study chemical reactions (first principles, limits of statistical reaction ratetheories), simulators with quantum dynamics for quantum degrees of freedomsystems issuesintegration of applied math and computer science to obtain more powerfulsoftware tools for the chemical sciences and engineering, modeling the origin anddevelopment of life, alternative chemical bases for life (other worlds), computational disease modeling, design of bioinspired materials, global climate controlinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.appendix g193(the earth as a chemical factoryñsimilar to grovesõ chip as a chemical factory),modeling of atmospheric chemistrymeasurementsincorporation of computational chemistry methods into instruments, virtualmeasurement, enabling ubiquitous sensing, in vivo biosensorseducation and usage issuestranslate molecularly based modeling into education, expert systems formethod choices (precision in context of time and cost)infrastructurewhat are the issues at the intersection of computing and the chemical sciences for which there are structural challenges and opportunitiesñin teaching,research, equipment, codes and software, facilities, and personnel?yellowred group reporttopics to be addressedteaching, research, equipment, codes and software, facilities, personnelteachingrelated infrastructure issuescomputational chemistry courses, interdisciplinary computational sciencecourses, software engineering (not just programming), test problems and design casestudies, support for software maintenance, modeling and simulationbased courses;top two issues: modelingandsimulation course, and mathematics trainingresearchrelated infrastructure issuesmultiscale modeling algorithms, representation and treatment of uncertainties, standard test cases (software, experiments), funding of interdisciplinary research, shared instruments; top two issues: multiscale modeling algorithms andfunding of interdisciplinary researchequipmentrelated infrastructure issuesaccess to highperformance computational facilities; highbandwidth accessto instruments and computers; collaborative environments; clusters and tightlyinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.194appendix gcoupled, diverse architectures; assimilation of information from sensors;interoperability and portability; distributed databases; shared resources (equipment, software, computers); access to a diverse set of architectures; top two issues: access to a diverse set of architectures, and interoperability and portabilitycodes and softwarerelated infrastructure issuesintegration of multivendor software systems, software maintenance (laboratory technicians), open source, code and data warehouses, component architectures, interoperability, security; top two issues: component architectures and opensource codepersonnel relatedinfrastructure issuespoor computer and software engineering skill levels of incoming students,tenure and promotion of interdisciplinary people, training of chemists with chemical engineering concepts, people in the pipeline, attracting interest of computerscience community; top two issues: tenure and promotion of interdisciplinarypeople, and people in the pipelinegreenred group reportwhat parts are working?commercial software companies serving this area: computational chemistry,chemical processeschemistðchemical engineer collaboration works well where it existsmodern programming tools have speeded code developmentnetworking: internet highspeed connectivitywhat parts are not working?commercial software efforts limited by market size: limited new science,porting to new machines, commercialization can kill technology, coding complexprograms still very difficult, highperformance computing centers are not generally useful, proposal process too restrictive, doe centers working better thannsf centerseducation of new computational chemists and chemical engineers inadequate: undergraduateðgraduate curriculum needs to address these subjects, applications and scientific programming (colorado school of mines engineeringweb site is doing this)academic code sharing and support mechanisms poor: code developmentinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.appendix g195not supported by grants, no money or staff for maintenance and support, duplication of effort on trivial code; need open source toolkits and librariesdatabases and knowledge bases: not generally available except inbioinformatics, need standards and data validationcrossdiscipline collaboration and teamwork inadequate: overspecializationin graduate school departmentswhat payoffs are expected?proof of profitñdemonstration that simulation methods can make differencein business outcome, òhome runó in drug discovery, more incorporation of methods into educational programs (better understanding of scope and limitations ofmethods), make specialists in industry more connected and useful (leveragingcurrent staff), better communication and teaming of chemists and chemical engineers (more joint appointments, maybe combining departments), commonality inunderstanding and language between users and nonusershow can community assure reliability, availability, and maintenance ofcodes?recognize contributions of coders, support full life cycle of codes, agenciesshould fund code development and maintenance as part of grants, demonstrate valueof simulation codes, funding incentives for crossdisciplinary software creationinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.hlist of acronymsacsamerican chemical societyadmetoxabsorption, distribution, metabolism, excretion, and toxicityafosrair force office of scientific researchafsandrew file systemaicheamerican institute of chemical engineersasciadvanced simulation and computing programbomdconventional bornoppenheimer molecular dynamicscbscomplete basis setcccbdbnist computational chemistry comparison and benchmarkdatabaseccsdcoupled cluster theory with single and double excitationsccsd(t)ccsd with perturbative correction for triple excitationsciconfiguration interactioncmdecollaborative modelingdata environmentscpmdcarparinello molecular dynamicsdaedifferentialalgebraic equationsdddirect dynamicsdftdensity functional theoryecepp/3empirical conformational energy program for peptidesemslenvironmental molecular sciences laboratory196information and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.appendix h197gbgeneralized bornhfhartreefockilpinteger linear programmingitinformation technologyg2gaussian2 theoryldlinkage disequilibriumldalocal density approximationllnllawrence livermore national laboratorymdmolecular dynamicsmp2second order perturbation theorymrimagnetic resonance imagingodeordinary differential equationsqmmmquantum mechanicsðmolecular mechanicsmemsmicroelectromechanical system(s)nasanational aeronautics and space administrationncrennorth carolina research and education networkncsanational center for supercomputing applicationsnfsnetwork file systemnihnational institutes of healthnistnational institute of standards and technologynmrnuclear magnetic resonancensecnational science and engineering centernsfnational science foundationpdbprotein data bankpdepartial differential equationspbpoissonboltzmannpdpattern discoverypetscportable, extensible toolkit for scientific computingpnnlpacific northwest national laboratorypscpittsburgh supercomputing centerpunchpurdue university network computing hubsqcdquantum chromodynamicsqcdocqcd on a chipinformation and communications: challenges for the chemical sciences in the 21st centurycopyright national academy of sciences. all rights reserved.198appendix hqmmmquantum mechanicsðmolecular mechanicsqsarquantitative structureactivity relationshipqsprquantitative structureproperty relationshipsnpsingle nucleotide polymorphismsunicoreuniform interface to computing resourcesvnmrfvirtual nmr facilitywtecworld technology evaluation center, inc.
detailsdistribution, posting, or copying of this pdf is strictly prohibited without written permission of the national academies press. (request permission) unless otherwise indicated, all materials in this pdf are copyrighted by the national academy of sciences.copyright © national academy of sciences. all rights reserved.the national academies pressvisit the national academies press at nap.edu and login or register to get:œ œ 10% off the price of print titlesœ special offers and discountsget this bookfind related titlesthis pdf is available at sharecontributorshttp://nap.edu/18258the future of scientific knowledge discovery in opennetworked environments: summary of a workshop200 pages | 8.5 x 11 | paperbackisbn 9780309267915 | doi 10.17226/18258paul f. uhlir, rapporteur; board on research data and information; policy andglobal affairs; national research councilthe future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments  summary of a workshop          the national academies press washington, d.c. www.nap.edu the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved. the national academies press 500 fifth street, nw washington, dc 20001 notice: the project that is the subject of this report was approved by the governing board of the national research council, whose members are drawn from the councils of the national academy of sciences, the national academy of engineering, and the institute of medicine. the members of the committee responsible for the report were chosen for their special competences and with regard for appropriate balance. this study was supported by the national science foundation under grant no. 1042078. this report was prepared as an account of work sponsored by an agency of the united states government. neither the united states government nor any agency thereof, nor any of their employees, makes any warranty, express or implied, or assumes any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed, or represents that its use would not infringe privately owned rights. reference herein to any specific commercial product, process, or service by trade name, trademark, manufacturer, or otherwise does not necessarily constitute or imply its endorsement, recommendation, or favoring by the united states government or any agency thereof. any opinions, findings, conclusions, or recommendations expressed in this publication are those of the author(s) and do not necessarily reflect the views of the organizations or agencies that provided support for the project. international standard book number13: 9780309267915 international standard book number10: 9780309267919  additional copies of this report are available from the national academies press, 500 fifth street, nw, room 360, washington, dc 20001; (800) 6246242 or (202) 3343313; http://www.nap.edu.  copyright 2012 by the national academy of sciences. all rights reserved. printed in the united states of america   the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved. the national academy of sciences is a private, nonprofit, selfperpetuating society of distinguished scholars engaged in scientific and engineering research, dedicated to the furtherance of science and technology and to their use for the general welfare. upon the authority of the charter granted to it by the congress in 1863, the academy has a mandate that requires it to advise the federal government on scientific and technical matters. dr. ralph j. cicerone is president of the national academy of sciences. the national academy of engineering was established in 1964, under the charter of the national academy of sciences, as a parallel organization of outstanding engineers. it is autonomous in its administration and in the selection of its members, sharing with the national academy of sciences the responsibility for advising the federal government. the national academy of engineering also sponsors engineering programs aimed at meeting national needs, encourages education and research, and recognizes the superior achievements of engineers. dr. charles m. vest is president of the national academy of engineering. the institute of medicine was established in 1970 by the national academy of sciences to secure the services of eminent members of appropriate professions in the examination of policy matters pertaining to the health of the public. the institute acts under the responsibility given to the national academy of sciences by its congressional charter to be an adviser to the federal government and, upon its own initiative, to identify issues of medical care, research, and education. dr. harvey v. fineberg is president of the institute of medicine. the national research council was organized by the national academy of sciences in 1916 to associate the broad community of science and technology with the academy™s purposes of furthering knowledge and advising the federal government. functioning in accordance with general policies determined by the academy, the council has become the principal operating agency of both the national academy of sciences and the national academy of engineering in providing services to the government, the public, and the scientific and engineering communities. the council is administered jointly by both academies and the institute of medicine. dr. ralph j. cicerone and dr. charles m. vest are chair and vice chair, respectively, of the national research council. www.nationalacademies.org  the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.v the future of scientific knowledge discovery in open networked environments: a national workshop steering committee john leslie king, chair william warner bishop collegiate professor of information university of michigan hal abelson professor, massachusetts institute of technology francine berman vice president of research, rensselaer polytechnic institute bonnie carroll president, information international associates michael carroll professor, american university, washington college of law alyssa goodman professor, harvard university sara graves director, information technology and systems center university professor of computer science university of alabama in huntsville michael lesk professor, rutgers university gilbert omenn professor, university of michigan  project staff paul f. uhlir board director the national academies daniel cohen program officer the national academies [on detail from library of congress] cheryl levey senior program associate the national academies the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.vi board on research data and information œ membership michael lesk, chair (until 11/2011) rutgers university roberta balstad, vice chair (until 11/2011) columbia university francine berman, cochair rensselaer polytechnic institute clifford lynch, cochair coalition for networked information maureen baginski (until 11/2011) serco laura bartolo kent state university r. steven berry (until 11/2011) university of chicago christine borgman (until 11/2011) university of california, los angeles philip bourne university of california, san diego norman bradburn (until 11/2011) university of chicago henry brady university of california, berkeley mark brender geoeye foundation bonnie carroll information international associates michael carroll  american university, washington college of law sayeed choudhury  the johns hopkins university keith clarke university of california, santa barbara paul a. david  stanford university kelvin droegemeier university of oklahoma clifford duke ecological society of america barbara entwisle  university of north carolina stephen friend sage bionetworks michael goodchild (until 11/2011) university of california, santa barbara alyssa goodman (until 11/2011) harvard university margaret hedstrom  university of michigan michael keller (until 11/2011) stanford university alexa t. mccray harvard medical school michael r. nelson (until 11/2011) georgetown university daniel reed (until 11/2011) microsoft research, microsoft inc. alan m. title lockheed martin advanced technology center ann j. wolpert massachusetts institute of technology cathy h. wu (until 11/2011) university of delaware and georgetown university medical center the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.vii board on research data and information staff paul f. uhlir board director subhash kuvelker senior program officer  daniel cohen program officer [on detail from library of congress]  cheryl levey senior program associate  the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.ix preface and acknowledgments  digital technologies and networks are now part of everyday work in the sciences, and have enhanced access to and use of scientific data, information, and literature significantly. they offer the promise of accelerating the discovery and communication of knowledge, both within the scientific community and in the broader society, as scientific data and information are made openly available online. the phrase ﬁscientific knowledge discovery in open networked environmentsﬂ is subject to many definitions. for purposes of this project, the focus was on computermediated or computational scientific knowledge discovery, taken broadly as any research processes enabled by digital computing technologies. such technologies may include data mining, information retrieval and extraction, artificial intelligence, distributed grid computing, and others. these technological capabilities support computermediated knowledge discovery, which some believe is a new paradigm in the conduct of research. the emphasis was primarily on digitally networked data, rather than on the scientific, technical, and medical literature. the meeting also focused mostly on the advantages of knowledge discovery in open networked environments, although some of the disadvantages were raised as well. the workshop brought together a set of stakeholders in this area for intensive and structured discussions. the purpose was not to make a final declaration about the directions that should be taken, but to further the examination of trends in computational knowledge discovery in the open networked environments, based on the following questions and tasks: 1. opportunities and benefits: what are the opportunities over the next 5 to 10 years associated with the use of computermediated scientific knowledge discovery across disciplines in the open online environment? what are the potential benefits to science and society of such techniques? 2. techniques and methods for development and study of computermediated scientific knowledge discovery: what are the techniques and methods used in government, academia, and industry to study and understand these processes, the validity and reliability of their results, and their impact inside and outside science? 3. barriers: what are the major scientific, technological, institutional, sociological, and policy barriers to computermediated scientific knowledge discovery in the open online environment within the scientific community? what needs to be known and studied about each of these barriers to help achieve the opportunities for interdisciplinary science and complex problem solving? 4. range of options: based on the results obtained in response to items 1œ3, define a range of options that can be used by the sponsors of the project, as well as other similar organizations, to obtain and promote a better understanding of the computermediated scientific knowledge discovery processes and mechanisms for openly available data and information online across the scientific domains. the objective of defining these options is to improve the activities of the sponsors (and other similar organizations) and the activities of researchers that they fund externally in this emerging research area.  the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.x the first day of the 2day meeting consisted primarily of invited expert speakers, who addressed tasks 1œ3. this was followed immediately by a workshop on the second day to leverage the expertise of the invitees to address task 4, based on the discussions of tasks 1œ3 on the first day. the slides presented by the speakers at the meeting are posted on the national academy of sciences™ board on research data and information web site and the entire meeting was webcast.1 this report has been prepared by the workshop rapporteur as a factual summary of what occurred at the workshop. the committee™s role was limited to planning and convening the workshop. the views contained in the report are those of the individual workshop participants and do not necessarily represent the views of all workshop participants, the steering committee, or the national academies. it can be argued that too much time has passed since the meeting took place, and that the results of this effort are not timely enough to provide insight. in fact, the elapsed time between the events reported here and this report has provided time to assess the issues with more care. we are grateful to the national science foundation (nsf) for support of this project under nsf grant number 1042078. this volume has been reviewed in draft form by individuals chosen for their technical expertise, in accordance with procedures approved by the national research council™s report review committee. the purpose of this independent review is to provide candid and critical comments that will assist the institution in making its published report as sound as possible and to ensure that the report meets institutional standards for quality. the review comments and draft manuscript remain confidential to protect the integrity of the process. we wish to thank the following individuals for their review of this report: sayeed choudhury, johns hopkins university; stephen hilgartner, cornell university; michael kurtz, harvard university; robert mcdonald, indiana university; mark parsons, university of colorado; jack stankovic, university of virginia; and katherine strandburg, new york university. although the reviewers listed above have provided constructive comments and suggestions, they were not asked to endorse the content of the individual papers. responsibility for the final content of the papers rests with the individual authors. 1available at http://sites.nationalacademies.org/pga/brdi/pga060424. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.xi we would especially like to recognize the contributions of daniel cohen, on assignment to the national academies from the u.s. library of congress, who assisted with the editing and the production of the manuscript cheryl levey of the board staff also helped with the review process and the preparation of this volume. finally, we would like to thank raed sharif for his editorial support in completing this manuscript.  john leslie king steering committee chair paul f. uhlir project director      the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.xiii  contents 1. opening session 1 introduction, 1 john leslie king opening remarks by project sponsors, 3 alan blatecky sylvia spengler keynote address: an overview of the state of the art, 7 tony hey discussion, 17 2. experiences with developing open scientific knowledge discovery in research and applications 19 case studies international online astronomy research, 19 alberto conti integrative genomic analysis, 25 stephen friend geoinformatics: linked environments for atmospheric discovery, 31 sara graves implications of the three scientific knowledge discovery case studies œ the user perspective international online astronomy research, c. 2011, 37 alyssa goodman integrative genomic analysis, 45 joel dudley geoinformatics, 55 mohan ramamurthy discussion, 61 3. how might open online knowledge discovery advance the progress of science? 69 technological factors session chair: hal abelson interoperability, standards, and linked data, 71 james hendler national technological needs and issues, 77 deborah crawford discussion, 81 sociocultural, institutional, and organizational factors, 83 session chair: michael lesk sociocultural dimensions, 85 clifford lynch institutional factors, 89 paul edwards discussion, 96 policy and legal factors, 99 session chair: michael carroll legal aspects, 101 michael madison knowledge discovery in open networked environments: some policy issues, 107 gregory a. jackson discussion, 114 how can we tell? what needs to be known and studied to improve potential for success?, 117 session chair: francine berman introduction, 119 the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.xiv francine berman an academic perspective, 121 victoria stodden a government perspective, 127 walter l. warnick discussion, 130 4. summary of workshop results from day one and discussion of additional issues 133 introduction, 135 bonnie carroll opportunities and benefits for automated scientific knowledge discovery in open networked environments, 137 puneet kishor techniques and methods for development and study of automated scientific knowledge discovery, 147 alberto pepe barriers to automated scientific knowledge discovery in open networked environments, 155 alberto pepe range of options for further research, 163 puneet kishor 5. appendix: workshop agenda 179 the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 1  1. opening session introduction john leslie king university of michigan as the chair of the committee that is organizing this twoday event on ﬁthe future of scientific knowledge discovery in open networked environments,ﬂ i would like to thank primarily the national science foundation, but also the library of congress and the department of energy, for sponsoring this project. the event was organized by two boards of the national academies, the board on research data and information in collaboration with the computer sciences telecommunications board. in my opening remarks, i would like to address two points: one on the motivation for this event and the other one on the means. the motivation lies in the fact that we are undergoing a relatively rapid change in capability, because of technology and improved knowledge. what is possible today was not even thinkable some years ago for data generation, storage, management, and sharing. developments in these areas are having a rather profound impact on the nature of science. this is not new historically, however. when the technologies of optics were developed and the microscope and telescope were created, these technologies changed what we could do, but they did not immediately change what we did do. over time, there was an evolution of social conventions, of data record keeping, and so forth. take for example the classical story of tycho brahe, who spent years looking through telescopes and taking detailed measurements, and later his data were instrumental in johannes kepler™s proof that the orbits of the plants were elliptical rather than circular. that kind of practice has been repeated many times in the history of science, and it is being repeated now. some of my colleagues at the university of michigan are working on the atlas project at the large hadron collider in cern, the european center for nuclear research. atlas is an interesting operation. it has nearly 3,000 principal investigators, and many of the papers produced through this project have more pages dedicated to the listing of authors than to the paper itself. obviously they have had to develop new conventions to deal with this situation. if we ask them about how they know who did what in a project or a paper, they will say that they just know. that is probably more tacit knowledge than it is explicit knowledge. the atlas detector is capable of generating a petabyte of data per second; most of these data are thrown away immediately as not relevant for the work. the amount of data left is still huge, however, and cern distributes large volumes of data globally through different networks. such big and important operations have to be planned. we cannot just expect that information technology people will take care of that simply because nobody has ever done it before. it took years to plan the data distribution plan for cern. this, of course, did not involve the details of how the data get used, how credit gets assigned, and so forth. the point i am trying to make is that this is an emerging and very challenging frontier, and that we are still the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.2 the future of scientific knowledge discovery in open networked environments  at the infant stage of it. the means are to address four issues: (1) the opportunityœbenefit space; (2) the techniques that we know now and those that we think are going to emerge; (3) the barriers we have to overcome; and (4) the options that we can use to move forward. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 3  opening remarks by project sponsors  alan blatecky national science foundation  thinking about the cyberinfrastructure and the impact of data, we need to understand that it is an ecosystem with multiple components that are dependent upon each other. figure 11 presents a very complex and interconnected world of cyberinfrastructure. although data is a key component, there are other components that impact data dramatically; this includes networking, software, computational resources, expertise, and so on. figure 11 cyberinfrastructure. source: alan blatecky  furthermore, figure 11 makes two points. first, data cannot be considered as a separate entity. second, as data capabilities grow, this has an impact on networking requirements, on organizations, on expertise, and so forthšand also vice versa. advances in one component affect other areas. as mentioned with the atlas project example in the opening presentation, as advanced scientific instruments are developed, they enable tremendous new capabilities. the challenge is how to accommodate and take advantage of these new capabilities, and how does this impact the cyberinfrastructure environment? figure 12 addresses four data challenges. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.4 the future of scientific knowledge discovery in open networked environments  figure 12 scientific data challenges. source: alan blatecky  the first point is the growth and useful lifetime in data from today to what is expected in 2020. while terabytes and petabytes of data and information are currently being generated, in less than 10 years we are going to have volumes in petabytes and exabytes. in short, there is going to be a tremendous growth in the amount of scientific data being generated, and there can be a long lifetime associated with the data. therefore, issues of curation, sustainability, preservation, and metadata approaches need to be addressed. the second challenge is the volume of data by discipline, instrument, or observatory. in figure 12, the vertical dimension of the ellipses is intended to illustrate the volume of data being generated by that area. as prof. king pointed out, the large hadron collider (lhc) will be generating a gigabyte per secondšmultiple terabytes a day, in essencešand the figure illustrates how much volume that is. the third challenge is distribution. in figure 12, the horizontal dimension of the ellipse illustrates the amount of distribution for that data. for example, the data from the lhc or large synoptic survey telescope will be stored in a few locations; hence, the ellipse is fairly narrow. genomics data or climate data, on the other hand, will be located at literally hundreds, if not thousands, of locations. the point here is that distribution is a very different challenge from volume or lifetime. genomic data are being generated at incredible rates at many universities and medical centers, and managing that data across hundreds of sites is a significant challenge. the fourth challenge deals with data access. how will the data be accessed and what policies are required for access (e.g., privacy, security, integrity)? these are some of the challenges we need to address. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 5  the point of all of this is exactly what prof. king suggested in his opening presentationšthat science, technology, and society will be transformed by data. modern science will depend on it. we need to be focusing on multidisciplinary activities and collaborations. in this new age of observation, there is a whole new world opening up for everyone; this is similar to what happened with the development of telescopes, only now data will be the ﬁinstrumentﬂ that will change the way we understand the world. the last point i will make is that the national science foundation (nsf) advisory committee on cyberinfrastructure established six task forces that worked for almost 2 years to develop a series of recommendations on cyberinfrastructure. the data task force addressed these issues in depth, and as the task force concluded, there are infrastructure components to deal with, as well as culture changes and economic sustainability. it is necessary to determine how to manage data and deal with such diverse issues as ethics, intellectual property rights, and economic sustainability. in this regard, the nsf should address the following issues:  infrastructure: recognize data infrastructure and services (including visualization) as essential research assets fundamental to today™s science and as longterm investments in national prosperity. culture change: reinforce expectations for data sharing; support the establishment of new citation models in which data and software tool providers and developers are credited with their contributions. economic sustainability: develop and publish realistic cost models to underpin institutional and national business plans for research repositories and data services. data management guidelines: identify and share best practices for the critical areas of data management.  ethics and internet protocol: train researchers in privacypreserving data access. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.6 the future of scientific knowledge discovery in open networked environments  sylvia spengler national science foundation  i am pleased to have this meeting organized, because of the opportunity for the board on research data and information and the computer sciences telecommunications board to come together and begin to explore where the policy, legal, social, and computer science challenges intersect in this area. i am also pleased that we were able to bring together individuals who represent different communities and fields. this is a good opportunity to sustain conversations that have been scattered around the globe for the past 15 years. from a personal perspective, i am gratified to see that some of the issues that i have long worried about in the fields of data and analytics have finally come to the attention of the people who are going to use the data. we have at the table not only scientists from a single scientific domain, but scientists from many different kinds of domains. science is inherently multidisciplinary and will require different kinds of expertise, different communities, and new kinds of social environments within science. this is an opportunity to begin to think about that. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 7  keynote address: an overview of the state of the art  tony hey microsoft research i will provide a summary of where i think we are in the dataintensive knowledge discovery area and offer a personal view of some of the issues that we are facing. as mentioned earlier, we are in the midst of a revolution, but we have been in this revolution before. i like the example of kepler, because tycho brahe™s measurements were sufficiently accurate that kepler could not fit the planetary orbits to circles and was forced to consider ellipses, which led eventually to newton™s laws. however, it was actually the data that tycho brahe took without a telescope that led kepler to his conclusion. some of my slides are based on dr. jim gray™s last talk in 2007. dr. gray was a computer scientist at microsoft until he disappeared at sea in january 2007 while on his sailboat. dr. gray made the argument that we are moving into a new area in which people will need different skills. obviously the skills for experimental and theoretical science are not going to disappearšwe still need and use themšbut computational science requires people to know about, for example, numerical methods, algorithms, computer architectures, and parallel programming. this is a set of specialized skills for doing work in areas such as climate prediction and modeling galaxy formation. today dataintensive science essentially requires a new set of skills for handling and manipulating large amounts of data, visualizing them, and so on. when i ran the u.k.™s escience program, the agenda was about tools and infrastructure to support dataintensive science. it was distributed, it was data intensive, and it involved a new way of thinking about what we are doing. that is how i distinguish between escience and dataintensive science. figure 13 is from jim gray™s talk.  figure 13 xinfo. source: microsoft research  the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.8 the future of scientific knowledge discovery in open networked environments  figure 13 shows field x evolving into two separate subdisciplines. there is computational biology, and there is bioinformatics; computational chemistry, and then chemoinformatics. almost every field (e.g., astroinformatics, archeoinformatics) today is being pervaded by informatics and the need to deal with data. it is no longer just science. dr. gray saw that in responding to the problems of getting and manipulating the data there is a need to teach people new skills. managing a petabyte of data is not easy, because, for example, it is difficult to send a petabyte easily over the internet. we need to think about organizing and reorganizing the data in case a wrong assumption was made. other issues to think about include sharing the data, query and visualization, integrating data and literature, and so on. as we can see, there are many new challenges. dr. gray had this vision of moving from literature to computation and then back to literature through a process of linking and combining different sources of data to produce new data and knowledge. he believed that this was the future global digital library for science and that it could lead to a huge increase in scientific productivity, because the research process is fairly inefficient at present, as described in figure 14. figure 14 all scientific data online. source: microsoft research  the other issue that i would like to emphasize is the reproducibility of science. if a scientist is interested in a paper, accessing the data on which that paper is based is not easy, and it is probably only in the minority of cases that a scientist can do it. so this was dr. gray™s visionša big, globaldistributed digital libraryšand that was why i accepted to cochair a national science foundation (nsf) office of cyberinfrastructure taskforce on data and visualization. we tried to produce a report quickly, because the data agenda is a very important area for all the agencies, not just nsf2. 2 the final version of the report can be found on the nsf web site http://www.nsf.gov/od/oci/taskforces/. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 9  the report covers areas such as infrastructure, delivery, culture and sociological change, roles and responsibilities, ethics and privacy, economic value and sustainability, and data management guidelines. also, nsf is now requiring that its grantees have a data management plan. another point i would like to make is related to funding. to illustrate this point, i will give an example from the coastal ocean observation lab at rutgers university. after a boating or aircraft accident at sea, the u.s. coast guard (uscg) historically has relied on current charts and wind gauges to determine where to hunt for survivors. but thanks to data originally collected by rutgers university oceanographers3 to answer scientific questions about earthoceanatmosphere interactions, the uscg has a new resource that promises to literally save lives. this is a powerful example that large datasets can drive myriad new and unexpected opportunities, and it is an argument for funding and building robust systems to manage and store the data. however, one of the group™s frustrations today, unfortunately, is the lack of funding to design and support longterm preservation of data. a large fraction of the data the rutgers team collects has to be thrown out, because there is no room to store it and no support within existing research projects to better curate and manage the data. ﬁi can get funding to put equipment into the ocean, but not to analyze that data on the back end,ﬂ says schofield. given the political climate that we are in, there is a need to convince the taxpayers who fund this kind research that it is worthwhile. thus, we need engagement with the citizens. let me provide another example. the galaxy zoo uses the sloan digital sky survey data, which contains around 300 million galaxies. since there are only about 10,000 professional astronomers, several oxford university astronomers enlisted public support to help classify them in their galaxy zoo project. they have had more than 200,000 individuals participating who have helped classify galaxies and have found various interesting discoveries, such as a blue galaxy called hanny™s voorwerp. hanny is a dutch school teacher. another example is a new classification of galaxies called ﬁgreen pea galaxies.ﬂ hence, engaging the public is a good strategy. then there is the issue of return on investment. why do scientists want to save the data? to address this issue, i will use this interesting collaboration called the supernova factory. because data curation and management were considered a priority in this project, today the supernova factory is a shining example of a significant return on investment, both in financial resources and in scientific productivity. it reduced labor, it reduced false supernova identification by 40 percent, it improved scanning and vetting times by 70 percent, and it reduced labor for search and scanning from six to eight people for 4 hours to one person for 1 hour. not only did the system pay for itself operationally within oneandahalf years, but it enabled new science discovery. the important factor in this case is that data curation and management were considered a priority from the very beginning, so the developers built a system that embodied that.  3 at rutgers university™s coastal ocean observation lab, scientists have been collecting highfrequency radar data that can remotely measure ocean surface waves and currents. the data are generated from antennae located along the eastern seaboard from massachusetts to chesapeake bay. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.10 the future of scientific knowledge discovery in open networked environments  in his talk, dr. gray also listed some calls for action. one of them is related to funding and support of software tools. nsf and other funding agencies are working on developments in support of software tools, but this is always going to be a problem. we can buy some tools off the shelf, but there are always going to be gaps that the commercial software industry does not cover. i personally came to the conclusion that building complete cyberinfrastructure using opensource software is very difficult. if scientists could buy different pieces of the infrastructure from various vendors and use any relevant opensource software available for them, then they could focus their funding and software development efforts into building the pieces that are not available in the commercial market. dr. gray pointed out that while the biggest projects have a software budget, the small projects generally do not. he felt there was a range of actions that could be done, and that is what he meant by funding generic laboratory information management systemsšsupport going from data capture to publication. his advice was to ﬁlet a thousand flowers bloomﬂšscientific data management research, data analysis, data visualization, and new algorithms and tools. figure 15 illustrates the data life cycle. it begins with data acquisition and modeling. because of collaboration, the data are usually distributed. then the question is how to get the data into a usable form. this would include data analysis, data mining, and visualization. next is the stage of writing up the data and sharing them in various ways. the final stage in the life cycle is data archiving and preservation, which, for example, can be important for environmental studies where scientists cannot repeat the same measurement. there is no hope of preserving all of the data, though, because this may be too expensive.  figure 15 data life cycle. source: microsoft research  i will now talk about a tool called chronozoom that we have been collaborating on with prof. walter alvarez at the university of california, berkeley. what prof. alvarez is trying to do is to develop a new ﬁbig historyﬂ perspective that begins with the origins of the universe, but makes it possible to look at history on various scales. it will help scientists to put not only biological evolution but also geological evolution and historical events in the system, the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 11  and then compare them and see what influenced what in these areas, by looking at it from these different scales. the zoom metaphor is what appealed to prof. alvarez, so we built him a tool that enables detailed examination and exploration of huge differences in time scales. this example illustrates that working with such data is not just for the sciences but also for the arts and the humanities. when i arrived at microsoft in 2005, bill gates was giving a talk at the supercomputing conference. figure 16 is one of the slides that he used, which illustrates a new era of research reporting. envisioning a new era of research reporting dynamicdocumentsreputation& influencereproducible researchinteractive datacollaboration figure 16 envisioning a new era of research reporting. source: microsoft research  mr. gates envisioned that the documents would be dynamic, so that someone could get the data, interact with the data and images, change the parameters, and run it again. furthermore, besides it being peer reviewed, there could also be a tool like an amazon rating, or it could be similar to a social network that someone follows. for example, a scientist might always pay attention to what a certain expert from the massachusetts institute of technology (mit) recommends to read and follow that expert rather than following others in the field. i thought that was a very farforwardlooking vision for mr. gates™s talk in 2005. another important issue is being able to identify datasets. datacite4 is promoting the use of digital object identifiers for datasets. the scientific community needs to have easier access to research data and to be able to make the data citable. the open researcher and contributor id (orcid)5 collaboration is trying to solve the issue of contributor name  4 datacite is an international consortium to establish easier access to scientific research data on the internet, and increase acceptance of research data as legitimate, citable contributions to the scientific record, and to support data archiving that will permit results to be verified and repurposed for future study. (see http://datacite.org/) 5 orcid aims to solve the authorcontributor name ambiguity problem in scholarly communications by creating a central registry of unique identifiers for individual researchers and an open and transparent linking mechanism the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.12 the future of scientific knowledge discovery in open networked environments  ambiguityšto determine how to know, for example, that anthony j. g. hey and tony hey are the same person. it is a particular problem in asia. thus, orcid and datacite are important trends. there is also a revolution being forced upon us in scholarly communication. dr. gray called for establishing digital libraries that support other sciences in the same way that the national library of medicine does for medicine. he also called for funding the development of new authoring tools and publication models, and for exploring the development of digital data libraries that contain scientific data and not just the metadata. it does not necessarily have to be done within databases, although that is what dr. gray was thinking, but we should certainly support integration with the published literature. i would like to address the journal subscription issue now, and i will use data from the university of michigan as an example. the university of michigan libraries are canceling some journal subscriptions because of budget cuts and the increasing costs of the subscriptions (in some cases, 70 percent of the library collection budget). university librarian paul courant said that about 2,500 were canceled in the 2007 fiscal year. the university library budget has gone up by an average of 3.1 percent per year since 2004, and according to library journal magazine, the average subscription price of national arts and humanities journals has increased 6.8 percent per year since 2003. national social science journals increased 9.2 percent and national science journals increased by 8.3 percent. i was trying to get more uptodate figures, so i contacted a librarian at the university who informed me that there are currently some plans at the university to close some libraries. we should not be closing libraries. we should not spend so much money on subscriptions, especially since the research reported in these journals has already been paid for in some sense. when i was dean of engineering at the university of southampton in the united kingdom, i was supposed to look at the research output of 200 faculty and 500 postdoctoral and graduate students. at the same time, the university library would send me a form every year asking which journals to cancel. if the library cannot afford to subscribe to all the journals in which my staff publish, then how could i start a new fieldšorder some new journals in, for example, nanobioengineering? this is a broken system. therefore, it seemed absolutely essential to me that a university keep a digital copy of all its research output. as a result, we set up a repository that has now become a university repository. the university now keeps fulltext digital copies of everything that its staff and students produce. it is not just the papers that will eventually appear in nature or other journals, but it is also the research reports, conference papers, theses, and even software and images. no one is asked to violate the publisher™s copyright, since these are not necessarily the versions that appear in print. about 70 or 80 percent of publishers allow keeping a nonfinal version on the web. what is important is to get the scholarly products captured at the time of production with the relevant metadata so that it becomes easy for the researchers to access these materials.  between orcid and other current author identification schemes. these identifiers, and the relationships among them, can be linked to the researcher™s output to enhance the scientific discovery process and to improve the efficiency of research funding and collaboration within the research community. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 13  a fellow dean at virginia tech told me that he had required electronic theses to be online in 1997. virginia tech had 200,000 requests for pdfs that year. less than 10 years later they had 20 million requests. these are extraordinary figures. it shows that making your data available is important. i do not advocate webometrics as a way of ranking departments or universities, but looking at just the google scholar ranking, which looks at papers and citations, we get the list shown in figure 17. webometricsgoogle scholar rankingjuly 2010southampton# 21virginiatech# 37cambridge # 97oxford# 115clearly nota ‚perfect™ metric but equally clearly, this must measure something of relevance for the research reputation of a university –łinstitutional research repository must be part of the university™s ‚reputation management™ strategy figure 17 webometrics google scholar ranking. source: microsoft research  harvard university and mit are at the top. but there is also a large amount of apparent randomness in this ranking. according to citations in google scholar, the top university in the united kingdom is southampton, at number 21, well above cambridge and oxford. this might appear hard to believe, but it is saying something serious about research. if a university is regarded to be a serious research university, it should be showing up on such ranking, because people like such tables. it is a question of reputation management, and i believe that the university research repository is an important piece of that. as to the future of research repositories, i think they will contain not only full text and the other kinds of scholarly products that i have talked about but also data, images, and software as part of the intellectual output. some of those items may have an identifier attached that can be used, for example, in a court case. in my view, the university library should be doing that. let me offer two examples of how work in this area can be done. there is a large amount of data in databases at the national library of medicine so that researchers can have a ﬁwalled garden.ﬂ dr. david lipman and colleagues at the national institutes of health have the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.14 the future of scientific knowledge discovery in open networked environments  created a system that allows anyone not only to have access to the text provided through pubmed central but also to have a range of databases and crossdatabase searching. that is a centralized repository model. most fields are not going to be fortunate enough to have such a centralized model, because it is expensive; therefore, it is important to develop a more distributed model. one example is worldwidescience.org. dr. walter warnick and his colleagues at the u.s. department of energy are building a consortium whose membership is now up to 65 nations. it offers many publications and databases online, and there is a translation service that allows people to type a query in english, have it translated into chinese or russian, and after the search is done, translate the results back into english. the major advantage of this search system over a google search is that this system can get up to 96 percent unique results that cannot be captured by a simple google search. this is an example of a distributed system that is getting close to dr. gray™s vision. we all know about vannevar bush™s article ﬁas we may think5.ﬂ in a similar vein, the physicist paul ginsparg wrote a good article called ﬁas we may read6.ﬂ prof. ginsparg started arxiv at los alamos national laboratory for use by highenergy physicists. his idea was to reproduce the way that physicists used to circulate preprints back when papers were typed. they used to make photocopies of papers they were submitting to, for example, physical review and send them around to a hundred institutions. since he had access to a server, he suggested to other physicists that they send digital copies of their papers to him and he would circulate them. that is essentially how the arxiv started. the project is now located at cornell university. in the conclusion of his paper, prof. ginsparg wrote, ﬁon a onedecade timescale it is likely that more research communities will join some form of global unified archive system without the current partitioning and access restrictions familiar with the paper medium for the simple reason that it is the best way to communicate knowledge and hence to create new knowledge. ironically, it is also possible that the technology of the 21st century will allow the traditional players, namely the professional societies and institutional libraries, to return to their dominant role in support of the research enterprise.7ﬂ next, to talk about future research cyberinfrastructure, i thought i would go back in history and look at a slide from 2004. these are the six key elements for a global cyberinfrastructure, as opposed to einfrastructure, which was the term used in europe.  5 atlantic magazine, july 1945. 6 the journal of neuroscience, 20 september 2006, 26(38): 96069608; doi: 10.1523/jneurosci.316106.2006 7 ibid. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 15  1.2.3.4.technologies and standards for data provenance, curationand preservation5.open access to data and publications via interoperable repositories6. figure 18 six key elements for a global einfrastructure for escience. source: microsoft research  i highlighted the points on technologies and standards for data provenance, curation and preservation, and open access to data and publications by interoperable repositories. i am not sure we have made much progress since then, but it was interesting that we were thinking along those lines 7 years ago. we funded a number of projects. we set up a digital curation center in 2004. it took the people there some time to find their way, but i think they are doing useful work now. they advise people about data management plans, for example. we also set up a software engineering organization called open middleware infrastructure institute. its job was to take some of the software we produced in our escience projects that got the most use and then do testing, documentation, writing specifications, and finally make the software available. the institute worked to make software usable to scientists who had not written it. it brought the level of the opensource software up to a reasonable engineering level. the nsf should be helping its grantees do similar work. another example is the national center for text mining (nctm). my colleagues in the computer sciences area claim that the technology is now mature. i do not think that is the case. i do not think we have proven that intelligent text mining is used by the community yet. the nctm can be considered as an experiment that is only halfway successful in this area. i should also mention jisc, which is an nsflike funding agency in the united kingdom. jisc funded many pioneering activities in digital libraries and repositories. i was the chair of the jisc committee for support and research. one project that jisc funded was the tardis project, which led to the repository at southampton. another jisc project was the malibu project, which was about hybrid libraries and the invisible webša lot of information is locked behind z39.50 and not accessible to web browsers, so how could anyone find it? the final example was the claddier project with the british atmospheric data center, which was concerned with linking data to publications, and, again, it was very interesting. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.16 the future of scientific knowledge discovery in open networked environments  let me now focus on where we are going in the future. at present, about two papers are being deposited in the national library of medicine every minute. no one can keep up with this rate, so how do we deal with that deluge of publications? that does not even include the data generated for those papers. i think that semantic computing will help. currently computers are great tools for storing, computing, managing, and indexing huge amounts of data. in the future, we will need computers to help with automatic acquisition, aggregation, correlation, interpretation, discovery, organization, analysis, and inference from all of the world™s data. if the system knows that a scientist is looking for information about a star, for example, it should be able to identify which papers contain data about that star and get those papers for the searcher without being asked. it should ﬁunderstandﬂ what we are looking for. the computer should be computing on your behalf and understanding your intent. these are the important things that we need to do. i do think that we are moving to a world where all the data are linked, and that at some point a scientist will be informed that paper x is about star y without even searching for this paper. this is what semantic computing will be able to do. this is coming and is an exciting challenge for the information technology industry and for computer science to see if they can build systems that are useful. i also think that, in the future, researchers will keep some data and tools locally, but they will also use some services from the cloud. we already know about blogs and social networking in the cloud as well as identity services. amazon and microsoft offer storage and computing services. thus, depending on the area or application, scientists may want to use some kind of cloud service. for example, a scientist may use the cloud to replicate the data to a safe place to store them or to share them for collaboration purposes. semantic computing is also coming, and i think that we are going to realize dr. gray™s big digital global library that contains data and publications. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 17  discussion discussant: there is a scientist at rutgers university named christopher rose who is known for, among other things, his paper about why the search for extraterrestrial intelligence is not worthwhile. his point is that the energy required to fill the universe with so much energy that somebody in another galaxy can pick it up is absolutely unbelievable. he also said that the only thing that makes any sense is that if someone throws an object toward a place outside his or her galaxy, it means that this person has to produce half the energy, and the recipient has to produce the other half in the form of a radar system that detects it. when i hear someone saying that we are going to fill the world with knowledge in the hope that you pick up some of it as it passes your head, i wonder if there is not something to be said for the traditional system of trying to put the person who is producing a bit of chemical knowledge in a room with the people who might use chemical knowledge and see if they recognize the intersection. dr. hey: it is a vision, and visions can be wrong. i think that we can do much better. we spend a lot of money in the biological sciences, for example, with everybody repeating similar work. also, there was a medical study recently released that found that almost 400 medical research projects did not cite obviously relevant papers on the same subject. i think we can do better. i do not think we will fix all the problems, but i do think we are all fortunate enough to have a national library of medicine. discussant: dr. hey, if you look at where the scientific community is going with publications and data linkage, arguably this is already being done in the real world. if you go to the new york times web site, for example, or other publications™ sites, you see that the private sector has provided the engine to have the kinds of searchable ranked articles that we would like to have in the scientific community. what do you think it will take to close the gap? and what is the next level of developments that we should be aiming at today? dr. hey: if there are commercial systems that work, then we should use them, and we should not try to reproduce tools and opensource software with our limited research funds. it is absolutely important to have standards and interoperability, but there are experiences that we can learn from the commercial world. where we are going? supercomputer centers produce huge amounts of data, and it will be difficult to manage and transfer the petabytes of data that they produce, so i think that we will have supercomputer centers with data centers. i think there will probably be some regional data centers. there are currently attempts to put some of this infrastructure in place, and i think there is an opportunity for the united states to be leading in this exercise. i am pleased to see that nsf is galvanized to do something in this area. discussant: much of what was shown in the presentation, including the slide from bill gates™s talk in 2005, now exists. tools like mendeley for sharing publications and fancy xml documents with objects embedded inside of documents with different formats do exist. but in astronomy, maybe 1 percent of the astronomers use them and maybe 5 percent know that they exist. what did we learn about this culture shift that is needed to let us use tools and technologies more efficiently? the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.18 the future of scientific knowledge discovery in open networked environments  dr. hey: i absolutely agree. we used to produce useful tools and give them to the users, but we found that they were not using them 6 months later. even though we would show them how useful they were and they had all agreed, they did not use them. there is a real problem in engaging scientists. we have to produce tools that are as close as possible to the way that scientists are working now, but that at the same time give them an improvement. advocates are needed, and it will be important to have some people showing how scientists can do great research with these tools. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 19  2. experiences with developing open scientific knowledge discovery in research and applications case studies international online astronomy research alberto conti space telescope science institute  i am the archive scientist at the optical uv archive at the space telescope science institute, which is the place that operates the hubble space telescope for the national aeronautics and space administration (nasa) and which is going to operate the successor of hubble, the james webb space telescope. as archive scientists, we have to do a lot of work to bring data to the users, but we focus mainly on three areas. we like to optimize the science for the community by not just storing the data that we get from users but also trying to deliver valueadded data. we also try to develop and nurture innovation in space astronomy, particularly for hubble and its successor. we are good at that because we have been doing it for 20 years. and we like to collaborate with the next generation of space telescopes as well as with groundbased telescopes, because that field is quite busy. figure 21 shows some key astronomical missions, which provides an idea of the kind of astronomy and astrophysics data that exist and are being developed. figure 21 astronomy project timeline. source: space telescope science institute  the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.20 the future of scientific knowledge discovery in open networked environments  the kepler mission, for example, is a planetfinding mission. the ones on top are space based, and the bottom ones are ground based, some of which are going to produce very large amounts of data. at my institution, we collaborate directly or indirectly with most of these missions and deal in some way with the data they produce. how are we going to manage the large amounts of data from these missions? over the past 25 years, astronomy has been changing quite radically. astronomers have been very good at building large telescopes to collect more data. we have been much better at building larger mirrors. but we have been a hundred times better than that at building detectors that allow us to collect extremely large amounts of data. since the detectors roughly follow moore™s law, every year or so our data collection doubles, and this raises many important issues. we realize that we are not alone in this area. we know that fields from earth science and biology to economics are dealing with massive amounts of data that must be handled. i am not particularly fond of using the word ﬁescienceﬂ to describe this field; i prefer to speak of ﬁdataintensive scientific discovery,ﬂ because i think that is exactly the field that we should be moving to, because we are going to be driven by data. however, while astronomy is similar to other fields in managing big volumes of data, the astronomy field is somehow specialšnot because the data are intrinsically different and special in their own right, but because they have no commercial value. they belong to every one of us, so they are an ideal test bed for trying out complex algorithms that are based on very large dimensions. currently there are missions that have in excess of 300 dimensions, which can be very useful if a scientist wants a dataset with many dimensions to analyze. we also have to be aware that things have been changing, for example, in the geographic information system world, where our perception of our planet has been changed by such tools as terraserver, microsoft™s virtual earth, google earth, and google maps. the way that we interact with our planet is now different than it was, and as we use all of these tools, we have no concept of what is really going on underneath, because we are focusing on the discoveries. therefore, some of us are trying to do the same thing for data in other areas. what is this new size paradigm? why is so much data a problem? in figure 22, the red curve represents how much data we collect in our systems as a function of time. the big spike in the middle corresponds to the installation of new instruments for hubble, for example. this is a trend, but it is not a problem. the real problem is the curve above it, which is the amount of data that we serve to the community. this is a large multiple of the data that we collect, so this potentially can be a problem. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 21  figure 22 new science paradigm for astronomy. source: space telescope science institute  this was a concern, at least until a few years ago, because of how we used to work with the data. as a researcher with the space telescope science institute, i can use a few protocols to access the very different data centers, but every time i interface with a different data center, i have to learn the jargonšthe language of that data center. i have to learn all the idiosyncrasies of the data in this particular data center. after i learn all this, my only option is to download the data to my local machine and spend a long time filtering and analyzing them. in astronomy this limits us to small observations of very narrowly selected samples. in 2001, scientists realized that this was not a good model, so the national science foundation (nsf) funded the national virtual observatory for astronomy. the goal was to lower the barrier for access to all the different data centers so that a scientist did not have to worry about the complexities behind the data centers. they just had to worry about how to get data and do their science. the national virtual observatory was established in 2001, and for several years it was in the development phase. it moved into the operational phase with funding from nsf and nasa in 2010. by this time it was called the virtual astronomical observatory. at the same time these developments were taking place in the united states, people across the planet were building their own standards, their own protocols, and their own data centers. at some point it became evident that we could not ignore what was happening around the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.22 the future of scientific knowledge discovery in open networked environments  the world, because many of the ideas that were being proposed to deal with this deluge of data were very worthwhile and could be integrated into the thought process that goes on inside of any astronomical archive. hence, in the middle of developing the virtual observatory, a collaboration called the international virtual observatory alliance (ivoa) was begun with the goal of lowering the barrier for access at all of the data centers across the world. today the ivoa allows scientists to access all these different missions and to become more knowledgeable about what each mission does and how they are different from each other. it also has helped scientists identify where data are stored and what they are called. this is a major leap, but it has come at a cost, and the cost is that we have developed very ad hoc data standards for storing and sharing the data. after a long development process, we adopted protocols that turned out to not be very effective at certain times. furthermore, after that many years of development, we do not have very effective tools, because doing the data mining is very difficult. we are still in the mode where most of the data have to be downloaded and filtered locally, which is not what we would like to do. although we have standards and protocols, the ivoa has come under some pressure lately because it is not changing perhaps as fast as some would like. to some people™s creditšparticularly dr. alex szalay of johns hopkins university, who is one of the founding fathers of the virtual observatory ideašit was predicted that this transition would be chaotic, and it has been chaotic. it is very hard to integrate very large data centers across the planet into a seamless process of discovery, but some people are trying to do that. the chaotic part has come to pass. unfortunately we do not yet have a uniformly federated and distributed regime of sources across the planet that we can access in a seamless manner, so this is part of our existing problem. in the future, i think the ivoa would still be at the core of developing such a distributed system. some people, like me, believe that the ivoa should have a much smaller and limited role in defining the requirements and standards for this distributed system that allows people to access and process on a cloudlike system. the ivoa should try to build standards that are not ad hoc, but rather are based on industry standards. metadata is another important frontier that we must explore so that we can characterize these data and mine them appropriately. the idea is that we want to enable new science and new discoveries. here is what i see as the challenges we are facing. we capture data very efficiently, but we need to reduce obstacles to organizing and analyzing the data. we are also not very good at summarizing, visualizing, and curating these large amounts of data. another issue is that most of the time the algorithms that produce the papers that we see are never associated with the data. we should consider this as a problem. a scientist should be able to reproduce exactly the research result once he or she finds the data. semantic technology is key to being able to deal with many of these obstacles. we have to be prepared, because this will introduce a fundamental change in the way astronomical research is conducted. another issue is that we do not have available to us an intensive datamining infrastructure, and if we are to build one ourselves, that will be very expensive. many of these efforts in astronomy are not funded at a level that would allow us to build our own. a solution for handling very large datasets is emerging. such solutions are not pervasive in astronomy, which is also a problem. as for hosting, if we host our large datasets on the cloud the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 23  for redundancy, for performance, and so on, that will come at a cost, and some archives might not be able to sustain that cost even if it does not seem to be very large. finally, i think we have the opportunity to do a few things. we have to be a little humble and accept that we have to ask for help from computer science, statistics, applied mathematics, and from others who can use our highdimensional data for their purposes, and to apply their useful algorithms. we should leverage partnerships with those who are already doing this work on the web scale. there are companiesšmicrosoft, google, amazon, and many othersšthat are handling and trying to solve these problems, and we should definitely partner with them. they may not have the solution for us, but they will have a big part of some solution. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 25  integrative genomic analysis stephen friend sage bionetworks  there are three points that i would like to focus on today. one concerns the current paradigm shift in medicine, and i am going to make a case for why we need better maps of diseases. the secondšand probably most importantšpoint is that it is not about what we do, but how we do it: it is all about behavior. third, i want to discuss some aspects of building a precompetitive commons. when we are working with commercially valuable informationšunlike the case with astronomy, there is serious money involved with information in the medical areaša number of issues arise. the cost of drug discovery, which is at the intersection between the knowledge and being able to impact people, is around $1 to 2 billion per molecule. there is about $60 billion being spent every year to make drugs, but there are fewer than 30 new drugs that are being approved. people are beginning to realize this lack of efficiency, however, and the good news is that there is information coming along that makes discovery easier. to put it in perspective, even after drugs are approved their rate of effectiveness is low. this is true for cancer drugs and also for the statins used to treat high cholesterol levels. only 40 percent of patients who take statins have their cholesterol lowered, while more than half do not, and of those who do, less than half see any benefit in decreased occurrence of heart attacks, yet we still give these drugs and say everybody has got to be on a statin. the problem is that when trying to figure out what is going on inside cells and to understand diseases in terms of pathways, we are using maps that are akin to prekepler models of biology. people get very excited about understanding pathways and believe that if they could just understand the pathway of a disease, they could make sense of it. almost every single protein has a drugdiscovery effort associated with it, and all of these researchers are dreaming that they will be able to make a drug against this or that disease without understanding the context and the circuitry that underlies it. we have gone through these transitions before. we got beyond the idea that god throws lightning bolts, and we got beyond the concept of earth being at the center of the universe. in the next 10 to 15 years, we are going to have a change that is similarly dramatic and will be remembered for centuries. it is going to be driven by a recognition that the classical way of thinking about pathways is a narrative approach that we favor, because our minds are wired for the narrative approach. the reason this change will happen is that technologies are developingšanalogous to telescopesšthat will allow us to look inside of cells. in the last 10 or 15 years, we have gone from the first sequencing of a whole genome to the point where it will soon be cheaper to get your genome sequenced than it is to get a ct scan or a blood test. think of that in terms of the amount of information associated with it. it will take a few hours to get your whole genome sequenced, and it will be under $1,000. the cost of ct scans is going up, while the cost of sequencing genomes is going down. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.26 the future of scientific knowledge discovery in open networked environments  i know of a project in china that is planning to do the whole genome sequence of everyone in a village with a population of 1 million people. they believe they will get whole genome sequences on the village population in the next 8 years. the problem we have is that most people are still looking at altered component lists, and astronomers have already gone through this. altered component lists do not equal knowledge of what is happening. while people are excited about the sequencing of genomes and understanding the specifics, the reality is that we are stuck in a world where biological scientists are still generating onedimensional slices of information and not understanding that those altered onedimensional slices of information will not add up to understanding altered function, which is ultimately what you have to do. the reason that this is important is that the circuitry in the cell has, over a billion years, been hardwired to buffer any type of changes, and the complexity that the system has in it to tolerate a modification is hardwired together within one cell, within multiple cells within organs, and so on. a project started in 2003 at rosetta in seattle asked how much it would cost to build a predictive model of disease that used integrated layers of information. by integrated, i do not mean raw sequence leads leading through to a genome. i am talking about integrating different layers of information. an experiment done by eric schadt represented a fundamental seismic shift in how we think of this science when it is asked: ﬁwhy not use every dna variation in each of us as a perturbation experiment?ﬂ imagine that we are each living experiments and that if we know all of those variations, we could take a topdown approach similar to the blue key that is on your computer that feeds any error through to microsoft or other places. imagine a way in which we could build knowledge not from looking at publications. this would be very important. i will argue that publications are a derivative, and while they are helpful, that is not where we want to be focused. we should be looking at the raw data and building those up. imagine that we could look at putative causal genes, that we could look at disease traits, and that we could be zooming across thousands of individuals and asking what happens at intermediate layers and what happens with causal traits. the central dogma in biology, which is that dna produces rna, which in turn produces proteins, allows us to build causal maps that enable us to look collectively at changes in dna, changes in rna, and effects on behavior and then begin to ask, ﬁwhat was responsible for doing what to what?ﬂ there are many mathematical models, from causal inferences to coexpression networks, but it does not matter which statistical tools we use to do topdown systems biology. the result is that we do such tasks as taking brain samples and expression, building correlation matrixes and connection matrixes, then clustering them and building network models that use a key driver of a particular effect, not because it was in the literature, but because it was experimented with in different databases. a key paper that used this approach was published in nature genetics in 2005. it was looking for the key drivers of diabetes. the idea behind the paper was to rank the order of the 10 genes most likely to be responsible for going from a normal state to diabetes and from diabetes to normal. over the next 4 years, that work was validated by the community, which found that 90 percent of the genes on that rankordered list were correct. that is unheard of. those trivial early maps did not have nearly as much power compared to what is coming, but they were good enough to make that type of analysis. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 27  with such an approach, we could look at drugs that are on the market and drugs that are in development and try to identify those that are going to be responsible for certain subtypes of effects on behavior or on cardiovascular function, or we can look for toxicity. this is the type of shift that is emerging in the medical world for understanding compounds. there are already about 80 or 90 papers that have been published in different disease areas. the laboratory of atul butte at stanford is one of the 10 labs that have made an art out of exploring human diseases by going into public datasets and building these types of maps. the first takehome message is that we have been living in a world where we think of narrative descriptions of diseasesšthis disease is caused by this one agent. at sage bionetworks, we now use maps that are not hardwired narratives, but that look at statistical probabilities and the real complexity. for example, imagine that some components in the cells talk to each other and they need to listen to each other to make a decision on what physiologic module is coming out of that component of the cell that then talks to other parts of the cell. what we are finding is that key proteins aggregate into key regulatory drivers that are able to say, ﬁthis is what i need to do.ﬂ this is the command language. we are not looking at the genetic language, but rather at a molecular language that determines what is going on within a cell, that says, ﬁi have enough of this,ﬂ and interacts with cells in different parts of the body. the complexity of this approach is outstripping our ability to visualize it and to work with it, but the data are there. those key ingredients are coming. maps of diseases will be very accurate for who should get what drugs, and what drugs are able to be developed. this is going to totally revolutionize the way we think of disease. disease today is still stuck in a symptombased approach. if you are depressed, you have depression, and you try to find a drug as if all depression were the same. the same is true for diabetes or cancer. the level of the molecular analysis that we will soon be able to do is going to be important, but substantial progress will require significant resources and decades to complete. those maps are almost like thin peeloff slices of what is happening, and we realized that the group doing this work could not be one institution or one company. for that reason, a year and a half ago i left the job i had running the oncology division at merck and started a notforprofit foundation to do a proof of concept. this foundation, which has about 30 people and a budget of about $30 million, is asking how to do the same work for genomics that has been done for astronomy. we are now trying in genomics to do the virtual equivalents to what has been done in astronomy. the people who are driving it are lee hartwell; hans wizgell from the karolinska institute; wang jun, the visionary who is sequencing the million genomes in china; and jeff hammerbacher, who was the chief architect of facebook. we brought hammerbacher in because we think that this is not strictly about science. we think we need to ask why a young investigator would want to share his or her data. why would that investigator allow someone else to work on them? we think the social aspects of this are very important. i will offer four quick examples. in breast cancer research, we are working on ways to identify key drivers by using publicly available datasets and running coexpression and bayesian networks. we are going to be publishing that work soon. the second concerns cancer drugs. it turns out that virtually all of the cancer drugs that are the standard of care, the first line of drugs that a patient gets, are usually not very effective. we are looking at cisplatin doublet therapies for ovarian cancer, avastin for breast cancer, and sutent for renal cell carcinoma. we realized that we should identify who is not responding to those drugs so that these patients can be given something else. it has been very difficult to get the national the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.28 the future of scientific knowledge discovery in open networked environments  institutes of health and the food and drug administration interested, so i had to go to china for funding. the third example involves getting data from pharmaceutical companies. the richest trove of data is inside these companies, because they often collect genomic and clinical information when assembling information for a drug filing; but they lock all these data up, because they do not want to share what it was that gave them their advantage. accordingly we took a different approach. we asked to use the data from the comparative arm rather than the data for the investigational drug. this helped, and all of these companies have said they will give us these data. that brings up the point that dr. conti was making earlieršthat when we have all these data, we must worry about the annotation and curation. that is what we are doing. we are saying, ﬁyou give us the data, and we will annotate, curate, host, and make them available to anyone.ﬂ there is a very big difference between accessible and usable data. we tell the pharmaceutical companies that if they make the data accessible, we will make them usable. i am sure we are going to do it in a bad way that has to be redone, but at least we will get it started. the fourth example is a project in which we took five labs that are supercompetitive with each other and linked them together so that they could share their data, models, and tools. this type of linking opens up new opportunities. we found that we have broken the siloed principal investigator mentality of science. this is part of that fourth dimension. science is presently practiced like a feudal system: ﬁi have the money, those are my postdocs, and this is my data.ﬂ but imagine a world in which scientists could go anywhere they wanted to get anyone™s data before it is published in the same way that astronomers work. we have found that it is the scientists under 35 years old who know what to do. we have been making openly available global datasets, models, and tools. i want to describe the behavior we have run into. first, clinical genomic data are accessible, but minimally usable, because there is no incentive to annotate and curate the data. second, people who get the data think they own the data. if someone pays them to generate the data, somehow they think they own them. we are living in a huntergatherer community, which is preventing data sharing, and, for the most part, the principal investigators have no interest in changing that situation. this needs to fundamentally change. just as eisenhower in the 1960s talked about the militaryindustrial complex, i think we now have a medicalindustrial complex in which the goals of individual researchers to get tenure, the goals of institutions to improve their reputation, and the goals of the pharmaceutical industry to make money create a situation in which the patients are not benefiting. there is a time for intellectual property, there is a time for innovation, but the current situation is crippling our ability to share the data. the system is not set up to make it easy to reproduce results, as in astronomy. you do not hold people accountable when they publish a paper to explain what they did so that it can be reproduced. we have to figure all of that out. finally, i want to ask why we do not use what the software industry has already figured out for tracking workflows and versioning. we have to change the biologic world so that the evolution of a software project is clear, with a code repository and the branches and releases. what we are doing now at sage bionetworks is determining the details on the repositories, collaboration, workflows, and cloud computing. we find that the world is saturated with early the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 29  efforts, so it would be absurd to think that we ourselves need to do this. we are interested in such efforts as taverna, amalga, and work at google. we are identifying who has what so that we can stitch this together. the hardest thing is that we do not have the support of patients in this area yet. we have to bring the patients and citizens in. otherwise the datasharing, privacy, and access issues are going to tear this approach apart. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 31  geoinformatics: linked environments for atmospheric discovery sara graves university of alabama in huntsville i will be giving this presentation on behalf of dr. kelvin droegemeier, who had an emergency to which he has to attend. dr. droegemeier™s area is geosciences. he is from the university of oklahoma and serves on the national science board. he was planning to talk about a project called linked environments for atmospheric discovery (lead). i worked on lead as one of the coinvestigators, as did mohan ramamurthy. lead was a multiuniversity and multidiscipline research program that was created by the national science foundation (nsf). i heard on the weather report this morning that 100 million people will be affected by a severe weather situation in the northeast and other parts of the country, so i thought that it was quite fitting to be talking about lead. the annual economic losses due to weather are greater than $13 billion per year. every time we try to get people ready for some weather event, there is a cost to that preparation, and if the weather event does not occur as predicted, there are additional costs. we have a few weather technologies today that we are familiar with, and we use the output from many of them. according to some meteorologists, however, we still have many technological improvements to make. radars do not adaptively scan, so one of the keys that we were looking into with lead was how to get more adaptation and more dynamic processing of information. we wanted to take into account the complexity of all of the factors and not just approach it linearly. operational models run largely on fixed schedules and fixed domains, and the cyberinfrastructure that we often use is quite static. we teach our students about current weather data, but we do not allow them to interact with it. the previous two talks discussed how to get more people involved, not just in an academic setting but also in citizen science. that is certainly something we would like to do in this domain as well. weather is local, high impact, heterogeneous, and rapidly evolving, yet our technologies and our thinking have not evolved that quickly. one of the fundamental research questions we looked into with this project was whether we can better understand the atmosphere, educate others more effectively about it, and forecast more accurately if we adapt our technologies. even animals adapt and respond, so why should this not be true for the resources that we have? this was a multiuniversity project, and there was an associated project called collaborative adaptive sensing of the atmosphere (casa), which i will describe shortly, that interacted with lead. the goal of lead was to revolutionize the abilities of scientists, students, and operational practitioners. the operational practitioners were a key part of the vision, because we were trying to work not just with the typical student scientist but also with the national oceanic and atmospheric administration and some other practitioners to observe, analyze, predict, understand, and respond to intense local weather by interacting with it dynamically and adaptively in real time. oklahoma and alabama have many tornadoes. we did not have this project just for that reason, but severe weather does have a local impact. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.32 the future of scientific knowledge discovery in open networked environments  what does adaptation mean? let us take the example of a tornadic storm that occurred in fort worth in march 2000. a local tv station radar system and a national weather service system with 12hour computer forecast that was valid at 6 p.m. central time did not explicitly show any evidence of precipitation in north texas. the reality was quite different, because there was in fact a tornado. the approach that the project took was to have streaming observations. we wanted to have constant observations and not just frequent static pictures. we did a lot of mining of both the streaming data and some of the archived data to feed into the weather research and forecasting (wrf) model. we needed ondemand computing, so we were using the nsf teragrid computational capabilities. the cloud could be used in that respect too. what comes out of the computation models is fed back into the mining engine, which creates a feedback loop. what does it take to make all of this possible? of course, you need adaptive weather tools such as adaptive sensors, adaptive cyberinfrastructure, and a usercentered framework in which these tools can mutually interact. we developed and continued working on these tools with lead. what lead was doing is creating a web service. we had our linked environments, our processing capabilities scattered around distributed areas, and web services to produce some of this information much faster. you also need to link together the services and workflows. one of the lessons that was learned from this project was the recognition that we can create very elaborate workflows and save them in a special database so that it is possible to go back and recreate some of those problems, both for academic purposes and for practitioner and operational capabilities. we did significant work in creating the workflows and some of the tools. since all of this work is openly available to others, many of these tools and processes are being used today by various entities. to develop more accurate forecasts, we needed to come up with better tools. we did a lot of ensemble modeling, and we have continued to develop this capability with wrf. figure 23 shows the current operational radar system in the united states. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 33  figure 23 current operational radar system in the united states. source: kelvin droegemeier  there are limitations to nexrad (nextgeneration radar), as it is not set up for some of the issues i have discussed. it is fairly independent of the prevailing weather conditions because of the way it operates, and it is very independent from the models and algorithms that use the data, since the earth™s curvature prevents 72 percent of the atmosphere below 1 kilometer from being observed. there is also a nextgeneration capability being developed now. the other project that we worked on was mainly headquartered at colorado state university, and dr. droegemeier was also involved with it. it was focused on collaborative adaptive sensing of the atmosphere. we would mine and find an event that is occurring and that we could see with the radar data. then, we would try to steer some of these radars to detect the event through dynamic adaptation. figure 24 depicts the oklahoma test bed where they had the various capabilities. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.34 the future of scientific knowledge discovery in open networked environments  figure 24 oklahoma test bed. source: kelvin droegemeier  figure 25 is an example of adaptive sampling.  figure 25 example of adaptive sampling. source: kelvin droegemeier  and finally, figure 26 shows the casa experiment looking at what they were seeing from the radars and the way they could steer them versus what you could get with nexrad. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 35  figure 26 comparing casa with nexrad. source: kelvin droegemeier  the potential, of course, lies with the fact that we are trying to constantly transform and improve our teaching, education, and workforce development. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 37  implications of the three scientific knowledge discovery case studies œ the user perspective international online astronomy research, c. 2011 alyssa goodman harvard university  dr. conti provided a great overview of the challenges that face our field of astronomy. instead of focusing on only one example, i will draw a map of the way that things work in our field, and then give a few brief examples. figure 27 shows water droplets on a spiderweb. figure 27 image of water droplets on a spider web. source: alyssa goodman  this is the way that i like to think of ﬁthe cloudﬂ with the international virtual observatory alliance. basically each one of those little water droplets is some data repository, service, or person that provides something in this ecosystem of online astronomy research. the way that those kinds of services are connected on the world wide web is how i think that online astronomy research should be donešin a slightly less orchestrated fashion that people might have thought of a decade or more ago. in my work, we call this ﬁseamless astronomy.ﬂ there is a researcher in the middle of the image in figure 28. imagine that this researcher is drawing on two kinds of primary sources of information: data on the left, and literature on the right. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.38 the future of scientific knowledge discovery in open networked environments  figure 28 realm of seamless astronomy. source: alyssa goodman  in astronomy, our literature is organized very well by a service called the astrophysics data system (ads). much of what i am going to discuss is how the ads team is working on connecting the data to the literature through the ads labs at the center for astrophysics in cambridge. i am going to take this weblike approach and try to explain a few of the interconnections between the two sides of the picture: literature on the right and data on the left. i have reduced this model to just a few services on which i will focus, but as the disclaimer at the bottom says, this slide shows key excerpts from within the astronomy community and excludes more general software that is used, such as papers, graphing and statistic packages, datahandling software, search engines, and so forth. as i mentioned earlier, the literature is very organized, so i have put it all in one accessible repository, and it is accessed through the ads system. as for the data, there is a set of archives that i will cover shortly. figure 29 is a view of the international online astronomy ecosystem that i will use to frame the rest of this talk. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 39  figure 29 international online astronomy ecosystem. source: alyssa goodman  in the middle, i have depicted some tools in a regime called seamless astronomy, which offers links between data and literature that are still partly missing, but are now forming and expected to lead to the increase in scientific productivity. the ﬁads labsﬂ is a project in which the ads team is taking what is already a successful system for accessing the research literature in astronomy and trying to draw that system closer to the data by using the articles themselves as a retrieval mechanism for data. since ads already stands for ﬁastrophysics data system,ﬂ why not develop a way to retrieve articles using the literature as a filter for the data? there are also linkages between many services that do different work. what you should notice, however, is that there are many yellow lines in the figure, which connect all of these services in a variety of ways. it is important to note that this is not a linear process. there are many different ways that the services can access each other without the user even knowing about it. four of these applicationsšthe four highlighted in the ﬁsampﬂ box in the figure (samp is a messagepassing protocol)šare actually connected to each other in a way that allows them to interoperate as a set of services and as a research environment. i would like to point out that the flags in the figure represent the various countries involved. there is also an international flag that indicates this activity is not associated with any one particular country. some of the services i am going to talk about are from the united states, but others are not, such as topcat from the u.k. escience program. figure 210 shows a screenshot of what happens when we run many of these services together as well as the samp hub that allows the services to talk to each other. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.40 the future of scientific knowledge discovery in open networked environments  figure 210 samp hub. source: alyssa goodman  suppose that i was looking at some particular region in the sky in one application. i can open another application that looks at the exact same region. worldwide telescope can superimpose catalogs in ways that are different from how the first application superimposes catalogs. and topcat, which allows us to statistically manipulate those catalogs, will show the results through live links. it is important to note that these are from three different sets of countries, and the applications are working together through this opensource messagepassing protocol. at the moment, my view is that probably only about 5 percent of astronomers know how to do this correctly. now let me show you the ads labs itself. i should point out that ads is funded through the u.s. virtual observatory funding, provided by the national aeronautics and space administration (nasa). it also has a mirror site in france run by centre de données astronomiques de strasbourg (cds). ads labs was released in january 2011, at the american astronomical society meeting in seattle. the idea was to take ads and, instead of just doing a keyword search, do more intelligent searching. there are some interesting semantic aspects of this kind of search. let me give an example of how the ads can be used. i was working on an observing schedule recently to observe an obscure quantum mechanical effect called the zeeman effect in the molecule ch from a particular telescope. i wanted to make sure that no one has ever done this before, so i checked for related papers. if i use the most recent work function of ads and type ﬁzeeman effect,ﬂ i get something that looks like the image in figure 211. there is a filter list down the side, and i can choose particular authors, key words, missions, objects, and so on. the list of papers that i get will change depending on what is chosen. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 41  figure 211 ads labs beta search. source: nasa  the green box in the figure highlights some linkages between the data and the literature. if i click on one of the particular objects that seems to be mentioned very frequently in these papers, i get four different choices: (1) i can look up all the publications about that object; (2) i can look up data on that object using the simbad data service from cds; (3) i can see that object in context using worldwide telescope; or (4) i can do a similar thing, although it is a narrower context, in anther service called aladin. if i clicked on worldwide telescope, i would get a view like the one in figure 212. the object turned out to be a planetary nebula that i did not know. since this is not what i was interested in, i concluded that no one has observed what i was looking at and that this object was irrelevant. but perhaps i became curious about this object, and so i could click on the button at the bottom that says ﬁresearch.ﬂ by clicking there, i can get another set of choices, linking to yet other services. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.42 the future of scientific knowledge discovery in open networked environments  ﬁshiftclickﬂon objectwwt/seamless astronomy core collaborationj. fay (msr), a. goodman (cfa), g. muench (cfa), c. wong (msr) figure 212 worldwide telescope. source: alyssa goodman  basically what we are being offered when clicking on the different options under ﬁresearchﬂ is either more data or more literature. the links are such things as ﬁlook up all the publications about that particular object,ﬂ ﬁlook up that object on wikipedia,ﬂ or ﬁlook up that object in astronomy databases.ﬂ this system offers a lot of flexibility. however, even though the system is supposed to list the most relevant papers, it is employing a very low level of semantics to do the search. for example, because i know this field, i can tell if the list of papers is incomplete. it is partially right, but it is not giving the full picture to a scientist who has never worked in this field, and that can be very misleading. at this point, i should recognize the people who are involved in this project. worldwide telescope was envisioned by curtis wong and implemented by jonathan fay, who is a software engineer at microsoft research. at the harvardsmithsonian center for astrophysics, gus muench and i have been working closely with them since the beginning, but curtis and jonathan get all the credit. let me give another example. assume that i find an astronomical image somewhere on the web or that i go outside on a clear night with a camera and i take a picture of the sky. with this service called astrometry.net, i can submit a picture, and the service will take the image and miraculously calculate exactly where it goes in the sky and, if the picture is good enough, even figure out when it was taken. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 43  there was a service that some researchers were hosting to do this work, but they did not have enough money or servers. so they made a group called ﬁastrometryﬂ on flickr. as illustrated in figure 213, when you upload a picture to flickr, not only does it host the image but it also offers some services that, for example, tell you what is in that picture. as soon as a picture finishes loading, if you add it to the astrometry group, you will soon see all sorts of little boxes pop up on the picture, and it will tell you famous objects in the picture that it recognizes. the reason flickr can do this is because the astrometry.net program went in and blindly solved for the coordinates of the image. on flickr, after an image is solved by astrometry.net, you will see a button at the bottom of the page that says ﬁview in worldwide telescope.ﬂ if i click this button, then i will see that object overlaid on the sky where it goes, and i can compare it to 40 different layers of multiwavelength data. i can also use all the catalogsearching tools that i showed before and interact with all the statistical software, plotting software, et cetera, live. figure 213 seamless astronomy. source: alyssa goodman  there are two more projects in which alberto pepe is involved. in one of them, we are taking all of the historical images that are online, including all of the literature, and making a historical image map of the sky. with the second project, we can tell where particular papers on the sky are and why those papers were written, so you can get heat maps of articles on the sky. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.44 the future of scientific knowledge discovery in open networked environments  i will conclude by highlighting a couple of examples where one field of study has borrowed from another. we published a paper (goodman et al. 2009, nature) 2 years ago9 that has embedded in it an interactive threedimensional (3d) diagram; the idea is that you go there, click it, and you can put the data in the literature (figure 214). figure 214 data in literature. source: alyssa goodman  the techniques used to create and study the 3d data were borrowed from medical imaging and used in astronomy. in the new astronomy dataverse (theastrodata.org), we have borrowed tools from the social science world as well. the astronomy dataverse tools allow individuals or groups to publish their data online in ways that can be retrieved in the future and that are permanent and linked to literature. at present, we are experimenting with the astronomy dataverse at the center for astrophysics, to let researchers who have small datasets and want to share them with everybody have a way to publish them that is linked to the literature and is persistent. 9 nature, volume 457, january 1, 2009. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 45  integrative genomic analysis joel dudley stanford university  in this talk, i want to offer some tangible examples showing why it is important to integrate data in a certain way. it is not like a seed that you bury in the ground, and then in a few years it is going to bear fruit. rather, it is a process that produces immediate discoveries. i will present what we have done in just a few years by dealing with data in this way, essentially on a shoestring budget, and hopefully this will motivate larger investments in this area. we published a paper10 a few years ago describing how we took a gold standard set of type 2 diabetes and obesity genes and wanted to see how well we could recapture them. this is a classification problem. we looked at different modalities individually, for example, genetic experiments that are measuring type 2 diabetes, or proteomics, another modality for measuring diabetes in figure 215, the diagonal is essentially a random chance classifier, and anything worthwhile needs to be above it. figure 215 rates of detection for type 2 diabetes. source: joel dudley  we put all the modalities together, which is represented by the black line, and tried to recover a known set of type 2 diabetes genes. this is a very simple classifier, but what it  10 s. english and a. j. butte, ﬁevaluation and integration of 49 genomewide experiments and the prediction of previously unknown obesityrelated genes,ﬂ bioinformatics (2007) vol. 23 (21), p. 291. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.46 the future of scientific knowledge discovery in open networked environments  shows is that by putting all the data together, we are able to beat all of the individual modalities. compared with all the single investigators working very hard on their own single modalities, we were able to surpass each one of them by putting all their data together from the public domain. one of our main sources of data is the gene expression omnibus (geo) at the national center for biotechnology information. there is also a european counterpart called array express. the geo is a public repository. if a scientist publishes a paper that incorporates gene expression data, the journals require that this data be submitted into geo. geo is growing exponentially and has all kinds of useful data. people deposit their data in there reluctantly, but they are doing it. however, as dr. friend pointed out, there is a difference between accessibility and usability. geo is freely available and it is very accessible (e.g., high school students can access and download all the data they want), but it is not very usable. there are hundreds of thousands of samples. one sample is essentially data derived from microarrays. this is roughly a 10yearold technology. the very small chips that measure how much a gene is turned on or off in a biological state by measuring the abundance of its expressed transcripts are roughly a 10yearold technology. there will be certain levels of mrna expressed off the dna, and the chip has probes for each of the genes in the genome. by measuring the responses with a scanning laser, you get an analog readout of how much each gene is turned on or off. to put these data into geo in a structured format, they used a standard called miame (minimum information about a microarray experiment), but there were no semantic requirements put on the data. as a result, we have all the data in geo, but we do not know what is being measured. this means that we could not use geo to find, for example, all the experiments that measure a disease. it is just not set up that way, so we came up with a solution. since all these experiments are linked to a publication, we could find the publication in pubmed central and download the medical subject headings, and then map them onto the unified medical language system, which is a huge medical thesaurus created by the national library of medicine. in other words, since a paper linked to an experiment must be related to the experiment in some way, we could use the paper annotations to figure out what the experiment is about. we can indeed do that, and it has worked fairly well. there are other sorts of annotations related to which state is being measured in an experiment. for example, if we are going to do an experiment in which we measure healthy and sick people, some of the chips will be measuring the healthy people, and some chips will be measuring the sick people. there are some annotations that tell us which chip is which. this is very important information to have if there are remote users of the data. unfortunately it is free text, so that it is not done to a uniform standard. to refer to a person with diabetes, some cases might say ﬁdiabetic,ﬂ some might say ﬁtype 2 diabetes,ﬂ others might say ﬁinsulin resistance,ﬂ and so on. i saw an experiment where the annotations read ﬁdeadﬂ and ﬁnot dead.ﬂ to figure out which chips belonged to which subset, we did some text mining. the next problem we encountered was how to determine which genes were being measured on these chips. there are thousands of different platforms. some of them are custom made by universities. also, there are different commercial vendors with different technologies and types of probes. we had to determine which genes were on these chips. this problem had not arisen before, because scientists were not trying to map across these different types of the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 47  technologies; they got their chip for their particular study and stayed within their own technology. we also found out that these annotations were changing by 10 percent or so every year. we thus had to build an entire system that would constantly update and remap all these probes to tell us which genes were being measured on the different chips. we then had to determine the quality of the data. to do that we looked at how well data from different labs matched up. for example, we would look at data from one lab using a particular technology that measured type 2 diabetes in muscle, and compared that with data from another lab using another technology that measured type 2 diabetes in muscle in completely different patients. it turned out that they match up fairly wellšquite a bit better than random. so even though there are all these possible confounders, the quality of the public data is quite good. this was very encouraging. once you have all this data annotated in this way, it can be visualized using a matrix. in such a matrix, you can have 300 or so diseases in the rows and then 20,000 genes in columns, which produces a big grid of all diseases and all genes in a very clean, standardized way, showing how the genes change with the different diseases, as illustrated in figure 216.  figure 216 human disease gene expression collection. source: joel dudley  at this point we can start thinking of interesting questions to ask. you could simply cluster that grid and see where the diseases fall with regard to the various molecular bases. this will provide the molecular classification of disease, and then you can see some patterns. brain the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.48 the future of scientific knowledge discovery in open networked environments  cancers are clustering together, which makes sense, but then you see that pulmonary fibrosis is clustering with cardiomyopathy, which seems strange. it is a lung disease clustering with a heart disease. typically these are very different diseases, but maybe at their molecular level they have some similarities. then you start to wonder what the implications might be for drug treatment. we have all kinds of drugs for cardiomyopathy and hypertrophy and similar cases, but we do not have any drugs for pulmonary fibrosis. therefore if the molecular basis is so similar, we might be able to borrow drugs from one disease to deal with another. we do not quite use this representation to figure that out. instead we have discovered a new way to mine the public data for drug repositioning, as it is called when we borrow a drug from another disease. we have a compendium of clean disease data that is semantically annotated, and we did the same thing for drugs. see figure 217. this gives us both the disease and the drug data. now we can look at treated versus untreated diseases and get a signature of the disease state versus the healthy state, and the signature in this case is how the genes are changing between the two conditions.  figure 217 mining public data for drug repositioning. source: joel dudley  similarly a scientist can look at how the genes are changing in response to different drugs and do pattern matching to match them up statistically. this is exactly what we did, and it produced a representation like the one in figure 218. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 49  figure 218 reference database of drug gene expression. source: joel dudley  if a scientist has thousands of drugs and hundreds of diseases, and gives a therapeutic score to each pair, figure 219 shows how the heat map would look.  figure 219 heat map of drugs and diseases. source: joel dudley  after we have done that, we start to see interesting clusters. we might ask why all these drugs and diseases appear in a particular cluster, and check to see if the drugs in the cluster are known to treat the diseases in this same cluster. we might also ask if the drugs that have not been identified as treating those diseases might actually be effective against them. the first computational prediction that we tested was whether cimetidine, an antiulcer drug that anyone can buy inexpensively at any pharmacy, would treat nonsmallcell lung cancer. it seemed a very strange prediction, but we thought we would try it (figure 220). the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.50 the future of scientific knowledge discovery in open networked environments  figure 220 antiulcer drug inhibits lung adenocarcinoma in vitro and in vivo. source: joel dudley  when we put human tumors in mice and tested the drug, we got a dosedependent response. it shrank the tumors, and it was even getting close to a chemotherapeutic drug, doxorubicin. we can see on the top left image the cimetidinetreated tumors on the left and the control on the right. we can also see on the right side the fluorescence microscopy of the actual cancer cells. the green lighting in the bottom right box indicates that when the cells were treated with cimetidine, there was a lot of apoptosis, or cell death, in the cancer cells. we predicted that this overthecounter antiulcer drug would be antineoplastic, or anticancer, and it seemed to be efficacious in this animal and human cell line study. another prediction we made was that an antiepileptic drug would treat inflammatory bowel disease, which was another strange connection. we got the rats that have an induced phenotype of inflammatory bowel disease, the tnbs model. a very caustic compound is put in the rat™s colon to induce the inflammatory response. when we induced the disease and treated the rats with the drug, it ameliorated the disease. this is a very harsh phenotype, but the drug appeared to reduce the inflammation. this is an antiepileptic neurological drug working on the colon. at the bottom of the image in figure 221, you can see the pink colons of the rats that were treated, while in the untreated rats on the top you can see all the necrosis. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 51  figure 221 antiseizure drug works against a rat model of inflammatory bowel disease. source: joel dudley figure 222 provides another recent example. we predicted that a cardiotonic drug, which was researched quite heavily in the 1980s, but never approved for any disease, would also work for inflammatory disorders. we put it in rats that had an inflammatory phenotype. what can be seen in the table are the blood level measurements of a cytokine called tnfalpha, which is involved in inflammation. on the right you can see that this heart medication substantially reduced tnfalpha levels and plasma in rats. figure 222 cardiotonic drug inhibits or ameliorates inflammatory cytokine tnfalfa. source: joel dudley the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.52 the future of scientific knowledge discovery in open networked environments  also, we took several experiments related to transplant rejection from the public data, intersected them in a certain way, joined them with a number of serum measurements of proteins, and found a biomarker for transplant rejection. this is a particularly important biomarker, because when someone gets a new kidney, for example, medical personnel stick a big needle through the side of the patient™s body, suck out a little tissue around the kidney, look at it under a microscope, and, if this person is lucky, they get the part that is being rejected, and they will try to manage it that way. just by using the public data and integrating them with public datasets, we computationally predicted a new bloodbased biomarker. we are testing it, and it looks like it is also able to be found in the urine. therefore, instead of putting a big needle into the side of the patient™s body, it should be possible to take a urine sample and tell from that whether the kidney will be rejected. again, this is a repurposing of public data. we did not generate any new data from the patients at stanford university to do this, although that is where it was tested in the clinic. there are many more examples from our lab. we just found a new disease risk variant for type 2 diabetes, which has been validated in a japanese population of thousands of cases and thousands of controls. we found a new drug target for type 2 diabetes and validated it. indeed, these are all validated experimentally. we found biomarkers for medulloblastoma, pancreatic cancer, lung cancer, and atherosclerosisšall validatedšand this is just from our lab, but it should not be just from our lab. the reason we can do this kind of work is that we have significant computational capabilities. everyone who works in our lab has professional or graduatelevel training in molecular biology and computer science. they can draw cell signaling pathways from memory and also program a compiler. it is unfortunate, however, because much of this technology is an expensive commodity, and the people who actually have the questions cannot do anything with their data. another example involves the singlenucleotide polymorphism (snp) chips, which measure genetic variation on about a million different regions of the genome. we can combine them with the gene expression microarrays, and for relatively little expense, a commissioned researcher can then assemble a group of patients and measure both their dna variation and their gene expression variation. from that, the researcher can figure out such things as which genetic variants are really behind a disease. the snp chip contains a million probes, however, and the gene expression microarray measures 50,000 probes, so the researcher is looking at 50 billion array comparisons to determine which are related, which would take several years to do on a desktop computer. we therefore did a case study that showed it would be possible to do the research in the cloud and pull out 100 nodes with a minimal cost (figure 223). it required a lot of programming that the clinical researcher typically would not be able to do, and there are no tools available that allow the researchers who generate the data and have good questions to answer the questions with their own data. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 53  figure 223 in silico research in the era of cloud computing. source: joel dudley  i think, however, there is a pragmatic first step that can be taken with cloud computing. many people will disagree with me on this, but there are many people working on perfect workflow systems, and although people in biology do not seem to like to use them, i think those tools will become the standard. one problem is that people like to keep everything inhousešand probably always willšbecause they are at the forefront of these research areas, and the standard tools do not necessarily allow them to do what they need to do. one valuable aspect about the cloud is that the entire infrastructure is virtualized, so, for instance, i was able to set up a pipeline for doing drug prediction. it is simply a number of linux servers and our code. what i can do is to freeze that in place, and at the very least i can hand it over to someone in the cloud who can clone it. i am a big believer in simple first steps and rapid iteration versus locking yourself in a room for years to come up with a perfect system that solves all the problems. i think there is some promise there, and we are starting to see it happen. there are some good tools that are being created to facilitate reproducible computing in the cloud. let me now focus on some of the lessons learned from the work in our lab. so far, sticks have worked better than carrots. biologists hate sharing their data. the data are voluminous and although the researchers are supposed to publish them in geo, there are many microarray experiments that do not get published. when we find that a paper that is funded by the national institutes of health and it has microarray data that are not in geo, we call the microarray gene expression data society, and the people there follow up with the researcher and ask that person to post the relevant data there. we constantly have to do that, so we hope that carrots will work better at some point. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.54 the future of scientific knowledge discovery in open networked environments  in ontology, we have been able to accomplish much by doing very lightweight and simple annotations on gene identifications. we did not need to model biology perfectly in a higher ontology to do what we needed to do. computation is a major bottleneck. right now there is a privileged computational elite. there are people who cannot even answer questions from their own data on their own patients, and, on the other hand, there are the computational elite who have 500 cpu clusters in their labs and the power to do this kind of work. obviously this is not fair. questions first, data second. what i mean by this is that we did not really know the type of system we needed until we knew the questions we wanted to ask. there are many people in informatics who are pattern hunting and trying to find hypotheses, but it was not until we knew the questions we wanted to ask from the data that it became apparent what type of system we needed, and it was very different than what we had. finally, to sum up, i would like to emphasize that the advancement of new biology and medicine is possible through public data. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 55  geoinformatics mohan ramamurthy unidata i have been with the unidata program for the last 8 years, and before that i was on the faculty at the university of illinois in the atmospheric science department for 16plus years. my talk, therefore, will cover the perspectives of the different hats i have worn in the past 25 years or so. unidata is a facility funded by the national science foundation (nsf) for advancing geoscience education and research. we are part of the university corporation for atmospheric research (ucar) and a sister program to the national center for atmospheric research (ncar) in boulder, colorado. part of our mission is to provide realtime meteorological data to users throughout the world. it started with academic universities getting the data, and that data distribution has expanded to include other organizations. we do not give just data to our users; we also give them a suite of tools for analyzing, visualizing, and processing them. one of the software products for which we are best known is netcdf, because it has become a de facto standard in the geosciences. the data we provide get integrated into research and education in universities, as well as in many high school classrooms. we have more than 30,000 registered users. there are many more who do not register, but get the software. the users come from 1,500 academic institutions and 7,000 organizations in 150 countries around the world, representing all sectors of the geosciencesšnot just academia, but government, nongovernmental organizations, and the private sector. they get the data and the software from us, because everything we do is available at no cost. i want to go back almost 25 years to the work that my former colleague, dr. robert wilhelmson, did at the university of illinois and at the national center for supercomputing applications (ncsa), which ushered in a new era of geoinformatics for my field. that work was the result of a confluence of three factors. first was the availability of highperformance computing. because the ncsa had been established a few years earlier, he had access to cuttingedge, highperformance computing systems. second, he had software for visualizing and creating fantastic graphics, which won first prize at the london computer graphics film festival in 1989 and was also submitted for an academy award nomination. third, there were massive quantities of data coming from stateoftheart cloud computer models. from those factors, geoinformatics was born, and it revolutionized our understanding of supercell thunderstorms, convective storms that spawn tornadoes, and mesoscale connective systems, such as the one dr. graves described earlier. in february 2011, science magazine had an entire issue devoted to data, and one of the articles focused on climate data and how they are revolutionizing our understanding of climate changeša point that will be woven into my talk throughout this presentation. we know that data are the lifeblood of our science, but we need to move from creating data to creating knowledge and discovering new frontiers. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.56 the future of scientific knowledge discovery in open networked environments  in 2008, wired magazine published a provocative article with the title, ﬁthe end of theory,ﬂ11 which argued that the data deluge makes the traditional scientific method obsolete. i am not sure i agree with that assessment, but it was provocative, because it was essentially modeled after what google has done in the search business, where all search results and page ranks are driven by data rather than an a priori model of how things should be organized and ranked. this was the original yahoo model, but that became obsolete as a result of google™s work. i want to focus now on the multidisciplinary aspects of the geosciences. whether we are talking about ozone depletion over the south pole, mud slides in china, forest fires or deforestation of the amazon, climate change in general, or the impact of el niño or la niñašto make advances in any of these areasšscientists have to utilize data from many geoscience disciplines, related social sciences information, and other ecological information. francis bretherton, one of the former directors of ncar, created the famous earth systems science schema, which scientists now refer to as the ﬁbretherton diagram.ﬂ dr. bretherton got scientists to realize that they should not be looking at just atmospheric systems, hydrologic systems, or ocean systems. all of these different systems are interconnected, whether it is the cryosphere or the hydrosphere, or even ecology and chemistry, and looking at questions from a systems perspective is the key. however, to do systems science and research and get meaningful results, we need to integrate information across all these different systems as well as to synthesize them across the domains. one of the challenges for geoinformatics is providing the right type of data in the right format to the right application in the right place. one of the key aspects of that process is interoperability of data across all the different disciplines that are involved. the founding president of ucar and the first director of ncar, dr. walter orr roberts, had a famous mantra: science in service of society. we should not stop at just doing science, but we should keep in mind that science has a meaningful impact on society, which means that we should be thinking about geoinformatics and data services that are endtoend, that are chained through workflows. for example, scientists should not stop at the prediction part, which was the focus of the work that dr. graves presented, but they should go beyond that and couple the predictions with the flooding models, ecosystem models, and so on. if scientists are looking at the societal impact of flooding from tropical cyclones and hurricanes, they should integrate data from atmospheric sciences, oceanography, hydrology, geology and geography, and social sciences, and then use the results with decision support systems. in short, we need geoinformatics studies that encompass these endtoend aspects. the issues associated with the deluge of data came up earlier. i like the ﬁsea of dataﬂ metaphor for reasons explained in a talk i heard by tim killeen, the current assistant director for geosciences at the nsf. when we look at it as a sea of data as opposed to a deluge, we have two options: to drown, or to set sail to new land and make new discoveries. that is very appropriate, because data bring us to new discoveries in the world. speaking of a sea of data, here is some information about the growth of data that is expected in the future. today we have less than 15 petabytes total of atmospheric science data, but that is predicted to grow to 350 petabytes by 2030, with the data coming largely from  11 available at http://www.wired.com/science/discoveries/magazine/1607/pbtheory. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 57  models, particularly those with high resolution, in ensemble mode, and integrated in time scales from days to 100plus years. there will also be data coming in from the new generation of remotesensing systems, including radars and satellites. dr. graves mentioned the traditional radar systems, but in norman, oklahoma, they are developing a new kind of radar that was previously deployed by the military 20œ30 years ago. it is called phased array radar, and it can scan an entire thunderstorm in a matter of seconds, as opposed to the 8 to 10 minutes that it takes today to scan a thunderstorm with traditional radar. furthermore, the new generation of geostationary weather satellites will have 1,600 channels in the vertical for sounding capability. currentgeneration satellite gives 20 channels; so we are talking about an 80fold increase in the number of channels that give us detailed information about the atmosphere. the polar orbiting meteorological satellites will also provide large volumes of data once the joint polar satellite system program is finished. it has been delayed many years now because of various difficulties, but once it is deployed, 3 terabytes of data are expected to be produced every day. with all these data being produced, how do we extract knowledge and make new discoveries? the field of knowledge extraction and data mining addresses the questionšgoing from data to information to knowledge to wisdomšbut the field is still very much in its infancy. some great work is being done at the university of alabama in huntsville under dr. graves™s leadership, but our field is still mired in these simplelooking plots that produce spaghetti diagrams of ensemble model output or postage stampœtype maps. these may be of some value when there are only 10 ensemble members, but when there are 1,000 ensemble members, there is no meaningful way to understand which data are together, which are clustered, or which have similar patterns and relationships, so an automated capability to extract that knowledge and information is very much needed. every 5 years or so, the intergovernmental panel on climate change (ipcc) assessment reports are written. the group™s fourth assessment report resulted in the nobel peace prize being awarded to the ipcc, along with al gore, for work in bringing awareness about climate change issues. one of the simple actions that happened in the geoinformatics world was that the ipcc fourth assessment climate modelers were mandated to submit all of their model output using the netcdf climate forecast convention to the program for climate model diagnostics and intercomparison (pcmdi)œearth system grid (esg) data portal. the result was approximately 600 publications within 6 months after the ipcc fourth assessment report activity was completed. all the model runs were submitted, and they were made available by the pcmdiesg data portal, so everyone could compare their models immediately because they all used exactly the same structure format standard and convention for their data. that mandate had a huge impact on producing new discoveries in a very short period of time. the next assessment report is now being prepared and the model runs are getting started. the ipcc is looking at the total assimilation of 90,000 years of climate change model results, consisting of many petabytes of output from 60 experiments and 20 modeling centers around the world. they have been mandated to produce the data and output and to have all of the necessary metadata available for the model runs, which was not required for the last assessment. for the next assessment, all of the models need to document, in a consistent way, the metadata associated with how the model was configured, how it was run, and so forth, and the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.58 the future of scientific knowledge discovery in open networked environments  to make the data available through the earth system grid portal, using the netcdfcf format. this again raises the question of how we are going to process all of the data the other type of ensemble work that i did in my previous work was to make ensemble predictions for hurricanes and landfalling tropical cyclones. in this work, a scientist can create ensemble numbers by varying the initial conditions, boundary conditions, physics of the model, sea surface temperatures, and other criteria a scientist has for a prediction system from the external forcing standpoint. this particular system was generating 150 to 200 runs, considering the different permutations and combinations, and we needed an endtoend efficient workflow system, as well as an informatics system for providing seamless access to all of the data and information that was created. therefore, we need to be thinking about geoinformatics systems that are capable of facilitating these things and delivering information. the hurricane research community has proposed a framework for improving hurricane predictions, and, as part of its metrics, the group is attempting to reduce both average track error and intensity error by 50 percent for a 1to5day forecast. given that the intensity prediction for tropical cyclones and hurricanes has not improved by even 10 percent in the last 20 years, this would be a gigantic leap, and they are expecting to do very highresolution predictions as well as ensemblemode predictions to make that possible. all of the data from this endeavor will be openly available. that was mandated by the authors of the 2008 ﬁproposed framework for addressing the national hurricane research and forecast improvement initiatives,ﬂ because the group figured this is the only way it is going to realize these results and the metrics they are aiming for. i also want to talk about data visualization as a way of enabling new insights and discoveries. i will focus on one of the software programs that unidata has developed, the integrated data viewer (idv). i am not a geophysicist, but i know that the idv was used by a group called unavco, in the geon (geoscience network) project, which was another large nsf information technology research project under the linked environments for atmospheric discovery project (lead), described in the previous talk by dr. graves, and which got started about 8 years ago. unavco adopted the integrated data viewer as the framework for visualizing data from the solid earth community. figure 224 shows the idv integrating geophysical data. we are looking at seismic plate activity along the west coast. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 59  figure 224 integrated data viewer (idv). source: unavco  at the bottom left are stress vectors and fluids, the mantle convection assimilation, and mount st. helen™s seismic activity underneath. the people who were doing this work told us that they could never see these aspects before this threedimensional idv visualization tool became available. it opened their eyes to how the earth works underneath the surface through plate movements and mantle convection. i also want to mention geographic information systems (gis) integration, because i work in a field that is focused on geospecialties. we need geospecialtyenabled geoinformatics cyberinfrastructure so that we can integrate locationbased information, because many of these processes and events are very much related to the geography. we should not be thinking of gis as an afterthought. the historical way of thinking about gis is that atmospheric scientists would do their work and then eventually somebody else would take over from there and put the information into some kind of a gis framework. unidata is working to enable netcdf data models to interface directly with gis capabilities through the open geospatial consortium standards for web coverage service, web features service, and web map service. as a result, when we have a netcdf data cube, that data cube becomes available as a web gis service, and we can produce maps like the one to the right in figure 224 from that particular capability. i cannot conclude this talk without talking about data citation, because it has already come up in almost every presentation so far. to me this is the next frontier, i want to give an the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.60 the future of scientific knowledge discovery in open networked environments  anecdotal story to demonstrate that this is not a technical challenge, but a cultural and social challenge. this particular topic came up at the american meteorological society. the publication commissioner, who is in charge of all scientific publications of the american meteorological society, took a proposal modeled after a royal meteorological society proposal, which was intended to encourage data transparency. he went into the meeting and said that the condition for publication should be that authors be required to agree to make data and software techniques available promptly to readers upon request and that the assets must be available to editors and prereviewers, if required, at the time of review. he was basically told that his proposal is a nonstarter, because we will never get the cooperation of the authors. this shows that we still have a lot of work to do. i also want to talk about globally networked science. the ipcc activity is the gold standard for that, but i also want to mention one other activity that ncar and unidata have been involved with, which is the observing system research and predictability experiment (thorpex), an interactive and global ensemble project. it is generating 500 gigabytes a day, and the archive is now up to almost a half a petabyte. it brings in data from 10 different modeling centers twice a day. we need to be able to create geoinformatics for these kinds of databases. these are some of the challenges. i want to come back to the point that was made earlier about interfacing social networking systems. as a geoscientist, i have seen gps, coupled with mobile sensors, revolutionize the geosciences in a major way. i think there are some incredible opportunities, not just for citizens and science, but also for providing workforce development, geospatial awareness, and education for our students at all levels. this is very important, and how we use social networking tools like facebook for getting the scientific community to provide commentaries, share information and data, and work in a collaborative way is something that is a real opportunity and definitely a challenge. in closing, i would like to emphasize that we live in an exciting era in which advances in computing and communication technologies, coupled with a new generation of geoinformatics, are accelerating scientific research, creating new knowledge, and leading to new discoveries at an unprecedented rate. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 61  discussion discussant: mr. dudley, in your chart of supporters, i see some national institutes of health (nih) institutes, but none from the national science foundation (nsf). however, you were working at the intersection of what nih and nsf medicine would fund. i find that curious. when i was at nsf 10 years ago, i discussed with nih colleagues the cultural differences, such as how nsf will fund soft money research and how nsf likes proposals to be more exploratory, while nih likes them to be more incremental. we did not come to an agreement, though. there was a division director at nsf who liked to encourage joint work between computer science and medicine. now that you are doing this kind of work, what organizational steps would you take to help in that area? mr. dudley: our funding strategy has always been to involve the computer scientists in whatever we are doing. one way we are trying to solve it at stanford university is by trying to mix the computer scientists and the biologists more, letting the computer science experts figure out what the important problems are in medicine and how they can engage in those problems, especially for the algorithmic issues. we have been struggling with this problem for a long time, and we have just been pragmatic about it and have gone where we can get the money most easily. discussant: if i am collaborating with the doctor, he writes the nih proposals and hires my students. who writes the nsf proposals? mr. dudley: at stanford, we now have a lot of rebranding. for example, computer science people are rebranding themselves as bioinformatics experts, because they need funding. therefore, instead of trying to deal with the problem, they rebranded themselves and went to nih. discussant: as a lawyer, i sometimes think of myself as a social engineer. most speakers have described a series of social engineering problems, such as institutional culture affecting funding and institutional culture within the disciplines, so it is a big problem. imagine this scenario: i am at a midtier institution, and what the presentations showed me is that i am the ﬁsharecropper.ﬂ you have got both the brain power and the computing power to utilize all the data, process them, and do different kinds of work with them that i will never be able to do. therefore, my only competitive advantage against you for funding is to hide the data. how do you respond to that? if you are going to break the logjam, what are the incentives for researchers to share their data with you? mr. dudley: you are exactly right about that, and that was the point i was trying to make when i said there are computational elites. it should not be that way, however. i think that if we can get some good tools and the cloud, we can level the playing field for costs and accessibility for many researchers at midtier institutions. in fact, we wrote a grant for the nih to build such a system, which was rejected. dr. friend: i think the point you made is important. i want to separate researchers into data gatherers and data analyzers. right now the way it works is that the person who collects the data expects to have the right to take it all the way through to the insight, and then the next person should go back and do it again. that system is wrong. it is just inefficient. i think that having core data zones where people who did not generate the data can mine the data and work with them is important enough that funding institutions and journal publishers need the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.62 the future of scientific knowledge discovery in open networked environments  to determine a way to make it happen. a second issue is the matter of humility. it is very hard to get the ﬁtitans on olympusﬂ to share their data even among themselves. dr. hey: we work with some members of the hiv committee. the moment they send us their data, we send them the results with some tools that we developed. there is no real reason we could not make those tools available in the cloud, and we are planning to do that. however, i do think the cloud can empower scientists in certain cases. we are also working with the ameriflux community. they have about 150 sensor towers and fields all over america. typically they are owned by a principal investigator, and they take data and publish it with their graduate and postdoctoral students, so it is generally one professor and two students. by putting the data into a central data server, like the skyserver, the community is gradually realizing that there is much more value to be gained by publishing not just one tower™s data but several. that is a sociocultural change, because instead of one professor and two students we now can have 10 professors and 20 students, which makes it a 30author paper. for that particular community, this is a big change in culture, and it takes time. dr. goodman: in astronomy, the situation is very different, and those of us who like the astroinformatics approach are lucky if we have tenure, because people still do not appreciate that kind of approach. getting the last three photons out of a keck telescope is what gets someone the ﬁolympianﬂ status these days. that is changing slowly, but it is a big battle. mr. uhlir: i would like to follow up with a comment and a question. the comment has to do with the issue of incentives. many people talk about the importance of citation of data and the role of that in giving people incentives to share data, through recognition and rewards, and changing the sociology of how people receive their data and share them. the board on research data and information is about to start a 2year project in collaboration with some other groups, in particular, the international committee on data for science and technology (codata), on data attributions and citations. we will start with a symposium in berkeley, california, in august 2011. the other issue, which most of you have largely avoided, is the human infrastructure element. obviously this symposium is about automated knowledge discoveries, so you have appropriately focused on the technical infrastructure and automated knowledge tools, but there seem to be two schools of thought about how this is going to be managed and promoted in the future. one is that the technology will disintermediate many professionals in libraries of science or in information management, because everything is going to be automated and work perfectly, and we will live in a wonderful automated information age. the other school of thought is that the deluge of data will require not only more people to manage it, but also a retraining of the information managers and reorientation of the traditional library and information schools. so my question is, which school of thought is correct? or perhaps they both are. also, what do the speakers feel needs to be done to develop this kind of new expertise, because a lot of it seems to be done by people mostly in the middle of their careers? is there a new approach to educationšhigher education and training at the outset of careersšthat is being implemented, and if not, what would you suggest? dr. graves: there are new programs starting in data science and informatics. we are initiating one at the university of alabama in huntsville. i know rensselaer polytechnic institute (rpi) is also starting some programs, as well as several other universities, and they are attempting to be interdisciplinary. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 63  dr. ramamurthy: the creation of unidata was intended to address the concern that you just expressed. atmospheric sciences has been at the forefront of data sharing and of bringing data into the classroom and giving studentsšat all levels, whether it is an introductory atmospheric science class or master™s and ph.d. studentsšaccess to the same kinds of data and tools that are used by professionals in their operational forecasting or in research. having that facility that has been sustained for 26 years has made the issue of access to atmospheric science information and data systems almost a nonissue from a workforce development, because literally hundreds and thousands of students go through their labs and classrooms and use stateoftheart software and data technologies in their daily work. the key is having that kind of a sustained infrastructure that will provide the services that are needed for training the next generation of students and for creating a workforce that will be very skilled in dealing with geoinformatics of the future. dr. conti: there is a need for a paradigm shift in astronomy. the vision that the younger generations bring to this area is very refreshing. they suggest that we can solve this problem from a completely different point of view. at the same time, many of them bring very little knowledge of astronomy combined with an extremely high knowledge of computer science, and those have the ability to solve problems much more quickly and therefore to ask many more questions. therefore, in our programs, we try to find a good balance between people being trained astronomers, but also having a focus on computer science as well, because the future is going to depend on how they are going to manage data and how they can ask new questions that were not asked before. the other issue is that once we have all these data, our ability to mine them is extremely important in the sense that we need to be able to understand how the data can be manipulated. therefore, it is important to change the curriculum to reflect the fact that computer science has to be part of our daily life. we find that younger students are used to managing and producing astronomy and looking at astronomy through the worldwide telescope. they do not know the inner working or what it takes to produce plenty of data, but they know what they want to do. they know how to discover new pathways through the data. there is a lot of resistance among senior researchers, but perhaps we just need to learn to say, ﬁi do not really understand how you get to this, but show me how, because we can open a new frontier.ﬂ dr. friend: i want to make three points. the first concerns what we have found to be necessary to bridge the gap between the data mining and the understanding of biology. it is very rare that a single individual does both, and so we split it into two parts. we have people that we call network biologists, who may be mathematical physicists, coming toward the models from one direction, and then we have systems biologists, who understand enough of the math and understand the biology. the people who can do that sort of polymath linkage are rare, and so the education and career paths are split in a way that is similar to what happened previously when someone chose to become either a molecular biologist or an enzymologist. it is important to not expect someone to bridge the entire gap. the second point is that dr. eric schadt, who drove much of this work, and several others have just started a new journal. the journal will have a section that is just for publishing models that are not validated. a validation often comes later, and the point is to make the model available so that people can work on it. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.64 the future of scientific knowledge discovery in open networked environments  the third issue relates to the discussion about structuring rewards. in medicine, we are convinced that the patients and citizens themselves must break the logjam between the scientists who do not want to provide the data and those who are trying to get access to large amounts of data. if there are electronically generated data from a patient, the patient can get them back. this is going to be a very fundamental changešshifts in how data are going to flow. the development of the law and the role of the citizen are going to be very important in getting the data flowing, because academic institutions are not necessarily going to do it on their own. dr. hey: i have three brief comments. first, i do think that research libraries are in danger of being disintermediated. librarians are caught in a difficult trap. subscription costs are clearly a crisis, because the budgets are not increasing at the same rate. there is an interesting move toward ischools, but the question remains: what should they train the next generation of librarians to do? second, training will be important in changing the culture of science. we should work with graduate students in each science, because it is clear we are not going to convince the faculty. i think we should train people to have at least depth in a couple of disciplines. universities change very slowly, however, so my worry is that we will produce bright graduate students with nowhere to go. i think that is a real issue, although some universities are doing things better. third, there are three communities: the scientists doing the real research, the computer scientists who know the technologies, but we also need information technology experts, who can turn a research prototype into a tool that people can actually use. disussant: i see the success in bioinformatics and geoinformatics as disciplines, and i see communities of scientists working in multidisciplinary ways with other communities, such as statisticians and computer scientists, but we do not see that much in astronomy. one difference is that there was a tipping point reachedšcertainly this was true in the medical communityšthat so many papers are being published that it is impossible for anybody to keep up with what is happening. you therefore need some kind of a summarization or aggregation and an informatics approach to give you the highlevel picture. i like the historical example at the university of california, berkeley, where you can move from a higherlevel view down to the exact specific event. imagine doing the same thing with our databases. maybe in astronomy we have not quite reached that tipping point. we have learned with the virtual observatory that if it is built, users will not necessarily come, and so many astronomers are not using this infrastructure. some are using it, but not as many as we would like. then, as i was listening to this education discussion, i thought that perhaps that tipping point could be completely different from the one i was thinking. it would be more the education system that is the tipping point, with the younger people who are familiar with social networking and computerbased tools, whether we call it web 2.0 or science 2.0. i would like to call it citizen science, that is, getting people in touch with the science and them being inspired to do science, knowing that they approach it with either the skill sets in using these tools or at least an interest in learning to work with them, because that is the way their lives are, and now they discover they can do science with it. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 65  my university, george mason university, is one of those places, like rpi and a few others, that are offering this sort of informatics space education. we have found that it is hard to draw the students into the program, especially the undergraduates. our graduate students already understand it, so the graduate program has a good number of students, but our undergraduates still do not. we talk to students and ask what are they interested in, and they say, ﬁwe like biology, but i want to do something with computers.ﬂ when we finally get these students into our classes, their eyes open up and they say, ﬁi did not know i could do science with computers.ﬂ so, in a sense, the younger generation is reaching that tipping point. they are discovering that they can do what they enjoy doing, such as social networking and online experiences, and do science at the same time. what i would like to see as a goal is not only doing this kind of work at the graduate level, but moving it to the undergraduate level, as we are doing now, and then moving it even further down to the high school students. i would like to teach data mining to school children. it would not be so much the algorithms of data mining, but it would be evidencebased reasoning and evidencebased discoveryšbasically detective storiesšand in this way we could inspire a generation of new scientists just by using a dataoriented approach to the way we teach these topics. dr. goodman: perhaps the core or at least part of the problem is the scientific paper as a unit of publication. i will give an example. seven firstyear graduate students in our department started on their own project called astro bites. they go to astroph, which is the archive server for astronomy, select the articles that they like, and review them. this is now being read all over the world. it is searchable by any search engine, and they can discover useful materials there. this is kind of the junior version of a blog called astrobetter, which is produced worldwide by astronomers who are of the mindset that the last discussant was talking aboutša kind of astroinformatics community that is evolving in a mostly 30something world. those are the same people who would like to take their model or dataset and make it available for people to experiment with. right now there is an interesting concept of the data paper in astronomy. i did not even know what this was, but fortunately when we finished a very big survey about 5 or 6 years ago, i had a postdoctoral student who said, ﬁwe have to publish a data paper now,ﬂ and i said, ﬁwhat is a data paper?ﬂ it is basically a vacuous paper that says, ﬁhere are my data. cite me.ﬂ she has gotten hundreds or thousands of citations because of that data paper. this should not be this way. going back to mr. uhlir™s point about people and training, there is a whole new system that is emerging. i do not know exactly what it looks like, but it does not involve 12page papers as the currency of everything, and it does not involve even looking at those papers to find the data. such papers are just one of many resources. how we develop the resources by all these other new people, i do not know. discussant: part of what we are seeing is the realization that a citation count has become an outmoded way of thinking, and yet it is still the h index that gets people into the academy. take, for example, danah boyd. she has 50,000 twitter followers. she is not too worried about her citation countsšshe does fine in her papers. but, again, what does it mean and why does that work? discussant: the right solution is for those of us who are thinking about this area to engage in that conversation, to look for ways to balance the traditional measures with some the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.66 the future of scientific knowledge discovery in open networked environments  new kinds of metrics. we should look for impact metrics, as well as for sustaining papers. people should still write papers, but we need to get a better mix that looks across a much broader range. dr. goodman: where does the data scientist, who maybe does not typically get included in a publication, come in? dr. conti: most of the time we are penalized for being a data scientist. perhaps one scientist does not have as many citations as some others, and does not care to publish that many data papers; as a result, the tenuretrack path is harder. discussant: one key is that those of us who are involved in other people™s tenure decisions have to rethink what we examine, and not get the junior faculty to rethink what they are publishing. dr. friend: some people who have been looking at this have noted that the music industry is fairly advanced in figuring out microattribution and that there are ways to borrow from that experience, by looking at impacts without using citations. we should think about using tools that others have developed to determine impact factor. discussant: i work for the national ecological observatory network, or neon. it is funded by the nsf to build a network of 60 sites across the united states to collect data on the effects of climate change and land use on the ecosystem. we will be measuring something like 500plus variables, and we will be delivering these measurements freely to the community. the measurements range from precipitation and soil ph all the way to things like productivity. we have talked about dataintensive science, of bringing a new era of how scientists can work, but what about dataintensive science in forming our policy and in forming how we design our cities, for example? from neon™s perspective, we will be one of the many data providers, along with the national aeronautics and space administration (nasa), the national oceanic and atmospheric administration (noaa), and other institutions that will be providing similar data, but somebody else has to do the aggregation of those data into knowledge. those people will need tools, so who is going to build the tools? and will these tools be used by federal agencies, resource management makers, and policy makers to craft how they will plan the nation™s national resources? dr. hey: we have a couple of projects that show there is some hope. one is a project called healthy waterways in brisbane. they had a big drought, now they have too much water, but it is relevant in either case. they have a system for doing ecology reports on their water supplies, and it took several people several days to get a report written and distributed, which is released every week. by putting in some serious information technology, but nothing research grade, one person can now do it in 1 hour or so, and that person can get much more flow of information, so ecologists have a much better evidence base with which to do emergency measures, for example. the other project is an urbanization project in china where they are building 10millionpeople cities. they will need to worry about all the environmental impact issues, as well as designing the city. i think there is hope, but i agree it is a challenge. i think that more case studies are needed. mr. dudley: arizona state university has a building called decision theater. i do not know how successful it has been, but this building has liquid crystal displays around the wall, 360 degrees, and it was built specifically for policy makers to develop evidencebased the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 67  views of policy making. i heard rumors that the state legislature wants to defund the entire arizona state university, so maybe it was not too successful, but it is something to look into. dr. ramamurthy: as atmospheric scientists, geoscientists, and geoinformatics people, almost all of the data we produce has societal relevance. it is important not to stop with scientific informatics, but to try to determine a way to interface it with decisionsupport systems and other kinds of policy tools. in the climate change arena, everyone is focusing on mitigation and adaptation, because it is not just whether the globe is going to be two or four degrees warmer in the next 50 or 100 years, but what it means for the watershed, for agriculture, for urbanization, for coastal communities, and so forth. it is absolutely critical that the scientific information systems and geoinformatics systems can interface with the other tools and other systems. we do not build those systems in my program, but we do think about interfacing with geographic information systems, such as what needs to be extracted from climatechange models to help people talk about what the agricultural picture is going to look like 50 years from now, whether for growing wheat or barley or something else. dr. goodman: i have a question for the people from the funding agencies. dr. hey said ﬁsome serious information technology, but nothing research grade.ﬂ this is a huge problem. here is a true story. at a meeting i had yesterday we had an undergraduate researcher who works with a postdoctoral student on a project, who said, ﬁi do not know where we are going to put these data, because we need a couple of terabytes, and the computation facility has a lot of security systems, so they will not let us run the right software,ﬂ and so on. i knew that my assistant had some extra money from the division and that she bought a stack of 40 boxes of terabyte drives, so i gave a terabyte drive to each of the researchers, and i solved the problem. the point is that there was an obstacle and that there was no person whose job it was to facilitate that and to make the data usable and streamlined for the future. so my question is, how are we going to pay those people in the future? where are we going to get those people? dr. berman: if we think about the impact of digital informationšwhat we need to store, how long we need to store it, how we need to curate it, and how we need to respond to data management policiesšthe university libraries are reconceptualizing themselves and can step into that role, but they need some help. so the question is, how can we jumpstart the system, so that when universities are thinking about infrastructure costs, they consider the data bill along with the water bill, the liability insurance, and so on? the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 69  3. how might open online knowledge discovery advance the progress of science? technological factors session chair: hal abelson mit the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 71  interoperability, standards, and linked data  james hendler rensselaer polytechnic institute  i will focus on the technological challenge of online knowledge discovery. i will first address interoperability issues and then talk about some promising developments. scientists can learn a lot about data sharing just from observing what is happening in the outside world. there are many data initiatives and developments outside academia to which we should be paying more attention. expressive ontologies are not a scalable solution. they are a necessary solution in certain domains, but they do not solve the interoperability problem widely. they allow a scientist to build his or her silo better, and sometimes they even let the silo get a little wider, but they are not good at breaking down the silos unless the scientist can start linking ontologies, in which case he or she has to deal with the ontology interoperability issue, as well as the costs and difficulty of building them and their brittleness. i am known for the slogan ﬁa little semantics goes a long way.ﬂ i have said this so often that about 10 years ago people started calling it the hendler hypothesis at the defense advanced research projects agency (darpa). we are used to thinking that a major science problem is searching for information in papers, and we have forgotten that we also have to find and use the data underlying the science. a traditional scientific view of the world might be, ﬁi get my data, i do my work, and then i want to share it.ﬂ but the sharing should be part of how we do science, and other issues such as visualization, archiving, and integration should be in that life cycle too. i will talk about these issues, and then i will discuss the kinds of technologies that are starting to mature in this area. these technologies have not yet solved the problems of science and, in fact, largely have not been applied to the problems of science. scientists do use extremely large databases, and many of these data are crucial to society. on the other hand, we scientists tend to be fairly pejorative about something like facebook, because facebook is not being used to solve the problems of the world. i wish science could get the number of minutes per day that facebook gets, which is roughly equivalent to the entire workforce of a very large company for a year. we are also not used to thinking about facebook as confronting a data challenge per se, but it collects, according to the published figures, 25 terabytes of logged data per day. that sounds like the kind of numbers for large science data collections. facebook™s valuation is estimated to be well over $33 billion, which is the size of the entire national institutes of health budget. not surprisingly, they are able to deal with some of these data issues that are discussed here. we need to look at what they are doing and determine if we could take advantage of some of their approaches. i do not have similar numbers for google, because they have not been published. the last good estimate i could find was in 2008, which was 20 petabytes per day, but that was 3 years ago. that also does not include the exchange of or the storage of youtube data, which i cannot find good numbers about either. google™s valuation in 2011 is about $190 billion, the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.72 the future of scientific knowledge discovery in open networked environments  which is roughly a third of the department of defense budget. not the research budgetšthe entire budget. therefore, if we think that this kind of work is expensive, yes, it is, but there are other people doing it, and they are investing large amounts of money. it is not surprising that they have been focusing on big data problems in a way different from scientists, and they have been able to explore some areas that are very difficult for us to explore. if a researcher wanted to buy 10,000 computers for data mining purposes, it would difficult to do because of the lack of resources. several speakers talked about semantics in the context of annotation and finding related work in the research literature. it is an important problem, but i do not think it is the key unaddressed problem in science. in fact, i would contend that we have spent a huge amount of money on that problem, much of it on trying to reinvent things that already exist in a better form from open sources in the real world. most companies today that want to work with natural language processing start with opensource software. for example, there is a company that is taking everything from the twitter stream, running it through a number of natural language tools, and doing some categorization, visualization, and other similar work. they did not build any of their language processing software. i am also told that watson, the ibm jeopardy computer, had a large team of programmers, but, in fact, that the basic language technology used was mostly off the shelf, and that it was mostly statistical. semantic medline is a project that the department of health and human services has invested in. it does a fairly good job but does not understand the literature sufficiently to find exactly what we want. but we are not able to do that in any kind of literature, and google is working on that problem as well. hence, i do not see the point of yet another program to do that for yet another subfield or against yet another domain. instead, we need to start thinking about how to put these kinds of technologies together. the web is a hard technology to monitor and track because it moves very fast. it has been moving very fast for a long time, however, and, as scientists, we need to start taking advantage of it much more than reinventing it. there are a few different tools and models on the web that are worth thinking about. one is to move away from purely relational models; from assuming that the only way to share data is to have a single common model of the data. in other words, to put data in a database or to create a data warehouse, we need to have a model, and that is done for a particular kind of query efficiency. sometimes, however, that efficiency is not the most important factor. google had to move away from traditional databases to deal with the 20 petabytes of data generated every day. nobody has a data warehouse that does that, so google has been inventing many useful techniques. ﬁbig dataﬂ was one of their names for their file system. we cannot easily get the details of how google does it, but we can get published papers from google people that will be useful in learning how to do similar work. the nosql community is a fairly large and growing movement of people who are saying that when you are dealing with large volumes of data there is a need for something different from the traditional data warehouse. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 73  i had a discussion with some server experts who said, ﬁjust give us the schemas, and we can do this work.ﬂ i pointed out that we cannot always get the schemas, that some of these datasets were not even built using database technologies with schemas, and in some cases someone else owns the schema and will not share it. we can still obtain the data, however, either through an application programming interface (api) or through a datasharing web site. although facebook™s and google™s big data solutions are proprietary, the economics of applications, cell phones, and other new technologies are pushing toward much more interoperability and exchangeability for the data, which is mandating some sort of a semantics approach. consider, for example, the ﬁlikeﬂ buttons on facebook. facebook basically wants their tools and content to be everywhere, not just in facebook, which means it has to start thinking more about how to get those data, who will get that data, whether it wants data to be shared or not, and what formats and technologies to use. as a result, there are many technologies behind that ﬁlikeﬂ button. similarly, there are a number of approaches that google employs to find better meanings for their advertising technologies. what happens is that google recognizes that at one point it will not be able do the work by itself, because there is a longtailed distribution on the web. this means that it has to move the work to where webmasters and other people will be able to develop the semantic representations for their domains. that is happening with all the search engines, and the big issue now in that area is simple metadata and lightweight semantics. the notion of the complex ontology is getting replaced by the notion of fairly simple descriptive termsšthat is, i can probably describe the data in my dataset with 10 or 12 different terms that will be enough for me to put in a federated catalog for people to decide whether they want to read my data dictionary. the idea here is that if the data are going to be in a 200page, carefully constructed metadata format with the required fieldspecific types, and they have to be compliant with the standards of the internet engineering task force and iso (international standards organization), the vocabularies get harder to work with. it is an engineering problem that is very similar to the integration problem, and what many people are realizing is that you can arrange the data hierarchically and get good results for small investments. here is an example. many governments are putting raw data on the web now. they are not just putting visualizations of the data online; they are putting the datasets themselves. from a political point of view, the two biggest motivations are enhanced transparency and the chance to inspire innovation. promoting innovation can proceed in two different ways. first, the governments are hoping that some people will figure out how to make useful and innovative tools using the data. more important, especially for local governments, is that they pay many people to build web sites and interactive applications that they cannot afford to build anymore. for instance, a government agency may be able to pay one contractor to build a big system for a problem that is of high priority, but it may have 57 more priorities that it cannot afford to fund. the governments need someone to develop those applications, so if it makes the data available, at least some of those applications are done by other people. the development of such applications is out of the agency™s control, but if the work is getting done at no cost or for some small amount of money, it can start planning strategically for other priorities. the united states and the united kingdom are the two leading providers in terms of organizing their data. the united states has about 300,000 datasets, most of them geodata. if the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.74 the future of scientific knowledge discovery in open networked environments  we take out the geodata, there are probably 20,000 to 30,000 datasets that have meaningful raw data. the united kingdom has less, but it probably has the best in the sense that, for example, you can get the street crime data for the entire country for a number of years. they are releasing very highquality data down to a local level. many countries, including ours, are releasing scientifically interesting data, but you would have to work to find them. to use these data, combining them with other data, can be more important than just looking at them. those entities releasing data include countries, nongovernmental organizations, cities, and other groups. for example, one of the indian tribes in new york state has released much of its casino data. there are groups all over the world working with these data. my group is one, and the massachusetts institute of technology has a group working jointly with southampton university, mostly on u.k. data. many of these groups are in academia, but there are many nonacademic groups doing this kind of work. one of my suggestions is to think about data applications. you may have a large database, and if parts of it can be made available through an application, an interface, or through an api, people would be using the data in a sharablešand often an attributablešway. getting back to science issues such as attribution and credit, we have one of our government applications in the itunes store. i know exactly how many people downloaded it yesterday, and how many people are still using an old version. when some students and i were in china, we discovered that china was releasing much of its economic data, so we took china™s gross domestic product (gdp) data and the u.s. gdp data to do a comparison. to do that comparison, we needed to find the exchange rate. luckily, there is a dataset from the federal reserve that has all of the exchange rates for the u.s. dollar weekly for the past 50 years or so. we got those data, we mashed them together, and we got a chart that looks like figure 31:  figure 31 gdp chart. source: james hendler  the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 75  we also divided the gdp by the population (data we found in wikipedia), so that we could click a button to go between total gdp and per capita gdp. the model was built in less than 8 hours, including the conversion of the data, the web interface, and the visualization. that is a gamechanger. it would completely change the way we work if we could get this down to a few minutes. when my group started working with this kind of technology several years ago, it would take us weeks to do this kind of work. part of the improvement is because some of the technology was immature, part of it is because the tools are now available commercially, and another part is because there is now visualization technology freely available on the web. building visualizations is labor intensive, so by moving to simple visualizations, we can use visualizations much earlier. we are also able to link government data to social media. one interesting question that has not really been part of the discussion that we have had in the scientific community is how to find data. for example, we noticed that most of the u.s. government data were about the states, and the metadata would represent the data as being about the 50 states, but very few of the sets actually covered all the states. some states were missing. some databases included american samoa, guam, puerto rico, et cetera, and the district of columbia (which is not officially a state). in this case, there is a very loose definition of a state as opposed to a territory, and no one has much problem with that. but if a researcher wants to find datasets about alaska, it can be erroneous to just assume that all of the datasets that say they cover the states will actually have alaska data. the other problem is that we cannot search for the keyword ﬁalaskaﬂ within the datasets. if there is a column that represents the states, it may be called alaska, it may be called ak, it may be called state two, or it may be called s17:14b/x. the government has terabytes of data, so how do we find the data that are for alaska, for example? we cannot just call for building a data warehouse and rationalizing the process, because these datasets are being released by different people in different agencies in different ways. thus, metadata becomes very important. simple and easytocollect metadata can allow building faceted browsers and similar tools. it is a real research challenge, however, to determine the kind of metadata for real scientific data that are powerful enough to allow useful searches. with the government data, one problem is that all of the foreign databases are in their own languages, and it is hard for those of us who are english language speakers to figure out what is in the chinese dataset, unless you hire a chinese student. you cannot just use google translate and expect the result to be academically useful. people are starting to consider integrating text search and data search. this is an application that one of my staff did, joined with elsevier™s new sciverse, which was featured on the u.s. data.gov site (figure 32). what this application does is that when we are doing a keyword search for scientific data, it is also looking for datasets that might be related to that same term. we are using very lightweight ontologies that mostly just use the keyword. we are working on making it better. we think it would be good when someone is looking for papers, the data in or about that paper were available, but also finding what is in the world™s data repositories that might be useful.  the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.76 the future of scientific knowledge discovery in open networked environments  figure 32 search in sciverse hub on climate change. source: james hendler  when we start thinking about data integration, using data, or searching for data, one thing that happens in the discovery space is that the data become part of the hypothesis formation. that means that looking at, visualizing, and exchanging the data cannot be an expensive addon to a project. it has got to become a very key part of the standard scientific workflow, with appropriate tools to reduce the costs. what is promising in this area? we have been looking at linked open data issues outside of science, and some of its promise has been explored (mostly within the ontology area). genomics and astronomy are two fields where we have actually seen semantic web technologies deployed, but many other science communities are still mostly thinking about their own data holdings, not about being part of a much larger data universe. it is interesting that when we talk to the environmental protection agency, or to the national oceanic and atmospheric administration, or similar agencies, they say that they are providing data to many communities, so they cannot easily use the ontologies of a particular one. hence, we need to learn how to map between these approaches. how do i know what is in a large data store? how do i know what is in a virtual observatory? i need services, metadata, and apis. i also need to know the rules for using the data. other issues we need to think about are related to policies. if i take someone™s data from a paper, mix them with someone else™s data, and republish them, those people would probably like to get credit for the data being reused. or if i do some work on your data that you do not like, you might want to rectify it. how do we do that? the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 77  national technological needs and issues deborah crawford drexel university  i am currently the vice provost for research at drexel university, but i used to work at the national science foundation (nsf). i spent almost 20 years at nsf and was fairly significantly involved in the agency™s cyberinfrastructure activities. my experience within nsf and now at drexel has provided me with an interesting perspective on what dataintensive science means in a research university that aspires to be more research intensive. the main message of my talk is that there have been many advances in dataintensive science in some fields, but we have massive amounts of work still to do if dataintensive science is to realize its full potential across all of the disciplines. there have been many reports issued over the past decade that address the importance of dataintensive science and the role of information technology in advancing science. an important one is the atkins report that dr. hey referred to earlier. in that report, dan atkins of the university of michigan and his committee speak of revolutionizing the conduct of science and engineering research and education through cyberinfrastructure, and they examine democratizing science and engineering. those were tremendously powerful statements in 2003, and they stimulated a great deal of excitement within the scientific community. since then, there have been a number of reports that have specifically addressed dataintensive science, several of which were released in the past couple of years. this is a quote from a joint workshop between the nsf and the u.k. joint information systems committee that was held in 2007: ﬁthe widespread availability of digital content creates opportunities for new kinds of research and scholarship that are qualitatively different from traditional ways of using academic publications and research data.ﬂ what we have heard so far in this workshop was from those who i would describe as the visionaries and the trailblazers in science and engineering, representing those communities that were very motivated to take advantage of information technology in their work in order to advance their field of science. i want to focus now on the long tail of sciencešthe researchers in those fields where the immediate advantages of information technology and collaboration are not so readily apparent. there is tremendous opportunity in those communities, but we do not quite know how to realize those opportunities. i would like to provide a snapshot of some surveys of researchers working in different communities. the main message is that computermediated knowledge discovery practices vary widely among scientific communities and among colleges and universities. in the united states and in the united kingdom, for example, this is certainly true. there are some colleges and universities that know how to take advantage of their digital technologies and their digital capabilities, while there are others that simply are way behind the curve. there are three fundamental issues that communities or universities must address: (1) what kinds of data and information are made open, at what stage in the research process, and the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.78 the future of scientific knowledge discovery in open networked environments  how? (2) to which groups of people are the data and information made available and on what terms or conditions? (3) who develops and who has access to the tools and training to leverage the power of this discovery modality? these are fundamental questions and are key to the realization of dataintensive science. i am going to talk about two case studies, one conducted in the united kingdom by the digital curation center, and one conducted within yale university. both point to some key features we need to address. the study done by the digital curation center in the united kingdom was called ﬁopen to all.ﬂ its purpose was to understand how the principles of digitally enabled openness are translated into practice in a range of disciplines. these are the kinds of questions we need to be asking ourselves to determine the actions that we need to take to make sure that this modality of science is accessible and advantageous to everyone. in this study, the authors characterized a research lifecycle model and asked different communities how they were using digital technologies within the context of that life cycle to further their science. they surveyed groups among six communities: chemistry, astronomy, image bioinformatics, clinical neuroimaging, language technology, and epidemiology. the range of responses within those different communities was fascinating to see. surprisingly chemistry was the trailblazing community, at least among the individuals surveyed for this study. the chemists were using social networking tools, open notebook science, wikis, and all the modalities of digital technologies to collaborate, and to collect, analyze, and publish their data. everything was quite seamless from a community that i had not anticipated to be one of the trailblazing communities. it was interesting to hear from the clinical neuroimaging group. they were so skeptical of the value of dataintensive science and open data sharing that in this study they refused to even disclose their identities as individuals. we therefore went from one extreme to the other, and we saw the range of practices and values within different scientific communities. from their conversations with these communities, the group that conducted these case studies in the united kingdom came up with a list of the perceived benefits of open dataintensive science. it included improving the efficiency of research and not duplicating previous efforts, sharing findings, sharing best practices, and increasing the quality of research and scholarly rigor. this last one was especially true for the members of the chemistry community who were surveyed. they saw a great opportunity in making available in blogs the daytoday data that they collectedšnot just raw data, but derived data. they found tremendous benefits in making that open to the community to provide more scholarly rigor. among the other perceived benefits were enhancing the visibility of access to science, enabling researchers to ask new questions, and enhancing collaboration in communitybuilding, which all of the groups surveyed agreed was a benefit. there was also a perceived benefit of increasing the economic and social impact of research about which the report was ambiguous. although economic and social impacts each are treated separately as a perceived benefit, the sense was that the real value could not actually be measured. so, there was a question: can we create economic value by making our datašessentially our intellectual propertyšmuch more openly accessible? the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 79  one of the perceived impediments was the lack of evidence of benefits. researchers who felt this way were not motivated to make their data openly available. other impediments were the lack of clear incentives to change and the values in academe not being consistent with open data sharing. for tenure and promotion and the drive to publish, the perceptions were that the only way to publish is in the open literature and that it is not in a researcher™s interest to share data, because someone else may use the data to advance farther than the one who shared the data. competitiveness is a big issue. the conflict with the culture of independence and competition is absolutely related to the lack of clear incentives to change. other impediments were inadequate skills, a lack of time, and insufficient access to resources. another big concern was how to train both the scientists who are practicing today and the scientists and engineers of the future. researchers were also worried about how long it took them to prepare their data for open access, about quality, and about ethical, legal, and other restrictions to access. those were big issues, especially in the life sciences community. the report™s recommendations called for policies and practices for data managing and sharing. communities are desperate for guidance on these issues. what have the trailblazers learned that can be shared and applied more broadly? contributions to the research infrastructure should be valued morešthat is something we have heard often. training and professional development should be provided, and there should be an increased awareness of the potential of open business models. that is related to the attitude among researchers that their data are their intellectual property and they want to derive some value from that; thus, they feel that if they make their data openly available, they are giving that value away. assessment and quality assurance mechanisms should be developed. the study done by the yale university research data taskforce was conducted by an organization within yale called the office of digital assets and infrastructure, which is an organization established to accrue the value over time of the digital assets that result from research and scholarly activity within the institution. the office conducted this study to determine the requirements and components of a coherent technical infrastructure, to provide service definitions, and to recommend comprehensive policies to support the lifecycle management of research data at yale. given the discussion earlier about research libraries, this is interesting. here is yale university doing a survey that includes both the librarians and the information technology enterprise staff at the university to determine what their faculty base most needs for managing digital data. very much like the other survey, this one found that datasharing practices vary widely among the disciplines they surveyed. the researcher has the most at stake when determining what the datasharing practices are. yale is going to create an institutional repository to secure, store, and share large volumes of data. there are certainly some institutional pioneers in this area, such as the massachusetts institute of technology and indiana university, and much of what i characterize as ﬁslow followers.ﬂ a major concern is how an institution can afford to create and maintain infrastructure like this. yale university understands that it needs to develop and deliver research data curation services and tools to all of its interested parties, not only in science and engineering, but also in the humanities and in the arts. recognizing the importance of persistent access and the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.80 the future of scientific knowledge discovery in open networked environments  preservation, yale is trying to establish a digitalpreservation program to determine what they need to preserve data in the long term and what value will be accrued from it. data ownership was a big issue in the report. who actually owns data? most faculty believed they owned their data. it will be important to develop clear policies regarding data ownership, especially in those communities where data sharing is not a standard practice or is not valued. my main message, then, is that changes are in play at multiple scales: international, national, institutional, individual, and community. most of the effort at this time has been focused on the community level, because communities have, for the most part, driven the transition to open data sharing in communities where clear value accrues from that sharing. culture does not change because we desire it to change; it changes when our practices change. the bottom line is that all of this costs money. we will need to determine the appropriate balance between the cost of improved access to scientific opportunity and the benefits that accrue from it, and those are not easy questions to answer. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.future of scientific knowledge discovery in open networked environments 81  discussion discussant: is it going to get to the point where ibm™s watson knows the scientific literature and we do not ever need to do any work ourselves on the literature databases? dr. hendler: no. for jeopardy, watson took many fairly highend processors to beat ken jennings. for the medical version, they are trying to get that down to a more manageable numberšmaybe 20œ50. they have already announced that the first issue they are pursuing with watson is differential diagnosis, helping doctors access the medical literature. they designed the computer to be repurposable. this was the plan until 2009, but once they had the agreement that they were going to play ken jennings; they stopped doing anything except jeopardyrelated work. now that they have won the game, they are back to the earlier focus. my guess is it will be 5 or 10 years until that becomes widely deployable. dr. hey: i am interested in the financial implications of sharing literature and data. just as the music industry has to deal with what happens when someone makes a digital copy and distributes it for free, openly available data changes the business model. similarly, with publications, we do not need printing presses anymore; we have the web, and we can make digital copies and disseminate them. the big budgets that go to the publishers with their present business models perpetuate the system, but the system is being broken and it is up to universities and university libraries to respond. they should not be doing such deals with the publishers. they should be thinking about their role in the future. they should be thinking about how libraries can assist the research process of the institutions. i think the answer can be found in places like the ischools. it is up to places like drexel university and rensselaer polytechnic institute to move forward, but how do we get that to take place? how do we catalyze the major research libraries and major research institutions to act together? dr. crawford: there is nothing like a crisis to stimulate action. i know that at drexel we are having conversations about the future of our libraries. we have an ischool as well, so there is an opportunity there, because having shared interests in reinventing the model is essentially what it will take. there are trailblazer institutions, and it is a question of whether we can unite and define what the library of the future is in this digital world. but i absolutely agree that it is necessary. it is the future of the knowledge enterprise. i had a meeting with drexel™s president last week in which we talked about the ischool being the centerpiece of the twentyfirstcentury university and what that means in the context of research and scholarly activity. dr. hendler: i would also mention projects like vivo12, which is funded by the national science foundation. it has gotten a few schools involved, and it is growing. they have established the sharing of data and integrating that for the purpose of tracking researchers who agree to become a part of that system. each library can maintain its own holdings. if i have a list of all my papers that i can get to my university in a way that they can curate, share, and archive, then it does not necessarily mean anymore that it must be in a particular book, in a particular building. we are seeing a lot of pressure in that direction, and it is promising that several of these big libraries are helping to lead that area.  12 see: http://vivo.ufl.edu the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.82 the future of scientific knowledge discovery in open networked environments  i think the idea of a ﬁcatalog of catalogs,ﬂ the federated catalogs for not just the library holdings but also the people resources at a universityšthe individual papers and publications of the people, including who, what, when, where aspectsšis a very powerful and interesting idea. exactly how it will be funded remains to be seen, but once we get 100,000 or a million people accessing a page, the money can be found to maintain it. the problem is that until we reach that number, we cannot figure out how to monetize it. this is a situation in which scalable solutions that are experimented with and evolved may be better than trying to design a solution all in one step. dr. abelson: when scientists say, ﬁmy data is my intellectual property,ﬂ i am not sure under what legal regime someone™s data is their intellectual property. perhaps it is their trade secret or something similar. but there is tremendous confusion, and i think it is good to have some healthy criticism when people use that language. i would like to resist having people saying their data are their intellectual property. dr. crawford: it is a misguided perception, but there is a lot of that. dr. hendler: dr. abelson has been working on how we can make these policies more explicit through commonuse licenses and the data more sharable, to actually tie the policies in a computable way onto the data and assets. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 83  sociocultural, institutional, and organizational factors  session chair: michael lesk rutgers university the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 85 sociocultural dimensions  clifford lynch coalition for networked information sociocultural factors in this area cover a very wide range of topics. in preparing these remarks, i deliberately wanted to avoid some of the very common themes that i expected to hear in this workshop and to focus on a few additional important, but less widely discussed, issues. i will not thoroughly discuss, for example, the issue of incentives, which is obviously a major problem. the one point i will make in this regard, however, is that we have not talked much about the incentive to receive research funding in the first place, which is a fairly compelling one. if certain datasharing behaviors are a condition of receiving research funds, and adherence to these behaviors is enforced as a condition of continuing eligibility for funding, this would be a real incentive. it is interesting to see what is happening in some of the communities with foundations dedicated to the cure of specific, often rare, diseases. these foundations tend to be very focused on what they are doing, which is searching for treatments and cures. it is now common for them to insist that the scientists they fund should make their data openly available at least to other researchers within that community, on an immediate basis. this is the phenomenon of a funder who is focused on solving a problem and is not very interested in accommodating much longterm careerbuilding by the participants. we will see more of this focus. let me move to the main issues i want to address. first, i want to talk about the scholarly literature and the realities of mining that literature, both as a research project in and of itself (i.e., to study what is going on in science) and as a pragmatic means of, for example, finding relevant earlier work. there is a longstanding history of researchers trying to extract hypotheses, insights, or facts by computational analysis of large bodies of scholarly literature, sometimes in conjunction with ancillary datasets of various kinds and sometimes not. what has happened with the way people compute on the web is that we are coming to think about the scholarly record as an object of computation and to see that this computation can facilitate the discovery of new knowledge. it is important to understand that while the potential of this approach is great, the pragmatics of realizing it are phenomenally challenging. it is clear that very large amounts of the scholarly literature are in one or another electronic format, some formats being far more hospitable to computation than others. they are often available in multiple formats, with the most computationally friendly ones only available internally on the publishers™ sites, since they are not designed to be directly read by human readers. but the licenses that universities typically enter into with publishers to use collections of online journals specifically forbid mass copying of the material from those journal collections, as someone would need to do to build up a corpus to compute upon. in fact, if a researcher tries to download a large number of journal articles from many of these databases, the librarian at that university will get a call from one of the publishers, because they have got massdownload detectors that notice a big spike of downloads. the publisher will demand that the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.86 the future of scientific knowledge discovery in open networked environments  the university librarian do something about this or it will shut off the university until it fixes this issue, at which point the researcher will get a phone call acquainting him or her with the terms and conditions under which the university has access. fixing this issue is not simply a matter of renegotiating one contract. a large university will have hundreds of such contracts, and thus far, publishers have not been very accommodating of this, with a few exceptions, mostly openaccess publishers that do not require license agreements anyway. of course, there are legitimate publisher concerns that need to be recognized in the renegotiations, but they are not simple to address. in short, there is a substantial pragmatic barrier to getting literature to mine. note also that any comprehensive subject corpus, most of the time, needs to be drawn from a range of publishers, not just one. even if someone can get past the license issues, a second very real challenge to literature mining is the need to normalize data from a wide range of sources into a consistent format. one of the previous presenters talked about what it took to make sense out of one gene array dataset and the cleanup that was needed for that process. imagine trying to deal with 50 or 60 different data sources, some of which vary depending on whether the journal issue is from 2003 or 1992, or is something from the 1960s that had to be converted into digital form. this is a very significant amount of work, particularly if it has to be done over and over again by different literaturemining groups. a final subissue on barriers to mining the current scholarly literature base is related to copyright issues. consider, for example, what happens if someone is performing a computational process on tens of thousands of copyrighted objects. is the output a derivative work? if so, producing a derivative work from thousands of copyrights raises all sorts of questions. for example, if we did the same computation, but omitted 10 articles, does the fact that we included the 10 articles in our original computation make the computation a derivative of those 10 as well as of the other 9,990, even though they made no difference in the final product? there is not much legal certainty about anything in this area, as far as i know. let me address some practical barriers to mining the existing literature base. today there is little consensus about the role of publishers in facilitating literature mining. we could imagine scenarios in which publishers, or some third party working with publishers, offer literature mining as a service for a database created just for that purpose. this might reduce some barriers, but it also raises concerns about the economics of text mining and the flexibility of the mining technology that it would be possible to apply to the literature base. focusing on the idea of computing on a corpus of scholarly literature and a body of experimental results, we also need to think about how to change the character and composition of this corpus to make it as valuable as possible for this kind of computational analysis. for example, it has often been observed that the published record, especially when it was read solely by human beings, has a strong bias toward positive results. people rarely get articles published in highvisibility venues saying, ﬁi tried these six approaches, and none of them worked,ﬂ or ﬁthis compound seems to be good for nothing, it does not react with anything.ﬂ these are valuable contributions in a computational environment, and we need to think carefully, especially in a world full of robotic experimental equipment and largescale screening systems, about how we get more of this negative information out there so that we can compute on it. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 87 a closely related issue is the tremendous amount of data that industry produces and represents negative results or at least ﬁprepositive results.ﬂ we should be able to disseminate and share these results. we need to sort through the notion of value in data, particularly (but far from exclusively) as it affects industry. there is a school of thought that a database has value even if we invested in it and it proved to be a worthless investment, because if we keep it secret, we can force your competitors to waste time and money recreating that database themselves. thus, it will consume resources that they could have spent elsewhere, and that will help us in competition. we need to reject that thinking, especially in environments such as the pharmaceutical arena, where we are struggling to contain costs and improve productive research. there is an enormous social cost to secrecy for negative advantage rather than for positive advantage. i do not know how we fix it other than by talking openly about it, but it does seem that we should look for ways to make as much negative data available as possible, in addition to positive data, to facilitate computation on the scholarly record. one of the other challenges we face is how to make it easy for scientists and scholars to share data. we heard in some of the earlier presentations how difficult that can be. one key is simply giving scientists places to put materials that can be openly available to the world or that can be shared on a rather controlled basis. anyone who has tried to do interinstitutional data sharing without making them public to the worldšto share data just with groups of collaboratorsšknows how horrible that can be, because you do not have a common federated identitymanagement system across those institutions, so you have to resort to all kinds of clumsy ad hoc solutions. one of the contributions that supporting organizations, such as information technology and libraries, can make is to help supply infrastructure that supports simple sharing. i see institutional repositories as keyšbut far from exclusivešplayers in that role. we see this from the disciplinerelated repositories that exist for specific genres of data. those make sharing very easy, but we need to look for more mechanisms to facilitate that. the last area i want to talk about is privacy. we face a big burden dealing with personally identifiable data of various kinds, the need in university settings to get involved with institutional review boards (irbs), and the specific challenges of sharing and reusing data, which seems philosophically opposed to what most irbs want to achieve. for personally identifiable data, they want to be as constraining and specific as possible. we need to recognize that we are getting into situations where we need to be able to reuse data more fluidly, and we are going to need some serious conversations about how this interacts with policies about privacy and informed consent. there are several possible approaches in this regard. in many cases we want to anonymize the information, and we need to learn how to do a better job of that technically and to look for easy ways to share anonymized data with an agreement that the recipient will not attempt to deanonymize it as a second line of defense. in other cases, personally identifiable information may be needed, particularly as we look at the interfaces between personal histories of various kinds and genomics or other kinds of population studies. i wonder whether there are not some new ways to think about this issue. there is a tremendous interest in citizen science, as was mentioned earlier. there are many people who are interested in documenting their own behaviors, their own physical health. i was at a the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.88 the future of scientific knowledge discovery in open networked environments  meeting recently looking at developments in personal archiving, and it is clear there is a growing interest in collecting and sharing this kind of information among a sector of the population. we also have some very interesting developments in personal genotyping, for example, where communities of people are sharing the results that they get back from companies like 23andme that do this sort of work. i wonder if there is some way to connect these kinds of social changes into the process, or at least to think about whether we can do something better than the very constrained kinds of informed consent approaches that are in use today. we are significantly limited right now in doing the kinds of knowledge discovery that cross different kinds of databases, ranging from biological databases to databases about people and about behaviors and populations. i would suggest that is a major cultural and social issue that we need to be exploring in detail. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 89 institutional factors paul edwards university of michigan the focus of my talk is on institutions and open data. i want to start with a success story. the meteorological data infrastructure is one of the oldest global infrastructures. it goes back to the 1850s. in the interwar period, a rudimentary, global datasharing system emerged that used shortwave radio, telegraph, and many other technologies. at that time, meteorologists threw away almost all the data they received, because they had no way to use it. once they got computers, they became able to process that data. since the 1960s, the combination of a global observing system with many kinds of instruments linked to a global telecommunications system has been used. the system pours data from all over the world into computers that make forecasts and data products and then delivers those to national meteorological services, and it works well. however, we do have a problem. figure 33 is a graph of the famous global temperature curve as collected by different investigators since the nineteenth century. figure 33 global temperature time series. source: intergovernmental panel on climate change  the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.90 the future of scientific knowledge discovery in open networked environments  there are enormous differences in the temperatures recorded by the datasets from the nineteenth century and the early part of the twentieth century, especially before 1940. this is not much data by today™s standards, but the ability to crunch these numbers was not there before computers. before 1963, almost all of these datasets included fewer than 450 stations. they did this because these were the stations known to be highly reliable. they were the stations that had been there over the entire period, and which had not made many changes to their locations, instrumentation, and so on. so their data were considered reliable. the later temperature curves, put together in 1986, 2006, and more recently, use computer models to add in data from stations that were not trustworthy enough earlieršeither because their records were incomplete or because they had gone through some changes, such as having been moved or engulfed by an urban environment, which made their record suspect. a recent project at the university of california, berkeley is trawling the web to find station records and add them into a global dataset containing more than 30,000 stations. the quality of that dataset is open to question, but it is one of these websourcing ideas that we have heard a lot about today in other presentations. this story offers an example of an extremely open data system that has existed for more than 150 years. one of the reasons the weather data system works so well is that in the early days of weather data reporting, data were freely exchanged, because they had no economic value. we could not do anything with them, because it was difficult to make much use of them in many ways, so the international telegraph system began to carry weather data at no cost. since sharing these data was advantageous to everyone, they did it freely. in the 1990s, that started to change. some countries are charging for weather data products in an effort to recoup the costs of their observing systems. figure 34 shows datasets that the european center for medium range weather forecasts is trying to recuperate now, in order to add them into the global climate record and improve the quality of that record. there are many of these datasets.  figure 34 focus on pre1957 meteorological data in sensitive regions. source: european centre for mediumrange weather forecasts the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 91 the point of this story is partly that the effort to build this digital climate data record has already been going on for almost 30 years, and it is still under way. this is difficult work, even though the relative size of these datasets is small compared to the size of some of the databases we have now. the results of the global collection effort are good, however. figure 35 presents four independent analyses of the global temperature record. they converge much betterševen in the nineteenth centuryšthan the versions created earlier. figure 35 ipcc ar4 (2007). source: intergovernmental panel on climate change  i want to focus now on data problems. some of these are institutional and some of them are not. most of us still work at a local workplace, and almost all of us work for some kind of hierarchical organization that pays our salary. this means that the issues that concern us locally about the institution that we work for will almost always trump concerns about things that are remote. this is true for various psychological and institutional reasons. studies of facetoface interaction show, for instance, that it generally works better and faster than interaction at a distance. there is also a trust aspect. the people you know are the people you are more likely to share with, and whose data you are more likely to trust. the story about meteorology and climate science has some temporal issues that have not come up much in this discussion, but are nonetheless very important. it is easy to think that the data we are trying to federate and collect will be there forever, but of course they will not. the audience may be familiar with the story when mars data were lost, because the software with which data tapes were made had been lost. digital information is quite fragile, because the strings of bits cannot be interpreted without the code that created themšand that code is dependent on other code, such as operating systems. losing data by losing the software and hardware context is the kind of problem that could happen again and that already does happen quite often. metadata is another issue that appears in the climate and weather data story. it is not obvious to us what people in the future will need to know about the data we collect nowšand the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.92 the future of scientific knowledge discovery in open networked environments  we cannot know that with any certainty. we may not collect what they will need at all, or what we collect may not be sufficient. finally, there is a figurative dimension to distance, which is the distance between disciplines. the great promise of open data is that we can work on problems that have never been studied before, because they required collaborations among different fields. that is also a potential failing, however, because if an ecologist wants to work with data from, for example, an atmospheric physicist, the ecologist might be willing to accept that person™s data at face value. after all, what does the ecologist know about atmospheric physics? yet those data may be defective in various ways. we may need to make an extra effort to know more about those data and the problems they may have. this raises the problem of trust. the further away that some user is from the source of the data, the more possible it is for the members of that user community to find it completely unpersuasive. hence, sharing data will not automatically facilitate communication and understanding. the world we think we are in, and want to be in, is now thoroughly networked, with easy sharing of data and information among all scientists and disciplines. but the world that we are actually in is often much more hierarchical and stovepiped. in science, the hierarchy begins with the subdisciplines, rises to the disciplines, and rises again to larger traditions of scientific practice (figures 36 and 37). studying realworld problems, such as climate change, involves many disciplines, but the place where your scientific reputation is made is at the much lower level of your subdiscipline. figures 36 and 37 hierarchies and incentives. source: paul edwards  the same observation is true for our institutions. academic scientists are individual, they work in a department, and the department is part of a university. there might be levels above that all the way up to international professional societiesšbut the place that pays the salary is the university, and the place where the scientists™ reputation is evaluated is their department. therefore, these relatively local entities can often trump larger collaborations, networks, virtual organizations, and so on. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 93 one of the phenomena we see in the formation of many infrastructures is a phase in which we build gatewaysšsystems that allow us to move information, for example, from one place to another without having to worry about the particular features of every infrastructure involved in the transfer. the meteorological information infrastructure, the world weather watch of the u.n. world meteorological organization, is a great example. its bottom level consists of more than 150 national meteorological services. each one contributes its observational data to the global observing system. the global communication system sends them to central processing centers, where data from the entire planet are processed and then returned to the national weather services. this is great for understanding and forecasting the weather, but once we start confronting larger problems, such as climate (which involves many earth systems besides the atmosphere), the world weather watch is no longer enough. therefore, we get another, higher level of institution. for example, the global earth observing system is a collection of systems, and each of those systems is, in fact, a collection of other systems. what i see when i look at the world of scientific institutions is a constant need to climb up a level to try to get an understanding of the general area that is being studied. i have done some work with people in the global organization of earth system science portals (goessp). these are people who build tools like the earth system grid. they are concerned about adequate support and coherence. the problem they face is that their activities for goessp are, in general, unfunded. they feel strongly that it is necessary, because they want to do things that link the various portals and make them into a more coherent, interoperating systemšbut nobody is paying them to do that. thus, again, the institutional level of concern and the one at which they actually want to work are different and conflicting. my late colleague, susan leigh star, coined this phrase: ﬁmetadata are nobody™s job.ﬂ everybody wants good metadata, but nobody knows who is supposed to create them. we would like to push it off onto the scientists, but they do not want to do it. they are moving on to the next project after they have finished their datasets, and they do not want to go back and document what they have done. perhaps there is a class of data managers who can handle it, or perhaps it should be the ﬁcrowd.ﬂ maybe somehow younger scientists will do it. what we often see in labs is that this work is pushed down onto the graduate students. maybe social scientists have the magic answer. i am a social scientist, and, as far as i know, we do not. my research group has been doing some ethnographic work with environmental scientists, and we have observed that there is an obsession with metadata as a product. in other words, metadata are conceived as a kind of library catalog that will describe all the data that someone has. the reality of data sharing, however, is quite often that metadata exist mainly as an ephemeral process. many people say that the most important issue when dealing with data collected by someone else is to talk to that person. ﬁi need to know more about what you did, how you collected this data, what it means, what the formats areﬂšthere is an endless list of questions, and it is impossible that all of those will ever be answered by a catalogstyle metadata product. one lesson, therefore, is that when someone is conceiving metadata, creating channels for communication among researchers is at least as important as creating the ultimate catalog that we would all like to have. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.94 the future of scientific knowledge discovery in open networked environments  finally, i am going to talk about data and software. data cannot be read without software. an even more important point is that many things that we call data now are actually the product of software. climate model output is an example, as are instrument data after they have been collected and processed through some sort of data model to be put into the final dataset that is used in a general publication. which of those things are we talking about? if it is the latter, we need to know exactly how it was processed. for the last 15 years or so, the journal money, credit, and banking has required authors to submit both their code and their data so that their results, in principle, could be replicated. in 2006, researchers found that of 150 articles only 58 actually had some data or a code, and of those, for only 15 could the results be replicated with the code and data the authors provided. the journal editors concluded that there needs to be a stricter system for people who are submitting their code and data. but my question is, how much is it worth? what they did not do was attempt to communicate with the authors to find the missing information. my last points are related to the problem of the academic reputation system. this is the way the system works now: scientists have some data sources, they use them to do research, they publish the results, get citations, and that builds their reputations. somewhere in there are services, and that is where data sharing and software, as well as the writing of scientific software, both sit. my colleague, james howison at carnegie mellon university, has been thinking about how we might design a reputation system that would work for scientific software writing. many scientists write little bits of code, or sometimes much more elaborate code. that is part of their job, but it is not their main job. yet once they have written the software, they get involved in the problem that if their code is going to stay active, they have to maintain it. there is an ecology of software, and as the operating systems and associated software libraries change, this piece of scientific software will eventually stop working. therefore, the scientist has to keep updating it, to keep modernizing it, if people are going to keep using it. sometimes this turns into a sideline, and the scientist actually becomes a software developer. more often, however, what happens is that the code eventually just dies, and then there is the problem of replication. how can the research that used it be replicated? i collect a lot of data, and then what happens to it? sharing the data requires work from me. writing out all the metadata is work. one of the best examples we have of a wellfunctioning collection of large datasets is the collection for cmip5, the coupled model intercomparison project, that my colleague from unidata spoke about earlier. it is working well because there is a major incentive, which is that if someone wants to have data in the intergovernmental panel on climate change (ipcc) report, that is the only way to do it. the scientist must fill out the metadata questionnaire. how long does filling out the metadata questionnaire take? up to 30 days for each model, because there are many runs and the questionnaire has to be filled out separately for each run. the point is that data sharing is work, and it always will be work. thus, one of our big issues is to find ways to pay people for that and then also to include it in reputation systems so that they can get credit for it. there is a conflict between the mandates of institutions like the ipcc, the national science foundation, and the national institutes of health that want us to publish and share data, but there are few career incentives for sharing data. the career incentives are for the the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 95 results that come from the data. that is true of building software. it is also true of sharing data. moreover, the problems that software developers face are also there for people who want to share data, though perhaps to a lesser degree. data have to be maintained, which is why the issue of curation is a critical one. who will maintain the data? there are many possible solutions, however, and some of them will work for some areas. it is important to realize that there is no one thing called data, and that the differences among scientific fields are enormous. what works in one area may not work in another. maybe we are talking about a virtual organization, or maybe it is a crowdsourcing project. among the people i have been working with, we have often heard wikipedia, linux, mozilla, and apache held up as models for the future: ﬁthis is what we will do. opensource software works. openaccess models work.ﬂ i do not think so. for many fields, there is going to be a problem, because there are not enough people with the right skills. metadata standards may help, but someone needs carrots or sticks to get that process to happen. i have heard many people say that the young people will do it. they assume the young people will know how to do it, but if we do not know how to do it, why is it that they will know? they need to be trained. studies of young people using even simple things like google search show that most of them do not know how to use google search in an effective and efficient way, and that is equally true of young scientists. some of them will be very good, but most of them will not, and it might be that we actually need new paradigms of education. i love the idea of science informatics. perhaps it will come from an ischool, but the ischools we have are not set up for this yet. perhaps it will come from science departments, but most of the science departments i know are not set up to do that either. science informatics is not computer science, and most computer science departments are not interested in this problem at all except for their own kinds of models and data. therefore, it will probably be some combination of computer science, ischools, and domain science. and the combination may need to be different for different domains and problems. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.96 the future of scientific knowledge discovery in open networked environments  discussion discussant: there was a discussion about how difficult it is to get large amounts of text for literature analysis, but people have been trying to do that since the 1970s, long before copyrights became a big issue, and there are large bodies of text that are not available. do you think the technology works even if someone could get the text, because i do not see that? dr. lynch: parts of the technology work, and in fact there are some successes to which we can point. the notion that someone has a computer that reads the literature on a certain subject and understands it in a deep way is fairly farfetched. one of the things that google has suggested, however, is that when someone gets enough data, he or she can do some interesting work through statistical correlations. discussant: there is a general problem about where the credit lies, where the value lies, and who is responsible. but the main question is: what is the business model? who is responsible among universities for ensuring that there is a cyberinfrastructure and that appropriate credit is given? dr. lynch: i do not think there are simple answers. i do suspect that even in a period where resources are very tight, if we look carefully at most of the cyberinfrastructure investment, it is largely cost effective. this investment should be making the overall research enterprise more effective and providing some real leverage. i think those are the parts that will probably move ahead. one of the issues in the area of data that we need to be very mindful of is that data preservation per se is almost a pure overhead activity, especially when we get past the first couple of years when people might want to reproduce new results. the activity with the payoff is reuse. that, not preservation, leads to new discovery, so it is important to be focused on facilitating reuse and not just looking for the longest possible list of data that we need to preserve. there are clearly materials we should preserve even if there are not obvious reuses for them right now, but we need to be judicious in that category and compete for the resources. discussant: it used to be that the reason an institution would have a repository was because there were physical objects that made sense to keep locally at an institution. individuals did not have a chance to use an institutional repository unless they were at an institution. consider, for example, a person™s personal photography collection. perhaps this person is in a bizarre hybrid situation, where he or she tries different services and ends up with a few hard drives, where many copies of this material are saved. there should be a different solution, however, where that person could take advantage of a big institution. longterm trust is the main problem. he or she does not want those photographs to be lost. what if those companies go out of business? that person would still want everything on a personal hard drive. if we move from digital photographs to scholarly output, the issues are the same. why is harvard university, for example, going to have a repository for everybody™s information, when it could just have some enterprise solution that gave everything to microsoft? what is the balance between an institution and the individual? dr. lynch: i am not sure that i parse the landscape the same way you do. i would say that the very critical issue about an institutional repository is that an institution is taking the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 97 responsibility for longterm preservation. institutions and corporations, unlike people, may live forever, or at least indefinitely, and have processes that allow them to operate on much longer timescales than individuals. discussant: if a university is paying money to another institution, is that still an institutional repository? dr. lynch: yes. the way i view this kind of arrangement is that if the institution identifies the data and takes responsibility for their preservation, then it is making a much longer commitment than an individual could do, perhaps setting up a trust fund or some other mechanism that outlives the individual. operationally, we may see many universities over time contracting the preservation and curation of specific kinds of data to other universities, or to other forprofit or notforprofit archives. we may also see the emergence of more consortia, as well as straightout contracting. the locus of ultimate responsibility is the key thing here. discussant: does that mean that the universities can act like the individual researcher? dr. lynch: except that presumably the universities have thought through issues like multisourcing and what happens when a supplier goes broke, how to back up the data, how to audit contractors and move data from one contractor to the next, and other similar issues. the other point i want to make is that we often confuse distribution systems with preservations systems. flickr is a great example. people put their pictures up on flickr, and i think of that as a means of distributing them, making them available, and building some social interaction around them. however, there are people who believe this is a preservation venue, and i would urge them not to think that. if you really want your pictures, by all means put them up on flickr or anywhere else, but keep a set for yourself offsite and make lots of copies. dr. lesk: in terms of how we get somebody to do the preservation and the data curation, assuming that scientific data is indeed worth havingši wish we had more data on that pointšthe data are either a public good or a private good. if they are a private good, it means that the owner gets credit for having them, presumably by getting tenure. however, that means waiting for universities to give tenure for collecting data instead of for research results or for the papers interpreting the data. coming from a university where departments complain that the administration is too slow, i am not optimistic about that. the other option is that the data are a public good. dr. berman made a comment earlier about unfunded mandates. i have the feeling that if the national science foundation (nsf) imposed a data management mandatešwe are not going to get any more pages in the proposal, but we are going to get 5 percent more money as the upper limit of what we can ask forši think the world would be completely different. obviously there would have to be a requirement that the 5 percent be spent on the data curation, but it would completely change the attitude about how the research community does this. dr. spengler: it is important to point out that the nsf asks for a data management plan. it does not ask a scientist to preserve the data. it does not ask that there always be copies. even more important, if we read all the details, we do say that we are willing to have the scientist put requests in the proposed budget for curation purposes. we say that we are willing to support it. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.98 the future of scientific knowledge discovery in open networked environments  discussant: if scientists decide that their data are worth preserving, is there some sort of mechanism for them to appeal that? dr. spengler: some of my colleagues and some of the directorates had planned to include in the nsf data management policy that a grantee has to keep the data for 5 years beyond the award period. that got taken out. discussant: is there a current plan for that? dr. spengler: no. some disciplines have mechanisms set up for that, however. for example, if we look at some of the programs within the nsf, we will see that they do not normally say that the nsf wants data management plans. however, researchers will get in trouble if they do not have the data deposited in the designated database when it is time for their annual report or project renewal, because it will not be approved. hence, some places have thought long term about it, but it requires a very large commitment by a given domain. the social sciences have stepped up in their resources, as has astronomy. many disciplines and organizations have made more of a commitment, but questions about the longterm cost are still open. dr. lynch: there are some interesting developments at the institutional level as well. for example, earlier this year princeton university offered a storage service for their researchers that allows them to pay in advance for storage at some rate per terabyte, allegedly for eternity, or as long as princeton will exist. the university made some projections about the cost of technology, the refresh rate, and the return on capital for the prefunding. we can argue about whether the numbers are right, but it is interesting to see someone step forward with a model like that, and i think we will probably see some more of it.  the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 99 policy and legal factors session chair: michael carroll washington college of law, american university there was a comment made earlier about how researchers think of themselves as huntergatherers. this goes to the philosopher john locke™s labor theory: i did it, and therefore i own it, and i have a property right in it. lawyers think of property in different ways, but i want to highlight the fact that there is a ﬁthereﬂ there, even though the words ﬁintellectual propertyﬂ are being imprecisely used in this context. the way that an intellectual property lawyer might think about ownership and rights over the data is in terms of the rights over the first copy of the zeroes and ones. in a way, this is almost like tangible property in the sense that when there is only one copy, these zeroes and ones might be owned insofar as i have the medium on which these zeroes and ones sit, and you cannot have it unless i say so. this is a claim of ownership over a form of property. intellectual property rights are a layer above the zeroes and ones. they are entirely intangible, and these are rights that tell us what uses are acceptable. may we access the zeroes and ones? may we copy them? may we change the order of the zeroes and ones? these are use regulations. think about intellectual property law as access and use regulations that apply to the zeroes and ones rather than being embedded directly in the zeroes and ones. once we make this conceptual separation, when we think about the policy issues, we are thinking about the regulations over data acquisition, data reuse, and the like. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 101 legal aspects madison university of pittsburgh school of law  i want to highlight some central legal questions in this area. in particular, i am going to focus on intellectual property (ip) law questions. there are some interesting and important privacy law issues as well, but i am not going to discuss those. figure 38 highlights in a very simplified, stylized way how i organize my thoughts on this topic.  figure 38 intellectual property challenges to data access. source: michael madison  there are four clouds of different datasets, and a researcher in the middle who is extractingšor recombining or usingšdata from each of these sources. there is a different kind of a labelšor, in one case at least, no label whatsoeveršattached to each of those datasets, specifying terms and conditions under which data from that source might be used, recombined, shared, and so forth. the question is, what does the person in the middle do, and how does the legal system frame that set of questions? in the upper left corner, there is no label whatsoever, just a pot of data. the lower left is ﬁall rights reserved,ﬂ or the standard copyrightstyle notice that would be attached to any kind of copyrighted work and that is often attached to materials that are not actually copyrighted, often out of ignorance or sometimes in an attempt to deliberately claim rights in things that cannot be claimed. it is a default copyright label. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.102 the future of scientific knowledge discovery in open networked environments  on the lower right side is a contextspecific restriction that is phrased ﬁdata mining not allowed.ﬂ this is to illustrate the idea that people who collect data or create copyrighted works increasingly have the power to customdesign labels or notices that they attach to their works before they send their works out into the world, and they expect both the world to respect that label and the legal system to enforce it somehow. in the upper right corner is a label of a type that is increasingly common, used to specify or declare that the material inside the pot is in the public domain or, if it was not in the public domain in the first place but in fact was copyrighted to begin with, has now been dedicated to the public domain by some kind of affirmative notice or label. it is the cc0 label, which is a form of creative commons license that basically declares that the material covered by the license was copyrighted, but the author of the material, or the owner of the copyrighted material, is affirmatively dedicating that content to the public domain. one of the questions i will address is the extent to which that kind of labeling or notice device is actually effective. there are three general categories i want to cover, with questions in each. one is the default legal status of data in these environments, both with respect to the underlying data and also with respect to the adjacent information, formats, coding, metadata, and literature that arise from these things. the second one pertains to contracts, licenses, notices, restrictive notices, and enabling notices. the third category is what the legal system has to say about the problem of managing a commons or managing a community of shared resources. i will begin with some default copyright rules within the intellectual property systemšjust what the rules are. first, data are treated in copyright law as facts and therefore cannot be copyrighted. they are by design in the public domain. this is one of the few blackletter legal rules that is easy to state. this would include experimental results, observations, sensor readings, and the like. so for people who assert that they own their data, if we were to apply a formal legal model to this, they cannot own the individual items in the dataset. they might, however, be able to claim a copyright in the compilation of the data or the database as a whole. regarding collections of data and databases, copyright law gives us this phrase ﬁselection, coordination, or arrangement.ﬂ if the selection, coordination, or arrangement of the data, or if the selection, coordination, or arrangement of practically anything, is originalšthat is, it has been touched by human judgment or creativity in some modest wayšthen there is at least the spark of a copyright that attaches to that collection. not to the individual items, but to the collection. if it is a collection of facts or a dataset that has been compiled in a way that has been organized by human judgment, then copyright may well apply, although the scope of the copyright may be relatively narrow. copyright comes in different widths. there is narrower copyright for factoriented materials and broader copyright for classically entertainmentoriented things such as novels, films, and popular music. when i say a ﬁnarrowﬂ copyright, or what copyright lawyers would call a ﬁthinﬂ copyright, i mean that infringement of that kind of copyright will generally consist of verbatim or wholesale copying of the contents of the work. copyright law also has this idea of substantially similar copying that can be infringement, but it generally would not apply to a thin copyright in a compilation of facts. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 103 codes, formats, metadata, data structures, or anything that involves any kind of interpretation or characterization of the data up to and including actual scholarly papers or scholarly output is going to be eligible for copyright protection. the more human judgment, human creativity, or human originality that goes into the production of that material, the stronger the copyright can be. thus, a scientist™s scholarly papers, output, and the literature itself will be copyrightprotected in a much more robust way than the underlying datasets. in short, there is a spectrum of copyright protection that starts at a relatively early stage, immediately after the creation or identification of the individual data point. i will note two other points about default rules. first, if we are talking about a source in the european union (e.u.), the european union has the 1996 database directive, which operates in europe in parallel with copyright rights. the database directive has some overlap with copyright and some distinct character. the most important feature of the database directive is that it prohibits extraction and reuse of a substantial part of the database. therefore, picking out bits and pieces of an e.u. database may not violate the database directive, but if the directive applies, then using the database wholesale may violate the database directive (in the manner that the directive has been incorporated into national law) as well as local european copyright law, and perhaps other laws. copyright law in the united states is subject to fair use. fair use refers to the use of particular portions of copyrighted works for critical purposes: teaching, research, scholarship, study, and so forth. fair use in this area is not an especially reliable or useful tool, although it is potentially quite important in several other respects. one possible application of fair use is to defend the copying of a copyrighted work at an intermediate stage on the way to producing something else. for example, u.s. case law in one instance focuses on intermediate copying for reverse engineering an underlying software product. if someone wants to build an interoperable or complementary software productšthe case law generally deals with video gamesšthen it may be permissible to make an intermediate copy of a copyrighted work to extract underlying public domain ideas and then use those public domain ideas to build a complementary work. a related way in which fair use might help would be related to building and using tools in the database area. if someone wants to be able to connect to an application programming interface to make use of a particular tool, fair use might be a way, using this case law, to exploit that particular possibility. second, turning to licenses and contracts, it is important to note that all the default rules, with some exceptions, can be changed by voluntary agreement, by a license or a contract, or a license bundled into a contract. there is some controversy about the extent to which fair use can be negotiated away in a license or a contract. the better view is that fair use is a mandatory legal requirement, but there is some dialogue about that in the world of law practice. the most important point about contracts and licenses is that there is a distinction between a unilateral notice, or a license as a concept of property law, and a contract, which is something that you would voluntarily assent to. in practice, both in the software world generally and in this space of datasets, there is much ambiguity and confusion about the legal status of particular terms and conditions that come bundled with these collections of data. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.104 the future of scientific knowledge discovery in open networked environments  a contract is something that typically requires that someone agrees to it, for example, by clicking on the ﬁi agreeﬂ or ﬁi assentﬂ or ﬁi acknowledgeﬂ button. a license is a property term that grants a person permission to do things and that might require him or her to do some things with the copyrighted material, but a license does not necessarily require that the person agree to something in return. that is the first area that makes licenses tricky. the second area that makes licenses unclear, besides the fact that a person is not necessarily required to agree to them, is that someone might not necessarily see them. they might come in machinereadable form, but a researcher might not have a humanreadable equivalent. or a researcher might have a humanreadable version of a license, but might not have a machinereadable or automated discoveryœreadable version of the license. therefore, it might not be apparent enough for a researcher actually to understand that there are some terms and conditions to deal with there. a third complication is that particular terms and conditions, as illustrated on figure 38, might be different, incompatible, or at least not perfectly aligned from one dataset to another. thus, if a scientist is doing a largescale project that entails extracting data or using data from many different sources, that scientist might end up with multiple legal obligations and rights for different chunks of the data. that has been a big problem. turning to opensource software, much of the discussion about such software focuses on the general public license as the standard license for opensource software products. there are many licenses, however, that are open source in their style and their content, and those opensource licenses may not all say precisely the same thing. therefore, we must be careful not only to read the particular opensource license that we are dealing with but also to be aware that if the source code comes from different sources, with different opensource licenses, we may end up with multiple obligations bundled in a single product. the next issue about licenses is a bit more optimistic. in the ip world, people tend to think of licenses as a way to prevent people from doing certain things: i will share my content with you, so long as you do not steal it, cheat on it, share it, forward it, and the like. the underlying concept of licenseœaspermission can be enabling, however. increasingly over the last decade, there has been innovation in the law in designing licenses that authorize people to do things that you want them to do. creative commons licenses are the paradigmatic example of this. opensource licenses are often used in this way as well. the fact that lawyers have made innovations like creative commons and opensource licenses demonstrates the possibility that others could create all kinds of additional new mechanisms for enabling what people can do, alongside ways for restricting what people can do. both for the restrictive licenses and enabling licenses, the law is not fully clear regarding which, if any, of this is actually enforceable. the creative commons and opensource licenses are put out there and characterized and conceptualized within their respective communities as licenses rather than contracts. the idea behind them is that they are enforceable because the license is attached to the underlying copyrighted object and travels with the underlying copyrighted object. anybody who encounters the underlying copyrighted object requires access to it along with the accompanying obligation, which is, i think, an interesting and useful concept. that concept has not genuinely been tested on its merits in the courts, and the problem with it is that there is a dark underside that some people have identified. that is, it is one thing the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 105 to attach a happy public domain style or ﬁplease use this and share itﬂ kind of style notice, but what is the difference, legally speaking, between attaching a ﬁplease go and share thisﬂ notice and attaching a ﬁplease do not do anything i do not like with thisﬂ notice? suppose i were to share that dataset with you and attach a notice that said you may not do anything: you may not publish your results, you may not criticize my method, and you may not combine or collaborate with anybody else. legally speaking, it is difficult to distinguish the restrictive notice from the enabling notice, and the legal system has not really come to terms with how to justify one and not the other. the last point in this area has to do with the character of the enforcement of the license. so long as these licenses are designed by private parties and are attached to copyrighted works or databases by private parties, then enforcement lies with the copyright owner or with the holder of the database or whoever attached the notice. that creates obvious problems in inconsistency and conflicts. one solution has to do with the identity of the enforcer. having either a government agent or some other thirdparty neutral in the middle, as custodian of the legal rights for purposes of monitoring and enforcement, is something to consider when designing an enforceable scheme that would accomplish the actual goals of the enterprise. i am now going to move to the last section, which concerns management of data collected from a variety of sources. assume that a researcher has identified and overcome various licensing hurdles and iprights hurdles associated with the underlying data. the researcher has a project that has data from various sources. management of this database is often understood primarily by policy choices, but there is also an important role here for law. my research at the university of pittsburgh focuses on commons communities, with commons understood as a structured or designed pool of resources or a sharing environment. here is the question: if we have a choice between maintaining public domain status of the underlying content or understanding the mechanics of a particular license or contractual regime, are there potential upsides to a license or contractual regime that might make that more attractive than a public domain alternative? consider the questions in box 31: the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.106 the future of scientific knowledge discovery in open networked environments  managing the results of data collectionforwardlooking issues related to governing a data commons:1.data/dataset integrity.2.translation and interoperability of data from different sources, in different formats: designing and enforcing standards; managing and maintaining data consistency. (ensuring pd status of data may be inadequate to deal with this challenge.)3.who has access to the new collection of data, and for what purposes?4.what are participants™ duties and rights regarding standardization and data consistency, and resharing, recombining, reusing data?5.how is compliance monitored and enforced?6.when do those duties and rights pass to downstream partieswho did not obtain the data in the first place?7.compare the costs and benefits of governmentsponsored enforcement with those of private enforcement via licenses and contracts, and with those of informal/community enforcement. box 31 source: michael madison  box 31 identifies governance topics that relate to a community of researchers or to a particular researcher, using data from various resources that might be bundled in a license instrument and made enforceable as part of that license. it is important to identify both the ﬁwhatﬂšthat is, where things are coming from in terms of the underlying datašand the ﬁwho.ﬂ we want to decide who can participate in this particular enterprise and which researchers are eligible. we also want to know what their obligations will be, what their duties will be relative to each other and relative to the data, and also what the enforcement mechanism will be. who has authority to enforce breaches of the underlying protocols? what are the carrots and sticks associated with ensuring compliance, monitoring performance, and making sure that the data is preserved in its integrity over time and combined in a way that is suitable to the underlying license obligations? this last point is particularly important, and the license instrument can be especially useful for managing downstream obligations. when the questions are how these duties will pass to the next generation of researchers and who you may or may not know about in the beginning, then having a welldesigned license instrument might be able to help you specify who those people are and what their obligations will be. that is the mechanics of an opensource software license, but the concept can be ported into this area as well. in closing, i will quickly respond to the claim that the legal system might provide clarity and a deterministic foundation on which to build policy analysis or other analysis. that is not the case. there is no onesizefitsall solution that the legal system can offer to these problems. what the creative commons example and the opensource software community have shown with its licensing experience is that the set of tools available to manage these collections is much more diverse and complex than it was 10 or 15 years ago. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 107 knowledge discovery in open networked environments: some policy issues gregory a. jackson educause the coleman report, released in 1966, was one of the first big studies of american elementary and secondary education, and especially of equality therein. some years after the initial report, i was working in a group that was reanalyzing data from that report. we had one of what we were told were two remaining copies of a dataset that had been collected as part of that study but had never been used. it was a 12inch reel of halfinch magnetic tape containing the coleman ﬁprincipalsﬂ data, which derived from a survey of principals in high schools and elementary schools. the first challenge was to decipher the tape itself, which meant trying every possible combination of labeling protocol, track count, and parity bit until one of them yielded data rather than gibberish. once we did thatšthe tape turned out to be seven track, even parity, unlabeledšthe next challenge was to make sure the codebooks (which gave the data layoutšthe schema, in modern termsšbut not the original questions or choice sets) matched what was on the tape. by the time we did all that, we had decided that the principals data were not that relevant to our work, and so we put the analysis and the tape aside, and eventually the tape was thrown away. unfortunately, apparently what i had had was actually the last remaining copy of the principals data. the other socalled original copy had been discarded on the assumption that our research group would keep the tape. that illustrates what can happen with the lockss strategy (lots of copies keeps stuff safe) for data preservation: if everybody thinks somebody else is keeping a copy, then lockss does not work very well. many of us who work in the social sciences, particularly in the policy area, never gather our own data. the research we do is almost always based on data that were collected by someone else, but typically not analyzed by them. this notion that data collectors should be separate from data analysts is actually very well established and routine in my field of work. the coleman work came early in my doctoral studies. most of my work on research projects back then (at the huron institute, the center for the study of public policy, and harvard™s administration, planning, and social policy programs in education) involved secondary analysis. later, when it came time to do my own research, i used a large secondary dataset from the national longitudinal study of the high school class of 1972 (nls72). this study went on for years and is now a huge longitudinal array of data. my research question, based on the first nls72 followup, was whether financial aid made any difference in students™ decisions whether to enter college. the answer is yes, but the more complex question is whether that effect is big enough to matter. nls72 taught me how important the relationship was between those who gather data and those who use it, and so it is good to be here today to reflect on what i have learned since then. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.108 the future of scientific knowledge discovery in open networked environments  my current employer, educause, is an association of most of the highereducation institutions in the united states and some elsewhere. among other things, we collect data from our members on different questions: how much do you spend on personal computers? how many helpdesk staff do you have? to whom does the chief information officer report? we gather all of these data, and then our members and many other people use the core data service for all sorts of purposes (figure 39).  figure 39 educause core data service. source: gregory a. jackson  one of the issues that struck us over time is that we get very few questions from people about what a data item actually means. users apparently make their own assumptions about what each item of data means, and so although they are all producing research based ostensibly on the same data from the same source, because of this interpretation problem they sometimes get very different results, even if they proceed identically from a statistical point of view. if issues like this go unexamined, then research based on secondary, ﬁdiscoveredﬂ sources can be very misleading. it is critical, in doing such analysis, to be clear about some important attributes of data that are discovered rather than collected directly. i want to touch quickly on five attributes of discovered data that warrant attention: quality, location, format, access, and support. quality  the classic quality problem for secondary analysis is that people use data for a given purpose without understanding that the data collection may have been inappropriate for that purpose. there are two general issues here. one has to do with very traditional measures of data quality: whether the questions were valid and reliable, what the methodology was, and other such attributes. since that dimension of quality is well understood, i will not discuss it further. the other is something most people do not usually consider a quality issue, but any archivist would say it is absolutely critical: we have to think about where data came from and why they were gatheredštheir provenancešbecause why people do things makes a difference in how they do them, and how people do things makes a difference in whether data are reusable. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 109 we hear arguments that this is not true in the hard sciences and is completely true in the social sciences, but the reverse is equally true much of the time. the question of why someone gathered data, therefore, is very important. one key element of provenance i call primacy, which is whether we are getting data from the people who gathered them or there have been intermediaries along the way. people often do not consider that. they say, ﬁi've found some relevant data,ﬂ and that is the end of it. i was once assigned, as part of a huge huron institute review of federal programs for young children, to determine what we knew about latchkey kids. these are children who come home after school when their parents are not home, and let themselves in with a key (in the popular imagery of the time, with the key kept around their necks on a string). the question was, how many latchkey kids are there? this was pregoogle, so i did a lot of library research and discovered that there were many studies attempting to answer this question. curiously, though, all of them estimated about the same number of latchkey kids. i was intrigued by that, because i had done enough data work by then to find such consistency improbable. i looked more deeply into the studies, determining where each researcher had gotten his or her data. the result was that every one of these studies traced to a single study, and that single study had been done in one town by someone who was arguing for a particular public policy and therefore was interested in showing that the number was relatively high. the original purpose of the data had been lost as they were reused by other researchers, and by the time i reviewed the finding, people thought that the latchkeykid phenomenon was well and robustly understood based on multiple studies. the same thing can happen with data mining. we can see multiple studies and think everyone has got separate data, but in reality everyone is using the same data. therefore, provenance and primacy become very important issues. location in many cases communicating data becomes a financial issue: how do i get data from there to here? if the amount of data is small, the problem can be solved without tradeoffs. however, for enormous collections of datašxray data from a satellite, for example, or even financial transaction data from supermarketsšhow data get from there to here and where and how they are stored become policy issues, because sometimes the only way to get data from source to user and to store them are by summarizing, filtering, or otherwise cleaning or compressing the data. large datasets gathered elsewhere frequently are subject to such preprocessing, especially when they involve images or substantial detection noise, and this is important for the secondary analyst to know. constraints on data located elsewhere arise too. there may be copyright constraints: someone can use the data, but cannot keep them, or can use them and keep them, but cannot publish anything unless the data collector gets to seešor, worse, approvešthe results. all of these things typically have to do with where the data are located, because the conditions accompany the data from where they were. unlike the original data collector, the secondary analyst has no ability to change the conditions. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.110 the future of scientific knowledge discovery in open networked environments  format there are fewer and fewer libraries that actually have working lantern slide projectors such as the one shown in figure 310. yet there are many lantern slides that are the only records of certain events or images. in most cases, nobody has the funds to digitize or otherwise preserve those slides. as is the case for lantern slides, whether data can be used depends on their format, and so format affects what data are available. there are three separate kinds of issues related to format: robustness, degradation, and description. figure 310 projector for obsolete format photographic slides. source: gregory a. jackson  robustness has to do with the chain of custody, and especially with accidental or intentional changes. part of the reason the seventrack coleman had even parity was so that i could check each 6 bits of data to make sure the ones and zeroes added up to an even number. if they did not, something had happened to those 6 bits in transit, and the associated byte of data could not be trusted. hence, one question about secondary data is: is the format robust? does it resist change, or at least indicate it? that is, does data format stand up to the vagaries of time and of technology? degradation is about losing data over time, which happens to all datasets regardless of format. errorcorrection mechanisms can sometimes restore data that have been lost, especially if multiple copies of the data exist, but none of those mechanisms is perfect. it is important to know how data might have degraded, and especially what measures have been employed to combat or reverse degradation. finally, most data are useless without a description of the items in this dataset: not just how the data are recorded on the medium or the database schema, but also how they were the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 111 measured, what the different values are, what was done when there were missing data, whether any data correction was done, and so on. therefore, as a matter of policy, the ﬁcodebookﬂ becomes an important piece of format. sometimes codebooks come with data, but quite often we get to the codebook by a path that is different from the one leading to the data, or we have to infer what the codebook was by reading someone™s prior research. both are dangerous. that is what we had to do with the coleman principals data, for example, because all we had were summary tables. we had to deduce what the questions were and which questions had which values. it is probably just as well we never used the data for analysis. access two policy issues arise. the first access issue is promotion: researchers trying to market their data. the risks in that should be obvious, and it is important that secondary analysts seek out the best data sources rather than vice versa. as an example, as i was preparing this presentation, a furor erupted over a publicradio fundraiser who had been recorded apparently offering to tilt coverage in return for sponsorship. that is not the data issuešrather, it was the flood of ﬁexpertsﬂ offering themselves as commentators as the media frenzy erupted. experts were available, apparently and ironically, to document almost any perspective on the relationship between media funding and reporting. the experts that the chronicle of philanthropy quoted were very different from the ones forbes quoted. the second issue is restriction. some data are sensitive, and people may not want them to be seen. there are regulations and standard practices to handle this, but sometimes people go further and attempt to censor specific data values rather than restrict access. the most frequent problem is the desire on the part of data collectors to control analysis or publications based on their data. most cases, of course, lie somewhere between promotion and censorship. the key policy point is, all data flow through a process in which there may be some degree of promotion or censorship, and secondary analysts ignore that at their peril. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.112 the future of scientific knowledge discovery in open networked environments  support support has become a big issue for institutions. suppose a researcher on campus a uses data that originated with a researcher at campus b. a whole set of issues arises. some of them are technical issues. some of them are coding issues. many of these i have already mentioned under location and format, above. a wonderful new yorker cartoon13 captures the issue perfectly: a car is driving in circles in front of a garage, and one mechanic says to another, ﬁat what point does this become our problem?ﬂ whatever the issues are, whom does the researcher at campus a approach for help? for substantive questions, the answer is often doctoral students at a, but a better answer might come from the researcher at b. for technical things, a™s central information technology organization might be the better source of help, but some technical questions can only be solved with guidance from the originator at b. is support for secondary analysis an institutional role, or the researcher™s responsibility? that is, do all costs of research flow to the principal investigator, or are they part of central research infrastructure? in either case, does the responsibility for support lie with the originatoršbšor with the secondary researcher? these questions often get answered financially rather than substantively, to the detriment of data quality. when data collection carries a requirement that access to the data be preserved for some period beyond the research project, a second support question arises. i spoke with someone earlier at this meeting, for example, about the problem of faculty moving from institution to institution. suppose that a faculty member comes to campus c and gets a national science foundation (nsf) grant. the grant is to the institution. the researcher gathers some data, does his or her own analysis, publishes, and becomes famous. fame has its rewards: campus d makes an offer too good to refuse, off the faculty member goes, and now campus c is responsible for providing access to the data to another researcher, at campus e. the original principal investigator is gone, and nsf probably has redirected the funds to campus d, so c is now paying the costs of serving e™s researcher out of c™s own funds. there is no good answer to this issue, and most of the regulations that cause the problem pretend it does not exist. a caution about cautions let me conclude by citing a favorite frazz comic, which i do not have permission to reproduce here. frazz is a great strip, a worthy successor to the legendary calvin and hobbes. frazz is a renaissance man, an avid runner who works as a school janitor. in one strip, he starts reading directions: ﬁdo not use in the bathtub.ﬂ caulfield (a student, frazz™s protégé, quite possibly calvin grown up a bit) reads on: ﬁnor while operating a motor vehicle.ﬂ they continue reading: ﬁand not to be used near a fire extinguisher, not recommended for unclogging plumbing, and you do not stick it in your ear and turn it on.ﬂ finally, caulfield says, ﬁokay, i think we are good,ﬂ and he puts on his helmet. then the principal, watching the kid, who is wearing skates and about to try rocketing himself down an icedover sidewalk by pointing a leaf blower backwards and turning it on, says, ﬁthis  13available at http://gregj.us/pfj6en. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 113 cannot be a good idea.ﬂ to which frazz replies, ﬁwhen the warnings are that comprehensive, the implication is that they are complete.ﬂ if there is a warning about policy advice, it is that the list i just gave cannot possibly be complete. the kinds of things we have talked about today require constant thought! the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.114 the future of scientific knowledge discovery in open networked environments  discussion discussant: is there any foreign country where you think the law works well? there are a variety of regimes around the world, ranging from afghanistan, which has no intellectual property laws at all, up to us. is there some place where you think it works well? mr. carroll: when the united states invaded iraq, shortly after we occupied that country we sent hilary rosen, the former head of the recording industry, to iraq to help them write their copyright law. generally speaking, the united states has been fairly aggressive over the past 12 or so years in negotiating bilateral trade agreements with a host of modest countries around the world that are explicitly designed to export u.s.style intellectual property rights, including most all of copyright. discussant: at the moment, the countries that are not part of the berne convention include afghanistan and somalia and other places, such as east timor. mr. carroll: the u.s. weather data policies were among the first examples of an open approach to public information. many of these countries, particularly in europe, had the cost recovery model, but now europe is starting to see that open data might be important to promote the innovation that can be built on open platforms and that the cost recovery model for sharing weather data might be inhibiting the pace of innovation. therefore, one of the arguments in favor of openšrather than closedšsystems is that with an open system we get innovation that cannot otherwise be predicted. discussant: i want to make sure that i do not come across as being overly negative or critical of u.s.style intellectual property rights. one of the great strengths of the u.s.style copyright system is that it has supported a very lively innovation space over the last decade or two. there is a fair amount of potential for using the existing copyright leversšincluding the public domain lever, enabling licensing, and so forth, all of which exist as part of the default landscape of american intellectual property lawšto build legal institutions that enable the kind of collaboration that we have been talking about today. mr. uhlir: i would like to elaborate on the e.u. database directive. it is a block to the kinds of automated knowledge discovery and open networking environments that we are discussing here. one of the negative aspects is that an insubstantial part of the database gives rise to the right. so if we take it either qualitatively or quantitatively, as defined by the owner of the database, this leaves the user completely unaware of how much extraction of the data is really a cause of action or an infringement. second, there is no economic harm needed to trigger the statute. third, it applies to government data as well as to privatesector data. fourth, it is potentially a perpetual right, because it is triggered for another 15 years every time the database is updated substantially. fifth, it is an unprecedented exclusive property right in that it protects investment rather than intellectual creativity, which is a fundamental aspect of copyright law. and sixth, there are no mandatory exclusions or limitations. for example, even though this was supposed to harmonize the law in the european union, there are at least three e. u. countries that have no exclusions whatsoever, no exceptions for scholarly use or education or any other purpose. this applies to factual information rather than creative content. it is thus highly pernicious in the sense that the controls over factual information and the reintegration of information absolutely forbid the type of reuse of information that scientists typically make. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 115 mr. madison: in reaction to that, i want to note that none of this legal landscape is static. it is all moving at different rates of speed. the e.u. database directive was a major panic button in the intellectual property policy space for a long time. as mr. carroll said, to a certain degree it seems that the panic mode seems to have receded, although on the u.s. side of the atlantic there seems to be renewed interest in the misappropriation doctrine relative to information from databases as a way to avoid some of the limitations that he described. there needs to be ongoing vigilance. the other consideration in this area is that, as is often the case, the best defense is a good offense when trying to mitigate the harmful effects of some barriers like this. one way to offset some of the attractiveness of either the e.u.style regulation or renewed interest in tort law, appropriationsstyle activity would be for universities, consortia, funders, and others to work proactively to use some of these legal levers, particularly in the area of commons, to build these commons proactively and then to have a proof of concept to show how they can actually succeed in specific settings. there are opportunities to build some of these legal questions into the design of the research enterprises from the beginning. suppose, for example, that we take the national science foundation (nsf) data management plan and leverage off of that, from a legal standpoint, to see if that cannot be made to succeed in some other settings as a proof of concept. discussant: could you address the problems that face chief information officers (cios) of universities, given that there are often residual complaints on these problems? dr. jackson: cios are the people typically in charge of the boring part of the use of information technology on a campus. they make sure the training is run, for example. scientific data discovery poses three major challenges for cios. the first is, particularly for highvolume or timesensitive scientific data, the question of whether the network will be there to move data in the right way at the right time and whether the storage is there to receive it. there is a set of infrastructure issues, all of which are episodic, meaning that most of the time the datasets sit there unused, then occasionally something happens that results in a peak use that demands all the capacity, and then back to the state of being relatively unused. this is a characteristic of most highperformance networks and computing. it is typically not true for storage. it is a very difficult argument for a cio to make that the university should invest a lot of money in infrastructure that, on average, is not used. the argument that somebody needs it occasionally is a very difficult argument to make. the second issue that cios face is the support question. this happens, for example, when a faculty member moves on, or a postdoc gets a faculty appointment elsewhere, or a doctoral student finishes a degree. suddenly someone is looking around and saying, ﬁwho will i rely on for support?ﬂ of course, they will mention the overhead rate and say that the institution should provide the support. the third issue, which is growing very fast, is the question of how we store data that the institution is required to hold and make available. this is an enormous technical challenge, not because it is hard technically, but because the technology is expensive and should be shared. the institutions are not good at sharing these data or putting them into the google cloud, however. it sounds attractive, but it is hard to write the right contract. this means that someone at the university must take care of these problems, make sure that the data do not degrade, that error correction gets done, and so on. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.116 the future of scientific knowledge discovery in open networked environments  one of my favorite questions to ask is, you have a body of data that you need to put in one place and have it available 10 years from nowšwhat mechanism do you choose to do that? there is one good answer to that question, which is that you give it to a relatively expensive service to do it. and every so oftenštypically a matter of months, maybe every yearšthis service, which has two copies of the data, is going to read the two copies, compare them to one another, and do a certain amount of error correction and restoration. that is the only way to be sure that the data will be there 10 years from now. putting the data on cds or dvds does not work. it actually does work if you can manage to print it out on archival quality paper and then copy it, but a lot of data cannot be rendered that way. mr. carroll: think about the law as providing the default terms of use that apply to data. it was pointed out that those default terms of use are fairly uncertain for data, because it can be difficult to tell exactly where the line is between what is copyrighted and what is not, especially for certain types of data, such as image data. if we do not do anything, we are still making a policy choice, we are choosing the default terms. let the uncertainty be the rule. to defend the nsf™s unfunded data management mandate, i think part of what nsf is saying is that researchers have to confront what the policy choices are, what the data management choices are. the researchers can no longer leave those unstated or underdetermined. this is a good idea, as we think about a network environment where people have to work together and we think about longitudinal issues, the demand for the intellectual resources, and the institutional resources, that we should think about such things as what happens to the data tomorrow or 5 or 10 years from now and who has rights to that data. these are the kinds of decisions that are worth making now, because as the flood of data continues, these questions will only become more pressing. when we lawyers are doing our job right, we are problem solvers. and when we are doing our job really right, we are identifying problems before they have emerged and become serious, and are helping develop a solution. all of these data management issues are only going to become more pressing, and if we leave all these areas undetermined and not thought about, we will end up in a much worse situation in the future. we will find ourselves dependent on somebody else™s data, and suddenly they have some ability to claim rights and hold us up, because we did not think about the use and access issues in advance. therefore, one important point from this discussion is that these are issues that deserve attention. attention is expensive, but if we do not attend to it now, it will only be more expensive later. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 117 3. how can we tell? what needs to be known and studied to improve potential for success?  session chair: francine berman rensselaer polytechnic institute  the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 119 introduction francine berman rensselaer polytechnic institute  to set the stage for this session, i want to start by talking about how research itself is evolving. modern researchers approach their work in multiple ways. there were two approaches that were mentioned in our discussions earlier. one approach is analogous to looking for a needle in a haystack. the needle is a welldefined question, for example, does p=np?, which is a classic question in computer science. we know what to do to see if our proposed solution addresses the question, and often the question itself will suggest various approaches to a solution. another approach is analogous to identifying where the haystacks are. we may have huge amounts of data and be seeking patterns in the data that are likely to provide useful insights and information. for example, the original seti@home application combed through massive amounts of data to identify patterns that may indicate extraterrestrial intelligence of some sort. to support modern research, today™s digital data are being analyzed, used in models and simulations, used to create additional derived data (e.g., visualizations and movies from computer simulations), distributed among a wide variety of participants for filtering, and so on. there is almost no area of modern research that is not being transformed by the availability and ubiquity of digital data. another emerging differentiator between today™s modes of research and the past is the broad spectrum of individuals involved. in traditional settings, research was done by professional expertsšprofessors, individuals whose profession is research and development, and other experts. research was their ﬁday job,ﬂ and they spent a substantial amount of time doing it. today we are increasingly seeing some research being done by the ﬁcrowd,ﬂ many individuals spending varying amounts of time contributing, who are not necessarily professional experts. the millions who have contributed to seti@home or galaxy zoo have created new information in aggregate. the number of real discoveries and innovations coming from these crowd applications is increasing, and crowd discovery or citizen science is emerging as an innovative new model for research. this also means that the infrastructure for dataenabled research must support these new crowd applications. during the discussion today, we heard about enabling digital research infrastructure, and the importance of interoperability between its systems and components. we heard about semantic webs and the need for diverse relationships among the data. we focused on the need for human computer interfaces, portals, ways of accessing the data, and ways of using the data. we discussed the different characteristics of the data, about privacy and the sharing of data. we also heard about gapsšgaps between academia and industry, and gaps between earlier generations and the millennial generation who are using data in a very different way. all of these issues are critical to consider in developing a viable infrastructure to support dataenabled research, and they will inform what we will discuss in this session. our panelists will examine what works, how we might assess the effectiveness and success of our the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.120 the future of scientific knowledge discovery in open networked environments  infrastructure, and what we are doing. we will also discuss what needs to be done now, and what needs to be studied so that we can continue to accelerate researchdriven discovery and innovation in the future. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 121 an academic perspective  victoria stodden columbia university  my work focuses on the intersection of computational science and legal and policy issues. aspects related to scientific communication are critical to this discussion. as scientists, one of our first duties is to be deeply skeptical. our first assumption when we see other people™s scientific research or the presentation of a scientific fact is that it is not correct. then we think more and say, ﬁwell, maybe this is right,ﬂ or we have managed to address the errors in this area or in that area. we move the science forward in that way, and it is a core defining principle of how we should address these issues as technology changes our scientific communication, our modes of research, and the different modalities that we use. at the core, it is about getting at error control. while it is true that the data deluge is hitting various places and giving us new questions and new solutions, this is not the only way science is being affected. it is also true that fields themselves are becoming transformed. in my field of statistics, the journal of the american statistical association is one of the flagship publications. i took a look at the proportion of computational articles in the journal beginning in 1996. that year a little less than half of the articles, 9 of 20, were computational, and none of them talked about where to get the codes so that i could actually verify those results. ten years later, in 2006, 33 of 35 articles published in that journal were computational. nine percent of those gave me a link to get the code or pointed me to a package. by 2009, all of the articles were computational. of those, only 16 percent tell me how i can actually get into the code. therefore, if someone is publishing in this journal as a statistician, he or she is almost certainly publishing computational work. when computational research is done, however, what is communicated in the paper is typically not enough to allow us to understand how the results were generated. in almost all cases, we need to get into the code and the data. without access to the code or the data, we are engendering a credibility crisis in computational science. we have enormous amounts of data being collected. what is typical is that the intellectual and scientific contributions are encoded in the software, but the software is not shared. such things as data filtering, how the data were prepared for analysis, and how the analysis was done, which can all be very deep intellectual contributions, are in the software and are not easily captured in the paper. both the data and the code are important. much of the discussion here is about the data, although code did sometimes come up. it is equally important that they both be shared and be open, which they are not. this is at the root of our credibility crisis in computational science. how should we proceed? as a scientist, the first thought is to fall back on the scientific method, and how the problems of openness and communication of discoveries have both been the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.122 the future of scientific knowledge discovery in open networked environments  addressed in the other two branches of the scientific method, the deductive and the empirical branches. in the deductive branch in mathematics and logic, people have worked out what it means to have a proof, to really communicate the thinking behind the conclusions that are being published. similarly, in the empirical sciences, there are standards that have been developed, such as controlled experiments and the machinery of hypothesis testing, and how they are communicated. in a methods section, there is a very clear way that these results are to be written for publication, designed so that other people can reproduce the thinking and the results themselves. in computational science, we are now struggling with this issue: how to communicate the innovations that are happening in computational science in such a way that they will meet the standards that have been established in the deductive and empirical branches of science. my approach is to understand these issues in terms of the reproducibility of computational science results, and this gives me the imperative to share the code and the data. we have seen many interesting examples of how reuse can be facilitated and what happens when someone actually shares open data. this gives rise to a host of issues about ontologies, standards, and metadata. this is framed within the context of reproducibility. the reason that we are putting the data and the code online is to make sure that the results can be verified and reproduced. here is an example. in 2007, a series of clinical trials were started at duke university that have since been terminated, but it took a few years to terminate them. they were based on computational science results in personalized medicine that had been published in prestigious journals, such as nature medicine. researchers at the m.d. anderson cancer center tried to replicate the computational work that had gone into the underlying science, and uncovered serious flaws undetected by peer review. the study was plagued with a myriad of issues, such as flipped drug labels in the dataset and errors of alignment between observations in treatment and control groupsšerrors that are simple to make. the clinical trials were canceled in late 2010, after patients had been assigned into treatment and control groups and had been given drugs. one of the principal investigators resigned from duke at the end 2010. the point is that we have to assume that errors are everywhere in science, and our focus should be on how we address and root out those errors. there was a discussion earlier of how the data deluge is a larger manifestation of issues that have been seen before. also, dr. hey gave the example of brahe and kepler and how what must have been a data deluge in their context ended up engendering significant scientific discoveries. in that sense, there is nothing fundamentally new here. we are doing the same thing in a methodological sense as we have always done, but we are doing it on a much larger scale. the scope of the questions that we are addressing has changed. in that sense, the nature of the research has changed. dr. hey told us that we need more skills to address this concern. dr. friend then said we need verifiability, a point that i have also attempted to make in this talk. this means that the infrastructure and incentives need to adapt to the research reality even though the process of science is not changing in any fundamental way. that in turn means that it will be important to develop tools for reproducibility and collaboration. for example, some presenters also talked about provenance and workflowtracking tools and openness in the publication of discoveries. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 123 in short, there are many different efforts that are needed. the solutions in this area will not be something that comes down as an executive order and then all scientists are suddenly open with their data and code. the problems are much too granular, and so the solutions must emerge from within the communities and within the different research and subject areas. vis trails is a scientific workflow management system. it was developed by a team from the university of utah that is now moving to new york university. it tracks the order in which scientists call their functions and the parameter settings that they have when generating the results. these workflows can then be shared as part of the publication, and they can be regenerated as necessary. vis trails also promotes collaboration. some recent work by david donoho and matan gavish was presented for the first time in a symposium on reproducibility and computational science, held at the american association for the advancement of science. they have developed a system for automatically storing results in the cloud in a verifiable way as they are generated, and creating an identifier that is associated with each of the published results. for example, if a paper contains a figure, then we would be able to click on it and see how it was done and reproduce the results, as the means to do this are automatically in the cloud. another useful tool is colwiz, a name derived from ﬁcollective wisdom.ﬂ its purpose is to manage the research process for scientists. one of the major problems with reproducibility is that, unless a scientist is using these specialized tools, there is no automatic mechanism for researchers to save their steps as they advance. after they have finished an experiment and written the paper, they may find that going back and reproducing the experiment is even more painful than going through it the first time. thus, tools like colwiz could help both with communicating scientific discovery and with reproducibility. these issues are all related to the production of scientific data and results. there are also some aspects related to publication and peer review. it is a lot of work to request reviewers, who are already overworked, to review code or data and incorporate them into the review process. maybe we will get there one day for computational work, but certainly not now. the journals molecular systems biology and the european molecular biology organization are publishing the correspondence between the reviewer and the authors, anonymously but openly, along with the actual published results. this is one approach that is being tried to be more open and transparent. one of the reasons for this practice is that there is a great inequality in the power of the reviewers and the authors. in particular, reviewers can ask for additional experiments and more exploration of data from the person who is trying to get the paper published. particularly for prestigious journals, reviewers have a lot of power. these journals are trying to balance this power by allowing readers to see the dialogue that took place between editors and authors before a publication. furthermore, many journals now have supplemental materials sections in which datasets and code can be housed and made available for experimentation. the sections are not reviewed and so far have had varying amounts of success. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.124 the future of scientific knowledge discovery in open networked environments  in the february 11, 2011, issue of science, there was an editorial emphasizing that, in addition to data, science is now requiring that code be available for published computational results. that is extremely forward thinking on the part of science. science is folding this into its existing policy, which is that if someone contacts an author after publication and asks for the datašand now the codešthe author must make it available. an approach that other journals have taken is to employ an associate editor for reproducibility. the journal of biostatistics and the biometrical journal do that. the associate editor for reproducibility will regenerate and verify the results, and if the editor can produce the same results that are in the manuscript, the journal will kitemark the published article with a ﬁdﬂ for data or ﬁcﬂ for code. in this case the journal can advertise that readers can have confidence in the results, because they have been independently verified. there are also new journals that are trying to address the lack of credit authors get for releasing and putting effort into code or data or for attaching metadata. they are trying to address the issue of incentives, and their focus is on open code and open data. open research computation offers data notes, for example, and biomed central has research notes. pubmed central and open access are older concepts embedded within the infrastructure of the national institutes of health (nih). but why does it stop with nih? could we have a pub central for all areas and allow people to deposit their publications when they publish, similar to the nih policy? there has been much discussion about the peerreview data management plan at the national science foundation (nsf). this is a very important step even though it has been called an unfunded mandate. it is an important experiment in that it creates the possibility of gathering information about how much it will cost and how data should be managed. it is, in a way, a survey of researchers on how they are conceiving of these issues. maybe the costs are less than nsf worries about, or maybe they are more, but at least we will be able to get a sense of this. one report that i was involved with along with john king was for the nsf office of cyberinfrastructure on virtual communities. we advocated reproducibility as part of the way forward for the collaborative, very highpowered computing problems that we are addressing. as part of the fallout from the problem i mentioned with the duke university clinical trials, the institute of medicine convened a committee to review omicsbased tests for predicting patient outcomes in clinical trials. ﬁomicsﬂ refers to genomics, proteomics, and so on. the committee is chaired by gil omenn, and part of its mandate concerns issues of replicability and repeatability and how the articles published in nature medicine that led to the clinical trials could have gotten through with what were, in hindsight, such glaring errors. there seems to be a hesitation on the part of some funding agencies to fund the software development or infrastructure necessary to address reproducibility and many of the other issues that we have discussed so far. let me give an example of an email that was sent to a group email. the author of the email was talking about how his group develops opensource software for research. he is a prominent researcher, who is very well known and very influential. his group develops opensource tools, and it is very difficult for him to get funding even when applying to nih programs that are targeted at promoting software development and maintenance. in particular, he said, ﬁmy group developed an opensource tool written in java. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 125 we started with microarrays and extended the tool to other data. there were 25,000 downloads of this tool last year. so we submitted a grant proposal. two reviewers loved it. the third one did not because he or she felt it was not innovative enough. we proposed three releases per year, mapped out the methods we would add, included user surveys, user support, and instructional workshops. we had 100 letters of support.ﬂ this quote is from the negative review: ﬁthis is not science. this is software development. this should be done by a company.ﬂ we can see that there seems to be a bifurcation in understanding the role that software plays in the development of science. one idea for the funders of research might be: why not fund a few projects to be fully reproducible to see what barriers they run into? is the problem that they do not have repositories where they can deposit their data? is the problem that they encounter issues of maintaining the software? where are the problems? let us do a few experiments to see the stumbling blocks that they encounter. on the subject of citation and contributions, as we incorporate databases and code, we need to think about how to reward these contributions. many contributions to databases now are very small, and there are databases where 25,000 people have contributed annotations. hence, there are questions about how to cite and how to reward this work. what is the relative worth between, for example, a typical article with a scientific idea versus software development versus maintaining the databases? typically the last two have not been well rewarded, and our discussion here calls that practice into question. i will end with a figure from a survey i did of the machinelearning community (figure 311). these are the barriers that people said that they faced most dramatically when they were sharing code and sharing data. figure 311 barriers to data and code sharing in computational science. source: victoria stodden the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 127 a government perspective  walter l. warnick department of energy  i am the director of the office of scientific and technical information, which manages many of the scientific and technical information operations of the department of energy (doe). our goal is to maximize the value of the research and development (r&d) results coming from the department. to put this into perspective, each government agency has an organization that manages information. those organizations have gotten together and formed an interagency group called cendi. bonnie carroll is the executive secretary of cendi. the national science foundation is represented in cendi by phil bogden, and the national institutes of health (nih) is represented by jerry sheehan. i represent the doe, and all the other agencies have representatives too, including the library of congress, government printing office, department of agriculture, department of the interior, and practically every other organization that has a big r&d program. ninetyeight percent of the government™s r&d is represented, and several organizations that do not have r&d programs are also in cendi. the results of the u.s. government r&d investment, which amounts to about $140 billion a year, are shared via different formats. there is the text format, which includes journals, eprints, technical reports, conference proceedings, and more. there is nontext data, which includes numeric datasets, visualizations such as geographic information systems, nonnumeric data such as genome sequences, and much more. and there are other formats, including video and audio. each format is in a state of change and presents its own set of challenges. with journal articles, for example, the big issue within the government is public open access versus proprietary access. i think we all agree that the gold standard for textbased scientific technical information is the peerreviewed journal article. many highly respected journals are available only by proprietary subscription access. nih has pioneered a transition to make journal literature publicly accessible. the effort has attracted a lot of attention, and it is still getting a lot of attention within the government. principal investigators are asked to submit journalaccepted manuscripts for publication in the nih publicaccess tool, pubmed central. what is significant now is that the america competes reauthorization act, which became law in december 2010, calls upon the office of science and technology policy (ostp) to initiate a process to determine if public access to journal literature sponsored by government should be expanded. for now, nih is the only agency that makes a requirement of public access to journal articles. the doe and other agencies are already empowered by law to adopt that requirement, but we do not have to adopt it, and as a matter of practice we do not. i think that ostp will soon formulate a committee, which the competes act calls for, to get input from stakeholders, consider the issues, and develop some recommendations. beyond the journal literature, there are gray literature issues, and integrating them with journal literature is important. gray literature includes technical reports, conference proceedings, and eprints. it is typically open access, but not all of it is. all of the doe™s the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.128 the future of scientific knowledge discovery in open networked environments  classified research is reported in gray literature, and of course that is closely held, but i am talking here about the open access part of doe™s offerings. gray literature is often organized into singlepurpose databases. for example, in the doe we have something called the information bridge, which has 300,000 technical reports produced by doe researchers from 1991 to the present. it is all fulltext searchable. the average report is 60 pages long, so they are fairly detailed. many other agencies have similar resources. other databases handle eprints, conference proceedings, patents, and more. doe pioneered novel and inexpensive ways to make multiple databases act as if they are an integrated whole, one example of which is science.gov. science.gov posts the publications of all the agencies that are in cendi, so it is a very large virtual collection of databases. it is all searchable, and a single query brings back results ranked by relevance. it has won awards for being easy to use and is an example of transparency in government. the doe™s largest virtual collection that integrates gray literature, and some journal literature, is worldwidescience.org, which is a computermediated scientific knowledge discovery tool. worldwidescience.org makes the knowledge published by, or on behalf of, the governments of 74 countries, including the united states, all searchable by a single query. the amount of content is huge, about 400 million pages. a user can enter a query in any one of nine languages, and the system will search all the databases in the language of the database and then bring back the list of hits in the language requested. it is new, and it is growing very rapidly under the supervision of the international worldwidescience alliance. we also manage nontext sourcesšthe numeric data, genome sequences, and so forth. the main questions are to what extent should such sources be made accessible and for how long. some agencies are grappling with the issue by formulating data policies. some agencies require principal investigators to propose datamanagement plans. the america competes reauthorization act calls upon ostp to initiate a process to encourage agencies to consider these issues, and it is the same part of the act that i mentioned previously that addresses journal literature. hence, committees stemming from the act are handling both text items and nontext items. everything we do entails cost. whether it is just sharing information or doing analysis of the information, there is always a cost. here is a way that i talk to my funding sources about cost. imagine a graph whose vertical axis is the pace of scientific discovery and whose horizontal axis is the percentage of funding for sharing of scientific knowledge (see figure 312). i think everybody agrees that science advances only if knowledge is shared. therefore, let us postulate an imaginary situation in which no one shared any knowledge. that would take us to the origin of this graph, because there would be no funds expended for the sharing, but there would be no real advance in science either. at the other end of the xaxis, at the 100 percent mark, if we spent all our money sharing and none of it doing bench research, soon your pace of scientific discovery would draw down close to zero too. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 129  figure 312 knowledge investment curve. source: walter warnick  we have two data points on this graph, both lying on the xaxis. in between the two points, there is a curve, which we call the knowledge investment curve. we do not know the shape of the curve, but it is likely to have a maximum. the point is that decision makers affect the pace of discovery when they determine the fraction of r&d funding dedicated to sharing. that is the argument i make to my funders. the point of the knowledge investment curve is to make funders realize that while they can dedicate funds to buy computers, hire more researchers, or build a new facility, they should also weigh that investment against the benefits of getting information out better, sharing it with more people, making searches more precise, and doing the kinds of analyses and data mining we have talked about here today. it would require a significant research program to calculate what the shape of this curve is and where that optimum is, but we know that such an optimum exists somewhere. furthermore, the optimum is not the minimum, which is another message i give to the funding sources. if we think that the purpose of an information organization is to be a repository where information goes in, seldom comes back out, and seldom sees the light of day, that is not the optimal expenditure for sharing. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.130 the future of scientific knowledge discovery in open networked environments  discussion discussant: my question to dr. warnick and dr. stodden is related to the knowledge investment curve presented by dr. warnick. in a sense, the peak of that graph is the amount of funding spent on infrastructure that enables research versus the amount of funding spent on research. where do you think that should fall for any example you choose? dr. warnick: we would probably reach a consensus that there is not enough money being spent on, for example, sharing of knowledge and analysis, and the development of some of the tools that were discussed earlier. as to how much below the optimum it is, let me give an example. consider the national library of medicine as an excellent example of an information management organization. the funding for that organization exceeds the funding for all the other information organizations combined. i am not suggesting that the national library of medicine is overfunded, but i will say that the others are underfunded. dr. stodden: i absolutely agree. i think it became much harder than it has been traditionally to share our work. as our science becomes more data intensive and involves code, those two areas add extra expenses involved with sharing that are not wholly taken care of in funder budgets. the science itself, through technology, has changed, and it is making ripple effects through our funding agencies, which have not quite caught up yet, i think. disussant: dr. hey talked about the new dataintensive work as a new paradigm, yet so much of the discussion has been about things like reproducibility in the traditional sense, but with code and data added, or sharing in the traditional sense, but with code and data added. so where does thinking about new paradigms or new ways of doing things come in? where do you see that falling in the spectrum of who is responsible and how that affects this whole question? dr. warnick: even the sharing part is being subjected to new paradigms. just to make a point, the infrastructure behind science.gov and worldwidescience.org is something that we see very rarely in everyday experience on the web, and it was developed and matured as a result of some government investment. to take your point directly about the other kinds of analysis that were discussed earlier, however, since the government is providing $140 billion of funding for research, then the analysis that gets more mileage out of that research ought to be funded by the government too. of course, the government always welcomes the idea that the private sector can take and utilize these results, but it must be the funding agencies that do the initial work. i think that the reason why people have not heard the department of energy mentioned in this discussion before now is because we were doing very little in this regard compared to the national institutes of health (nih) and the national science foundation, and that ought to change. dr. stodden: my perspective within academia is that processes are changing for hiring, promotion, and work committees. the scientists who are clued into these issues of reproducibility, open code, and open data seem to be a little more interested than people who are carrying on in a different paradigm. academia is conservative in the sense that things change slowly. therefore, it takes time for these practices to percolate through. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 131 dr. berman: the issue of the gap really intrigues and concerns me, because in our real life, if we want to find a restaurant, we can go to yelp. we can find which restaurants are near us, what is available, who likes them, and so on. there is an application for that. we can get this information on an iphone. consider taking all of our scientific products and putting them in that world. is there a place where we can find scientific databases and see who liked them? can we see who added metadata to them in a very userfriendly way? can we access them easily? we are starting to see crossover between the academic world and the world of commercial applications. phil bourne has a project called scivee, where we can show how to do different kinds of experiments or give a talk on a data youtubestyle. we can imagine using many of these commercial types of applications and technologies in academia. some interesting questions arise: what does it mean if we have a data collection and many people like it? does that mean it is a good data collection? does that mean it is an economically sustainable data collection? we should not utilize one set of tools for our academic work and another set of tools for applications in the real world without bridging the gap. mr. uhlir: there is a proposed act in congress, the federal research public access act, that broadens the nih pubmed central grantee deposit policy to include other agencies with annual research and development budgets of $100 million or more, although i do not know if it is going to actually become law. also, in the list of peer reviews presented earlier, there is one other model that was missing: postpublication review. it is not a traditional peer review. it is an open peer review, it is moderated, and it is ongoing. there are two kinds of this model. one is just commentary, and one is papers generated in response to a big paper. the model i am thinking of is the european geosciences union™s journal of atmospheric chemistry and physics in munich. i do not know how many other journals do that, but it is yet another model for a review. even if the code and the data are not available, people can ask very pointed questions that can be answered by the researchers. that is a potentially valuable way of reviewing results. in response to dr. hendler™s comment, there have been several people who have made some intermediate suggestions, such as dr. friend™s journal of models being published. but the fundamental problem is that we have moved all the print journals wholesale onto the web without hardly changing the model at all. to obtain good models, one would deconstruct the scholarly communication process used in the print paradigm and reconstruct it in a way that makes sense on the web. thus, we are repeating everything we have done before and not really thinking about what the web allows us to change in order to achieve greater efficiency. i think the print journal system itself is an outmoded way of communicating. i am sure there are all kinds of new paradigms, but i will leave it at that.  the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 133 4. summary of workshop results from day one and discussion of additional issues  the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 135  introduction  bonnie carroll information international associates today, we will take the results of the discussion from the first day of the workshop and build on them. we would like to hear about some options for what could be done over the next few years and what the value proposition is. closely coupled with that is the issue of how we will know when we have succeeded. we had some excellent concepts and ideas during the first day of the workshop that highlight much about the nature of the changes that are ongoing. i heard that we should not stay static and that the community might benefit from a less linear and more dynamic approach. we talked about trailblazers versus the long tail of data activities, and whether we are getting into a credibility crisis, because many of the things we are doing now are not as shareable as they were when they were in print. many codes are not available, for instance. the title of the workshop is ﬁthe future of scientific knowledge discovery in open networked environments.ﬂ the title recognizes that we are adding a fourth paradigm to the research process. we already have three paradigmsšthe theoretical, the experimental and observationalšbut now we have added the computational or the data intensive. the presentations we heard during the first day of the workshop focused mainly on the latter paradigm. how do these approaches build on each other and work together? why did we immediately jump to the dataintensive sciences when the title referred to a networked environment? the answers are obvious. first, it is because the technology is so enabling, and second is the data deluge. in the text that follows, we discuss the four sets of topics identified in the statement of task. each section begins with an introductory summary of the issues identified in day one of the workshop. in the first and fourth sections, puneet kishor reviews the sets of issues from the first and fourth items in the statement of task, namely, the opportunities and the benefits and the range of options. in the second and third sections, alberto pepe addresses the techniques and methods, and then the barriers, respectively. each of these introductory summaries of the issues identified on day one of the meeting by these two rapporteurs is then followed by a summary of the discussion of each topical area by all the workshop discussants on day two of the meeting.  the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 137  4. opportunities and benefits for automated scientific knowledge discovery in open networked environments  introductory summary of the issues identified on day one  puneet kishor university of wisconsin  among the issues we heard about during the first day of the workshop was that certain problems in science, no matter what domain, are really the same. a scientist performing observational research, for instance, generates data and then manipulates, organizes, analyzes, visualizes, and reports on them, or maybe preserves them, and then the cycle repeats. the digital medium is one of the best ways to communicate knowledge and is probably also the best way to create knowledge. the concept of a knowledge ecosystem, the whole system of producing, storing, and retrieving knowledge, depends on standards, metadata, discovery, association, and dissemination. one presentation focused on creating a precompetitive commons, using examples from drug discovery, where fierce competitors can cooperate to produce evolving models of diseases. cooperation can improve the translation of publicly available molecular data into biomarkers, and increase the opportunities for using drugs meant for one disease to treat another disease. the ﬁcrowdﬂ plus the ﬁcloudﬂ concepts are important to the future of the network. there was an assertion that science can learn about data sharing from the world wide web, which could be argued either way. discussants also mentioned exploring linked open data for interdisciplinary uses. access to many data is very restricted. also, there was much focus on the long tail of science, which is defined as those fields where the benefits of information technologies are not immediately apparent. there were conversations about quality, format, access, financial support, and policy issues. other topics that were discussed related to opportunities for innovation that we cannot even imagine right now, serendipitous innovation, still others focused on government policies that encourage or even require scientists to share their scientific data and information, such as the america competes act.the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.138 the future of scientific knowledge discovery in open networked environments  summary of the discussion of opportunities and benefits this session examines the opportunities and benefits in a 5 to 10year time horizon, so it is important that we understand the value proposition. do we really know what the benefits and the opportunities are?  building and sustaining knowledge networks and infrastructure can we say that one opportunity is to provide more and better infrastructure? if a common infrastructure is not there, is it possible to have such a common infrastructure so that anybody can use it? when we hear a term such as knowledge ecosystem, it raises the concept of a knowledge network. the discussion on the first day of the workshop about a nonlinear way of thinking or a linear way of doing science reminds us of a neural network. that is, there are many different synapses in the brain that connect different facts and different memories and experiences that lead to an understanding of a situation. the same kind of process happens in science. we saw the example of the pharmacological studies that found linkages between drugs that were used to treat one disease and that could be used to treat another disease because there was a common gene sequence. there was a connection in the network that showed that the same thing that is being used somewhere else actually does link back. this suggests moving toward not just linked data or a knowledge ecosystem but a linked knowledge system. that is, we could merge those two ideas. we could benefit across all disciplines and truly enable longtail sciencešthe science that is so removed from our experience that we never would have thought of doing it. we are likely in a period of persistent restricted resources, however. hopefully it is not a zero sum game, but assuming it is such an era, what is the best way to deal with that situation? do we need to think of ways of developing systems that could be used by many different kinds of scientists? are there general characteristics of such systems? again, we do not need something only for computer science; we may also want something for the agencies funding research. demonstrating the value of data sharing change is always going to be before us, so how do we work together in our publicprivate partnerships to adapt to change? we need to show the value of sharing data to our principal investigators. science is going to be done differently than it was by scientists 20 or 30 years ago, so we have an opportunity to redefine the science as well. we can imagine a scenario in which a scientist is speaking to a congressional staffer who has heard that scientists are just ﬁwelfare queensﬂ in white coats and wants to know why they are being supported. we need some convincing examples where data have led to important discoveries. earth sciences data and medical data could provide some useful anecdotes, for instance. we could survey the number of hours people spend analyzing the databases as opposed to the number of hours they spend reading traditional journals. for example, princeton university spends about $10,000 per faculty member per year subscribing to journals. if we could quantify the relative costs, we could put a value on the database. the bottom line is that the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 139 when talking to somebody who is not a scientist, such people do not necessarily start with the assumption that science is worth something, so we need more descriptive stories. for example, the digital library program is justified by pointing to google. we need more compelling stories that appeal to outsiders. also, a protein chemist cannot work without the protein data bank. there are many scientists who do not work the way it was done before there were digital databases. they are datamining researchers. if you take away the databases, their type of research would stop. you will get different answers in different kinds of research, however. for instance, there is much more to online astronomical research than the official national virtual observatory or virtual astronomical observatory (vao) projects. although the vao has led to many useful results, so far it has focused only on infrastructure, without building enduser tools. in astronomy or in other sciences, there are databases that are even larger than the protein data bank. researchers in these fields use them every day, but the results in some seminal papers may not be cited. astronomers use data collection facilities such as the sloan sky survey, the two micron all sky survey (2mass), and the upcoming large synoptic survey telescope. scientists see these huge databases as something on which they have come to rely. what they may not understand well, however, is what would happen if the medium and smallscale datasets that are attached to literature would also be openly contributed. incentives alone might not work. scientists may not be convinced that their datasets are going to be worth depositing, unless the funding agencies make that a condition of their grants. big science examples may be the wrong approach to convince scientists, however, because they are too common. in the physics community, researchers would rather work with the large hadron collider (lhc) than their own small data collections, because the lhc will have a huge amount of data. there are other fields in which the generation of the data may not be very expensive, especially when amortized over a longer time period, but may nonetheless be costly to maintain, because many people are involved or for some other reason. there also may be a potential problem in asserting that it is more efficient to share data, because if this were true, why are scientists asking for more money to do that? if researchers are going to be more efficient, they should be able to perform better science with fewer dollars. if this is not the case, then there is a question about the efficiency of the system. also, when we say that it is going to be more efficient, another issue is: where will the savings go? the savings could go into the analysis of existing data, rather than into collecting and organizing new data. put differently, the budgets may not go down, but the analytical capabilities could increase. the research funders could identify the different areas where data sharing is extremely important, and they could then establish a reasonable time span for making the data available. some guidelines to the scientific community could be useful in this regard and constitute an opportunity. for example, we might eliminate the 2year gap between obtaining the research results to the sharing of those results through publication. this could allow the timelier sharing of the methodology, the data, and even the codes developed for the study. it could lead to real benefits. too much specificity concerning what should be shared, however, can be a problem as well. if science were segmented into different categories according to a level of what should be shared, it could be counterproductive. a specific time frame for sharing data absent the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.140 the future of scientific knowledge discovery in open networked environments  countervailing considerations, such as the health insurance portability and accountability act (hipaa) or similar requirements, could be very useful and would signal to scientists that there is recognition that sharing is very important. it would encourage more scientists to start thinking about the next steps after they produce the data, what infrastructure they will need, how they will have to reorganize their own budgets and funding to do that, and so on. a rigorous deadline can be a problem. voltaire said, ﬁthe best is the enemy of the good.ﬂ everybody is quick to complain that data curation is too much work. what is the cost, however, to the nation™s scientific effort to peer review weak papers? in some fields, preprint distributions and conference presentations have essentially taken over the publication function, and the journals exist so that people can get tenure. a system relying on something like the cornell arxiv might be better for scholarly communication, if there were something equivalent to page ranking that would help people decide what they should be reading. if the publication system were less strongly tied to tenure, it might be easier for the universities to determine that they could use other metrics. this could save a lot of money if the universities no longer relied on the ranking of publications to determine who would get tenure. on the one hand, the promise of data sharing is becoming real. on the other hand, there is a powerful reinforcement of max planck™s observation that science proceeds funeral by funeral. in the health sciences, the progress of science may be slower than it has been historically. some scientific processes may even retard the pace of scientific progress. the most difficult organizations to consult for are the ones that are doing well, because if they need to change the things that they did to make them successful or rich, their view, of course, is that they are fine and they are not receptive to good ideas. the difficulty is to get the existing order to change their approach, when it is needed. this is a collective action problem. individual scientists who would like to fulfill their ideals as scientists often find that they cannot. framing the problem in terms of reproducibility of scientific results can be very appealing to scientists, however. this can be a way that gives them guidance about when to share data and what types of data to share. the same argument can be helpful in convincing people of the importance of openness in this networked environment, because this is a core principle of science. nobody argues that reproducibility is not important in the sciences, so this can be useful as a guiding framework for what steps are important. the climate data controversy at the university of east anglia in the united kingdom and elsewhere has indicated that transparency is also key. the average person is not going to reproduce those climate simulations, but citizens want some assurance that there is access to how the model simulation was made and what data came out of it. therefore, transparency and reproducibility are both important. we also may separate the different axes along which we can argue that there are benefits. we talked about the speed of communicating research results, so presumably the discovery of new results is one aspect. we have talked about value, and a couple of discussants reminded us that the value of discovery, at least when we are talking to the public, is predicated on the listener™s view of the value of the underlying science. if someone believes that many discoveries in the underlying science are not particularly valuable in the first place, being able to make more of them faster does not seem to accomplish very much. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 141 a strong case for value can be made in the biomedical sciences, particularly as research is translated into health care. for example, it might be possible to determine the real dollar value in repurposing pharmaceuticals. it might be worth picking an area in which the economic value is fairly noncontroversial, as opposed to something like basic mathematics or astronomy, and then start quantifying the value and take a look specifically at data reuse in those settings. one thing that has not been mentioned in this discussion, but was talked about yesterday, is the idea of data as an unexploited resource, because data have not been part of the traditional journalbased way of communicating science. negative results in early clinical trials were traditionally not reported, for instance. because doing that work costs money, there might be some quantitative way to get at the value of what is now being withheld. a couple of the discussants at this meeting were recently at a geosciences data workshop, and the question arose about how to demonstrate the value of initiatives that integrate data and make those data easily available. for example, congressional staff members are interested in problems such as energy, food, and water. if we are asking for additional resources to enable data sharing, it can be useful to tie that to solving some of the issues important to themšto medical benefits, for instance, or better ways of managing natural resources. that also bears on the discussion of incentives and mandates, because when scientists feel that their work is directly related to producing a social benefit, there is additional incentive for them to share the data.  speeding up the pace of science the repurposing of data can be a real opportunity in speeding up research. how do we learn from the world wide web? what are the inherent opportunities? companies are investing a lot of money and making advances in communication that people are using every day. scientists may have a problem in seizing the opportunities, yet everyone is using the commercial media to communicate, to make decisions, and do many other things. is there something we can learn from the commercial community to apply to science in this regard? can we encapsulate very concisely some of the opportunities or the benefits? in march 2011, none of the u.s newspapers had any headlines about the earthquake and tsunami in japan, because, of course, it happened early in the morning, and they had already gone to press. in the old days of newspaperbased journalism it would be in the evening edition before most of us knew what was happening. then when radio and tv journalism came along, they became the media that provided news during the day. now we can watch videos of the tsunami almost instantaneously on the internet. think about science at that pace. an astronomy course 20 years ago had films that were not very compelling. it was like playing chess. not only did you have to be smart but also very patient while the other player moved. to keep up with the modern world, the pace of science has to change. it is clear that data sciencešnot just the big data science but the type of science where we can find the appropriate data, pull it together, and quickly get it into a workflowšis what will change the face of science. we need to keep up with that kind of speed. much of the reason that we may use the internet as an analogy for how science can be done is because of the speed of change. just a few years ago, twitter did not exist. many people who use it now cannot live without it. we are being asked to solve problems that are changing at the pace of the internet. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.142 the future of scientific knowledge discovery in open networked environments  the geosciences are well positioned to react to this realtime aspect that was just alluded to, whether it is in the study of earthquakes, tsunamis, flash floods, or tornadoes. scientists have the capability today to bring that information to be used in real time within seconds of the event happening. this also bears on the point that was made earlier about tying the research to societal benefits that resonate with people in congress. what does this mean for the protection of lives and property? the rudimentary informatics are in place that can be greatly expanded in the future with the mobility that comes with the smartphones and other mobile devices, so that you do not have to be at your computer anymore to get realtime information. the web and semantic data, and linked open data, have had major effects, but not all science may be done at the speed of twitter. it is one thing to report some results quickly, which is what twitter and similar tools do; it is another thing to trust science that has been rigorously tested. for example, there is a 10year study on body fat as a predictor for heart diseases. a 10year study cannot be compressed into 24 hours, however. some things just cannot be hastened to completion. this comparison with the speed of twitter and facebook, therefore, can be misleading, because it is comparing apples to oranges. the problem with putting these sorts of ideas in a recommendation is that a policy maker might latch onto this and say we want science to be done overnight, which could be even more of a problem rather than a benefit. preserving the rigor in science is a very important message, but in the current system there is a major delay from obtaining the research results to publishing them. we should not say: ﬁi am sorry you cannot see my data, because that is how i am going to win my nobel prize 30 years from now.ﬂ these practices and attitudes are barriers to the goal of solving people™s real needs with science and to better understand the phenomena around them. the 10year study is great when you need a 10year study, but not if it is the primary way of doing science. for the next 9 years, you may have people dying who could have been treated by something that might have been ready to work. verifying, validating, and doing rigorous science, of course, are important. it is the sharing of information that can naturally speed up some of these processes, however. people in unrelated fields right now may not easily discover prominent work until it has appeared in the archival journal. this means it has been written up and the visualization of data is prepared, sent to the journal, reviewed, and gone through the publication deadline. there are faster approaches that tend to be very different from field to field, but the main issue is the delay of disclosure, the long time it takes to go through this life cycle, workflow, or ecosystem. we also ought to keep in mind that one size does not fit all for these processes. when we examine the management of data, we may be talking about longterm experiments or about datasets that are developed to help in an emergency. just those two instances present very different kinds of data types and uses. supporting interdisciplinary research most of the presentations on the challenges from the first day of the workshop were very interdisciplinary, and most of the presentations on the solutions that described what people are building were inside disciplinary boundaries. that actually has to do more with how the funding mechanisms work than with what people would work on in a natural setting, but that interdisciplinarity of the key challenges is important. what happens on the web is that a the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 143 small number of people who can work between areas can transfer huge amounts of information back and forth between those areas. that kind of work tends to be viewed as an ancillary process, not as a major part of doing science. further, most scientists who have done this kind of work did so at the peril of their careers and may have been fortunate. another point is the importance of informal communication. scientists are used to formal communication. some of the systems that were mentioned yesterday and some of what the astronomers are doing now have, in addition to the primary scientific channel, an extra social and communicative back channel. for example, informal communication is how scientists may get information from conferences they have not attended personally. journal papers are the end product that gets put in the library for historical reasons. that is why they are called archival journals and not always the primary reporting of the scientific activity. recognizing the need for much more of that informal interchange is also important. those are two key aspects of researchšan interdisciplinary approach and informal social communication. informal communication can be structured like the world wide web: as a network of information and knowledge that you can search and also find the linkages between them. we can take advantage of that type of structure, which does not eliminate the journal system, but improves the communication among different sciences, and not just within a single science. if we were going to try to find these commonalities that allow us to cross boundaries in data sharing, those are problems that can be addressed. there are highdimensional image formats, there are complex text formats, there are big table formats, and it does not matter which discipline community produces them. the tools for analyzing, visualizing, and integrating them depend on the structure of the data, not on the contents of the data. thus, we can look at what those different kinds of datasets are and what the solutions might be. we have talked about knowledge networks and collaborative environments as being able to communicate and get results out more quickly. one caution is that there are many models for those kinds of practices. as new ways of communicating and collaborating are developed, it will be important to think about what actually is happening (in knowledge and experience), how we collaborate and communicate, and how we capture that so that it can be reused and mined. one of the issues of citizen science and informal education is the quality of the data, how good the data are for ﬁreal scientific research,ﬂ and how to describe the quality of the data. that remains an ongoing question. the national ecological observatory network (neon) is sponsoring another project, called budburst. it is a project in which people with cell phones go around and take photos when the first leaves come out from deciduous trees and when flowers first appear, because that timing is expected to change with climate change. it is a very potent opportunity to provide a bit of informal education to citizen scientists. they try to figure out what a plant is, they learn something about how climate and water affect the plants, and when they upload their data, the web site offers some educational opportunities.   the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.144 the future of scientific knowledge discovery in open networked environments  effective incentive systems there is a 2003 report from the national academies press called sharing publicationrelated data and materials: responsibilities and authorship in the life sciences14. it has many kinds of recommendations that focus on how it is good to share research inputs and what it means to promote the progress of science. it probably has not had a tremendous effect, however. given that we have been there before, a case can be made for the greater use of deterrents. the stickversuscarrot approach has other dimensions. recently, there was a thoughtprovoking book called switch by chip and dan heath15, which talked about the elephant, the writer, and the path. it made the argument that you are never going to get the elephant to move in the desired direction with a stick. it has to be persuaded to go in that direction, and you need the right incentives, the right carrots. more important, however, is that you need the path to be paved. therefore, we should not focus just on the carrotversusstick approach. how can we lower the barriers for the scientists and investigators to share their data, information, and products with the broader world? not all scientists are opposed to sharing their data because they want to hold onto them for the nobel prize. they just do not have the time, resources, or incentives to be able to share them. agencies can focus on enabling and motivating the investigators to share their data, and on giving them the right tools for effective data sharing and data citation. it was pointed out that in one discussant™s organization, it is the bench scientists who are demanding national data systems to be built, but this is not happening broadly. the use of one of the u.s. climate change data systems has been adopted by 20 countries, and they are supporting it as well. it is the bench scientists who support this, because they want to do better science and they understand it. in that organization, they are the ones who are demanding, so a stick is not necessary. they would love to do it faster and have a greater impact, but they do not have the funding to expand their scope. the money may serve as a carrot or a stick, but if researchers had a little more funding they might expand their impact quite significantly. in that agency the bench scientists are demanding greater sharing of data, and they are the ones who are realizing that their work is going to be affected. they are changing the way they are doing science. they need some direction from the general science community so that they are not going to be wasting their limited resources. for them, the incentives can be additional funding and a clear direction on how they ought to be documenting and preserving their data. the separation of science into gathering and analyzing data would mean that people could be scientists with a smaller skill set than is now possible. such an approach could open up the field to wider participation by people who at the moment cannot easily do so, because they have not gotten all of the training to do both sides of the job. the heritage health prize is a $3 million netflixlike challenge to work on hospital data. it has motivated a huge amount of interdisciplinary effort at stanford university among the students and faculty in business, computer science, and biology, as well as medical doctors.  14 available at http://www.nap.edu/catalog.php?recordid=10613 15 chip and dan heath, switch: how to change things when change is hard, broadway books, 2010. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 145 people have formed teams to work on this prize, because there is no barrier to form such teams. many of these prizes have no barriers to participating if the person has the required skills. the america competes act explicitly authorizes the national science foundation (nsf) to put forth scientific challenges and to have reward money for solving them. it is being ﬁlegitimizedﬂ with the federal agencies. the question thus arises whether in an open network environment welldocumented crowdsourcing can be an opportunity for advancing knowledge discovery. policies of research agencies there was a very compelling diagram presented during the first day of the workshop about the optimum curve that depicted how much of the scientific community™s research investment should go toward data sharing. we could pose that question to the national institutes of health (nih) or to the nsf and ask them to develop a theory based on that and then to manage their portfolios accordingly. federal science agency managers may generally understand these areas related to data sharing. every agency has, in its own way, made a commitment to funding different kinds of resources. for example, nsf has been supporting the national center for atmospheric research along with other research centers for a long time, which has put those institutions in a very good position. also, nih, the department of energy, and the department of agriculture have provided a great deal of genomic funding, which has helped the funded projects to be able to harvest the resulting data and use them for other useful and interesting ends. a onesizefitsall solution, a monolithic approach that everybody has to use, is probably not appropriate. there are small and simple solutions, but there are many of them and they are interconnected, they are designed for specific purposes, and they are run at economies of scale. the nsf and other researchfunding organizations have their own mechanismsšones that are good for their scientists. they can offer solutions to their scientists in the way that the genbank or the protein data bank were offered, and the scientists can choose to use them or, in some cases, have to use them. for many scientists, however, that opportunity for a solution is missing.  the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 147  techniques and methods for development and study of automated scientific knowledge discovery  introductory summary of the techniques and methods discussed on day one alberto pepe harvardšsmithsonian center for astrophysics fewer techniques were identified from the discussion on the first day than barriers. one technique that we already have been using for discovery of scientific knowledge is to develop and install openaccess repositories that function not only for full text but also for images, data, and software. the need for alternatives to traditional ranking mechanisms for scholarship was also mentioned. for example, the university of southampton could be ranked higher than oxford or cambridge universities in the united kingdom, if we look just at the impact of the university on the web. therefore, when looking at the number of citations that a university receives, we could use not only traditional scholarship but also the peerreviewed scholarship that exists online. another technique that was mentioned as a need by many presenters is semantic computing, or the development of systems that take semantic meaning into account. as one of the discussants noted, computers are great at storing, managing, indexing, and accessing information, but in the future, they will also need to make sense of all the information that they store, manage, index, and access. many presenters also spoke about linked data and the idea of a faceted search. for example, we have seen the new astrophysics data system (ads) lab interface that allows us to provide an interface to the information as we search for it and to somehow filter down the information for which we are looking . yet another technique for scientific knowledge discovery is providing direct access to the data. the example of the ads labs and the faceted interface is relevant here as well. also, one of the presenters referred to using data as a filter to literature. in order to allow the greatest possible access and to enable discovery of scientific knowledge, we could use the data as a filter to the literature and not just use the publications as a filter to the data. versioning was another technique that was mentioned throughout the discussion yesterday. a presenter noted that we could probably use softer versioning systems to capture scientific data, practices, and the artifacts that are produced across the scientific life cycle. two types of integration were raised. one of the techniques required for the enabling of scientific knowledge discovery is the integration of existing scientific tools with web 2.0 tools, or simply with existing tools on the web. an example is the astrometry.net site. it is a research tool that is primarily intended for the scientific community. it determines the position of the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.148 the future of scientific knowledge discovery in open networked environments  images uploaded by users on flickr. this is an excellent example of a tool that seamlessly integrates with an existing application on the web. the second type of integration is with social networking sites. scientists can leverage the user base that already exists in these services to accomplish several objectives. the first is the scientific discovery process. how can a researcher use data and content that are already published by users on social networking sites to enable scientific discovery? another objective is to enable citizen science. computersupported technologies and social media can be used by nonscientists to take part in different scientific enterprises. finally, how do we use social networking sites to allow people to collect data, either citizen scientists or actual scientists who want to collect and publish data on the web? another issue identified in the discussion was applying largescale statistical methods and computational models of public scientific data to both predict and track scientific knowledge discoveries and constructs. one example was the heat map used in the biomedical research area that was described on the first day of this meeting. it examined the use of drugs against diseases. this is a kind of research that is made possible by the use of computational sciencešdata sciencešon publicly available scientific data. it also could be done in parallel with computational social sciences, which attempt to do exactly the same thing using public social data. some people suggested that we need discovery tools for scientific knowledge that are data drivenšthat is, they are bottomup rather than topdown. google uses distributed data to organize, rank, and provide access to information rather than using a prior classification scheme. researchers might do something similar and use the features of the data themselves to discover scientific knowledge. this is probably the most efficient and popular technique for scientific knowledge discovery today: data visualization. statistical visualization and other visualization techniques enable knowledge discovery and make possible the observation of scientific phenomena that data alone may not reveal. we heard about different examples of using visualization in astronomy, as well as in geospatial and atmospheric research. we may not be able to see some features of the data just by doing data analysis, so there are some phenomena that visualization allows us to understand and observe. finally, returning to the issue of semantics, the notion of using simple metadata and lightweight semantics to annotate the data was brought up. this was described as microlevel semantics. some presenters suggested that in some cases the use of lightweight semanticsšvery light annotation schemesšis enough to describe scientific data and provide metadata for the data that someone is trying to discover and explore. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 149 summary of the discussion of techniques and methods the title of this workshop is ﬁthe future of scientific knowledge discovery in the open networked environment.ﬂ although much scientific research is taking place in this open networked environment, it is not proceeding as quickly as it could, because of the various barriers that have been identified. it is important to understand these barriers as well as take advantage of the opportunities that we have cited to help us move more quickly into the open networked environment. it will be a mix of both bottomup and topdown approaches, however. also, both young and senior scientists work together and there will be success stories in both directions. computational models and semantic computing it should be noted at the outset that the discussion focused on data, rather than the scientific, technical, and medical literature. the sheer size of the datasets that are being managed is a potential barrier, but there is also a potential solution or a technique to deal with that issue. we can ship the algorithm to the data rather than ship the data to the algorithm. through web services or some other approach, we could allow a scientist to process the data wherever they are rather than bringing the data to the scientist. it is still difficult to store a petabyte of data on a desktop. this may be needed, however, not because it is hard to move a petabyte dataset, but because it is the way by which the data owner can control what other people are allowed to do. the idea of shipping the computational programs to the data makes good sense, because the software is much smaller than the data in size. the problem is that scientists often do not work with one dataset. they may be analyzing 10 different datasets. what will they do in those cases? ship software to 10 different sites and have researchers perform part of the work there, then ship it somewhere else, perform part of the work, etc.? it does not work that way. for example, geographers may do a simple analysis and have perhaps 16 geographic layers for analysis. therefore, the issue still comes down to access to data. researchers often need to develop an intimate knowledge of the data they are using. sometimes it is a great idea just to ship the algorithm to the data, but if scientists do not actually see the data, if they do not have the data on their computer, and if they are not running some tests and some examples and establish that sort of relationship with the data, then they also cannot run an algorithm. consequently, sending the algorithm to the data can be a great idea, but in some cases there is a barrier that has to do with how we manage scientific data. the following anecdote is relevant in this regard. years ago an astronomer started working with computer scientists. the astronomer™s interest was in distributed data mining, because she had data that the virtual astronomy observatory was making available. for the first few months of their collaboration, the parties were not communicating well. not only were they using different terminology, but they were not even talking about the same paradigm of research. they finally realized that the astronomer™s interest was to mine distributed data, while the computer scientists™ research interest was the distributed mining of data. they reached a compromise to use both approaches. it was relevant, because for it to be considered research in the computer scientists™ domain, it had to be computer science research, not astronomy research, so just downloading data to the astronomer™s laptop and enabling her to run an algorithm was not computer science research. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.150 the future of scientific knowledge discovery in open networked environments  to perform distributed principal component analysis (pca), we need to do an iterative solution on the pca by shipping parts of it to distributed data sites. the data are never brought to one central location. it is possible to get very good accuracy on the pca vectors in only a few iterations, however. that was an interesting result for both parties. they had to have enough network bandwidth to get the data to the astronomers, but also, for cases where that was not possible, to have the computer science researchers ship the code to the data. offering many collections of data for downloading is a fairly simple service that can be provided either by an individual researcher without much sophisticated work or on an institutional basis with a onesizefitsall approach through a repository, for example. when you query to database, however, you open up to a pandora‚s box regarding computational security. you need to make sure that queries do not run astray in various ways. these can be managed, but require a greater degree of operational sophistication. the last thing we would need as researchers that are encouraged to share more data is media coverage of a highprofile, security breach. one discussant noted that he has been lucky enough to work with some of the top people in the web community. he got his credibility when he gave a talk and had the word ﬁtheﬂ with a kill ring over it. he said, ﬁon the web there is no the.ﬂ however, he is still hearing: ﬁthe way to do this,ﬂ ﬁthe way to think about this,ﬂ ﬁthe community i work with could only do it this way.ﬂ the open network is not bounded narrowly, which means that some things will work for one group and some things will work for others. it is important not to try to propose the same architecture for all scientific data sharing. we can examine a litany of such approaches going back approximately 40 years. remember collaboratories and the access grid, and how that was going to change science? part of the reason it did not was because such concepts were built on a single model that worked great for some people, but not for others. we therefore should try to explore many different ideas, and not have a closed boundary: explore different ways of doing things. there will be some small solutions that are the only solutions that support certain communities, and we should not resist those solutions because they do not work for the rest of us. at the same time, there will be some very large communities or some aspects of our work that cut across the sciences, and those techniques may benefit greatly from the sort of soft infrastructure that was discussed on the first day of this meeting. it is important to remember that we are talking about a very largescale, multiplescience, and interdisciplinary set of issues. there is not going to be a single architecture that solves all of that, including the web itself. one discussant wanted to underscore a question that was raised earlier in the workshop about how effective some of these techniques are, particularly the textmining techniques. it could be very useful to find some specific successes for those kinds of methods of knowledge discovery. this is an area of technology that has had a lot of investment. it would be possible to make a lot more investment, but it would be helpful to understand the efficacy of the work before significant investment is made. another discussant wanted to know to whom the techniques or the methods or the improvements are specifically directed. there are many stakeholders involved. if the techniques try to address everybody, they address nobody. in the case of an openaccess the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 151 repository for the medical community to improve pharmaceuticals, who are those stakeholders? there could be equity fund managers, program managers, congress, the public, and others. they are all different, and they are going to look at it differently. if we do not associate the techniques, methods, and barriers with those people, we may miss the audience in showing them and explaining the impact. as these topics are discussed, the primary stakeholder at issue ought to be identified. when talking about openaccess data repositories, we should look at models too. much effort has been devoted to openaccess repositories for scientific literature and data, but various models could be added to the analysis, because understanding different ones can be key to the future of dataintensive science. other issues include semantics, data visualization, and data mining. the purpose of semantics is to help the computer understand. data visualization is important once there is something to see, while data mining is the computer discoveries that you have in a database. those are very different things, each of which comes from a different subdiscipline, but they all need to be dealt with each on their own terms. paul peters, who died in 1996, said something along the following lines, ﬁin the paleolithic age, there was a saying that information was food for thought, but in the future age, information will be the predator and the human will be the prey.ﬂ if you think about that, this is the ultimate insight in the presaging of data mining and what is really going to happen. information is going to be finding us, so how do we end up there? another issue is that if data are going to be a filter to the literature, then we need to be able to cite data and their connections to the literature. there are many issues in data citation; thus, it becomes much more important that we know how to define what we are talking about and how to name it. two more issues were raised in this content. the first is that reproducibility is really hard. simply having the code and the data does not give you scientific reproducibility today, let alone in 10 or 20 years. there is much more to making sure that things work together and that we can use that code effectively and intelligently to get the right results and be able to interpret these results. it is not at all clear that if we just had the data and the codes, reproduction of the results could be accomplished; they most likely would not. also, when discussing code reproducibility or reusability, brand is important. for example, there is google labs and all their products. there are millions of lines of code in source forge, however, that never get reused by anybody, simply because they are not recognized as being a reliable brand of quality software that can be easily obtained and plugged into different tools. data visualization visualization is one of the key tools that can enable knowledge discovery. visualization is important, because it is needed not just to understand the data after they have been analyzed but to understand them while they are being analyzed. for example, sometimes we cannot figure out what is going wrong with a thousandbythousand matrix, and it would be useful to just be able to see it and have something that would allow a scientist to do that. it would be good to focus on visualization tools for that reason. however, visualization is not sufficient the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.152 the future of scientific knowledge discovery in open networked environments  alone, because there is an infrastructure that needs to be in place before we can take advantage of visualization. data mining and data visualization were described as separate areas. there was a provocative conversation at one of the workshops on data and visualization that dan atkins and tony hey convened for their advisory group to the national science foundation™s (nsf) office for cyberinfrastructure. their topical area had covered both data management and visualization, and visualization seemed to be a very siloed kind of technique. we can take a more holistic view of data analytics, however, that comprises mining, visualization, and perhaps some other intermediate or allied techniques. it could be useful to think about these issues, because there is a tremendous amount of siloed work going on in various places, and as the data science scales up, a more holistic approach would be desirable. one discussant noted that the word ﬁanalyticsﬂ is seen frequently in the literature these days, but ﬁinformaticsﬂ better describes what we are talking about. informatics includes visualization, mining, statistics, semantics, knowledge organization, data management, data organization, and data structures. that is a holistic viewpoint. what was indicated during the first day of the workshop is that people are willing to use generic visualization tools that are repurposedšwhich they can get from companies such as googleš as a substitute for what they might have done with higherend software. we can call this ﬁmodular craftsmanship,ﬂ which involves people who are very good at making tools and making them more accessible to the point that anyone can plug them into a system. those tools will most likely continue to get better. such tools also could have useful connections to highly specialized data formats that might be specific to a field. the scientific community could use applications that are being developed commercially but are available free of charge. for example, a youtube for visualizing data, a ﬁviztube,ﬂ could be useful because it represents a class of data functions. there really is a huge amount of work done in information visualization that goes way beyond what scientists themselves have ever done. most scientists do not make the connection between amazing graphics in the new york times or the guardian that were produced with some easy, reusable tool and something that they could do for their own data. there is thus an educational aspect here, informing scientists that there are easytouse, reusable tools. there is something like this in astronomy, but it is not as easy and as wellbehaved technologically as, for example, what newspapers such as the new york times or the guardian do. the guardian is actually an incredible model for graphics and visualization. when someone takes these tools from the analytics or informatics communities that are generic and freely available, and then tailors them to a scientific use, there can be a hidden cost sometimes to make it work well. it may not be a major cost, but it does need to be covered somehow. that can be a problem, because it is not research in the eyes of the funding entities, even though it can be valuable in enabling much more research. one of the visualization issues is related to standards. scientists use tools to create the visualizations and other formats that can be put online. many of the formats and tools are proprietary, and the vendors who own them are not supporting many of the open visualization the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 153 frameworks. the scientific community could benefit from being more aware of the standards for these visualization tools and who is determining them. a similar situation exists for workflow software. the problem with workflow standards is that there are too many of them, and yet the tools of some groups do not support any of these standards. the tools are just a black box to the user. therefore, pushing the tool vendors and pushing the scientific community to be smart purchasers can be very important.  integration of scientific discovery tools with web 2.0 another topic concerns the social network sites. how do we get scientists to participate in them? that is a big question that has come up in the search for extraterrestrial intelligence (seti) community. the issue concerning the encouragement of more participation and how we compete for mindshare or people™s time then leads to the ﬁapp storeﬂ package. the app store can be a real opportunity to get more participation. apple put a lot of effort into building their app store brand to make it useful and productive for people. it may be worth thinking about it from a science perspective, especially how to build our professional brandsšan nsf brand or some other trusted brandšto indicate that a tool is high quality and should be generally useful to a large swath of scientists. also, a greater focus on publicprivate partnerships could be beneficial. academics then might be able to exploit the tools developed in the private sector more efficiently with the public™s money. one discussant from a company that works a lot with the national aeronautics and space administration (nasa) and the national oceanic and atmospheric administration (noaa) noted that he has been thinking about democratizing access to data and analysis for more than 15 years. the notions of citizen science and public data collection are still seen as fringe activities and not in the center of scientific research. the opensource community likes to say, ﬁif you put enough eyeballs on a problem, all bugs become shallow.ﬂ the same situation is potentially true of scientific discovery. for example, a few years ago, there was a wellpublicized case involving ordinary citizens who discovered some unique geological formations just by browsing the satellite imagery available through google maps or google earth. these formations had been there in plain sight, but no one had found them, even though relatively few of them exist, so it was big news. on a much larger scale, nasa is about to launch the npoess preparatory project (npp) satellite.16 it was never intended to be an operational satellite for applications such as weather forecasting, so a nearrealtime, dataaccess stream is still being discussed. there is an opportunity for direct broadcast systems to share those data for nearrealtime use.  evaluation techniques another methodological issue is related to how do people know whether what they are doing is successful or an improvement? the web metrics that dr. hey mentioned was just one example of a different kind of metric that now exists. dr. stodden had a slide in the discussion yesterday about peer review and how we evaluate computational research results that are published and the different modes of doing that.  16 national polarorbiting operational environmental satellite system (npoess). the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.154 the future of scientific knowledge discovery in open networked environments  there are techniques for evaluating the literature that are outdated and do not take into consideration the evolving web paradigm. they are based on the old print paradigm. that is one example of evaluation, and virtually everyone in the academic sector works toward those evaluation criteria, such things as the isi citation index. if they are not updated, those types of evaluation can retard the progress of science and not advance it. new tools, criteria, metrics, and indicators of progress of what is valuable and of what these new scientific technologies and processes enable could be useful. in particular, there are very few useful indicators for data management and use. tenure is not based on data work; it is based on outdated publication indicators. it might be useful to discuss the kinds of research that could help change that culture, and the metrics and the evaluation tools, to get a better understanding of what is actually happening rather than what happened a few decades ago. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 155 barriers to automated scientific knowledge discovery in open networked environments introductory summary and the barriers discussed on day one  alberto pepe harvardšsmithsonian center for astrophysics  this is a summary of the barriers that were repeatedly mentioned in the presentations yesterday. one of the first issues that arose from the discussion was that scientific knowledge discovery efforts exist at the national and international levels, such as the virtual observatory in astronomy, but without any proper coordination mechanisms. also, ad hoc data standards and protocols are sometimes developed and adopted at the national or organizational levels. the second barrier that was mentioned yesterday was the issue of interoperability. infrastructure solutions to knowledge discovery often are not interoperable. some people referred to the need for a googlelike search and data management service for science, which is something we do not have. some people mentioned different efforts in the virtual observatory that failed to interoperate in a similar way. the different systems that exist interoperate to a certain point, but usually not across different disciplines. funding was another issue that was repeatedly mentioned throughout the presentations. such things as data hosting, data management, data mining, and cloud computing can be expensive. what are publishable artifacts? what should end up in journals? some people referred to the importance of considering data, software, algorithms, and other supplemental materials as publishable items. regarding proprietary data issues, discussants addressed locked and unlocked data. some people referred to the rich genomic data that are held by pharmaceutical companies as an example of locked data. unlocked data include much of the data in astronomy. it was suggested that even with locked data, the owners of such data might want to allow data annotation, because it can add value to the data and, in most cases, makes them reusable. if data are not annotated, they may not be usable at all. another topic that was discussed referred to the multiplicity of efforts, solutions, and tools that have been developed for scientific knowledge discovery and data management. we have seen and discussed many of these efforts, and here again the interpretability issue arises. in some cases the various tools are connected to one another, but in other cases they are not connected in a way that makes sense and that allows the tools to communicate. one topic that was repeatedly highlighted in the presentations was the need for a standardized and widely adopted mechanism to cite and reference scientific data. although there are some efforts to standardize data citation techniques and mechanisms, there is not a widely adopted standard. for example, many scientists cite data online or just present links to data in the footnotes. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.156 the future of scientific knowledge discovery in open networked environments  there was also a discussion of sociocultural barriers. specifically, one of the questions raised was related to whether researchers or social scientists who are interested in scientific knowledge discovery fit in the academic or the industry job market. another question was related to how they get recognition for their work. this topic is connected to the next point that was raised, which is the issue of rewards. on many occasions, discussants talked about the changes related to the shape and role of the academic paper and that the reward and recognition systems, including such things as citation and authorship, are also changing. these functions are likely to change even more in the next generation. today there are not enough incentives to change to the direction of open scholarship and open data. in a way, as mentioned yesterday, such openness conflicts with the culture of independence and competition that we traditionally have seen in scholarship. a barrier that was mentioned throughout the discussion was related to the difficulty to integrate computer science topics or ideas into the curriculum of the various research disciplines. the refocusing of scientific disciplines and their curricula around computer science and data issues enables not only the solving of problems much more quickly and in an automated fashion but also the asking of novel questions. many presentations pointed out that big datasets already exist on the web and that science is not the only enterprise that produces large datasets. enterprises such as facebook and twitter deal with large amounts of data and other types of largescale usergenerated content, and they are very well funded. another barrier that was raised is the simple fact that data searches are hard to do. indexing, searching, and retrieving scientific data can be difficult. similarly, extracting knowledge from scientific data in publications can be problematic. for example, extracting hypotheses and results from scholarly articles is very hard to do. it can be accomplished using annotation systems, but usually not in an automated fashion. the theme of diversity recurred throughout the presentations. some presenters noted that computermediated scientific knowledge discovery has significant differences among scientific communities, disciplines, and institutions. the issue of disciplinary versus institutional repositories arose several times during our discussions as well. should there be data and literature repositories at the disciplinary level or at the institutional level? whatever we choose to do, interoperability of the two platforms is important. physical barriers and the fact that distance matters were also discussed. many large cyberinfrastructure initiatives assume that collaboration can just happen using computersupported technologies and that scientists can perform work remotely. what we have seen is that most of the time scientific output and trust improves with facetoface interaction. the issue of temporal barriers, the idea that software and metadata requirements change with time, was also highlighted. what we collect today may not be what you want tomorrow. towards the end of the day, we learned from the lawyers that the law does not always help. the laws that deal with data are not a clearly defined issue. it was also mentioned that with the current legal system, it is difficult to achieve simplicity. the system is designed for securing property, not for sharing knowledge. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 157 finally, the last point that was discussed was related to the idea that scientific enterprises become progressively more computational, and therefore data and code sharing at the time of publication becomes crucial to the reproducibility of scientific results. summary of the discussion certain problems may be common to all fields. we ingest data, manage large amounts of data, organize, analyze, visualize, and preserve them, and therefore the data life cycle is something that might be examined more closely. what many have said is that for us to be able to manage all this data, we ought to lower the barriers to be able to ingest, manage, and visualize the data. technical infrastructure a point about the need for googlelike search in management service for scientific data was raised during the first day of the workshop. there are a number of such mechanisms. one of them is for astronomy, as in the case of the astrophysics data system (ads), which was discussed during the first day of the workshop and which has complete user adoption. all astronomers use electronic publications, but they do not use the internet to access data. there are many interesting techniques, but we could benefit from having more of them and they could be more broadly available. some of these mechanisms allow us to find where data are, but they do not allow us to get the data. many of them are not at the point of allowing us to have the tools to use the data, so there are plenty of improvements that can be made. the worldwidescience.org and science.gov portals also were mentioned yesterday. these are tools for deep web searching that can be better than searching google for science. they have fairly good use statistics, although not like google, of course. another problem is that as more researchers can generate a lot of data, they cannot visualize them properly or even analyze them. it is becoming extremely difficult. as mr. dudley said earlier, he has a group of 20 people that have 500 computers in a room, but that is not typical. that can be a big problem, because scientists can generate huge amounts of data, but the data may just sit there or are lost.  institutional factors the greatest barriers can be based on the social or ﬁsoftﬂ infrastructure, not about the technical aspects. the technology moves very rapidly, and the solutions, even if they are not straightforward, are worked on constantly. institutional structures, models for communication from a human infrastructure standpoint, the legal aspects, and the funding mechanisms are the things that tend to change very slowly. also, they do not adapt very quickly to new technology changes and always lag behind technological progress. we are always catching up from an institutional standpoint. the point was made yesterday that we are still mostly stuck in the print paradigm on scholarly communication, even though we have moved wholesale onto the web. it would be useful to rethink how to organize scholarly communication and how to be more responsive and adaptive to the technological opportunities in a way that will promote greater productivity of the technology that is available. in this particular activity, we should look to the generation that is called the digital natives. if you know any staffers in congress, you know that many of them are young, and the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.158 the future of scientific knowledge discovery in open networked environments  senior members of congress tend to trust these younger people. one of the barriers that could be overcome is the standard structural barrier related to older people making decisions when younger people should be included more in the conversation. also, one of the most common explanations given when you ask somebody about barriers is the difficulty of doing the job. therefore, one issue is getting tools that will make a given task easier. on the soft barrier side, people who have the computing skills to do some of the data work find it difficult to be rewarded in the scientific discipline in which they are working, but also find it difficult to get credit in computer science if they are perceived as working as an assistant to somebody in a scientific area. the same problem exists in statistics. therefore, another issue is the unwillingness of universities to reward people for activities that seem to be helping someone in a competing department rather than in their own department. another institutional barrier is related to legal issues. a large midwestern university, which shall go unnamed, decided to review its copyright policies with respect to faculty. the statute of anne, which many people believe to be the first copyright act, was enacted more than 300 years ago, and we still have not figured out exactly what copyright is. at that university, the faculty were upset with a decision by the technology transfer office (tto), which had been given plenary jurisdiction by a regental bylaw over all issues related to intellectual property, including copyright. the tto™s position, in effect, held that university researchers may obtain copyright to their works, because historically they have received the copyright, but that the university administration would decide after the fact whether the researchers used extraordinary university resources. if the researchers had done so, then they had to share the proceeds with the university. the university faculty thought this rule was unfair and lobbied to get it changed. this evolved into a standoff between the tto and the faculty that lasted for 2 years before it was resolved in favor of traditional copyright privileges of faculty. this is an example of a routine stepbystep process that occurs in some universities regarding arcane problems that we would think have been settled by now. when a university™s budget is being cut by tens of millions of dollars a year, this does not sound like a very interesting topic to discuss at the regent level. a separate topic is the complex set of issues concerning the sharing of human subjects data, which has not been looked at nearly enough. one of the barriers is with the geographic information systems (gis) mapping. if the data are unavailable for the research, scientists can be disadvantaged in the gis market, which is one of the markets most in demand besides twitter and facebook. a key question is whether the gis being used is proprietary or open source. if we cannot get to the data, then we cannot do anything. we cannot create any maps without the data. there are no standards to release the data. if the data have not been released, the scientific results suffer as well. for example, data.gov only tells us what is there, but it does not make the data interoperable. data interoperability in the ideal world, it would be wonderful if everybody could reach everybody else™s data. simply being able to find the dataševen if it only gets users to a humanreadable page that tells them who to contact to get the actual datašwould still be a major breakthrough. unfortunately we are not there yet. data.gov is a great example. every dataset is available if it is in one of the catalogues, but there are other features available to find out more about the data. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 159 gis is a very important area, but the standards do not exist to interoperate. for example, the guidelines for putting boundaries on the map are not the same as the ones for putting items on the map, which, in turn, are not the same for reporting where things are. solving that is a complex problem in which the scientific community should participate, but does not have to lead. finding out if someone has done a study and has the ground truth pixel map for some region or that someone has the scientific data about the weather patterns can be difficult between different discipline communities. the geosciences community is advanced in the processes required for integrating and interoperating. it is important to separate those two problems and to recognize that the first barrier is finding the data. a second barrier is obtaining them. then there is the technical integration of the data. an important issue now being addressed is the soft infrastructure for finding it, for knowing who to talk to and what is in a database before investing the money rather than discovering afterward that it does not have what you want. the hubble space telescope archive is fairly large, but people do not have a problem finding hubble data. they know where to go and it is relatively easy. the problem is to serve the data, because the difference between the data that we have and the manner that we serve is gigantic. we serve ﬁnﬂ times the data we have in our archives, where n is a very large number. that number is not sustainable when we get to petascale data, and there is no infrastructure that we can develop that can support these kinds of requests. thus, the help that we need is in figuring out how to stop this from happening. we do not want to stop people from doing the research, but we cannot serve petabytes of data, because then we are going to become a service that just gives out data and never do anything else, because that would take much of our resources. this is a very fundamental problem: after scientists collect the data, how do they allow others to apply data mining to the data? how do they allow people to apply algorithms to the data without moving petabytes? we have separated the public outreach portion into a completely separate network from the research one. we know those numbers exactly, and they are very large. the issue is also that new instruments are going to come online and produce a huge amount of data. even if a scientist wants data for a small portion of the sky, the data are still considerably large. many people want the same area of the sky, however. hubble is a flagship mission, and it is possible to do incredible science with it, but how the hubble archive can sustain the data that scientists want served directly to them is not clear. interoperability also is a very important issue for soft and technical infrastructure. can we imagine a startup today, such as twitter, trying to build its own cloud without being interoperable, meaning not having an application programming interface (api) with which data can be mined? that cannot happen if someone wants to be successful as a company, but it happens every day in science, because the barrier is so high. how do scientists publish the data? they do not have access to some data programmatically, so that is a very high barrier that really hinders interoperability. interoperability is a deeper issue. it is not just about discovery and search. it is the ability to take somebody else™s data and mix them with your own data, which is what many scientists want to do, rather than work with someone else™s data alone. the barriers to that are technological, semantic, and legal. if licenses do not permit scientists to interoperate and mix the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.160 the future of scientific knowledge discovery in open networked environments  the data, they cannot do that. thus, there are many different barriers that go along with interoperability. interoperability is being conflated with discovery.  reward structures the issues of a reward structure, workforce development, and the computer science curriculum can be linked. the university department heads that are enlightened and have figured out that they need to add particular courses to their curricula may still forget the data science component. there are university departments that include the data science component, but not that many. informatics courses include things like data mining, visualization, statistics, and data management. this leads into the issue of workforce development. if we are training the next generation to take over the dataintensive science that we are creating today, then there ought to be courses and curricula that reflect that. this in turn goes to the issue of the reward structure. for example, one discussant spent 20 years in the national aeronautics and space administration (nasa) system and then went to teach at a university. he reached tenure in his late fifties. it turned out to be harder than he thought, despite having had a successful career and having published. there was a lot of resistance in his college of sciencešalthough not in his own departmentšto astroinformatics, or even just to data science, as a research discipline. why is that? one reason is that the university administrators do not understand it. a second reason is that most of the places where he can publish are not the typical astronomy journals. they are peerreviewed conference proceedings. in the physical sciences, unlike in computer sciences, peerreviewed conference proceedings have a much lower acceptance than traditional science journals. the impact numbers for astronomy journals are in the high double digits, whereas peerreviewed conference proceedings are in the very low double digits. he had success because he spoke up. he is a senior scientist now, so he can argue with deans and tenured full professors. younger people do not, so this is definitely a barrier. one possible solution would be to have some kind of mentorship or advocacy for young scientists who are doing this kind of work and who cannot themselves counter the arguments made by senior professors and deans who say that what they are doing is not research. younger scientists, unless they are very secure, will not be able to react this way. the younger generations need advocates and mentors to help in this process as they are trained in dataintensive sciencešor dataoriented science. statisticians may say that the data they are working with are not large data; they are complex data. it is not so much the size of the datasets, but the orientation of the research, that is different. focus of funding mechanisms and science policy another barrier is that the funding mechanisms for science in the united states are set up primarily along disciplinary areas. even within the larger funding agencies like the national science foundation (nsf), it is separated out. crosscuts for infrastructure and for interdisciplinary work are mainly done by each of the agencies trying to do it themselves. there is very little crosscut infrastructure work between the agencies, although there are notable exceptions. generally it is difficult to get funding for the crosscuts. the national academies have written a number of recommendations about the funding of interdisciplinary the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 161 work, but these have been almost completely ignored. both the technical infrastructure and especially the soft infrastructure are absolutely crucial for data work. over the past few years there has been a lot of emphasis within the federal government on scientific data policies and plans. for example, the new nsfissued policy guidance addresses issues related to data management plans. many research agencies are moving aggressively forward on data policies. the lack of an agencywide data policy was considered a barrier that set the stage for managing data as an agency resource. not every agency is doing this, because they are all somewhat different, but among the leaders are nasa and the national oceanic and atmospheric administration (). also, the environmental protection agency is moving quickly now to develop a complete agency data policy. in this regard, the report harnessing the power of digital data for science and society17 discussed data policies and data plans. the office of science and technology policy™s interagency working group on digital data is continuing the work on this. one thing that did not become a final recommendation in that report, because it was not acceptable across all agencies, was to have a chief data officer in federal agencies. data are an asset that needs to be managed, and thus, somebody who understands science and data may be needed at a senior level. there was a followup workshop that reemphasized all of these concepts, including the need for a chief data officer. consequently that is another challenge within the federal government. is it also a barrier in universities? to follow up on this point, the incentives of technology transfer offices for data and code dissemination are somewhat orthogonal to those of most research scientists. the primary interest of ttos is in licensing the technology or working with it in a commercial sense. it is worth recognizing that part of the question that is raised about legal barriers to sharing, at least in the context of this meeting, is a distortion of the traditional incentives for university scientists. that can be a very real barrier at the institutional level to data dissemination. following up on the comment about ttos, this certainly has been a problem for a long time. a counterforce has been emerging in the universities through the interests in institutional repositories, in open scholarly communication, and in other similar initiatives. we need to be careful where to ask questions in the university, depending on the answer we want to get. demonstrating work impact and value one of the biggest barriers within the scientific documentation community is how to prove that you have had an impact. how do you prove that the resources that you use to run an office of scientific and technical information or a defense technical information center really have an impact? there is a lack of metrics, so we are forced to revert to anecdotes, to cases that prove the point or at least incite the imagination. we tried to do that in the harnessing report that was mentioned earlier, and we got nice vignettes, but they are all philosophical: ﬁthis will change scienceﬂ was the assertion, but there was none that actually showed how it did change science. that is one of the biggest barriers, just proving the point. the anecdote problem is very real. it has been surprisingly hard to get more than a small handful of now overly retold good stories. it seems as if there are two models. one is a  17 report of the interagency working group on digital data to the committee on science of the national science and technology council. available at http://www.nitrd.gov/about/harnessingpowerweb.pdf. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.162 the future of scientific knowledge discovery in open networked environments  model of data contribution toward a collective resource that everybody uses, like genbank or the protein data bank, with many people pooling into a common resource to create peertopeer data sharing and reuse. that seems to be much more amenable to measuring levels of reuse and impact and maybe even to measuring contribution. for example, if someone says, ﬁi have sequenced four organisms that are in four species that are in genbank,ﬂ it makes a statement that is much more quantifiable than saying, ﬁi collected a lot of ecological data and shared it with three colleagues who used it in their studies.ﬂ that is the barrier to the reward structure. there is the reward structure, and then there is the business case, the value and the impact. regarding evaluation, there was a meeting in january 2011 organized by the federation of earth science information partners (esip). the people involved in this organization talk a lot about interoperability and discoverability of data. at this meeting, one of the guest speakers was ann doucette, the director of the evaluators™ institute at george washington university. one of the takeaway points that she raised is that if we want to develop metrics that prove the value of an initiative, we need to invest in that upfront. she said we should not wait until the end of the performance period before we ask the institute to get involved. she said if they get involved very early in the research initiative, her group can help do some baseline assessments, so at the end of an initiative that lasts 5 or 6 years, they will be able to develop specific metrics that people can use to prove how the investments have been beneficial. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 163 range of options for further research introductory summary of the range of options discussed on day one puneet kishor university of wisconsin  this is a summary of the options for future research that were discussed yesterday. the options may also be seen as opportunities. optimizing science for the community was one option. this is a metalevel issue, so it would be useful to distill it to specifics. other issues included analyzing knowledge as an ecosystem, rich authoring (integrating data with the literature), and semantic data storage. building a precompetitive digital commons seems possible. then there was comprehensive monitoring of many biomedical traits at the population level. breaking the siloed principal investigator mentality is important. that is an institutional issue; the funding agencies can enable that. another option was to have some sort of code versioning for data. other options included tools for adaptive modeling, lightweight integration instead of fullscale ontology, and incentives for those who collect data. this last issue came up repeatedly. how do you reward people for collecting data? discussants also mentioned the development of decisionsupport tools, simple metadata and better semantics for data, and making visualization a firstclass citizen of science. quick and easy visualization would be very useful, and it does not have to be complicated. for example, there are many fun things that could be in a youtube for science. simple metadata that can be searched and localized in different languages could be helpful. one discussant noted that we can change culture by changing practices. this is important, because the culture of different scientific disciplines cannot change by itself. incentives to produce results versus a mandate to share data were mentioned as future options: there now are incentives to produce results, but few incentives to share data. there are some mandates, however, to share data. finally, there were four licensing options for academic data: (1) do not put any label on your data, in which case, the default rule protects the copyrightable portions of a database; (2) put all of the data in the public domain with a waiver of rights; (3) use an orderly set of wellrecognized licenses, such as the creative commons family of licenses; or (4) have a complete freeforall, a babble of licenses. these are the four conceptual buckets for making data available with or without a license. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.164 the future of scientific knowledge discovery in open networked environments  summary of the discussion of the range of options for further research  fostering innovative approaches based on this discussion, there seems to be an important feature of the dataintensive phenomenon. this area is moving relatively quickly and in very interesting ways, and much of that is being done outside of the traditional channels of academia. for example, people are taking data and information and putting them on flickr or similar tools, even if they are misconstruing flickr as an archive rather than as a dissemination mechanism. therefore, there is much that falls outside of the normal boundaries of science and scholarship in the university context. the younger generations of researchers will set the standards. for example, facebook was a major global activity in only 5 or 6 years. it may be a young researcher who understands why all this matters and who will start something that is going to spread through the scientific community. in this budget environment, the topdown approach is not effective. one example is the superhappydevhouse located in the san francisco bay area. it is a huge mansion in cupertino, california, in which many hackers live. they open it to the public every month or so, and whoever wants to can go and hack all night. at least one very successful company started from superhappydevhouse. there is a similar idea in mountain view, california, called the hacker dojo. for a relatively small amount of money people have rented a huge abandoned warehouse, and the hacker dojo is open to anyone who wants to come in and start a company, for example. there are biohackers in there now starting a lab with some old polymerase chain reaction machines they bought on ebay. that is one interesting bottomup approach. we are in a new networked environment and in the middle of a change that we cannot completely understand. we can look at other parts of the networked environment to see what has happened there, and this creates an opening to examine the bottomup approach, which has resulted in many of the successful developments identified here and elsewhere. a scientist who identifies a problem looks for a solution. young scientists have new approaches. what do they do with their images, for instance? they deposit their images into flickr, so flickr suddenly becomes a scientific database. some hackers are also biologists. they identify data and they hack on them, and suddenly they are advancing drug discovery in ways that never would have been expected. many scientists might now recognize themselves in these examples. scientists have always collaborated in some way, but with the new digital, networked capabilities combining with the social reality, science is becoming even more collaborative. in the first day of this workshop, there was an interesting discussion about the sociological dimensions to these issues. there might be a new vector of development that happens through the research libraries community. there might be another vector that occurs through the vice presidents for research at universities. there may be other channels in which people respond with more immediate action. it would be interesting to examine what those channels might be. identifying, recognizing, rewarding, and supporting the entrepreneurs in this area is another potential approach. it could be useful to compare some of the entrepreneurial effortsšthe future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 165 wherever they fall on the generational spectrum, whether it is people coming through the research pipeline at the beginning of their career or people who are more senior and advanced in their careersšto better established models of research science so that the efforts are not characterized as so dramatically new and transgressive. this would both serve a rhetorical purpose, so that these developments do not come across as threatening to the various audiences, and show that what is going on in these instances does echo some very traditional understandings of collaboration in the sciences. many scientists have valued the sharing of their efforts and knowledge. making those values explicit would be a way of being authentic to what the progression is, as well as being politically sensitive to the audiences.  encouraging good network uses one interesting thing about the modeling of how the networked entrepreneurial versions tie back to more traditional understandings is that there is a lack of welldefined research on how these collaborative or commons mechanisms work in the networked environment. that is, there is a great deal of descriptive, qualitative, or anecdotal research on the character of science, but there is not much wellmodeled, empirical research about the role of the research infrastructure and networked platforms, and how sociological, economic, and legal factors are related. it is not necessarily an eitheror issue, however. there could be multiple messages going out to different communities and mobilizing those communities in positive ways. there are some arguments that are essentially inevitability argumentsšthat something is going to happen regardless of what the community wants. because if it is inevitable, we can argue that progress should be made quickly, but if it is inevitable, what is the rush? thus, if this change is happening irrespective of what people think or prefer, then one option is to get ready to take advantage of it. on the issue of the bottomup and topdown approaches, it is worth comparing the united states and the chinese management methods. there is a real difference in how the two countries approach research and policy formation. most innovators herešand also largely in europešwork from the bottom up. the system supports risktaking and failure, especially in the private sector, and so there is a tremendous amount of creativity and initiative that happens at the working level. then finally, the policymaking community catches on, and ideally it institutionalizes approaches that are productive. the chinese model is very top down, and the potential innovators at the bottom generally will not take initiatives that are not approved. although the bottomup approach has produced a great deal of creativity, it has many inefficiencies because there are many dead ends, it is uncoordinated, and it does not always work. each system has its good aspects, but neither performs optimally. the bottomup issue is important for this discussion, because virtually all the examples and everything that has been discussed has happened at the initiative of individuals and communities. from the government topdown perspective, it is worth noting that significant change is occurring. consider the tenor of the reports, or look at the programs that now include scientific data. the focus used to be on the grid until that first atkins report came out.18 18 revolutionizing science and engineering through cyberinfrastructure: report of the national science foundation. blueribbon advisory panel on cyberinfrastructure. january 2003. available at http://www.nsf.gov/od/oci/reports/atkins.pdf. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.166 the future of scientific knowledge discovery in open networked environments  one approach to network citizenship from the top down is to emphasize the ﬁnetworkﬂ and articulate the values that are implicit in the idea of the network as the hub of that conversation. a related, but distinct, approach is to put more emphasis on the idea of values and citizenship as part of the scientific research community. that is, how do the traditional values in that domain get expressed in this new technological environment? the theme of the good network citizen is very interesting. one discussant noted that he works with a project called vivo (see http//vivo.ufl.edu), which uses semantic faculty profiles for ﬁenabling national networking of scientists.ﬂ vivo has been trying to surpass that 80 percent completion rate that is typical for linkedin members who get their profiles up to date. vivo is aiming to get an automatic 80 percent from administrative data and from bibliographic resources, but getting beyond that 80 percent rate has been difficult. one of the things vivo has done is to use information in grants. they have been able to do that with some of their funding because the national institutes of health (nih) and the nih national center for research resources were amenable to supporting that. vivo has six projects that will have some interesting results with short turnaround times. the 1970s film network has the classic line ﬁi™m mad as hell, and i™m not going to take this anymoreﬂ by a broadcast journalist working in the television networks. that was superseded fairly soon by the current network, which is entirely different. the discussion here has made it clear that the emerging network is an incredibly enabling, individually empowering, and selforganizing system. it is the kind of technology that supports bottomup research and business, but also enables a google or a facebook to arise and become a multibilliondollar enterprise. the new network™s versatility is potentially available for all the science examples described in the past day. the characteristics of the internet are important, because this environment can completely change the way research and data sharing are done. it is also important to identify the type of research that is needed to get a better understanding of how to make better use of the emerging network for science and applications. issues in reproducing or reusing data facebook and twitter open up the science of social networks in a tremendous way. for example, consider data mining capabilities: when you have a huge portion of the planet connected like that, there is a big opportunity for research, just to learn from data mining. existing social network firms certainly lend themselves to statistical analysis and research problems involving large datasets, but there is a lot of informatics work that could be done by simple tools that still needs to be funded and developed. as an example, one discussant worked for a confocal microscopy lab. the images from a confocal microscope are not flat, twodimensional images that you can put onto flickr. they are threedimensional. they have multiple channels for different wavelengths of light. in short, they are multidimensional images that a scientist needs to be able to visualize in three dimensions, turn around in a computer, and isolate the different channels of light. flickr will not do that, so special tools are necessary to make those images useful. furthermore, there are about 150plus different file formats for confocal microscopy, and it is a huge problem, because most of those come bundled with the microscope as part of some proprietary software. the data become unusable after a couple of years, because nobody is using that software or that microscope any longer. that is a good example of a failure of the the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 167 bottomup approach, because the demand for a better file format has not come from the scientists who use those microscopes. they might wish that it would happen, but when they purchase the equipment, they are purchasing a lens first, and a file format is not high on the list. the researchers have not yet come together to demand that the vendors provide a better file format. another discussant had a different experience, however. there was a pdf from highdimensional datasets that she coauthored and published in nature. part of the point of doing that was to be able to take the file formats, of which there are many proprietary ones and have a format in which people could publish the data, not necessarily in their original richness, but in a way that somebody could reuse them. there are many people in the vanguard of this community who think the pdf format is an obsolescent format. however, this discussant and her coauthors were able to do that because she was at one of the hacker places in silicon valley on a trip, and learned that it was possible to easily make threedimensional (3d) pdfs. peter jackson employed so many 3d engineers in new zealand that some formed a company called right hemisphere and then licensed its technology to adobe. adobe had put a 3d feature in its free reader that few people knew about. the authors made a deal with nature that if they got their paper accepted by that publication, the paper would have 3d content that people would be able to use. the way that right hemisphere normally makes money is with car companies and other companies that use computeraided design in their pipeline. they set up milliondollar customized pipelines for a community that has 40 or 50 different file formats and wants something in a standard formatša job, a tool, a pdf, a powerpointšto come out the other end in such a way that their engineers and their marketing people can just press a button. nature thought it would do this on a trial basis and then get a million dollars or so to buy one of these tools for the magazine. the only problem was that they never got the million dollars to set this up. the discussant and her colleagues are now about to publish the first one in the astrophysical journal. it may take another 6 years until many more researchers do it. the idea that people do not care about the file formats they are using or that they are not interested in having their data reproduced or reused varies by discipline and application. the kind of investment someone would make would be to find some smart people who could develop a streamlined way to deploy this milliondollar system on the web so that others could instantly turn those 40 different confocal microscopy formats into a pdf that they could just send to nature. that is the kind of thing that would enable sciencešnot the actual creation of the content of the article, but the pipeline for making that a whole new kind of article. there may be one group that could be a very receptive community of end users. there are many resource managersšfor example, people in state governments in various areasšwho tend to be very appreciative when someone from the scientific community talks to them about what their needs are. it is so important to get the research results to the end user. it is not just putting the data out into the world; it is the data usability that is so important. in many cases, simply having access to the data does them no good at all. they do not know what to do with the data until the scientists and technicians provide the tools. it is important to think not just in terms of publishing research papers and data but also how those results can be used.  the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.168 the future of scientific knowledge discovery in open networked environments  reviewing computational research results one point dr. stodden made on the first day was that the thing people hate to do more than anything else is to review someone else™s software. it therefore is unclear how many people would review software that someone else has written. there may be some ways to do that, however. if a piece of software has 500 or 1,000 users, that is a fairly good indication that it is a usable and useful program, and the review could then be much less difficult. this issue comes up frequently in the discussions about reproducibility and other aspects of working science. a code review as part of the peerreview process before publication is difficult, because it is an enormous amount of work that people do not feel equipped to do. there are middleground approaches that might be able to be implemented, however. nature published two articles in october 2010 on software in science that discussed how it is used and often broken, that it is generally not reviewed, that it should be open, and so on. mark gerstein, a bioinformatics professor at yale, and victoria stodden both wrote a letter to nature commenting on this and saying that the scientific community needed to move toward code review. they suggested a broader adoption of what some journals have done, which is to have an associate editor for reproducibility who will look at the code and try to reproduce it. then it would not be such a burden that is imposed on reviewers. that is another possible way forward, having code submitted, made open, and then incorporating this aspect of review. nature did not want to publish that letter, but it is on dr. stodden™s blog, and the idea is being discussed further. another discussant noted that she has reviewed code plus data for journal publication and an example of the kind of problems that come up is ﬁthe compiler did not run because they changed something in the most recent version of unix.ﬂ every now and then you find real code errors. there are a number of domains that do this as a regular task. a list of approved software is not likely, but different audiences and different kinds of followup are definitely worthwhile. incentives versus mandates of the 10 stories that wired magazine listed as the biggest science breakthroughs of last year, 5 seemed to be based on the use of databasesš2 concerning genbank and 3 concerning astronomy. the year before, there were three genbank successes identified, but no astronomy and no earth science breakthroughs. there are two fundamental human motivations: greed and fear. greed is working, but very slowly. this carrotœstick dichotomy indicates that it is still something to be sold to researchers rather than something that is a socalled ﬁkiller app.ﬂ should we be appealing to fear, telling researchers that the real problem with not having data is that somebody in india or china will be running rings around them? perhaps the most effective approach is to pick one message, whether it be enforce the data management mandates, support tool building, or support sociological analysis of what scientists do with data, and describe why just one of these messages should be used and focus on it, because 15 seconds of attention is all that we are likely to get from the research community and the decision makers. much of the successful work happens through bottomup approaches, and this is a lesson we should learn. identifying and rewarding the people who are already doing this is a the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 169 strong option. focusing on what can be done from the bottomup to acknowledge good work and to help keep researchers doing that work can get them more visibility across the sciences and other benefits. the web can be a very forgiving medium in the sense that if something wrong is fixed, it is okay as long as it is fixed fast. the scientific community generally does not have the kind of culture that gets things moving quickly, however. we do a lot of design and architecture. there is nothing wrong with that, but as this area evolves from the top down, the bottomup efforts can be rewarded as well. journals also can help enforce reporting guidelines if there are standard metadata on which you can get a community to agree. in this case, journals can tell authors that anybody who submits a certain kind of experiment has to include certain information about their experiment. they are effective in applying pressure to authors, because publishing is one of their motivating factors. regarding prizes, a couple of years ago something like kiva19 was proposed for scienceša type of microloan that researchers could apply for, say, $50,000 from the national science foundation (nsf), to try something new. perhaps it would be $100,000 or $200,000, depending on the scope. to determine how scientists should meet the data management plan requirements, we could have a contest for a couple of years where people could try different approaches. if a technology has the right branding, then other people will discover it more easily. referring to the ﬁapp storeﬂ example, the app store has applications that get recommended by apple, and suddenly millions of people download them. another option is for nsf or some other entity to have a competition with small grants to find approaches that seem to work as widely applicable solutions. the prize would be that they get a brand that says something like ﬁnsf approved.ﬂ another similar suggestion focused on some sort of sandbox, where people can experiment with different approaches. the people who would take on these kinds of projects would not take on all the ones that were listed earlier in this summary, but they would have various missions. some individual investigators might accept a 5 percent tax so that technologies for data management systems could be developed. there is already a group of people who are reusing data and making good discoveries. we have seen some policies started, such as the call for data management plans, which are helping to get people who produce data to think about managing and sharing them. one of the best incremental steps on the production side right now, other than letting some of those mandates play out and trying to extend them to other funding agencies, will be to continue to encourage people to contribute to collective databases. there are many mechanisms for doing that, such as journals requiring registry numbers. we actually know a good deal about how to get collective databases implemented successfully. another option that can be pursued is to enable innovators who are doing productive work. this can be done in different ways. one option would be to highlight the researchers 19 kiva is a nonprofit organization with a mission to connect people through lending to alleviate poverty. leveraging the internet and a worldwide network of microfinance institutions, kiva lets individuals lend as little as $25 to help create opportunity around the world. see http://www.kiva.org/about  the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.170 the future of scientific knowledge discovery in open networked environments  who are responsible for breakthroughs, as suggested earlier. another option would be to initiate a prize for the most innovative piece of data reuse for the yearšnot for the people who supplied the data, but for the person who had the bright idea and went after the data and did something extraordinary with them. another option is to work with people who are producing results by reusing other people™s data to get them more data. if the theory is that ﬁmore data wins,ﬂ let us try in some very focused areas to further enable the people who are most active. the nsf data management plan can be seen as the first iteration of the network plan. that is, from the topdown perspective, the way to break through the discipline silos is to require people within each silo to say how they are going to use their funding to be good network citizens. in what way are they going to be a responsible steward of the research funds with the network in mind? tell us how you are thinking about the network in your plan to use these resources. will you make your database available so that it becomes a networked resource, and will you annotate it? the point is that if government agencies start requiring people to plan for using the networks, that could lead to a powerful shift in the way people think about it themselves. if a checklist of the properties of a good network scientist citizen were compiled, not every scientist would check off all the boxes, because science is heterogeneous. if scientists checked a certain number of those boxes, however, then you would tend to move toward those goals. that would be a documentable approach. when a scientist submits a proposal for funding, he or she can point to a previous project and say, ﬁi was a good network citizen based on this.ﬂ for example, one of the boxes could be about reproducibility. if a researcher™s field is not amenable to reproducibility because of the size of its datasets, it might not apply, but it would apply to someone else. there would be some basket of these properties that would determine how good a network citizen you are. that would be something measurable and doable. the department of defense (dod) has gone completely netcentric. it talks about everything in terms of being netcentric. it might be interesting to look at what the dod is doing and see how civilian science agencies might think about netcentricity. keep in mind, however, that the dod is very commandandcontrol oriented, so it can do these things top down. a study could be conducted to look at the impact of the nih access policy and ask whether it should be extended to any other federal research agencies. other agencies also could follow nih™s lead in requiring that those who submit grant proposals include in their bibliographies not only lists of their papers but also lists of the places where that information is publicly available. when a scientist reports on results of prior research, the scientist could list where those data are publicly available. the national library of medicine (nlm) has developed many process metrics. these include, for example, determining the percentage of the documents produced with nih grants that are getting deposited in pubmed central. that number has substantially increased. at this juncture, rather than investing heavily in making data available, maybe time is ripe to put some resources into the people who are actively trying to get the data to do some valuable scientific work. maybe that would be a good strategy for a year or two, and maybe that would be a good thematic response.  the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 171  developing the supporting infrastructure the nih™s national center for research resources (subsequently abolished) and the chinese have a strategic plan for research that includes infrastructure development and funding. since there is a lot of talk about funding the enabling tools for research, one option could be a similar kind of mechanism in the u.s. government for general science, not just for biomedical science. this discussion has identified a problem or barrier to the development of infrastructure that crosscuts across science domains. this could be further investigated at a higher level. in fact, some previous reports have noted that, and the president™s council of advisors on science and technology (pcast) report on the federal networking and information technology research and development (nitrd)20 program also looked at this. this infrastructure is hard to develop, however, because it depends in part on a large research group that can support it. the community would have to determine how best to promote and cite that to help solve some of these problems. a topdown management approach is one option, but it could also be good to think about doing it through bottomup scientific innovation. whatever approach may be taken requires an advocate to make that happen, and it cannot be done very easily. technology transfer mechanisms the role of the university technology transfer office in all of these processes remains a question. the typical instantiation is toward promotion of commercial interests and licensing. what is the relationship between the technology transfer office at the institutional level and the funders themselves? could that relationship be a bridge toward addressing some of these unfunded areas? if there is a way to follow up on this idea of a public arm of the technology transfer office, that could be a way to disseminate more broadly the technology, information, and datasets that have commercial potential, but also to do other things, such as resolve ownership issues. if academic datasets were shared more broadly, we would see certain patterns evolving, and those could become templates for how those issuesšin a legal and a citation sensešcould be sorted out. the technology transfer offices could be one venue in which to build partnerships to encourage this, at least at the institutional level. would there be a possibility of organizing some projects? among the universities and other entities, could a few demonstration projects be organized to show how we see it being done? they would experience various difficulties, but in the end they could see how it can be done. if a project cannot get funding because it is too ﬁapplied,ﬂ could the nsf have a technology transfer operation? the idea would be not to fund things that are possible to be commercialized, but rather to fund things that would be used to advance scientific research. if there were a genbanklike division at nsf, for example, it could encourage the application of the results of research that it funds, although not necessarily all the way through to commercialization. in some cases, such as drug discovery mechanisms, it could lead to commercialization, but in other cases, such as discovering stars in the sky, probably not. 20 available at http://www.nitrd.gov/. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.172 the future of scientific knowledge discovery in open networked environments  if nsf did have a technology transfer concept, how would it be different from a university™s technology transfer function? the nsf does have the interdisciplinary office of cyberinfrastructure, which is supporting the development of new software systems, but it tends to be a much higherlevel activity at the moment. one of the challenges is determining if this could be done at a specific domain level, or whether we would want to do it more generally. in addition, a 5 percent tax on research budgets is an option that could be explored further. it is not clear how many people, particularly the program officers at funding agencies, would endorse a 5 percent tax of the funding, but they might. maybe the agreement could be that if one agreed to pay the tax, it could come with certain benefits, such as access to some repository. that is, researchers could have the carrot and the stick at the same time. another model within government science agencies could be to collect technologies that might be broadly used from various directorates or projects and have them available in one place, or at least have pointers to all of them. this is already being done to some extent, but informally. technologies are created using public research funding, and if they are of value, then they may be put on the web to let others know about them. there is nothing very systematic, however, that gives nsf or some other agencies credit for having funded something that resulted in a technology or a tool that can be broadly used. so one question would be: is there a mechanism within the office of cyberinfrastructure that could be used to make these tools or technologies available? there are other government entities that are not explicitly funding agenciesšnitrd is a good examplešthat have the mission of supporting information technology (it) research and development and have the use of that it for national priorities, including science. nitrd could be a good ally in this area. the nitrd director is very interested in data issues and dataintensive science, but the organization has no money. all it has is the right to convene groups of experts, mostly in it hardware, but it could be beneficial for nitrd to be involved. the recent pcast report that was discussed above makes the point that the issue of infrastructure needs to be moved up to a level where it is not individual agencies competing with the individual scientists™ budgets, but rather at a level where somebody is looking at this infrastructure as a national priority for innovation and science. there is now much more discussion at the top levels and the research agency managers are beginning to see the value of it. it is harder to do than it sounds, however. at the university of california, for example, anything that costs more than $500 has been considered capital equipment for many years. this is not rational, because the university has enormous amounts of space. it has junkyards of technologies that cost more than $500, because if they were more than $500, they have to be on the capital equipment inventory, and it costs more to dispose of them than to store them. the reason for such a ridiculously low limit is that anything that is considered to be capital equipment does not get taxed for overhead on grants. the principal investigators have wanted to keep the capital equipment threshold as low as possible, because it benefits them in the grants. it sounds preposterous, but california still has a $500 threshold for capital equipment, and it has been this way for some 15 years. publicly funded research organizations are doing projects that are overlapping or similar to each other, whether in medicine or climate change or other areas. it would seem as though they could repackage those projects to demonstrate the multidisciplinary needs and the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 173 benefits of such work. such projects sometimes also have international partners, such as brazil, australia, and europe, for example. that would allow them to demonstrate something similar on an international scale. the virtual acquisition office science advisory committee made the mistake of asking the members of the committee to come up with test projects. some people were very opposed to limiting something like that to a small number of investigators. a regular (small) grant competition could specify that it is closer to infrastructure than research. this would have to be made extremely clear to the reviewers, because otherwise they will not understand. grant competitions have become a grandiose vision with millions of dollars to do research. that was not what was originally intended. cultural change another important feature of this dataintensive phenomenon is that there is a general sense that we ought to change practices and change culture so that the new and better ways are accommodated along with the existing ways of doing things. how do we change culture by changing practices? how do you change culture? how do you change practices? it is an easy thing to say, but it is a very hard thing to do. it takes a lot of time and effort, and often a lot of pressure, because people™s interests at the local level are not served by changing practices to which they have become accustomed. you can also turn it around to make it ﬁchange practices by changing culture.ﬂ the idea that you need voluntary kinds of encouragement to stimulate open access has been found to not work very well. that is why we have had to resort to mandates, whether through the journals or legislation or funding mechanisms, to change both culture and practice. at the same time, so much has been added to the list of mandates that accompany proposals. we have to promote minorities, help kœ12 education, and so on. of course, those who submit proposals claim that they are totally behind these mandates and support them, but frequently they do not do anything to advance these goals. one of the things that was proposed to the nsf leadership was letting grantees pick what they are going to affect and then incorporate that in the proposal. these interest groups, however, have worked for a long time to get themselves into that mandate list. the fact that not much happens from them being in a mandate list is not as important to them as being in the mandate list itself. there is a kind of culture of incumbency just around being listed. it does not necessarily have anything to do with meaningful change. we can push hard on this changingpractice, changingculture approach, because it is hard to do. what can the agencies do to help change practices and cultures? probably no amount of studies will result in changing practices. adding something to the mandate list may not change practice either, because the mandate list is already so long that it is just an exercise. the steps involved in influencing culture are not interchangeable, however. the way to change culture is by changing practice, and this example of nsf requiring data management plans is a good first step, although it is a very tentative step, and there is still a long way to go. everyone now is holding workshops on how to write the correct data management plan. maybe in 3 years there will be many more people thinking that this is a good thing to do, because they would have been made to do it and they would have started seeing the benefits of doing so. thus, there could be some value in recommending specific things for changing practices. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.174 the future of scientific knowledge discovery in open networked environments  one answer to this question is the old adage ﬁyou get what you measure.ﬂ if we are not measuring the impact of the data plan, we are not tracking it, we are not enforcing it, and then nobody cares. suppose, for example, researchers were asked how many minorities they supported on their last nsf grant? if it was onethird of what was promised, then the researchers could get onethird of the requested money on the new grant. you would suddenly have many more people matching those mandates. it is easy to talk about measurement and enforcement. it is harder to make that work. we may be coming to a point where we need to start thinking about what is going to work. the nsf seems to be trapped in this situation where it is supposed to be getting ideas from the community about where to go with the science. although there may be many people interested in this issue, the population of people doing science and being funded by nsf is much larger, so the view being expressed by those sufficiently interested in any particular issue is a minority view within that sourcing process. how do you change this? where are the pressure points and the leverage that will work, given the limited resources available? another discussant made a comment about the sociocultural attitudes that we may wish to change. reflecting on the tradition of science, it is important to celebrate that tradition and use it as a basis to build upon. understanding traditions is very effective in introducing change to different cultures. in working with different cultures you discover that you need to take different approaches. if, for example, we are talking about the transmission of hiv in a culture that has polygamy, it is irrelevant to talk about being faithful. therefore, if you know the important traditions to select, you can build upon and influence that change more rapidly.  role of libraries in setting up a national informationorganizing center that oversees and manages standards and vocabularies, it is important to remember that when data and information are created, they may have to be maintained over long periods of time and adapted as things change. therefore, there is a certain amount of infrastructure that has to be supported from the top down. it is not possible to get universities to do this in a distributed way. libraries are giving much of the advice for data management plans right now. research libraries have seized upon this as an opportunity to do outreach to the science departments at their universities and to help them figure out how to do the data management plans that the researchers are now being required to do. hence, research librarians are allies on the ground at universities, and they reach out to the scientists to help disseminate cultural ideas and strategies that funders and policy makers would like also to see implemented. one of the big questions circulating in the library community currently is what to do with print collections now that we are moving much more toward digital materials. there are a number of issues that have never been encountered before. we have been building up print collections for the past several hundred years, so we know a lot about that. we do not know much about reducing the collections, however. it is hard. it is more of an ecological approach than telling others what they should do. a problem is that it is very easy for scientists to say that issues or concerns expressed by one community have no corresponding value in their community. there are many ingrained practices, attitudes, and traditions that are candidates for change, but can we deal with them the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 175 sensibly? research librarians may be able to provide assistance to scientists that would not otherwise be available, particularly at the college and university level. one thing that may be helpful is delineating the roles of different kinds of experts who are involved in the data management process. the funding agencies are like the data investors. the data producers are interested in their data, but not necessarily in how the data will be reused. a problem in some cases is that research funders want data producers to let other people reuse their data, but it is extra work for the producers to deposit their data someplace and to annotate the data in a way that makes them useful. then you have the data reusers or analyzers as well as those who are the intermediaries between that data producer and the data reuser, acting as the data translator, manager, or marketer. this latter type of person will help the data producersšwho really do not want to do this kind of work or think they do not have the tools available to do itšbecome aware of the practices or tools that exist for them to do this kind of work, and also manage the data and train them in how to do that more efficiently. research librarians do that kind of work in genomics, for example. in addition to the nih center for research resources that was mentioned earlier, there is another institute at nih that deals entirely with information science and information resources: the national library of medicine. there is not an equivalent library for the basic sciences; although nlm has done some work expanding its focus into other related areas, it does not cover geosciences and other disciplines. in the agricultural research service (ars) there is the national agricultural library. the ars includes librarians in research project planning and data management. without trust among the parties, however, we cannot open up a legitimate dialogue, innovative direction, and mutual support. how to build trust really depends on those who are involved in a research project, but without trust they may fight against each other. finally, one of the options that was suggested for changing and improving data management practices is to reduce the costs. better leadership can direct people to what those practices need to be. one research agency has a saying concerning data management: ﬁshould we, could we, and will we?ﬂ there is no argument about whether data should be managed, but the scientists want to know how to do that. clear answers can foster effective leadership in different disciplines for preserving data. some sort of basic minimum guidelines would be helpfulšif not necessarily just tools or methods. technologies have changed what we can do, but they have not changed what we do. appropriate leadership can help us define what we do. communicating and influencing understanding of scientific knowledge discovery in open networked environments it appears that there are at least four communities that are listening and thinking about these issues. one is government leaders and elites, who believe that there is something big here and are looking for guidance on how to think about it. then there are the people in the institutionsšlibrarians, research leaders, and othersšwho are thinking about these issues from a different perspective. for example, many state universities are now focusing on what to do with the flagship institutions of higher education among the others. the flagship schools are ostensibly the research institutions, but the distinction that some schools are for research and some are for teaching is somewhat dated. one big question here is what the flagships should do for the others. should they do anything? the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.176 the future of scientific knowledge discovery in open networked environments  this will have an effect on access. a lot of science and scholarship can be done by secondary sources. as those secondary sources get better, the quality of scientific scholarship will improve, and it can be ever more distributed. that is a productivity issue as well as a participation issue. therefore, this is a second audience, the combination of locally based research entities and the elites of those locales. a third audience is researchers who are thinking seriously about the approaching change and wondering how to respond. this is important. it is not like it was 40 years ago when the more senior scientists were trained. this is different. what do we do about this? perhaps that is a channel to approach some of the younger people, because they are coming up through the apprenticeship structure. the final, fourth, group that is usually not addressed are the creators and innovatorsšthe facebook and google types and the people who are doing research in these hacking centers. in short, good things can happen in this selfmotivating way. moreover, they do not have to ask for federal money or congressional approval to do it. there is a big difference between just communicating versus the process of scientific knowledge discovery. some of what is being discussed is the need for better tools and applications to do science over facebook and other new approaches. according to some studies, data practices are driven very much by the kinds of tools they use. by talking to people, we can know immediately the kinds of data practices and the kinds of research that they are doing, just by knowing the kind of tools that they are using. one discussant noted that he just got on a big grant with people at the mayo clinic that he has never met in person. he only met them through twitter, but he is now on their grant. his best paper recommendations come through people he follows on twitter. many of his own paper citations can be attributed to tweeting about his work. he also gets invited to conferences through people who follow him on twitter. this is an example of the value of scientific networking. another discussant noted that the topics of creating scientific discoveries and communicating knowledge are being conflated to some extent in this discussion. he uses software and writes code, but does it for a science that can never be practiced on a social network. for example, there are foresters who work with trees that take 50 or 60 years to grow. there is much science that cannot be done socially. at the same time, the communication of science or even enabling the process of science can be done with these tools. he is in user groups, and when he asks for help, people respond to that. he is able to read up quickly and does not have to go through a hierarchy to acquire that knowledge. he can utilize tools that are changeable, because they are all opensource. therefore, separating the functions of creating knowledge from communicating knowledge is important. there may be some overlap, but they are not the same things at all. it is helpful to discriminate between the two and to ask what we get from doing that. it is also useful to note that things that we think of as nailed down, such as mathematical proofs, are not settled until the mathematicians say so. mathematics is essentially a social process. there are some things, such as the proof of fermat™s last theorem, that not all mathematicians agree on at this point. a preponderance of mathematicians think it has been proved, but there are some holdouts who are very strongly opposed to that position. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 177 scholarship is communication. we should not assume that geology is going to be done on the web. geology is probably going to be done with rocks and other things gathered onsite. but the phenomenon of geologyšwhat people agree about or come to hold as geological factšis going to be socially constructed over time, and that is a communication process. the idea that science and communication are two different things is not valid. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 179 5. appendix: workshop agenda the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved. the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 181  the future of scientific knowledge discovery in open networked environments: a national workshop washington, dc, march 1011, 2011 board on research data and information in collaboration with computer science and telecommunications board national academy of sciences march 10 agenda day 1: workshop venable, llp, 8 west conference center capitol room 575 7th street, nw, washington, dc i. opening session session chair: john king, university of michigan 8:30 i.a chair™s welcoming remarks john king, university of michigan  8:40 i.b opening remarks by the project sponsors  sylvia spengler and alan blatecky, nat™l science foundation 9:00 i.c keynote: an overview of the state of the art tony hey, microsoft research  ii. experiences with developing open scientific knowledge discovery environments and using themšcase studies and lessons learned session chair: sara graves, university of alabama at huntsville  9:30 ii.a experiences with developing open scientific knowledge discovery in research and applications  case studies:  international online astronomy research alberto conti, space telescope sci. institute  the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.182 the future of scientific knowledge discovery in open networked environments   integrative genomic analysis: sage bionetworks, seattle, wa (sagebase.org)  stephen friend, sage bionetworks  geoinformatics: linked environments for atmospheric discovery (https://portal.leadproject.org/gridsphere/gridsphere)  kelvin droegemeier, university of oklahoma 10:30 break  10:50 ii.b implications of the three scientific knowledge discovery case studiesšthe user perspective   international online astronomy research  alyssa goodman, harvard university   integrative genomic analysis joel dudley, stanford university  geoinformatics mohan ramamurthy, unidata, ucar  11:50 ii.c benefits and drawbacks of the three case studiespanel discussion of open scientific knowledge discovery system developers and users panelists: same 6 speakers as in ii.a and ii.b, plus session chair  12:30 lunch in venable cafeteria, 8th floor iii. how might open online knowledge discovery advance the progress of science?  13:30 iii.a technological factors session chair: hal abelson, mit  interoperability, standards, and linked data james hendler, rpi  national technological needs and issues  deborah crawford, drexel university  14:30 iii.b sociocultural, institutional, and organizational factors session chair: michael lesk, rutgers university  sociocultural dimensions clifford lynch, the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 183  coalition for networked information   institutional factors  paul edwards, university of michigan  15:30 break 15:50 iii.c policy and legal factors session chair: michael carroll, washington college of law  legal aspects  michael madison, university of pittsburgh school of law  policy issues  gregory jackson, educause  16:50 iii.d how can we tell? what needs to be known and studied to improve potential for success? session chair: fran berman, rpi  government perspective walter warnick, office of s&t information, department of energy  academic perspective victoria stodden, columbia university law school 17:50 concluding remarks john king, university of michigan  17:55 adjourn closed session 18:15 reception and working dinner for committee and speakers  the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.184 the future of scientific knowledge discovery in open networked environments  the future of scientific knowledge discovery in open networked environments: a national workshop washington, dc, march 1011, 2011 board on research data and information in collaboration with computer science and telecommunications board national academy of sciences march 11 agenda day 2: workshop national academy of sciences keck center room 100 500 fifth street nw, washington, dc i. recap of symposium results and discussion of future issues session chair: bonnie carroll, information international associates rapporteurs: puneet kishor, university of wisconsin, and alberto pepe, harvard 8:45 i.a summary of opportunities to automated scientific knowledge discovery in open networked environments  discussion to validate results from the symposium and identify issues in next 510 years 9:45 1.b summary of the barriers  discussion to validate results from the symposium and identify issues in next 510 years 10:45 coffee break 11:00 1.c summary of techniques and methods for development and study of automated scientific knowledge discovery  discussion to validate results from the symposium and identify issues in next 510 years 12:00 lunch in the keck cafeteria 13:00 ii. range of options for further research session chair: john king, university of michigan based on the results obtained in response to the preceding discussions, define a range of options that can be used by the sponsors of the project, as well as other similar organizations, to obtain and promote a better understanding of the computermediated scientific knowledge discovery processes and mechanisms for openly available data and information online across the scientific domains. the objective of defining these options is to improve the activities of the sponsors (and other similar the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved.the future of scientific knowledge discovery in open networked environments 185 organizations) and the activities of researchers that they fund externally in this emerging research area.  discussion 15:30 end of meeting  the future of scientific knowledge discovery in open networked environments: summary of a workshopcopyright national academy of sciences. all rights reserved. 